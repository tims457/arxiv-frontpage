{"created":"2024-03-21 17:59:59","title":"Zero-Shot Multi-Object Shape Completion","abstract":"We present a 3D shape completion method that recovers the complete geometry of multiple objects in complex scenes from a single RGB-D image. Despite notable advancements in single object 3D shape completion, high-quality reconstructions in highly cluttered real-world multi-object scenes remains a challenge. To address this issue, we propose OctMAE, an architecture that leverages an Octree U-Net and a latent 3D MAE to achieve high-quality and near real-time multi-object shape completion through both local and global geometric reasoning. Because a na\\\"ive 3D MAE can be computationally intractable and memory intensive even in the latent space, we introduce a novel occlusion masking strategy and adopt 3D rotary embeddings, which significantly improves the runtime and shape completion quality. To generalize to a wide range of objects in diverse scenes, we create a large-scale photorealistic dataset, featuring a diverse set of 12K 3D object models from the Objaverse dataset which are rendered in multi-object scenes with physics-based positioning. Our method outperforms the current state-of-the-art on both synthetic and real-world datasets and demonstrates a strong zero-shot capability.","sentences":["We present a 3D shape completion method that recovers the complete geometry of multiple objects in complex scenes from a single RGB-D image.","Despite notable advancements in single object 3D shape completion, high-quality reconstructions in highly cluttered real-world multi-object scenes remains a challenge.","To address this issue, we propose OctMAE, an architecture that leverages an Octree U-Net and a latent 3D MAE to achieve high-quality and near real-time multi-object shape completion through both local and global geometric reasoning.","Because a na\\\"ive 3D MAE can be computationally intractable and memory intensive even in the latent space, we introduce a novel occlusion masking strategy and adopt 3D rotary embeddings, which significantly improves the runtime and shape completion quality.","To generalize to a wide range of objects in diverse scenes, we create a large-scale photorealistic dataset, featuring a diverse set of 12K 3D object models from the Objaverse dataset which are rendered in multi-object scenes with physics-based positioning.","Our method outperforms the current state-of-the-art on both synthetic and real-world datasets and demonstrates a strong zero-shot capability."],"url":"http://arxiv.org/abs/2403.14628v1","category":"cs.CV"}
{"created":"2024-03-21 17:59:58","title":"MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images","abstract":"We propose MVSplat, an efficient feed-forward 3D Gaussian Splatting model learned from sparse multi-view images. To accurately localize the Gaussian centers, we propose to build a cost volume representation via plane sweeping in the 3D space, where the cross-view feature similarities stored in the cost volume can provide valuable geometry cues to the estimation of depth. We learn the Gaussian primitives' opacities, covariances, and spherical harmonics coefficients jointly with the Gaussian centers while only relying on photometric supervision. We demonstrate the importance of the cost volume representation in learning feed-forward Gaussian Splatting models via extensive experimental evaluations. On the large-scale RealEstate10K and ACID benchmarks, our model achieves state-of-the-art performance with the fastest feed-forward inference speed (22 fps). Compared to the latest state-of-the-art method pixelSplat, our model uses $10\\times $ fewer parameters and infers more than $2\\times$ faster while providing higher appearance and geometry quality as well as better cross-dataset generalization.","sentences":["We propose MVSplat, an efficient feed-forward 3D Gaussian Splatting model learned from sparse multi-view images.","To accurately localize the Gaussian centers, we propose to build a cost volume representation via plane sweeping in the 3D space, where the cross-view feature similarities stored in the cost volume can provide valuable geometry cues to the estimation of depth.","We learn the Gaussian primitives' opacities, covariances, and spherical harmonics coefficients jointly with the Gaussian centers while only relying on photometric supervision.","We demonstrate the importance of the cost volume representation in learning feed-forward Gaussian Splatting models via extensive experimental evaluations.","On the large-scale RealEstate10K and ACID benchmarks, our model achieves state-of-the-art performance with the fastest feed-forward inference speed (22 fps).","Compared to the latest state-of-the-art method pixelSplat, our model uses $10\\times $ fewer parameters and infers more than $2\\times$ faster while providing higher appearance and geometry quality as well as better cross-dataset generalization."],"url":"http://arxiv.org/abs/2403.14627v1","category":"cs.CV"}
{"created":"2024-03-21 17:59:50","title":"MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?","abstract":"The remarkable progress of Multi-modal Large Language Models (MLLMs) has garnered unparalleled attention, due to their superior performance in visual contexts. However, their capabilities in visual math problem-solving remain insufficiently evaluated and understood. We investigate current benchmarks to incorporate excessive visual content within textual questions, which potentially assist MLLMs in deducing answers without truly interpreting the input diagrams. To this end, we introduce MathVerse, an all-around visual math benchmark designed for an equitable and in-depth evaluation of MLLMs. We meticulously collect 2,612 high-quality, multi-subject math problems with diagrams from publicly available sources. Each problem is then transformed by human annotators into six distinct versions, each offering varying degrees of information content in multi-modality, contributing to 15K test samples in total. This approach allows MathVerse to comprehensively assess whether and how much MLLMs can truly understand the visual diagrams for mathematical reasoning. In addition, we propose a Chain-of-Thought (CoT) evaluation strategy for a fine-grained assessment of the output answers. Rather than naively judging True or False, we employ GPT-4(V) to adaptively extract crucial reasoning steps, and then score each step with detailed error analysis, which can reveal the intermediate CoT reasoning quality by MLLMs. We hope the MathVerse benchmark may provide unique insights to guide the future development of MLLMs. Project page: https://mathverse-cuhk.github.io","sentences":["The remarkable progress of Multi-modal Large Language Models (MLLMs) has garnered unparalleled attention, due to their superior performance in visual contexts.","However, their capabilities in visual math problem-solving remain insufficiently evaluated and understood.","We investigate current benchmarks to incorporate excessive visual content within textual questions, which potentially assist MLLMs in deducing answers without truly interpreting the input diagrams.","To this end, we introduce MathVerse, an all-around visual math benchmark designed for an equitable and in-depth evaluation of MLLMs.","We meticulously collect 2,612 high-quality, multi-subject math problems with diagrams from publicly available sources.","Each problem is then transformed by human annotators into six distinct versions, each offering varying degrees of information content in multi-modality, contributing to 15K test samples in total.","This approach allows MathVerse to comprehensively assess whether and how much MLLMs can truly understand the visual diagrams for mathematical reasoning.","In addition, we propose a Chain-of-Thought (CoT) evaluation strategy for a fine-grained assessment of the output answers.","Rather than naively judging True or False, we employ GPT-4(V) to adaptively extract crucial reasoning steps, and then score each step with detailed error analysis, which can reveal the intermediate CoT reasoning quality by MLLMs.","We hope the MathVerse benchmark may provide unique insights to guide the future development of MLLMs.","Project page: https://mathverse-cuhk.github.io"],"url":"http://arxiv.org/abs/2403.14624v1","category":"cs.CV"}
{"created":"2024-03-21 17:59:41","title":"Simplified Diffusion Schr\u00f6dinger Bridge","abstract":"This paper introduces a novel theoretical simplification of the Diffusion Schr\\\"odinger Bridge (DSB) that facilitates its unification with Score-based Generative Models (SGMs), addressing the limitations of DSB in complex data generation and enabling faster convergence and enhanced performance. By employing SGMs as an initial solution for DSB, our approach capitalizes on the strengths of both frameworks, ensuring a more efficient training process and improving the performance of SGM. We also propose a reparameterization technique that, despite theoretical approximations, practically improves the network's fitting capabilities. Our extensive experimental evaluations confirm the effectiveness of the simplified DSB, demonstrating its significant improvements. We believe the contributions of this work pave the way for advanced generative modeling. The code is available at https://github.com/tzco/Simplified-Diffusion-Schrodinger-Bridge.","sentences":["This paper introduces a novel theoretical simplification of the Diffusion Schr\\\"odinger Bridge (DSB) that facilitates its unification with Score-based Generative Models (SGMs), addressing the limitations of DSB in complex data generation and enabling faster convergence and enhanced performance.","By employing SGMs as an initial solution for DSB, our approach capitalizes on the strengths of both frameworks, ensuring a more efficient training process and improving the performance of SGM.","We also propose a reparameterization technique that, despite theoretical approximations, practically improves the network's fitting capabilities.","Our extensive experimental evaluations confirm the effectiveness of the simplified DSB, demonstrating its significant improvements.","We believe the contributions of this work pave the way for advanced generative modeling.","The code is available at https://github.com/tzco/Simplified-Diffusion-Schrodinger-Bridge."],"url":"http://arxiv.org/abs/2403.14623v1","category":"cs.LG"}
{"created":"2024-03-21 17:59:34","title":"GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction and Generation","abstract":"We introduce GRM, a large-scale reconstructor capable of recovering a 3D asset from sparse-view images in around 0.1s. GRM is a feed-forward transformer-based model that efficiently incorporates multi-view information to translate the input pixels into pixel-aligned Gaussians, which are unprojected to create a set of densely distributed 3D Gaussians representing a scene. Together, our transformer architecture and the use of 3D Gaussians unlock a scalable and efficient reconstruction framework. Extensive experimental results demonstrate the superiority of our method over alternatives regarding both reconstruction quality and efficiency. We also showcase the potential of GRM in generative tasks, i.e., text-to-3D and image-to-3D, by integrating it with existing multi-view diffusion models. Our project website is at: https://justimyhxu.github.io/projects/grm/.","sentences":["We introduce GRM, a large-scale reconstructor capable of recovering a 3D asset from sparse-view images in around 0.1s.","GRM is a feed-forward transformer-based model that efficiently incorporates multi-view information to translate the input pixels into pixel-aligned Gaussians, which are unprojected to create a set of densely distributed 3D Gaussians representing a scene.","Together, our transformer architecture and the use of 3D Gaussians unlock a scalable and efficient reconstruction framework.","Extensive experimental results demonstrate the superiority of our method over alternatives regarding both reconstruction quality and efficiency.","We also showcase the potential of GRM in generative tasks, i.e., text-to-3D and image-to-3D, by integrating it with existing multi-view diffusion models.","Our project website is at: https://justimyhxu.github.io/projects/grm/."],"url":"http://arxiv.org/abs/2403.14621v1","category":"cs.CV"}
{"created":"2024-03-21 17:59:16","title":"ClusteringSDF: Self-Organized Neural Implicit Surfaces for 3D Decomposition","abstract":"3D decomposition/segmentation still remains a challenge as large-scale 3D annotated data is not readily available. Contemporary approaches typically leverage 2D machine-generated segments, integrating them for 3D consistency. While the majority of these methods are based on NeRFs, they face a potential weakness that the instance/semantic embedding features derive from independent MLPs, thus preventing the segmentation network from learning the geometric details of the objects directly through radiance and density. In this paper, we propose ClusteringSDF, a novel approach to achieve both segmentation and reconstruction in 3D via the neural implicit surface representation, specifically Signal Distance Function (SDF), where the segmentation rendering is directly integrated with the volume rendering of neural implicit surfaces. Although based on ObjectSDF++, ClusteringSDF no longer requires the ground-truth segments for supervision while maintaining the capability of reconstructing individual object surfaces, but purely with the noisy and inconsistent labels from pre-trained models.As the core of ClusteringSDF, we introduce a high-efficient clustering mechanism for lifting the 2D labels to 3D and the experimental results on the challenging scenes from ScanNet and Replica datasets show that ClusteringSDF can achieve competitive performance compared against the state-of-the-art with significantly reduced training time.","sentences":["3D decomposition/segmentation still remains a challenge as large-scale 3D annotated data is not readily available.","Contemporary approaches typically leverage 2D machine-generated segments, integrating them for 3D consistency.","While the majority of these methods are based on NeRFs, they face a potential weakness that the instance/semantic embedding features derive from independent MLPs, thus preventing the segmentation network from learning the geometric details of the objects directly through radiance and density.","In this paper, we propose ClusteringSDF, a novel approach to achieve both segmentation and reconstruction in 3D via the neural implicit surface representation, specifically Signal Distance Function (SDF), where the segmentation rendering is directly integrated with the volume rendering of neural implicit surfaces.","Although based on ObjectSDF++, ClusteringSDF no longer requires the ground-truth segments for supervision while maintaining the capability of reconstructing individual object surfaces, but purely with the noisy and inconsistent labels from pre-trained models.","As the core of ClusteringSDF, we introduce a high-efficient clustering mechanism for lifting the 2D labels to 3D and the experimental results on the challenging scenes from ScanNet and Replica datasets show that ClusteringSDF can achieve competitive performance compared against the state-of-the-art with significantly reduced training time."],"url":"http://arxiv.org/abs/2403.14619v1","category":"cs.CV"}
{"created":"2024-03-21 17:59:10","title":"Simulation-Based Inference of the sky-averaged 21-cm signal from CD-EoR with REACH","abstract":"The redshifted 21-cm signal from the Cosmic Dawn and Epoch of Reionization carries invaluable information about the cosmology and astrophysics of the early Universe. Analyzing the data from a sky-averaged 21-cm signal experiment typically involves navigating through an intricate parameter space to accurately address various factors such as foregrounds, beam uncertainties, ionospheric distortions, and receiver noise for the search of the cosmological 21-cm signal. The traditional likelihood-based sampling methods for modeling these effects could become computationally demanding for such highly complex models, which makes it infeasible to include physically motivated 21-cm signal models in the analysis. Moreover, the inference with these traditional methods is driven by the assumed functional form of the likelihood function. This work demonstrates how Simulation-Based Inference through Truncated Marginal Neural Ratio Estimation (TMNRE) can naturally handle these issues at a significantly reduced computational cost than the likelihood-based methods. We estimate the posterior distribution on our model parameters with TMNRE for simulated mock observations, composed of beam-weighted foregrounds, physically motivated 21-cm signal, and radiometric noise. We find that maximizing the information content by simultaneously analyzing the data from multiple time slices and antennas significantly improves the parameter constraints and leads to a better exploration of the cosmological signal. We discuss the application of TMNRE for the current configuration of the REACH experiment and demonstrate how it can be utilized to explore potential avenues for REACH. The method presented here can be easily generalized for any sky-averaged 21-cm signal experiment.","sentences":["The redshifted 21-cm signal from the Cosmic Dawn and Epoch of Reionization carries invaluable information about the cosmology and astrophysics of the early Universe.","Analyzing the data from a sky-averaged 21-cm signal experiment typically involves navigating through an intricate parameter space to accurately address various factors such as foregrounds, beam uncertainties, ionospheric distortions, and receiver noise for the search of the cosmological 21-cm signal.","The traditional likelihood-based sampling methods for modeling these effects could become computationally demanding for such highly complex models, which makes it infeasible to include physically motivated 21-cm signal models in the analysis.","Moreover, the inference with these traditional methods is driven by the assumed functional form of the likelihood function.","This work demonstrates how Simulation-Based Inference through Truncated Marginal Neural Ratio Estimation (TMNRE) can naturally handle these issues at a significantly reduced computational cost than the likelihood-based methods.","We estimate the posterior distribution on our model parameters with TMNRE for simulated mock observations, composed of beam-weighted foregrounds, physically motivated 21-cm signal, and radiometric noise.","We find that maximizing the information content by simultaneously analyzing the data from multiple time slices and antennas significantly improves the parameter constraints and leads to a better exploration of the cosmological signal.","We discuss the application of TMNRE for the current configuration of the REACH experiment and demonstrate how it can be utilized to explore potential avenues for REACH.","The method presented here can be easily generalized for any sky-averaged 21-cm signal experiment."],"url":"http://arxiv.org/abs/2403.14618v1","category":"astro-ph.CO"}
{"created":"2024-03-21 17:59:03","title":"Videoshop: Localized Semantic Video Editing with Noise-Extrapolated Diffusion Inversion","abstract":"We introduce Videoshop, a training-free video editing algorithm for localized semantic edits. Videoshop allows users to use any editing software, including Photoshop and generative inpainting, to modify the first frame; it automatically propagates those changes, with semantic, spatial, and temporally consistent motion, to the remaining frames. Unlike existing methods that enable edits only through imprecise textual instructions, Videoshop allows users to add or remove objects, semantically change objects, insert stock photos into videos, etc. with fine-grained control over locations and appearance. We achieve this through image-based video editing by inverting latents with noise extrapolation, from which we generate videos conditioned on the edited image. Videoshop produces higher quality edits against 6 baselines on 2 editing benchmarks using 10 evaluation metrics.","sentences":["We introduce Videoshop, a training-free video editing algorithm for localized semantic edits.","Videoshop allows users to use any editing software, including Photoshop and generative inpainting, to modify the first frame; it automatically propagates those changes, with semantic, spatial, and temporally consistent motion, to the remaining frames.","Unlike existing methods that enable edits only through imprecise textual instructions, Videoshop allows users to add or remove objects, semantically change objects, insert stock photos into videos, etc. with fine-grained control over locations and appearance.","We achieve this through image-based video editing by inverting latents with noise extrapolation, from which we generate videos conditioned on the edited image.","Videoshop produces higher quality edits against 6 baselines on 2 editing benchmarks using 10 evaluation metrics."],"url":"http://arxiv.org/abs/2403.14617v1","category":"cs.CV"}
{"created":"2024-03-21 17:58:56","title":"Hierarchical Text-to-Vision Self Supervised Alignment for Improved Histopathology Representation Learning","abstract":"Self-supervised representation learning has been highly promising for histopathology image analysis with numerous approaches leveraging their patient-slide-patch hierarchy to learn better representations. In this paper, we explore how the combination of domain specific natural language information with such hierarchical visual representations can benefit rich representation learning for medical image tasks. Building on automated language description generation for features visible in histopathology images, we present a novel language-tied self-supervised learning framework, Hierarchical Language-tied Self-Supervision (HLSS) for histopathology images. We explore contrastive objectives and granular language description based text alignment at multiple hierarchies to inject language modality information into the visual representations. Our resulting model achieves state-of-the-art performance on two medical imaging benchmarks, OpenSRH and TCGA datasets. Our framework also provides better interpretability with our language aligned representation space. Code is available at https://github.com/Hasindri/HLSS.","sentences":["Self-supervised representation learning has been highly promising for histopathology image analysis with numerous approaches leveraging their patient-slide-patch hierarchy to learn better representations.","In this paper, we explore how the combination of domain specific natural language information with such hierarchical visual representations can benefit rich representation learning for medical image tasks.","Building on automated language description generation for features visible in histopathology images, we present a novel language-tied self-supervised learning framework, Hierarchical Language-tied Self-Supervision (HLSS) for histopathology images.","We explore contrastive objectives and granular language description based text alignment at multiple hierarchies to inject language modality information into the visual representations.","Our resulting model achieves state-of-the-art performance on two medical imaging benchmarks, OpenSRH and TCGA datasets.","Our framework also provides better interpretability with our language aligned representation space.","Code is available at https://github.com/Hasindri/HLSS."],"url":"http://arxiv.org/abs/2403.14616v1","category":"cs.CV"}
{"created":"2024-03-21 17:58:04","title":"DreamReward: Text-to-3D Generation with Human Preference","abstract":"3D content creation from text prompts has shown remarkable success recently. However, current text-to-3D methods often generate 3D results that do not align well with human preferences. In this paper, we present a comprehensive framework, coined DreamReward, to learn and improve text-to-3D models from human preference feedback. To begin with, we collect 25k expert comparisons based on a systematic annotation pipeline including rating and ranking. Then, we build Reward3D -- the first general-purpose text-to-3D human preference reward model to effectively encode human preferences. Building upon the 3D reward model, we finally perform theoretical analysis and present the Reward3D Feedback Learning (DreamFL), a direct tuning algorithm to optimize the multi-view diffusion models with a redefined scorer. Grounded by theoretical proof and extensive experiment comparisons, our DreamReward successfully generates high-fidelity and 3D consistent results with significant boosts in prompt alignment with human intention. Our results demonstrate the great potential for learning from human feedback to improve text-to-3D models.","sentences":["3D content creation from text prompts has shown remarkable success recently.","However, current text-to-3D methods often generate 3D results that do not align well with human preferences.","In this paper, we present a comprehensive framework, coined DreamReward, to learn and improve text-to-3D models from human preference feedback.","To begin with, we collect 25k expert comparisons based on a systematic annotation pipeline including rating and ranking.","Then, we build Reward3D -- the first general-purpose text-to-3D human preference reward model to effectively encode human preferences.","Building upon the 3D reward model, we finally perform theoretical analysis and present the Reward3D Feedback Learning (DreamFL), a direct tuning algorithm to optimize the multi-view diffusion models with a redefined scorer.","Grounded by theoretical proof and extensive experiment comparisons, our DreamReward successfully generates high-fidelity and 3D consistent results with significant boosts in prompt alignment with human intention.","Our results demonstrate the great potential for learning from human feedback to improve text-to-3D models."],"url":"http://arxiv.org/abs/2403.14613v1","category":"cs.CV"}
{"created":"2024-03-21 17:57:31","title":"Explorative Inbetweening of Time and Space","abstract":"We introduce bounded generation as a generalized task to control video generation to synthesize arbitrary camera and subject motion based only on a given start and end frame. Our objective is to fully leverage the inherent generalization capability of an image-to-video model without additional training or fine-tuning of the original model. This is achieved through the proposed new sampling strategy, which we call Time Reversal Fusion, that fuses the temporally forward and backward denoising paths conditioned on the start and end frame, respectively. The fused path results in a video that smoothly connects the two frames, generating inbetweening of faithful subject motion, novel views of static scenes, and seamless video looping when the two bounding frames are identical. We curate a diverse evaluation dataset of image pairs and compare against the closest existing methods. We find that Time Reversal Fusion outperforms related work on all subtasks, exhibiting the ability to generate complex motions and 3D-consistent views guided by bounded frames. See project page at https://time-reversal.github.io.","sentences":["We introduce bounded generation as a generalized task to control video generation to synthesize arbitrary camera and subject motion based only on a given start and end frame.","Our objective is to fully leverage the inherent generalization capability of an image-to-video model without additional training or fine-tuning of the original model.","This is achieved through the proposed new sampling strategy, which we call Time Reversal Fusion, that fuses the temporally forward and backward denoising paths conditioned on the start and end frame, respectively.","The fused path results in a video that smoothly connects the two frames, generating inbetweening of faithful subject motion, novel views of static scenes, and seamless video looping when the two bounding frames are identical.","We curate a diverse evaluation dataset of image pairs and compare against the closest existing methods.","We find that Time Reversal Fusion outperforms related work on all subtasks, exhibiting the ability to generate complex motions and 3D-consistent views guided by bounded frames.","See project page at https://time-reversal.github.io."],"url":"http://arxiv.org/abs/2403.14611v1","category":"cs.CV"}
{"created":"2024-03-21 17:57:03","title":"T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy","abstract":"We present T-Rex2, a highly practical model for open-set object detection. Previous open-set object detection methods relying on text prompts effectively encapsulate the abstract concept of common objects, but struggle with rare or complex object representation due to data scarcity and descriptive limitations. Conversely, visual prompts excel in depicting novel objects through concrete visual examples, but fall short in conveying the abstract concept of objects as effectively as text prompts. Recognizing the complementary strengths and weaknesses of both text and visual prompts, we introduce T-Rex2 that synergizes both prompts within a single model through contrastive learning. T-Rex2 accepts inputs in diverse formats, including text prompts, visual prompts, and the combination of both, so that it can handle different scenarios by switching between the two prompt modalities. Comprehensive experiments demonstrate that T-Rex2 exhibits remarkable zero-shot object detection capabilities across a wide spectrum of scenarios. We show that text prompts and visual prompts can benefit from each other within the synergy, which is essential to cover massive and complicated real-world scenarios and pave the way towards generic object detection. Model API is now available at \\url{https://github.com/IDEA-Research/T-Rex}.","sentences":["We present T-Rex2, a highly practical model for open-set object detection.","Previous open-set object detection methods relying on text prompts effectively encapsulate the abstract concept of common objects, but struggle with rare or complex object representation due to data scarcity and descriptive limitations.","Conversely, visual prompts excel in depicting novel objects through concrete visual examples, but fall short in conveying the abstract concept of objects as effectively as text prompts.","Recognizing the complementary strengths and weaknesses of both text and visual prompts, we introduce T-Rex2 that synergizes both prompts within a single model through contrastive learning.","T-Rex2 accepts inputs in diverse formats, including text prompts, visual prompts, and the combination of both, so that it can handle different scenarios by switching between the two prompt modalities.","Comprehensive experiments demonstrate that T-Rex2 exhibits remarkable zero-shot object detection capabilities across a wide spectrum of scenarios.","We show that text prompts and visual prompts can benefit from each other within the synergy, which is essential to cover massive and complicated real-world scenarios and pave the way towards generic object detection.","Model API is now available at \\url{https://github.com/IDEA-Research/T-Rex}."],"url":"http://arxiv.org/abs/2403.14610v1","category":"cs.CV"}
{"created":"2024-03-21 17:55:16","title":"The Elements of Differentiable Programming","abstract":"Artificial intelligence has recently experienced remarkable advances, fueled by large models, vast datasets, accelerated hardware, and, last but not least, the transformative power of differentiable programming. This new programming paradigm enables end-to-end differentiation of complex computer programs (including those with control flows and data structures), making gradient-based optimization of program parameters possible. As an emerging paradigm, differentiable programming builds upon several areas of computer science and applied mathematics, including automatic differentiation, graphical models, optimization and statistics. This book presents a comprehensive review of the fundamental concepts useful for differentiable programming. We adopt two main perspectives, that of optimization and that of probability, with clear analogies between the two. Differentiable programming is not merely the differentiation of programs, but also the thoughtful design of programs intended for differentiation. By making programs differentiable, we inherently introduce probability distributions over their execution, providing a means to quantify the uncertainty associated with program outputs.","sentences":["Artificial intelligence has recently experienced remarkable advances, fueled by large models, vast datasets, accelerated hardware, and, last but not least, the transformative power of differentiable programming.","This new programming paradigm enables end-to-end differentiation of complex computer programs (including those with control flows and data structures), making gradient-based optimization of program parameters possible.","As an emerging paradigm, differentiable programming builds upon several areas of computer science and applied mathematics, including automatic differentiation, graphical models, optimization and statistics.","This book presents a comprehensive review of the fundamental concepts useful for differentiable programming.","We adopt two main perspectives, that of optimization and that of probability, with clear analogies between the two.","Differentiable programming is not merely the differentiation of programs, but also the thoughtful design of programs intended for differentiation.","By making programs differentiable, we inherently introduce probability distributions over their execution, providing a means to quantify the uncertainty associated with program outputs."],"url":"http://arxiv.org/abs/2403.14606v1","category":"cs.LG"}
{"created":"2024-03-21 17:54:56","title":"SDP Synthesis of Maximum Coverage Trees for Probabilistic Planning under Control Constraints","abstract":"The paper presents Maximal Covariance Backward Reachable Trees (MAXCOVAR BRT), which is a multi-query algorithm for planning of dynamic systems under stochastic motion uncertainty and constraints on the control input with explicit coverage guarantees. In contrast to existing roadmap-based probabilistic planning methods that sample belief nodes randomly and draw edges between them \\cite{csbrm_tro2024}, under control constraints, the reachability of belief nodes needs to be explicitly established and is determined by checking the feasibility of a non-convex program. Moreover, there is no explicit consideration of coverage of the roadmap while adding nodes and edges during the construction procedure for the existing methods. Our contribution is a novel optimization formulation to add nodes and construct the corresponding edge controllers such that the generated roadmap results in provably maximal coverage under control constraints as compared to any other method of adding nodes and edges. We characterize formally the notion of coverage of a roadmap in this stochastic domain via introduction of the h-$\\operatorname{BRS}$ (Backward Reachable Set of Distributions) of a tree of distributions under control constraints, and also support our method with extensive simulations on a 6 DoF model.","sentences":["The paper presents Maximal Covariance Backward Reachable Trees (MAXCOVAR BRT), which is a multi-query algorithm for planning of dynamic systems under stochastic motion uncertainty and constraints on the control input with explicit coverage guarantees.","In contrast to existing roadmap-based probabilistic planning methods that sample belief nodes randomly and draw edges between them \\cite{csbrm_tro2024}, under control constraints, the reachability of belief nodes needs to be explicitly established and is determined by checking the feasibility of a non-convex program.","Moreover, there is no explicit consideration of coverage of the roadmap while adding nodes and edges during the construction procedure for the existing methods.","Our contribution is a novel optimization formulation to add nodes and construct the corresponding edge controllers such that the generated roadmap results in provably maximal coverage under control constraints as compared to any other method of adding nodes and edges.","We characterize formally the notion of coverage of a roadmap in this stochastic domain via introduction of the h-$\\operatorname{BRS}$ (Backward Reachable Set of Distributions) of a tree of distributions under control constraints, and also support our method with extensive simulations on a 6 DoF model."],"url":"http://arxiv.org/abs/2403.14605v1","category":"cs.RO"}
{"created":"2024-03-21 17:52:08","title":"ReNoise: Real Image Inversion Through Iterative Noising","abstract":"Recent advancements in text-guided diffusion models have unlocked powerful image manipulation capabilities. However, applying these methods to real images necessitates the inversion of the images into the domain of the pretrained diffusion model. Achieving faithful inversion remains a challenge, particularly for more recent models trained to generate images with a small number of denoising steps. In this work, we introduce an inversion method with a high quality-to-operation ratio, enhancing reconstruction accuracy without increasing the number of operations. Building on reversing the diffusion sampling process, our method employs an iterative renoising mechanism at each inversion sampling step. This mechanism refines the approximation of a predicted point along the forward diffusion trajectory, by iteratively applying the pretrained diffusion model, and averaging these predictions. We evaluate the performance of our ReNoise technique using various sampling algorithms and models, including recent accelerated diffusion models. Through comprehensive evaluations and comparisons, we show its effectiveness in terms of both accuracy and speed. Furthermore, we confirm that our method preserves editability by demonstrating text-driven image editing on real images.","sentences":["Recent advancements in text-guided diffusion models have unlocked powerful image manipulation capabilities.","However, applying these methods to real images necessitates the inversion of the images into the domain of the pretrained diffusion model.","Achieving faithful inversion remains a challenge, particularly for more recent models trained to generate images with a small number of denoising steps.","In this work, we introduce an inversion method with a high quality-to-operation ratio, enhancing reconstruction accuracy without increasing the number of operations.","Building on reversing the diffusion sampling process, our method employs an iterative renoising mechanism at each inversion sampling step.","This mechanism refines the approximation of a predicted point along the forward diffusion trajectory, by iteratively applying the pretrained diffusion model, and averaging these predictions.","We evaluate the performance of our ReNoise technique using various sampling algorithms and models, including recent accelerated diffusion models.","Through comprehensive evaluations and comparisons, we show its effectiveness in terms of both accuracy and speed.","Furthermore, we confirm that our method preserves editability by demonstrating text-driven image editing on real images."],"url":"http://arxiv.org/abs/2403.14602v1","category":"cs.CV"}
{"created":"2024-03-21 17:51:04","title":"Wall-Crossing Effects on Quiver BPS Algebras","abstract":"BPS states in supersymmetric theories can admit additional algebro-geometric structures in their spectra, described as quiver Yangian algebras. Equivariant fixed points on the quiver variety are interpreted as vectors populating a representation module, and matrix elements for the generators are then defined as Duistermaat-Heckman integrals in the vicinity of these points. The well-known wall-crossing phenomena is that the fixed point spectrum establishes a dependence on the stability (Fayet-Illiopolous) parameters $\\zeta$, jumping abruptly across the walls of marginal stability, which divide the $\\zeta$-space into a collection of stability chambers -- \"phases\" of the theory. The standard construction of the quiver Yangian algebra relies heavily on the molten crystal model, valid in a sole cyclic chamber where all the $\\zeta$-parameters have the same sign. We propose to lift this restriction and investigate the effects of the wall-crossing phenomena on the quiver Yangian algebra and its representations -- starting with the example of affine super-Yangian $\\mathsf{Y}(\\widehat{\\mathfrak{gl}}_{1|1})$. In addition to the molten crystal construction more general atomic structures appear, in other non-cyclic phases (chambers of the $\\zeta$-space). We call them \\emph{glasses} and also divide in a few different classes. For some of the new phases we manage to associate an algebraic structure again as a representation of the same affine Yangian $\\mathsf{Y}(\\widehat{\\mathfrak{gl}}_{1|1})$. This observation supports an earlier conjecture that the BPS algebraic structures can be considered as the new wall-crossing invariants.","sentences":["BPS states in supersymmetric theories can admit additional algebro-geometric structures in their spectra, described as quiver Yangian algebras.","Equivariant fixed points on the quiver variety are interpreted as vectors populating a representation module, and matrix elements for the generators are then defined as Duistermaat-Heckman integrals in the vicinity of these points.","The well-known wall-crossing phenomena is that the fixed point spectrum establishes a dependence on the stability (Fayet-Illiopolous) parameters $\\zeta$, jumping abruptly across the walls of marginal stability, which divide the $\\zeta$-space into a collection of stability chambers -- \"phases\" of the theory.","The standard construction of the quiver Yangian algebra relies heavily on the molten crystal model, valid in a sole cyclic chamber where all the $\\zeta$-parameters have the same sign.","We propose to lift this restriction and investigate the effects of the wall-crossing phenomena on the quiver Yangian algebra and its representations -- starting with the example of affine super-Yangian $\\mathsf{Y}(\\widehat{\\mathfrak{gl}}_{1|1})$. In addition to the molten crystal construction more general atomic structures appear, in other non-cyclic phases (chambers of the $\\zeta$-space).","We call them \\emph{glasses} and also divide in a few different classes.","For some of the new phases we manage to associate an algebraic structure again as a representation of the same affine Yangian $\\mathsf{Y}(\\widehat{\\mathfrak{gl}}_{1|1})$. This observation supports an earlier conjecture that the BPS algebraic structures can be considered as the new wall-crossing invariants."],"url":"http://arxiv.org/abs/2403.14600v1","category":"hep-th"}
{"created":"2024-03-21 17:51:01","title":"MyVLM: Personalizing VLMs for User-Specific Queries","abstract":"Recent large-scale vision-language models (VLMs) have demonstrated remarkable capabilities in understanding and generating textual descriptions for visual content. However, these models lack an understanding of user-specific concepts. In this work, we take a first step toward the personalization of VLMs, enabling them to learn and reason over user-provided concepts. For example, we explore whether these models can learn to recognize you in an image and communicate what you are doing, tailoring the model to reflect your personal experiences and relationships. To effectively recognize a variety of user-specific concepts, we augment the VLM with external concept heads that function as toggles for the model, enabling the VLM to identify the presence of specific target concepts in a given image. Having recognized the concept, we learn a new concept embedding in the intermediate feature space of the VLM. This embedding is tasked with guiding the language model to naturally integrate the target concept in its generated response. We apply our technique to BLIP-2 and LLaVA for personalized image captioning and further show its applicability for personalized visual question-answering. Our experiments demonstrate our ability to generalize to unseen images of learned concepts while preserving the model behavior on unrelated inputs.","sentences":["Recent large-scale vision-language models (VLMs) have demonstrated remarkable capabilities in understanding and generating textual descriptions for visual content.","However, these models lack an understanding of user-specific concepts.","In this work, we take a first step toward the personalization of VLMs, enabling them to learn and reason over user-provided concepts.","For example, we explore whether these models can learn to recognize you in an image and communicate what you are doing, tailoring the model to reflect your personal experiences and relationships.","To effectively recognize a variety of user-specific concepts, we augment the VLM with external concept heads that function as toggles for the model, enabling the VLM to identify the presence of specific target concepts in a given image.","Having recognized the concept, we learn a new concept embedding in the intermediate feature space of the VLM.","This embedding is tasked with guiding the language model to naturally integrate the target concept in its generated response.","We apply our technique to BLIP-2 and LLaVA for personalized image captioning and further show its applicability for personalized visual question-answering.","Our experiments demonstrate our ability to generalize to unseen images of learned concepts while preserving the model behavior on unrelated inputs."],"url":"http://arxiv.org/abs/2403.14599v1","category":"cs.CV"}
{"created":"2024-03-21 17:50:47","title":"PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model","abstract":"PSALM is a powerful extension of the Large Multi-modal Model (LMM) to address the segmentation task challenges. To overcome the limitation of the LMM being limited to textual output, PSALM incorporates a mask decoder and a well-designed input schema to handle a variety of segmentation tasks. This schema includes images, task instructions, conditional prompts, and mask tokens, which enable the model to generate and classify segmentation masks effectively. The flexible design of PSALM supports joint training across multiple datasets and tasks, leading to improved performance and task generalization. PSALM achieves superior results on several benchmarks, such as RefCOCO/RefCOCO+/RefCOCOg, COCO Panoptic Segmentation, and COCO-Interactive, and further exhibits zero-shot capabilities on unseen tasks, such as open-vocabulary segmentation, generalized referring expression segmentation and video object segmentation, making a significant step towards a GPT moment in computer vision. Through extensive experiments, PSALM demonstrates its potential to transform the domain of image segmentation, leveraging the robust visual understanding capabilities of LMMs as seen in natural language processing. Code and models are available at https://github.com/zamling/PSALM.","sentences":["PSALM is a powerful extension of the Large Multi-modal Model (LMM) to address the segmentation task challenges.","To overcome the limitation of the LMM being limited to textual output, PSALM incorporates a mask decoder and a well-designed input schema to handle a variety of segmentation tasks.","This schema includes images, task instructions, conditional prompts, and mask tokens, which enable the model to generate and classify segmentation masks effectively.","The flexible design of PSALM supports joint training across multiple datasets and tasks, leading to improved performance and task generalization.","PSALM achieves superior results on several benchmarks, such as RefCOCO/RefCOCO+/RefCOCOg, COCO Panoptic Segmentation, and COCO-Interactive, and further exhibits zero-shot capabilities on unseen tasks, such as open-vocabulary segmentation, generalized referring expression segmentation and video object segmentation, making a significant step towards a GPT moment in computer vision.","Through extensive experiments, PSALM demonstrates its potential to transform the domain of image segmentation, leveraging the robust visual understanding capabilities of LMMs as seen in natural language processing.","Code and models are available at https://github.com/zamling/PSALM."],"url":"http://arxiv.org/abs/2403.14598v1","category":"cs.CV"}
{"created":"2024-03-21 17:50:22","title":"Extended Reality for Enhanced Human-Robot Collaboration: a Human-in-the-Loop Approach","abstract":"The rise of automation has provided an opportunity to achieve higher efficiency in manufacturing processes, yet it often compromises the flexibility required to promptly respond to evolving market needs and meet the demand for customization. Human-robot collaboration attempts to tackle these challenges by combining the strength and precision of machines with human ingenuity and perceptual understanding. In this paper, we conceptualize and propose an implementation framework for an autonomous, machine learning-based manipulator that incorporates human-in-the-loop principles and leverages Extended Reality (XR) to facilitate intuitive communication and programming between humans and robots. Furthermore, the conceptual framework foresees human involvement directly in the robot learning process, resulting in higher adaptability and task generalization. The paper highlights key technologies enabling the proposed framework, emphasizing the importance of developing the digital ecosystem as a whole. Additionally, we review the existent implementation approaches of XR in human-robot collaboration, showcasing diverse perspectives and methodologies. The challenges and future outlooks are discussed, delving into the major obstacles and potential research avenues of XR for more natural human-robot interaction and integration in the industrial landscape.","sentences":["The rise of automation has provided an opportunity to achieve higher efficiency in manufacturing processes, yet it often compromises the flexibility required to promptly respond to evolving market needs and meet the demand for customization.","Human-robot collaboration attempts to tackle these challenges by combining the strength and precision of machines with human ingenuity and perceptual understanding.","In this paper, we conceptualize and propose an implementation framework for an autonomous, machine learning-based manipulator that incorporates human-in-the-loop principles and leverages Extended Reality (XR) to facilitate intuitive communication and programming between humans and robots.","Furthermore, the conceptual framework foresees human involvement directly in the robot learning process, resulting in higher adaptability and task generalization.","The paper highlights key technologies enabling the proposed framework, emphasizing the importance of developing the digital ecosystem as a whole.","Additionally, we review the existent implementation approaches of XR in human-robot collaboration, showcasing diverse perspectives and methodologies.","The challenges and future outlooks are discussed, delving into the major obstacles and potential research avenues of XR for more natural human-robot interaction and integration in the industrial landscape."],"url":"http://arxiv.org/abs/2403.14597v1","category":"cs.RO"}
{"created":"2024-03-21 17:49:53","title":"Shadows and Properties of Spin-Induced Scalarized Black Holes with and without a Ricci Coupling","abstract":"In this work, we explore the properties and shadows of spin-induced scalarized black holes, as well as investigate how a Ricci coupling influences them. Our findings reveal significant deviations from the Kerr metric in terms of the location and geodesic frequencies of the innermost stable circular orbit and light ring, with the former exhibiting more pronounced disparities. The shadows of scalarized black holes exhibit relatively minor deviations when compared to those of Kerr black holes with the same mass and spin. Overall, the presence of a Ricci coupling is observed to mitigate deviations from the Kerr metric.","sentences":["In this work, we explore the properties and shadows of spin-induced scalarized black holes, as well as investigate how a Ricci coupling influences them.","Our findings reveal significant deviations from the Kerr metric in terms of the location and geodesic frequencies of the innermost stable circular orbit and light ring, with the former exhibiting more pronounced disparities.","The shadows of scalarized black holes exhibit relatively minor deviations when compared to those of Kerr black holes with the same mass and spin.","Overall, the presence of a Ricci coupling is observed to mitigate deviations from the Kerr metric."],"url":"http://arxiv.org/abs/2403.14596v1","category":"gr-qc"}
{"created":"2024-03-21 17:47:28","title":"Envisioning the Next-Generation AI Coding Assistants: Insights & Proposals","abstract":"As a research-product hybrid group in AI for Software Engineering (AI4SE), we present four key takeaways from our experience developing in-IDE AI coding assistants. AI coding assistants should set clear expectations for usage, integrate with advanced IDE capabilities and existing extensions, use extendable backend designs, and collect app data responsibly for downstream analyses. We propose open questions and challenges that academia and industry should address to realize the vision of next-generation AI coding assistants.","sentences":["As a research-product hybrid group in AI for Software Engineering (AI4SE), we present four key takeaways from our experience developing in-IDE AI coding assistants.","AI coding assistants should set clear expectations for usage, integrate with advanced IDE capabilities and existing extensions, use extendable backend designs, and collect app data responsibly for downstream analyses.","We propose open questions and challenges that academia and industry should address to realize the vision of next-generation AI coding assistants."],"url":"http://arxiv.org/abs/2403.14592v1","category":"cs.SE"}
{"created":"2024-03-21 17:43:44","title":"ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for Contrastive Self-Training","abstract":"Language agents have demonstrated autonomous decision-making abilities by reasoning with foundation models. Recently, efforts have been made to train language agents for performance improvement, with multi-step reasoning and action trajectories as the training data. However, collecting such trajectories still requires considerable human effort, by either artificial annotations or implementations of diverse prompting frameworks. In this work, we propose A$^3$T, a framework that enables the Autonomous Annotation of Agent Trajectories in the style of ReAct. The central role is an ActRe prompting agent, which explains the reason for an arbitrary action. When randomly sampling an external action, the ReAct-style agent could query the ActRe agent with the action to obtain its textual rationales. Novel trajectories are then synthesized by prepending the posterior reasoning from ActRe to the sampled action. In this way, the ReAct-style agent executes multiple trajectories for the failed tasks, and selects the successful ones to supplement its failed trajectory for contrastive self-training. Realized by policy gradient methods with binarized rewards, the contrastive self-training with accumulated trajectories facilitates a closed loop for multiple rounds of language agent self-improvement. We conduct experiments using QLoRA fine-tuning with the open-sourced Mistral-7B-Instruct-v0.2. In AlfWorld, the agent trained with A$^3$T obtains a 1-shot success rate of 96%, and 100% success with 4 iterative rounds. In WebShop, the 1-shot performance of the A$^3$T agent matches human average, and 4 rounds of iterative refinement lead to the performance approaching human experts. A$^3$T agents significantly outperform existing techniques, including prompting with GPT-4, advanced agent frameworks, and fully fine-tuned LLMs.","sentences":["Language agents have demonstrated autonomous decision-making abilities by reasoning with foundation models.","Recently, efforts have been made to train language agents for performance improvement, with multi-step reasoning and action trajectories as the training data.","However, collecting such trajectories still requires considerable human effort, by either artificial annotations or implementations of diverse prompting frameworks.","In this work, we propose A$^3$T, a framework that enables the Autonomous Annotation of Agent Trajectories in the style of ReAct.","The central role is an ActRe prompting agent, which explains the reason for an arbitrary action.","When randomly sampling an external action, the ReAct-style agent could query the ActRe agent with the action to obtain its textual rationales.","Novel trajectories are then synthesized by prepending the posterior reasoning from ActRe to the sampled action.","In this way, the ReAct-style agent executes multiple trajectories for the failed tasks, and selects the successful ones to supplement its failed trajectory for contrastive self-training.","Realized by policy gradient methods with binarized rewards, the contrastive self-training with accumulated trajectories facilitates a closed loop for multiple rounds of language agent self-improvement.","We conduct experiments using QLoRA fine-tuning with the open-sourced Mistral-7B-Instruct-v0.2.","In AlfWorld, the agent trained with A$^3$T obtains a 1-shot success rate of 96%, and 100% success with 4 iterative rounds.","In WebShop, the 1-shot performance of the A$^3$T agent matches human average, and 4 rounds of iterative refinement lead to the performance approaching human experts.","A$^3$T agents significantly outperform existing techniques, including prompting with GPT-4, advanced agent frameworks, and fully fine-tuned LLMs."],"url":"http://arxiv.org/abs/2403.14589v1","category":"cs.AI"}
{"created":"2024-03-21 17:41:03","title":"Slow decay of correlations for generic mixing automorphisms","abstract":"Given $\\psi(n)\\to +0$ and non-zero $f\\in L_2$, we show that for the generic mixing automorphisms $T$ the set $\\{n\\,:\\, |(T^nf,f)|>\\psi(n)\\}$ is infinite.","sentences":["Given $\\psi(n)\\to +0$ and non-zero $f\\in L_2$, we show that for the generic mixing automorphisms $T$ the set $\\{n\\,:\\, |(T^nf,f)|>\\psi(n)\\}$ is infinite."],"url":"http://arxiv.org/abs/2403.14585v1","category":"math.DS"}
{"created":"2024-03-21 17:36:08","title":"Large Language Models for Multi-Choice Question Classification of Medical Subjects","abstract":"The aim of this paper is to evaluate whether large language models trained on multi-choice question data can be used to discriminate between medical subjects. This is an important and challenging task for automatic question answering. To achieve this goal, we train deep neural networks for multi-class classification of questions into the inferred medical subjects. Using our Multi-Question (MQ) Sequence-BERT method, we outperform the state-of-the-art results on the MedMCQA dataset with an accuracy of 0.68 and 0.60 on their development and test sets, respectively. In this sense, we show the capability of AI and LLMs in particular for multi-classification tasks in the Healthcare domain.","sentences":["The aim of this paper is to evaluate whether large language models trained on multi-choice question data can be used to discriminate between medical subjects.","This is an important and challenging task for automatic question answering.","To achieve this goal, we train deep neural networks for multi-class classification of questions into the inferred medical subjects.","Using our Multi-Question (MQ) Sequence-BERT method, we outperform the state-of-the-art results on the MedMCQA dataset with an accuracy of 0.68 and 0.60 on their development and test sets, respectively.","In this sense, we show the capability of AI and LLMs in particular for multi-classification tasks in the Healthcare domain."],"url":"http://arxiv.org/abs/2403.14582v1","category":"cs.CL"}
{"created":"2024-03-21 17:24:43","title":"Generating Photon Pairs in a Hybrid Si-BTO Platform","abstract":"Here we show photon pair generation from ring resonator and waveguide structures in a hybrid silicon-BTO on insulator platform with a pulsed pump. Our analysis of single photon and coincidence generation rates show that Spontaneous Four-Wave Mixing is comparable to that expected from SOI devices of similar characteristics and find a $\\gamma_{eff}$ of (14.7 $\\pm$ 1.3) and (2.0 $\\pm$ 0.3) MHz/mW$^{2}$ for ring resonator and waveguide structures respectively.","sentences":["Here we show photon pair generation from ring resonator and waveguide structures in a hybrid silicon-BTO on insulator platform with a pulsed pump.","Our analysis of single photon and coincidence generation rates show that Spontaneous Four-Wave Mixing is comparable to that expected from SOI devices of similar characteristics and find a $\\gamma_{eff}$ of (14.7 $\\pm$ 1.3) and (2.0 $\\pm$ 0.3) MHz/mW$^{2}$ for ring resonator and waveguide structures respectively."],"url":"http://arxiv.org/abs/2403.14575v1","category":"quant-ph"}
{"created":"2024-03-21 17:21:04","title":"A subtraction scheme for processes involving fragmentation functions at NLO","abstract":"We present a novel subtraction method to remove the soft and collinear divergences at next-to-leading order for processes involving an arbitrary number of fragmentation functions, where this method acts directly in the hadronic centre-of-mass frame. We provide the analytical formulae of the subtraction terms in the general case where all the final state partons fragment to hadrons and for the two special cases when one of the partons of the final state does not fragment, i.e. it is a photon or involved in a jet.","sentences":["We present a novel subtraction method to remove the soft and collinear divergences at next-to-leading order for processes involving an arbitrary number of fragmentation functions, where this method acts directly in the hadronic centre-of-mass frame.","We provide the analytical formulae of the subtraction terms in the general case where all the final state partons fragment to hadrons and for the two special cases when one of the partons of the final state does not fragment, i.e. it is a photon or involved in a jet."],"url":"http://arxiv.org/abs/2403.14574v1","category":"hep-ph"}
{"created":"2024-03-21 17:20:21","title":"Implicit Style-Content Separation using B-LoRA","abstract":"Image stylization involves manipulating the visual appearance and texture (style) of an image while preserving its underlying objects, structures, and concepts (content). The separation of style and content is essential for manipulating the image's style independently from its content, ensuring a harmonious and visually pleasing result. Achieving this separation requires a deep understanding of both the visual and semantic characteristics of images, often necessitating the training of specialized models or employing heavy optimization. In this paper, we introduce B-LoRA, a method that leverages LoRA (Low-Rank Adaptation) to implicitly separate the style and content components of a single image, facilitating various image stylization tasks. By analyzing the architecture of SDXL combined with LoRA, we find that jointly learning the LoRA weights of two specific blocks (referred to as B-LoRAs) achieves style-content separation that cannot be achieved by training each B-LoRA independently. Consolidating the training into only two blocks and separating style and content allows for significantly improving style manipulation and overcoming overfitting issues often associated with model fine-tuning. Once trained, the two B-LoRAs can be used as independent components to allow various image stylization tasks, including image style transfer, text-based image stylization, consistent style generation, and style-content mixing.","sentences":["Image stylization involves manipulating the visual appearance and texture (style) of an image while preserving its underlying objects, structures, and concepts (content).","The separation of style and content is essential for manipulating the image's style independently from its content, ensuring a harmonious and visually pleasing result.","Achieving this separation requires a deep understanding of both the visual and semantic characteristics of images, often necessitating the training of specialized models or employing heavy optimization.","In this paper, we introduce B-LoRA, a method that leverages LoRA (Low-Rank Adaptation) to implicitly separate the style and content components of a single image, facilitating various image stylization tasks.","By analyzing the architecture of SDXL combined with LoRA, we find that jointly learning the LoRA weights of two specific blocks (referred to as B-LoRAs) achieves style-content separation that cannot be achieved by training each B-LoRA independently.","Consolidating the training into only two blocks and separating style and content allows for significantly improving style manipulation and overcoming overfitting issues often associated with model fine-tuning.","Once trained, the two B-LoRAs can be used as independent components to allow various image stylization tasks, including image style transfer, text-based image stylization, consistent style generation, and style-content mixing."],"url":"http://arxiv.org/abs/2403.14572v1","category":"cs.CV"}
{"created":"2024-03-21 17:16:25","title":"On the group cohomology of groups of the form $\\mathbb{Z}^n\\rtimes \\mathbb{Z}/m$ with $m$ free of squares","abstract":"We provide an explicit computation of the cohomology groups (with untwisted coefficients) of semidirect products of the form $\\mathbb{Z}^n\\rtimes \\mathbb{Z}/m$ with $m$ free of squares, by means of formulas that only depend on $n$, $m$ and the action of $\\mathbb{Z}/m$ on $\\mathbb{Z}^n$. We want to highlight the fact that we are not impossing any conditions on the $\\mathbb{Z}/m$-action on $\\mathbb{Z}^n$, and as far as we know our formulas are the first in the literature in this generality. This generalizes previous computations of L\\\"uck-Davis and Adem-Ge-Pan-Petrosyan. In order to show that our formulas are usable, we develop a concrete example of the form $\\mathbb{Z}^5\\rtimes \\mathbb{Z}/6$ where its cohomology groups are described in full detail.","sentences":["We provide an explicit computation of the cohomology groups (with untwisted coefficients) of semidirect products of the form $\\mathbb{Z}^n\\rtimes \\mathbb{Z}/m$ with $m$ free of squares, by means of formulas that only depend on $n$, $m$ and the action of $\\mathbb{Z}/m$ on $\\mathbb{Z}^n$. We want to highlight the fact that we are not impossing any conditions on the $\\mathbb{Z}/m$-action on $\\mathbb{Z}^n$, and as far as we know our formulas are the first in the literature in this generality.","This generalizes previous computations of L\\\"uck-Davis and Adem-Ge-Pan-Petrosyan.","In order to show that our formulas are usable, we develop a concrete example of the form $\\mathbb{Z}^5\\rtimes \\mathbb{Z}/6$ where its cohomology groups are described in full detail."],"url":"http://arxiv.org/abs/2403.14569v1","category":"math.AT"}
{"created":"2024-03-21 17:09:20","title":"A survey on Concept-based Approaches For Model Improvement","abstract":"The focus of recent research has shifted from merely increasing the Deep Neural Networks (DNNs) performance in various tasks to DNNs, which are more interpretable to humans. The field of eXplainable Artificial Intelligence (XAI) has observed various techniques, including saliency-based and concept-based approaches. Concept-based approaches explain the model's decisions in simple human understandable terms called Concepts. Concepts are human interpretable units of data and are the thinking ground of humans. Explanations in terms of concepts enable detecting spurious correlations, inherent biases, or clever-hans. With the advent of concept-based explanations, there have been various concept representation methods and automatic concept discovery algorithms. Some recent methods use concepts for post-hoc model disentanglement evaluation, while others use them for ante-hoc training. The concept-based approaches are new, with many representations coming up, and there is very limited work on Concept-based Model improvement. We provide a systematic review and taxonomy of various concept representations and their discovery algorithms in DNNs, specifically in vision. We also provide details on concept-based model improvement literature, which is the first to survey concept-based model improvement methods.","sentences":["The focus of recent research has shifted from merely increasing the Deep Neural Networks (DNNs) performance in various tasks to DNNs, which are more interpretable to humans.","The field of eXplainable Artificial Intelligence (XAI) has observed various techniques, including saliency-based and concept-based approaches.","Concept-based approaches explain the model's decisions in simple human understandable terms called Concepts.","Concepts are human interpretable units of data and are the thinking ground of humans.","Explanations in terms of concepts enable detecting spurious correlations, inherent biases, or clever-hans.","With the advent of concept-based explanations, there have been various concept representation methods and automatic concept discovery algorithms.","Some recent methods use concepts for post-hoc model disentanglement evaluation, while others use them for ante-hoc training.","The concept-based approaches are new, with many representations coming up, and there is very limited work on Concept-based Model improvement.","We provide a systematic review and taxonomy of various concept representations and their discovery algorithms in DNNs, specifically in vision.","We also provide details on concept-based model improvement literature, which is the first to survey concept-based model improvement methods."],"url":"http://arxiv.org/abs/2403.14566v1","category":"cs.AI"}
{"created":"2024-03-21 17:09:08","title":"A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students' Formative Assessment Responses in Science","abstract":"This paper explores the use of large language models (LLMs) to score and explain short-answer assessments in K-12 science. While existing methods can score more structured math and computer science assessments, they often do not provide explanations for the scores. Our study focuses on employing GPT-4 for automated assessment in middle school Earth Science, combining few-shot and active learning with chain-of-thought reasoning. Using a human-in-the-loop approach, we successfully score and provide meaningful explanations for formative assessment responses. A systematic analysis of our method's pros and cons sheds light on the potential for human-in-the-loop techniques to enhance automated grading for open-ended science assessments.","sentences":["This paper explores the use of large language models (LLMs) to score and explain short-answer assessments in K-12 science.","While existing methods can score more structured math and computer science assessments, they often do not provide explanations for the scores.","Our study focuses on employing GPT-4 for automated assessment in middle school Earth Science, combining few-shot and active learning with chain-of-thought reasoning.","Using a human-in-the-loop approach, we successfully score and provide meaningful explanations for formative assessment responses.","A systematic analysis of our method's pros and cons sheds light on the potential for human-in-the-loop techniques to enhance automated grading for open-ended science assessments."],"url":"http://arxiv.org/abs/2403.14565v1","category":"cs.CL"}
{"created":"2024-03-21 17:08:17","title":"On K\u00f6the's normality question for locally finite-dimensional central division algebras","abstract":"This paper considers K\\\"{o}the's question of whether every associative locally finite-dimensional (abbr., LFD) central division algebra $R$ over a field $K$ is a normally locally finite (abbr., NLF) algebra over $K$, that is, whether every nonempty finite subset $Y$ of $R$ is contained in a finite-dimensional central $K$-subalgebra $\\mathcal{R} _{Y}$ of $R$. It shows that the answer to the posed question is negative if $K$ is a purely transcendental extension of infinite transcendence degree over an algebraically closed field $k$. On the other hand, central division LFD-algebras over $K$ turn out to be NLF in the following special cases: (i) $K$ is a finitely-generated extension of a finite or a pseudo-algebraically closed field; (ii) $K$ is a higher-dimensional local field with a finite last residue field.","sentences":["This paper considers K\\\"{o}the's question of whether every associative locally finite-dimensional (abbr., LFD) central division algebra $R$ over a field $K$ is a normally locally finite (abbr., NLF) algebra over $K$, that is, whether every nonempty finite subset $Y$ of $R$ is contained in a finite-dimensional central $K$-subalgebra $\\mathcal{R} _{Y}$ of $R$. It shows that the answer to the posed question is negative if $K$ is a purely transcendental extension of infinite transcendence degree over an algebraically closed field $k$.","On the other hand, central division LFD-algebras over $K$ turn out to be NLF in the following special cases: (i) $K$ is a finitely-generated extension of a finite or a pseudo-algebraically closed field; (ii) $K$ is a higher-dimensional local field with a finite last residue field."],"url":"http://arxiv.org/abs/2403.14564v1","category":"math.RA"}
{"created":"2024-03-21 17:06:17","title":"The Era of Semantic Decoding","abstract":"Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs. We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space. Specifically, we conceptualize LLMs as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts). LLMs are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms. This concept draws a direct parallel to the well-studied problem of syntactic decoding, which involves crafting algorithms to best exploit auto-regressive language models for extracting high-utility sequences of syntactic tokens. By focusing on the semantic level and disregarding syntactic details, we gain a fresh perspective on the engineering of AI systems, enabling us to imagine systems with much greater complexity and capabilities. In this position paper, we formalize the transition from syntactic to semantic tokens as well as the analogy between syntactic and semantic decoding. Subsequently, we explore the possibilities of optimizing within the space of semantic tokens via semantic decoding algorithms. We conclude with a list of research opportunities and questions arising from this fresh perspective. The semantic decoding perspective offers a powerful abstraction for search and optimization directly in the space of meaningful concepts, with semantic tokens as the fundamental units of a new type of computation.","sentences":["Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs.","We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space.","Specifically, we conceptualize LLMs as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts).","LLMs are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors.","Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs.","We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms.","This concept draws a direct parallel to the well-studied problem of syntactic decoding, which involves crafting algorithms to best exploit auto-regressive language models for extracting high-utility sequences of syntactic tokens.","By focusing on the semantic level and disregarding syntactic details, we gain a fresh perspective on the engineering of AI systems, enabling us to imagine systems with much greater complexity and capabilities.","In this position paper, we formalize the transition from syntactic to semantic tokens as well as the analogy between syntactic and semantic decoding.","Subsequently, we explore the possibilities of optimizing within the space of semantic tokens via semantic decoding algorithms.","We conclude with a list of research opportunities and questions arising from this fresh perspective.","The semantic decoding perspective offers a powerful abstraction for search and optimization directly in the space of meaningful concepts, with semantic tokens as the fundamental units of a new type of computation."],"url":"http://arxiv.org/abs/2403.14562v1","category":"cs.CL"}
{"created":"2024-03-21 16:59:45","title":"Visibility-Aware Keypoint Localization for 6DoF Object Pose Estimation","abstract":"Localizing predefined 3D keypoints in a 2D image is an effective way to establish 3D-2D correspondences for 6DoF object pose estimation. However, unreliable localization results of invisible keypoints degrade the quality of correspondences. In this paper, we address this issue by localizing the important keypoints in terms of visibility. Since keypoint visibility information is currently missing in dataset collection process, we propose an efficient way to generate binary visibility labels from available object-level annotations, for keypoints of both asymmetric objects and symmetric objects. We further derive real-valued visibility-aware importance from binary labels based on PageRank algorithm. Taking advantage of the flexibility of our visibility-aware importance, we construct VAPO (Visibility-Aware POse estimator) by integrating the visibility-aware importance with a state-of-the-art pose estimation algorithm, along with additional positional encoding. Extensive experiments are conducted on popular pose estimation benchmarks including Linemod, Linemod-Occlusion, and YCB-V. The results show that, VAPO improves both the keypoint correspondences and final estimated poses, and clearly achieves state-of-the-art performances.","sentences":["Localizing predefined 3D keypoints in a 2D image is an effective way to establish 3D-2D correspondences for 6DoF object pose estimation.","However, unreliable localization results of invisible keypoints degrade the quality of correspondences.","In this paper, we address this issue by localizing the important keypoints in terms of visibility.","Since keypoint visibility information is currently missing in dataset collection process, we propose an efficient way to generate binary visibility labels from available object-level annotations, for keypoints of both asymmetric objects and symmetric objects.","We further derive real-valued visibility-aware importance from binary labels based on PageRank algorithm.","Taking advantage of the flexibility of our visibility-aware importance, we construct VAPO (Visibility-Aware POse estimator) by integrating the visibility-aware importance with a state-of-the-art pose estimation algorithm, along with additional positional encoding.","Extensive experiments are conducted on popular pose estimation benchmarks including Linemod, Linemod-Occlusion, and YCB-V. The results show that, VAPO improves both the keypoint correspondences and final estimated poses, and clearly achieves state-of-the-art performances."],"url":"http://arxiv.org/abs/2403.14559v1","category":"cs.CV"}
{"created":"2024-03-21 16:57:11","title":"No time to derive: unraveling total time derivatives in in-in perturbation theory","abstract":"The in-in formalism provides a way to systematically organize the calculation of primordial correlation functions. Although its theoretical foundations are now firmly settled, the treatment of total time derivative interactions, incorrectly trivialized as ``boundary terms'', has been the subject of intense discussions and conceptual mistakes. In this work, we demystify the use of total time derivatives -- as well as terms proportional to the linear equations of motion -- and show that they can lead to artificially large contributions cancelling at different orders of the in-in operator formalism. We discuss the treatment of total time derivative interactions in the Lagrangian path integral formulation of the in-in perturbation theory, and we showcase the importance of interaction terms proportional to linear equations of motion. We then provide a new route to the calculation of primordial correlation functions, which avoids the generation of total time derivatives, by working directly at the level of the full Hamiltonian in terms of phase-space variables. Instead of integrating by parts, we perform canonical transformations to simplify interactions. We explain how to retrieve correlation functions of the initial phase-space variables from the knowledge of the ones after canonical transformations, and we provide diagrammatic rules to systematically classify all possible contributions. As an important first application, we find the explicit sizes of Hamiltonian cubic interactions in single-field inflation with canonical kinetic terms and for any background evolution [...]. Our results are important for performing complete calculations of exchange diagrams in inflation [...]. Being already written in a form amenable to characterize quantum properties of primordial fluctuations, they also promise to shed light on the non-linear dynamics of quantum states during inflation.","sentences":["The in-in formalism provides a way to systematically organize the calculation of primordial correlation functions.","Although its theoretical foundations are now firmly settled, the treatment of total time derivative interactions, incorrectly trivialized as ``boundary terms'', has been the subject of intense discussions and conceptual mistakes.","In this work, we demystify the use of total time derivatives -- as well as terms proportional to the linear equations of motion -- and show that they can lead to artificially large contributions cancelling at different orders of the in-in operator formalism.","We discuss the treatment of total time derivative interactions in the Lagrangian path integral formulation of the in-in perturbation theory, and we showcase the importance of interaction terms proportional to linear equations of motion.","We then provide a new route to the calculation of primordial correlation functions, which avoids the generation of total time derivatives, by working directly at the level of the full Hamiltonian in terms of phase-space variables.","Instead of integrating by parts, we perform canonical transformations to simplify interactions.","We explain how to retrieve correlation functions of the initial phase-space variables from the knowledge of the ones after canonical transformations, and we provide diagrammatic rules to systematically classify all possible contributions.","As an important first application, we find the explicit sizes of Hamiltonian cubic interactions in single-field inflation with canonical kinetic terms and for any background evolution [...].","Our results are important for performing complete calculations of exchange diagrams in inflation [...].","Being already written in a form amenable to characterize quantum properties of primordial fluctuations, they also promise to shed light on the non-linear dynamics of quantum states during inflation."],"url":"http://arxiv.org/abs/2403.14558v1","category":"astro-ph.CO"}
{"created":"2024-03-21 16:55:29","title":"Rescue Craft Allocation in Tidal Waters of the North and Baltic Sea","abstract":"This paper aims to improve the average response time for naval accidents in the North and Baltic Sea. To do this we optimize the strategic distribution of the vessel fleet used by the Deutsche Gesellschaft zur Rettung Schiffbr\\\"uchiger (German Maritime Search and Rescue Service) (DGzRS) across several home stations. Based on these locations, in case of an incoming distress call the vessel with the lowest response time is dispatched. A particularity of the region considered is the fact that due to low tide, at predictable times some vessels and stations are not operational. In our work, we build a corresponding mathematical model for the allocation of rescue crafts to multiple stations. Thereafter, we show that the problem is NP-hard. Next, we provide an Integer Programming (IP) formulation. Finally, we propose several methods of simplifying the model and do a case study to compare their effectiveness. For this, we generate test instances based on real-world data.","sentences":["This paper aims to improve the average response time for naval accidents in the North and Baltic Sea.","To do this we optimize the strategic distribution of the vessel fleet used by the Deutsche Gesellschaft zur Rettung Schiffbr\\\"uchiger","(German Maritime Search and Rescue Service) (DGzRS) across several home stations.","Based on these locations, in case of an incoming distress call the vessel with the lowest response time is dispatched.","A particularity of the region considered is the fact that due to low tide, at predictable times some vessels and stations are not operational.","In our work, we build a corresponding mathematical model for the allocation of rescue crafts to multiple stations.","Thereafter, we show that the problem is NP-hard.","Next, we provide an Integer Programming (IP) formulation.","Finally, we propose several methods of simplifying the model and do a case study to compare their effectiveness.","For this, we generate test instances based on real-world data."],"url":"http://arxiv.org/abs/2403.14556v1","category":"math.OC"}
{"created":"2024-03-21 16:52:55","title":"Schur line defect correlators and giant graviton expansion","abstract":"We consider Schur line defect correlators in four dimensional $\\mathcal N=4$ $U(N)$ SYM and their giant graviton expansion encoding finite $N$ corrections to the large $N$ limit. We compute in closed form the single giant graviton contribution to correlators with general insertions of $\\frac{1}{2}$-BPS charged Wilson lines. For the 2-point function with fundamental and anti-fundamental Wilson lines, we match the result from fluctuations of two half-infinite strings ending on the giant graviton, recently proposed in arXiv:2403.11543. In particular, we prove exact factorization of the defect contribution with respect to wrapped D3 brane fluctuations representing the single giant graviton correction to the undecorated Schur index. This follows from a finite-difference representation of the Schur line defect index in terms of the index without defects, and similar factorization holds quite generally for more complicated defect configurations. In particular, the single giant graviton contribution to the 4-point function with two fundamental and two anti-fundamental lines is computed and discussed in this perspective.","sentences":["We consider Schur line defect correlators in four dimensional $\\mathcal N=4$ $U(N)$ SYM and their giant graviton expansion encoding finite $N$ corrections to the large $N$ limit.","We compute in closed form the single giant graviton contribution to correlators with general insertions of $\\frac{1}{2}$-BPS charged Wilson lines.","For the 2-point function with fundamental and anti-fundamental Wilson lines, we match the result from fluctuations of two half-infinite strings ending on the giant graviton, recently proposed in arXiv:2403.11543.","In particular, we prove exact factorization of the defect contribution with respect to wrapped D3 brane fluctuations representing the single giant graviton correction to the undecorated Schur index.","This follows from a finite-difference representation of the Schur line defect index in terms of the index without defects, and similar factorization holds quite generally for more complicated defect configurations.","In particular, the single giant graviton contribution to the 4-point function with two fundamental and two anti-fundamental lines is computed and discussed in this perspective."],"url":"http://arxiv.org/abs/2403.14553v1","category":"hep-th"}
{"created":"2024-03-21 16:52:01","title":"Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling","abstract":"Today's most accurate language models are trained on orders of magnitude more language data than human language learners receive - but with no supervision from other sensory modalities that play a crucial role in human learning. Can we make LMs' representations and predictions more accurate (and more human-like) with more ecologically plausible supervision? This paper describes LexiContrastive Grounding (LCG), a grounded language learning procedure that leverages visual supervision to improve textual representations. LexiContrastive Grounding combines a next token prediction strategy with a contrastive visual grounding objective, focusing on early-layer representations that encode lexical information. Across multiple word-learning and sentence-understanding benchmarks, LexiContrastive Grounding not only outperforms standard language-only models in learning efficiency, but also improves upon vision-and-language learning procedures including CLIP, GIT, Flamingo, and Vokenization. Moreover, LexiContrastive Grounding improves perplexity by around 5% on multiple language modeling tasks. This work underscores the potential of incorporating visual grounding into language models, aligning more closely with the multimodal nature of human language acquisition.","sentences":["Today's most accurate language models are trained on orders of magnitude more language data than human language learners receive - but with no supervision from other sensory modalities that play a crucial role in human learning.","Can we make LMs' representations and predictions more accurate (and more human-like) with more ecologically plausible supervision?","This paper describes LexiContrastive Grounding (LCG), a grounded language learning procedure that leverages visual supervision to improve textual representations.","LexiContrastive Grounding combines a next token prediction strategy with a contrastive visual grounding objective, focusing on early-layer representations that encode lexical information.","Across multiple word-learning and sentence-understanding benchmarks, LexiContrastive Grounding not only outperforms standard language-only models in learning efficiency, but also improves upon vision-and-language learning procedures including CLIP, GIT, Flamingo, and Vokenization.","Moreover, LexiContrastive Grounding improves perplexity by around 5% on multiple language modeling tasks.","This work underscores the potential of incorporating visual grounding into language models, aligning more closely with the multimodal nature of human language acquisition."],"url":"http://arxiv.org/abs/2403.14551v1","category":"cs.CL"}
{"created":"2024-03-21 16:50:12","title":"Dynamic Explanation Emphasis in Human-XAI Interaction with Communication Robot","abstract":"Communication robots have the potential to contribute to effective human-XAI interaction as an interface that goes beyond textual or graphical explanations. One of their strengths is that they can use physical and vocal expressions to add detailed nuances to explanations. However, it is not clear how a robot can apply such expressions, or in particular, how we can develop a strategy to adaptively use such expressions depending on the task and user in dynamic interactions. To address this question, this paper proposes DynEmph, a method for a communication robot to decide where to emphasize XAI-generated explanations with physical expressions. It predicts the effect of emphasizing certain points on a user and aims to minimize the expected difference between predicted user decisions and AI-suggested ones. DynEmph features a strategy for deciding where to emphasize in a data-driven manner, relieving engineers from the need to manually design a strategy. We further conducted experiments to investigate how emphasis selection strategies affect the performance of user decisions. The results suggest that, while a naive strategy (emphasizing explanations for an AI's most probable class) does not necessarily work better, DynEmph effectively guides users to better decisions under the condition that the performance of the AI suggestion is high.","sentences":["Communication robots have the potential to contribute to effective human-XAI interaction as an interface that goes beyond textual or graphical explanations.","One of their strengths is that they can use physical and vocal expressions to add detailed nuances to explanations.","However, it is not clear how a robot can apply such expressions, or in particular, how we can develop a strategy to adaptively use such expressions depending on the task and user in dynamic interactions.","To address this question, this paper proposes DynEmph, a method for a communication robot to decide where to emphasize XAI-generated explanations with physical expressions.","It predicts the effect of emphasizing certain points on a user and aims to minimize the expected difference between predicted user decisions and AI-suggested ones.","DynEmph features a strategy for deciding where to emphasize in a data-driven manner, relieving engineers from the need to manually design a strategy.","We further conducted experiments to investigate how emphasis selection strategies affect the performance of user decisions.","The results suggest that, while a naive strategy (emphasizing explanations for an AI's most probable class) does not necessarily work better, DynEmph effectively guides users to better decisions under the condition that the performance of the AI suggestion is high."],"url":"http://arxiv.org/abs/2403.14550v1","category":"cs.HC"}
{"created":"2024-03-21 16:45:19","title":"Superpower E-Beam Sources and Performance Estimates for Compact THz FELs","abstract":"High peak and average power free-electron lasers (FELs) in the terahertz region (THz) require small diameter, low-emittance, and high voltage electron beams. This paper presents two 1.5-2 MV, 100-200 A, thermionic cathode electron source approaches for compact megawatt range peak power, multi-kilowatt average power, high repetition rate, THz FELs. The preferred beam generation system includes grading electrodes and is quite compact compared to the other standard diode gun approaches. Both provide highly compressed beams at the waist having low values of normalized rms emittance. In particular, the new injector approach with grading electrodes, operating at a body voltage of 1.5-2 MV and 100 A, has a normalized rms emittance of roughly 10 mm-mrad at the beam waist. Power supply switching considerations are considered in the paper as well as considerations for very high-voltage multi-stage depressed collectors for device efficiency enhancement. Based on these designs, we provide performance estimates for FELs operating in the THz spectral region.","sentences":["High peak and average power free-electron lasers (FELs) in the terahertz region (THz) require small diameter, low-emittance, and high voltage electron beams.","This paper presents two 1.5-2 MV, 100-200 A, thermionic cathode electron source approaches for compact megawatt range peak power, multi-kilowatt average power, high repetition rate, THz FELs.","The preferred beam generation system includes grading electrodes and is quite compact compared to the other standard diode gun approaches.","Both provide highly compressed beams at the waist having low values of normalized rms emittance.","In particular, the new injector approach with grading electrodes, operating at a body voltage of 1.5-2 MV and 100 A, has a normalized rms emittance of roughly 10 mm-mrad at the beam waist.","Power supply switching considerations are considered in the paper as well as considerations for very high-voltage multi-stage depressed collectors for device efficiency enhancement.","Based on these designs, we provide performance estimates for FELs operating in the THz spectral region."],"url":"http://arxiv.org/abs/2403.14546v1","category":"physics.acc-ph"}
{"created":"2024-03-21 16:41:20","title":"Dynamical Edge Modes and Entanglement in Maxwell Theory","abstract":"Previous work on black hole partition functions and entanglement entropy suggests the existence of \"edge\" degrees of freedom living on the (stretched) horizon. We identify a local and \"shrinkable\" boundary condition on the stretched horizon that gives rise to such degrees of freedom. They can be interpreted as the Goldstone bosons of gauge transformations supported on the boundary, with the electric field component normal to the boundary as their symplectic conjugate. Applying the covariant phase space formalism for manifolds with boundary, we show that both the symplectic form and Hamiltonian exhibit a bulk-edge split. We then show that the thermal edge partition function is that of a codimension-two ghost compact scalar living on the horizon. In the context of a de Sitter static patch, this agrees with the edge partition functions found by Anninos et al. in arbitrary dimensions. It also yields a 4D entanglement entropy consistent with the conformal anomaly. Generalizing to Proca theory, we find that the prescription of Donnelly and Wall reproduces existing results for its edge partition function, while its classical phase space does not exhibit a bulk-edge split.","sentences":["Previous work on black hole partition functions and entanglement entropy suggests the existence of \"edge\" degrees of freedom living on the (stretched) horizon.","We identify a local and \"shrinkable\" boundary condition on the stretched horizon that gives rise to such degrees of freedom.","They can be interpreted as the Goldstone bosons of gauge transformations supported on the boundary, with the electric field component normal to the boundary as their symplectic conjugate.","Applying the covariant phase space formalism for manifolds with boundary, we show that both the symplectic form and Hamiltonian exhibit a bulk-edge split.","We then show that the thermal edge partition function is that of a codimension-two ghost compact scalar living on the horizon.","In the context of a de Sitter static patch, this agrees with the edge partition functions found by Anninos et al. in arbitrary dimensions.","It also yields a 4D entanglement entropy consistent with the conformal anomaly.","Generalizing to Proca theory, we find that the prescription of Donnelly and Wall reproduces existing results for its edge partition function, while its classical phase space does not exhibit a bulk-edge split."],"url":"http://arxiv.org/abs/2403.14542v1","category":"hep-th"}
{"created":"2024-03-21 16:41:12","title":"EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling","abstract":"Recently, Large Language Models (LLMs) have demonstrated outstanding performance across a wide range of downstream language tasks. Temperature sampling is a commonly used decoding strategy for LLMs' generation process. However, a fixed temperature parameter is used in most cases, which may not always be an optimal choice for balancing generation quality and diversity. In this paper, we propose an effective Entropy-based Dynamic Temperature (EDT) Sampling method, to achieve a more balanced performance in terms of both generation quality and diversity by dynamically selecting the temperature parameter. Additionally, we also show model performance and comprehensive analyses for 4 different generation benchmarks. Our experiments show that EDT significantly outperforms the existing strategies across different tasks.","sentences":["Recently, Large Language Models (LLMs) have demonstrated outstanding performance across a wide range of downstream language tasks.","Temperature sampling is a commonly used decoding strategy for LLMs' generation process.","However, a fixed temperature parameter is used in most cases, which may not always be an optimal choice for balancing generation quality and diversity.","In this paper, we propose an effective Entropy-based Dynamic Temperature (EDT) Sampling method, to achieve a more balanced performance in terms of both generation quality and diversity by dynamically selecting the temperature parameter.","Additionally, we also show model performance and comprehensive analyses for 4 different generation benchmarks.","Our experiments show that EDT significantly outperforms the existing strategies across different tasks."],"url":"http://arxiv.org/abs/2403.14541v1","category":"cs.CL"}
{"created":"2024-03-21 16:40:10","title":"Object-Centric Domain Randomization for 3D Shape Reconstruction in the Wild","abstract":"One of the biggest challenges in single-view 3D shape reconstruction in the wild is the scarcity of <3D shape, 2D image>-paired data from real-world environments. Inspired by remarkable achievements via domain randomization, we propose ObjectDR which synthesizes such paired data via a random simulation of visual variations in object appearances and backgrounds. Our data synthesis framework exploits a conditional generative model (e.g., ControlNet) to generate images conforming to spatial conditions such as 2.5D sketches, which are obtainable through a rendering process of 3D shapes from object collections (e.g., Objaverse-XL). To simulate diverse variations while preserving object silhouettes embedded in spatial conditions, we also introduce a disentangled framework which leverages an initial object guidance. After synthesizing a wide range of data, we pre-train a model on them so that it learns to capture a domain-invariant geometry prior which is consistent across various domains. We validate its effectiveness by substantially improving 3D shape reconstruction models on a real-world benchmark. In a scale-up evaluation, our pre-training achieves 23.6% superior results compared with the pre-training on high-quality computer graphics renderings.","sentences":["One of the biggest challenges in single-view 3D shape reconstruction in the wild is the scarcity of <3D shape, 2D image>-paired data from real-world environments.","Inspired by remarkable achievements via domain randomization, we propose ObjectDR which synthesizes such paired data via a random simulation of visual variations in object appearances and backgrounds.","Our data synthesis framework exploits a conditional generative model (e.g., ControlNet) to generate images conforming to spatial conditions such as 2.5D sketches, which are obtainable through a rendering process of 3D shapes from object collections (e.g., Objaverse-XL).","To simulate diverse variations while preserving object silhouettes embedded in spatial conditions, we also introduce a disentangled framework which leverages an initial object guidance.","After synthesizing a wide range of data, we pre-train a model on them so that it learns to capture a domain-invariant geometry prior which is consistent across various domains.","We validate its effectiveness by substantially improving 3D shape reconstruction models on a real-world benchmark.","In a scale-up evaluation, our pre-training achieves 23.6% superior results compared with the pre-training on high-quality computer graphics renderings."],"url":"http://arxiv.org/abs/2403.14539v1","category":"cs.CV"}
{"created":"2024-03-21 16:34:37","title":"Anomaly in open quantum systems and its implications on mixed-state quantum phases","abstract":"In this paper, we develop a systematic way to characterize the 't Hooft anomaly in open quantum systems. Owing to nontrivial couplings to the environment, symmetries in such systems manifest as either strong or weak type. By representing their symmetry transformation through superoperators, we incorporate them in a unified framework and calculate their anomalies. In the case where the full symmetry group is $K\\times G$, with $K$ the strong symmetry and $G$ the weak symmetry, we find that anomalies of bosonic systems are classified by $H^{d+2}(K\\times G,U(1))/H^{d+2}(G,U(1))$ in $d$ spatial dimensions. To illustrate the power of anomalies in open quantum systems, we generally prove that anomaly must lead to nontrivial mixed-state quantum phases as long as the weak symmetry is imposed.   Analogous to the ``anomaly matching\" condition ensuring nontrivial low-energy physics in closed systems, anomaly also guarantees nontrivial long-time dynamics, specifically steady states of Lindbladians, in open quantum systems. Notably, we identify a new exotic phase in $(1+1)$-D where the steady state shows no nontrivial correlation function in the bulk, but displays spontaneous symmetry breaking order on the boundary, which is enforced by anomalies. We discuss the general relations between mixed-state anomalies and such unconventional boundary correlation. Finally, we explore the generalization of the ``anomaly inflow\" mechanism in open quantum systems. We construct $(1+1)$-D and $(2+1)$-D Lindbladians whose steady states have mixed-state symmetry-protected-topological order in the bulk, with corresponding edge theories characterized by nontrivial anomalies.","sentences":["In this paper, we develop a systematic way to characterize the 't Hooft anomaly in open quantum systems.","Owing to nontrivial couplings to the environment, symmetries in such systems manifest as either strong or weak type.","By representing their symmetry transformation through superoperators, we incorporate them in a unified framework and calculate their anomalies.","In the case where the full symmetry group is $K\\times G$, with $K$ the strong symmetry and $G$ the weak symmetry, we find that anomalies of bosonic systems are classified by $H^{d+2}(K\\times G,U(1))/H^{d+2}(G,U(1))$ in $d$ spatial dimensions.","To illustrate the power of anomalies in open quantum systems, we generally prove that anomaly must lead to nontrivial mixed-state quantum phases as long as the weak symmetry is imposed.   ","Analogous to the ``anomaly matching\" condition ensuring nontrivial low-energy physics in closed systems, anomaly also guarantees nontrivial long-time dynamics, specifically steady states of Lindbladians, in open quantum systems.","Notably, we identify a new exotic phase in $(1+1)$-D where the steady state shows no nontrivial correlation function in the bulk, but displays spontaneous symmetry breaking order on the boundary, which is enforced by anomalies.","We discuss the general relations between mixed-state anomalies and such unconventional boundary correlation.","Finally, we explore the generalization of the ``anomaly inflow\" mechanism in open quantum systems.","We construct $(1+1)$-D and $(2+1)$-D Lindbladians whose steady states have mixed-state symmetry-protected-topological order in the bulk, with corresponding edge theories characterized by nontrivial anomalies."],"url":"http://arxiv.org/abs/2403.14533v1","category":"quant-ph"}
{"created":"2024-03-21 16:30:41","title":"Green's matching: an efficient approach to parameter estimation in complex dynamic systems","abstract":"Parameters of differential equations are essential to characterize intrinsic behaviors of dynamic systems. Numerous methods for estimating parameters in dynamic systems are computationally and/or statistically inadequate, especially for complex systems with general-order differential operators, such as motion dynamics. This article presents Green's matching, a computationally tractable and statistically efficient two-step method, which only needs to approximate trajectories in dynamic systems but not their derivatives due to the inverse of differential operators by Green's function. This yields a statistically optimal guarantee for parameter estimation in general-order equations, a feature not shared by existing methods, and provides an efficient framework for broad statistical inferences in complex dynamic systems.","sentences":["Parameters of differential equations are essential to characterize intrinsic behaviors of dynamic systems.","Numerous methods for estimating parameters in dynamic systems are computationally and/or statistically inadequate, especially for complex systems with general-order differential operators, such as motion dynamics.","This article presents Green's matching, a computationally tractable and statistically efficient two-step method, which only needs to approximate trajectories in dynamic systems but not their derivatives due to the inverse of differential operators by Green's function.","This yields a statistically optimal guarantee for parameter estimation in general-order equations, a feature not shared by existing methods, and provides an efficient framework for broad statistical inferences in complex dynamic systems."],"url":"http://arxiv.org/abs/2403.14531v1","category":"stat.ME"}
{"created":"2024-03-21 16:26:19","title":"Click to Grasp: Zero-Shot Precise Manipulation via Visual Diffusion Descriptors","abstract":"Precise manipulation that is generalizable across scenes and objects remains a persistent challenge in robotics. Current approaches for this task heavily depend on having a significant number of training instances to handle objects with pronounced visual and/or geometric part ambiguities. Our work explores the grounding of fine-grained part descriptors for precise manipulation in a zero-shot setting by utilizing web-trained text-to-image diffusion-based generative models. We tackle the problem by framing it as a dense semantic part correspondence task. Our model returns a gripper pose for manipulating a specific part, using as reference a user-defined click from a source image of a visually different instance of the same object. We require no manual grasping demonstrations as we leverage the intrinsic object geometry and features. Practical experiments in a real-world tabletop scenario validate the efficacy of our approach, demonstrating its potential for advancing semantic-aware robotics manipulation. Web page: https://tsagkas.github.io/click2grasp","sentences":["Precise manipulation that is generalizable across scenes and objects remains a persistent challenge in robotics.","Current approaches for this task heavily depend on having a significant number of training instances to handle objects with pronounced visual and/or geometric part ambiguities.","Our work explores the grounding of fine-grained part descriptors for precise manipulation in a zero-shot setting by utilizing web-trained text-to-image diffusion-based generative models.","We tackle the problem by framing it as a dense semantic part correspondence task.","Our model returns a gripper pose for manipulating a specific part, using as reference a user-defined click from a source image of a visually different instance of the same object.","We require no manual grasping demonstrations as we leverage the intrinsic object geometry and features.","Practical experiments in a real-world tabletop scenario validate the efficacy of our approach, demonstrating its potential for advancing semantic-aware robotics manipulation.","Web page: https://tsagkas.github.io/click2grasp"],"url":"http://arxiv.org/abs/2403.14526v1","category":"cs.RO"}
{"created":"2024-03-21 16:25:41","title":"Optimizing queues with deadlines under infrequent monitoring","abstract":"In this paper, we aim to improve the percentage of packets meeting their deadline in discrete-time M/M/1 queues with infrequent monitoring. More specifically, we look into policies that only monitor the system (and subsequently take actions) after a packet arrival. We model the system as an MDP and provide the optimal policy for some special cases. Furthermore, we introduce a heuristic algorithm called \"AB-n\" for general deadlines. Finally, we provide numerical results demonstrating the desirable performance of \"AB-n\" policies.","sentences":["In this paper, we aim to improve the percentage of packets meeting their deadline in discrete-time M/M/1 queues with infrequent monitoring.","More specifically, we look into policies that only monitor the system (and subsequently take actions) after a packet arrival.","We model the system as an MDP and provide the optimal policy for some special cases.","Furthermore, we introduce a heuristic algorithm called \"AB-n\" for general deadlines.","Finally, we provide numerical results demonstrating the desirable performance of \"AB-n\" policies."],"url":"http://arxiv.org/abs/2403.14525v1","category":"eess.SY"}
{"created":"2024-03-21 16:23:25","title":"Invisible Needle Detection in Ultrasound: Leveraging Mechanism-Induced Vibration","abstract":"In clinical applications that involve ultrasound-guided intervention, the visibility of the needle can be severely impeded due to steep insertion and strong distractors such as speckle noise and anatomical occlusion. To address this challenge, we propose VibNet, a learning-based framework tailored to enhance the robustness and accuracy of needle detection in ultrasound images, even when the target becomes invisible to the naked eye. Inspired by Eulerian Video Magnification techniques, we utilize an external step motor to induce low-amplitude periodic motion on the needle. These subtle vibrations offer the potential to generate robust frequency features for detecting the motion patterns around the needle. To robustly and precisely detect the needle leveraging these vibrations, VibNet integrates learning-based Short-Time-Fourier-Transform and Hough-Transform modules to achieve successive sub-goals, including motion feature extraction in the spatiotemporal space, frequency feature aggregation, and needle detection in the Hough space. Based on the results obtained on distinct ex vivo porcine and bovine tissue samples, the proposed algorithm exhibits superior detection performance with efficient computation and generalization capability.","sentences":["In clinical applications that involve ultrasound-guided intervention, the visibility of the needle can be severely impeded due to steep insertion and strong distractors such as speckle noise and anatomical occlusion.","To address this challenge, we propose VibNet, a learning-based framework tailored to enhance the robustness and accuracy of needle detection in ultrasound images, even when the target becomes invisible to the naked eye.","Inspired by Eulerian Video Magnification techniques, we utilize an external step motor to induce low-amplitude periodic motion on the needle.","These subtle vibrations offer the potential to generate robust frequency features for detecting the motion patterns around the needle.","To robustly and precisely detect the needle leveraging these vibrations, VibNet integrates learning-based Short-Time-Fourier-Transform and Hough-Transform modules to achieve successive sub-goals, including motion feature extraction in the spatiotemporal space, frequency feature aggregation, and needle detection in the Hough space.","Based on the results obtained on distinct ex vivo porcine and bovine tissue samples, the proposed algorithm exhibits superior detection performance with efficient computation and generalization capability."],"url":"http://arxiv.org/abs/2403.14523v1","category":"eess.IV"}
{"created":"2024-03-21 16:13:39","title":"Dynamics of systems with varying number of particles: from Liouville equations to general master equations for open systems","abstract":"A varying number of particles is the most relevant characteristic of systems of interest in nature and technology; ranging from the exchange of energy and matter with the surrounding environment to the change of particle numbers through internal dynamics such as reactions. The physico-mathematical modeling of these systems is extremely challenging, with the major difficulty being the time dependence of the number of degrees of freedom and the additional constraint that the increment or reduction of the number and species of particles must not violate basic physical laws. Theoretical models, in such a case, represent the key tool for the design of computational strategies for numerical studies that deliver trustful results. In this manuscript, we discuss complementary physico-mathematical approaches of varying numbers of particles inspired by different specific numerical goals and provide a generalized equation that embeds all of them.","sentences":["A varying number of particles is the most relevant characteristic of systems of interest in nature and technology; ranging from the exchange of energy and matter with the surrounding environment to the change of particle numbers through internal dynamics such as reactions.","The physico-mathematical modeling of these systems is extremely challenging, with the major difficulty being the time dependence of the number of degrees of freedom and the additional constraint that the increment or reduction of the number and species of particles must not violate basic physical laws.","Theoretical models, in such a case, represent the key tool for the design of computational strategies for numerical studies that deliver trustful results.","In this manuscript, we discuss complementary physico-mathematical approaches of varying numbers of particles inspired by different specific numerical goals and provide a generalized equation that embeds all of them."],"url":"http://arxiv.org/abs/2403.14517v1","category":"math-ph"}
{"created":"2024-03-21 16:11:57","title":"A Mathematical Introduction to Deep Reinforcement Learning for 5G/6G Applications","abstract":"Algorithmic innovation can unleash the potential of the beyond 5G (B5G)/6G communication systems. Artificial intelligence (AI)-driven zero-touch network slicing is envisaged as a promising cutting-edge technology to harness the full potential of heterogeneous 6G networks and enable the automation of demand-aware management and orchestration (MANO). The network slicing continues towards numerous slices with micro or macro services in 6G networks, and thereby, designing a robust, stable, and distributed learning mechanism is considered a necessity. In this regard, robust brain-inspired and dopamine-like learning methods, such as Actor-Critic approaches, can play a vital role. The tutorial begins with an introduction to network slicing, reinforcement learning (RL), and recent state-of-the-art (SoA) algorithms. Then, the paper elaborates on the combination of value-based and policy-based methods in the form of Actor-Critic techniques tailored to the needs of future wireless networks.","sentences":["Algorithmic innovation can unleash the potential of the beyond 5G (B5G)/6G communication systems.","Artificial intelligence (AI)-driven zero-touch network slicing is envisaged as a promising cutting-edge technology to harness the full potential of heterogeneous 6G networks and enable the automation of demand-aware management and orchestration (MANO).","The network slicing continues towards numerous slices with micro or macro services in 6G networks, and thereby, designing a robust, stable, and distributed learning mechanism is considered a necessity.","In this regard, robust brain-inspired and dopamine-like learning methods, such as Actor-Critic approaches, can play a vital role.","The tutorial begins with an introduction to network slicing, reinforcement learning (RL), and recent state-of-the-art (SoA) algorithms.","Then, the paper elaborates on the combination of value-based and policy-based methods in the form of Actor-Critic techniques tailored to the needs of future wireless networks."],"url":"http://arxiv.org/abs/2403.14516v1","category":"cs.NI"}
{"created":"2024-03-21 16:11:44","title":"Building a Language-Learning Game for Brazilian Indigenous Languages: A Case of Study","abstract":"In this paper we discuss a first attempt to build a language learning game for brazilian indigenous languages and the challenges around it. We present a design for the tool with gamification aspects. Then we describe a process to automatically generate language exercises and questions from a dependency treebank and a lexical database for Tupian languages. We discuss the limitations of our prototype highlighting ethical and practical implementation concerns. Finally, we conclude that new data gathering processes should be established in partnership with indigenous communities and oriented for educational purposes.","sentences":["In this paper we discuss a first attempt to build a language learning game for brazilian indigenous languages and the challenges around it.","We present a design for the tool with gamification aspects.","Then we describe a process to automatically generate language exercises and questions from a dependency treebank and a lexical database for Tupian languages.","We discuss the limitations of our prototype highlighting ethical and practical implementation concerns.","Finally, we conclude that new data gathering processes should be established in partnership with indigenous communities and oriented for educational purposes."],"url":"http://arxiv.org/abs/2403.14515v1","category":"cs.CL"}
{"created":"2024-03-21 16:07:30","title":"Universal Differential Equations as a Common Modeling Language for Neuroscience","abstract":"The unprecedented availability of large-scale datasets in neuroscience has spurred the exploration of artificial deep neural networks (DNNs) both as empirical tools and as models of natural neural systems. Their appeal lies in their ability to approximate arbitrary functions directly from observations, circumventing the need for cumbersome mechanistic modeling. However, without appropriate constraints, DNNs risk producing implausible models, diminishing their scientific value. Moreover, the interpretability of DNNs poses a significant challenge, particularly with the adoption of more complex expressive architectures. In this perspective, we argue for universal differential equations (UDEs) as a unifying approach for model development and validation in neuroscience. UDEs view differential equations as parameterizable, differentiable mathematical objects that can be augmented and trained with scalable deep learning techniques. This synergy facilitates the integration of decades of extensive literature in calculus, numerical analysis, and neural modeling with emerging advancements in AI into a potent framework. We provide a primer on this burgeoning topic in scientific machine learning and demonstrate how UDEs fill in a critical gap between mechanistic, phenomenological, and data-driven models in neuroscience. We outline a flexible recipe for modeling neural systems with UDEs and discuss how they can offer principled solutions to inherent challenges across diverse neuroscience applications such as understanding neural computation, controlling neural systems, neural decoding, and normative modeling.","sentences":["The unprecedented availability of large-scale datasets in neuroscience has spurred the exploration of artificial deep neural networks (DNNs) both as empirical tools and as models of natural neural systems.","Their appeal lies in their ability to approximate arbitrary functions directly from observations, circumventing the need for cumbersome mechanistic modeling.","However, without appropriate constraints, DNNs risk producing implausible models, diminishing their scientific value.","Moreover, the interpretability of DNNs poses a significant challenge, particularly with the adoption of more complex expressive architectures.","In this perspective, we argue for universal differential equations (UDEs) as a unifying approach for model development and validation in neuroscience.","UDEs view differential equations as parameterizable, differentiable mathematical objects that can be augmented and trained with scalable deep learning techniques.","This synergy facilitates the integration of decades of extensive literature in calculus, numerical analysis, and neural modeling with emerging advancements in AI into a potent framework.","We provide a primer on this burgeoning topic in scientific machine learning and demonstrate how UDEs fill in a critical gap between mechanistic, phenomenological, and data-driven models in neuroscience.","We outline a flexible recipe for modeling neural systems with UDEs and discuss how they can offer principled solutions to inherent challenges across diverse neuroscience applications such as understanding neural computation, controlling neural systems, neural decoding, and normative modeling."],"url":"http://arxiv.org/abs/2403.14510v1","category":"cs.CE"}
{"created":"2024-03-21 16:06:27","title":"Modeling and optimization for arrays of water turbine OWC devices","abstract":"Wave energy conversion is emerging as a promising technology for generating energy from renewable sources. Large-scale implementation of this technology requires the installation of parks of devices. We study the problem of optimizing the park layout and control for wave energy converters of the oscillating water column type. As a test case, we consider a device with a semi-submerged chamber and a Wells turbine working in the liquid phase. First, a novel model based on a nonlinear ordinary differential equation is derived to describe the behavior of the water column and used to estimate the power matrix of an isolated device. Then, its linearization is derived in order to enable the fast simulation of large parks with a high number of devices. The choice of the hydrodynamic model allows obtaining the gradient of the power with respect to the positions through an adjoint approach, making it especially convenient for optimization. We consider in particular the case of interaction with the piles of a floating wind energy plant. The results from the developed computational framework allow us to draw interesting conclusions that are useful when designing the layout of a park. In particular, we observe that interaction effects can be significant even in parks made up of devices of small size, which would exhibit negligible diffraction and radiation properties in isolated conditions, if the number of devices is large enough. Moreover, results show that wave reflection from the piles of an offshore platform can have positive effects on energy production.","sentences":["Wave energy conversion is emerging as a promising technology for generating energy from renewable sources.","Large-scale implementation of this technology requires the installation of parks of devices.","We study the problem of optimizing the park layout and control for wave energy converters of the oscillating water column type.","As a test case, we consider a device with a semi-submerged chamber and a Wells turbine working in the liquid phase.","First, a novel model based on a nonlinear ordinary differential equation is derived to describe the behavior of the water column and used to estimate the power matrix of an isolated device.","Then, its linearization is derived in order to enable the fast simulation of large parks with a high number of devices.","The choice of the hydrodynamic model allows obtaining the gradient of the power with respect to the positions through an adjoint approach, making it especially convenient for optimization.","We consider in particular the case of interaction with the piles of a floating wind energy plant.","The results from the developed computational framework allow us to draw interesting conclusions that are useful when designing the layout of a park.","In particular, we observe that interaction effects can be significant even in parks made up of devices of small size, which would exhibit negligible diffraction and radiation properties in isolated conditions, if the number of devices is large enough.","Moreover, results show that wave reflection from the piles of an offshore platform can have positive effects on energy production."],"url":"http://arxiv.org/abs/2403.14509v1","category":"math.OC"}
{"created":"2024-03-21 16:02:52","title":"Constrained Reinforcement Learning with Smoothed Log Barrier Function","abstract":"Reinforcement Learning (RL) has been widely applied to many control tasks and substantially improved the performances compared to conventional control methods in many domains where the reward function is well defined. However, for many real-world problems, it is often more convenient to formulate optimization problems in terms of rewards and constraints simultaneously. Optimizing such constrained problems via reward shaping can be difficult as it requires tedious manual tuning of reward functions with several interacting terms. Recent formulations which include constraints mostly require a pre-training phase, which often needs human expertise to collect data or assumes having a sub-optimal policy readily available. We propose a new constrained RL method called CSAC-LB (Constrained Soft Actor-Critic with Log Barrier Function), which achieves competitive performance without any pre-training by applying a linear smoothed log barrier function to an additional safety critic. It implements an adaptive penalty for policy learning and alleviates the numerical issues that are known to complicate the application of the log barrier function method. As a result, we show that with CSAC-LB, we achieve state-of-the-art performance on several constrained control tasks with different levels of difficulty and evaluate our methods in a locomotion task on a real quadruped robot platform.","sentences":["Reinforcement Learning (RL) has been widely applied to many control tasks and substantially improved the performances compared to conventional control methods in many domains where the reward function is well defined.","However, for many real-world problems, it is often more convenient to formulate optimization problems in terms of rewards and constraints simultaneously.","Optimizing such constrained problems via reward shaping can be difficult as it requires tedious manual tuning of reward functions with several interacting terms.","Recent formulations which include constraints mostly require a pre-training phase, which often needs human expertise to collect data or assumes having a sub-optimal policy readily available.","We propose a new constrained RL method called CSAC-LB (Constrained Soft Actor-Critic with Log Barrier Function), which achieves competitive performance without any pre-training by applying a linear smoothed log barrier function to an additional safety critic.","It implements an adaptive penalty for policy learning and alleviates the numerical issues that are known to complicate the application of the log barrier function method.","As a result, we show that with CSAC-LB, we achieve state-of-the-art performance on several constrained control tasks with different levels of difficulty and evaluate our methods in a locomotion task on a real quadruped robot platform."],"url":"http://arxiv.org/abs/2403.14508v1","category":"cs.LG"}
{"created":"2024-03-21 15:59:32","title":"Efficient Quantum Cooling Algorithm for Fermionic Systems","abstract":"We present a cooling algorithm for ground state preparation of fermionic Hamiltonians. Our algorithm makes use of the Hamiltonian simulation of the considered system coupled to an ancillary fridge, which is regularly reset to its known ground state. We derive suitable interaction Hamiltonians that originate from ladder operators of the free theory and initiate resonant gaps between system and fridge. We further propose a spectroscopic scan to find the relevant eigenenergies of the system using energy measurements on the fridge. With these insights, we design a ground state cooling algorithm for fermionic systems that is efficient, i.e. its runtime is polynomial in the system size, as long as the initial state is prepared in a low energy sector of polynomial size. We achieve the latter via a fast, quasi-adiabatic sweep from a parameter regime whose ground state can be easily prepared. We generalize the algorithm to prepare thermal states and demonstrate our findings on the Fermi-Hubbard model.","sentences":["We present a cooling algorithm for ground state preparation of fermionic Hamiltonians.","Our algorithm makes use of the Hamiltonian simulation of the considered system coupled to an ancillary fridge, which is regularly reset to its known ground state.","We derive suitable interaction Hamiltonians that originate from ladder operators of the free theory and initiate resonant gaps between system and fridge.","We further propose a spectroscopic scan to find the relevant eigenenergies of the system using energy measurements on the fridge.","With these insights, we design a ground state cooling algorithm for fermionic systems that is efficient, i.e. its runtime is polynomial in the system size, as long as the initial state is prepared in a low energy sector of polynomial size.","We achieve the latter via a fast, quasi-adiabatic sweep from a parameter regime whose ground state can be easily prepared.","We generalize the algorithm to prepare thermal states and demonstrate our findings on the Fermi-Hubbard model."],"url":"http://arxiv.org/abs/2403.14506v1","category":"quant-ph"}
{"created":"2024-03-21 15:56:15","title":"Soft Learning Probabilistic Circuits","abstract":"Probabilistic Circuits (PCs) are prominent tractable probabilistic models, allowing for a range of exact inferences. This paper focuses on the main algorithm for training PCs, LearnSPN, a gold standard due to its efficiency, performance, and ease of use, in particular for tabular data. We show that LearnSPN is a greedy likelihood maximizer under mild assumptions. While inferences in PCs may use the entire circuit structure for processing queries, LearnSPN applies a hard method for learning them, propagating at each sum node a data point through one and only one of the children/edges as in a hard clustering process. We propose a new learning procedure named SoftLearn, that induces a PC using a soft clustering process. We investigate the effect of this learning-inference compatibility in PCs. Our experiments show that SoftLearn outperforms LearnSPN in many situations, yielding better likelihoods and arguably better samples. We also analyze comparable tractable models to highlight the differences between soft/hard learning and model querying.","sentences":["Probabilistic Circuits (PCs) are prominent tractable probabilistic models, allowing for a range of exact inferences.","This paper focuses on the main algorithm for training PCs, LearnSPN, a gold standard due to its efficiency, performance, and ease of use, in particular for tabular data.","We show that LearnSPN is a greedy likelihood maximizer under mild assumptions.","While inferences in PCs may use the entire circuit structure for processing queries, LearnSPN applies a hard method for learning them, propagating at each sum node a data point through one and only one of the children/edges as in a hard clustering process.","We propose a new learning procedure named SoftLearn, that induces a PC using a soft clustering process.","We investigate the effect of this learning-inference compatibility in PCs.","Our experiments show that SoftLearn outperforms LearnSPN in many situations, yielding better likelihoods and arguably better samples.","We also analyze comparable tractable models to highlight the differences between soft/hard learning and model querying."],"url":"http://arxiv.org/abs/2403.14504v1","category":"cs.LG"}
{"created":"2024-03-21 15:44:56","title":"How Human-Centered Explainable AI Interface Are Designed and Evaluated: A Systematic Survey","abstract":"Despite its technological breakthroughs, eXplainable Artificial Intelligence (XAI) research has limited success in producing the {\\em effective explanations} needed by users. In order to improve XAI systems' usability, practical interpretability, and efficacy for real users, the emerging area of {\\em Explainable Interfaces} (EIs) focuses on the user interface and user experience design aspects of XAI. This paper presents a systematic survey of 53 publications to identify current trends in human-XAI interaction and promising directions for EI design and development. This is among the first systematic survey of EI research.","sentences":["Despite its technological breakthroughs, eXplainable Artificial Intelligence (XAI) research has limited success in producing the {\\em effective explanations} needed by users.","In order to improve XAI systems' usability, practical interpretability, and efficacy for real users, the emerging area of {\\em Explainable Interfaces} (EIs) focuses on the user interface and user experience design aspects of XAI.","This paper presents a systematic survey of 53 publications to identify current trends in human-XAI interaction and promising directions for EI design and development.","This is among the first systematic survey of EI research."],"url":"http://arxiv.org/abs/2403.14496v1","category":"cs.HC"}
{"created":"2024-03-21 15:44:03","title":"Optimization for MIMO Integrated Sensing and Communications","abstract":"The fundamentals of MIMO communications and MIMO sensing are firstly analyzed with regard to channel and sensing capacities. It is shown that the different objectives of communications and sensing lead to different signaling waveforms required for achieving their capacities. Hence, the optimization of integrated sensing and communications (ISAC) is relied on a trade-off expected between the performance of communications and that of sensing. Following this observation, the design and resource optimization in general MIMO ISAC systems are discussed along with the analysis of some existing ISAC schemes. Furthermore, the design of ISAC in mmWave communications is addressed. Specifically, the principle of sensing in mmWave systems is established, and a range of optimization alternatives for ISAC design in mmWave systems are reviewed.","sentences":["The fundamentals of MIMO communications and MIMO sensing are firstly analyzed with regard to channel and sensing capacities.","It is shown that the different objectives of communications and sensing lead to different signaling waveforms required for achieving their capacities.","Hence, the optimization of integrated sensing and communications (ISAC) is relied on a trade-off expected between the performance of communications and that of sensing.","Following this observation, the design and resource optimization in general MIMO ISAC systems are discussed along with the analysis of some existing ISAC schemes.","Furthermore, the design of ISAC in mmWave communications is addressed.","Specifically, the principle of sensing in mmWave systems is established, and a range of optimization alternatives for ISAC design in mmWave systems are reviewed."],"url":"http://arxiv.org/abs/2403.14495v1","category":"eess.SP"}
{"created":"2024-03-21 15:42:17","title":"Learning to Project for Cross-Task Knowledge Distillation","abstract":"Traditional knowledge distillation (KD) relies on a proficient teacher trained on the target task, which is not always available. In this setting, cross-task distillation can be used, enabling the use of any teacher model trained on a different task. However, many KD methods prove ineffective when applied to this cross-task setting. To address this limitation, we propose a simple modification: the use of an inverted projection. We show that this drop-in replacement for a standard projector is effective by learning to disregard any task-specific features which might degrade the student's performance. We find that this simple modification is sufficient for extending many KD methods to the cross-task setting, where the teacher and student tasks can be very different. In doing so, we obtain up to a 1.9% improvement in the cross-task setting compared to the traditional projection, at no additional cost. Our method can obtain significant performance improvements (up to 7%) when using even a randomly-initialised teacher on various tasks such as depth estimation, image translation, and semantic segmentation, despite the lack of any learned knowledge to transfer. To provide conceptual and analytical insights into this result, we show that using an inverted projection allows the distillation loss to be decomposed into a knowledge transfer and a spectral regularisation component. Through this analysis we are additionally able to propose a novel regularisation loss that allows teacher-free distillation, enabling performance improvements of up to 8.57% on ImageNet with no additional training costs.","sentences":["Traditional knowledge distillation (KD) relies on a proficient teacher trained on the target task, which is not always available.","In this setting, cross-task distillation can be used, enabling the use of any teacher model trained on a different task.","However, many KD methods prove ineffective when applied to this cross-task setting.","To address this limitation, we propose a simple modification: the use of an inverted projection.","We show that this drop-in replacement for a standard projector is effective by learning to disregard any task-specific features which might degrade the student's performance.","We find that this simple modification is sufficient for extending many KD methods to the cross-task setting, where the teacher and student tasks can be very different.","In doing so, we obtain up to a 1.9% improvement in the cross-task setting compared to the traditional projection, at no additional cost.","Our method can obtain significant performance improvements (up to 7%) when using even a randomly-initialised teacher on various tasks such as depth estimation, image translation, and semantic segmentation, despite the lack of any learned knowledge to transfer.","To provide conceptual and analytical insights into this result, we show that using an inverted projection allows the distillation loss to be decomposed into a knowledge transfer and a spectral regularisation component.","Through this analysis we are additionally able to propose a novel regularisation loss that allows teacher-free distillation, enabling performance improvements of up to 8.57% on ImageNet with no additional training costs."],"url":"http://arxiv.org/abs/2403.14494v1","category":"cs.CV"}
{"created":"2024-03-21 15:36:26","title":"Physics-Based Causal Reasoning for Safe & Robust Next-Best Action Selection in Robot Manipulation Tasks","abstract":"Safe and efficient object manipulation is a key enabler of many real-world robot applications. However, this is challenging because robot operation must be robust to a range of sensor and actuator uncertainties. In this paper, we present a physics-informed causal-inference-based framework for a robot to probabilistically reason about candidate actions in a block stacking task in a partially observable setting. We integrate a physics-based simulation of the rigid-body system dynamics with a causal Bayesian network (CBN) formulation to define a causal generative probabilistic model of the robot decision-making process. Using simulation-based Monte Carlo experiments, we demonstrate our framework's ability to successfully: (1) predict block tower stability with high accuracy (Pred Acc: 88.6%); and, (2) select an approximate next-best action for the block stacking task, for execution by an integrated robot system, achieving 94.2% task success rate. We also demonstrate our framework's suitability for real-world robot systems by demonstrating successful task executions with a domestic support robot, with perception and manipulation sub-system integration. Hence, we show that by embedding physics-based causal reasoning into robots' decision-making processes, we can make robot task execution safer, more reliable, and more robust to various types of uncertainty.","sentences":["Safe and efficient object manipulation is a key enabler of many real-world robot applications.","However, this is challenging because robot operation must be robust to a range of sensor and actuator uncertainties.","In this paper, we present a physics-informed causal-inference-based framework for a robot to probabilistically reason about candidate actions in a block stacking task in a partially observable setting.","We integrate a physics-based simulation of the rigid-body system dynamics with a causal Bayesian network (CBN) formulation to define a causal generative probabilistic model of the robot decision-making process.","Using simulation-based Monte Carlo experiments, we demonstrate our framework's ability to successfully: (1) predict block tower stability with high accuracy (Pred Acc: 88.6%); and, (2) select an approximate next-best action for the block stacking task, for execution by an integrated robot system, achieving 94.2% task success rate.","We also demonstrate our framework's suitability for real-world robot systems by demonstrating successful task executions with a domestic support robot, with perception and manipulation sub-system integration.","Hence, we show that by embedding physics-based causal reasoning into robots' decision-making processes, we can make robot task execution safer, more reliable, and more robust to various types of uncertainty."],"url":"http://arxiv.org/abs/2403.14488v1","category":"cs.RO"}
{"created":"2024-03-21 15:35:42","title":"DesignEdit: Multi-Layered Latent Decomposition and Fusion for Unified & Accurate Image Editing","abstract":"Recently, how to achieve precise image editing has attracted increasing attention, especially given the remarkable success of text-to-image generation models. To unify various spatial-aware image editing abilities into one framework, we adopt the concept of layers from the design domain to manipulate objects flexibly with various operations. The key insight is to transform the spatial-aware image editing task into a combination of two sub-tasks: multi-layered latent decomposition and multi-layered latent fusion. First, we segment the latent representations of the source images into multiple layers, which include several object layers and one incomplete background layer that necessitates reliable inpainting. To avoid extra tuning, we further explore the inner inpainting ability within the self-attention mechanism. We introduce a key-masking self-attention scheme that can propagate the surrounding context information into the masked region while mitigating its impact on the regions outside the mask. Second, we propose an instruction-guided latent fusion that pastes the multi-layered latent representations onto a canvas latent. We also introduce an artifact suppression scheme in the latent space to enhance the inpainting quality. Due to the inherent modular advantages of such multi-layered representations, we can achieve accurate image editing, and we demonstrate that our approach consistently surpasses the latest spatial editing methods, including Self-Guidance and DiffEditor. Last, we show that our approach is a unified framework that supports various accurate image editing tasks on more than six different editing tasks.","sentences":["Recently, how to achieve precise image editing has attracted increasing attention, especially given the remarkable success of text-to-image generation models.","To unify various spatial-aware image editing abilities into one framework, we adopt the concept of layers from the design domain to manipulate objects flexibly with various operations.","The key insight is to transform the spatial-aware image editing task into a combination of two sub-tasks: multi-layered latent decomposition and multi-layered latent fusion.","First, we segment the latent representations of the source images into multiple layers, which include several object layers and one incomplete background layer that necessitates reliable inpainting.","To avoid extra tuning, we further explore the inner inpainting ability within the self-attention mechanism.","We introduce a key-masking self-attention scheme that can propagate the surrounding context information into the masked region while mitigating its impact on the regions outside the mask.","Second, we propose an instruction-guided latent fusion that pastes the multi-layered latent representations onto a canvas latent.","We also introduce an artifact suppression scheme in the latent space to enhance the inpainting quality.","Due to the inherent modular advantages of such multi-layered representations, we can achieve accurate image editing, and we demonstrate that our approach consistently surpasses the latest spatial editing methods, including Self-Guidance and DiffEditor.","Last, we show that our approach is a unified framework that supports various accurate image editing tasks on more than six different editing tasks."],"url":"http://arxiv.org/abs/2403.14487v1","category":"cs.CV"}
{"created":"2024-03-21 15:31:28","title":"HyperGALE: ASD Classification via Hypergraph Gated Attention with Learnable Hyperedges","abstract":"Autism Spectrum Disorder (ASD) is a neurodevelopmental condition characterized by varied social cognitive challenges and repetitive behavioral patterns. Identifying reliable brain imaging-based biomarkers for ASD has been a persistent challenge due to the spectrum's diverse symptomatology. Existing baselines in the field have made significant strides in this direction, yet there remains room for improvement in both performance and interpretability. We propose \\emph{HyperGALE}, which builds upon the hypergraph by incorporating learned hyperedges and gated attention mechanisms. This approach has led to substantial improvements in the model's ability to interpret complex brain graph data, offering deeper insights into ASD biomarker characterization. Evaluated on the extensive ABIDE II dataset, \\emph{HyperGALE} not only improves interpretability but also demonstrates statistically significant enhancements in key performance metrics compared to both previous baselines and the foundational hypergraph model. The advancement \\emph{HyperGALE} brings to ASD research highlights the potential of sophisticated graph-based techniques in neurodevelopmental studies. The source code and implementation instructions are available at GitHub:https://github.com/mehular0ra/HyperGALE.","sentences":["Autism Spectrum Disorder (ASD) is a neurodevelopmental condition characterized by varied social cognitive challenges and repetitive behavioral patterns.","Identifying reliable brain imaging-based biomarkers for ASD has been a persistent challenge due to the spectrum's diverse symptomatology.","Existing baselines in the field have made significant strides in this direction, yet there remains room for improvement in both performance and interpretability.","We propose \\emph{HyperGALE}, which builds upon the hypergraph by incorporating learned hyperedges and gated attention mechanisms.","This approach has led to substantial improvements in the model's ability to interpret complex brain graph data, offering deeper insights into ASD biomarker characterization.","Evaluated on the extensive ABIDE II dataset, \\emph{HyperGALE} not only improves interpretability but also demonstrates statistically significant enhancements in key performance metrics compared to both previous baselines and the foundational hypergraph model.","The advancement \\emph{HyperGALE} brings to ASD research highlights the potential of sophisticated graph-based techniques in neurodevelopmental studies.","The source code and implementation instructions are available at GitHub:https://github.com/mehular0ra/HyperGALE."],"url":"http://arxiv.org/abs/2403.14484v1","category":"cs.LG"}
{"created":"2024-03-21 15:29:24","title":"Utilizing the LightGBM Algorithm for Operator User Credit Assessment Research","abstract":"Mobile Internet user credit assessment is an important way for communication operators to establish decisions and formulate measures, and it is also a guarantee for operators to obtain expected benefits. However, credit evaluation methods have long been monopolized by financial industries such as banks and credit. As supporters and providers of platform network technology and network resources, communication operators are also builders and maintainers of communication networks. Internet data improves the user's credit evaluation strategy. This paper uses the massive data provided by communication operators to carry out research on the operator's user credit evaluation model based on the fusion LightGBM algorithm. First, for the massive data related to user evaluation provided by operators, key features are extracted by data preprocessing and feature engineering methods, and a multi-dimensional feature set with statistical significance is constructed; then, linear regression, decision tree, LightGBM, and other machine learning algorithms build multiple basic models to find the best basic model; finally, integrates Averaging, Voting, Blending, Stacking and other integrated algorithms to refine multiple fusion models, and finally establish the most suitable fusion model for operator user evaluation.","sentences":["Mobile Internet user credit assessment is an important way for communication operators to establish decisions and formulate measures, and it is also a guarantee for operators to obtain expected benefits.","However, credit evaluation methods have long been monopolized by financial industries such as banks and credit.","As supporters and providers of platform network technology and network resources, communication operators are also builders and maintainers of communication networks.","Internet data improves the user's credit evaluation strategy.","This paper uses the massive data provided by communication operators to carry out research on the operator's user credit evaluation model based on the fusion LightGBM algorithm.","First, for the massive data related to user evaluation provided by operators, key features are extracted by data preprocessing and feature engineering methods, and a multi-dimensional feature set with statistical significance is constructed; then, linear regression, decision tree, LightGBM, and other machine learning algorithms build multiple basic models to find the best basic model; finally, integrates Averaging, Voting, Blending, Stacking and other integrated algorithms to refine multiple fusion models, and finally establish the most suitable fusion model for operator user evaluation."],"url":"http://arxiv.org/abs/2403.14483v1","category":"cs.LG"}
{"created":"2024-03-21 15:29:08","title":"Assessing exchange-correlation functionals for heterogeneous catalysis of nitrogen species","abstract":"Increasing interest in sustainable synthesis of ammonia, nitrates, and urea has led to an increase in studies of catalytic conversion between nitrogen-containing compounds using heterogeneous catalysts. Density functional theory (DFT) is commonly employed to obtain molecular-scale insight into these reactions, but there have been relatively few assessments of the exchange-correlation functionals that are best suited for heterogeneous catalysis of nitrogen compounds. Here, we assess a range of functionals ranging from the generalized gradient approximation (GGA) to the random phase approximation (RPA) for the formation energies of gas-phase nitrogen species, the lattice constants of representative solids from several common classes of catalysts (metals, oxides, and metal-organic frameworks (MOFs)), and the adsorption energies of a range of nitrogen-containing intermediates on these materials. The results reveal that the choice of exchange-correlation functional and van der Waals correction can have a surprisingly large effect and that increasing the level of theory does not always improve the accuracy for nitrogen-containing compounds. This suggests that the selection of functionals should be carefully evaluated on the basis of the specific reaction and material being studied.","sentences":["Increasing interest in sustainable synthesis of ammonia, nitrates, and urea has led to an increase in studies of catalytic conversion between nitrogen-containing compounds using heterogeneous catalysts.","Density functional theory (DFT) is commonly employed to obtain molecular-scale insight into these reactions, but there have been relatively few assessments of the exchange-correlation functionals that are best suited for heterogeneous catalysis of nitrogen compounds.","Here, we assess a range of functionals ranging from the generalized gradient approximation (GGA) to the random phase approximation (RPA) for the formation energies of gas-phase nitrogen species, the lattice constants of representative solids from several common classes of catalysts (metals, oxides, and metal-organic frameworks (MOFs)), and the adsorption energies of a range of nitrogen-containing intermediates on these materials.","The results reveal that the choice of exchange-correlation functional and van der Waals correction can have a surprisingly large effect and that increasing the level of theory does not always improve the accuracy for nitrogen-containing compounds.","This suggests that the selection of functionals should be carefully evaluated on the basis of the specific reaction and material being studied."],"url":"http://arxiv.org/abs/2403.14482v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-21 15:22:54","title":"Linearized Boltzmann collision operator for a mixture of monatomic and polyatomic chemically reacting species","abstract":"At higher altitudes, for high temperature gases, for instance near space shuttles moving at hypersonic speed, not only mechanical collisions are affecting the gas flow, but also chemical reactions have an impact on such hypersonic flows. In this work we insert chemical reactions, in form of dissociations and recombinations (associations), in an existing model for a mixture of mono- and polyatomic (non-reacting) species. More general chemical reactions, e.g., bimolecular ones, can be obtained by instant combinations of associations and dissociations. The polyatomicity is modelled by a continuous internal energy variable and the evolution of the gas is described by a Boltzmann equation.   In, e.g., the Chapman-Enskog process - and related half-space problems - the linearized Boltzmann collision operator plays a central role. Here we extend some important properties of the linearized Boltzmann collision operator to the introduced model with chemical reactions. A compactness result, stating that the linearized operator can be decomposed into a sum of a positive multiplication operator - the collision frequency - and a compact integral operator, is proven under reasonable assumptions on the collision kernel. The strategy is to show that the terms of the integral operator are (at least) uniform limits of Hilbert-Schmidt integral operators and therefore also compact operators. Self-adjointness of the linearized operator is a direct consequences. Moreover, bounds on - including coercivity of - the collision frequency are obtained for hard sphere like, as well as hard potentials with cutoff like models. As consequence, Fredholmness, as well as, the domain, of the linearized collision operator are obtained","sentences":["At higher altitudes, for high temperature gases, for instance near space shuttles moving at hypersonic speed, not only mechanical collisions are affecting the gas flow, but also chemical reactions have an impact on such hypersonic flows.","In this work we insert chemical reactions, in form of dissociations and recombinations (associations), in an existing model for a mixture of mono- and polyatomic (non-reacting) species.","More general chemical reactions, e.g., bimolecular ones, can be obtained by instant combinations of associations and dissociations.","The polyatomicity is modelled by a continuous internal energy variable and the evolution of the gas is described by a Boltzmann equation.   ","In, e.g., the Chapman-Enskog process - and related half-space problems - the linearized Boltzmann collision operator plays a central role.","Here we extend some important properties of the linearized Boltzmann collision operator to the introduced model with chemical reactions.","A compactness result, stating that the linearized operator can be decomposed into a sum of a positive multiplication operator - the collision frequency - and a compact integral operator, is proven under reasonable assumptions on the collision kernel.","The strategy is to show that the terms of the integral operator are (at least) uniform limits of Hilbert-Schmidt integral operators and therefore also compact operators.","Self-adjointness of the linearized operator is a direct consequences.","Moreover, bounds on - including coercivity of - the collision frequency are obtained for hard sphere like, as well as hard potentials with cutoff like models.","As consequence, Fredholmness, as well as, the domain, of the linearized collision operator are obtained"],"url":"http://arxiv.org/abs/2403.14477v1","category":"math.AP"}
{"created":"2024-03-21 15:21:47","title":"Loss-induced quantum nonreciprocity","abstract":"Attribute to their robustness against loss and external noise, nonreciprocal photonic devices hold great promise for applications in quantum information processing. Recent advancements have demonstrated that nonreciprocal optical transmission in linear systems can be achieved through the strategic introduction of loss. However, a crucial question remains unanswered: can loss be harnessed as a resource for generating nonreciprocal quantum correlations? Here, we take a counterintuitive stance by engineering loss to generate a novel form of nonreciprocal quantum correlations, termed nonreciprocal photon blockade. We examine a dissipative three-cavity system comprising two nonlinear cavities and a linear cavity. The interplay of loss and nonlinearity leads to a robust nonreciprocal single- and two-photon blockade, facilitated by destructive quantum interference. Furthermore, we demonstrate the tunability of this nonreciprocal photon blockade by manipulating the relative phase between the two nonlinear cavities. Remarkably, this allows for the reversal of the direction of nonreciprocity. Our study not only sheds new light on the concept of loss-engineered quantum nonreciprocity but also opens up a unique pathway for the design of quantum nonreciprocal photonic devices.","sentences":["Attribute to their robustness against loss and external noise, nonreciprocal photonic devices hold great promise for applications in quantum information processing.","Recent advancements have demonstrated that nonreciprocal optical transmission in linear systems can be achieved through the strategic introduction of loss.","However, a crucial question remains unanswered: can loss be harnessed as a resource for generating nonreciprocal quantum correlations?","Here, we take a counterintuitive stance by engineering loss to generate a novel form of nonreciprocal quantum correlations, termed nonreciprocal photon blockade.","We examine a dissipative three-cavity system comprising two nonlinear cavities and a linear cavity.","The interplay of loss and nonlinearity leads to a robust nonreciprocal single- and two-photon blockade, facilitated by destructive quantum interference.","Furthermore, we demonstrate the tunability of this nonreciprocal photon blockade by manipulating the relative phase between the two nonlinear cavities.","Remarkably, this allows for the reversal of the direction of nonreciprocity.","Our study not only sheds new light on the concept of loss-engineered quantum nonreciprocity but also opens up a unique pathway for the design of quantum nonreciprocal photonic devices."],"url":"http://arxiv.org/abs/2403.14476v1","category":"quant-ph"}
{"created":"2024-03-21 15:18:30","title":"Detoxifying Large Language Models via Knowledge Editing","abstract":"This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. We conduct experiments to compare knowledge editing approaches with previous baselines, indicating that knowledge editing has the potential to efficiently detoxify LLMs with limited impact on general performance. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxify approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to a certain extent, making permanent adjustments. We hope that these insights could shed light on future work of developing detoxifying approaches and the underlying knowledge mechanisms of LLMs. Code and benchmark are available at https://github.com/zjunlp/EasyEdit.","sentences":["This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs).","We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation.","We conduct experiments to compare knowledge editing approaches with previous baselines, indicating that knowledge editing has the potential to efficiently detoxify LLMs with limited impact on general performance.","Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance.","We further provide an in-depth analysis of the internal mechanism for various detoxify approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to a certain extent, making permanent adjustments.","We hope that these insights could shed light on future work of developing detoxifying approaches and the underlying knowledge mechanisms of LLMs.","Code and benchmark are available at https://github.com/zjunlp/EasyEdit."],"url":"http://arxiv.org/abs/2403.14472v1","category":"cs.CL"}
{"created":"2024-03-21 15:17:05","title":"CBX: Python and Julia packages for consensus-based interacting particle methods","abstract":"We introduce CBXPy and ConsensusBasedX.jl, Python and Julia implementations of consensus-based interacting particle systems (CBX), which generalise consensus-based optimization methods (CBO) for global, derivative-free optimisation. The raison d'\\^etre of our libraries is twofold: on the one hand, to offer high-performance implementations of CBX methods that the community can use directly, while on the other, providing a general interface that can accommodate and be extended to further variations of the CBX family. Python and Julia were selected as the leading high-level languages in terms of usage and performance, as well as their popularity among the scientific computing community. Both libraries have been developed with a common ethos, ensuring a similar API and core functionality, while leveraging the strengths of each language and writing idiomatic code.","sentences":["We introduce CBXPy and ConsensusBasedX.jl, Python and Julia implementations of consensus-based interacting particle systems (CBX), which generalise consensus-based optimization methods (CBO) for global, derivative-free optimisation.","The raison d'\\^etre of our libraries is twofold: on the one hand, to offer high-performance implementations of CBX methods that the community can use directly, while on the other, providing a general interface that can accommodate and be extended to further variations of the CBX family.","Python and Julia were selected as the leading high-level languages in terms of usage and performance, as well as their popularity among the scientific computing community.","Both libraries have been developed with a common ethos, ensuring a similar API and core functionality, while leveraging the strengths of each language and writing idiomatic code."],"url":"http://arxiv.org/abs/2403.14470v1","category":"math.OC"}
{"created":"2024-03-21 15:16:50","title":"ChatGPT Alternative Solutions: Large Language Models Survey","abstract":"In recent times, the grandeur of Large Language Models (LLMs) has not only shone in the realm of natural language processing but has also cast its brilliance across a vast array of applications. This remarkable display of LLM capabilities has ignited a surge in research contributions within this domain, spanning a diverse spectrum of topics. These contributions encompass advancements in neural network architecture, context length enhancements, model alignment, training datasets, benchmarking, efficiency improvements, and more. Recent years have witnessed a dynamic synergy between academia and industry, propelling the field of LLM research to new heights. A notable milestone in this journey is the introduction of ChatGPT, a powerful AI chatbot grounded in LLMs, which has garnered widespread societal attention. The evolving technology of LLMs has begun to reshape the landscape of the entire AI community, promising a revolutionary shift in the way we create and employ AI algorithms. Given this swift-paced technical evolution, our survey embarks on a journey to encapsulate the recent strides made in the world of LLMs. Through an exploration of the background, key discoveries, and prevailing methodologies, we offer an up-to-the-minute review of the literature. By examining multiple LLM models, our paper not only presents a comprehensive overview but also charts a course that identifies existing challenges and points toward potential future research trajectories. This survey furnishes a well-rounded perspective on the current state of generative AI, shedding light on opportunities for further exploration, enhancement, and innovation.","sentences":["In recent times, the grandeur of Large Language Models (LLMs) has not only shone in the realm of natural language processing but has also cast its brilliance across a vast array of applications.","This remarkable display of LLM capabilities has ignited a surge in research contributions within this domain, spanning a diverse spectrum of topics.","These contributions encompass advancements in neural network architecture, context length enhancements, model alignment, training datasets, benchmarking, efficiency improvements, and more.","Recent years have witnessed a dynamic synergy between academia and industry, propelling the field of LLM research to new heights.","A notable milestone in this journey is the introduction of ChatGPT, a powerful AI chatbot grounded in LLMs, which has garnered widespread societal attention.","The evolving technology of LLMs has begun to reshape the landscape of the entire AI community, promising a revolutionary shift in the way we create and employ AI algorithms.","Given this swift-paced technical evolution, our survey embarks on a journey to encapsulate the recent strides made in the world of LLMs.","Through an exploration of the background, key discoveries, and prevailing methodologies, we offer an up-to-the-minute review of the literature.","By examining multiple LLM models, our paper not only presents a comprehensive overview but also charts a course that identifies existing challenges and points toward potential future research trajectories.","This survey furnishes a well-rounded perspective on the current state of generative AI, shedding light on opportunities for further exploration, enhancement, and innovation."],"url":"http://arxiv.org/abs/2403.14469v1","category":"cs.CL"}
{"created":"2024-03-21 15:15:00","title":"AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks","abstract":"Video-to-video editing involves editing a source video along with additional control (such as text prompts, subjects, or styles) to generate a new video that aligns with the source video and the provided control. Traditional methods have been constrained to certain editing types, limiting their ability to meet the wide range of user demands. In this paper, we introduce AnyV2V, a novel training-free framework designed to simplify video editing into two primary steps: (1) employing an off-the-shelf image editing model (e.g. InstructPix2Pix, InstantID, etc) to modify the first frame, (2) utilizing an existing image-to-video generation model (e.g. I2VGen-XL) for DDIM inversion and feature injection. In the first stage, AnyV2V can plug in any existing image editing tools to support an extensive array of video editing tasks. Beyond the traditional prompt-based editing methods, AnyV2V also can support novel video editing tasks, including reference-based style transfer, subject-driven editing, and identity manipulation, which were unattainable by previous methods. In the second stage, AnyV2V can plug in any existing image-to-video models to perform DDIM inversion and intermediate feature injection to maintain the appearance and motion consistency with the source video. On the prompt-based editing, we show that AnyV2V can outperform the previous best approach by 35\\% on prompt alignment, and 25\\% on human preference. On the three novel tasks, we show that AnyV2V also achieves a high success rate. We believe AnyV2V will continue to thrive due to its ability to seamlessly integrate the fast-evolving image editing methods. Such compatibility can help AnyV2V to increase its versatility to cater to diverse user demands.","sentences":["Video-to-video editing involves editing a source video along with additional control (such as text prompts, subjects, or styles) to generate a new video that aligns with the source video and the provided control.","Traditional methods have been constrained to certain editing types, limiting their ability to meet the wide range of user demands.","In this paper, we introduce AnyV2V, a novel training-free framework designed to simplify video editing into two primary steps: (1) employing an off-the-shelf image editing model (e.g. InstructPix2Pix, InstantID, etc) to modify the first frame, (2) utilizing an existing image-to-video generation model (e.g. I2VGen-XL) for DDIM inversion and feature injection.","In the first stage, AnyV2V can plug in any existing image editing tools to support an extensive array of video editing tasks.","Beyond the traditional prompt-based editing methods, AnyV2V also can support novel video editing tasks, including reference-based style transfer, subject-driven editing, and identity manipulation, which were unattainable by previous methods.","In the second stage, AnyV2V can plug in any existing image-to-video models to perform DDIM inversion and intermediate feature injection to maintain the appearance and motion consistency with the source video.","On the prompt-based editing, we show that AnyV2V can outperform the previous best approach by 35\\% on prompt alignment, and 25\\% on human preference.","On the three novel tasks, we show that AnyV2V also achieves a high success rate.","We believe AnyV2V will continue to thrive due to its ability to seamlessly integrate the fast-evolving image editing methods.","Such compatibility can help AnyV2V to increase its versatility to cater to diverse user demands."],"url":"http://arxiv.org/abs/2403.14468v1","category":"cs.CV"}
{"created":"2024-03-21 15:14:25","title":"Recourse for reclamation: Chatting with generative language models","abstract":"Researchers and developers increasingly rely on toxicity scoring to moderate generative language model outputs, in settings such as customer service, information retrieval, and content generation. However, toxicity scoring may render pertinent information inaccessible, rigidify or \"value-lock\" cultural norms, and prevent language reclamation processes, particularly for marginalized people. In this work, we extend the concept of algorithmic recourse to generative language models: we provide users a novel mechanism to achieve their desired prediction by dynamically setting thresholds for toxicity filtering. Users thereby exercise increased agency relative to interactions with the baseline system. A pilot study ($n = 30$) supports the potential of our proposed recourse mechanism, indicating improvements in usability compared to fixed-threshold toxicity-filtering of model outputs. Future work should explore the intersection of toxicity scoring, model controllability, user agency, and language reclamation processes -- particularly with regard to the bias that many communities encounter when interacting with generative language models.","sentences":["Researchers and developers increasingly rely on toxicity scoring to moderate generative language model outputs, in settings such as customer service, information retrieval, and content generation.","However, toxicity scoring may render pertinent information inaccessible, rigidify or \"value-lock\" cultural norms, and prevent language reclamation processes, particularly for marginalized people.","In this work, we extend the concept of algorithmic recourse to generative language models: we provide users a novel mechanism to achieve their desired prediction by dynamically setting thresholds for toxicity filtering.","Users thereby exercise increased agency relative to interactions with the baseline system.","A pilot study ($n = 30$) supports the potential of our proposed recourse mechanism, indicating improvements in usability compared to fixed-threshold toxicity-filtering of model outputs.","Future work should explore the intersection of toxicity scoring, model controllability, user agency, and language reclamation processes -- particularly with regard to the bias that many communities encounter when interacting with generative language models."],"url":"http://arxiv.org/abs/2403.14467v1","category":"cs.HC"}
{"created":"2024-03-21 15:13:54","title":"Universal Feature Selection for Simultaneous Interpretability of Multitask Datasets","abstract":"Extracting meaningful features from complex, high-dimensional datasets across scientific domains remains challenging. Current methods often struggle with scalability, limiting their applicability to large datasets, or make restrictive assumptions about feature-property relationships, hindering their ability to capture complex interactions. BoUTS's general and scalable feature selection algorithm surpasses these limitations to identify both universal features relevant to all datasets and task-specific features predictive for specific subsets. Evaluated on seven diverse chemical regression datasets, BoUTS achieves state-of-the-art feature sparsity while maintaining prediction accuracy comparable to specialized methods. Notably, BoUTS's universal features enable domain-specific knowledge transfer between datasets, and suggest deep connections in seemingly-disparate chemical datasets. We expect these results to have important repercussions in manually-guided inverse problems. Beyond its current application, BoUTS holds immense potential for elucidating data-poor systems by leveraging information from similar data-rich systems. BoUTS represents a significant leap in cross-domain feature selection, potentially leading to advancements in various scientific fields.","sentences":["Extracting meaningful features from complex, high-dimensional datasets across scientific domains remains challenging.","Current methods often struggle with scalability, limiting their applicability to large datasets, or make restrictive assumptions about feature-property relationships, hindering their ability to capture complex interactions.","BoUTS's general and scalable feature selection algorithm surpasses these limitations to identify both universal features relevant to all datasets and task-specific features predictive for specific subsets.","Evaluated on seven diverse chemical regression datasets, BoUTS achieves state-of-the-art feature sparsity while maintaining prediction accuracy comparable to specialized methods.","Notably, BoUTS's universal features enable domain-specific knowledge transfer between datasets, and suggest deep connections in seemingly-disparate chemical datasets.","We expect these results to have important repercussions in manually-guided inverse problems.","Beyond its current application, BoUTS holds immense potential for elucidating data-poor systems by leveraging information from similar data-rich systems.","BoUTS represents a significant leap in cross-domain feature selection, potentially leading to advancements in various scientific fields."],"url":"http://arxiv.org/abs/2403.14466v1","category":"cs.LG"}
{"created":"2024-03-21 15:13:36","title":"CathFlow: Self-Supervised Segmentation of Catheters in Interventional Ultrasound Using Optical Flow and Transformers","abstract":"In minimally invasive endovascular procedures, contrast-enhanced angiography remains the most robust imaging technique. However, it is at the expense of the patient and clinician's health due to prolonged radiation exposure. As an alternative, interventional ultrasound has notable benefits such as being radiation-free, fast to deploy, and having a small footprint in the operating room. Yet, ultrasound is hard to interpret, and highly prone to artifacts and noise. Additionally, interventional radiologists must undergo extensive training before they become qualified to diagnose and treat patients effectively, leading to a shortage of staff, and a lack of open-source datasets. In this work, we seek to address both problems by introducing a self-supervised deep learning architecture to segment catheters in longitudinal ultrasound images, without demanding any labeled data. The network architecture builds upon AiAReSeg, a segmentation transformer built with the Attention in Attention mechanism, and is capable of learning feature changes across time and space. To facilitate training, we used synthetic ultrasound data based on physics-driven catheter insertion simulations, and translated the data into a unique CT-Ultrasound common domain, CACTUSS, to improve the segmentation performance. We generated ground truth segmentation masks by computing the optical flow between adjacent frames using FlowNet2, and performed thresholding to obtain a binary map estimate. Finally, we validated our model on a test dataset, consisting of unseen synthetic data and images collected from silicon aorta phantoms, thus demonstrating its potential for applications to clinical data in the future.","sentences":["In minimally invasive endovascular procedures, contrast-enhanced angiography remains the most robust imaging technique.","However, it is at the expense of the patient and clinician's health due to prolonged radiation exposure.","As an alternative, interventional ultrasound has notable benefits such as being radiation-free, fast to deploy, and having a small footprint in the operating room.","Yet, ultrasound is hard to interpret, and highly prone to artifacts and noise.","Additionally, interventional radiologists must undergo extensive training before they become qualified to diagnose and treat patients effectively, leading to a shortage of staff, and a lack of open-source datasets.","In this work, we seek to address both problems by introducing a self-supervised deep learning architecture to segment catheters in longitudinal ultrasound images, without demanding any labeled data.","The network architecture builds upon AiAReSeg, a segmentation transformer built with the Attention in Attention mechanism, and is capable of learning feature changes across time and space.","To facilitate training, we used synthetic ultrasound data based on physics-driven catheter insertion simulations, and translated the data into a unique CT-Ultrasound common domain, CACTUSS, to improve the segmentation performance.","We generated ground truth segmentation masks by computing the optical flow between adjacent frames using FlowNet2, and performed thresholding to obtain a binary map estimate.","Finally, we validated our model on a test dataset, consisting of unseen synthetic data and images collected from silicon aorta phantoms, thus demonstrating its potential for applications to clinical data in the future."],"url":"http://arxiv.org/abs/2403.14465v1","category":"eess.IV"}
{"created":"2024-03-21 15:07:57","title":"Towards Single-System Illusion in Software-Defined Vehicles -- Automated, AI-Powered Workflow","abstract":"We propose a novel model- and feature-based approach to development of vehicle software systems, where the end architecture is not explicitly defined. Instead, it emerges from an iterative process of search and optimization given certain constraints, requirements and hardware architecture, while retaining the property of single-system illusion, where applications run in a logically uniform environment. One of the key points of the presented approach is the inclusion of modern generative AI, specifically Large Language Models (LLMs), in the loop. With the recent advances in the field, we expect that the LLMs will be able to assist in processing of requirements, generation of formal system models, as well as generation of software deployment specification and test code. The resulting pipeline is automated to a large extent, with feedback being generated at each step.","sentences":["We propose a novel model- and feature-based approach to development of vehicle software systems, where the end architecture is not explicitly defined.","Instead, it emerges from an iterative process of search and optimization given certain constraints, requirements and hardware architecture, while retaining the property of single-system illusion, where applications run in a logically uniform environment.","One of the key points of the presented approach is the inclusion of modern generative AI, specifically Large Language Models (LLMs), in the loop.","With the recent advances in the field, we expect that the LLMs will be able to assist in processing of requirements, generation of formal system models, as well as generation of software deployment specification and test code.","The resulting pipeline is automated to a large extent, with feedback being generated at each step."],"url":"http://arxiv.org/abs/2403.14460v1","category":"cs.SE"}
{"created":"2024-03-21 15:06:14","title":"Multi-Level Explanations for Generative Language Models","abstract":"Perturbation-based explanation methods such as LIME and SHAP are commonly applied to text classification. This work focuses on their extension to generative language models. To address the challenges of text as output and long text inputs, we propose a general framework called MExGen that can be instantiated with different attribution algorithms. To handle text output, we introduce the notion of scalarizers for mapping text to real numbers and investigate multiple possibilities. To handle long inputs, we take a multi-level approach, proceeding from coarser levels of granularity to finer ones, and focus on algorithms with linear scaling in model queries. We conduct a systematic evaluation, both automated and human, of perturbation-based attribution methods for summarization and context-grounded question answering. The results show that our framework can provide more locally faithful explanations of generated outputs.","sentences":["Perturbation-based explanation methods such as LIME and SHAP are commonly applied to text classification.","This work focuses on their extension to generative language models.","To address the challenges of text as output and long text inputs, we propose a general framework called MExGen that can be instantiated with different attribution algorithms.","To handle text output, we introduce the notion of scalarizers for mapping text to real numbers and investigate multiple possibilities.","To handle long inputs, we take a multi-level approach, proceeding from coarser levels of granularity to finer ones, and focus on algorithms with linear scaling in model queries.","We conduct a systematic evaluation, both automated and human, of perturbation-based attribution methods for summarization and context-grounded question answering.","The results show that our framework can provide more locally faithful explanations of generated outputs."],"url":"http://arxiv.org/abs/2403.14459v1","category":"cs.CL"}
{"created":"2024-03-21 15:04:49","title":"Self-distributive structures in physics","abstract":"It is an important feature of our existing physical theories that observables generate one-parameter groups of transformations. In classical Hamiltonian mechanics and quantum mechanics, this is due to the fact that the observables form a Lie algebra, and it manifests itself in Noether's theorem. In this paper, we introduce \\emph{Lie quandles} as the minimal mathematical structure needed to express the idea that observables generate transformations. This is based on the notion of a quandle used primarily in knot theory, whose main defining property is the self-distributivity equation $x \\triangleright (y \\triangleright z) = (x \\triangleright y) \\triangleright (x \\triangleright z)$. We argue that Lie quandles can be thought of as nonlinear generalizations of Lie algebras. We also observe that taking convex combinations of points in vector spaces, which physically corresponds to mixing states, satisfies the same form of self-distributivity.","sentences":["It is an important feature of our existing physical theories that observables generate one-parameter groups of transformations.","In classical Hamiltonian mechanics and quantum mechanics, this is due to the fact that the observables form a Lie algebra, and it manifests itself in Noether's theorem.","In this paper, we introduce \\emph{Lie quandles} as the minimal mathematical structure needed to express the idea that observables generate transformations.","This is based on the notion of a quandle used primarily in knot theory, whose main defining property is the self-distributivity equation $x \\triangleright (y \\triangleright z) = (x \\triangleright y) \\triangleright (x \\triangleright z)$.","We argue that Lie quandles can be thought of as nonlinear generalizations of Lie algebras.","We also observe that taking convex combinations of points in vector spaces, which physically corresponds to mixing states, satisfies the same form of self-distributivity."],"url":"http://arxiv.org/abs/2403.14458v1","category":"quant-ph"}
{"created":"2024-03-21 15:04:32","title":"gTBLS: Generating Tables from Text by Conditional Question Answering","abstract":"Distilling large, unstructured text into a structured, condensed form such as tables is an open research problem. One of the primary challenges in automatically generating tables is ensuring their syntactic validity. Prior approaches address this challenge by including additional parameters in the Transformer's attention mechanism to attend to specific rows and column headers. In contrast to this single-stage method, this paper presents a two-stage approach called Generative Tables (gTBLS). The first stage infers table structure (row and column headers) from the text. The second stage formulates questions using these headers and fine-tunes a causal language model to answer them. Furthermore, the gTBLS approach is amenable to the utilization of pre-trained Large Language Models in a zero-shot configuration, presenting a solution for table generation in situations where fine-tuning is not feasible. gTBLS improves prior approaches by up to 10% in BERTScore on the table construction task and up to 20% on the table content generation task of the E2E, WikiTableText, WikiBio, and RotoWire datasets.","sentences":["Distilling large, unstructured text into a structured, condensed form such as tables is an open research problem.","One of the primary challenges in automatically generating tables is ensuring their syntactic validity.","Prior approaches address this challenge by including additional parameters in the Transformer's attention mechanism to attend to specific rows and column headers.","In contrast to this single-stage method, this paper presents a two-stage approach called Generative Tables (gTBLS).","The first stage infers table structure (row and column headers) from the text.","The second stage formulates questions using these headers and fine-tunes a causal language model to answer them.","Furthermore, the gTBLS approach is amenable to the utilization of pre-trained Large Language Models in a zero-shot configuration, presenting a solution for table generation in situations where fine-tuning is not feasible.","gTBLS improves prior approaches by up to 10% in BERTScore on the table construction task and up to 20% on the table content generation task of the E2E, WikiTableText, WikiBio, and RotoWire datasets."],"url":"http://arxiv.org/abs/2403.14457v1","category":"cs.CL"}
{"created":"2024-03-21 15:02:03","title":"Prediction of Translation Techniques for the Translation Process","abstract":"Machine translation (MT) encompasses a variety of methodologies aimed at enhancing the accuracy of translations. In contrast, the process of human-generated translation relies on a wide range of translation techniques, which are crucial for ensuring linguistic adequacy and fluency. This study suggests that these translation techniques could further optimize machine translation if they are automatically identified before being applied to guide the translation process effectively. The study differentiates between two scenarios of the translation process: from-scratch translation and post-editing. For each scenario, a specific set of experiments has been designed to forecast the most appropriate translation techniques. The findings indicate that the predictive accuracy for from-scratch translation reaches 82%, while the post-editing process exhibits even greater potential, achieving an accuracy rate of 93%.","sentences":["Machine translation (MT) encompasses a variety of methodologies aimed at enhancing the accuracy of translations.","In contrast, the process of human-generated translation relies on a wide range of translation techniques, which are crucial for ensuring linguistic adequacy and fluency.","This study suggests that these translation techniques could further optimize machine translation if they are automatically identified before being applied to guide the translation process effectively.","The study differentiates between two scenarios of the translation process: from-scratch translation and post-editing.","For each scenario, a specific set of experiments has been designed to forecast the most appropriate translation techniques.","The findings indicate that the predictive accuracy for from-scratch translation reaches 82%, while the post-editing process exhibits even greater potential, achieving an accuracy rate of 93%."],"url":"http://arxiv.org/abs/2403.14454v1","category":"cs.CL"}
{"created":"2024-03-21 15:00:34","title":"On Weighted Trigonometric Regression for Suboptimal Designs in Circadian Biology Studies","abstract":"Studies in circadian biology often use trigonometric regression to model phenomena over time. Ideally, protocols in these studies would collect samples at evenly distributed and equally spaced time points over a 24 hour period. This sample collection protocol is known as an equispaced design, which is considered the optimal experimental design for trigonometric regression under multiple statistical criteria. However, implementing equispaced designs in studies involving individuals is logistically challenging, and failure to employ an equispaced design could cause a loss of statistical power when performing hypothesis tests with an estimated model. This paper is motivated by the potential loss of statistical power during hypothesis testing, and considers a weighted trigonometric regression as a remedy. Specifically, the weights for this regression are the normalized reciprocals of estimates derived from a kernel density estimator for sample collection time, which inflates the weight of samples collected at underrepresented time points. A search procedure is also introduced to identify the concentration hyperparameter for kernel density estimation that maximizes the Hessian of weighted squared loss, which relates to both maximizing the $D$-optimality criterion from experimental design literature and minimizing the generalized variance. Simulation studies consistently demonstrate that this weighted regression mitigates variability in inferences produced by an estimated model. Illustrations with three real circadian biology data sets further indicate that this weighted regression consistently yields larger test statistics than its unweighted counterpart for first-order trigonometric regression, or cosinor regression, which is prevalent in circadian biology studies.","sentences":["Studies in circadian biology often use trigonometric regression to model phenomena over time.","Ideally, protocols in these studies would collect samples at evenly distributed and equally spaced time points over a 24 hour period.","This sample collection protocol is known as an equispaced design, which is considered the optimal experimental design for trigonometric regression under multiple statistical criteria.","However, implementing equispaced designs in studies involving individuals is logistically challenging, and failure to employ an equispaced design could cause a loss of statistical power when performing hypothesis tests with an estimated model.","This paper is motivated by the potential loss of statistical power during hypothesis testing, and considers a weighted trigonometric regression as a remedy.","Specifically, the weights for this regression are the normalized reciprocals of estimates derived from a kernel density estimator for sample collection time, which inflates the weight of samples collected at underrepresented time points.","A search procedure is also introduced to identify the concentration hyperparameter for kernel density estimation that maximizes the Hessian of weighted squared loss, which relates to both maximizing the $D$-optimality criterion from experimental design literature and minimizing the generalized variance.","Simulation studies consistently demonstrate that this weighted regression mitigates variability in inferences produced by an estimated model.","Illustrations with three real circadian biology data sets further indicate that this weighted regression consistently yields larger test statistics than its unweighted counterpart for first-order trigonometric regression, or cosinor regression, which is prevalent in circadian biology studies."],"url":"http://arxiv.org/abs/2403.14452v1","category":"stat.ME"}
{"created":"2024-03-21 14:58:07","title":"Maximal $\u03b1$-Leakage for Quantum Privacy Mechanisms","abstract":"In this work, maximal $\\alpha$-leakage is introduced to quantify how much a quantum adversary can learn about any sensitive information of data upon observing its disturbed version via a quantum privacy mechanism. We first show that an adversary's maximal expected $\\alpha$-gain using optimal measurement is characterized by measured conditional R\\'enyi entropy. This can be viewed as a parametric generalization of K\\\"onig et al.'s famous guessing probability formula [IEEE Trans. Inf. Theory, 55(9), 2009]. Then, we prove that the $\\alpha$-leakage and maximal $\\alpha$-leakage for a quantum privacy mechanism are determined by measured Arimoto information and measured R\\'enyi capacity, respectively. Various properties of maximal $\\alpha$-leakage, such as data processing inequality and composition property are established as well. Moreover, we show that regularized $\\alpha$-leakage and regularized maximal $\\alpha$-leakage for identical and independent quantum privacy mechanisms coincide with $\\alpha$-tilted sandwiched R\\'enyi information and sandwiched R\\'enyi capacity, respectively.","sentences":["In this work, maximal $\\alpha$-leakage is introduced to quantify how much a quantum adversary can learn about any sensitive information of data upon observing its disturbed version via a quantum privacy mechanism.","We first show that an adversary's maximal expected $\\alpha$-gain using optimal measurement is characterized by measured conditional R\\'enyi entropy.","This can be viewed as a parametric generalization of K\\\"onig et al.'s famous guessing probability formula [IEEE Trans.","Inf.","Theory, 55(9), 2009].","Then, we prove that the $\\alpha$-leakage and maximal $\\alpha$-leakage for a quantum privacy mechanism are determined by measured Arimoto information and measured R\\'enyi capacity, respectively.","Various properties of maximal $\\alpha$-leakage, such as data processing inequality and composition property are established as well.","Moreover, we show that regularized $\\alpha$-leakage and regularized maximal $\\alpha$-leakage for identical and independent quantum privacy mechanisms coincide with $\\alpha$-tilted sandwiched R\\'enyi information and sandwiched R\\'enyi capacity, respectively."],"url":"http://arxiv.org/abs/2403.14450v1","category":"quant-ph"}
{"created":"2024-03-21 14:56:46","title":"Bringing Robots Home: The Rise of AI Robots in Consumer Electronics","abstract":"On March 18, 2024, NVIDIA unveiled Project GR00T, a general-purpose multimodal generative AI model designed specifically for training humanoid robots. Preceding this event, Tesla's unveiling of the Optimus Gen 2 humanoid robot on December 12, 2023, underscored the profound impact robotics is poised to have on reshaping various facets of our daily lives. While robots have long dominated industrial settings, their presence within our homes is a burgeoning phenomenon. This can be attributed, in part, to the complexities of domestic environments and the challenges of creating robots that can seamlessly integrate into our daily routines.","sentences":["On March 18, 2024, NVIDIA unveiled Project GR00T, a general-purpose multimodal generative AI model designed specifically for training humanoid robots.","Preceding this event, Tesla's unveiling of the Optimus Gen 2 humanoid robot on December 12, 2023, underscored the profound impact robotics is poised to have on reshaping various facets of our daily lives.","While robots have long dominated industrial settings, their presence within our homes is a burgeoning phenomenon.","This can be attributed, in part, to the complexities of domestic environments and the challenges of creating robots that can seamlessly integrate into our daily routines."],"url":"http://arxiv.org/abs/2403.14449v1","category":"cs.RO"}
{"created":"2024-03-21 14:53:51","title":"Billiard tables with analytic Birkhoff normal form are generically Gevrey divergent","abstract":"The problem of the existence of an analytic normal form near an equilibrium point of an area-preserving map and analyticity of the associated coordinate change is a classical problem in dynamical systems going back to Poincar\\'e and Siegel. One important class of examples of area-preserving maps consists of the collision maps for planar billiards. Recently, Treschev discovered a formal $\\mathbb{Z}_2 \\times \\mathbb{Z}_2$ symmetric billiard with locally linearizable dynamics and conjectured its convergence. Since then, a Gevrey regularity for such a billiard was proven by Wang and Zhang, but the original problem about analyticity still remains open.   We extend the class of billiards by relaxing the symmetry condition and allowing conjugacies to non-linear analytic integrable normal forms. To keep the formal solution unique, odd table derivatives and the normal form are treated as parameters of the problem. We show that for the new problem, the series of the billiard table diverge for general parameters by proving the optimality of Gevrey bounds. The general parameter set is prevalent (in a certain sense has full measure) and it contains an open set.   Instead of considering the problem in a functional sense and iterating approximation procedures, we employ formal power series methods and one-by-one directly reconstruct all the Taylor coefficients of the table. In order to prove that on an open set Taylor series diverges we define a Taylor recurrence operator and prove that it has a cone property. All solutions in that cone are only Gevrey regular and not analytic.","sentences":["The problem of the existence of an analytic normal form near an equilibrium point of an area-preserving map and analyticity of the associated coordinate change is a classical problem in dynamical systems going back to Poincar\\'e and Siegel.","One important class of examples of area-preserving maps consists of the collision maps for planar billiards.","Recently, Treschev discovered a formal $\\mathbb{Z}_2 \\times \\mathbb{Z}_2$ symmetric billiard with locally linearizable dynamics and conjectured its convergence.","Since then, a Gevrey regularity for such a billiard was proven by Wang and Zhang, but the original problem about analyticity still remains open.   ","We extend the class of billiards by relaxing the symmetry condition and allowing conjugacies to non-linear analytic integrable normal forms.","To keep the formal solution unique, odd table derivatives and the normal form are treated as parameters of the problem.","We show that for the new problem, the series of the billiard table diverge for general parameters by proving the optimality of Gevrey bounds.","The general parameter set is prevalent (in a certain sense has full measure) and it contains an open set.   ","Instead of considering the problem in a functional sense and iterating approximation procedures, we employ formal power series methods and one-by-one directly reconstruct all the Taylor coefficients of the table.","In order to prove that on an open set Taylor series diverges we define a Taylor recurrence operator and prove that it has a cone property.","All solutions in that cone are only Gevrey regular and not analytic."],"url":"http://arxiv.org/abs/2403.14448v1","category":"math.DS"}
{"created":"2024-03-21 14:52:49","title":"BRST construction for infinite spin field on $AdS_4$","abstract":"We generalize the first class constraints that describe the infinite spin irreducible $4D$ Poincar\\'{e} group representation in flat space to new first class constraints in $AdS_4$ space. The constraints are realized as operators acting in Fock space spanned by the creation and annihilation operators with two-component spinor indices. As a result, we obtain a new closed gauge algebra on $AdS_4$ with the known flat space limit. Using this gauge algebra, we construct the BRST charge and derive the Lagrangian and gauge transformations for free bosonic infinite spin field theory in $AdS_4$ space.","sentences":["We generalize the first class constraints that describe the infinite spin irreducible $4D$ Poincar\\'{e} group representation in flat space to new first class constraints in $AdS_4$ space.","The constraints are realized as operators acting in Fock space spanned by the creation and annihilation operators with two-component spinor indices.","As a result, we obtain a new closed gauge algebra on $AdS_4$ with the known flat space limit.","Using this gauge algebra, we construct the BRST charge and derive the Lagrangian and gauge transformations for free bosonic infinite spin field theory in $AdS_4$ space."],"url":"http://arxiv.org/abs/2403.14446v1","category":"hep-th"}
{"created":"2024-03-21 14:48:37","title":"Language Models Can Reduce Asymmetry in Information Markets","abstract":"This work addresses the buyer's inspection paradox for information markets. The paradox is that buyers need to access information to determine its value, while sellers need to limit access to prevent theft. To study this, we introduce an open-source simulated digital marketplace where intelligent agents, powered by language models, buy and sell information on behalf of external participants. The central mechanism enabling this marketplace is the agents' dual capabilities: they not only have the capacity to assess the quality of privileged information but also come equipped with the ability to forget. This ability to induce amnesia allows vendors to grant temporary access to proprietary information, significantly reducing the risk of unauthorized retention while enabling agents to accurately gauge the information's relevance to specific queries or tasks. To perform well, agents must make rational decisions, strategically explore the marketplace through generated sub-queries, and synthesize answers from purchased information. Concretely, our experiments (a) uncover biases in language models leading to irrational behavior and evaluate techniques to mitigate these biases, (b) investigate how price affects demand in the context of informational goods, and (c) show that inspection and higher budgets both lead to higher quality outcomes.","sentences":["This work addresses the buyer's inspection paradox for information markets.","The paradox is that buyers need to access information to determine its value, while sellers need to limit access to prevent theft.","To study this, we introduce an open-source simulated digital marketplace where intelligent agents, powered by language models, buy and sell information on behalf of external participants.","The central mechanism enabling this marketplace is the agents' dual capabilities: they not only have the capacity to assess the quality of privileged information but also come equipped with the ability to forget.","This ability to induce amnesia allows vendors to grant temporary access to proprietary information, significantly reducing the risk of unauthorized retention while enabling agents to accurately gauge the information's relevance to specific queries or tasks.","To perform well, agents must make rational decisions, strategically explore the marketplace through generated sub-queries, and synthesize answers from purchased information.","Concretely, our experiments (a) uncover biases in language models leading to irrational behavior and evaluate techniques to mitigate these biases, (b) investigate how price affects demand in the context of informational goods, and (c) show that inspection and higher budgets both lead to higher quality outcomes."],"url":"http://arxiv.org/abs/2403.14443v1","category":"cs.AI"}
{"created":"2024-03-21 14:45:54","title":"Analysing Diffusion Segmentation for Medical Images","abstract":"Denoising Diffusion Probabilistic models have become increasingly popular due to their ability to offer probabilistic modeling and generate diverse outputs. This versatility inspired their adaptation for image segmentation, where multiple predictions of the model can produce segmentation results that not only achieve high quality but also capture the uncertainty inherent in the model. Here, powerful architectures were proposed for improving diffusion segmentation performance. However, there is a notable lack of analysis and discussions on the differences between diffusion segmentation and image generation, and thorough evaluations are missing that distinguish the improvements these architectures provide for segmentation in general from their benefit for diffusion segmentation specifically. In this work, we critically analyse and discuss how diffusion segmentation for medical images differs from diffusion image generation, with a particular focus on the training behavior. Furthermore, we conduct an assessment how proposed diffusion segmentation architectures perform when trained directly for segmentation. Lastly, we explore how different medical segmentation tasks influence the diffusion segmentation behavior and the diffusion process could be adapted accordingly. With these analyses, we aim to provide in-depth insights into the behavior of diffusion segmentation that allow for a better design and evaluation of diffusion segmentation methods in the future.","sentences":["Denoising Diffusion Probabilistic models have become increasingly popular due to their ability to offer probabilistic modeling and generate diverse outputs.","This versatility inspired their adaptation for image segmentation, where multiple predictions of the model can produce segmentation results that not only achieve high quality but also capture the uncertainty inherent in the model.","Here, powerful architectures were proposed for improving diffusion segmentation performance.","However, there is a notable lack of analysis and discussions on the differences between diffusion segmentation and image generation, and thorough evaluations are missing that distinguish the improvements these architectures provide for segmentation in general from their benefit for diffusion segmentation specifically.","In this work, we critically analyse and discuss how diffusion segmentation for medical images differs from diffusion image generation, with a particular focus on the training behavior.","Furthermore, we conduct an assessment how proposed diffusion segmentation architectures perform when trained directly for segmentation.","Lastly, we explore how different medical segmentation tasks influence the diffusion segmentation behavior and the diffusion process could be adapted accordingly.","With these analyses, we aim to provide in-depth insights into the behavior of diffusion segmentation that allow for a better design and evaluation of diffusion segmentation methods in the future."],"url":"http://arxiv.org/abs/2403.14440v1","category":"eess.IV"}
{"created":"2024-03-21 14:45:41","title":"Raw Instinct: Trust Your Classifiers and Skip the Conversion","abstract":"Using RAW-images in computer vision problems is surprisingly underexplored considering that converting from RAW to RGB does not introduce any new capture information. In this paper, we show that a sufficiently advanced classifier can yield equivalent results on RAW input compared to RGB and present a new public dataset consisting of RAW images and the corresponding converted RGB images. Classifying images directly from RAW is attractive, as it allows for skipping the conversion to RGB, lowering computation time significantly. Two CNN classifiers are used to classify the images in both formats, confirming that classification performance can indeed be preserved. We furthermore show that the total computation time from RAW image data to classification results for RAW images can be up to 8.46 times faster than RGB. These results contribute to the evidence found in related works, that using RAW images as direct input to computer vision algorithms looks very promising.","sentences":["Using RAW-images in computer vision problems is surprisingly underexplored considering that converting from RAW to RGB does not introduce any new capture information.","In this paper, we show that a sufficiently advanced classifier can yield equivalent results on RAW input compared to RGB and present a new public dataset consisting of RAW images and the corresponding converted RGB images.","Classifying images directly from RAW is attractive, as it allows for skipping the conversion to RGB, lowering computation time significantly.","Two CNN classifiers are used to classify the images in both formats, confirming that classification performance can indeed be preserved.","We furthermore show that the total computation time from RAW image data to classification results for RAW images can be up to 8.46 times faster than RGB.","These results contribute to the evidence found in related works, that using RAW images as direct input to computer vision algorithms looks very promising."],"url":"http://arxiv.org/abs/2403.14439v1","category":"cs.CV"}
{"created":"2024-03-21 14:42:37","title":"Spectral Methods for Quantum Optimal Control: Artificial Boundary Conditions","abstract":"The problem of quantum state preparation is one of the main challenges in achieving the quantum advantage. Furthermore, classically, for multi-level problems, our ability to solve the corresponding quantum optimal control problems is rather limited. The ability of the latter to feed into the former may result in significant progress in quantum computing. To address this challenge, we propose a formulation of quantum optimal control that makes use of artificial boundary conditions for the Schr\\\"odinger equation in combination with spectral methods. The resulting formulations are well suited for investigating periodic potentials and lend themselves to direct numerical treatment using conventional methods for bounded domains.","sentences":["The problem of quantum state preparation is one of the main challenges in achieving the quantum advantage.","Furthermore, classically, for multi-level problems, our ability to solve the corresponding quantum optimal control problems is rather limited.","The ability of the latter to feed into the former may result in significant progress in quantum computing.","To address this challenge, we propose a formulation of quantum optimal control that makes use of artificial boundary conditions for the Schr\\\"odinger equation in combination with spectral methods.","The resulting formulations are well suited for investigating periodic potentials and lend themselves to direct numerical treatment using conventional methods for bounded domains."],"url":"http://arxiv.org/abs/2403.14436v1","category":"quant-ph"}
{"created":"2024-03-21 14:41:58","title":"Biased Binary Attribute Classifiers Ignore the Majority Classes","abstract":"To visualize the regions of interest that classifiers base their decisions on, different Class Activation Mapping (CAM) methods have been developed. However, all of these techniques target categorical classifiers only, though most real-world tasks are binary classification. In this paper, we extend gradient-based CAM techniques to work with binary classifiers and visualize the active regions for binary facial attribute classifiers. When training an unbalanced binary classifier on an imbalanced dataset, it is well-known that the majority class, i.e. the class with many training samples, is mostly predicted much better than minority class with few training instances. In our experiments on the CelebA dataset, we verify these results, when training an unbalanced classifier to extract 40 facial attributes simultaneously. One would expect that the biased classifier has learned to extract features mainly for the majority classes and that the proportional energy of the activations mainly reside in certain specific regions of the image where the attribute is located. However, we find very little regular activation for samples of majority classes, while the active regions for minority classes seem mostly reasonable and overlap with our expectations. These results suggest that biased classifiers mainly rely on bias activation for majority classes. When training a balanced classifier on the imbalanced data by employing attribute-specific class weights, majority and minority classes are classified similarly well and show expected activations for almost all attributes","sentences":["To visualize the regions of interest that classifiers base their decisions on, different Class Activation Mapping (CAM) methods have been developed.","However, all of these techniques target categorical classifiers only, though most real-world tasks are binary classification.","In this paper, we extend gradient-based CAM techniques to work with binary classifiers and visualize the active regions for binary facial attribute classifiers.","When training an unbalanced binary classifier on an imbalanced dataset, it is well-known that the majority class, i.e. the class with many training samples, is mostly predicted much better than minority class with few training instances.","In our experiments on the CelebA dataset, we verify these results, when training an unbalanced classifier to extract 40 facial attributes simultaneously.","One would expect that the biased classifier has learned to extract features mainly for the majority classes and that the proportional energy of the activations mainly reside in certain specific regions of the image where the attribute is located.","However, we find very little regular activation for samples of majority classes, while the active regions for minority classes seem mostly reasonable and overlap with our expectations.","These results suggest that biased classifiers mainly rely on bias activation for majority classes.","When training a balanced classifier on the imbalanced data by employing attribute-specific class weights, majority and minority classes are classified similarly well and show expected activations for almost all attributes"],"url":"http://arxiv.org/abs/2403.14435v1","category":"cs.CV"}
{"created":"2024-03-21 14:41:42","title":"Maillet's property and Mahler's Conjecture on Liouville numbers fail for matrices","abstract":"In the early 1900's, Maillet proved that the image of any Liouville number under a rational function with rational coefficients is again a Liouville number. The analogous result for quadratic Liouville matrices in higher dimension turns out to fail. In fact, using a result by Kleinbock and Margulis, we show that among analytic matrix functions in dimension $n\\ge 2$, Maillet's invariance property is only true for M\\\"obius transformations with special coefficients. This implies that the analogue in higher dimension of an open question of Mahler on the existence of transcendental entire functions with Maillet's property has a negative answer. On the other hand, extending a topological argument of Erd\\H{o}s, we prove that for any injective continuous self mapping on the space of rectangular matrices, many Liouville matrices are mapped to Liouville matrices. Dropping injectivity, we consider setups similar to Alnia\\c{c}ik and Saias and show that the situation depends on the matrix dimensions $m,n$. Finally we discuss extensions of a related result by Burger to quadratic matrices. We state several open problems along the way.","sentences":["In the early 1900's, Maillet proved that the image of any Liouville number under a rational function with rational coefficients is again a Liouville number.","The analogous result for quadratic Liouville matrices in higher dimension turns out to fail.","In fact, using a result by Kleinbock and Margulis, we show that among analytic matrix functions in dimension $n\\ge 2$, Maillet's invariance property is only true for M\\\"obius transformations with special coefficients.","This implies that the analogue in higher dimension of an open question of Mahler on the existence of transcendental entire functions with Maillet's property has a negative answer.","On the other hand, extending a topological argument of Erd\\H{o}s, we prove that for any injective continuous self mapping on the space of rectangular matrices, many Liouville matrices are mapped to Liouville matrices.","Dropping injectivity, we consider setups similar to Alnia\\c{c}ik and Saias and show that the situation depends on the matrix dimensions $m,n$. Finally we discuss extensions of a related result by Burger to quadratic matrices.","We state several open problems along the way."],"url":"http://arxiv.org/abs/2403.14434v1","category":"math.NT"}
{"created":"2024-03-21 14:39:28","title":"On the continuity and smoothness of the value function in reinforcement learning and optimal control","abstract":"The value function plays a crucial role as a measure for the cumulative future reward an agent receives in both reinforcement learning and optimal control. It is therefore of interest to study how similar the values of neighboring states are, i.e., to investigate the continuity of the value function. We do so by providing and verifying upper bounds on the value function's modulus of continuity. Additionally, we show that the value function is always H\\\"older continuous under relatively weak assumptions on the underlying system and that non-differentiable value functions can be made differentiable by slightly \"disturbing\" the system.","sentences":["The value function plays a crucial role as a measure for the cumulative future reward an agent receives in both reinforcement learning and optimal control.","It is therefore of interest to study how similar the values of neighboring states are, i.e., to investigate the continuity of the value function.","We do so by providing and verifying upper bounds on the value function's modulus of continuity.","Additionally, we show that the value function is always H\\\"older continuous under relatively weak assumptions on the underlying system and that non-differentiable value functions can be made differentiable by slightly \"disturbing\" the system."],"url":"http://arxiv.org/abs/2403.14432v1","category":"eess.SY"}
{"created":"2024-03-21 14:37:50","title":"Ranking Distillation for Open-Ended Video Question Answering with Insufficient Labels","abstract":"This paper focuses on open-ended video question answering, which aims to find the correct answers from a large answer set in response to a video-related question. This is essentially a multi-label classification task, since a question may have multiple answers. However, due to annotation costs, the labels in existing benchmarks are always extremely insufficient, typically one answer per question. As a result, existing works tend to directly treat all the unlabeled answers as negative labels, leading to limited ability for generalization. In this work, we introduce a simple yet effective ranking distillation framework (RADI) to mitigate this problem without additional manual annotation. RADI employs a teacher model trained with incomplete labels to generate rankings for potential answers, which contain rich knowledge about label priority as well as label-associated visual cues, thereby enriching the insufficient labeling information. To avoid overconfidence in the imperfect teacher model, we further present two robust and parameter-free ranking distillation approaches: a pairwise approach which introduces adaptive soft margins to dynamically refine the optimization constraints on various pairwise rankings, and a listwise approach which adopts sampling-based partial listwise learning to resist the bias in teacher ranking. Extensive experiments on five popular benchmarks consistently show that both our pairwise and listwise RADIs outperform state-of-the-art methods. Further analysis demonstrates the effectiveness of our methods on the insufficient labeling problem.","sentences":["This paper focuses on open-ended video question answering, which aims to find the correct answers from a large answer set in response to a video-related question.","This is essentially a multi-label classification task, since a question may have multiple answers.","However, due to annotation costs, the labels in existing benchmarks are always extremely insufficient, typically one answer per question.","As a result, existing works tend to directly treat all the unlabeled answers as negative labels, leading to limited ability for generalization.","In this work, we introduce a simple yet effective ranking distillation framework (RADI) to mitigate this problem without additional manual annotation.","RADI employs a teacher model trained with incomplete labels to generate rankings for potential answers, which contain rich knowledge about label priority as well as label-associated visual cues, thereby enriching the insufficient labeling information.","To avoid overconfidence in the imperfect teacher model, we further present two robust and parameter-free ranking distillation approaches: a pairwise approach which introduces adaptive soft margins to dynamically refine the optimization constraints on various pairwise rankings, and a listwise approach which adopts sampling-based partial listwise learning to resist the bias in teacher ranking.","Extensive experiments on five popular benchmarks consistently show that both our pairwise and listwise RADIs outperform state-of-the-art methods.","Further analysis demonstrates the effectiveness of our methods on the insufficient labeling problem."],"url":"http://arxiv.org/abs/2403.14430v1","category":"cs.CV"}
{"created":"2024-03-21 14:36:59","title":"Style-Extracting Diffusion Models for Semi-Supervised Histopathology Segmentation","abstract":"Deep learning-based image generation has seen significant advancements with diffusion models, notably improving the quality of generated images. Despite these developments, generating images with unseen characteristics beneficial for downstream tasks has received limited attention. To bridge this gap, we propose Style-Extracting Diffusion Models, featuring two conditioning mechanisms. Specifically, we utilize 1) a style conditioning mechanism which allows to inject style information of previously unseen images during image generation and 2) a content conditioning which can be targeted to a downstream task, e.g., layout for segmentation. We introduce a trainable style encoder to extract style information from images, and an aggregation block that merges style information from multiple style inputs. This architecture enables the generation of images with unseen styles in a zero-shot manner, by leveraging styles from unseen images, resulting in more diverse generations. In this work, we use the image layout as target condition and first show the capability of our method on a natural image dataset as a proof-of-concept. We further demonstrate its versatility in histopathology, where we combine prior knowledge about tissue composition and unannotated data to create diverse synthetic images with known layouts. This allows us to generate additional synthetic data to train a segmentation network in a semi-supervised fashion. We verify the added value of the generated images by showing improved segmentation results and lower performance variability between patients when synthetic images are included during segmentation training. Our code will be made publicly available at [LINK].","sentences":["Deep learning-based image generation has seen significant advancements with diffusion models, notably improving the quality of generated images.","Despite these developments, generating images with unseen characteristics beneficial for downstream tasks has received limited attention.","To bridge this gap, we propose Style-Extracting Diffusion Models, featuring two conditioning mechanisms.","Specifically, we utilize 1) a style conditioning mechanism which allows to inject style information of previously unseen images during image generation and 2) a content conditioning which can be targeted to a downstream task, e.g., layout for segmentation.","We introduce a trainable style encoder to extract style information from images, and an aggregation block that merges style information from multiple style inputs.","This architecture enables the generation of images with unseen styles in a zero-shot manner, by leveraging styles from unseen images, resulting in more diverse generations.","In this work, we use the image layout as target condition and first show the capability of our method on a natural image dataset as a proof-of-concept.","We further demonstrate its versatility in histopathology, where we combine prior knowledge about tissue composition and unannotated data to create diverse synthetic images with known layouts.","This allows us to generate additional synthetic data to train a segmentation network in a semi-supervised fashion.","We verify the added value of the generated images by showing improved segmentation results and lower performance variability between patients when synthetic images are included during segmentation training.","Our code will be made publicly available at [LINK]."],"url":"http://arxiv.org/abs/2403.14429v1","category":"cs.CV"}
{"created":"2024-03-21 14:19:06","title":"Uniqueness of static vacuum asymptotically flat black holes and equipotential photon surfaces in $n+1$ dimensions \u00e0 la Robinson","abstract":"In this paper, we generalize to higher dimensions Robinson's approach to proving the uniqueness of connected $(3+1)$-dimensional static vacuum asymptotically flat black hole spacetimes. In consequence, we prove geometric inequalities for connected $(n+1)$-dimensional static vacuum asymptotically flat black holes, recovering results by Agostiniani and Mazzieri. We also obtain similar geometric inequalities for connected $(n+1)$-dimensional static vacuum asymptotically flat equipotential photon surfaces and in particular photon spheres. Assuming a natural upper bound on the total scalar curvature, we recover the well-known uniqueness results for such black holes and equipotential photon surfaces. We also extend the known equipotential photon surface uniqueness results to spacetimes with negative mass. In order to replace the role of the Cotton tensor in Robinson's proof, we introduce techniques inspired by the analysis of Ricci solitons.","sentences":["In this paper, we generalize to higher dimensions Robinson's approach to proving the uniqueness of connected $(3+1)$-dimensional static vacuum asymptotically flat black hole spacetimes.","In consequence, we prove geometric inequalities for connected $(n+1)$-dimensional static vacuum asymptotically flat black holes, recovering results by Agostiniani and Mazzieri.","We also obtain similar geometric inequalities for connected $(n+1)$-dimensional static vacuum asymptotically flat equipotential photon surfaces and in particular photon spheres.","Assuming a natural upper bound on the total scalar curvature, we recover the well-known uniqueness results for such black holes and equipotential photon surfaces.","We also extend the known equipotential photon surface uniqueness results to spacetimes with negative mass.","In order to replace the role of the Cotton tensor in Robinson's proof, we introduce techniques inspired by the analysis of Ricci solitons."],"url":"http://arxiv.org/abs/2403.14422v1","category":"math.DG"}
{"created":"2024-03-21 14:17:28","title":"DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning","abstract":"Text-to-image diffusion models have been shown to suffer from sample-level memorization, possibly reproducing near-perfect replica of images that they are trained on, which may be undesirable. To remedy this issue, we develop the first differentially private (DP) retrieval-augmented generation algorithm that is capable of generating high-quality image samples while providing provable privacy guarantees. Specifically, we assume access to a text-to-image diffusion model trained on a small amount of public data, and design a DP retrieval mechanism to augment the text prompt with samples retrieved from a private retrieval dataset. Our \\emph{differentially private retrieval-augmented diffusion model} (DP-RDM) requires no fine-tuning on the retrieval dataset to adapt to another domain, and can use state-of-the-art generative models to generate high-quality image samples while satisfying rigorous DP guarantees. For instance, when evaluated on MS-COCO, our DP-RDM can generate samples with a privacy budget of $\\epsilon=10$, while providing a $3.5$ point improvement in FID compared to public-only retrieval for up to $10,000$ queries.","sentences":["Text-to-image diffusion models have been shown to suffer from sample-level memorization, possibly reproducing near-perfect replica of images that they are trained on, which may be undesirable.","To remedy this issue, we develop the first differentially private (DP) retrieval-augmented generation algorithm that is capable of generating high-quality image samples while providing provable privacy guarantees.","Specifically, we assume access to a text-to-image diffusion model trained on a small amount of public data, and design a DP retrieval mechanism to augment the text prompt with samples retrieved from a private retrieval dataset.","Our \\emph{differentially private retrieval-augmented diffusion model} (DP-RDM) requires no fine-tuning on the retrieval dataset to adapt to another domain, and can use state-of-the-art generative models to generate high-quality image samples while satisfying rigorous DP guarantees.","For instance, when evaluated on MS-COCO, our DP-RDM can generate samples with a privacy budget of $\\epsilon=10$, while providing a $3.5$ point improvement in FID compared to public-only retrieval for up to $10,000$ queries."],"url":"http://arxiv.org/abs/2403.14421v1","category":"cs.LG"}
{"created":"2024-03-21 14:13:37","title":"Current density functional framework for spin-orbit coupling: Extension to periodic systems","abstract":"Spin-orbit coupling induces a current density in the ground state, which consequently requires a generalization for meta-generalized gradient approximations. That is, the exchange-correlation energy has to be constructed as an explicit functional of the current density and a generalized kinetic energy density has to be formed to satisfy theoretical constraints. Herein, we generalize our previously presented formalism of spin-orbit current density functional theory [Holzer et al., J. Chem. Phys. 157, 204102 (2022)] to non-magnetic and magnetic periodic systems of arbitrary dimension. Besides the ground-state exchange-correlation potential, analytical derivatives such as geometry gradients and stress tensors are implemented. The importance of the current density is assessed for band gaps, lattice constants, magnetic transitions, and Rashba splittings. For the latter, the impact of the current density may be larger than the deviation between different density functional approximations.","sentences":["Spin-orbit coupling induces a current density in the ground state, which consequently requires a generalization for meta-generalized gradient approximations.","That is, the exchange-correlation energy has to be constructed as an explicit functional of the current density and a generalized kinetic energy density has to be formed to satisfy theoretical constraints.","Herein, we generalize our previously presented formalism of spin-orbit current density functional theory [Holzer et al., J. Chem.","Phys. 157, 204102 (2022)] to non-magnetic and magnetic periodic systems of arbitrary dimension.","Besides the ground-state exchange-correlation potential, analytical derivatives such as geometry gradients and stress tensors are implemented.","The importance of the current density is assessed for band gaps, lattice constants, magnetic transitions, and Rashba splittings.","For the latter, the impact of the current density may be larger than the deviation between different density functional approximations."],"url":"http://arxiv.org/abs/2403.14420v1","category":"physics.chem-ph"}
{"created":"2024-03-21 14:08:31","title":"Five-flavour scheme predictions for $t\\bar{t}b\\bar{b}$ at next-to-leading order accuracy","abstract":"We compute top quark pair production in association with a bottom quark pair at the LHC within the five-flavour scheme, employing the FxFx merging scheme for $t\\bar{t}+\\textrm{jets}$ production with up to 2 jets at NLO accuracy. To enhance the selection efficiency for the events with $b$-jets within the inclusive five-flavour sample, we augment the generation probability of bottom quark flavours in the short-distance event generation. Our analysis reveals significant differences from NLO predictions within the four-flavour scheme.","sentences":["We compute top quark pair production in association with a bottom quark pair at the LHC within the five-flavour scheme, employing the FxFx merging scheme for $t\\bar{t}+\\textrm{jets}$ production with up to 2 jets at NLO accuracy.","To enhance the selection efficiency for the events with $b$-jets within the inclusive five-flavour sample, we augment the generation probability of bottom quark flavours in the short-distance event generation.","Our analysis reveals significant differences from NLO predictions within the four-flavour scheme."],"url":"http://arxiv.org/abs/2403.14419v1","category":"hep-ph"}
{"created":"2024-03-21 13:59:32","title":"Efficient Model Learning and Adaptive Tracking Control of Magnetic Micro-Robots for Non-Contact Manipulation","abstract":"Magnetic microrobots can be navigated by an external magnetic field to autonomously move within living organisms with complex and unstructured environments. Potential applications include drug delivery, diagnostics, and therapeutic interventions. Existing techniques commonly impart magnetic properties to the target object,or drive the robot to contact and then manipulate the object, both probably inducing physical damage. This paper considers a non-contact formulation, where the robot spins to generate a repulsive field to push the object without physical contact. Under such a formulation, the main challenge is that the motion model between the input of the magnetic field and the output velocity of the target object is commonly unknown and difficult to analyze. To deal with it, this paper proposes a data-driven-based solution. A neural network is constructed to efficiently estimate the motion model. Then, an approximate model-based optimal control scheme is developed to push the object to track a time-varying trajectory, maintaining the non-contact with distance constraints. Furthermore, a straightforward planner is introduced to assess the adaptability of non-contact manipulation in a cluttered unstructured environment. Experimental results are presented to show the tracking and navigation performance of the proposed scheme.","sentences":["Magnetic microrobots can be navigated by an external magnetic field to autonomously move within living organisms with complex and unstructured environments.","Potential applications include drug delivery, diagnostics, and therapeutic interventions.","Existing techniques commonly impart magnetic properties to the target object,or drive the robot to contact and then manipulate the object, both probably inducing physical damage.","This paper considers a non-contact formulation, where the robot spins to generate a repulsive field to push the object without physical contact.","Under such a formulation, the main challenge is that the motion model between the input of the magnetic field and the output velocity of the target object is commonly unknown and difficult to analyze.","To deal with it, this paper proposes a data-driven-based solution.","A neural network is constructed to efficiently estimate the motion model.","Then, an approximate model-based optimal control scheme is developed to push the object to track a time-varying trajectory, maintaining the non-contact with distance constraints.","Furthermore, a straightforward planner is introduced to assess the adaptability of non-contact manipulation in a cluttered unstructured environment.","Experimental results are presented to show the tracking and navigation performance of the proposed scheme."],"url":"http://arxiv.org/abs/2403.14414v1","category":"cs.RO"}
{"created":"2024-03-21 13:59:19","title":"Model Uncertainty in Evolutionary Optimization and Bayesian Optimization: A Comparative Analysis","abstract":"Black-box optimization problems, which are common in many real-world applications, require optimization through input-output interactions without access to internal workings. This often leads to significant computational resources being consumed for simulations. Bayesian Optimization (BO) and Surrogate-Assisted Evolutionary Algorithm (SAEA) are two widely used gradient-free optimization techniques employed to address such challenges. Both approaches follow a similar iterative procedure that relies on surrogate models to guide the search process. This paper aims to elucidate the similarities and differences in the utilization of model uncertainty between these two methods, as well as the impact of model inaccuracies on algorithmic performance. A novel model-assisted strategy is introduced, which utilizes unevaluated solutions to generate offspring, leveraging the population-based search capabilities of evolutionary algorithm to enhance the effectiveness of model-assisted optimization. Experimental results demonstrate that the proposed approach outperforms mainstream Bayesian optimization algorithms in terms of accuracy and efficiency.","sentences":["Black-box optimization problems, which are common in many real-world applications, require optimization through input-output interactions without access to internal workings.","This often leads to significant computational resources being consumed for simulations.","Bayesian Optimization (BO) and Surrogate-Assisted Evolutionary Algorithm (SAEA) are two widely used gradient-free optimization techniques employed to address such challenges.","Both approaches follow a similar iterative procedure that relies on surrogate models to guide the search process.","This paper aims to elucidate the similarities and differences in the utilization of model uncertainty between these two methods, as well as the impact of model inaccuracies on algorithmic performance.","A novel model-assisted strategy is introduced, which utilizes unevaluated solutions to generate offspring, leveraging the population-based search capabilities of evolutionary algorithm to enhance the effectiveness of model-assisted optimization.","Experimental results demonstrate that the proposed approach outperforms mainstream Bayesian optimization algorithms in terms of accuracy and efficiency."],"url":"http://arxiv.org/abs/2403.14413v1","category":"cs.NE"}
{"created":"2024-03-21 13:59:00","title":"CombiNeRF: A Combination of Regularization Techniques for Few-Shot Neural Radiance Field View Synthesis","abstract":"Neural Radiance Fields (NeRFs) have shown impressive results for novel view synthesis when a sufficiently large amount of views are available. When dealing with few-shot settings, i.e. with a small set of input views, the training could overfit those views, leading to artifacts and geometric and chromatic inconsistencies in the resulting rendering. Regularization is a valid solution that helps NeRF generalization. On the other hand, each of the most recent NeRF regularization techniques aim to mitigate a specific rendering problem. Starting from this observation, in this paper we propose CombiNeRF, a framework that synergically combines several regularization techniques, some of them novel, in order to unify the benefits of each. In particular, we regularize single and neighboring rays distributions and we add a smoothness term to regularize near geometries. After these geometric approaches, we propose to exploit Lipschitz regularization to both NeRF density and color networks and to use encoding masks for input features regularization. We show that CombiNeRF outperforms the state-of-the-art methods with few-shot settings in several publicly available datasets. We also present an ablation study on the LLFF and NeRF-Synthetic datasets that support the choices made. We release with this paper the open-source implementation of our framework.","sentences":["Neural Radiance Fields (NeRFs) have shown impressive results for novel view synthesis when a sufficiently large amount of views are available.","When dealing with few-shot settings, i.e. with a small set of input views, the training could overfit those views, leading to artifacts and geometric and chromatic inconsistencies in the resulting rendering.","Regularization is a valid solution that helps NeRF generalization.","On the other hand, each of the most recent NeRF regularization techniques aim to mitigate a specific rendering problem.","Starting from this observation, in this paper we propose CombiNeRF, a framework that synergically combines several regularization techniques, some of them novel, in order to unify the benefits of each.","In particular, we regularize single and neighboring rays distributions and we add a smoothness term to regularize near geometries.","After these geometric approaches, we propose to exploit Lipschitz regularization to both NeRF density and color networks and to use encoding masks for input features regularization.","We show that CombiNeRF outperforms the state-of-the-art methods with few-shot settings in several publicly available datasets.","We also present an ablation study on the LLFF and NeRF-Synthetic datasets that support the choices made.","We release with this paper the open-source implementation of our framework."],"url":"http://arxiv.org/abs/2403.14412v1","category":"cs.CV"}
{"created":"2024-03-21 13:57:45","title":"GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning","abstract":"Deep neural networks often exhibit sub-optimal performance under covariate and category shifts. Source-Free Domain Adaptation (SFDA) presents a promising solution to this dilemma, yet most SFDA approaches are restricted to closed-set scenarios. In this paper, we explore Source-Free Universal Domain Adaptation (SF-UniDA) aiming to accurately classify \"known\" data belonging to common categories and segregate them from target-private \"unknown\" data. We propose a novel Global and Local Clustering (GLC) technique, which comprises an adaptive one-vs-all global clustering algorithm to discern between target classes, complemented by a local k-NN clustering strategy to mitigate negative transfer. Despite the effectiveness, the inherent closed-set source architecture leads to uniform treatment of \"unknown\" data, impeding the identification of distinct \"unknown\" categories. To address this, we evolve GLC to GLC++, integrating a contrastive affinity learning strategy. We examine the superiority of GLC and GLC++ across multiple benchmarks and category shift scenarios. Remarkably, in the most challenging open-partial-set scenarios, GLC and GLC++ surpass GATE by 16.7% and 18.6% in H-score on VisDA, respectively. GLC++ enhances the novel category clustering accuracy of GLC by 4.3% in open-set scenarios on Office-Home. Furthermore, the introduced contrastive learning strategy not only enhances GLC but also significantly facilitates existing methodologies.","sentences":["Deep neural networks often exhibit sub-optimal performance under covariate and category shifts.","Source-Free Domain Adaptation (SFDA) presents a promising solution to this dilemma, yet most SFDA approaches are restricted to closed-set scenarios.","In this paper, we explore Source-Free Universal Domain Adaptation (SF-UniDA) aiming to accurately classify \"known\" data belonging to common categories and segregate them from target-private \"unknown\" data.","We propose a novel Global and Local Clustering (GLC) technique, which comprises an adaptive one-vs-all global clustering algorithm to discern between target classes, complemented by a local k-NN clustering strategy to mitigate negative transfer.","Despite the effectiveness, the inherent closed-set source architecture leads to uniform treatment of \"unknown\" data, impeding the identification of distinct \"unknown\" categories.","To address this, we evolve GLC to GLC++, integrating a contrastive affinity learning strategy.","We examine the superiority of GLC and GLC++ across multiple benchmarks and category shift scenarios.","Remarkably, in the most challenging open-partial-set scenarios, GLC and GLC++ surpass GATE by 16.7% and 18.6% in H-score on VisDA, respectively.","GLC++ enhances the novel category clustering accuracy of GLC by 4.3% in open-set scenarios on Office-Home.","Furthermore, the introduced contrastive learning strategy not only enhances GLC but also significantly facilitates existing methodologies."],"url":"http://arxiv.org/abs/2403.14410v1","category":"cs.CV"}
{"created":"2024-03-21 13:57:43","title":"Locating and Mitigating Gender Bias in Large Language Models","abstract":"Large language models(LLM) are pre-trained on extensive corpora to learn facts and human cognition which contain human preferences. However, this process can inadvertently lead to these models acquiring biases and stereotypes prevalent in society. Prior research has typically tackled the issue of bias through a one-dimensional perspective, concentrating either on locating or mitigating it. This limited perspective has created obstacles in facilitating research on bias to synergistically complement and progressively build upon one another. In this study, we integrate the processes of locating and mitigating bias within a unified framework. Initially, we use causal mediation analysis to trace the causal effects of different components' activation within a large language model. Building on this, we propose the LSDM (Least Square Debias Method), a knowledge-editing based method for mitigating gender bias in occupational pronouns, and compare it against two baselines on three gender bias datasets and seven knowledge competency test datasets. The experimental results indicate that the primary contributors to gender bias are the bottom MLP modules acting on the last token of occupational pronouns and the top attention module acting on the final word in the sentence. Furthermore, LSDM mitigates gender bias in the model more effectively than the other baselines, while fully preserving the model's capabilities in all other aspects.","sentences":["Large language models(LLM) are pre-trained on extensive corpora to learn facts and human cognition which contain human preferences.","However, this process can inadvertently lead to these models acquiring biases and stereotypes prevalent in society.","Prior research has typically tackled the issue of bias through a one-dimensional perspective, concentrating either on locating or mitigating it.","This limited perspective has created obstacles in facilitating research on bias to synergistically complement and progressively build upon one another.","In this study, we integrate the processes of locating and mitigating bias within a unified framework.","Initially, we use causal mediation analysis to trace the causal effects of different components' activation within a large language model.","Building on this, we propose the LSDM (Least Square Debias Method), a knowledge-editing based method for mitigating gender bias in occupational pronouns, and compare it against two baselines on three gender bias datasets and seven knowledge competency test datasets.","The experimental results indicate that the primary contributors to gender bias are the bottom MLP modules acting on the last token of occupational pronouns and the top attention module acting on the final word in the sentence.","Furthermore, LSDM mitigates gender bias in the model more effectively than the other baselines, while fully preserving the model's capabilities in all other aspects."],"url":"http://arxiv.org/abs/2403.14409v1","category":"cs.CL"}
{"created":"2024-03-21 13:56:32","title":"Quantum Machine Learning With a Limited Number Of Qubits","abstract":"Quantum circuit partitioning is a hybrid quantum-classical approach that aims to simulate large quantum systems on smaller quantum computers. The quantum computation is divided into smaller circuits and results of measurements on these circuits are combined using classical processing. Current approaches involve performing the Hadamard test or SWAP test and thus require an ancillary qubit with full qubit-connectivity. In this study, we show that for certain quantum states and observables, the approach can be realized by performing simple measurements of expectation values. However, this comes with a limitation on the applicable space of quantum states and observables. The approach was demonstrated in the realm of quantum machine learning, specifically to the digits dataset. When applied to the classification between the digits 3 and 6, we were able to generalize to out-of-sample data with an accuracy of $100 \\%$.","sentences":["Quantum circuit partitioning is a hybrid quantum-classical approach that aims to simulate large quantum systems on smaller quantum computers.","The quantum computation is divided into smaller circuits and results of measurements on these circuits are combined using classical processing.","Current approaches involve performing the Hadamard test or SWAP test and thus require an ancillary qubit with full qubit-connectivity.","In this study, we show that for certain quantum states and observables, the approach can be realized by performing simple measurements of expectation values.","However, this comes with a limitation on the applicable space of quantum states and observables.","The approach was demonstrated in the realm of quantum machine learning, specifically to the digits dataset.","When applied to the classification between the digits 3 and 6, we were able to generalize to out-of-sample data with an accuracy of $100 \\%$."],"url":"http://arxiv.org/abs/2403.14406v1","category":"quant-ph"}
{"created":"2024-03-21 13:52:55","title":"Physics-Informed Diffusion Models","abstract":"Generative models such as denoising diffusion models are quickly advancing their ability to approximate highly complex data distributions. They are also increasingly leveraged in scientific machine learning, where samples from the implied data distribution are expected to adhere to specific governing equations. We present a framework to inform denoising diffusion models on underlying constraints on such generated samples during model training. Our approach improves the alignment of the generated samples with the imposed constraints and significantly outperforms existing methods without affecting inference speed. Additionally, our findings suggest that incorporating such constraints during training provides a natural regularization against overfitting. Our framework is easy to implement and versatile in its applicability for imposing equality and inequality constraints as well as auxiliary optimization objectives.","sentences":["Generative models such as denoising diffusion models are quickly advancing their ability to approximate highly complex data distributions.","They are also increasingly leveraged in scientific machine learning, where samples from the implied data distribution are expected to adhere to specific governing equations.","We present a framework to inform denoising diffusion models on underlying constraints on such generated samples during model training.","Our approach improves the alignment of the generated samples with the imposed constraints and significantly outperforms existing methods without affecting inference speed.","Additionally, our findings suggest that incorporating such constraints during training provides a natural regularization against overfitting.","Our framework is easy to implement and versatile in its applicability for imposing equality and inequality constraints as well as auxiliary optimization objectives."],"url":"http://arxiv.org/abs/2403.14404v1","category":"cs.LG"}
{"created":"2024-03-21 13:52:30","title":"Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity","abstract":"Retrieval-Augmented Large Language Models (LLMs), which incorporate the non-parametric knowledge from external knowledge bases into LLMs, have emerged as a promising approach to enhancing response accuracy in several tasks, such as Question-Answering (QA). However, even though there are various approaches dealing with queries of different complexities, they either handle simple queries with unnecessary computational overhead or fail to adequately address complex multi-step queries; yet, not all user requests fall into only one of the simple or complex categories. In this work, we propose a novel adaptive QA framework, that can dynamically select the most suitable strategy for (retrieval-augmented) LLMs from the simplest to the most sophisticated ones based on the query complexity. Also, this selection process is operationalized with a classifier, which is a smaller LM trained to predict the complexity level of incoming queries with automatically collected labels, obtained from actual predicted outcomes of models and inherent inductive biases in datasets. This approach offers a balanced strategy, seamlessly adapting between the iterative and single-step retrieval-augmented LLMs, as well as the no-retrieval methods, in response to a range of query complexities. We validate our model on a set of open-domain QA datasets, covering multiple query complexities, and show that ours enhances the overall efficiency and accuracy of QA systems, compared to relevant baselines including the adaptive retrieval approaches. Code is available at: https://github.com/starsuzi/Adaptive-RAG.","sentences":["Retrieval-Augmented Large Language Models (LLMs), which incorporate the non-parametric knowledge from external knowledge bases into LLMs, have emerged as a promising approach to enhancing response accuracy in several tasks, such as Question-Answering (QA).","However, even though there are various approaches dealing with queries of different complexities, they either handle simple queries with unnecessary computational overhead or fail to adequately address complex multi-step queries; yet, not all user requests fall into only one of the simple or complex categories.","In this work, we propose a novel adaptive QA framework, that can dynamically select the most suitable strategy for (retrieval-augmented) LLMs from the simplest to the most sophisticated ones based on the query complexity.","Also, this selection process is operationalized with a classifier, which is a smaller LM trained to predict the complexity level of incoming queries with automatically collected labels, obtained from actual predicted outcomes of models and inherent inductive biases in datasets.","This approach offers a balanced strategy, seamlessly adapting between the iterative and single-step retrieval-augmented LLMs, as well as the no-retrieval methods, in response to a range of query complexities.","We validate our model on a set of open-domain QA datasets, covering multiple query complexities, and show that ours enhances the overall efficiency and accuracy of QA systems, compared to relevant baselines including the adaptive retrieval approaches.","Code is available at: https://github.com/starsuzi/Adaptive-RAG."],"url":"http://arxiv.org/abs/2403.14403v1","category":"cs.CL"}
{"created":"2024-03-21 13:49:42","title":"Pensieve: Retrospect-then-Compare Mitigates Visual Hallucination","abstract":"Multi-modal Large Language Models (MLLMs) demonstrate remarkable success across various vision-language tasks. However, they suffer from visual hallucination, where the generated responses diverge from the provided image. Are MLLMs completely oblivious to accurate visual cues when they hallucinate? Our investigation reveals that the visual branch may simultaneously advocate both accurate and non-existent content. To address this issue, we propose Pensieve, a training-free method inspired by our observation that analogous visual hallucinations can arise among images sharing common semantic and appearance characteristics. During inference, Pensieve enables MLLMs to retrospect relevant images as references and compare them with the test image. This paradigm assists MLLMs in downgrading hallucinatory content mistakenly supported by the visual input. Experiments on Whoops, MME, POPE, and LLaVA Bench demonstrate the efficacy of Pensieve in mitigating visual hallucination, surpassing other advanced decoding strategies. Additionally, Pensieve aids MLLMs in identifying details in the image and enhancing the specificity of image descriptions.","sentences":["Multi-modal Large Language Models (MLLMs) demonstrate remarkable success across various vision-language tasks.","However, they suffer from visual hallucination, where the generated responses diverge from the provided image.","Are MLLMs completely oblivious to accurate visual cues when they hallucinate?","Our investigation reveals that the visual branch may simultaneously advocate both accurate and non-existent content.","To address this issue, we propose Pensieve, a training-free method inspired by our observation that analogous visual hallucinations can arise among images sharing common semantic and appearance characteristics.","During inference, Pensieve enables MLLMs to retrospect relevant images as references and compare them with the test image.","This paradigm assists MLLMs in downgrading hallucinatory content mistakenly supported by the visual input.","Experiments on Whoops, MME, POPE, and LLaVA Bench demonstrate the efficacy of Pensieve in mitigating visual hallucination, surpassing other advanced decoding strategies.","Additionally, Pensieve aids MLLMs in identifying details in the image and enhancing the specificity of image descriptions."],"url":"http://arxiv.org/abs/2403.14401v1","category":"cs.CV"}
{"created":"2024-03-21 13:47:40","title":"Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning","abstract":"Translation-tailored Large language models (LLMs) exhibit remarkable translation capabilities, even competing with supervised-trained commercial translation systems. However, off-target translation remains an unsolved problem, especially for low-resource languages, hindering us from developing accurate LLMs-based translation models. To mitigate the off-target translation problem and enhance the performance of LLMs on translation, recent works have either designed advanced prompting strategies to highlight the functionality of translation instructions or exploited the in-context learning ability of LLMs by feeding few-shot demonstrations. However, these methods essentially do not improve LLM's ability to follow translation instructions, especially the language direction information. In this work, we design a two-stage fine-tuning algorithm to improve the instruction-following ability (especially the translation direction) of LLMs. Specifically, we first tune LLMs with the maximum likelihood estimation loss on the translation dataset to elicit the basic translation capabilities. In the second stage, we construct instruction-conflicting samples by randomly replacing the translation directions with a wrong one within the instruction, and then introduce an extra unlikelihood loss to learn those samples. Experiments on IWSLT and WMT benchmarks upon the LLaMA model spanning 16 zero-shot directions show that, compared to the competitive baseline -- translation-finetuned LLama, our method could effectively reduce the off-target translation ratio (averagely -53.3\\%), thus improving translation quality with average +5.7 SacreBLEU and +16.4 BLEURT. Analysis shows that our method could preserve the model's general task performance on AlpacaEval. Code and models will be released at \\url{https://github.com/alphadl/LanguageAware_Tuning}.","sentences":["Translation-tailored Large language models (LLMs) exhibit remarkable translation capabilities, even competing with supervised-trained commercial translation systems.","However, off-target translation remains an unsolved problem, especially for low-resource languages, hindering us from developing accurate LLMs-based translation models.","To mitigate the off-target translation problem and enhance the performance of LLMs on translation, recent works have either designed advanced prompting strategies to highlight the functionality of translation instructions or exploited the in-context learning ability of LLMs by feeding few-shot demonstrations.","However, these methods essentially do not improve LLM's ability to follow translation instructions, especially the language direction information.","In this work, we design a two-stage fine-tuning algorithm to improve the instruction-following ability (especially the translation direction) of LLMs.","Specifically, we first tune LLMs with the maximum likelihood estimation loss on the translation dataset to elicit the basic translation capabilities.","In the second stage, we construct instruction-conflicting samples by randomly replacing the translation directions with a wrong one within the instruction, and then introduce an extra unlikelihood loss to learn those samples.","Experiments on IWSLT and WMT benchmarks upon the LLaMA model spanning 16 zero-shot directions show that, compared to the competitive baseline -- translation-finetuned LLama, our method could effectively reduce the off-target translation ratio (averagely -53.3\\%), thus improving translation quality with average +5.7 SacreBLEU and +16.4 BLEURT.","Analysis shows that our method could preserve the model's general task performance on AlpacaEval.","Code and models will be released at \\url{https://github.com/alphadl/LanguageAware_Tuning}."],"url":"http://arxiv.org/abs/2403.14399v1","category":"cs.CL"}
{"created":"2024-03-21 13:43:49","title":"Regularized Adaptive Momentum Dual Averaging with an Efficient Inexact Subproblem Solver for Training Structured Neural Network","abstract":"We propose a Regularized Adaptive Momentum Dual Averaging (RAMDA) algorithm for training structured neural networks. Similar to existing regularized adaptive methods, the subproblem for computing the update direction of RAMDA involves a nonsmooth regularizer and a diagonal preconditioner, and therefore does not possess a closed-form solution in general. We thus also carefully devise an implementable inexactness condition that retains convergence guarantees similar to the exact versions, and propose a companion efficient solver for the subproblems of both RAMDA and existing methods to make them practically feasible. We leverage the theory of manifold identification in variational analysis to show that, even in the presence of such inexactness, the iterates of RAMDA attain the ideal structure induced by the regularizer at the stationary point of asymptotic convergence. This structure is locally optimal near the point of convergence, so RAMDA is guaranteed to obtain the best structure possible among all methods converging to the same point, making it the first regularized adaptive method outputting models that possess outstanding predictive performance while being (locally) optimally structured. Extensive numerical experiments in large-scale modern computer vision, language modeling, and speech tasks show that the proposed RAMDA is efficient and consistently outperforms state of the art for training structured neural network. Implementation of our algorithm is available at http://www.github.com/ismoptgroup/RAMDA/.","sentences":["We propose a Regularized Adaptive Momentum Dual Averaging (RAMDA) algorithm for training structured neural networks.","Similar to existing regularized adaptive methods, the subproblem for computing the update direction of RAMDA involves a nonsmooth regularizer and a diagonal preconditioner, and therefore does not possess a closed-form solution in general.","We thus also carefully devise an implementable inexactness condition that retains convergence guarantees similar to the exact versions, and propose a companion efficient solver for the subproblems of both RAMDA and existing methods to make them practically feasible.","We leverage the theory of manifold identification in variational analysis to show that, even in the presence of such inexactness, the iterates of RAMDA attain the ideal structure induced by the regularizer at the stationary point of asymptotic convergence.","This structure is locally optimal near the point of convergence, so RAMDA is guaranteed to obtain the best structure possible among all methods converging to the same point, making it the first regularized adaptive method outputting models that possess outstanding predictive performance while being (locally) optimally structured.","Extensive numerical experiments in large-scale modern computer vision, language modeling, and speech tasks show that the proposed RAMDA is efficient and consistently outperforms state of the art for training structured neural network.","Implementation of our algorithm is available at http://www.github.com/ismoptgroup/RAMDA/."],"url":"http://arxiv.org/abs/2403.14398v1","category":"cs.LG"}
{"created":"2024-03-21 13:42:02","title":"Probing Primordial Black Holes and Dark Matter Clumps in the Solar System with Gravimeter and GNSS Networks","abstract":"We show that Global Navigation Satellite Systems (GNSS) and gravimeters on Earth and in space can potentially offer the most accurate direct measurement of local density of near-Earth asteroid-mass Primordial Black Holes (PBHs) and Dark Matter (DM) clumps in the solar system by means of gravitational influence. Using semi-analytical methods and Monte Carlo simulation, this paper revisits the analysis of the trajectories of DM clumps in the solar system, including both captured objects and hyperbolic trajectories. A link is thus made between the frequency and distance of Earth overflights for a given mass flux, and a direct measure of dark matter clump density in the solar system. We then model the signature of a close flyby of a DM object on orbital data from GNSS satellites and gravity measurements from gravimeters. We thus obtain a first assessment of the single probe sensitivity. It paves the way for an exhaustive statistical analysis of 28 years of gravimeters and GNSS data to obtain observational constraints on the density of the PBHs and DM clumps within the solar system, for the mass range $[10^8-10^{17}]$ kg. In addition, our methodology offers a possibility of direct detection in cases where DM clumps are endowed with an additional long-range clump-matter fifth-force beyond gravity.","sentences":["We show that Global Navigation Satellite Systems (GNSS) and gravimeters on Earth and in space can potentially offer the most accurate direct measurement of local density of near-Earth asteroid-mass Primordial Black Holes (PBHs) and Dark Matter (DM) clumps in the solar system by means of gravitational influence.","Using semi-analytical methods and Monte Carlo simulation, this paper revisits the analysis of the trajectories of DM clumps in the solar system, including both captured objects and hyperbolic trajectories.","A link is thus made between the frequency and distance of Earth overflights for a given mass flux, and a direct measure of dark matter clump density in the solar system.","We then model the signature of a close flyby of a DM object on orbital data from GNSS satellites and gravity measurements from gravimeters.","We thus obtain a first assessment of the single probe sensitivity.","It paves the way for an exhaustive statistical analysis of 28 years of gravimeters and GNSS data to obtain observational constraints on the density of the PBHs and DM clumps within the solar system, for the mass range $[10^8-10^{17}]$ kg.","In addition, our methodology offers a possibility of direct detection in cases where DM clumps are endowed with an additional long-range clump-matter fifth-force beyond gravity."],"url":"http://arxiv.org/abs/2403.14397v1","category":"astro-ph.CO"}
{"created":"2024-03-21 13:40:50","title":"Infinite horizon McKean-Vlasov FBSDEs and applications to mean field control problems","abstract":"In this paper, we study a class of infinite horizon fully coupled McKean-Vlasov forward-backward stochastic differential equations (FBSDEs). We propose a generalized monotonicity condition involving two flexible functions. Under this condition, we establish the well-posedness results for infinite horizon McKean-Vlasov FBSDEs by the method of continuation, including the unique solvability, an estimate of the solution, and the related continuous dependence property of the solution on the coefficients. Based on the solvability result, we study an infinite horizon mean field control problem. Moreover, by choosing appropriate form of the flexible functions, we can eliminate the different phenomenon between the linear-quadratic (LQ) problems on infinite horizon and finite horizon proposed in Wei and Yu (SIAM J. Control Optim. 59: 2594-2623, 2021).","sentences":["In this paper, we study a class of infinite horizon fully coupled McKean-Vlasov forward-backward stochastic differential equations (FBSDEs).","We propose a generalized monotonicity condition involving two flexible functions.","Under this condition, we establish the well-posedness results for infinite horizon McKean-Vlasov FBSDEs by the method of continuation, including the unique solvability, an estimate of the solution, and the related continuous dependence property of the solution on the coefficients.","Based on the solvability result, we study an infinite horizon mean field control problem.","Moreover, by choosing appropriate form of the flexible functions, we can eliminate the different phenomenon between the linear-quadratic (LQ) problems on infinite horizon and finite horizon proposed in Wei and Yu (SIAM J. Control Optim.","59: 2594-2623, 2021)."],"url":"http://arxiv.org/abs/2403.14396v1","category":"math.OC"}
{"created":"2024-03-21 13:29:54","title":"From Large to Tiny: Distilling and Refining Mathematical Expertise for Math Word Problems with Weakly Supervision","abstract":"Addressing the challenge of high annotation costs in solving Math Word Problems (MWPs) through full supervision with intermediate equations, recent works have proposed weakly supervised task settings that rely solely on the final answer as a supervised signal. Existing leading approaches typically employ various search techniques to infer intermediate equations, but cannot ensure their semantic consistency with natural language descriptions. The rise of Large Language Models (LLMs) like ChatGPT has opened up new possibilities for addressing MWPs directly. However, the computational demands of LLMs make them less than ideal for use in settings where resources are tight. In light of these challenges, we introduce an innovative two-stage framework that adeptly transfers mathematical Expertise from large to tiny language models. In \\emph{Distillation Stage}, we propose a series of extraction processes that satisfy the properties of MWPs to distill mathematical knowledge from LLMs to construct problem-equation pairs required for supervised training. In \\emph{Refinement Stage}, Due to Knowledge distilling method cannot guarantee the full utilization of all data, we further utilize the unsuccessfully searched data effectively by Knowledge Refine method. Finally, We train a small model using distilled data generated through two-stage methods. As our method fully leverages the semantic understanding capabilities during the searching 'problem-equation' pair, it demonstrates significantly improved performance on the Math23K and Weak12K datasets compared to existing small model methods, while maintaining a much lower computational cost than ChatGPT.","sentences":["Addressing the challenge of high annotation costs in solving Math Word Problems (MWPs) through full supervision with intermediate equations, recent works have proposed weakly supervised task settings that rely solely on the final answer as a supervised signal.","Existing leading approaches typically employ various search techniques to infer intermediate equations, but cannot ensure their semantic consistency with natural language descriptions.","The rise of Large Language Models (LLMs) like ChatGPT has opened up new possibilities for addressing MWPs directly.","However, the computational demands of LLMs make them less than ideal for use in settings where resources are tight.","In light of these challenges, we introduce an innovative two-stage framework that adeptly transfers mathematical Expertise from large to tiny language models.","In \\emph{Distillation Stage}, we propose a series of extraction processes that satisfy the properties of MWPs to distill mathematical knowledge from LLMs to construct problem-equation pairs required for supervised training.","In \\emph{Refinement Stage}, Due to Knowledge distilling method cannot guarantee the full utilization of all data, we further utilize the unsuccessfully searched data effectively by Knowledge Refine method.","Finally, We train a small model using distilled data generated through two-stage methods.","As our method fully leverages the semantic understanding capabilities during the searching 'problem-equation' pair, it demonstrates significantly improved performance on the Math23K and Weak12K datasets compared to existing small model methods, while maintaining a much lower computational cost than ChatGPT."],"url":"http://arxiv.org/abs/2403.14390v1","category":"cs.CL"}
{"created":"2024-03-21 13:15:25","title":"Editing Knowledge Representation of Language Lodel via Rephrased Prefix Prompts","abstract":"Neural language models (LMs) have been extensively trained on vast corpora to store factual knowledge about various aspects of the world described in texts. Current technologies typically employ knowledge editing methods or specific prompts to modify LM outputs. However, existing knowledge editing methods are costly and inefficient, struggling to produce appropriate text. Additionally, prompt engineering is opaque and requires significant effort to find suitable prompts. To address these issues, we introduce a new method called PSPEM (Prefix Soft Prompt Editing Method), that can be used for a lifetime with just one training. It resolves the inefficiencies and generalizability issues in knowledge editing methods and overcomes the opacity of prompt engineering by automatically seeking optimal soft prompts. Specifically, PSPEM utilizes a prompt encoder and an encoding converter to refine key information in prompts and uses prompt alignment techniques to guide model generation, ensuring text consistency and adherence to the intended structure and content, thereby maintaining an optimal balance between efficiency and accuracy. We have validated the effectiveness of PSPEM through knowledge editing and attribute inserting. On the COUNTERFACT dataset, PSPEM achieved nearly 100\\% editing accuracy and demonstrated the highest level of fluency. We further analyzed the similarities between PSPEM and original prompts and their impact on the model's internals. The results indicate that PSPEM can serve as an alternative to original prompts, supporting the model in effective editing.","sentences":["Neural language models (LMs) have been extensively trained on vast corpora to store factual knowledge about various aspects of the world described in texts.","Current technologies typically employ knowledge editing methods or specific prompts to modify LM outputs.","However, existing knowledge editing methods are costly and inefficient, struggling to produce appropriate text.","Additionally, prompt engineering is opaque and requires significant effort to find suitable prompts.","To address these issues, we introduce a new method called PSPEM (Prefix Soft Prompt Editing Method), that can be used for a lifetime with just one training.","It resolves the inefficiencies and generalizability issues in knowledge editing methods and overcomes the opacity of prompt engineering by automatically seeking optimal soft prompts.","Specifically, PSPEM utilizes a prompt encoder and an encoding converter to refine key information in prompts and uses prompt alignment techniques to guide model generation, ensuring text consistency and adherence to the intended structure and content, thereby maintaining an optimal balance between efficiency and accuracy.","We have validated the effectiveness of PSPEM through knowledge editing and attribute inserting.","On the COUNTERFACT dataset, PSPEM achieved nearly 100\\% editing accuracy and demonstrated the highest level of fluency.","We further analyzed the similarities between PSPEM and original prompts and their impact on the model's internals.","The results indicate that PSPEM can serve as an alternative to original prompts, supporting the model in effective editing."],"url":"http://arxiv.org/abs/2403.14381v1","category":"cs.CL"}
{"created":"2024-03-21 13:14:40","title":"On the Conversational Persuasiveness of Large Language Models: A Randomized Controlled Trial","abstract":"The development and popularization of large language models (LLMs) have raised concerns that they will be used to create tailor-made, convincing arguments to push false or misleading narratives online. Early work has found that language models can generate content perceived as at least on par and often more persuasive than human-written messages. However, there is still limited knowledge about LLMs' persuasive capabilities in direct conversations with human counterparts and how personalization can improve their performance. In this pre-registered study, we analyze the effect of AI-driven persuasion in a controlled, harmless setting. We create a web-based platform where participants engage in short, multiple-round debates with a live opponent. Each participant is randomly assigned to one of four treatment conditions, corresponding to a two-by-two factorial design: (1) Games are either played between two humans or between a human and an LLM; (2) Personalization might or might not be enabled, granting one of the two players access to basic sociodemographic information about their opponent. We found that participants who debated GPT-4 with access to their personal information had 81.7% (p < 0.01; N=820 unique participants) higher odds of increased agreement with their opponents compared to participants who debated humans. Without personalization, GPT-4 still outperforms humans, but the effect is lower and statistically non-significant (p=0.31). Overall, our results suggest that concerns around personalization are meaningful and have important implications for the governance of social media and the design of new online environments.","sentences":["The development and popularization of large language models (LLMs) have raised concerns that they will be used to create tailor-made, convincing arguments to push false or misleading narratives online.","Early work has found that language models can generate content perceived as at least on par and often more persuasive than human-written messages.","However, there is still limited knowledge about LLMs' persuasive capabilities in direct conversations with human counterparts and how personalization can improve their performance.","In this pre-registered study, we analyze the effect of AI-driven persuasion in a controlled, harmless setting.","We create a web-based platform where participants engage in short, multiple-round debates with a live opponent.","Each participant is randomly assigned to one of four treatment conditions, corresponding to a two-by-two factorial design: (1) Games are either played between two humans or between a human and an LLM; (2) Personalization might or might not be enabled, granting one of the two players access to basic sociodemographic information about their opponent.","We found that participants who debated GPT-4 with access to their personal information had 81.7% (p < 0.01; N=820 unique participants) higher odds of increased agreement with their opponents compared to participants who debated humans.","Without personalization, GPT-4 still outperforms humans, but the effect is lower and statistically non-significant (p=0.31).","Overall, our results suggest that concerns around personalization are meaningful and have important implications for the governance of social media and the design of new online environments."],"url":"http://arxiv.org/abs/2403.14380v1","category":"cs.CY"}
{"created":"2024-03-21 13:12:33","title":"Tensor network compressibility of convolutional models","abstract":"Convolutional neural networks (CNNs) represent one of the most widely used neural network architectures, showcasing state-of-the-art performance in computer vision tasks. Although larger CNNs generally exhibit higher accuracy, their size can be effectively reduced by \"tensorization\" while maintaining accuracy. Tensorization consists of replacing the convolution kernels with compact decompositions such as Tucker, Canonical Polyadic decompositions, or quantum-inspired decompositions such as matrix product states, and directly training the factors in the decompositions to bias the learning towards low-rank decompositions. But why doesn't tensorization seem to impact the accuracy adversely? We explore this by assessing how truncating the convolution kernels of dense (untensorized) CNNs impact their accuracy. Specifically, we truncated the kernels of (i) a vanilla four-layer CNN and (ii) ResNet-50 pre-trained for image classification on CIFAR-10 and CIFAR-100 datasets. We found that kernels (especially those inside deeper layers) could often be truncated along several cuts resulting in significant loss in kernel norm but not in classification accuracy. This suggests that such ``correlation compression'' (underlying tensorization) is an intrinsic feature of how information is encoded in dense CNNs. We also found that aggressively truncated models could often recover the pre-truncation accuracy after only a few epochs of re-training, suggesting that compressing the internal correlations of convolution layers does not often transport the model to a worse minimum. Our results can be applied to tensorize and compress CNN models more effectively.","sentences":["Convolutional neural networks (CNNs) represent one of the most widely used neural network architectures, showcasing state-of-the-art performance in computer vision tasks.","Although larger CNNs generally exhibit higher accuracy, their size can be effectively reduced by \"tensorization\" while maintaining accuracy.","Tensorization consists of replacing the convolution kernels with compact decompositions such as Tucker, Canonical Polyadic decompositions, or quantum-inspired decompositions such as matrix product states, and directly training the factors in the decompositions to bias the learning towards low-rank decompositions.","But why doesn't tensorization seem to impact the accuracy adversely?","We explore this by assessing how truncating the convolution kernels of dense (untensorized) CNNs impact their accuracy.","Specifically, we truncated the kernels of (i) a vanilla four-layer CNN and (ii) ResNet-50 pre-trained for image classification on CIFAR-10 and CIFAR-100 datasets.","We found that kernels (especially those inside deeper layers) could often be truncated along several cuts resulting in significant loss in kernel norm but not in classification accuracy.","This suggests that such ``correlation compression'' (underlying tensorization) is an intrinsic feature of how information is encoded in dense CNNs.","We also found that aggressively truncated models could often recover the pre-truncation accuracy after only a few epochs of re-training, suggesting that compressing the internal correlations of convolution layers does not often transport the model to a worse minimum.","Our results can be applied to tensorize and compress CNN models more effectively."],"url":"http://arxiv.org/abs/2403.14379v1","category":"cs.CV"}
{"created":"2024-03-21 13:11:18","title":"Every finite-dimensional analytic space is $\u03c3$-homogeneous","abstract":"All spaces are assumed to be separable and metrizable. Building on work of van Engelen, Harrington, Michalewski and Ostrovsky, we obtain the following results: (1) Every finite-dimensional analytic space is $\\sigma$-homogeneous with analytic witnesses, (2) Every finite-dimensional analytic space is $\\sigma$-homogeneous with pairwise disjoint $\\mathbf{\\Delta}^1_2$ witnesses. Furthermore, the complexity of the witnesses is optimal in both of the above results. This completes the picture regarding $\\sigma$-homogeneity in the finite-dimensional realm. It is an open problem whether every analytic space is $\\sigma$-homogeneous. We also investigate finite unions of homogeneous spaces.","sentences":["All spaces are assumed to be separable and metrizable.","Building on work of van Engelen, Harrington, Michalewski and Ostrovsky, we obtain the following results: (1) Every finite-dimensional analytic space is $\\sigma$-homogeneous with analytic witnesses, (2) Every finite-dimensional analytic space is $\\sigma$-homogeneous with pairwise disjoint $\\mathbf{\\Delta}^1_2$ witnesses.","Furthermore, the complexity of the witnesses is optimal in both of the above results.","This completes the picture regarding $\\sigma$-homogeneity in the finite-dimensional realm.","It is an open problem whether every analytic space is $\\sigma$-homogeneous.","We also investigate finite unions of homogeneous spaces."],"url":"http://arxiv.org/abs/2403.14378v1","category":"math.GN"}
{"created":"2024-03-21 13:09:23","title":"Knowledge-Enhanced Recommendation with User-Centric Subgraph Network","abstract":"Recommendation systems, as widely implemented nowadays on various platforms, recommend relevant items to users based on their preferences. The classical methods which rely on user-item interaction matrices has limitations, especially in scenarios where there is a lack of interaction data for new items. Knowledge graph (KG)-based recommendation systems have emerged as a promising solution. However, most KG-based methods adopt node embeddings, which do not provide personalized recommendations for different users and cannot generalize well to the new items. To address these limitations, we propose Knowledge-enhanced User-Centric subgraph Network (KUCNet), a subgraph learning approach with graph neural network (GNN) for effective recommendation. KUCNet constructs a U-I subgraph for each user-item pair that captures both the historical information of user-item interactions and the side information provided in KG. An attention-based GNN is designed to encode the U-I subgraphs for recommendation. Considering efficiency, the pruned user-centric computation graph is further introduced such that multiple U-I subgraphs can be simultaneously computed and that the size can be pruned by Personalized PageRank. Our proposed method achieves accurate, efficient, and interpretable recommendations especially for new items. Experimental results demonstrate the superiority of KUCNet over state-of-the-art KG-based and collaborative filtering (CF)-based methods.","sentences":["Recommendation systems, as widely implemented nowadays on various platforms, recommend relevant items to users based on their preferences.","The classical methods which rely on user-item interaction matrices has limitations, especially in scenarios where there is a lack of interaction data for new items.","Knowledge graph (KG)-based recommendation systems have emerged as a promising solution.","However, most KG-based methods adopt node embeddings, which do not provide personalized recommendations for different users and cannot generalize well to the new items.","To address these limitations, we propose Knowledge-enhanced User-Centric subgraph Network (KUCNet), a subgraph learning approach with graph neural network (GNN) for effective recommendation.","KUCNet constructs a U-I subgraph for each user-item pair that captures both the historical information of user-item interactions and the side information provided in KG.","An attention-based GNN is designed to encode the U-I subgraphs for recommendation.","Considering efficiency, the pruned user-centric computation graph is further introduced such that multiple U-I subgraphs can be simultaneously computed and that the size can be pruned by Personalized PageRank.","Our proposed method achieves accurate, efficient, and interpretable recommendations especially for new items.","Experimental results demonstrate the superiority of KUCNet over state-of-the-art KG-based and collaborative filtering (CF)-based methods."],"url":"http://arxiv.org/abs/2403.14377v1","category":"cs.IR"}
{"created":"2024-03-21 13:05:18","title":"FIT-RAG: Black-Box RAG with Factual Information and Token Reduction","abstract":"Due to the extraordinarily large number of parameters, fine-tuning Large Language Models (LLMs) to update long-tail or out-of-date knowledge is impractical in lots of applications. To avoid fine-tuning, we can alternatively treat a LLM as a black-box (i.e., freeze the parameters of the LLM) and augment it with a Retrieval-Augmented Generation (RAG) system, namely black-box RAG. Recently, black-box RAG has achieved success in knowledge-intensive tasks and has gained much attention. Existing black-box RAG methods typically fine-tune the retriever to cater to LLMs' preferences and concatenate all the retrieved documents as the input, which suffers from two issues: (1) Ignorance of Factual Information. The LLM preferred documents may not contain the factual information for the given question, which can mislead the retriever and hurt the effectiveness of black-box RAG; (2) Waste of Tokens. Simply concatenating all the retrieved documents brings large amounts of unnecessary tokens for LLMs, which degenerates the efficiency of black-box RAG. To address these issues, this paper proposes a novel black-box RAG framework which utilizes the factual information in the retrieval and reduces the number of tokens for augmentation, dubbed FIT-RAG. FIT-RAG utilizes the factual information by constructing a bi-label document scorer. Besides, it reduces the tokens by introducing a self-knowledge recognizer and a sub-document-level token reducer. FIT-RAG achieves both superior effectiveness and efficiency, which is validated by extensive experiments across three open-domain question-answering datasets: TriviaQA, NQ and PopQA. FIT-RAG can improve the answering accuracy of Llama2-13B-Chat by 14.3\\% on TriviaQA, 19.9\\% on NQ and 27.5\\% on PopQA, respectively. Furthermore, it can save approximately half of the tokens on average across the three datasets.","sentences":["Due to the extraordinarily large number of parameters, fine-tuning Large Language Models (LLMs) to update long-tail or out-of-date knowledge is impractical in lots of applications.","To avoid fine-tuning, we can alternatively treat a LLM as a black-box (i.e., freeze the parameters of the LLM) and augment it with a Retrieval-Augmented Generation (RAG) system, namely black-box RAG.","Recently, black-box RAG has achieved success in knowledge-intensive tasks and has gained much attention.","Existing black-box RAG methods typically fine-tune the retriever to cater to LLMs' preferences and concatenate all the retrieved documents as the input, which suffers from two issues: (1) Ignorance of Factual Information.","The LLM preferred documents may not contain the factual information for the given question, which can mislead the retriever and hurt the effectiveness of black-box RAG; (2) Waste of Tokens.","Simply concatenating all the retrieved documents brings large amounts of unnecessary tokens for LLMs, which degenerates the efficiency of black-box RAG.","To address these issues, this paper proposes a novel black-box RAG framework which utilizes the factual information in the retrieval and reduces the number of tokens for augmentation, dubbed FIT-RAG.","FIT-RAG utilizes the factual information by constructing a bi-label document scorer.","Besides, it reduces the tokens by introducing a self-knowledge recognizer and a sub-document-level token reducer.","FIT-RAG achieves both superior effectiveness and efficiency, which is validated by extensive experiments across three open-domain question-answering datasets: TriviaQA, NQ and PopQA.","FIT-RAG can improve the answering accuracy of Llama2-13B-Chat by 14.3\\% on TriviaQA, 19.9\\% on NQ and 27.5\\% on PopQA, respectively.","Furthermore, it can save approximately half of the tokens on average across the three datasets."],"url":"http://arxiv.org/abs/2403.14374v1","category":"cs.CL"}
{"created":"2024-03-21 12:59:24","title":"Loop Improvement: An Efficient Approach for Extracting Shared Features from Heterogeneous Data without Central Server","abstract":"In federated learning, data heterogeneity significantly impacts performance. A typical solution involves segregating these parameters into shared and personalized components, a concept also relevant in multi-task learning. Addressing this, we propose \"Loop Improvement\" (LI), a novel method enhancing this separation and feature extraction without necessitating a central server or data interchange among participants. Our experiments reveal LI's superiority in several aspects: In personalized federated learning environments, LI consistently outperforms the advanced FedALA algorithm in accuracy across diverse scenarios. Additionally, LI's feature extractor closely matches the performance achieved when aggregating data from all clients. In global model contexts, employing LI with stacked personalized layers and an additional network also yields comparable results to combined client data scenarios. Furthermore, LI's adaptability extends to multi-task learning, streamlining the extraction of common features across tasks and obviating the need for simultaneous training. This approach not only enhances individual task performance but also achieves accuracy levels on par with classic multi-task learning methods where all tasks are trained simultaneously. LI integrates a loop topology with layer-wise and end-to-end training, compatible with various neural network models. This paper also delves into the theoretical underpinnings of LI's effectiveness, offering insights into its potential applications. The code is on https://github.com/axedge1983/LI","sentences":["In federated learning, data heterogeneity significantly impacts performance.","A typical solution involves segregating these parameters into shared and personalized components, a concept also relevant in multi-task learning.","Addressing this, we propose \"Loop Improvement\" (LI), a novel method enhancing this separation and feature extraction without necessitating a central server or data interchange among participants.","Our experiments reveal LI's superiority in several aspects: In personalized federated learning environments, LI consistently outperforms the advanced FedALA algorithm in accuracy across diverse scenarios.","Additionally, LI's feature extractor closely matches the performance achieved when aggregating data from all clients.","In global model contexts, employing LI with stacked personalized layers and an additional network also yields comparable results to combined client data scenarios.","Furthermore, LI's adaptability extends to multi-task learning, streamlining the extraction of common features across tasks and obviating the need for simultaneous training.","This approach not only enhances individual task performance but also achieves accuracy levels on par with classic multi-task learning methods where all tasks are trained simultaneously.","LI integrates a loop topology with layer-wise and end-to-end training, compatible with various neural network models.","This paper also delves into the theoretical underpinnings of LI's effectiveness, offering insights into its potential applications.","The code is on https://github.com/axedge1983/LI"],"url":"http://arxiv.org/abs/2403.14371v1","category":"cs.LG"}
{"created":"2024-03-21 12:57:30","title":"SyncTweedies: A General Generative Framework Based on Synchronized Diffusions","abstract":"We introduce a general framework for generating diverse visual content, including ambiguous images, panorama images, mesh textures, and Gaussian splat textures, by synchronizing multiple diffusion processes. We present exhaustive investigation into all possible scenarios for synchronizing multiple diffusion processes through a canonical space and analyze their characteristics across applications. In doing so, we reveal a previously unexplored case: averaging the outputs of Tweedie's formula while conducting denoising in multiple instance spaces. This case also provides the best quality with the widest applicability to downstream tasks. We name this case SyncTweedies. In our experiments generating visual content aforementioned, we demonstrate the superior quality of generation by SyncTweedies compared to other synchronization methods, optimization-based and iterative-update-based methods.","sentences":["We introduce a general framework for generating diverse visual content, including ambiguous images, panorama images, mesh textures, and Gaussian splat textures, by synchronizing multiple diffusion processes.","We present exhaustive investigation into all possible scenarios for synchronizing multiple diffusion processes through a canonical space and analyze their characteristics across applications.","In doing so, we reveal a previously unexplored case: averaging the outputs of Tweedie's formula while conducting denoising in multiple instance spaces.","This case also provides the best quality with the widest applicability to downstream tasks.","We name this case SyncTweedies.","In our experiments generating visual content aforementioned, we demonstrate the superior quality of generation by SyncTweedies compared to other synchronization methods, optimization-based and iterative-update-based methods."],"url":"http://arxiv.org/abs/2403.14370v1","category":"cs.CV"}
{"created":"2024-03-21 12:50:15","title":"Enabling Visual Composition and Animation in Unsupervised Video Generation","abstract":"In this work we propose a novel method for unsupervised controllable video generation. Once trained on a dataset of unannotated videos, at inference our model is capable of both composing scenes of predefined object parts and animating them in a plausible and controlled way. This is achieved by conditioning video generation on a randomly selected subset of local pre-trained self-supervised features during training. We call our model CAGE for visual Composition and Animation for video GEneration. We conduct a series of experiments to demonstrate capabilities of CAGE in various settings. Project website: https://araachie.github.io/cage.","sentences":["In this work we propose a novel method for unsupervised controllable video generation.","Once trained on a dataset of unannotated videos, at inference our model is capable of both composing scenes of predefined object parts and animating them in a plausible and controlled way.","This is achieved by conditioning video generation on a randomly selected subset of local pre-trained self-supervised features during training.","We call our model CAGE for visual Composition and Animation for video GEneration.","We conduct a series of experiments to demonstrate capabilities of CAGE in various settings.","Project website: https://araachie.github.io/cage."],"url":"http://arxiv.org/abs/2403.14368v1","category":"cs.CV"}
{"created":"2024-03-21 12:49:48","title":"Temperature dependence of micelle shape transitions in copolymer solutions: the role of inter-block incompatibility","abstract":"The nature of the transition between worm-like and spherical micelles in block copolymer dispersions varies between systems. In some formulations, heating drives a transition from worms to spheres, while in other systems the same transition is induced by cooling. In addition, a sphere-worm interconversion can be accompanied either by an increase or a decrease in the solvation of the core, even if the direction of the temperature dependence is the same. Here, self-consistent field theory is used to provide a potential explanation of this range of behaviour. Specifically, we show that, within this model, the dependence of the transition on the incompatibility $\\chi_{BS}$ of the solvophobic block B and the solvent S (the parameter most closely related to the temperature) is strongly influenced by the incompatibility $\\chi_{AB}$ between B and the solvophilic block A. When $\\chi_{AB}$ is small ($\\chi_{AB}\\le 0.1$), it is found that increasing $\\chi_{BS}$ produces a transition from worm-like micelles to spheres (or, more generally, from less curved to more curved structures). When $\\chi_{AB}$ is above 0.1, increasing $\\chi_{BS}$ drives the system from spheres to worm-like micelles. Whether a transition is observed within a realistic range of $\\chi_{BS}$ is also found to depend on the fraction of solvophilic material in the copolymer. The relevance of our calculations to experimental results is discussed, and we suggest that the direction of the temperature dependence may be controlled not only by the solution behaviour of the solvophobic block (upper critical solution temperature versus lower critical solution temperature) but also by $\\chi_{AB}$.","sentences":["The nature of the transition between worm-like and spherical micelles in block copolymer dispersions varies between systems.","In some formulations, heating drives a transition from worms to spheres, while in other systems the same transition is induced by cooling.","In addition, a sphere-worm interconversion can be accompanied either by an increase or a decrease in the solvation of the core, even if the direction of the temperature dependence is the same.","Here, self-consistent field theory is used to provide a potential explanation of this range of behaviour.","Specifically, we show that, within this model, the dependence of the transition on the incompatibility $\\chi_{BS}$ of the solvophobic block B and the solvent S (the parameter most closely related to the temperature) is strongly influenced by the incompatibility $\\chi_{AB}$ between B and the solvophilic block A.","When $\\chi_{AB}$ is small ($\\chi_{AB}\\le 0.1$), it is found that increasing $\\chi_{BS}$ produces a transition from worm-like micelles to spheres (or, more generally, from less curved to more curved structures).","When $\\chi_{AB}$ is above 0.1, increasing $\\chi_{BS}$ drives the system from spheres to worm-like micelles.","Whether a transition is observed within a realistic range of $\\chi_{BS}$ is also found to depend on the fraction of solvophilic material in the copolymer.","The relevance of our calculations to experimental results is discussed, and we suggest that the direction of the temperature dependence may be controlled not only by the solution behaviour of the solvophobic block (upper critical solution temperature versus lower critical solution temperature) but also by $\\chi_{AB}$."],"url":"http://arxiv.org/abs/2403.14367v1","category":"cond-mat.soft"}
{"created":"2024-03-21 12:45:01","title":"Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen Domains by Intrinsic Learning from Redundant LLM Semantics","abstract":"Generalized zero-shot learning (GZSL) focuses on recognizing seen and unseen classes against domain shift problem (DSP) where data of unseen classes may be misclassified as seen classes. However, existing GZSL is still limited to seen domains. In the current work, we pioneer cross-domain GZSL (CDGZSL) which addresses GZSL towards unseen domains. Different from existing GZSL methods which alleviate DSP by generating features of unseen classes with semantics, CDGZSL needs to construct a common feature space across domains and acquire the corresponding intrinsic semantics shared among domains to transfer from seen to unseen domains. Considering the information asymmetry problem caused by redundant class semantics annotated with large language models (LLMs), we present Meta Domain Alignment Semantic Refinement (MDASR). Technically, MDASR consists of two parts: Inter-class Similarity Alignment (ISA), which eliminates the non-intrinsic semantics not shared across all domains under the guidance of inter-class feature relationships, and Unseen-class Meta Generation (UMG), which preserves intrinsic semantics to maintain connectivity between seen and unseen classes by simulating feature generation. MDASR effectively aligns the redundant semantic space with the common feature space, mitigating the information asymmetry in CDGZSL. The effectiveness of MDASR is demonstrated on the Office-Home and Mini-DomainNet, and we have shared the LLM-based semantics for these datasets as the benchmark.","sentences":["Generalized zero-shot learning (GZSL) focuses on recognizing seen and unseen classes against domain shift problem (DSP) where data of unseen classes may be misclassified as seen classes.","However, existing GZSL is still limited to seen domains.","In the current work, we pioneer cross-domain GZSL (CDGZSL) which addresses GZSL towards unseen domains.","Different from existing GZSL methods which alleviate DSP by generating features of unseen classes with semantics, CDGZSL needs to construct a common feature space across domains and acquire the corresponding intrinsic semantics shared among domains to transfer from seen to unseen domains.","Considering the information asymmetry problem caused by redundant class semantics annotated with large language models (LLMs), we present Meta Domain Alignment Semantic Refinement (MDASR).","Technically, MDASR consists of two parts: Inter-class Similarity Alignment (ISA), which eliminates the non-intrinsic semantics not shared across all domains under the guidance of inter-class feature relationships, and Unseen-class Meta Generation (UMG), which preserves intrinsic semantics to maintain connectivity between seen and unseen classes by simulating feature generation.","MDASR effectively aligns the redundant semantic space with the common feature space, mitigating the information asymmetry in CDGZSL.","The effectiveness of MDASR is demonstrated on the Office-Home and Mini-DomainNet, and we have shared the LLM-based semantics for these datasets as the benchmark."],"url":"http://arxiv.org/abs/2403.14362v1","category":"cs.CV"}
{"created":"2024-03-21 12:41:18","title":"An Achievability Bound for Variable-Length Stop-Feedback Coding over the Gaussian Channel","abstract":"Feedback holds a pivotal role in practical communication schemes, even though it does not enhance channel capacity. Its main attribute includes adaptability in transmission that allows for a higher rate of convergence of the error probability to zero with respect to blocklength. Motivated by this fact, we present a non-asymptotic achievability bound for variable-length coding with stop-feedback. Specifically, a general achievability bound is derived, that employs a random coding ensemble in combination with minimum distance decoding. The general bound is particularized for the Gaussian channel. Numerical evaluation of the bound confirms the significant value of feedback compared to transmission with fixed blocklength coding and without feedback.","sentences":["Feedback holds a pivotal role in practical communication schemes, even though it does not enhance channel capacity.","Its main attribute includes adaptability in transmission that allows for a higher rate of convergence of the error probability to zero with respect to blocklength.","Motivated by this fact, we present a non-asymptotic achievability bound for variable-length coding with stop-feedback.","Specifically, a general achievability bound is derived, that employs a random coding ensemble in combination with minimum distance decoding.","The general bound is particularized for the Gaussian channel.","Numerical evaluation of the bound confirms the significant value of feedback compared to transmission with fixed blocklength coding and without feedback."],"url":"http://arxiv.org/abs/2403.14360v1","category":"cs.IT"}
{"created":"2024-03-21 12:37:54","title":"Exploring the Potential of Large Language Models in Graph Generation","abstract":"Large language models (LLMs) have achieved great success in many fields, and recent works have studied exploring LLMs for graph discriminative tasks such as node classification. However, the abilities of LLMs for graph generation remain unexplored in the literature. Graph generation requires the LLM to generate graphs with given properties, which has valuable real-world applications such as drug discovery, while tends to be more challenging. In this paper, we propose LLM4GraphGen to explore the ability of LLMs for graph generation with systematical task designs and extensive experiments. Specifically, we propose several tasks tailored with comprehensive experiments to address key questions regarding LLMs' understanding of different graph structure rules, their ability to capture structural type distributions, and their utilization of domain knowledge for property-based graph generation. Our evaluations demonstrate that LLMs, particularly GPT-4, exhibit preliminary abilities in graph generation tasks, including rule-based and distribution-based generation. We also observe that popular prompting methods, such as few-shot and chain-of-thought prompting, do not consistently enhance performance. Besides, LLMs show potential in generating molecules with specific properties. These findings may serve as foundations for designing good LLMs based models for graph generation and provide valuable insights and further research.","sentences":["Large language models (LLMs) have achieved great success in many fields, and recent works have studied exploring LLMs for graph discriminative tasks such as node classification.","However, the abilities of LLMs for graph generation remain unexplored in the literature.","Graph generation requires the LLM to generate graphs with given properties, which has valuable real-world applications such as drug discovery, while tends to be more challenging.","In this paper, we propose LLM4GraphGen to explore the ability of LLMs for graph generation with systematical task designs and extensive experiments.","Specifically, we propose several tasks tailored with comprehensive experiments to address key questions regarding LLMs' understanding of different graph structure rules, their ability to capture structural type distributions, and their utilization of domain knowledge for property-based graph generation.","Our evaluations demonstrate that LLMs, particularly GPT-4, exhibit preliminary abilities in graph generation tasks, including rule-based and distribution-based generation.","We also observe that popular prompting methods, such as few-shot and chain-of-thought prompting, do not consistently enhance performance.","Besides, LLMs show potential in generating molecules with specific properties.","These findings may serve as foundations for designing good LLMs based models for graph generation and provide valuable insights and further research."],"url":"http://arxiv.org/abs/2403.14358v1","category":"cs.LG"}
{"created":"2024-03-21 12:37:12","title":"A generalized notion of convergence of sequences of subspaces in an inner product space via ideals","abstract":"In this paper we introduce the notion of I-convergence of sequences of k-dimensional subspaces of an inner product space, where I is an ideal of subsets of N, the set of all natural numbers and k in N. We also study some basic properties of this notion.","sentences":["In this paper we introduce the notion of I-convergence of sequences of k-dimensional subspaces of an inner product space, where I is an ideal of subsets of N, the set of all natural numbers and k in N. We also study some basic properties of this notion."],"url":"http://arxiv.org/abs/2403.14357v1","category":"math.FA"}
{"created":"2024-03-21 12:35:46","title":"DomainLab: A modular Python package for domain generalization in deep learning","abstract":"Poor generalization performance caused by distribution shifts in unseen domains often hinders the trustworthy deployment of deep neural networks. Many domain generalization techniques address this problem by adding a domain invariant regularization loss terms during training. However, there is a lack of modular software that allows users to combine the advantages of different methods with minimal effort for reproducibility. DomainLab is a modular Python package for training user specified neural networks with composable regularization loss terms. Its decoupled design allows the separation of neural networks from regularization loss construction. Hierarchical combinations of neural networks, different domain generalization methods, and associated hyperparameters, can all be specified together with other experimental setup in a single configuration file. Hierarchical combinations of neural networks, different domain generalization methods, and associated hyperparameters, can all be specified together with other experimental setup in a single configuration file. In addition, DomainLab offers powerful benchmarking functionality to evaluate the generalization performance of neural networks in out-of-distribution data. The package supports running the specified benchmark on an HPC cluster or on a standalone machine. The package is well tested with over 95 percent coverage and well documented. From the user perspective, it is closed to modification but open to extension. The package is under the MIT license, and its source code, tutorial and documentation can be found at https://github.com/marrlab/DomainLab.","sentences":["Poor generalization performance caused by distribution shifts in unseen domains often hinders the trustworthy deployment of deep neural networks.","Many domain generalization techniques address this problem by adding a domain invariant regularization loss terms during training.","However, there is a lack of modular software that allows users to combine the advantages of different methods with minimal effort for reproducibility.","DomainLab is a modular Python package for training user specified neural networks with composable regularization loss terms.","Its decoupled design allows the separation of neural networks from regularization loss construction.","Hierarchical combinations of neural networks, different domain generalization methods, and associated hyperparameters, can all be specified together with other experimental setup in a single configuration file.","Hierarchical combinations of neural networks, different domain generalization methods, and associated hyperparameters, can all be specified together with other experimental setup in a single configuration file.","In addition, DomainLab offers powerful benchmarking functionality to evaluate the generalization performance of neural networks in out-of-distribution data.","The package supports running the specified benchmark on an HPC cluster or on a standalone machine.","The package is well tested with over 95 percent coverage and well documented.","From the user perspective, it is closed to modification but open to extension.","The package is under the MIT license, and its source code, tutorial and documentation can be found at https://github.com/marrlab/DomainLab."],"url":"http://arxiv.org/abs/2403.14356v1","category":"cs.LG"}
{"created":"2024-03-21 12:34:42","title":"The multiplicity of cyclic coverings of a singularity of an algebraic variety","abstract":"Let $V$ be an affine algebraic variety, and let $p\\in V$ be a singular point. For a regular function $g$ on $V$ such that $g(p)=0$ and for a positive integer $n$, we consider the cyclic covering $\\phi_n\\: V_n \\to V$ of degree $n$ branched along the hypersurface defined by $g$. We will prove that for sufficiently large $n$, the tangent cone of $V_n$ at $\\phi_n^{-1}(p)$ is, as an affine variety, the product of the tangent cone of the branch locus and the affine line. In particular, the multiplicity of the singularity $\\phi_n^{-1}(p) \\in V_n$, which is a function of $n$ determined by $V$ and $g$, remains constant for sufficiently large $n$. This result generalizes Tomaru's theorem for normal surface singularities.","sentences":["Let $V$ be an affine algebraic variety, and let $p\\in V$ be a singular point.","For a regular function $g$ on $V$ such that $g(p)=0$ and for a positive integer $n$, we consider the cyclic covering $\\phi_n\\: V_n \\to V$ of degree $n$ branched along the hypersurface defined by $g$. We will prove that for sufficiently large $n$, the tangent cone of $V_n$ at $\\phi_n^{-1}(p)$ is, as an affine variety, the product of the tangent cone of the branch locus and the affine line.","In particular, the multiplicity of the singularity $\\phi_n^{-1}(p) \\in V_n$, which is a function of $n$ determined by $V$ and $g$, remains constant for sufficiently large $n$. This result generalizes Tomaru's theorem for normal surface singularities."],"url":"http://arxiv.org/abs/2403.14355v1","category":"math.AG"}
{"created":"2024-03-21 12:25:17","title":"Annotation-Efficient Polyp Segmentation via Active Learning","abstract":"Deep learning-based techniques have proven effective in polyp segmentation tasks when provided with sufficient pixel-wise labeled data. However, the high cost of manual annotation has created a bottleneck for model generalization. To minimize annotation costs, we propose a deep active learning framework for annotation-efficient polyp segmentation. In practice, we measure the uncertainty of each sample by examining the similarity between features masked by the prediction map of the polyp and the background area. Since the segmentation model tends to perform weak in samples with indistinguishable features of foreground and background areas, uncertainty sampling facilitates the fitting of under-learning data. Furthermore, clustering image-level features weighted by uncertainty identify samples that are both uncertain and representative. To enhance the selectivity of the active selection strategy, we propose a novel unsupervised feature discrepancy learning mechanism. The selection strategy and feature optimization work in tandem to achieve optimal performance with a limited annotation budget. Extensive experimental results have demonstrated that our proposed method achieved state-of-the-art performance compared to other competitors on both a public dataset and a large-scale in-house dataset.","sentences":["Deep learning-based techniques have proven effective in polyp segmentation tasks when provided with sufficient pixel-wise labeled data.","However, the high cost of manual annotation has created a bottleneck for model generalization.","To minimize annotation costs, we propose a deep active learning framework for annotation-efficient polyp segmentation.","In practice, we measure the uncertainty of each sample by examining the similarity between features masked by the prediction map of the polyp and the background area.","Since the segmentation model tends to perform weak in samples with indistinguishable features of foreground and background areas, uncertainty sampling facilitates the fitting of under-learning data.","Furthermore, clustering image-level features weighted by uncertainty identify samples that are both uncertain and representative.","To enhance the selectivity of the active selection strategy, we propose a novel unsupervised feature discrepancy learning mechanism.","The selection strategy and feature optimization work in tandem to achieve optimal performance with a limited annotation budget.","Extensive experimental results have demonstrated that our proposed method achieved state-of-the-art performance compared to other competitors on both a public dataset and a large-scale in-house dataset."],"url":"http://arxiv.org/abs/2403.14350v1","category":"cs.CV"}
{"created":"2024-03-21 12:24:01","title":"A Comparative Study of Real-Time Implementable Cooperative Aerial Manipulation Systems","abstract":"This survey paper focuses on quadrotor- and multirotor- based cooperative aerial manipulation. Emphasis is first given on comparing and evaluating prototype systems that have been implemented and tested in real-time in diverse application environments. Underlying modeling and control approaches are also discussed and compared. The outcome of the survey allows for understanding the motivation and rationale to develop such systems, their applicability and implementability in diverse applications and also challenges that need to be addressed and overcome. Moreover, the survey provides a guide to develop the next generation of prototype systems based on preferred characteristics, functionality, operability and application domain.","sentences":["This survey paper focuses on quadrotor- and multirotor- based cooperative aerial manipulation.","Emphasis is first given on comparing and evaluating prototype systems that have been implemented and tested in real-time in diverse application environments.","Underlying modeling and control approaches are also discussed and compared.","The outcome of the survey allows for understanding the motivation and rationale to develop such systems, their applicability and implementability in diverse applications and also challenges that need to be addressed and overcome.","Moreover, the survey provides a guide to develop the next generation of prototype systems based on preferred characteristics, functionality, operability and application domain."],"url":"http://arxiv.org/abs/2403.14347v1","category":"cs.RO"}
{"created":"2024-03-21 12:23:12","title":"Modem Optimization of High-Mobility Scenarios: A Deep-Learning-Inspired Approach","abstract":"The next generation wireless communication networks are required to support high-mobility scenarios, such as reliable data transmission for high-speed railways. Nevertheless, widely utilized multi-carrier modulation, the orthogonal frequency division multiplex (OFDM), cannot deal with the severe Doppler spread brought by high mobility. To address this problem, some new modulation schemes, e.g. orthogonal time frequency space and affine frequency division multiplexing, have been proposed with different design criteria from OFDM, which promote reliability with the cost of extremely high implementation complexity. On the other hand, end-to-end systems achieve excellent gains by exploiting neural networks to replace traditional transmitters and receivers, but have to retrain and update continually with channel varying. In this paper, we propose the Modem Network (ModNet) to design a novel modem scheme. Compared with end-to-end systems, channels are directly fed into the network and we can directly get a modem scheme through ModNet. Then, the Tri-Phase training strategy is proposed, which mainly utilizes the siamese structure to unify the learned modem scheme without retraining frequently faced up with time-varying channels. Simulation results show the proposed modem scheme outperforms OFDM systems under different highmobility channel statistics.","sentences":["The next generation wireless communication networks are required to support high-mobility scenarios, such as reliable data transmission for high-speed railways.","Nevertheless, widely utilized multi-carrier modulation, the orthogonal frequency division multiplex (OFDM), cannot deal with the severe Doppler spread brought by high mobility.","To address this problem, some new modulation schemes, e.g. orthogonal time frequency space and affine frequency division multiplexing, have been proposed with different design criteria from OFDM, which promote reliability with the cost of extremely high implementation complexity.","On the other hand, end-to-end systems achieve excellent gains by exploiting neural networks to replace traditional transmitters and receivers, but have to retrain and update continually with channel varying.","In this paper, we propose the Modem Network (ModNet) to design a novel modem scheme.","Compared with end-to-end systems, channels are directly fed into the network and we can directly get a modem scheme through ModNet.","Then, the Tri-Phase training strategy is proposed, which mainly utilizes the siamese structure to unify the learned modem scheme without retraining frequently faced up with time-varying channels.","Simulation results show the proposed modem scheme outperforms OFDM systems under different highmobility channel statistics."],"url":"http://arxiv.org/abs/2403.14345v1","category":"eess.SP"}
{"created":"2024-03-21 12:17:59","title":"Beyond Surface Similarity: Detecting Subtle Semantic Shifts in Financial Narratives","abstract":"In this paper, we introduce the Financial-STS task, a financial domain-specific NLP task designed to measure the nuanced semantic similarity between pairs of financial narratives. These narratives originate from the financial statements of the same company but correspond to different periods, such as year-over-year comparisons. Measuring the subtle semantic differences between these paired narratives enables market stakeholders to gauge changes over time in the company's financial and operational situations, which is critical for financial decision-making. We find that existing pretrained embedding models and LLM embeddings fall short in discerning these subtle financial narrative shifts. To address this gap, we propose an LLM-augmented pipeline specifically designed for the Financial-STS task. Evaluation on a human-annotated dataset demonstrates that our proposed method outperforms existing methods trained on classic STS tasks and generic LLM embeddings.","sentences":["In this paper, we introduce the Financial-STS task, a financial domain-specific NLP task designed to measure the nuanced semantic similarity between pairs of financial narratives.","These narratives originate from the financial statements of the same company but correspond to different periods, such as year-over-year comparisons.","Measuring the subtle semantic differences between these paired narratives enables market stakeholders to gauge changes over time in the company's financial and operational situations, which is critical for financial decision-making.","We find that existing pretrained embedding models and LLM embeddings fall short in discerning these subtle financial narrative shifts.","To address this gap, we propose an LLM-augmented pipeline specifically designed for the Financial-STS task.","Evaluation on a human-annotated dataset demonstrates that our proposed method outperforms existing methods trained on classic STS tasks and generic LLM embeddings."],"url":"http://arxiv.org/abs/2403.14341v1","category":"cs.CL"}
{"created":"2024-03-21 12:14:02","title":"Exploring Task Unification in Graph Representation Learning via Generative Approach","abstract":"Graphs are ubiquitous in real-world scenarios and encompass a diverse range of tasks, from node-, edge-, and graph-level tasks to transfer learning. However, designing specific tasks for each type of graph data is often costly and lacks generalizability. Recent endeavors under the \"Pre-training + Fine-tuning\" or \"Pre-training + Prompt\" paradigms aim to design a unified framework capable of generalizing across multiple graph tasks. Among these, graph autoencoders (GAEs), generative self-supervised models, have demonstrated their potential in effectively addressing various graph tasks. Nevertheless, these methods typically employ multi-stage training and require adaptive designs, which on one hand make it difficult to be seamlessly applied to diverse graph tasks and on the other hand overlook the negative impact caused by discrepancies in task objectives between the different stages. To address these challenges, we propose GA^2E, a unified adversarially masked autoencoder capable of addressing the above challenges seamlessly. Specifically, GA^2E proposes to use the subgraph as the meta-structure, which remains consistent across all graph tasks (ranging from node-, edge-, and graph-level to transfer learning) and all stages (both during training and inference). Further, GA^2E operates in a \\textbf{\"Generate then Discriminate\"} manner. It leverages the masked GAE to reconstruct the input subgraph whilst treating it as a generator to compel the reconstructed graphs resemble the input subgraph. Furthermore, GA^2E introduces an auxiliary discriminator to discern the authenticity between the reconstructed (generated) subgraph and the input subgraph, thus ensuring the robustness of the graph representation through adversarial training mechanisms. We validate GA^2E's capabilities through extensive experiments on 21 datasets across four types of graph tasks.","sentences":["Graphs are ubiquitous in real-world scenarios and encompass a diverse range of tasks, from node-, edge-, and graph-level tasks to transfer learning.","However, designing specific tasks for each type of graph data is often costly and lacks generalizability.","Recent endeavors under the \"Pre-training + Fine-tuning\" or \"Pre-training + Prompt\" paradigms aim to design a unified framework capable of generalizing across multiple graph tasks.","Among these, graph autoencoders (GAEs), generative self-supervised models, have demonstrated their potential in effectively addressing various graph tasks.","Nevertheless, these methods typically employ multi-stage training and require adaptive designs, which on one hand make it difficult to be seamlessly applied to diverse graph tasks and on the other hand overlook the negative impact caused by discrepancies in task objectives between the different stages.","To address these challenges, we propose GA^2E, a unified adversarially masked autoencoder capable of addressing the above challenges seamlessly.","Specifically, GA^2E proposes to use the subgraph as the meta-structure, which remains consistent across all graph tasks (ranging from node-, edge-, and graph-level to transfer learning) and all stages (both during training and inference).","Further, GA^2E operates in a \\textbf{\"Generate then Discriminate\"} manner.","It leverages the masked GAE to reconstruct the input subgraph whilst treating it as a generator to compel the reconstructed graphs resemble the input subgraph.","Furthermore, GA^2E introduces an auxiliary discriminator to discern the authenticity between the reconstructed (generated) subgraph and the input subgraph, thus ensuring the robustness of the graph representation through adversarial training mechanisms.","We validate GA^2E's capabilities through extensive experiments on 21 datasets across four types of graph tasks."],"url":"http://arxiv.org/abs/2403.14340v1","category":"cs.LG"}
{"created":"2024-03-21 12:11:26","title":"$\\nabla \u03c4$: Gradient-based and Task-Agnostic machine Unlearning","abstract":"Machine Unlearning, the process of selectively eliminating the influence of certain data examples used during a model's training, has gained significant attention as a means for practitioners to comply with recent data protection regulations. However, existing unlearning methods face critical drawbacks, including their prohibitively high cost, often associated with a large number of hyperparameters, and the limitation of forgetting only relatively small data portions. This often makes retraining the model from scratch a quicker and more effective solution. In this study, we introduce Gradient-based and Task-Agnostic machine Unlearning ($\\nabla \\tau$), an optimization framework designed to remove the influence of a subset of training data efficiently. It applies adaptive gradient ascent to the data to be forgotten while using standard gradient descent for the remaining data. $\\nabla \\tau$ offers multiple benefits over existing approaches. It enables the unlearning of large sections of the training dataset (up to 30%). It is versatile, supporting various unlearning tasks (such as subset forgetting or class removal) and applicable across different domains (images, text, etc.). Importantly, $\\nabla \\tau$ requires no hyperparameter adjustments, making it a more appealing option than retraining the model from scratch. We evaluate our framework's effectiveness using a set of well-established Membership Inference Attack metrics, demonstrating up to 10% enhancements in performance compared to state-of-the-art methods without compromising the original model's accuracy.","sentences":["Machine Unlearning, the process of selectively eliminating the influence of certain data examples used during a model's training, has gained significant attention as a means for practitioners to comply with recent data protection regulations.","However, existing unlearning methods face critical drawbacks, including their prohibitively high cost, often associated with a large number of hyperparameters, and the limitation of forgetting only relatively small data portions.","This often makes retraining the model from scratch a quicker and more effective solution.","In this study, we introduce Gradient-based and Task-Agnostic machine Unlearning ($\\nabla \\tau$), an optimization framework designed to remove the influence of a subset of training data efficiently.","It applies adaptive gradient ascent to the data to be forgotten while using standard gradient descent for the remaining data.","$\\nabla \\tau$ offers multiple benefits over existing approaches.","It enables the unlearning of large sections of the training dataset (up to 30%).","It is versatile, supporting various unlearning tasks (such as subset forgetting or class removal) and applicable across different domains (images, text, etc.).","Importantly, $\\nabla \\tau$ requires no hyperparameter adjustments, making it a more appealing option than retraining the model from scratch.","We evaluate our framework's effectiveness using a set of well-established Membership Inference Attack metrics, demonstrating up to 10% enhancements in performance compared to state-of-the-art methods without compromising the original model's accuracy."],"url":"http://arxiv.org/abs/2403.14339v1","category":"cs.LG"}
{"created":"2024-03-21 12:05:58","title":"Optimal Floquet Engineering for Large Scale Atom Interferometers","abstract":"The effective control of atomic coherence with cold atoms has made atom interferometry an essential tool for quantum sensors and precision measurements. The performance of these interferometers is closely related to the operation of large wave packet separations. We present here a novel approach for atomic beam splitters based on the stroboscopic stabilization of quantum states in an accelerated optical lattice. The corresponding Floquet state is generated by optimal control protocols. In this way, we demonstrate an unprecedented Large Momentum Transfer (LMT) interferometer, with a momentum separation of 600 photon recoils ($600\\hbar k$) between its two arms. Each LMT beam splitter is realized in a remarkably short time (2 ms) and is highly robust against the initial velocity dispersion of the wave packet and lattice depth fluctuations. Our study shows that Floquet engineering is a promising tool for exploring new frontiers in quantum physics at large scales, with applications in quantum sensing and testing fundamental physics.","sentences":["The effective control of atomic coherence with cold atoms has made atom interferometry an essential tool for quantum sensors and precision measurements.","The performance of these interferometers is closely related to the operation of large wave packet separations.","We present here a novel approach for atomic beam splitters based on the stroboscopic stabilization of quantum states in an accelerated optical lattice.","The corresponding Floquet state is generated by optimal control protocols.","In this way, we demonstrate an unprecedented Large Momentum Transfer (LMT) interferometer, with a momentum separation of 600 photon recoils ($600\\hbar k$) between its two arms.","Each LMT beam splitter is realized in a remarkably short time (2 ms) and is highly robust against the initial velocity dispersion of the wave packet and lattice depth fluctuations.","Our study shows that Floquet engineering is a promising tool for exploring new frontiers in quantum physics at large scales, with applications in quantum sensing and testing fundamental physics."],"url":"http://arxiv.org/abs/2403.14337v1","category":"physics.atom-ph"}
{"created":"2024-03-21 11:59:10","title":"New bounds for normal approximation on product spaces with applications to monochromatic edges, random sums and an infinite de Jong CLT","abstract":"We extend the Malliavin theory for $L^2$-functionals on product probability spaces that has recently been developed by Decreusefond and Halconruy (2019) and by Duerinckx (2021), by characterizing the domains and investigating the actions of the three Malliavin operators in terms of the infinite Hoeffding decomposition in $L^2$, which we identify as the natural analogue of the famous Wiener-It\\^{o} chaos decomposition on Gaussian and Poisson spaces. We further combine this theory with Stein's method for normal approximation in order to provide three different types of abstract Berry-Esseen and Wasserstein bounds: a) Malliavin-Stein bounds involving the Malliavin gradient $D$ and the pseudo-inverse of the Ornstein-Uhlenbeck generator $L$, b) bounds featuring the carr\\'{e}-du-champ operator $\\Gamma$ and c) bounds making use of a Clark-Ocone type integration-by-parts formula. To demonstrate the flexibility of these abstract bounds, we derive quantitative central limit theorems for the number of monochromatic edges in a uniform random coloring of a graph sequence as well as for random sums and prove an infinite version of the quantitative de Jong CLT that has recently been proved by G. Peccati and the author (2017) and by the author (2023). As a further theoretical application, we deduce new abstract Berry-Esseen and Wasserstein bounds for functionals of a general independent Rademacher sequence.","sentences":["We extend the Malliavin theory for $L^2$-functionals on product probability spaces that has recently been developed by Decreusefond and Halconruy (2019) and by Duerinckx (2021), by characterizing the domains and investigating the actions of the three Malliavin operators in terms of the infinite Hoeffding decomposition in $L^2$, which we identify as the natural analogue of the famous Wiener-It\\^{o} chaos decomposition on Gaussian and Poisson spaces.","We further combine this theory with Stein's method for normal approximation in order to provide three different types of abstract Berry-Esseen and Wasserstein bounds: a) Malliavin-Stein bounds involving the Malliavin gradient $D$ and the pseudo-inverse of the Ornstein-Uhlenbeck generator $L$, b) bounds featuring the carr\\'{e}-du-champ operator $\\Gamma$ and c) bounds making use of a Clark-Ocone type integration-by-parts formula.","To demonstrate the flexibility of these abstract bounds, we derive quantitative central limit theorems for the number of monochromatic edges in a uniform random coloring of a graph sequence as well as for random sums and prove an infinite version of the quantitative de Jong CLT that has recently been proved by G. Peccati and the author (2017) and by the author (2023).","As a further theoretical application, we deduce new abstract Berry-Esseen and Wasserstein bounds for functionals of a general independent Rademacher sequence."],"url":"http://arxiv.org/abs/2403.14334v1","category":"math.PR"}
{"created":"2024-03-21 11:58:50","title":"CFPL-FAS: Class Free Prompt Learning for Generalizable Face Anti-spoofing","abstract":"Domain generalization (DG) based Face Anti-Spoofing (FAS) aims to improve the model's performance on unseen domains. Existing methods either rely on domain labels to align domain-invariant feature spaces, or disentangle generalizable features from the whole sample, which inevitably lead to the distortion of semantic feature structures and achieve limited generalization. In this work, we make use of large-scale VLMs like CLIP and leverage the textual feature to dynamically adjust the classifier's weights for exploring generalizable visual features. Specifically, we propose a novel Class Free Prompt Learning (CFPL) paradigm for DG FAS, which utilizes two lightweight transformers, namely Content Q-Former (CQF) and Style Q-Former (SQF), to learn the different semantic prompts conditioned on content and style features by using a set of learnable query vectors, respectively. Thus, the generalizable prompt can be learned by two improvements: (1) A Prompt-Text Matched (PTM) supervision is introduced to ensure CQF learns visual representation that is most informative of the content description. (2) A Diversified Style Prompt (DSP) technology is proposed to diversify the learning of style prompts by mixing feature statistics between instance-specific styles. Finally, the learned text features modulate visual features to generalization through the designed Prompt Modulation (PM). Extensive experiments show that the CFPL is effective and outperforms the state-of-the-art methods on several cross-domain datasets.","sentences":["Domain generalization (DG) based Face Anti-Spoofing (FAS) aims to improve the model's performance on unseen domains.","Existing methods either rely on domain labels to align domain-invariant feature spaces, or disentangle generalizable features from the whole sample, which inevitably lead to the distortion of semantic feature structures and achieve limited generalization.","In this work, we make use of large-scale VLMs like CLIP and leverage the textual feature to dynamically adjust the classifier's weights for exploring generalizable visual features.","Specifically, we propose a novel Class Free Prompt Learning (CFPL) paradigm for DG FAS, which utilizes two lightweight transformers, namely Content Q-Former (CQF) and Style Q-Former (SQF), to learn the different semantic prompts conditioned on content and style features by using a set of learnable query vectors, respectively.","Thus, the generalizable prompt can be learned by two improvements: (1) A Prompt-Text Matched (PTM) supervision is introduced to ensure CQF learns visual representation that is most informative of the content description.","(2) A Diversified Style Prompt (DSP) technology is proposed to diversify the learning of style prompts by mixing feature statistics between instance-specific styles.","Finally, the learned text features modulate visual features to generalization through the designed Prompt Modulation (PM).","Extensive experiments show that the CFPL is effective and outperforms the state-of-the-art methods on several cross-domain datasets."],"url":"http://arxiv.org/abs/2403.14333v1","category":"cs.CV"}
{"created":"2024-03-21 11:56:39","title":"Constraining scalars of $16_H$ through proton decays in non-renormalisable $SO(10)$ models","abstract":"Non-renormalisable versions of $SO(10)$ based on irreducible representations with lesser degrees of freedom, are free of running into the catastrophe of non-perturbativity of standard model gauge couplings in contrast to the renormalisable versions having tensors with many degrees of freedom. $16_H$ is the smallest representation, participates in Yukawa Lagrangian at the non-renormalisable level, contributing to the charged and neutral fermion masses, and has six distinct scalars with different $B-L$ charges. We computed the leptoquark and diquark couplings of different pairs of scalars stemming from all possible decomposition of the term resulting from the coupling of $16_{\\rm{H}}$ with the $\\mathbf{16}$ dimensional fermion multiplet of $SO(10)$, i.e. $\\frac{\\mathbf{16}\\,\\mathbf{16}\\,16_{\\rm{H}}\\,16_{\\rm{H}}}{\\Lambda}$. Computing the tree and loop level contribution of different pairs to the effective dimension six, $B-L$ conserving operators, it turns out only three pairs, viz $\\sigma\\big(1,1,0\\big)- T\\big(3,1,\\frac{1}{3}\\big)$, and $H\\big(1,2,-\\frac{1}{2}\\big)-\\Delta\\big(3,2,\\frac{1}{6}\\big)$, and $H-T$ can induce proton decay at tree level. Assuming that the Yukawa couplings of the $16_{\\rm{H}}$ are comparable to those of the $\\overline{126}_{\\rm{H}}$ of a realistic SO(10) model and setting the cutoff scale to the Planck scale typically constrains the $B-L$ breaking scale to be $4\\sim 5$ orders of magnitude less than the cutoff scale $(\\Lambda)$. Moreover, analysing the branching pattern of the leading two-body decay modes of the proton, we observed a preference for the proton to decay into second-generation mesons due to the hierarchical nature of Yukawa couplings. In a realistic $SO(10)$ scenario, we find that $M_T >10^{8}$ TeV, while $M_\\Delta$ could be as light as a few TeV$s$.","sentences":["Non-renormalisable versions of $SO(10)$ based on irreducible representations with lesser degrees of freedom, are free of running into the catastrophe of non-perturbativity of standard model gauge couplings in contrast to the renormalisable versions having tensors with many degrees of freedom.","$16_H$ is the smallest representation, participates in Yukawa Lagrangian at the non-renormalisable level, contributing to the charged and neutral fermion masses, and has six distinct scalars with different $B-L$ charges.","We computed the leptoquark and diquark couplings of different pairs of scalars stemming from all possible decomposition of the term resulting from the coupling of $16_{\\rm{H}}$ with the $\\mathbf{16}$ dimensional fermion multiplet of $SO(10)$, i.e. $\\frac{\\mathbf{16}\\,\\mathbf{16}\\,16_{\\rm{H}}\\,16_{\\rm{H}}}{\\Lambda}$. Computing the tree and loop level contribution of different pairs to the effective dimension six, $B-L$ conserving operators, it turns out only three pairs, viz $\\sigma\\big(1,1,0\\big)- T\\big(3,1,\\frac{1}{3}\\big)$, and $H\\big(1,2,-\\frac{1}{2}\\big)-\\Delta\\big(3,2,\\frac{1}{6}\\big)$, and $H-T$ can induce proton decay at tree level.","Assuming that the Yukawa couplings of the $16_{\\rm{H}}$ are comparable to those of the $\\overline{126}_{\\rm{H}}$ of a realistic SO(10) model and setting the cutoff scale to the Planck scale typically constrains the $B-L$ breaking scale to be $4\\sim 5$ orders of magnitude less than the cutoff scale $(\\Lambda)$. Moreover, analysing the branching pattern of the leading two-body decay modes of the proton, we observed a preference for the proton to decay into second-generation mesons due to the hierarchical nature of Yukawa couplings.","In a realistic $SO(10)$ scenario, we find that $M_T >10^{8}$ TeV, while $M_\\Delta$ could be as light as a few TeV$s$."],"url":"http://arxiv.org/abs/2403.14331v1","category":"hep-ph"}
{"created":"2024-03-21 11:54:45","title":"Distilling Reinforcement Learning Policies for Interpretable Robot Locomotion: Gradient Boosting Machines and Symbolic Regression","abstract":"Recent advancements in reinforcement learning (RL) have led to remarkable achievements in robot locomotion capabilities. However, the complexity and ``black-box'' nature of neural network-based RL policies hinder their interpretability and broader acceptance, particularly in applications demanding high levels of safety and reliability. This paper introduces a novel approach to distill neural RL policies into more interpretable forms using Gradient Boosting Machines (GBMs), Explainable Boosting Machines (EBMs) and Symbolic Regression. By leveraging the inherent interpretability of generalized additive models, decision trees, and analytical expressions, we transform opaque neural network policies into more transparent ``glass-box'' models. We train expert neural network policies using RL and subsequently distill them into (i) GBMs, (ii) EBMs, and (iii) symbolic policies. To address the inherent distribution shift challenge of behavioral cloning, we propose to use the Dataset Aggregation (DAgger) algorithm with a curriculum of episode-dependent alternation of actions between expert and distilled policies, to enable efficient distillation of feedback control policies. We evaluate our approach on various robot locomotion gaits -- walking, trotting, bounding, and pacing -- and study the importance of different observations in joint actions for distilled policies using various methods. We train neural expert policies for 205 hours of simulated experience and distill interpretable policies with only 10 minutes of simulated interaction for each gait using the proposed method.","sentences":["Recent advancements in reinforcement learning (RL) have led to remarkable achievements in robot locomotion capabilities.","However, the complexity and ``black-box'' nature of neural network-based RL policies hinder their interpretability and broader acceptance, particularly in applications demanding high levels of safety and reliability.","This paper introduces a novel approach to distill neural RL policies into more interpretable forms using Gradient Boosting Machines (GBMs), Explainable Boosting Machines (EBMs) and Symbolic Regression.","By leveraging the inherent interpretability of generalized additive models, decision trees, and analytical expressions, we transform opaque neural network policies into more transparent ``glass-box'' models.","We train expert neural network policies using RL and subsequently distill them into (i) GBMs, (ii) EBMs, and (iii) symbolic policies.","To address the inherent distribution shift challenge of behavioral cloning, we propose to use the Dataset Aggregation (DAgger) algorithm with a curriculum of episode-dependent alternation of actions between expert and distilled policies, to enable efficient distillation of feedback control policies.","We evaluate our approach on various robot locomotion gaits -- walking, trotting, bounding, and pacing -- and study the importance of different observations in joint actions for distilled policies using various methods.","We train neural expert policies for 205 hours of simulated experience and distill interpretable policies with only 10 minutes of simulated interaction for each gait using the proposed method."],"url":"http://arxiv.org/abs/2403.14328v1","category":"cs.RO"}
{"created":"2024-03-21 11:43:28","title":"Continuous dependence on data for linear evolution PDEs on the quarter-plane","abstract":"In this note, we announce a systematic analysis of continuous dependence on the data in classical spaces for the initial-boundary-value problem of the diffusion equation on the half-line, with data that are not necessarily compatible at the quadrant corner. This is based on a recent approach to rigorously analyzing integral representations derived via the unified transform method of Fokas. No exotic phenomena were discovered in this case, yet our findings appear to be new in the pertinent literature. These results supplement our previous investigations on existence and (non)uniqueness within the framework of well-posedness. The present detailed exposition elucidates the subtleties involved while also demonstrating a generic technique. Applications of the latter to several other IBVPs and PDEs will be reported elsewhere.","sentences":["In this note, we announce a systematic analysis of continuous dependence on the data in classical spaces for the initial-boundary-value problem of the diffusion equation on the half-line, with data that are not necessarily compatible at the quadrant corner.","This is based on a recent approach to rigorously analyzing integral representations derived via the unified transform method of Fokas.","No exotic phenomena were discovered in this case, yet our findings appear to be new in the pertinent literature.","These results supplement our previous investigations on existence and (non)uniqueness within the framework of well-posedness.","The present detailed exposition elucidates the subtleties involved while also demonstrating a generic technique.","Applications of the latter to several other IBVPs and PDEs will be reported elsewhere."],"url":"http://arxiv.org/abs/2403.14323v1","category":"math.AP"}
{"created":"2024-03-21 11:42:49","title":"Ab-initio Van der Waals electrodynamics: polaritons and electron scattering from plasmons and phonons in BN-capped graphene","abstract":"Plasmons and polar phonons are elementary electrodynamic excitations of matter. In 2d and at long wavelengths, they couple to light and act as the system polaritons. They also dictate the scattering of charged carriers. Van der Waals heterostructures offer the opportunity to couple excitations from different layers via long-range Coulomb interactions, modifying both their dispersion and their scattering of electrons. Even when the excitations do not couple, they are still influenced by the screening from all layers, leading to complex dynamical interactions between electrons, plasmons and polar phonons. We develop an efficient ab initio model to solve the dynamical electric response of Van der Waals heterostructures, accompanied by a formalism to extract relevant spectroscopic and transport quantities. Notably, we obtain scattering rates for electrons of the heterostructure coupling remotely with electrodynamic excitations. We apply those developments to BN-capped graphene, in which polar phonons from BN couple to plasmons in graphene. We study the nature of the coupled excitations, their dispersion and their coupling to graphene's electrons. Regimes driven by either phonons or plasmons are identified, as well as a truly hybrid regime at long wavelengths. Those are studied as a function of the graphene's Fermi level and the number of BN layers. In contrast with descriptions in terms of surface-optical phonons, we find that the electron-phonon interaction stems from different modes. Moreover, the dynamical screening of the coupling between BN's LO phonons and graphene's electrons crosses over from inefficient to metal-like depending on the relative value of the phonons' frequency and the energetic onset of interband transitions. While the coupling is significant in general, the associated scattering of graphene's carriers is found to be negligible in the context of electronic transport.","sentences":["Plasmons and polar phonons are elementary electrodynamic excitations of matter.","In 2d and at long wavelengths, they couple to light and act as the system polaritons.","They also dictate the scattering of charged carriers.","Van der Waals heterostructures offer the opportunity to couple excitations from different layers via long-range Coulomb interactions, modifying both their dispersion and their scattering of electrons.","Even when the excitations do not couple, they are still influenced by the screening from all layers, leading to complex dynamical interactions between electrons, plasmons and polar phonons.","We develop an efficient ab initio model to solve the dynamical electric response of Van der Waals heterostructures, accompanied by a formalism to extract relevant spectroscopic and transport quantities.","Notably, we obtain scattering rates for electrons of the heterostructure coupling remotely with electrodynamic excitations.","We apply those developments to BN-capped graphene, in which polar phonons from BN couple to plasmons in graphene.","We study the nature of the coupled excitations, their dispersion and their coupling to graphene's electrons.","Regimes driven by either phonons or plasmons are identified, as well as a truly hybrid regime at long wavelengths.","Those are studied as a function of the graphene's Fermi level and the number of BN layers.","In contrast with descriptions in terms of surface-optical phonons, we find that the electron-phonon interaction stems from different modes.","Moreover, the dynamical screening of the coupling between BN's LO phonons and graphene's electrons crosses over from inefficient to metal-like depending on the relative value of the phonons' frequency and the energetic onset of interband transitions.","While the coupling is significant in general, the associated scattering of graphene's carriers is found to be negligible in the context of electronic transport."],"url":"http://arxiv.org/abs/2403.14322v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-21 11:41:39","title":"Exosense: A Vision-Centric Scene Understanding System For Safe Exoskeleton Navigation","abstract":"Exoskeletons for daily use by those with mobility impairments are being developed. They will require accurate and robust scene understanding systems. Current research has used vision to identify immediate terrain and geometric obstacles, however these approaches are constrained to detections directly in front of the user and are limited to classifying a finite range of terrain types (e.g., stairs, ramps and level-ground). This paper presents Exosense, a vision-centric scene understanding system which is capable of generating rich, globally-consistent elevation maps, incorporating both semantic and terrain traversability information. It features an elastic Atlas mapping framework associated with a visual SLAM pose graph, embedded with open-vocabulary room labels from a Vision-Language Model (VLM). The device's design includes a wide field-of-view (FoV) fisheye multi-camera system to mitigate the challenges introduced by the exoskeleton walking pattern. We demonstrate the system's robustness to the challenges of typical periodic walking gaits, and its ability to construct accurate semantically-rich maps in indoor settings. Additionally, we showcase its potential for motion planning -- providing a step towards safe navigation for exoskeletons.","sentences":["Exoskeletons for daily use by those with mobility impairments are being developed.","They will require accurate and robust scene understanding systems.","Current research has used vision to identify immediate terrain and geometric obstacles, however these approaches are constrained to detections directly in front of the user and are limited to classifying a finite range of terrain types (e.g., stairs, ramps and level-ground).","This paper presents Exosense, a vision-centric scene understanding system which is capable of generating rich, globally-consistent elevation maps, incorporating both semantic and terrain traversability information.","It features an elastic Atlas mapping framework associated with a visual SLAM pose graph, embedded with open-vocabulary room labels from a Vision-Language Model (VLM).","The device's design includes a wide field-of-view (FoV) fisheye multi-camera system to mitigate the challenges introduced by the exoskeleton walking pattern.","We demonstrate the system's robustness to the challenges of typical periodic walking gaits, and its ability to construct accurate semantically-rich maps in indoor settings.","Additionally, we showcase its potential for motion planning -- providing a step towards safe navigation for exoskeletons."],"url":"http://arxiv.org/abs/2403.14320v1","category":"cs.RO"}
{"created":"2024-03-21 11:40:51","title":"A Lightweight Attention-based Deep Network via Multi-Scale Feature Fusion for Multi-View Facial Expression Recognition","abstract":"Convolutional neural networks (CNNs) and their variations have shown effectiveness in facial expression recognition (FER). However, they face challenges when dealing with high computational complexity and multi-view head poses in real-world scenarios. We introduce a lightweight attentional network incorporating multi-scale feature fusion (LANMSFF) to tackle these issues. For the first challenge, we have carefully designed a lightweight fully convolutional network (FCN). We address the second challenge by presenting two novel components, namely mass attention (MassAtt) and point wise feature selection (PWFS) blocks. The MassAtt block simultaneously generates channel and spatial attention maps to recalibrate feature maps by emphasizing important features while suppressing irrelevant ones. On the other hand, the PWFS block employs a feature selection mechanism that discards less meaningful features prior to the fusion process. This mechanism distinguishes it from previous methods that directly fuse multi-scale features. Our proposed approach achieved results comparable to state-of-the-art methods in terms of parameter counts and robustness to pose variation, with accuracy rates of 90.77% on KDEF, 70.44% on FER-2013, and 86.96% on FERPlus datasets. The code for LANMSFF is available at https://github.com/AE-1129/LANMSFF.","sentences":["Convolutional neural networks (CNNs) and their variations have shown effectiveness in facial expression recognition (FER).","However, they face challenges when dealing with high computational complexity and multi-view head poses in real-world scenarios.","We introduce a lightweight attentional network incorporating multi-scale feature fusion (LANMSFF) to tackle these issues.","For the first challenge, we have carefully designed a lightweight fully convolutional network (FCN).","We address the second challenge by presenting two novel components, namely mass attention (MassAtt) and point wise feature selection (PWFS) blocks.","The MassAtt block simultaneously generates channel and spatial attention maps to recalibrate feature maps by emphasizing important features while suppressing irrelevant ones.","On the other hand, the PWFS block employs a feature selection mechanism that discards less meaningful features prior to the fusion process.","This mechanism distinguishes it from previous methods that directly fuse multi-scale features.","Our proposed approach achieved results comparable to state-of-the-art methods in terms of parameter counts and robustness to pose variation, with accuracy rates of 90.77% on KDEF, 70.44% on FER-2013, and 86.96% on FERPlus datasets.","The code for LANMSFF is available at https://github.com/AE-1129/LANMSFF."],"url":"http://arxiv.org/abs/2403.14318v1","category":"cs.CV"}
{"created":"2024-03-21 11:34:26","title":"ChainLM: Empowering Large Language Models with Improved Chain-of-Thought Prompting","abstract":"Chain-of-Thought (CoT) prompting can enhance the reasoning capabilities of large language models (LLMs), establishing itself as a primary approach to solving complex reasoning tasks. Existing CoT synthesis approaches usually focus on simpler reasoning tasks and thus result in low-quality and inconsistent CoT prompts. In response to this challenge, we present an empirical investigation of CoT prompting and introduce CoTGenius, a novel framework designed for the automatic generation of superior CoT prompts. CoTGenius is developed based on three major evolution strategies, i.e., complicate, diversify, and specify-alongside two filtering mechanisms: evolutionary success judgement and correctness verification. We further employ CoTGenius to create an extensive CoT dataset, and subsequently fine-tune the Llama 2-Chat 7B and 13B models on this dataset. We call the resulting model ChainLM. To deal with the cumulative error issue in reasoning steps, we propose a step-level debating method, wherein multiple debaters discuss each reasoning step to arrive at the correct answer. Extensive experiments demonstrate that our ChainLM models exhibit enhanced proficiency in addressing a spectrum of complex reasoning problems compared to existing models. In addition, we conduct an in-depth analysis of the impact of data categories within CoTGenius on the model performance. We release our dataset and code at https://github.com/RUCAIBox/ChainLM.","sentences":["Chain-of-Thought (CoT) prompting can enhance the reasoning capabilities of large language models (LLMs), establishing itself as a primary approach to solving complex reasoning tasks.","Existing CoT synthesis approaches usually focus on simpler reasoning tasks and thus result in low-quality and inconsistent CoT prompts.","In response to this challenge, we present an empirical investigation of CoT prompting and introduce CoTGenius, a novel framework designed for the automatic generation of superior CoT prompts.","CoTGenius is developed based on three major evolution strategies, i.e., complicate, diversify, and specify-alongside two filtering mechanisms: evolutionary success judgement and correctness verification.","We further employ CoTGenius to create an extensive CoT dataset, and subsequently fine-tune the Llama 2-Chat 7B and 13B models on this dataset.","We call the resulting model ChainLM.","To deal with the cumulative error issue in reasoning steps, we propose a step-level debating method, wherein multiple debaters discuss each reasoning step to arrive at the correct answer.","Extensive experiments demonstrate that our ChainLM models exhibit enhanced proficiency in addressing a spectrum of complex reasoning problems compared to existing models.","In addition, we conduct an in-depth analysis of the impact of data categories within CoTGenius on the model performance.","We release our dataset and code at https://github.com/RUCAIBox/ChainLM."],"url":"http://arxiv.org/abs/2403.14312v1","category":"cs.CL"}
{"created":"2024-03-21 11:32:10","title":"Transformation-Free Fixed-Structure Model Reduction for LPV Systems","abstract":"In this paper, we propose a model reduction technique for linear parameter varying (LPV) systems based on available tools for fixed-structure controller synthesis. We start by transforming a model reduction problem into an equivalent controller synthesis problem by defining an appropriate generalized plant. The controller synthesis problem is then solved by using gradient-based tools available in the literature. Owing to the flexibility of the gradient-based synthesis tools, we are able to impose a desired structure on the obtained reduced model. Additionally, we obtain a bound on the approximation error as a direct output of the optimization problem. The proposed methods are applied on a benchmark mechanical system of interconnected masses, springs and dampers. To evaluate the effect of the proposed model-reduction approach on controller design, LPV controllers designed using the reduced models (with and without an imposed structure) are compared in closed-loop with the original model.","sentences":["In this paper, we propose a model reduction technique for linear parameter varying (LPV) systems based on available tools for fixed-structure controller synthesis.","We start by transforming a model reduction problem into an equivalent controller synthesis problem by defining an appropriate generalized plant.","The controller synthesis problem is then solved by using gradient-based tools available in the literature.","Owing to the flexibility of the gradient-based synthesis tools, we are able to impose a desired structure on the obtained reduced model.","Additionally, we obtain a bound on the approximation error as a direct output of the optimization problem.","The proposed methods are applied on a benchmark mechanical system of interconnected masses, springs and dampers.","To evaluate the effect of the proposed model-reduction approach on controller design, LPV controllers designed using the reduced models (with and without an imposed structure) are compared in closed-loop with the original model."],"url":"http://arxiv.org/abs/2403.14310v1","category":"eess.SY"}
{"created":"2024-03-21 11:30:57","title":"Probing modified Hawking evaporation with gravitational waves from the primordial black hole dominated universe","abstract":"It has been recently proposed that Hawking evaporation might slow down after a black hole has lost about half of its mass. Such an effect, called \"memory burden\", is parameterized as a suppression in the mass loss rate by negative powers $n$ of the black hole entropy and could considerably extend the lifetime of a black hole. We study the impact of memory burden on the Primordial Black Hole (PBH) reheating scenario. Modified PBH evaporation leads to a significantly longer PBH dominated stage. Requiring that PBHs evaporate prior enough to Big Bang Nucleosynthesis shrinks the allowed PBH mass range. Indeed, we find that for $n>2.5$ the PBH reheating scenario is not viable. The frequency of the Gravitational Waves (GWs) induced by PBH number density fluctuations is bound to be larger than about a Hz, while the amplitude of the GW spectrum is enhanced due to the longer PBH dominated phase. Interestingly, we show that, in some models, the slope of the induced GW spectrum might be sensitive to the modifications to Hawking evaporation, proving it may be possible to test the \"memory burden\" effect via induced GWs. Lastly, we argue that our results could also apply to general modifications of Hawking evaporation.","sentences":["It has been recently proposed that Hawking evaporation might slow down after a black hole has lost about half of its mass.","Such an effect, called \"memory burden\", is parameterized as a suppression in the mass loss rate by negative powers $n$ of the black hole entropy and could considerably extend the lifetime of a black hole.","We study the impact of memory burden on the Primordial Black Hole (PBH) reheating scenario.","Modified PBH evaporation leads to a significantly longer PBH dominated stage.","Requiring that PBHs evaporate prior enough to Big Bang Nucleosynthesis shrinks the allowed PBH mass range.","Indeed, we find that for $n>2.5$ the PBH reheating scenario is not viable.","The frequency of the Gravitational Waves (GWs) induced by PBH number density fluctuations is bound to be larger than about a Hz, while the amplitude of the GW spectrum is enhanced due to the longer PBH dominated phase.","Interestingly, we show that, in some models, the slope of the induced GW spectrum might be sensitive to the modifications to Hawking evaporation, proving it may be possible to test the \"memory burden\" effect via induced GWs.","Lastly, we argue that our results could also apply to general modifications of Hawking evaporation."],"url":"http://arxiv.org/abs/2403.14309v1","category":"gr-qc"}
{"created":"2024-03-21 11:23:40","title":"Ferromagnetic Ising Model on multiregular random graphs","abstract":"A family of multispecies Ising models on generalized regular random graphs is investigated in the thermodynamic limit. The architecture is specified by class-dependent couplings and magnetic fields. We prove that the magnetizations, neighbours correlations and free energy converge to suitable functions evaluated at the solution of a belief propagation fixed point equation. In absence of magnetic fields, a phase transition is identified and the corresponding critical parameters are determined by the spectral radius of a low-dimensional matrix.","sentences":["A family of multispecies Ising models on generalized regular random graphs is investigated in the thermodynamic limit.","The architecture is specified by class-dependent couplings and magnetic fields.","We prove that the magnetizations, neighbours correlations and free energy converge to suitable functions evaluated at the solution of a belief propagation fixed point equation.","In absence of magnetic fields, a phase transition is identified and the corresponding critical parameters are determined by the spectral radius of a low-dimensional matrix."],"url":"http://arxiv.org/abs/2403.14307v1","category":"math-ph"}
{"created":"2024-03-21 11:23:36","title":"Learning-to-Learn the Wave Angle Estimation","abstract":"A precise incident wave angle estimation in aerial communication is a key enabler in sixth-generation wireless communication network. With this goal, a generic 3-dimensional (3D) channel model is analyzed for air-to-air (A2A) networks under antenna misalignment, radio frequency impairments and polarization loss. The unique aspects of each aerial node are highlighted and the few-shot learning as a model agnostic meta-learning (MAML) classifier is proposed for learning-to-learn (L2L) incident wave angle estimation by utilizing the received signal strength (RSS). Additionally, a more computationally efficient technique, first order model agnostic meta-learning (FOMAML) is implemented. It has been observed that the proposed approach reaches up to 85% training accuracy and 75.4% evaluation accuracy with MAML. Regarding this, a convergence rate and accuracy trade-off have been established for several cases of MAML and FOMAML. For different L2L models trained with limited data, heuristic accuracy performance is determined by an upper bound of the probability of confidence.","sentences":["A precise incident wave angle estimation in aerial communication is a key enabler in sixth-generation wireless communication network.","With this goal, a generic 3-dimensional (3D) channel model is analyzed for air-to-air (A2A) networks under antenna misalignment, radio frequency impairments and polarization loss.","The unique aspects of each aerial node are highlighted and the few-shot learning as a model agnostic meta-learning (MAML) classifier is proposed for learning-to-learn (L2L) incident wave angle estimation by utilizing the received signal strength (RSS).","Additionally, a more computationally efficient technique, first order model agnostic meta-learning (FOMAML) is implemented.","It has been observed that the proposed approach reaches up to 85% training accuracy and 75.4% evaluation accuracy with MAML.","Regarding this, a convergence rate and accuracy trade-off have been established for several cases of MAML and FOMAML.","For different L2L models trained with limited data, heuristic accuracy performance is determined by an upper bound of the probability of confidence."],"url":"http://arxiv.org/abs/2403.14306v1","category":"eess.SP"}
{"created":"2024-03-21 11:19:13","title":"Assessing the Spurious Impacts of Ice-Constraining Methods on the Climate Response to Sea-Ice Loss using an Idealised Aquaplanet GCM","abstract":"Coupled climate model simulations designed to isolate the effects of Arctic sea-ice loss often apply artificial heating, either directly to the ice or through modification of the surface albedo, to constrain sea-ice in the absence of other forcings. Recent work has shown that this approach may lead to an overestimation of the climate response to sea-ice loss. In this study, we assess the spurious impacts of ice-constraining methods on the climate of an idealised aquaplanet general circulation model (GCM) with thermodynamic sea-ice. The true effect of sea-ice loss in this model is isolated by inducing ice loss through reduction of the freezing point of water, which does not require additional energy input. We compare results from freezing point modification experiments with experiments where sea-ice loss is induced using traditional ice-constraining methods, and confirm the result of previous work that traditional methods induce spurious additional warming. Furthermore, additional warming leads to an overestimation of the circulation response to sea-ice loss, which involves a weakening of the zonal wind and storm track activity in midlatitudes. Our results suggest that coupled model simulations with constrained sea-ice should be treated with caution, especially in boreal summer, where the true effect of sea-ice loss is weakest but we find the largest spurious response. Given that our results may be sensitive to the simplicity of the model we use, we suggest that devising methods to quantify the spurious effects of ice-constraining methods in more sophisticated models should be an urgent priority for future work.","sentences":["Coupled climate model simulations designed to isolate the effects of Arctic sea-ice loss often apply artificial heating, either directly to the ice or through modification of the surface albedo, to constrain sea-ice in the absence of other forcings.","Recent work has shown that this approach may lead to an overestimation of the climate response to sea-ice loss.","In this study, we assess the spurious impacts of ice-constraining methods on the climate of an idealised aquaplanet general circulation model (GCM) with thermodynamic sea-ice.","The true effect of sea-ice loss in this model is isolated by inducing ice loss through reduction of the freezing point of water, which does not require additional energy input.","We compare results from freezing point modification experiments with experiments where sea-ice loss is induced using traditional ice-constraining methods, and confirm the result of previous work that traditional methods induce spurious additional warming.","Furthermore, additional warming leads to an overestimation of the circulation response to sea-ice loss, which involves a weakening of the zonal wind and storm track activity in midlatitudes.","Our results suggest that coupled model simulations with constrained sea-ice should be treated with caution, especially in boreal summer, where the true effect of sea-ice loss is weakest but we find the largest spurious response.","Given that our results may be sensitive to the simplicity of the model we use, we suggest that devising methods to quantify the spurious effects of ice-constraining methods in more sophisticated models should be an urgent priority for future work."],"url":"http://arxiv.org/abs/2403.14304v1","category":"physics.ao-ph"}
{"created":"2024-03-21 11:16:42","title":"SpikingResformer: Bridging ResNet and Vision Transformer in Spiking Neural Networks","abstract":"The remarkable success of Vision Transformers in Artificial Neural Networks (ANNs) has led to a growing interest in incorporating the self-attention mechanism and transformer-based architecture into Spiking Neural Networks (SNNs). While existing methods propose spiking self-attention mechanisms that are compatible with SNNs, they lack reasonable scaling methods, and the overall architectures proposed by these methods suffer from a bottleneck in effectively extracting local features. To address these challenges, we propose a novel spiking self-attention mechanism named Dual Spike Self-Attention (DSSA) with a reasonable scaling method. Based on DSSA, we propose a novel spiking Vision Transformer architecture called SpikingResformer, which combines the ResNet-based multi-stage architecture with our proposed DSSA to improve both performance and energy efficiency while reducing parameters. Experimental results show that SpikingResformer achieves higher accuracy with fewer parameters and lower energy consumption than other spiking Vision Transformer counterparts. Notably, our SpikingResformer-L achieves 79.40% top-1 accuracy on ImageNet with 4 time-steps, which is the state-of-the-art result in the SNN field.","sentences":["The remarkable success of Vision Transformers in Artificial Neural Networks (ANNs) has led to a growing interest in incorporating the self-attention mechanism and transformer-based architecture into Spiking Neural Networks (SNNs).","While existing methods propose spiking self-attention mechanisms that are compatible with SNNs, they lack reasonable scaling methods, and the overall architectures proposed by these methods suffer from a bottleneck in effectively extracting local features.","To address these challenges, we propose a novel spiking self-attention mechanism named Dual Spike Self-Attention (DSSA) with a reasonable scaling method.","Based on DSSA, we propose a novel spiking Vision Transformer architecture called SpikingResformer, which combines the ResNet-based multi-stage architecture with our proposed DSSA to improve both performance and energy efficiency while reducing parameters.","Experimental results show that SpikingResformer achieves higher accuracy with fewer parameters and lower energy consumption than other spiking Vision Transformer counterparts.","Notably, our SpikingResformer-L achieves 79.40% top-1 accuracy on ImageNet with 4 time-steps, which is the state-of-the-art result in the SNN field."],"url":"http://arxiv.org/abs/2403.14302v1","category":"cs.NE"}
{"created":"2024-03-21 11:16:28","title":"DexDribbler: Learning Dexterous Soccer Manipulation via Dynamic Supervision","abstract":"Learning dexterous locomotion policy for legged robots is becoming increasingly popular due to its ability to handle diverse terrains and resemble intelligent behaviors. However, joint manipulation of moving objects and locomotion with legs, such as playing soccer, receive scant attention in the learning community, although it is natural for humans and smart animals. A key challenge to solve this multitask problem is to infer the objectives of locomotion from the states and targets of the manipulated objects. The implicit relation between the object states and robot locomotion can be hard to capture directly from the training experience. We propose adding a feedback control block to compute the necessary body-level movement accurately and using the outputs as dynamic joint-level locomotion supervision explicitly. We further utilize an improved ball dynamic model, an extended context-aided estimator, and a comprehensive ball observer to facilitate transferring policy learned in simulation to the real world. We observe that our learning scheme can not only make the policy network converge faster but also enable soccer robots to perform sophisticated maneuvers like sharp cuts and turns on flat surfaces, a capability that was lacking in previous methods. Video and code are available at https://github.com/SysCV/soccer-player","sentences":["Learning dexterous locomotion policy for legged robots is becoming increasingly popular due to its ability to handle diverse terrains and resemble intelligent behaviors.","However, joint manipulation of moving objects and locomotion with legs, such as playing soccer, receive scant attention in the learning community, although it is natural for humans and smart animals.","A key challenge to solve this multitask problem is to infer the objectives of locomotion from the states and targets of the manipulated objects.","The implicit relation between the object states and robot locomotion can be hard to capture directly from the training experience.","We propose adding a feedback control block to compute the necessary body-level movement accurately and using the outputs as dynamic joint-level locomotion supervision explicitly.","We further utilize an improved ball dynamic model, an extended context-aided estimator, and a comprehensive ball observer to facilitate transferring policy learned in simulation to the real world.","We observe that our learning scheme can not only make the policy network converge faster but also enable soccer robots to perform sophisticated maneuvers like sharp cuts and turns on flat surfaces, a capability that was lacking in previous methods.","Video and code are available at https://github.com/SysCV/soccer-player"],"url":"http://arxiv.org/abs/2403.14300v1","category":"cs.RO"}
{"created":"2024-03-21 11:04:41","title":"From Perils to Possibilities: Understanding how Human (and AI) Biases affect Online Fora","abstract":"Social media platforms are online fora where users engage in discussions, share content, and build connections. This review explores the dynamics of social interactions, user-generated contents, and biases within the context of social media analysis (analyzing works that use the tools offered by complex network analysis and natural language processing) through the lens of three key points of view: online debates, online support, and human-AI interactions. On the one hand, we delineate the phenomenon of online debates, where polarization, misinformation, and echo chamber formation often proliferate, driven by algorithmic biases and extreme mechanisms of homophily. On the other hand, we explore the emergence of online support groups through users' self-disclosure and social support mechanisms. Online debates and support mechanisms present a duality of both perils and possibilities within social media; perils of segregated communities and polarized debates, and possibilities of empathy narratives and self-help groups. This dichotomy also extends to a third perspective: users' reliance on AI-generated content, such as the ones produced by Large Language Models, which can manifest both human biases hidden in training sets and non-human biases that emerge from their artificial neural architectures. Analyzing interdisciplinary approaches, we aim to deepen the understanding of the complex interplay between social interactions, user-generated content, and biases within the realm of social media ecosystems.","sentences":["Social media platforms are online fora where users engage in discussions, share content, and build connections.","This review explores the dynamics of social interactions, user-generated contents, and biases within the context of social media analysis (analyzing works that use the tools offered by complex network analysis and natural language processing) through the lens of three key points of view: online debates, online support, and human-AI interactions.","On the one hand, we delineate the phenomenon of online debates, where polarization, misinformation, and echo chamber formation often proliferate, driven by algorithmic biases and extreme mechanisms of homophily.","On the other hand, we explore the emergence of online support groups through users' self-disclosure and social support mechanisms.","Online debates and support mechanisms present a duality of both perils and possibilities within social media; perils of segregated communities and polarized debates, and possibilities of empathy narratives and self-help groups.","This dichotomy also extends to a third perspective: users' reliance on AI-generated content, such as the ones produced by Large Language Models, which can manifest both human biases hidden in training sets and non-human biases that emerge from their artificial neural architectures.","Analyzing interdisciplinary approaches, we aim to deepen the understanding of the complex interplay between social interactions, user-generated content, and biases within the realm of social media ecosystems."],"url":"http://arxiv.org/abs/2403.14298v1","category":"cs.SI"}
{"created":"2024-03-21 11:03:56","title":"Impact Assessment of Missing Data in Model Predictions for Earth Observation Applications","abstract":"Earth observation (EO) applications involving complex and heterogeneous data sources are commonly approached with machine learning models. However, there is a common assumption that data sources will be persistently available. Different situations could affect the availability of EO sources, like noise, clouds, or satellite mission failures. In this work, we assess the impact of missing temporal and static EO sources in trained models across four datasets with classification and regression tasks. We compare the predictive quality of different methods and find that some are naturally more robust to missing data. The Ensemble strategy, in particular, achieves a prediction robustness up to 100%. We evidence that missing scenarios are significantly more challenging in regression than classification tasks. Finally, we find that the optical view is the most critical view when it is missing individually.","sentences":["Earth observation (EO) applications involving complex and heterogeneous data sources are commonly approached with machine learning models.","However, there is a common assumption that data sources will be persistently available.","Different situations could affect the availability of EO sources, like noise, clouds, or satellite mission failures.","In this work, we assess the impact of missing temporal and static EO sources in trained models across four datasets with classification and regression tasks.","We compare the predictive quality of different methods and find that some are naturally more robust to missing data.","The Ensemble strategy, in particular, achieves a prediction robustness up to 100%.","We evidence that missing scenarios are significantly more challenging in regression than classification tasks.","Finally, we find that the optical view is the most critical view when it is missing individually."],"url":"http://arxiv.org/abs/2403.14297v1","category":"cs.LG"}
{"created":"2024-03-21 10:56:12","title":"Open-Vocabulary Attention Maps with Token Optimization for Semantic Segmentation in Diffusion Models","abstract":"Diffusion models represent a new paradigm in text-to-image generation. Beyond generating high-quality images from text prompts, models such as Stable Diffusion have been successfully extended to the joint generation of semantic segmentation pseudo-masks. However, current extensions primarily rely on extracting attentions linked to prompt words used for image synthesis. This approach limits the generation of segmentation masks derived from word tokens not contained in the text prompt. In this work, we introduce Open-Vocabulary Attention Maps (OVAM)-a training-free method for text-to-image diffusion models that enables the generation of attention maps for any word. In addition, we propose a lightweight optimization process based on OVAM for finding tokens that generate accurate attention maps for an object class with a single annotation. We evaluate these tokens within existing state-of-the-art Stable Diffusion extensions. The best-performing model improves its mIoU from 52.1 to 86.6 for the synthetic images' pseudo-masks, demonstrating that our optimized tokens are an efficient way to improve the performance of existing methods without architectural changes or retraining.","sentences":["Diffusion models represent a new paradigm in text-to-image generation.","Beyond generating high-quality images from text prompts, models such as Stable Diffusion have been successfully extended to the joint generation of semantic segmentation pseudo-masks.","However, current extensions primarily rely on extracting attentions linked to prompt words used for image synthesis.","This approach limits the generation of segmentation masks derived from word tokens not contained in the text prompt.","In this work, we introduce Open-Vocabulary Attention Maps (OVAM)-a training-free method for text-to-image diffusion models that enables the generation of attention maps for any word.","In addition, we propose a lightweight optimization process based on OVAM for finding tokens that generate accurate attention maps for an object class with a single annotation.","We evaluate these tokens within existing state-of-the-art Stable Diffusion extensions.","The best-performing model improves its mIoU from 52.1 to 86.6 for the synthetic images' pseudo-masks, demonstrating that our optimized tokens are an efficient way to improve the performance of existing methods without architectural changes or retraining."],"url":"http://arxiv.org/abs/2403.14291v1","category":"cs.CV"}
{"created":"2024-03-21 10:52:07","title":"Formation of ultracold deeply-bound molecules via multi-state chainwise coincident pulses technique","abstract":"In this paper, a theoretical method for the efficient creation and detection of deeply bound molecules in three-state $\\Lambda$-type and five-state M-type molecular systems is proposed. The method is based on the three-state coincident pulses technique and the generalized five-state coincident pulses technique. For the three-state system, the technique can efficiently transfer the populations from the Feshbach state to the deeply-bound state via a train of $N$ pairs of resonant and coincident pump and Stokes pulses, with negligible transient populations of excited states. For the five-state system, it is found that this M-type system can be generalized into a $\\Lambda$-type structure with the simplest resonant coupling under the assumption of large one-photon detuning together with a requirement of the relation among the four incident pulses. Thereafter, this generalized model permits us to employ the reduced three-state propagator to design four coincident pulses to achieve the desired population transfer. For the numerical study, $^{87}$Rb$_2$ is considered and, it is shown that the weakly-bound Feshbach molecules can be efficiently transferred to their deeply-bound states without strong laser pulses, and the populations of all intermediate states can be well suppressed.","sentences":["In this paper, a theoretical method for the efficient creation and detection of deeply bound molecules in three-state $\\Lambda$-type and five-state M-type molecular systems is proposed.","The method is based on the three-state coincident pulses technique and the generalized five-state coincident pulses technique.","For the three-state system, the technique can efficiently transfer the populations from the Feshbach state to the deeply-bound state via a train of $N$ pairs of resonant and coincident pump and Stokes pulses, with negligible transient populations of excited states.","For the five-state system, it is found that this M-type system can be generalized into a $\\Lambda$-type structure with the simplest resonant coupling under the assumption of large one-photon detuning together with a requirement of the relation among the four incident pulses.","Thereafter, this generalized model permits us to employ the reduced three-state propagator to design four coincident pulses to achieve the desired population transfer.","For the numerical study, $^{87}$Rb$_2$ is considered and, it is shown that the weakly-bound Feshbach molecules can be efficiently transferred to their deeply-bound states without strong laser pulses, and the populations of all intermediate states can be well suppressed."],"url":"http://arxiv.org/abs/2403.14288v1","category":"physics.atom-ph"}
{"created":"2024-03-21 10:51:19","title":"Enhancing Historical Image Retrieval with Compositional Cues","abstract":"In analyzing vast amounts of digitally stored historical image data, existing content-based retrieval methods often overlook significant non-semantic information, limiting their effectiveness for flexible exploration across varied themes. To broaden the applicability of image retrieval methods for diverse purposes and uncover more general patterns, we innovatively introduce a crucial factor from computational aesthetics, namely image composition, into this topic. By explicitly integrating composition-related information extracted by CNN into the designed retrieval model, our method considers both the image's composition rules and semantic information. Qualitative and quantitative experiments demonstrate that the image retrieval network guided by composition information outperforms those relying solely on content information, facilitating the identification of images in databases closer to the target image in human perception. Please visit https://github.com/linty5/CCBIR to try our codes.","sentences":["In analyzing vast amounts of digitally stored historical image data, existing content-based retrieval methods often overlook significant non-semantic information, limiting their effectiveness for flexible exploration across varied themes.","To broaden the applicability of image retrieval methods for diverse purposes and uncover more general patterns, we innovatively introduce a crucial factor from computational aesthetics, namely image composition, into this topic.","By explicitly integrating composition-related information extracted by CNN into the designed retrieval model, our method considers both the image's composition rules and semantic information.","Qualitative and quantitative experiments demonstrate that the image retrieval network guided by composition information outperforms those relying solely on content information, facilitating the identification of images in databases closer to the target image in human perception.","Please visit https://github.com/linty5/CCBIR to try our codes."],"url":"http://arxiv.org/abs/2403.14287v1","category":"cs.CV"}
{"created":"2024-03-21 10:43:55","title":"How to be fair? A study of label and selection bias","abstract":"It is widely accepted that biased data leads to biased and thus potentially unfair models. Therefore, several measures for bias in data and model predictions have been proposed, as well as bias mitigation techniques whose aim is to learn models that are fair by design. Despite the myriad of mitigation techniques developed in the past decade, however, it is still poorly understood under what circumstances which methods work. Recently, Wick et al. showed, with experiments on synthetic data, that there exist situations in which bias mitigation techniques lead to more accurate models when measured on unbiased data. Nevertheless, in the absence of a thorough mathematical analysis, it remains unclear which techniques are effective under what circumstances. We propose to address this problem by establishing relationships between the type of bias and the effectiveness of a mitigation technique, where we categorize the mitigation techniques by the bias measure they optimize. In this paper we illustrate this principle for label and selection bias on the one hand, and demographic parity and ``We're All Equal'' on the other hand. Our theoretical analysis allows to explain the results of Wick et al. and we also show that there are situations where minimizing fairness measures does not result in the fairest possible distribution.","sentences":["It is widely accepted that biased data leads to biased and thus potentially unfair models.","Therefore, several measures for bias in data and model predictions have been proposed, as well as bias mitigation techniques whose aim is to learn models that are fair by design.","Despite the myriad of mitigation techniques developed in the past decade, however, it is still poorly understood under what circumstances which methods work.","Recently, Wick et al. showed, with experiments on synthetic data, that there exist situations in which bias mitigation techniques lead to more accurate models when measured on unbiased data.","Nevertheless, in the absence of a thorough mathematical analysis, it remains unclear which techniques are effective under what circumstances.","We propose to address this problem by establishing relationships between the type of bias and the effectiveness of a mitigation technique, where we categorize the mitigation techniques by the bias measure they optimize.","In this paper we illustrate this principle for label and selection bias on the one hand, and demographic parity and ``We're All Equal'' on the other hand.","Our theoretical analysis allows to explain the results of Wick et al.","and we also show that there are situations where minimizing fairness measures does not result in the fairest possible distribution."],"url":"http://arxiv.org/abs/2403.14282v1","category":"cs.LG"}
{"created":"2024-03-21 10:34:38","title":"Binding zero modes with fluxons in Josephson junctions of time-reversal invariant topological superconductors","abstract":"We study the joint dynamics of the phase bias $\\phi$ and the propagating Majorana fermions of the edge modes in Josephson junctions containing 2D time-reversal invariant topological superconductors (TRITOPS). We consider TRITOPS-TRITOPS junctions, as well as junctions between topological and non-topological superconductors (TRITOPS-S). Both types of junctions are described by effective Dirac Hamiltonians with a $\\phi$-dependent mass. We analyze the effect of the phase fluctuations in the junction, as well as solitonic solutions of $\\phi$ generated by fluxons trapped in the junction. We show that these solitons generate a spatial-dependent mass with a sign change akin to the Jackiw-Rebbi model. This enables the formation of zero-energy fermionic states localized at the fluxon. For the TRITOPS-TRITOPS junction these consist of a Kramers pair of Majorana modes, while for the TRITOPS-S one, there is a single Majorana fermion. The localized bound states hybridize in soliton-antisoliton configurations. Depending on the occupation state, these modes generate an effective attraction or repulsion in the dynamics of the soliton-antisoliton collision.","sentences":["We study the joint dynamics of the phase bias $\\phi$ and the propagating Majorana fermions of the edge modes in Josephson junctions containing 2D time-reversal invariant topological superconductors (TRITOPS).","We consider TRITOPS-TRITOPS junctions, as well as junctions between topological and non-topological superconductors (TRITOPS-S).","Both types of junctions are described by effective Dirac Hamiltonians with a $\\phi$-dependent mass.","We analyze the effect of the phase fluctuations in the junction, as well as solitonic solutions of $\\phi$ generated by fluxons trapped in the junction.","We show that these solitons generate a spatial-dependent mass with a sign change akin to the Jackiw-Rebbi model.","This enables the formation of zero-energy fermionic states localized at the fluxon.","For the TRITOPS-TRITOPS junction these consist of a Kramers pair of Majorana modes, while for the TRITOPS-S one, there is a single Majorana fermion.","The localized bound states hybridize in soliton-antisoliton configurations.","Depending on the occupation state, these modes generate an effective attraction or repulsion in the dynamics of the soliton-antisoliton collision."],"url":"http://arxiv.org/abs/2403.14277v1","category":"cond-mat.supr-con"}
{"created":"2024-03-21 10:28:18","title":"Multi-role Consensus through LLMs Discussions for Vulnerability Detection","abstract":"Recent advancements in large language models (LLMs) have highlighted the potential for vulnerability detection, a crucial component of software quality assurance. Despite this progress, most studies have been limited to the perspective of a single role, usually testers, lacking diverse viewpoints from different roles in a typical software development life-cycle, including both developers and testers. To this end, this paper introduces an approach to employ LLMs to act as different roles to simulate real-life code review process, engaging in discussions towards a consensus on the existence and classification of vulnerabilities in the code. Preliminary evaluation of the proposed approach indicates a 4.73% increase in the precision rate, 58.9% increase in the recall rate, and a 28.1% increase in the F1 score.","sentences":["Recent advancements in large language models (LLMs) have highlighted the potential for vulnerability detection, a crucial component of software quality assurance.","Despite this progress, most studies have been limited to the perspective of a single role, usually testers, lacking diverse viewpoints from different roles in a typical software development life-cycle, including both developers and testers.","To this end, this paper introduces an approach to employ LLMs to act as different roles to simulate real-life code review process, engaging in discussions towards a consensus on the existence and classification of vulnerabilities in the code.","Preliminary evaluation of the proposed approach indicates a 4.73% increase in the precision rate, 58.9% increase in the recall rate, and a 28.1% increase in the F1 score."],"url":"http://arxiv.org/abs/2403.14274v1","category":"cs.SE"}
{"created":"2024-03-21 10:26:47","title":"Reactor Optimization Benchmark by Reinforcement Learning","abstract":"Neutronic calculations for reactors are a daunting task when using Monte Carlo (MC) methods. As high-performance computing has advanced, the simulation of a reactor is nowadays more readily done, but design and optimization with multiple parameters is still a computational challenge. MC transport simulations, coupled with machine learning techniques, offer promising avenues for enhancing the efficiency and effectiveness of nuclear reactor optimization. This paper introduces a novel benchmark problem within the OpenNeoMC framework designed specifically for reinforcement learning. The benchmark involves optimizing a unit cell of a research reactor with two varying parameters (fuel density and water spacing) to maximize neutron flux while maintaining reactor criticality. The test case features distinct local optima, representing different physical regimes, thus posing a challenge for learning algorithms. Through extensive simulations utilizing evolutionary and neuroevolutionary algorithms, we demonstrate the effectiveness of reinforcement learning in navigating complex optimization landscapes with strict constraints. Furthermore, we propose acceleration techniques within the OpenNeoMC framework, including model updating and cross-section usage by RAM utilization, to expedite simulation times. Our findings emphasize the importance of machine learning integration in reactor optimization and contribute to advancing methodologies for addressing intricate optimization challenges in nuclear engineering. The sources of this work are available at our GitHub repository: https://github.com/Scientific-Computing-Lab-NRCN/RLOpenNeoMC","sentences":["Neutronic calculations for reactors are a daunting task when using Monte Carlo (MC) methods.","As high-performance computing has advanced, the simulation of a reactor is nowadays more readily done, but design and optimization with multiple parameters is still a computational challenge.","MC transport simulations, coupled with machine learning techniques, offer promising avenues for enhancing the efficiency and effectiveness of nuclear reactor optimization.","This paper introduces a novel benchmark problem within the OpenNeoMC framework designed specifically for reinforcement learning.","The benchmark involves optimizing a unit cell of a research reactor with two varying parameters (fuel density and water spacing) to maximize neutron flux while maintaining reactor criticality.","The test case features distinct local optima, representing different physical regimes, thus posing a challenge for learning algorithms.","Through extensive simulations utilizing evolutionary and neuroevolutionary algorithms, we demonstrate the effectiveness of reinforcement learning in navigating complex optimization landscapes with strict constraints.","Furthermore, we propose acceleration techniques within the OpenNeoMC framework, including model updating and cross-section usage by RAM utilization, to expedite simulation times.","Our findings emphasize the importance of machine learning integration in reactor optimization and contribute to advancing methodologies for addressing intricate optimization challenges in nuclear engineering.","The sources of this work are available at our GitHub repository: https://github.com/Scientific-Computing-Lab-NRCN/RLOpenNeoMC"],"url":"http://arxiv.org/abs/2403.14273v1","category":"cs.NE"}
{"created":"2024-03-21 10:23:08","title":"Accuracy Assessment of Discontinuous Galerkin Spectral Element Method in Simulating Supersonic Free Jets","abstract":"The study performs large-eddy simulations of supersonic free jet flows using the Discontinuous Galerkin Spectral Element Method (DGSEM). The main objective of the present work is to assess the resolution requirements for adequate simulation of such flows with the DGSEM approach. The study looked at the influence of the mesh and the spatial discretization accuracy on the simulation results. The present analysis involves four simulations, incorporating three different numerical meshes and two different orders of spatial discretization accuracy. The numerical meshes are generated with distinct mesh topologies and refinement levels. Detailed descriptions of the grid generation and refinement procedures are presented. The study compares flow property profiles and power spectral densities of velocity components with experimental data. The results show a consistent improvement in the computed data as the simulation resolution increases. This investigation revealed a trade-off between mesh and polynomial refinement, striking a balance between computational cost and the accuracy of large-eddy simulation results for turbulent flow analyses.","sentences":["The study performs large-eddy simulations of supersonic free jet flows using the Discontinuous Galerkin Spectral Element Method (DGSEM).","The main objective of the present work is to assess the resolution requirements for adequate simulation of such flows with the DGSEM approach.","The study looked at the influence of the mesh and the spatial discretization accuracy on the simulation results.","The present analysis involves four simulations, incorporating three different numerical meshes and two different orders of spatial discretization accuracy.","The numerical meshes are generated with distinct mesh topologies and refinement levels.","Detailed descriptions of the grid generation and refinement procedures are presented.","The study compares flow property profiles and power spectral densities of velocity components with experimental data.","The results show a consistent improvement in the computed data as the simulation resolution increases.","This investigation revealed a trade-off between mesh and polynomial refinement, striking a balance between computational cost and the accuracy of large-eddy simulation results for turbulent flow analyses."],"url":"http://arxiv.org/abs/2403.14272v1","category":"physics.flu-dyn"}
{"created":"2024-03-21 10:10:32","title":"Dirac's theorem for linear hypergraphs","abstract":"Dirac's theorem states that any $n$-vertex graph $G$ with even integer $n$ satisfying $\\delta(G) \\geq n/2$ contains a perfect matching. We generalize this to $k$-uniform linear hypergraphs by proving the following. Any $n$-vertex $k$-uniform linear hypergraph $H$ with minimum degree at least $\\frac{n}{k} + \\Omega(1)$ contains a matching that covers at least $(1-o(1))n$ vertices. This minimum degree condition is asymptotically tight and obtaining perfect matching is impossible with any degree condition. Furthermore, we show that if $\\delta(H) \\geq (\\frac{1}{k}+o(1))n$, then $H$ contains almost spanning linear cycles, almost spanning hypertrees with $o(n)$ leaves, and ``long subdivisions'' of any $o(\\sqrt{n})$-vertex graphs.","sentences":["Dirac's theorem states that any $n$-vertex graph $G$ with even integer $n$ satisfying $\\delta(G) \\geq n/2$ contains a perfect matching.","We generalize this to $k$-uniform linear hypergraphs by proving the following.","Any $n$-vertex $k$-uniform linear hypergraph $H$ with minimum degree at least $\\frac{n}{k} + \\Omega(1)$ contains a matching that covers at least $(1-o(1))n$ vertices.","This minimum degree condition is asymptotically tight and obtaining perfect matching is impossible with any degree condition.","Furthermore, we show that if $\\delta(H) \\geq (\\frac{1}{k}+o(1))n$, then $H$ contains almost spanning linear cycles, almost spanning hypertrees with $o(n)$ leaves, and ``long subdivisions'' of any $o(\\sqrt{n})$-vertex graphs."],"url":"http://arxiv.org/abs/2403.14269v1","category":"math.CO"}
{"created":"2024-03-21 10:09:46","title":"Speech-Aware Neural Diarization with Encoder-Decoder Attractor Guided by Attention Constraints","abstract":"End-to-End Neural Diarization with Encoder-Decoder based Attractor (EEND-EDA) is an end-to-end neural model for automatic speaker segmentation and labeling. It achieves the capability to handle flexible number of speakers by estimating the number of attractors. EEND-EDA, however, struggles to accurately capture local speaker dynamics. This work proposes an auxiliary loss that aims to guide the Transformer encoders at the lower layer of EEND-EDA model to enhance the effect of self-attention modules using speaker activity information. The results evaluated on public dataset Mini LibriSpeech, demonstrates the effectiveness of the work, reducing Diarization Error Rate from 30.95% to 28.17%. We will release the source code on GitHub to allow further research and reproducibility.","sentences":["End-to-End Neural Diarization with Encoder-Decoder based Attractor (EEND-EDA) is an end-to-end neural model for automatic speaker segmentation and labeling.","It achieves the capability to handle flexible number of speakers by estimating the number of attractors.","EEND-EDA, however, struggles to accurately capture local speaker dynamics.","This work proposes an auxiliary loss that aims to guide the Transformer encoders at the lower layer of EEND-EDA model to enhance the effect of self-attention modules using speaker activity information.","The results evaluated on public dataset Mini LibriSpeech, demonstrates the effectiveness of the work, reducing Diarization Error Rate from 30.95% to 28.17%.","We will release the source code on GitHub to allow further research and reproducibility."],"url":"http://arxiv.org/abs/2403.14268v1","category":"eess.AS"}
{"created":"2024-03-21 10:05:30","title":"Conditional variational autoencoder inference of neutron star equation of state from astrophysical observations","abstract":"We present a new inference framework for neutron star astrophysics based on conditional variational autoencoders. Once trained, the generator block of the model reconstructs the neutron star equation of state from a given set of mass-radius observations. While the pressure of dense matter is the focus of the present study, the proposed model is flexible enough to accommodate the reconstructing of any other quantity related to dense matter equation of state. Our results show robust reconstructing performance of the model, allowing to make instantaneous inference from any given observation set. The present framework contrasts with computationally expensive evaluation of the equation-of-state posterior probability distribution based on Markov chain Monte Carlo methods.","sentences":["We present a new inference framework for neutron star astrophysics based on conditional variational autoencoders.","Once trained, the generator block of the model reconstructs the neutron star equation of state from a given set of mass-radius observations.","While the pressure of dense matter is the focus of the present study, the proposed model is flexible enough to accommodate the reconstructing of any other quantity related to dense matter equation of state.","Our results show robust reconstructing performance of the model, allowing to make instantaneous inference from any given observation set.","The present framework contrasts with computationally expensive evaluation of the equation-of-state posterior probability distribution based on Markov chain Monte Carlo methods."],"url":"http://arxiv.org/abs/2403.14266v1","category":"nucl-th"}
{"created":"2024-03-21 09:59:53","title":"A Framework for Portrait Stylization with Skin-Tone Awareness and Nudity Identification","abstract":"Portrait stylization is a challenging task involving the transformation of an input portrait image into a specific style while preserving its inherent characteristics. The recent introduction of Stable Diffusion (SD) has significantly improved the quality of outcomes in this field. However, a practical stylization framework that can effectively filter harmful input content and preserve the distinct characteristics of an input, such as skin-tone, while maintaining the quality of stylization remains lacking. These challenges have hindered the wide deployment of such a framework. To address these issues, this study proposes a portrait stylization framework that incorporates a nudity content identification module (NCIM) and a skin-tone-aware portrait stylization module (STAPSM). In experiments, NCIM showed good performance in enhancing explicit content filtering, and STAPSM accurately represented a diverse range of skin tones. Our proposed framework has been successfully deployed in practice, and it has effectively satisfied critical requirements of real-world applications.","sentences":["Portrait stylization is a challenging task involving the transformation of an input portrait image into a specific style while preserving its inherent characteristics.","The recent introduction of Stable Diffusion (SD) has significantly improved the quality of outcomes in this field.","However, a practical stylization framework that can effectively filter harmful input content and preserve the distinct characteristics of an input, such as skin-tone, while maintaining the quality of stylization remains lacking.","These challenges have hindered the wide deployment of such a framework.","To address these issues, this study proposes a portrait stylization framework that incorporates a nudity content identification module (NCIM) and a skin-tone-aware portrait stylization module (STAPSM).","In experiments, NCIM showed good performance in enhancing explicit content filtering, and STAPSM accurately represented a diverse range of skin tones.","Our proposed framework has been successfully deployed in practice, and it has effectively satisfied critical requirements of real-world applications."],"url":"http://arxiv.org/abs/2403.14264v1","category":"cs.CV"}
{"created":"2024-03-21 09:57:05","title":"Inverse Design of Nonlinear Metasurfaces for Sum Frequency Generation","abstract":"Sum frequency generation (SFG) has multiple applications, from optical sources to imaging, where efficient conversion requires either long interaction distances or large field concentrations in a quadratic nonlinear material. Metasurfaces provide an essential avenue to enhanced SFG due to resonance with extreme field enhancements with an integrated ultrathin platform. In this work, we formulate a general theoretical framework for multi-objective topology optimization of nanopatterned metasurfaces that facilitate high-efficiency SFG and simultaneously select the emitted direction and tailor the metasurface polarization response. Based on this framework, we present novel metasurface designs showcasing ultimate flexibility in transforming the outgoing nonlinearly generated light for applications spanning from imaging to polarimetry. For example, one of our metasurfaces produces highly polarized and directional SFG emission with an efficiency of over 0.2 cm^2/GW in a 10 nm signal operating bandwidth.","sentences":["Sum frequency generation (SFG) has multiple applications, from optical sources to imaging, where efficient conversion requires either long interaction distances or large field concentrations in a quadratic nonlinear material.","Metasurfaces provide an essential avenue to enhanced SFG due to resonance with extreme field enhancements with an integrated ultrathin platform.","In this work, we formulate a general theoretical framework for multi-objective topology optimization of nanopatterned metasurfaces that facilitate high-efficiency SFG and simultaneously select the emitted direction and tailor the metasurface polarization response.","Based on this framework, we present novel metasurface designs showcasing ultimate flexibility in transforming the outgoing nonlinearly generated light for applications spanning from imaging to polarimetry.","For example, one of our metasurfaces produces highly polarized and directional SFG emission with an efficiency of over 0.2 cm^2/GW in a 10 nm signal operating bandwidth."],"url":"http://arxiv.org/abs/2403.14263v1","category":"physics.optics"}
{"created":"2024-03-21 09:36:36","title":"LLM-based Extraction of Contradictions from Patents","abstract":"Already since the 1950s TRIZ shows that patents and the technical contradictions they solve are an important source of inspiration for the development of innovative products. However, TRIZ is a heuristic based on a historic patent analysis and does not make use of the ever-increasing number of latest technological solutions in current patents. Because of the huge number of patents, their length, and, last but not least, their complexity there is a need for modern patent retrieval and patent analysis to go beyond keyword-oriented methods. Recent advances in patent retrieval and analysis mainly focus on dense vectors based on neural AI Transformer language models like Google BERT. They are, for example, used for dense retrieval, question answering or summarization and key concept extraction. A research focus within the methods for patent summarization and key concept extraction are generic inventive concepts respectively TRIZ concepts like problems, solutions, advantage of invention, parameters, and contradictions. Succeeding rule-based approaches, finetuned BERT-like language models for sentence-wise classification represent the state-of-the-art of inventive concept extraction. While they work comparatively well for basic concepts like problems or solutions, contradictions - as a more complex abstraction - remain a challenge for these models. This paper goes one step further, as it presents a method to extract TRIZ contradictions from patent texts based on Prompt Engineering using a generative Large Language Model (LLM), namely OpenAI's GPT-4. Contradiction detection, sentence extraction, contradiction summarization, parameter extraction and assignment to the 39 abstract TRIZ engineering parameters are all performed in a single prompt using the LangChain framework. Our results show that \"off-the-shelf\" GPT-4 is a serious alternative to existing approaches.","sentences":["Already since the 1950s TRIZ shows that patents and the technical contradictions they solve are an important source of inspiration for the development of innovative products.","However, TRIZ is a heuristic based on a historic patent analysis and does not make use of the ever-increasing number of latest technological solutions in current patents.","Because of the huge number of patents, their length, and, last but not least, their complexity there is a need for modern patent retrieval and patent analysis to go beyond keyword-oriented methods.","Recent advances in patent retrieval and analysis mainly focus on dense vectors based on neural AI Transformer language models like Google BERT.","They are, for example, used for dense retrieval, question answering or summarization and key concept extraction.","A research focus within the methods for patent summarization and key concept extraction are generic inventive concepts respectively TRIZ concepts like problems, solutions, advantage of invention, parameters, and contradictions.","Succeeding rule-based approaches, finetuned BERT-like language models for sentence-wise classification represent the state-of-the-art of inventive concept extraction.","While they work comparatively well for basic concepts like problems or solutions, contradictions - as a more complex abstraction - remain a challenge for these models.","This paper goes one step further, as it presents a method to extract TRIZ contradictions from patent texts based on Prompt Engineering using a generative Large Language Model (LLM), namely OpenAI's GPT-4.","Contradiction detection, sentence extraction, contradiction summarization, parameter extraction and assignment to the 39 abstract TRIZ engineering parameters are all performed in a single prompt using the LangChain framework.","Our results show that \"off-the-shelf\" GPT-4 is a serious alternative to existing approaches."],"url":"http://arxiv.org/abs/2403.14258v1","category":"cs.CL"}
{"created":"2024-03-21 09:25:24","title":"LayoutLLM: Large Language Model Instruction Tuning for Visually Rich Document Understanding","abstract":"This paper proposes LayoutLLM, a more flexible document analysis method for understanding imaged documents. Visually Rich Document Understanding tasks, such as document image classification and information extraction, have gained significant attention due to their importance. Existing methods have been developed to enhance document comprehension by incorporating pre-training awareness of images, text, and layout structure. However, these methods require fine-tuning for each task and dataset, and the models are expensive to train and operate. To overcome this limitation, we propose a new LayoutLLM that integrates these with large-scale language models (LLMs). By leveraging the strengths of existing research in document image understanding and LLMs' superior language understanding capabilities, the proposed model, fine-tuned with multimodal instruction datasets, performs an understanding of document images in a single model. Our experiments demonstrate improvement over the baseline model in various document analysis tasks.","sentences":["This paper proposes LayoutLLM, a more flexible document analysis method for understanding imaged documents.","Visually Rich Document Understanding tasks, such as document image classification and information extraction, have gained significant attention due to their importance.","Existing methods have been developed to enhance document comprehension by incorporating pre-training awareness of images, text, and layout structure.","However, these methods require fine-tuning for each task and dataset, and the models are expensive to train and operate.","To overcome this limitation, we propose a new LayoutLLM that integrates these with large-scale language models (LLMs).","By leveraging the strengths of existing research in document image understanding and LLMs' superior language understanding capabilities, the proposed model, fine-tuned with multimodal instruction datasets, performs an understanding of document images in a single model.","Our experiments demonstrate improvement over the baseline model in various document analysis tasks."],"url":"http://arxiv.org/abs/2403.14252v1","category":"cs.CL"}
{"created":"2024-03-21 09:22:23","title":"Safeguarding Medical Image Segmentation Datasets against Unauthorized Training via Contour- and Texture-Aware Perturbations","abstract":"The widespread availability of publicly accessible medical images has significantly propelled advancements in various research and clinical fields. Nonetheless, concerns regarding unauthorized training of AI systems for commercial purposes and the duties of patient privacy protection have led numerous institutions to hesitate to share their images. This is particularly true for medical image segmentation (MIS) datasets, where the processes of collection and fine-grained annotation are time-intensive and laborious. Recently, Unlearnable Examples (UEs) methods have shown the potential to protect images by adding invisible shortcuts. These shortcuts can prevent unauthorized deep neural networks from generalizing. However, existing UEs are designed for natural image classification and fail to protect MIS datasets imperceptibly as their protective perturbations are less learnable than important prior knowledge in MIS, e.g., contour and texture features. To this end, we propose an Unlearnable Medical image generation method, termed UMed. UMed integrates the prior knowledge of MIS by injecting contour- and texture-aware perturbations to protect images. Given that our target is to only poison features critical to MIS, UMed requires only minimal perturbations within the ROI and its contour to achieve greater imperceptibility (average PSNR is 50.03) and protective performance (clean average DSC degrades from 82.18% to 6.80%).","sentences":["The widespread availability of publicly accessible medical images has significantly propelled advancements in various research and clinical fields.","Nonetheless, concerns regarding unauthorized training of AI systems for commercial purposes and the duties of patient privacy protection have led numerous institutions to hesitate to share their images.","This is particularly true for medical image segmentation (MIS) datasets, where the processes of collection and fine-grained annotation are time-intensive and laborious.","Recently, Unlearnable Examples (UEs) methods have shown the potential to protect images by adding invisible shortcuts.","These shortcuts can prevent unauthorized deep neural networks from generalizing.","However, existing UEs are designed for natural image classification and fail to protect MIS datasets imperceptibly as their protective perturbations are less learnable than important prior knowledge in MIS, e.g., contour and texture features.","To this end, we propose an Unlearnable Medical image generation method, termed UMed.","UMed integrates the prior knowledge of MIS by injecting contour- and texture-aware perturbations to protect images.","Given that our target is to only poison features critical to MIS, UMed requires only minimal perturbations within the ROI and its contour to achieve greater imperceptibility (average PSNR is 50.03) and protective performance (clean average DSC degrades from 82.18% to 6.80%)."],"url":"http://arxiv.org/abs/2403.14250v1","category":"eess.IV"}
{"created":"2024-03-21 09:06:38","title":"Hint of a long-range dark sector fifth force in the current cosmological data?","abstract":"Many previous studies have explored models of dynamically interacting dark energy scenarios instead of a cosmological constant and cold dark matter. This work aims to offer a comprehensive investigation of the Yukawa-type interaction between dark energy and dark matter where dark matter mass is also a function of a dynamical scalar field value. Unlike previous work, instead of taking an approximate solution, we numerically solve the Klein-Gordon equation that governs the intricate interplay between these two components of the dark sector along with the corresponding perturbations, using a suitable shooting algorithm to determine precise initial conditions. We have conducted a thorough analysis of this interaction using the Markov Chain Monte Carlo method, incorporating diverse datasets such as Planck, BAO, Pantheon+, and SH$_0$ES. Our results, for the first time, reveal a discernible detection of the coupling constant, which encapsulates the strength of the dark sector interaction. We find non-zero $\\beta$ values of 0.0487, 0.0680, 0.0712, and 0.0822 ($68\\%$ Confidence Limit) corresponding to the inclusion of each dataset, respectively. Notably, the addition of SH$_0$ES data led to a significant increase in the prominence of $\\beta$, suggesting the potential for new physics in the dark sector. This study serves as a strategic road map for future endeavours with non-linear cosmology, which will contribute to an enhanced understanding and refinement of constraints on the dark sector interaction.","sentences":["Many previous studies have explored models of dynamically interacting dark energy scenarios instead of a cosmological constant and cold dark matter.","This work aims to offer a comprehensive investigation of the Yukawa-type interaction between dark energy and dark matter where dark matter mass is also a function of a dynamical scalar field value.","Unlike previous work, instead of taking an approximate solution, we numerically solve the Klein-Gordon equation that governs the intricate interplay between these two components of the dark sector along with the corresponding perturbations, using a suitable shooting algorithm to determine precise initial conditions.","We have conducted a thorough analysis of this interaction using the Markov Chain Monte Carlo method, incorporating diverse datasets such as Planck, BAO, Pantheon+, and SH$_0$ES.","Our results, for the first time, reveal a discernible detection of the coupling constant, which encapsulates the strength of the dark sector interaction.","We find non-zero $\\beta$ values of 0.0487, 0.0680, 0.0712, and 0.0822 ($68\\%$ Confidence Limit) corresponding to the inclusion of each dataset, respectively.","Notably, the addition of SH$_0$ES data led to a significant increase in the prominence of $\\beta$, suggesting the potential for new physics in the dark sector.","This study serves as a strategic road map for future endeavours with non-linear cosmology, which will contribute to an enhanced understanding and refinement of constraints on the dark sector interaction."],"url":"http://arxiv.org/abs/2403.14247v1","category":"astro-ph.CO"}
{"created":"2024-03-21 09:06:28","title":"CATSE: A Context-Aware Framework for Causal Target Sound Extraction","abstract":"Target Sound Extraction (TSE) focuses on the problem of separating sources of interest, indicated by a user's cue, from the input mixture. Most existing solutions operate in an offline fashion and are not suited to the low-latency causal processing constraints imposed by applications in live-streamed content such as augmented hearing. We introduce a family of context-aware low-latency causal TSE models suitable for real-time processing. First, we explore the utility of context by providing the TSE model with oracle information about what sound classes make up the input mixture, where the objective of the model is to extract one or more sources of interest indicated by the user. Since the practical applications of oracle models are limited due to their assumptions, we introduce a composite multi-task training objective involving separation and classification losses. Our evaluation involving single- and multi-source extraction shows the benefit of using context information in the model either by means of providing full context or via the proposed multi-task training loss without the need for full context information. Specifically, we show that our proposed model outperforms size- and latency-matched Waveformer, a state-of-the-art model for real-time TSE.","sentences":["Target Sound Extraction (TSE) focuses on the problem of separating sources of interest, indicated by a user's cue, from the input mixture.","Most existing solutions operate in an offline fashion and are not suited to the low-latency causal processing constraints imposed by applications in live-streamed content such as augmented hearing.","We introduce a family of context-aware low-latency causal TSE models suitable for real-time processing.","First, we explore the utility of context by providing the TSE model with oracle information about what sound classes make up the input mixture, where the objective of the model is to extract one or more sources of interest indicated by the user.","Since the practical applications of oracle models are limited due to their assumptions, we introduce a composite multi-task training objective involving separation and classification losses.","Our evaluation involving single- and multi-source extraction shows the benefit of using context information in the model either by means of providing full context or via the proposed multi-task training loss without the need for full context information.","Specifically, we show that our proposed model outperforms size- and latency-matched Waveformer, a state-of-the-art model for real-time TSE."],"url":"http://arxiv.org/abs/2403.14246v1","category":"eess.AS"}
{"created":"2024-03-21 09:04:16","title":"Hopfion-like solutions in de Sitter spacetime","abstract":"We construct electromagnetic field with non-trivial topological properties on de Sitter background. The field is closely related with Hopf fibration. We analyze energy, angular momentum and topological charges for this solution. The paper is a generalization of CQG \\textbf{35} (2018), no. 24, 245010 to de Sitter spacetime.","sentences":["We construct electromagnetic field with non-trivial topological properties on de Sitter background.","The field is closely related with Hopf fibration.","We analyze energy, angular momentum and topological charges for this solution.","The paper is a generalization of CQG \\textbf{35} (2018), no. 24, 245010 to de Sitter spacetime."],"url":"http://arxiv.org/abs/2403.14245v1","category":"gr-qc"}
{"created":"2024-03-21 09:02:31","title":"Isotropic Gaussian Splatting for Real-Time Radiance Field Rendering","abstract":"The 3D Gaussian splatting method has drawn a lot of attention, thanks to its high performance in training and high quality of the rendered image. However, it uses anisotropic Gaussian kernels to represent the scene. Although such anisotropic kernels have advantages in representing the geometry, they lead to difficulties in terms of computation, such as splitting or merging two kernels. In this paper, we propose to use isotropic Gaussian kernels to avoid such difficulties in the computation, leading to a higher performance method. The experiments confirm that the proposed method is about {\\bf 100X} faster without losing the geometry representation accuracy. The proposed method can be applied in a large range applications where the radiance field is needed, such as 3D reconstruction, view synthesis, and dynamic object modeling.","sentences":["The 3D Gaussian splatting method has drawn a lot of attention, thanks to its high performance in training and high quality of the rendered image.","However, it uses anisotropic Gaussian kernels to represent the scene.","Although such anisotropic kernels have advantages in representing the geometry, they lead to difficulties in terms of computation, such as splitting or merging two kernels.","In this paper, we propose to use isotropic Gaussian kernels to avoid such difficulties in the computation, leading to a higher performance method.","The experiments confirm that the proposed method is about {\\bf 100X} faster without losing the geometry representation accuracy.","The proposed method can be applied in a large range applications where the radiance field is needed, such as 3D reconstruction, view synthesis, and dynamic object modeling."],"url":"http://arxiv.org/abs/2403.14244v1","category":"cs.CV"}
{"created":"2024-03-21 09:02:17","title":"Dermacen Analytica: A Novel Methodology Integrating Multi-Modal Large Language Models with Machine Learning in tele-dermatology","abstract":"The rise of Artificial Intelligence creates great promise in the field of medical discovery, diagnostics and patient management. However, the vast complexity of all medical domains require a more complex approach that combines machine learning algorithms, classifiers, segmentation algorithms and, lately, large language models. In this paper, we describe, implement and assess an Artificial Intelligence-empowered system and methodology aimed at assisting the diagnosis process of skin lesions and other skin conditions within the field of dermatology that aims to holistically address the diagnostic process in this domain. The workflow integrates large language, transformer-based vision models and sophisticated machine learning tools. This holistic approach achieves a nuanced interpretation of dermatological conditions that simulates and facilitates a dermatologist's workflow. We assess our proposed methodology through a thorough cross-model validation technique embedded in an evaluation pipeline that utilizes publicly available medical case studies of skin conditions and relevant images. To quantitatively score the system performance, advanced machine learning and natural language processing tools are employed which focus on similarity comparison and natural language inference. Additionally, we incorporate a human expert evaluation process based on a structured checklist to further validate our results. We implemented the proposed methodology in a system which achieved approximate (weighted) scores of 0.87 for both contextual understanding and diagnostic accuracy, demonstrating the efficacy of our approach in enhancing dermatological analysis. The proposed methodology is expected to prove useful in the development of next-generation tele-dermatology applications, enhancing remote consultation capabilities and access to care, especially in underserved areas.","sentences":["The rise of Artificial Intelligence creates great promise in the field of medical discovery, diagnostics and patient management.","However, the vast complexity of all medical domains require a more complex approach that combines machine learning algorithms, classifiers, segmentation algorithms and, lately, large language models.","In this paper, we describe, implement and assess an Artificial Intelligence-empowered system and methodology aimed at assisting the diagnosis process of skin lesions and other skin conditions within the field of dermatology that aims to holistically address the diagnostic process in this domain.","The workflow integrates large language, transformer-based vision models and sophisticated machine learning tools.","This holistic approach achieves a nuanced interpretation of dermatological conditions that simulates and facilitates a dermatologist's workflow.","We assess our proposed methodology through a thorough cross-model validation technique embedded in an evaluation pipeline that utilizes publicly available medical case studies of skin conditions and relevant images.","To quantitatively score the system performance, advanced machine learning and natural language processing tools are employed which focus on similarity comparison and natural language inference.","Additionally, we incorporate a human expert evaluation process based on a structured checklist to further validate our results.","We implemented the proposed methodology in a system which achieved approximate (weighted) scores of 0.87 for both contextual understanding and diagnostic accuracy, demonstrating the efficacy of our approach in enhancing dermatological analysis.","The proposed methodology is expected to prove useful in the development of next-generation tele-dermatology applications, enhancing remote consultation capabilities and access to care, especially in underserved areas."],"url":"http://arxiv.org/abs/2403.14243v1","category":"cs.CL"}
{"created":"2024-03-21 09:01:21","title":"Weak Supervision with Arbitrary Single Frame for Micro- and Macro-expression Spotting","abstract":"Frame-level micro- and macro-expression spotting methods require time-consuming frame-by-frame observation during annotation. Meanwhile, video-level spotting lacks sufficient information about the location and number of expressions during training, resulting in significantly inferior performance compared with fully-supervised spotting. To bridge this gap, we propose a point-level weakly-supervised expression spotting (PWES) framework, where each expression requires to be annotated with only one random frame (i.e., a point). To mitigate the issue of sparse label distribution, the prevailing solution is pseudo-label mining, which, however, introduces new problems: localizing contextual background snippets results in inaccurate boundaries and discarding foreground snippets leads to fragmentary predictions. Therefore, we design the strategies of multi-refined pseudo label generation (MPLG) and distribution-guided feature contrastive learning (DFCL) to address these problems. Specifically, MPLG generates more reliable pseudo labels by merging class-specific probabilities, attention scores, fused features, and point-level labels. DFCL is utilized to enhance feature similarity for the same categories and feature variability for different categories while capturing global representations across the entire datasets. Extensive experiments on the CAS(ME)^2, CAS(ME)^3, and SAMM-LV datasets demonstrate PWES achieves promising performance comparable to that of recent fully-supervised methods.","sentences":["Frame-level micro- and macro-expression spotting methods require time-consuming frame-by-frame observation during annotation.","Meanwhile, video-level spotting lacks sufficient information about the location and number of expressions during training, resulting in significantly inferior performance compared with fully-supervised spotting.","To bridge this gap, we propose a point-level weakly-supervised expression spotting (PWES) framework, where each expression requires to be annotated with only one random frame (i.e., a point).","To mitigate the issue of sparse label distribution, the prevailing solution is pseudo-label mining, which, however, introduces new problems: localizing contextual background snippets results in inaccurate boundaries and discarding foreground snippets leads to fragmentary predictions.","Therefore, we design the strategies of multi-refined pseudo label generation (MPLG) and distribution-guided feature contrastive learning (DFCL) to address these problems.","Specifically, MPLG generates more reliable pseudo labels by merging class-specific probabilities, attention scores, fused features, and point-level labels.","DFCL is utilized to enhance feature similarity for the same categories and feature variability for different categories while capturing global representations across the entire datasets.","Extensive experiments on the CAS(ME)^2, CAS(ME)^3, and SAMM-LV datasets demonstrate PWES achieves promising performance comparable to that of recent fully-supervised methods."],"url":"http://arxiv.org/abs/2403.14240v1","category":"cs.CV"}
{"created":"2024-03-21 08:57:27","title":"Reinforcement Learning from Reflective Feedback (RLRF): Aligning and Improving LLMs via Fine-Grained Self-Reflection","abstract":"Despite the promise of RLHF in aligning LLMs with human preferences, it often leads to superficial alignment, prioritizing stylistic changes over improving downstream performance of LLMs. Underspecified preferences could obscure directions to align the models. Lacking exploration restricts identification of desirable outputs to improve the models. To overcome these challenges, we propose a novel framework: Reinforcement Learning from Reflective Feedback (RLRF), which leverages fine-grained feedback based on detailed criteria to improve the core capabilities of LLMs. RLRF employs a self-reflection mechanism to systematically explore and refine LLM responses, then fine-tuning the models via a RL algorithm along with promising responses. Our experiments across Just-Eval, Factuality, and Mathematical Reasoning demonstrate the efficacy and transformative potential of RLRF beyond superficial surface-level adjustment.","sentences":["Despite the promise of RLHF in aligning LLMs with human preferences, it often leads to superficial alignment, prioritizing stylistic changes over improving downstream performance of LLMs.","Underspecified preferences could obscure directions to align the models.","Lacking exploration restricts identification of desirable outputs to improve the models.","To overcome these challenges, we propose a novel framework: Reinforcement Learning from Reflective Feedback (RLRF), which leverages fine-grained feedback based on detailed criteria to improve the core capabilities of LLMs.","RLRF employs a self-reflection mechanism to systematically explore and refine LLM responses, then fine-tuning the models via a RL algorithm along with promising responses.","Our experiments across Just-Eval, Factuality, and Mathematical Reasoning demonstrate the efficacy and transformative potential of RLRF beyond superficial surface-level adjustment."],"url":"http://arxiv.org/abs/2403.14238v1","category":"cs.CL"}
{"created":"2024-03-21 08:54:24","title":"A Unified Framework for Model Editing","abstract":"Model editing is a growing area focused on updating the knowledge embedded within models. Among the various methodologies, ROME and MEMIT stand out as leading \"locate-and-edit\" model editing techniques. While MEMIT enables batched editing of memories, ROME is limited to changing one fact at a time. This paper introduces a unifying framework that brings ROME and MEMIT under a single conceptual umbrella, optimizing for the same goal, which we call the \"preservation-memorization\" objective. This objective aims to preserve the representations of certain selected vectors while memorizing the representations of new factual information. Specifically, ROME optimizes this objective using an equality constraint, whereas MEMIT employs a more flexible least-square constraint. In addition to making batched edits, MEMIT also edits the model at multiple layers. We disentangle the distribution of edits to multiple layers from the optimization objective of MEMIT and show that these edit-distribution algorithms should be considered separate entities worthy of their own line of research.   Finally, we present EMMET - an Equality-constrained Mass Model Editing algorithm for Transformers, a new batched memory-editing algorithm. With EMMET, we present a closed form solution for the equality-constrained version of the preservation-memorization objective. We show that EMMET is able to perform batched-edits on par with MEMIT up to a batch-size of 256 and discuss the challenges in stabilizing EMMET. By articulating the \"locate-and-edit\" model editing algorithms under a simple conceptual framework of \"preservation-memorization\", we aim to bridge the gap between intuition and mathematics and hope to simplify the journey for future researchers in model editing.","sentences":["Model editing is a growing area focused on updating the knowledge embedded within models.","Among the various methodologies, ROME and MEMIT stand out as leading \"locate-and-edit\" model editing techniques.","While MEMIT enables batched editing of memories, ROME is limited to changing one fact at a time.","This paper introduces a unifying framework that brings ROME and MEMIT under a single conceptual umbrella, optimizing for the same goal, which we call the \"preservation-memorization\" objective.","This objective aims to preserve the representations of certain selected vectors while memorizing the representations of new factual information.","Specifically, ROME optimizes this objective using an equality constraint, whereas MEMIT employs a more flexible least-square constraint.","In addition to making batched edits, MEMIT also edits the model at multiple layers.","We disentangle the distribution of edits to multiple layers from the optimization objective of MEMIT and show that these edit-distribution algorithms should be considered separate entities worthy of their own line of research.   ","Finally, we present EMMET - an Equality-constrained Mass Model Editing algorithm for Transformers, a new batched memory-editing algorithm.","With EMMET, we present a closed form solution for the equality-constrained version of the preservation-memorization objective.","We show that EMMET is able to perform batched-edits on par with MEMIT up to a batch-size of 256 and discuss the challenges in stabilizing EMMET.","By articulating the \"locate-and-edit\" model editing algorithms under a simple conceptual framework of \"preservation-memorization\", we aim to bridge the gap between intuition and mathematics and hope to simplify the journey for future researchers in model editing."],"url":"http://arxiv.org/abs/2403.14236v1","category":"cs.LG"}
{"created":"2024-03-21 08:49:34","title":"SoftPatch: Unsupervised Anomaly Detection with Noisy Data","abstract":"Although mainstream unsupervised anomaly detection (AD) algorithms perform well in academic datasets, their performance is limited in practical application due to the ideal experimental setting of clean training data. Training with noisy data is an inevitable problem in real-world anomaly detection but is seldom discussed. This paper considers label-level noise in image sensory anomaly detection for the first time. To solve this problem, we proposed a memory-based unsupervised AD method, SoftPatch, which efficiently denoises the data at the patch level. Noise discriminators are utilized to generate outlier scores for patch-level noise elimination before coreset construction. The scores are then stored in the memory bank to soften the anomaly detection boundary. Compared with existing methods, SoftPatch maintains a strong modeling ability of normal data and alleviates the overconfidence problem in coreset. Comprehensive experiments in various noise scenes demonstrate that SoftPatch outperforms the state-of-the-art AD methods on the MVTecAD and BTAD benchmarks and is comparable to those methods under the setting without noise.","sentences":["Although mainstream unsupervised anomaly detection (AD) algorithms perform well in academic datasets, their performance is limited in practical application due to the ideal experimental setting of clean training data.","Training with noisy data is an inevitable problem in real-world anomaly detection but is seldom discussed.","This paper considers label-level noise in image sensory anomaly detection for the first time.","To solve this problem, we proposed a memory-based unsupervised AD method, SoftPatch, which efficiently denoises the data at the patch level.","Noise discriminators are utilized to generate outlier scores for patch-level noise elimination before coreset construction.","The scores are then stored in the memory bank to soften the anomaly detection boundary.","Compared with existing methods, SoftPatch maintains a strong modeling ability of normal data and alleviates the overconfidence problem in coreset.","Comprehensive experiments in various noise scenes demonstrate that SoftPatch outperforms the state-of-the-art AD methods on the MVTecAD and BTAD benchmarks and is comparable to those methods under the setting without noise."],"url":"http://arxiv.org/abs/2403.14233v1","category":"cs.CV"}
{"created":"2024-03-21 08:37:15","title":"PeerGPT: Probing the Roles of LLM-based Peer Agents as Team Moderators and Participants in Children's Collaborative Learning","abstract":"In children's collaborative learning, effective peer conversations can significantly enhance the quality of children's collaborative interactions. The integration of Large Language Model (LLM) agents into this setting explores their novel role as peers, assessing impacts as team moderators and participants. We invited two groups of participants to engage in a collaborative learning workshop, where they discussed and proposed conceptual solutions to a design problem. The peer conversation transcripts were analyzed using thematic analysis. We discovered that peer agents, while managing discussions effectively as team moderators, sometimes have their instructions disregarded. As participants, they foster children's creative thinking but may not consistently provide timely feedback. These findings highlight potential design improvements and considerations for peer agents in both roles.","sentences":["In children's collaborative learning, effective peer conversations can significantly enhance the quality of children's collaborative interactions.","The integration of Large Language Model (LLM) agents into this setting explores their novel role as peers, assessing impacts as team moderators and participants.","We invited two groups of participants to engage in a collaborative learning workshop, where they discussed and proposed conceptual solutions to a design problem.","The peer conversation transcripts were analyzed using thematic analysis.","We discovered that peer agents, while managing discussions effectively as team moderators, sometimes have their instructions disregarded.","As participants, they foster children's creative thinking but may not consistently provide timely feedback.","These findings highlight potential design improvements and considerations for peer agents in both roles."],"url":"http://arxiv.org/abs/2403.14227v1","category":"cs.HC"}
{"created":"2024-03-21 08:31:36","title":"Posterior concentrations of fully-connected Bayesian neural networks with general priors on the weights","abstract":"Bayesian approaches for training deep neural networks (BNNs) have received significant interest and have been effectively utilized in a wide range of applications. There have been several studies on the properties of posterior concentrations of BNNs. However, most of these studies only demonstrate results in BNN models with sparse or heavy-tailed priors. Surprisingly, no theoretical results currently exist for BNNs using Gaussian priors, which are the most commonly used one. The lack of theory arises from the absence of approximation results of Deep Neural Networks (DNNs) that are non-sparse and have bounded parameters. In this paper, we present a new approximation theory for non-sparse DNNs with bounded parameters. Additionally, based on the approximation theory, we show that BNNs with non-sparse general priors can achieve near-minimax optimal posterior concentration rates to the true model.","sentences":["Bayesian approaches for training deep neural networks (BNNs) have received significant interest and have been effectively utilized in a wide range of applications.","There have been several studies on the properties of posterior concentrations of BNNs.","However, most of these studies only demonstrate results in BNN models with sparse or heavy-tailed priors.","Surprisingly, no theoretical results currently exist for BNNs using Gaussian priors, which are the most commonly used one.","The lack of theory arises from the absence of approximation results of Deep Neural Networks (DNNs) that are non-sparse and have bounded parameters.","In this paper, we present a new approximation theory for non-sparse DNNs with bounded parameters.","Additionally, based on the approximation theory, we show that BNNs with non-sparse general priors can achieve near-minimax optimal posterior concentration rates to the true model."],"url":"http://arxiv.org/abs/2403.14225v1","category":"stat.ML"}
{"created":"2024-03-21 08:24:10","title":"A comprehensive data-driven odyssey to explore the equation of state of dark energy","abstract":"We study the evolution of the dark energy equation of the state parameter without tying ourselves to any specific cosmological model or parametrization except spatial homogeneity, isotropy, and flatness leading to a flat Friedmann-Lema\\^itre-Robertson-Walker (FLRW) metric. Instead, we rely on actual observational data to guide our analysis. This is the first study in which we combine the cosmological background and the growth observations to reconstruct the equation of the state parameter of dark energy independent of the present values of the matter-energy density parameter and Hubble parameter. We use information about the Hubble parameter from cosmic chronometer data and the growth rate from observations related to growth rates. Our method involves a posterior approach of Gaussian process regression analysis to figure out the Hubble parameter and growth rate, plus their changes with redshift. The significant shift of paradigm in this study lies in the independence of the reconstruction of the dark energy equation of state from any prior knowledge of the present Hubble parameter and matter energy density parameter. We find a slight hint of dynamical behavior in dark energy. However, the evidence is not significant. We also find a leaning towards non-phantom behavior over phantom behavior. Intriguingly, we observe that the $\\Lambda$CDM model nearly touches the lower boundary of the 1$\\sigma$ confidence region for the reconstructed dark energy equation of state parameter in the redshift range $0.6 \\lesssim z \\lesssim 0.85$. However, it comfortably resides within the 1$\\sigma$ confidence region in the redshift range under investigation, $0\\leq z \\leq 1.5$. Consequently, the non-parametric, model-independent reconstruction of dark energy provides no compelling evidence to deviate from the $\\Lambda$CDM model when considering cosmic chronometer and growth rate observations.","sentences":["We study the evolution of the dark energy equation of the state parameter without tying ourselves to any specific cosmological model or parametrization except spatial homogeneity, isotropy, and flatness leading to a flat Friedmann-Lema\\^itre-Robertson-Walker (FLRW) metric.","Instead, we rely on actual observational data to guide our analysis.","This is the first study in which we combine the cosmological background and the growth observations to reconstruct the equation of the state parameter of dark energy independent of the present values of the matter-energy density parameter and Hubble parameter.","We use information about the Hubble parameter from cosmic chronometer data and the growth rate from observations related to growth rates.","Our method involves a posterior approach of Gaussian process regression analysis to figure out the Hubble parameter and growth rate, plus their changes with redshift.","The significant shift of paradigm in this study lies in the independence of the reconstruction of the dark energy equation of state from any prior knowledge of the present Hubble parameter and matter energy density parameter.","We find a slight hint of dynamical behavior in dark energy.","However, the evidence is not significant.","We also find a leaning towards non-phantom behavior over phantom behavior.","Intriguingly, we observe that the $\\Lambda$CDM model nearly touches the lower boundary of the 1$\\sigma$ confidence region for the reconstructed dark energy equation of state parameter in the redshift range $0.6 \\lesssim z \\lesssim 0.85$.","However, it comfortably resides within the 1$\\sigma$ confidence region in the redshift range under investigation, $0\\leq z \\leq","1.5$. Consequently, the non-parametric, model-independent reconstruction of dark energy provides no compelling evidence to deviate from the $\\Lambda$CDM model when considering cosmic chronometer and growth rate observations."],"url":"http://arxiv.org/abs/2403.14223v1","category":"astro-ph.CO"}
{"created":"2024-03-21 08:21:12","title":"Improving the Robustness of Large Language Models via Consistency Alignment","abstract":"Large language models (LLMs) have shown tremendous success in following user instructions and generating helpful responses. Nevertheless, their robustness is still far from optimal, as they may generate significantly inconsistent responses due to minor changes in the verbalized instructions. Recent literature has explored this inconsistency issue, highlighting the importance of continued improvement in the robustness of response generation. However, systematic analysis and solutions are still lacking. In this paper, we quantitatively define the inconsistency problem and propose a two-stage training framework consisting of instruction-augmented supervised fine-tuning and consistency alignment training. The first stage helps a model generalize on following instructions via similar instruction augmentations. In the second stage, we improve the diversity and help the model understand which responses are more aligned with human expectations by differentiating subtle differences in similar responses. The training process is accomplished by self-rewards inferred from the trained model at the first stage without referring to external human preference resources. We conduct extensive experiments on recent publicly available LLMs on instruction-following tasks and demonstrate the effectiveness of our training framework.","sentences":["Large language models (LLMs) have shown tremendous success in following user instructions and generating helpful responses.","Nevertheless, their robustness is still far from optimal, as they may generate significantly inconsistent responses due to minor changes in the verbalized instructions.","Recent literature has explored this inconsistency issue, highlighting the importance of continued improvement in the robustness of response generation.","However, systematic analysis and solutions are still lacking.","In this paper, we quantitatively define the inconsistency problem and propose a two-stage training framework consisting of instruction-augmented supervised fine-tuning and consistency alignment training.","The first stage helps a model generalize on following instructions via similar instruction augmentations.","In the second stage, we improve the diversity and help the model understand which responses are more aligned with human expectations by differentiating subtle differences in similar responses.","The training process is accomplished by self-rewards inferred from the trained model at the first stage without referring to external human preference resources.","We conduct extensive experiments on recent publicly available LLMs on instruction-following tasks and demonstrate the effectiveness of our training framework."],"url":"http://arxiv.org/abs/2403.14221v1","category":"cs.CL"}
{"created":"2024-03-21 08:08:31","title":"Toward Multi-class Anomaly Detection: Exploring Class-aware Unified Model against Inter-class Interference","abstract":"In the context of high usability in single-class anomaly detection models, recent academic research has become concerned about the more complex multi-class anomaly detection. Although several papers have designed unified models for this task, they often overlook the utility of class labels, a potent tool for mitigating inter-class interference. To address this issue, we introduce a Multi-class Implicit Neural representation Transformer for unified Anomaly Detection (MINT-AD), which leverages the fine-grained category information in the training stage. By learning the multi-class distributions, the model generates class-aware query embeddings for the transformer decoder, mitigating inter-class interference within the reconstruction model. Utilizing such an implicit neural representation network, MINT-AD can project category and position information into a feature embedding space, further supervised by classification and prior probability loss functions. Experimental results on multiple datasets demonstrate that MINT-AD outperforms existing unified training models.","sentences":["In the context of high usability in single-class anomaly detection models, recent academic research has become concerned about the more complex multi-class anomaly detection.","Although several papers have designed unified models for this task, they often overlook the utility of class labels, a potent tool for mitigating inter-class interference.","To address this issue, we introduce a Multi-class Implicit Neural representation Transformer for unified Anomaly Detection (MINT-AD), which leverages the fine-grained category information in the training stage.","By learning the multi-class distributions, the model generates class-aware query embeddings for the transformer decoder, mitigating inter-class interference within the reconstruction model.","Utilizing such an implicit neural representation network, MINT-AD can project category and position information into a feature embedding space, further supervised by classification and prior probability loss functions.","Experimental results on multiple datasets demonstrate that MINT-AD outperforms existing unified training models."],"url":"http://arxiv.org/abs/2403.14213v1","category":"cs.CV"}
{"created":"2024-03-21 08:03:48","title":"Phonon-induced band gap renormalization by dielectric dependent global hybrid density functional tight-binding","abstract":"Accurate electronic bandstructures of solids are indispensable for a wide variety of applications and should provide a sound prediction of phonon-induced band gap renormalization at finite temperatures. We employ our previously introduced formalism of general hybrid functionals within the approximate density functional method, DFTB, to present first insights into the accuracy of temperature dependent band gaps obtained by a dielectric-dependent global hybrid functional. The work targets the prototypical group-IV semiconductors diamond and silicon. Following Zacharias et al. [Phys. Rev. Lett. 115, 177401 (2015)], we sample the nuclear wave function by stochastic Monte-Carlo integration as well as the deterministic one-shot procedure [Phys. Rev. B 94, 075125 (2016)] derived from it. The computational efficiency of DFTB enables us to further compare these approaches, which fully take nuclear quantum effects into account, with classical Born-Oppenheimer molecular dynamic (BOMD) simulations. While the quantum mechanical treatments of Zacharias et al. yield band gaps in good agreement with experiment, calculations based on BOMD snapshots inadequately describe the renormalization effect at low temperatures. We demonstrate the importance of properly incorporating nuclear quantum effects by adapting the stochastic approach to normal amplitudes that arise from the classical equipartition principle. For low temperatures the results thus obtained closely resemble the BOMD predictions, while anharmonic effects become important beyond $500\\,\\mathrm{K}$. Comparisons between DFTB parametrized from semi-local DFT, and global hybrid DFTB, suggest that Fock-type exchange systematically yields a slightly more pronounced electron-phonon interaction, hence stronger gap renormalization and zero-point corrections.","sentences":["Accurate electronic bandstructures of solids are indispensable for a wide variety of applications and should provide a sound prediction of phonon-induced band gap renormalization at finite temperatures.","We employ our previously introduced formalism of general hybrid functionals within the approximate density functional method, DFTB, to present first insights into the accuracy of temperature dependent band gaps obtained by a dielectric-dependent global hybrid functional.","The work targets the prototypical group-IV semiconductors diamond and silicon.","Following Zacharias et al.","[Phys. Rev. Lett.","115, 177401 (2015)], we sample the nuclear wave function by stochastic Monte-Carlo integration as well as the deterministic one-shot procedure [Phys.","Rev. B 94, 075125 (2016)] derived from it.","The computational efficiency of DFTB enables us to further compare these approaches, which fully take nuclear quantum effects into account, with classical Born-Oppenheimer molecular dynamic (BOMD) simulations.","While the quantum mechanical treatments of Zacharias et al. yield band gaps in good agreement with experiment, calculations based on BOMD snapshots inadequately describe the renormalization effect at low temperatures.","We demonstrate the importance of properly incorporating nuclear quantum effects by adapting the stochastic approach to normal amplitudes that arise from the classical equipartition principle.","For low temperatures the results thus obtained closely resemble the BOMD predictions, while anharmonic effects become important beyond $500\\,\\mathrm{K}$. Comparisons between DFTB parametrized from semi-local DFT, and global hybrid DFTB, suggest that Fock-type exchange systematically yields a slightly more pronounced electron-phonon interaction, hence stronger gap renormalization and zero-point corrections."],"url":"http://arxiv.org/abs/2403.14210v1","category":"physics.comp-ph"}
{"created":"2024-03-21 07:56:09","title":"Unsupervised Audio-Visual Segmentation with Modality Alignment","abstract":"Audio-Visual Segmentation (AVS) aims to identify, at the pixel level, the object in a visual scene that produces a given sound. Current AVS methods rely on costly fine-grained annotations of mask-audio pairs, making them impractical for scalability. To address this, we introduce unsupervised AVS, eliminating the need for such expensive annotation. To tackle this more challenging problem, we propose an unsupervised learning method, named Modality Correspondence Alignment (MoCA), which seamlessly integrates off-the-shelf foundation models like DINO, SAM, and ImageBind. This approach leverages their knowledge complementarity and optimizes their joint usage for multi-modality association. Initially, we estimate positive and negative image pairs in the feature space. For pixel-level association, we introduce an audio-visual adapter and a novel pixel matching aggregation strategy within the image-level contrastive learning framework. This allows for a flexible connection between object appearance and audio signal at the pixel level, with tolerance to imaging variations such as translation and rotation. Extensive experiments on the AVSBench (single and multi-object splits) and AVSS datasets demonstrate that our MoCA outperforms strongly designed baseline methods and approaches supervised counterparts, particularly in complex scenarios with multiple auditory objects. Notably when comparing mIoU, MoCA achieves a substantial improvement over baselines in both the AVSBench (S4: +17.24%; MS3: +67.64%) and AVSS (+19.23%) audio-visual segmentation challenges.","sentences":["Audio-Visual Segmentation (AVS) aims to identify, at the pixel level, the object in a visual scene that produces a given sound.","Current AVS methods rely on costly fine-grained annotations of mask-audio pairs, making them impractical for scalability.","To address this, we introduce unsupervised AVS, eliminating the need for such expensive annotation.","To tackle this more challenging problem, we propose an unsupervised learning method, named Modality Correspondence Alignment (MoCA), which seamlessly integrates off-the-shelf foundation models like DINO, SAM, and ImageBind.","This approach leverages their knowledge complementarity and optimizes their joint usage for multi-modality association.","Initially, we estimate positive and negative image pairs in the feature space.","For pixel-level association, we introduce an audio-visual adapter and a novel pixel matching aggregation strategy within the image-level contrastive learning framework.","This allows for a flexible connection between object appearance and audio signal at the pixel level, with tolerance to imaging variations such as translation and rotation.","Extensive experiments on the AVSBench (single and multi-object splits) and AVSS datasets demonstrate that our MoCA outperforms strongly designed baseline methods and approaches supervised counterparts, particularly in complex scenarios with multiple auditory objects.","Notably when comparing mIoU, MoCA achieves a substantial improvement over baselines in both the AVSBench (S4:","+17.24%; MS3: +67.64%) and AVSS (+19.23%) audio-visual segmentation challenges."],"url":"http://arxiv.org/abs/2403.14203v1","category":"cs.CV"}
{"created":"2024-03-21 07:52:09","title":"Parametrizing $W$-weighted BT inverse to obtain the $W$-weighted $q$-BT inverse","abstract":"The core-EP and BT inverses for rectangular matrices were studied recently in the literature. The main aim of this paper is to unify both concepts by means of a new kind of generalized inverse called $W$-weighted $q$-BT inverse. We analyze its existence and uniqueness by considering an adequate matrix system. Basic properties and some interesting characterizations are proved for this new weighted generalized inverse. Also, we give a canonical form of the $W$-weighted $q$-BT inverse by means of the weighted core-EP decomposition.","sentences":["The core-EP and BT inverses for rectangular matrices were studied recently in the literature.","The main aim of this paper is to unify both concepts by means of a new kind of generalized inverse called $W$-weighted $q$-BT inverse.","We analyze its existence and uniqueness by considering an adequate matrix system.","Basic properties and some interesting characterizations are proved for this new weighted generalized inverse.","Also, we give a canonical form of the $W$-weighted $q$-BT inverse by means of the weighted core-EP decomposition."],"url":"http://arxiv.org/abs/2403.14201v1","category":"math.RA"}
{"created":"2024-03-21 07:50:45","title":"Debiasing surgeon: fantastic weights and how to find them","abstract":"Nowadays an ever-growing concerning phenomenon, the emergence of algorithmic biases that can lead to unfair models, emerges. Several debiasing approaches have been proposed in the realm of deep learning, employing more or less sophisticated approaches to discourage these models from massively employing these biases. However, a question emerges: is this extra complexity really necessary? Is a vanilla-trained model already embodying some ``unbiased sub-networks'' that can be used in isolation and propose a solution without relying on the algorithmic biases? In this work, we show that such a sub-network typically exists, and can be extracted from a vanilla-trained model without requiring additional training. We further validate that such specific architecture is incapable of learning a specific bias, suggesting that there are possible architectural countermeasures to the problem of biases in deep neural networks.","sentences":["Nowadays an ever-growing concerning phenomenon, the emergence of algorithmic biases that can lead to unfair models, emerges.","Several debiasing approaches have been proposed in the realm of deep learning, employing more or less sophisticated approaches to discourage these models from massively employing these biases.","However, a question emerges: is this extra complexity really necessary?","Is a vanilla-trained model already embodying some ``unbiased sub-networks'' that can be used in isolation and propose a solution without relying on the algorithmic biases?","In this work, we show that such a sub-network typically exists, and can be extracted from a vanilla-trained model without requiring additional training.","We further validate that such specific architecture is incapable of learning a specific bias, suggesting that there are possible architectural countermeasures to the problem of biases in deep neural networks."],"url":"http://arxiv.org/abs/2403.14200v1","category":"cs.LG"}
{"created":"2024-03-21 07:47:57","title":"Context Quality Matters in Training Fusion-in-Decoder for Extractive Open-Domain Question Answering","abstract":"Retrieval-augmented generation models augment knowledge encoded in a language model by providing additional relevant external knowledge (context) during generation. Although it has been shown that the quantity and quality of context impact the performance of retrieval-augmented generation models during inference, limited research explores how these characteristics affect model training. This paper explores how context quantity and quality during model training affect the performance of Fusion-in-Decoder (FiD), the state-of-the-art retrieval-augmented generation model, in extractive open-domain question answering tasks. Experimental results suggest that FiD models overfit to context quality during training and show suboptimal performance when evaluated on different context quality. Through the experimental results, we also reveal FiD models trained with different context quality have different cross-attention distribution patterns. Specifically, as context quality during training increases, FiD models tend to attend more uniformly to each passage in context. Finally, based on these observations, we propose a method to mitigate overfitting to specific context quality by introducing bias to the cross-attention distribution, which we demonstrate to be effective in improving the performance of FiD models on different context quality.","sentences":["Retrieval-augmented generation models augment knowledge encoded in a language model by providing additional relevant external knowledge (context) during generation.","Although it has been shown that the quantity and quality of context impact the performance of retrieval-augmented generation models during inference, limited research explores how these characteristics affect model training.","This paper explores how context quantity and quality during model training affect the performance of Fusion-in-Decoder (FiD), the state-of-the-art retrieval-augmented generation model, in extractive open-domain question answering tasks.","Experimental results suggest that FiD models overfit to context quality during training and show suboptimal performance when evaluated on different context quality.","Through the experimental results, we also reveal FiD models trained with different context quality have different cross-attention distribution patterns.","Specifically, as context quality during training increases, FiD models tend to attend more uniformly to each passage in context.","Finally, based on these observations, we propose a method to mitigate overfitting to specific context quality by introducing bias to the cross-attention distribution, which we demonstrate to be effective in improving the performance of FiD models on different context quality."],"url":"http://arxiv.org/abs/2403.14197v1","category":"cs.CL"}
{"created":"2024-03-21 07:44:15","title":"The $W$-weighted $m$-weak core inverse","abstract":"Recently, Malik and Ferreyra introduced the $m$-weak core inverse for complex square matrices which generalizes the core-EP inverse, the WC inverse, and therefore the core inverse. The main aim of this paper is to extend the concept of $m$-weak core inverse for complex rectangular matrices. This extension is called the $W$-weighted $m$-weak core inverse. We analyse its existence and uniqueness as solution of a system of matrix equations. We present various properties, representations and characterizations of the $W$-weighted $m$-weak core inverse, as well as its applications in solving certain matrix systems. A canonical form of the $W$-weighted $m$-weak core inverse is also provided by using a simultaneous unitary block upper triangularization of a pair of rectangular matrices.","sentences":["Recently, Malik and Ferreyra introduced the $m$-weak core inverse for complex square matrices which generalizes the core-EP inverse, the WC inverse, and therefore the core inverse.","The main aim of this paper is to extend the concept of $m$-weak core inverse for complex rectangular matrices.","This extension is called the $W$-weighted $m$-weak core inverse.","We analyse its existence and uniqueness as solution of a system of matrix equations.","We present various properties, representations and characterizations of the $W$-weighted $m$-weak core inverse, as well as its applications in solving certain matrix systems.","A canonical form of the $W$-weighted $m$-weak core inverse is also provided by using a simultaneous unitary block upper triangularization of a pair of rectangular matrices."],"url":"http://arxiv.org/abs/2403.14196v1","category":"math.RA"}
{"created":"2024-03-21 07:39:38","title":"Radiative inverse seesaw model with hidden $U(1)$ gauge symmetry enhancing lepton $g-2$","abstract":"We propose a new inverse seesaw model based on hidden local $U(1)$ symmetry framework where inverse seesaw mechanism is induced at one loop level. A Majorana mass term of singlet fermion is forbidden by the $U(1)$ symmetry and it is generated at one-loop level by introducing relevant particle contents to get loop diagram, inducing inverse seesaw mechanism. The same particle contents also contribute to lepton magnetic(electric) dipole moment and lepton flavor violating decays without chiral suppression. We can then obtain sizable muon anomalous magnetic dipole moment that accommodate with deviation from the standard model prediction. The constraints from lepton flavor violating decays and electron magnetic(electric) dipole moment are also discussed to explore testability of the model.","sentences":["We propose a new inverse seesaw model based on hidden local $U(1)$ symmetry framework where inverse seesaw mechanism is induced at one loop level.","A Majorana mass term of singlet fermion is forbidden by the $U(1)$ symmetry and it is generated at one-loop level by introducing relevant particle contents to get loop diagram, inducing inverse seesaw mechanism.","The same particle contents also contribute to lepton magnetic(electric) dipole moment and lepton flavor violating decays without chiral suppression.","We can then obtain sizable muon anomalous magnetic dipole moment that accommodate with deviation from the standard model prediction.","The constraints from lepton flavor violating decays and electron magnetic(electric) dipole moment are also discussed to explore testability of the model."],"url":"http://arxiv.org/abs/2403.14193v1","category":"hep-ph"}
{"created":"2024-03-21 07:35:53","title":"Fundamentals of Delay-Doppler Communications: Practical Implementation and Extensions to OTFS","abstract":"The recently proposed orthogonal time frequency space (OTFS) modulation, which is a typical Delay-Doppler (DD) communication scheme, has attracted significant attention thanks to its appealing performance over doubly-selective channels. In this paper, we present the fundamentals of general DD communications from the viewpoint of the Zak transform. We start our study by constructing DD domain basis functions aligning with the time-frequency (TF)-consistency condition, which are globally quasi-periodic and locally twisted-shifted. We unveil that these features are translated to unique signal structures in both time and frequency, which are beneficial for communication purposes. Then, we focus on the practical implementations of DD Nyquist communications, where we show that rectangular windows achieve perfect DD orthogonality, while truncated periodic signals can obtain sufficient DD orthogonality. Particularly, smoothed rectangular window with excess bandwidth can result in a slightly worse orthogonality but better pulse localization in the DD domain. Furthermore, we present a practical pulse shaping framework for general DD communications and derive the corresponding input-output relation under various shaping pulses. Our numerical results agree with our derivations and also demonstrate advantages of DD communications over conventional orthogonal frequency-division multiplexing (OFDM).","sentences":["The recently proposed orthogonal time frequency space (OTFS) modulation, which is a typical Delay-Doppler (DD) communication scheme, has attracted significant attention thanks to its appealing performance over doubly-selective channels.","In this paper, we present the fundamentals of general DD communications from the viewpoint of the Zak transform.","We start our study by constructing DD domain basis functions aligning with the time-frequency (TF)-consistency condition, which are globally quasi-periodic and locally twisted-shifted.","We unveil that these features are translated to unique signal structures in both time and frequency, which are beneficial for communication purposes.","Then, we focus on the practical implementations of DD Nyquist communications, where we show that rectangular windows achieve perfect DD orthogonality, while truncated periodic signals can obtain sufficient DD orthogonality.","Particularly, smoothed rectangular window with excess bandwidth can result in a slightly worse orthogonality but better pulse localization in the DD domain.","Furthermore, we present a practical pulse shaping framework for general DD communications and derive the corresponding input-output relation under various shaping pulses.","Our numerical results agree with our derivations and also demonstrate advantages of DD communications over conventional orthogonal frequency-division multiplexing (OFDM)."],"url":"http://arxiv.org/abs/2403.14192v1","category":"cs.IT"}
{"created":"2024-03-21 07:29:05","title":"Aging suppression in Multistrip Multigap Resistive Plate Chambers for high counting rate experiments","abstract":"A long term operation of Multi-Strip Multi-Gap Resistive Plate Chambers (MSMGRPC) with gas mixtures based on C2H2F4 and SF6 leads to aging effects, observed as depositions on the surface of the resistive electrodes. Moreover, enhanced depositions and higher noise rates were evidenced around the nylon spacers used for defining the gas gaps between the resistive electrodes. The aging effects are reflected in an increase of the dark current and dark counting rate, with negative impact on the long term performance of the chamber and data volume in a free running readout mode operation. MSMGRPC prototypes designed with a direct gas flow through the gas gaps and minimization of the number of spacers in the active area were developed as mitigation solution. Prototypes with this new design and different granularities were assembled using fishing line as spacers and investigated for aging effects. Although a significant reduction in the dark current and dark counting rate was evidenced, dark counting rate localized around the fishing line spacers remains. In this paper, a new generation of direct flow chambers based on discrete spacers is presented. The results of their aging investigations show that, even at lower gas flows, the aging effects become negligible.","sentences":["A long term operation of Multi-Strip Multi-Gap Resistive Plate Chambers (MSMGRPC) with gas mixtures based on C2H2F4 and SF6 leads to aging effects, observed as depositions on the surface of the resistive electrodes.","Moreover, enhanced depositions and higher noise rates were evidenced around the nylon spacers used for defining the gas gaps between the resistive electrodes.","The aging effects are reflected in an increase of the dark current and dark counting rate, with negative impact on the long term performance of the chamber and data volume in a free running readout mode operation.","MSMGRPC prototypes designed with a direct gas flow through the gas gaps and minimization of the number of spacers in the active area were developed as mitigation solution.","Prototypes with this new design and different granularities were assembled using fishing line as spacers and investigated for aging effects.","Although a significant reduction in the dark current and dark counting rate was evidenced, dark counting rate localized around the fishing line spacers remains.","In this paper, a new generation of direct flow chambers based on discrete spacers is presented.","The results of their aging investigations show that, even at lower gas flows, the aging effects become negligible."],"url":"http://arxiv.org/abs/2403.14190v1","category":"physics.ins-det"}
{"created":"2024-03-21 07:25:52","title":"Quantum-activated neural reservoirs on-chip open up large hardware security models for resilient authentication","abstract":"Quantum artificial intelligence is a frontier of artificial intelligence research, pioneering quantum AI-powered circuits to address problems beyond the reach of deep learning with classical architectures. This work implements a large-scale quantum-activated recurrent neural network possessing more than 3 trillion hardware nodes/cm$^2$, originating from repeatable atomic-scale nucleation dynamics in an amorphous material integrated on-chip, controlled with 0.07 nW electric power per readout channel. Compared to the best-performing reservoirs currently reported, this implementation increases the scale of the network by two orders of magnitude and reduces the power consumption by six, reaching power efficiencies in the range of the human brain, dissipating 0.2 nW/neuron. When interrogated by a classical input, the chip implements a large-scale hardware security model, enabling dictionary-free authentication secure against statistical inference attacks, including AI's present and future development, even for an adversary with a copy of all the classical components available. Experimental tests report 99.6% reliability, 100% user authentication accuracy, and an ideal 50% key uniqueness. Due to its quantum nature, the chip supports a bit density per feature size area three times higher than the best technology available, with the capacity to store more than $2^{1104}$ keys in a footprint of 1 cm$^2$. Such a quantum-powered platform could help counteract the emerging form of warfare led by the cybercrime industry in breaching authentication to target small to large-scale facilities, from private users to intelligent energy grids.","sentences":["Quantum artificial intelligence is a frontier of artificial intelligence research, pioneering quantum AI-powered circuits to address problems beyond the reach of deep learning with classical architectures.","This work implements a large-scale quantum-activated recurrent neural network possessing more than 3 trillion hardware nodes/cm$^2$, originating from repeatable atomic-scale nucleation dynamics in an amorphous material integrated on-chip, controlled with 0.07 nW electric power per readout channel.","Compared to the best-performing reservoirs currently reported, this implementation increases the scale of the network by two orders of magnitude and reduces the power consumption by six, reaching power efficiencies in the range of the human brain, dissipating 0.2 nW/neuron.","When interrogated by a classical input, the chip implements a large-scale hardware security model, enabling dictionary-free authentication secure against statistical inference attacks, including AI's present and future development, even for an adversary with a copy of all the classical components available.","Experimental tests report 99.6% reliability, 100% user authentication accuracy, and an ideal 50% key uniqueness.","Due to its quantum nature, the chip supports a bit density per feature size area three times higher than the best technology available, with the capacity to store more than $2^{1104}$ keys in a footprint of 1 cm$^2$. Such a quantum-powered platform could help counteract the emerging form of warfare led by the cybercrime industry in breaching authentication to target small to large-scale facilities, from private users to intelligent energy grids."],"url":"http://arxiv.org/abs/2403.14188v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-21 07:21:51","title":"StyleCineGAN: Landscape Cinemagraph Generation using a Pre-trained StyleGAN","abstract":"We propose a method that can generate cinemagraphs automatically from a still landscape image using a pre-trained StyleGAN. Inspired by the success of recent unconditional video generation, we leverage a powerful pre-trained image generator to synthesize high-quality cinemagraphs. Unlike previous approaches that mainly utilize the latent space of a pre-trained StyleGAN, our approach utilizes its deep feature space for both GAN inversion and cinemagraph generation. Specifically, we propose multi-scale deep feature warping (MSDFW), which warps the intermediate features of a pre-trained StyleGAN at different resolutions. By using MSDFW, the generated cinemagraphs are of high resolution and exhibit plausible looping animation. We demonstrate the superiority of our method through user studies and quantitative comparisons with state-of-the-art cinemagraph generation methods and a video generation method that uses a pre-trained StyleGAN.","sentences":["We propose a method that can generate cinemagraphs automatically from a still landscape image using a pre-trained StyleGAN.","Inspired by the success of recent unconditional video generation, we leverage a powerful pre-trained image generator to synthesize high-quality cinemagraphs.","Unlike previous approaches that mainly utilize the latent space of a pre-trained StyleGAN, our approach utilizes its deep feature space for both GAN inversion and cinemagraph generation.","Specifically, we propose multi-scale deep feature warping (MSDFW), which warps the intermediate features of a pre-trained StyleGAN at different resolutions.","By using MSDFW, the generated cinemagraphs are of high resolution and exhibit plausible looping animation.","We demonstrate the superiority of our method through user studies and quantitative comparisons with state-of-the-art cinemagraph generation methods and a video generation method that uses a pre-trained StyleGAN."],"url":"http://arxiv.org/abs/2403.14186v1","category":"cs.CV"}
{"created":"2024-03-21 07:21:42","title":"A LiDAR-Aided Channel Model for Vehicular Intelligent Sensing-Communication Integration","abstract":"In this paper, a novel channel modeling approach, named light detection and ranging (LiDAR)-aided geometry-based stochastic modeling (LA-GBSM), is developed. Based on the developed LA-GBSM approach, a new millimeter wave (mmWave) channel model for sixth-generation (6G) vehicular intelligent sensing-communication integration is proposed, which can support the design of intelligent transportation systems (ITSs). The proposed LA-GBSM is accurately parameterized under high, medium, and low vehicular traffic density (VTD) conditions via a sensing-communication simulation dataset with LiDAR point clouds and scatterer information for the first time. Specifically, by detecting dynamic vehicles and static building/tress through LiDAR point clouds via machine learning, scatterers are divided into static and dynamic scatterers. Furthermore, statistical distributions of parameters, e.g., distance, angle, number, and power, related to static and dynamic scatterers are quantified under high, medium, and low VTD conditions. To mimic channel non-stationarity and consistency, based on the quantified statistical distributions, a new visibility region (VR)-based algorithm in consideration of newly generated static/dynamic scatterers is developed. Key channel statistics are derived and simulated. By comparing simulation results and ray-tracing (RT)-based results, the utility of the proposed LA-GBSM is verified.","sentences":["In this paper, a novel channel modeling approach, named light detection and ranging (LiDAR)-aided geometry-based stochastic modeling (LA-GBSM), is developed.","Based on the developed LA-GBSM approach, a new millimeter wave (mmWave) channel model for sixth-generation (6G) vehicular intelligent sensing-communication integration is proposed, which can support the design of intelligent transportation systems (ITSs).","The proposed LA-GBSM is accurately parameterized under high, medium, and low vehicular traffic density (VTD) conditions via a sensing-communication simulation dataset with LiDAR point clouds and scatterer information for the first time.","Specifically, by detecting dynamic vehicles and static building/tress through LiDAR point clouds via machine learning, scatterers are divided into static and dynamic scatterers.","Furthermore, statistical distributions of parameters, e.g., distance, angle, number, and power, related to static and dynamic scatterers are quantified under high, medium, and low VTD conditions.","To mimic channel non-stationarity and consistency, based on the quantified statistical distributions, a new visibility region (VR)-based algorithm in consideration of newly generated static/dynamic scatterers is developed.","Key channel statistics are derived and simulated.","By comparing simulation results and ray-tracing (RT)-based results, the utility of the proposed LA-GBSM is verified."],"url":"http://arxiv.org/abs/2403.14185v1","category":"eess.SP"}
{"created":"2024-03-21 07:20:14","title":"Conservative Linear Envelopes for High-Dimensional, Hamilton-Jacobi Reachability for Nonlinear Systems via the Hopf Formula","abstract":"Hamilton-Jacobi reachability (HJR) analysis provides a value function that encodes (1) the set of states from which a nonlinear system with bounded control inputs can reach a goal (or avoid a failure set) despite any bounded disturbance, and (2) the corresponding optimal control policy to reach (or avoid). Though powerful, traditional methods for HJR rely on dynamic programming and suffer from exponential computation growth with respect to state dimension. The recently favored Hopf formula mitigates this ''curse of dimensionality'' by providing an efficient and space-parallelizable approach for solving the reachability problem. However, the Hopf formula can only be applied to linear time-varying systems. To overcome this limitation, we show that the error between a nonlinear system and a linear model can be transformed into an adversarial bounded artificial disturbance, making an envelope of the true value. One may then solve the dimension-robust Hopf formula for a linear game with this ''antagonistic error\" to perform guaranteed conservative reachability analysis and control synthesis of nonlinear systems; this can be done for problem formulations in which no other HJR method is both computationally feasible and guaranteed. In addition, we offer several technical methods for reducing conservativeness in the analysis. We demonstrate the theory by solving the safe linear envelope in the controlled Van der Pol system, where the true reachable set may be observed, and by solving a 5 agent (15D) pursuit-evasion game with Dubins cars.","sentences":["Hamilton-Jacobi reachability (HJR) analysis provides a value function that encodes (1) the set of states from which a nonlinear system with bounded control inputs can reach a goal (or avoid a failure set) despite any bounded disturbance, and (2) the corresponding optimal control policy to reach (or avoid).","Though powerful, traditional methods for HJR rely on dynamic programming and suffer from exponential computation growth with respect to state dimension.","The recently favored Hopf formula mitigates this ''curse of dimensionality'' by providing an efficient and space-parallelizable approach for solving the reachability problem.","However, the Hopf formula can only be applied to linear time-varying systems.","To overcome this limitation, we show that the error between a nonlinear system and a linear model can be transformed into an adversarial bounded artificial disturbance, making an envelope of the true value.","One may then solve the dimension-robust Hopf formula for a linear game with this ''antagonistic error\" to perform guaranteed conservative reachability analysis and control synthesis of nonlinear systems; this can be done for problem formulations in which no other HJR method is both computationally feasible and guaranteed.","In addition, we offer several technical methods for reducing conservativeness in the analysis.","We demonstrate the theory by solving the safe linear envelope in the controlled Van der Pol system, where the true reachable set may be observed, and by solving a 5 agent (15D) pursuit-evasion game with Dubins cars."],"url":"http://arxiv.org/abs/2403.14184v1","category":"eess.SY"}
{"created":"2024-03-21 07:15:37","title":"OTSeg: Multi-prompt Sinkhorn Attention for Zero-Shot Semantic Segmentation","abstract":"The recent success of CLIP has demonstrated promising results in zero-shot semantic segmentation by transferring muiltimodal knowledge to pixel-level classification. However, leveraging pre-trained CLIP knowledge to closely align text embeddings with pixel embeddings still has limitations in existing approaches. To address this issue, we propose OTSeg, a novel multimodal attention mechanism aimed at enhancing the potential of multiple text prompts for matching associated pixel embeddings. We first propose Multi-Prompts Sinkhorn (MPS) based on the Optimal Transport (OT) algorithm, which leads multiple text prompts to selectively focus on various semantic features within image pixels. Moreover, inspired by the success of Sinkformers in unimodal settings, we introduce the extension of MPS, called Multi-Prompts Sinkhorn Attention (MPSA), which effectively replaces cross-attention mechanisms within Transformer framework in multimodal settings. Through extensive experiments, we demonstrate that OTSeg achieves state-of-the-art (SOTA) performance with significant gains on Zero-Shot Semantic Segmentation (ZS3) tasks across three benchmark datasets.","sentences":["The recent success of CLIP has demonstrated promising results in zero-shot semantic segmentation by transferring muiltimodal knowledge to pixel-level classification.","However, leveraging pre-trained CLIP knowledge to closely align text embeddings with pixel embeddings still has limitations in existing approaches.","To address this issue, we propose OTSeg, a novel multimodal attention mechanism aimed at enhancing the potential of multiple text prompts for matching associated pixel embeddings.","We first propose Multi-Prompts Sinkhorn (MPS) based on the Optimal Transport (OT) algorithm, which leads multiple text prompts to selectively focus on various semantic features within image pixels.","Moreover, inspired by the success of Sinkformers in unimodal settings, we introduce the extension of MPS, called Multi-Prompts Sinkhorn Attention (MPSA), which effectively replaces cross-attention mechanisms within Transformer framework in multimodal settings.","Through extensive experiments, we demonstrate that OTSeg achieves state-of-the-art (SOTA) performance with significant gains on Zero-Shot Semantic Segmentation (ZS3) tasks across three benchmark datasets."],"url":"http://arxiv.org/abs/2403.14183v1","category":"cs.CV"}
{"created":"2024-03-21 07:11:13","title":"Genetic diversity of barley accessions and their response under abiotic stresses using different approaches","abstract":"In this investigation, five separate experiments were carried out. The first experiments were examined the molecular characteristics of 59 barley accessions collected from different regions in Iraq using three different molecular markers (ISSR, CDDP, and Scot). A total of 391 amplified polymorphic bands were generated using forty-four ISSR, nine CDDP, and twelve Scot primers, which they totally observed 255, 35, and 101 polymorphic bands respectively. The mean values of PIC for ISSR, CDDP, and Scot markers were 0.74, 0.63, and 0.80, respectively, indicating the efficiency of the underlying markers in detecting polymorphic status among the studied barley accessions. Based on the respective markers, the barley accessions were classified and clustered into two main groups using the UPGMA and population structure analysis. Results of claustral analyses showed that the variation patterns corresponded with the geographical distribution of barley accessions.","sentences":["In this investigation, five separate experiments were carried out.","The first experiments were examined the molecular characteristics of 59 barley accessions collected from different regions in Iraq using three different molecular markers (ISSR, CDDP, and Scot).","A total of 391 amplified polymorphic bands were generated using forty-four ISSR, nine CDDP, and twelve Scot primers, which they totally observed 255, 35, and 101 polymorphic bands respectively.","The mean values of PIC for ISSR, CDDP, and Scot markers were 0.74, 0.63, and 0.80, respectively, indicating the efficiency of the underlying markers in detecting polymorphic status among the studied barley accessions.","Based on the respective markers, the barley accessions were classified and clustered into two main groups using the UPGMA and population structure analysis.","Results of claustral analyses showed that the variation patterns corresponded with the geographical distribution of barley accessions."],"url":"http://arxiv.org/abs/2403.14181v1","category":"q-bio.GN"}
{"created":"2024-03-21 07:09:02","title":"Adaptive Target Detection for FDA-MIMO Radar with Training Data in Gaussian noise","abstract":"This paper addresses the problem of detecting a moving target embedded in Gaussian noise with an unknown covariance matrix for frequency diverse array multiple-input multiple-output (FDA-MIMO) radar. To end it, assume that obtaining a set of training data is available. Moreover, we propose three adaptive detectors in accordance with the one-step generalized likelihood ratio test (GLRT), two-step GLRT, and Rao criteria, namely OGLRT, TGLRT, and Rao. The LH adaptive matched filter (LHAMF) detector is also introduced when decomposing the Rao test. Next, all provided detectors have constant false alarm rate (CFAR) properties against the covariance matrix. Besides, the closed-form expressions for false alarm probability (PFA) and detection probability (PD) are derived. Finally, this paper substantiates the correctness of the aforementioned algorithms through numerical simulations.","sentences":["This paper addresses the problem of detecting a moving target embedded in Gaussian noise with an unknown covariance matrix for frequency diverse array multiple-input multiple-output (FDA-MIMO) radar.","To end it, assume that obtaining a set of training data is available.","Moreover, we propose three adaptive detectors in accordance with the one-step generalized likelihood ratio test (GLRT), two-step GLRT, and Rao criteria, namely OGLRT, TGLRT, and Rao.","The LH adaptive matched filter (LHAMF) detector is also introduced when decomposing the Rao test.","Next, all provided detectors have constant false alarm rate (CFAR) properties against the covariance matrix.","Besides, the closed-form expressions for false alarm probability (PFA) and detection probability (PD) are derived.","Finally, this paper substantiates the correctness of the aforementioned algorithms through numerical simulations."],"url":"http://arxiv.org/abs/2403.14180v1","category":"eess.SP"}
{"created":"2024-03-21 07:03:52","title":"Generalized multiscale finite element method for a nonlinear elastic strain-limiting Cosserat model","abstract":"For nonlinear Cosserat elasticity, we consider multiscale methods in this paper. In particular, we explore the generalized multiscale finite element method (GMsFEM) to solve an isotropic Cosserat problem with strain-limiting property (ensuring bounded linearized strains even under high stresses). Such strain-limiting Cosserat model can find potential applications in solids and biological fibers. However, Cosserat media with naturally rotational degrees of freedom, nonlinear constitutive relations, high contrast, and heterogeneities may produce challenging multiscale characteristics in the solution, and upscaling by multiscale methods is necessary. Therefore, we utilize the offline and residual-based online (adaptive or uniform) GMsFEM in this context while handling the nonlinearity by Picard iteration. Through various two-dimensional experiments (for perforated, composite, and stochastically heterogeneous media with small and big strain-limiting parameters), our numerical results show the approaches' convergence, efficiency, and robustness. In addition, these results demonstrate that such approaches provide good accuracy, the online GMsFEM gives more accurate solutions than the offline one, and the online adaptive strategy has similar accuracy to the uniform one but with fewer degrees of freedom.","sentences":["For nonlinear Cosserat elasticity, we consider multiscale methods in this paper.","In particular, we explore the generalized multiscale finite element method (GMsFEM) to solve an isotropic Cosserat problem with strain-limiting property (ensuring bounded linearized strains even under high stresses).","Such strain-limiting Cosserat model can find potential applications in solids and biological fibers.","However, Cosserat media with naturally rotational degrees of freedom, nonlinear constitutive relations, high contrast, and heterogeneities may produce challenging multiscale characteristics in the solution, and upscaling by multiscale methods is necessary.","Therefore, we utilize the offline and residual-based online (adaptive or uniform) GMsFEM in this context while handling the nonlinearity by Picard iteration.","Through various two-dimensional experiments (for perforated, composite, and stochastically heterogeneous media with small and big strain-limiting parameters), our numerical results show the approaches' convergence, efficiency, and robustness.","In addition, these results demonstrate that such approaches provide good accuracy, the online GMsFEM gives more accurate solutions than the offline one, and the online adaptive strategy has similar accuracy to the uniform one but with fewer degrees of freedom."],"url":"http://arxiv.org/abs/2403.14178v1","category":"math.NA"}
{"created":"2024-03-21 07:00:59","title":"Prediction of discretization of online GMsFEM using deep learning for Richards equation","abstract":"We develop a new coarse-scale approximation strategy for the nonlinear single-continuum Richards equation as an unsaturated flow over heterogeneous non-periodic media, using the online generalized multiscale finite element method (online GMsFEM) together with deep learning. A novelty of this approach is that local online multiscale basis functions are computed rapidly and frequently by utilizing deep neural networks (DNNs). More precisely, we employ the training set of stochastic permeability realizations and the computed relating online multiscale basis functions to train neural networks. The nonlinear map between such permeability fields and online multiscale basis functions is developed by our proposed deep learning algorithm. That is, in a new way, the predicted online multiscale basis functions incorporate the nonlinearity treatment of the Richards equation and refect any time-dependent changes in the problem's properties. Multiple numerical experiments in two-dimensional model problems show the good performance of this technique, in terms of predictions of the online multiscale basis functions and thus finding solutions.","sentences":["We develop a new coarse-scale approximation strategy for the nonlinear single-continuum Richards equation as an unsaturated flow over heterogeneous non-periodic media, using the online generalized multiscale finite element method (online GMsFEM) together with deep learning.","A novelty of this approach is that local online multiscale basis functions are computed rapidly and frequently by utilizing deep neural networks (DNNs).","More precisely, we employ the training set of stochastic permeability realizations and the computed relating online multiscale basis functions to train neural networks.","The nonlinear map between such permeability fields and online multiscale basis functions is developed by our proposed deep learning algorithm.","That is, in a new way, the predicted online multiscale basis functions incorporate the nonlinearity treatment of the Richards equation and refect any time-dependent changes in the problem's properties.","Multiple numerical experiments in two-dimensional model problems show the good performance of this technique, in terms of predictions of the online multiscale basis functions and thus finding solutions."],"url":"http://arxiv.org/abs/2403.14177v1","category":"math.NA"}
{"created":"2024-03-21 06:53:40","title":"Unified Static and Dynamic Network: Efficient Temporal Filtering for Video Grounding","abstract":"Inspired by the activity-silent and persistent activity mechanisms in human visual perception biology, we design a Unified Static and Dynamic Network (UniSDNet), to learn the semantic association between the video and text/audio queries in a cross-modal environment for efficient video grounding. For static modeling, we devise a novel residual structure (ResMLP) to boost the global comprehensive interaction between the video segments and queries, achieving more effective semantic enhancement/supplement. For dynamic modeling, we effectively exploit three characteristics of the persistent activity mechanism in our network design for a better video context comprehension. Specifically, we construct a diffusely connected video clip graph on the basis of 2D sparse temporal masking to reflect the \"short-term effect\" relationship. We innovatively consider the temporal distance and relevance as the joint \"auxiliary evidence clues\" and design a multi-kernel Temporal Gaussian Filter to expand the context clue into high-dimensional space, simulating the \"complex visual perception\", and then conduct element level filtering convolution operations on neighbour clip nodes in message passing stage for finally generating and ranking the candidate proposals. Our UniSDNet is applicable to both Natural Language Video Grounding (NLVG) and Spoken Language Video Grounding (SLVG) tasks. Our UniSDNet achieves SOTA performance on three widely used datasets for NLVG, as well as three datasets for SLVG, e.g., reporting new records at 38.88% R@1,IoU@0.7 on ActivityNet Captions and 40.26% R@1,IoU@0.5 on TACoS. To facilitate this field, we collect two new datasets (Charades-STA Speech and TACoS Speech) for SLVG task. Meanwhile, the inference speed of our UniSDNet is 1.56$\\times$ faster than the strong multi-query benchmark. Code is available at: https://github.com/xian-sh/UniSDNet.","sentences":["Inspired by the activity-silent and persistent activity mechanisms in human visual perception biology, we design a Unified Static and Dynamic Network (UniSDNet), to learn the semantic association between the video and text/audio queries in a cross-modal environment for efficient video grounding.","For static modeling, we devise a novel residual structure (ResMLP) to boost the global comprehensive interaction between the video segments and queries, achieving more effective semantic enhancement/supplement.","For dynamic modeling, we effectively exploit three characteristics of the persistent activity mechanism in our network design for a better video context comprehension.","Specifically, we construct a diffusely connected video clip graph on the basis of 2D sparse temporal masking to reflect the \"short-term effect\" relationship.","We innovatively consider the temporal distance and relevance as the joint \"auxiliary evidence clues\" and design a multi-kernel Temporal Gaussian Filter to expand the context clue into high-dimensional space, simulating the \"complex visual perception\", and then conduct element level filtering convolution operations on neighbour clip nodes in message passing stage for finally generating and ranking the candidate proposals.","Our UniSDNet is applicable to both Natural Language Video Grounding (NLVG) and Spoken Language Video Grounding (SLVG) tasks.","Our UniSDNet achieves SOTA performance on three widely used datasets for NLVG, as well as three datasets for SLVG, e.g., reporting new records at 38.88% R@1,IoU@0.7 on ActivityNet Captions and 40.26% R@1,IoU@0.5 on TACoS. To facilitate this field, we collect two new datasets (Charades-STA Speech and TACoS Speech) for SLVG task.","Meanwhile, the inference speed of our UniSDNet is 1.56$\\times$ faster than the strong multi-query benchmark.","Code is available at: https://github.com/xian-sh/UniSDNet."],"url":"http://arxiv.org/abs/2403.14174v1","category":"cs.CV"}
{"created":"2024-03-21 06:43:59","title":"M$^3$AV: A Multimodal, Multigenre, and Multipurpose Audio-Visual Academic Lecture Dataset","abstract":"Publishing open-source academic video recordings is an emergent and prevalent approach to sharing knowledge online. Such videos carry rich multimodal information including speech, the facial and body movements of the speakers, as well as the texts and pictures in the slides and possibly even the papers. Although multiple academic video datasets have been constructed and released, few of them support both multimodal content recognition and understanding tasks, which is partially due to the lack of high-quality human annotations. In this paper, we propose a novel multimodal, multigenre, and multipurpose audio-visual academic lecture dataset (M$^3$AV), which has almost 367 hours of videos from five sources covering computer science, mathematics, and medical and biology topics. With high-quality human annotations of the spoken and written words, in particular high-valued name entities, the dataset can be used for multiple audio-visual recognition and understanding tasks. Evaluations performed on contextual speech recognition, speech synthesis, and slide and script generation tasks demonstrate that the diversity of M$^3$AV makes it a challenging dataset.","sentences":["Publishing open-source academic video recordings is an emergent and prevalent approach to sharing knowledge online.","Such videos carry rich multimodal information including speech, the facial and body movements of the speakers, as well as the texts and pictures in the slides and possibly even the papers.","Although multiple academic video datasets have been constructed and released, few of them support both multimodal content recognition and understanding tasks, which is partially due to the lack of high-quality human annotations.","In this paper, we propose a novel multimodal, multigenre, and multipurpose audio-visual academic lecture dataset (M$^3$AV), which has almost 367 hours of videos from five sources covering computer science, mathematics, and medical and biology topics.","With high-quality human annotations of the spoken and written words, in particular high-valued name entities, the dataset can be used for multiple audio-visual recognition and understanding tasks.","Evaluations performed on contextual speech recognition, speech synthesis, and slide and script generation tasks demonstrate that the diversity of M$^3$AV makes it a challenging dataset."],"url":"http://arxiv.org/abs/2403.14168v1","category":"cs.CL"}
{"created":"2024-03-21 17:29:20","title":"Application of the VMM ASIC for SiPM-based calorimetry","abstract":"Highly integrated multichannel readout electronics is crucial in contemporary particle physics experiments. A novel silicon photomultiplier readout system based on the VMM3a ASIC was developed, for the first time exploiting this chip for calorimetric purposes. To extend the dynamic range the signal from each SiPM channel was processed by two electronics channels with different gain. A fully operational prototype system with 256 SiPM readout channels allowed the collection of data from a prototype of the ALICE Forward Hadron Calorimeter (FoCal-H). The design and the test beam results using high energy hadron beams are presented and discussed, confirming the applicability of VMM3a-based solutions for energy measurements in a high rate environment.","sentences":["Highly integrated multichannel readout electronics is crucial in contemporary particle physics experiments.","A novel silicon photomultiplier readout system based on the VMM3a ASIC was developed, for the first time exploiting this chip for calorimetric purposes.","To extend the dynamic range the signal from each SiPM channel was processed by two electronics channels with different gain.","A fully operational prototype system with 256 SiPM readout channels allowed the collection of data from a prototype of the ALICE Forward Hadron Calorimeter (FoCal-H).","The design and the test beam results using high energy hadron beams are presented and discussed, confirming the applicability of VMM3a-based solutions for energy measurements in a high rate environment."],"url":"http://arxiv.org/abs/2403.14577v1","category":"physics.ins-det"}
{"created":"2024-03-21 16:44:49","title":"Learning Hierarchical Control For Constrained Dynamic Task Assignment","abstract":"This paper introduces a novel data-driven hierarchical control scheme for managing a fleet of nonlinear, capacity-constrained autonomous agents in an iterative environment. We propose a control framework consisting of a high-level dynamic task assignment and routing layer and low-level motion planning and tracking layer. Each layer of the control hierarchy uses a data-driven MPC policy, maintaining bounded computational complexity at each calculation of a new task assignment or actuation input. We utilize collected data to iteratively refine estimates of agent capacity usage, and update MPC policy parameters accordingly. Our approach leverages tools from iterative learning control to integrate learning at both levels of the hierarchy, and coordinates learning between levels in order to maintain closed-loop feasibility and performance improvement of the connected architecture.","sentences":["This paper introduces a novel data-driven hierarchical control scheme for managing a fleet of nonlinear, capacity-constrained autonomous agents in an iterative environment.","We propose a control framework consisting of a high-level dynamic task assignment and routing layer and low-level motion planning and tracking layer.","Each layer of the control hierarchy uses a data-driven MPC policy, maintaining bounded computational complexity at each calculation of a new task assignment or actuation input.","We utilize collected data to iteratively refine estimates of agent capacity usage, and update MPC policy parameters accordingly.","Our approach leverages tools from iterative learning control to integrate learning at both levels of the hierarchy, and coordinates learning between levels in order to maintain closed-loop feasibility and performance improvement of the connected architecture."],"url":"http://arxiv.org/abs/2403.14545v1","category":"cs.RO"}
{"created":"2024-03-21 13:39:27","title":"Early Flood Warning Using Satellite-Derived Convective System and Precipitation Data -- A Retrospective Case Study of Central Vietnam","abstract":"This paper addresses the challenges of an early flood warning caused by complex convective systems (CSs), by using Low-Earth Orbit and Geostationary satellite data. We focus on a sequence of extreme events that took place in central Vietnam during October 2020, with a specific emphasis on the events leading up to the floods, i.e., those occurring before October 10th, 2020. In this critical phase, several hydrometeorological indicators could be identified thanks to an increasingly advanced and dense observation network composed of Earth Observation satellites, in particular those enabling the characterization and monitoring of a CS, in terms of low-temperature clouds and heavy rainfall. Himawari-8 images, both individually and in time-series, allow identifying and tracking convective clouds. This is complemented by the observation of heavy/violent rainfall through GPM IMERG data, as well as the detection of strong winds using radiometers and scatterometers. Collectively, these datasets, along with the estimated intensity and duration of the event from each source, form a comprehensive dataset detailing the intricate behaviors of CSs. All of these factors are significant contributors to the magnitude of flooding and the short-term dynamics anticipated in the studied region.","sentences":["This paper addresses the challenges of an early flood warning caused by complex convective systems (CSs), by using Low-Earth Orbit and Geostationary satellite data.","We focus on a sequence of extreme events that took place in central Vietnam during October 2020, with a specific emphasis on the events leading up to the floods, i.e., those occurring before October 10th, 2020.","In this critical phase, several hydrometeorological indicators could be identified thanks to an increasingly advanced and dense observation network composed of Earth Observation satellites, in particular those enabling the characterization and monitoring of a CS, in terms of low-temperature clouds and heavy rainfall.","Himawari-8 images, both individually and in time-series, allow identifying and tracking convective clouds.","This is complemented by the observation of heavy/violent rainfall through GPM IMERG data, as well as the detection of strong winds using radiometers and scatterometers.","Collectively, these datasets, along with the estimated intensity and duration of the event from each source, form a comprehensive dataset detailing the intricate behaviors of CSs.","All of these factors are significant contributors to the magnitude of flooding and the short-term dynamics anticipated in the studied region."],"url":"http://arxiv.org/abs/2403.14395v1","category":"eess.IV"}
{"created":"2024-03-21 12:45:12","title":"WikiFactDiff: A Large, Realistic, and Temporally Adaptable Dataset for Atomic Factual Knowledge Update in Causal Language Models","abstract":"The factuality of large language model (LLMs) tends to decay over time since events posterior to their training are \"unknown\" to them. One way to keep models up-to-date could be factual update: the task of inserting, replacing, or removing certain simple (atomic) facts within the model. To study this task, we present WikiFactDiff, a dataset that describes the evolution of factual knowledge between two dates as a collection of simple facts divided into three categories: new, obsolete, and static. We describe several update scenarios arising from various combinations of these three types of basic update. The facts are represented by subject-relation-object triples; indeed, WikiFactDiff was constructed by comparing the state of the Wikidata knowledge base at 4 January 2021 and 27 February 2023. Those fact are accompanied by verbalization templates and cloze tests that enable running update algorithms and their evaluation metrics. Contrary to other datasets, such as zsRE and CounterFact, WikiFactDiff constitutes a realistic update setting that involves various update scenarios, including replacements, archival, and new entity insertions. We also present an evaluation of existing update algorithms on WikiFactDiff.","sentences":["The factuality of large language model (LLMs) tends to decay over time since events posterior to their training are \"unknown\" to them.","One way to keep models up-to-date could be factual update: the task of inserting, replacing, or removing certain simple (atomic) facts within the model.","To study this task, we present WikiFactDiff, a dataset that describes the evolution of factual knowledge between two dates as a collection of simple facts divided into three categories: new, obsolete, and static.","We describe several update scenarios arising from various combinations of these three types of basic update.","The facts are represented by subject-relation-object triples; indeed, WikiFactDiff was constructed by comparing the state of the Wikidata knowledge base at 4 January 2021 and 27 February 2023.","Those fact are accompanied by verbalization templates and cloze tests that enable running update algorithms and their evaluation metrics.","Contrary to other datasets, such as zsRE and CounterFact, WikiFactDiff constitutes a realistic update setting that involves various update scenarios, including replacements, archival, and new entity insertions.","We also present an evaluation of existing update algorithms on WikiFactDiff."],"url":"http://arxiv.org/abs/2403.14364v1","category":"cs.CL"}
{"created":"2024-03-21 12:26:24","title":"Collecting Influencers: A Comparative Study of Online Network Crawlers","abstract":"Online network crawling tasks require a lot of efforts for the researchers to collect the data. One of them is identification of important nodes, which has many applications starting from viral marketing to the prevention of disease spread. Various crawling algorithms has been suggested but their efficiency is not studied well. In this paper we compared six known crawlers on the task of collecting the fraction of the most influential nodes of graph.   We analyzed crawlers behavior for four measures of node influence: node degree, k-coreness, betweenness centrality, and eccentricity. The experiments confirmed that greedy methods perform the best in many settings, but the cases exist when they are very inefficient.","sentences":["Online network crawling tasks require a lot of efforts for the researchers to collect the data.","One of them is identification of important nodes, which has many applications starting from viral marketing to the prevention of disease spread.","Various crawling algorithms has been suggested but their efficiency is not studied well.","In this paper we compared six known crawlers on the task of collecting the fraction of the most influential nodes of graph.   ","We analyzed crawlers behavior for four measures of node influence: node degree, k-coreness, betweenness centrality, and eccentricity.","The experiments confirmed that greedy methods perform the best in many settings, but the cases exist when they are very inefficient."],"url":"http://arxiv.org/abs/2403.14351v1","category":"cs.SI"}
{"created":"2024-03-21 12:22:47","title":"Tell Me What You Want (What You Really, Really Want): Addressing the Expectation Gap for Goal Conveyance from Humans to Robots","abstract":"Conveying human goals to autonomous systems (AS) occurs both when the system is being designed and when it is being operated. The design-step conveyance is typically mediated by robotics and AI engineers, who must appropriately capture end-user requirements and concepts of operations, while the operation-step conveyance is mediated by the design, interfaces, and behavior of the AI. However, communication can be difficult during both these periods because of mismatches in the expectations and expertise of the end-user and the roboticist, necessitating more design cycles to resolve. We examine some of the barriers in communicating system design requirements, and develop an augmentation for applied cognitive task analysis (ACTA) methods, that we call robot task analysis (RTA), pertaining specifically to the development of autonomous systems. Further, we introduce a top-down view of an underexplored area of friction between requirements communication -- implied human expectations -- utilizing a collection of work primarily from experimental psychology and social sciences. We show how such expectations can be used in conjunction with task-specific expectations and the system design process for AS to improve design team communication, alleviate barriers to user rejection, and reduce the number of design cycles.","sentences":["Conveying human goals to autonomous systems (AS) occurs both when the system is being designed and when it is being operated.","The design-step conveyance is typically mediated by robotics and AI engineers, who must appropriately capture end-user requirements and concepts of operations, while the operation-step conveyance is mediated by the design, interfaces, and behavior of the AI.","However, communication can be difficult during both these periods because of mismatches in the expectations and expertise of the end-user and the roboticist, necessitating more design cycles to resolve.","We examine some of the barriers in communicating system design requirements, and develop an augmentation for applied cognitive task analysis (ACTA) methods, that we call robot task analysis (RTA), pertaining specifically to the development of autonomous systems.","Further, we introduce a top-down view of an underexplored area of friction between requirements communication -- implied human expectations -- utilizing a collection of work primarily from experimental psychology and social sciences.","We show how such expectations can be used in conjunction with task-specific expectations and the system design process for AS to improve design team communication, alleviate barriers to user rejection, and reduce the number of design cycles."],"url":"http://arxiv.org/abs/2403.14344v1","category":"cs.RO"}
{"created":"2024-03-21 12:01:54","title":"FFT-based Selection and Optimization of Statistics for Robust Recognition of Severely Corrupted Images","abstract":"Improving model robustness in case of corrupted images is among the key challenges to enable robust vision systems on smart devices, such as robotic agents. Particularly, robust test-time performance is imperative for most of the applications. This paper presents a novel approach to improve robustness of any classification model, especially on severely corrupted images. Our method (FROST) employs high-frequency features to detect input image corruption type, and select layer-wise feature normalization statistics. FROST provides the state-of-the-art results for different models and datasets, outperforming competitors on ImageNet-C by up to 37.1% relative gain, improving baseline of 40.9% mCE on severe corruptions.","sentences":["Improving model robustness in case of corrupted images is among the key challenges to enable robust vision systems on smart devices, such as robotic agents.","Particularly, robust test-time performance is imperative for most of the applications.","This paper presents a novel approach to improve robustness of any classification model, especially on severely corrupted images.","Our method (FROST) employs high-frequency features to detect input image corruption type, and select layer-wise feature normalization statistics.","FROST provides the state-of-the-art results for different models and datasets, outperforming competitors on ImageNet-C by up to 37.1% relative gain, improving baseline of 40.9% mCE on severe corruptions."],"url":"http://arxiv.org/abs/2403.14335v1","category":"cs.CV"}
{"created":"2024-03-21 11:50:00","title":"Evaluation and Deployment of LiDAR-based Place Recognition in Dense Forests","abstract":"Many LiDAR place recognition systems have been developed and tested specifically for urban driving scenarios. Their performance in natural environments such as forests and woodlands have been studied less closely. In this paper, we analyzed the capabilities of four different LiDAR place recognition systems, both handcrafted and learning-based methods, using LiDAR data collected with a handheld device and legged robot within dense forest environments. In particular, we focused on evaluating localization where there is significant translational and orientation difference between corresponding LiDAR scan pairs. This is particularly important for forest survey systems where the sensor or robot does not follow a defined road or path. Extending our analysis we then incorporated the best performing approach, Logg3dNet, into a full 6-DoF pose estimation system -- introducing several verification layers for precise registration. We demonstrated the performance of our methods in three operational modes: online SLAM, offline multi-mission SLAM map merging, and relocalization into a prior map. We evaluated these modes using data captured in forests from three different countries, achieving 80% of correct loop closures candidates with baseline distances up to 5m, and 60% up to 10m.","sentences":["Many LiDAR place recognition systems have been developed and tested specifically for urban driving scenarios.","Their performance in natural environments such as forests and woodlands have been studied less closely.","In this paper, we analyzed the capabilities of four different LiDAR place recognition systems, both handcrafted and learning-based methods, using LiDAR data collected with a handheld device and legged robot within dense forest environments.","In particular, we focused on evaluating localization where there is significant translational and orientation difference between corresponding LiDAR scan pairs.","This is particularly important for forest survey systems where the sensor or robot does not follow a defined road or path.","Extending our analysis we then incorporated the best performing approach, Logg3dNet, into a full 6-DoF pose estimation system -- introducing several verification layers for precise registration.","We demonstrated the performance of our methods in three operational modes: online SLAM, offline multi-mission SLAM map merging, and relocalization into a prior map.","We evaluated these modes using data captured in forests from three different countries, achieving 80% of correct loop closures candidates with baseline distances up to 5m, and 60% up to 10m."],"url":"http://arxiv.org/abs/2403.14326v1","category":"cs.RO"}
{"created":"2024-03-21 11:21:17","title":"Bayesian Optimization for Sample-Efficient Policy Improvement in Robotic Manipulation","abstract":"Sample efficient learning of manipulation skills poses a major challenge in robotics. While recent approaches demonstrate impressive advances in the type of task that can be addressed and the sensing modalities that can be incorporated, they still require large amounts of training data. Especially with regard to learning actions on robots in the real world, this poses a major problem due to the high costs associated with both demonstrations and real-world robot interactions. To address this challenge, we introduce BOpt-GMM, a hybrid approach that combines imitation learning with own experience collection. We first learn a skill model as a dynamical system encoded in a Gaussian Mixture Model from a few demonstrations. We then improve this model with Bayesian optimization building on a small number of autonomous skill executions in a sparse reward setting. We demonstrate the sample efficiency of our approach on multiple complex manipulation skills in both simulations and real-world experiments. Furthermore, we make the code and pre-trained models publicly available at http://bopt-gmm. cs.uni-freiburg.de.","sentences":["Sample efficient learning of manipulation skills poses a major challenge in robotics.","While recent approaches demonstrate impressive advances in the type of task that can be addressed and the sensing modalities that can be incorporated, they still require large amounts of training data.","Especially with regard to learning actions on robots in the real world, this poses a major problem due to the high costs associated with both demonstrations and real-world robot interactions.","To address this challenge, we introduce BOpt-GMM, a hybrid approach that combines imitation learning with own experience collection.","We first learn a skill model as a dynamical system encoded in a Gaussian Mixture Model from a few demonstrations.","We then improve this model with Bayesian optimization building on a small number of autonomous skill executions in a sparse reward setting.","We demonstrate the sample efficiency of our approach on multiple complex manipulation skills in both simulations and real-world experiments.","Furthermore, we make the code and pre-trained models publicly available at http://bopt-gmm.","cs.uni-freiburg.de."],"url":"http://arxiv.org/abs/2403.14305v1","category":"cs.RO"}
{"created":"2024-03-21 11:00:11","title":"Human Reactions to Incorrect Answers from Robots","abstract":"As robots grow more and more integrated into numerous industries, it is critical to comprehend how humans respond to their failures. This paper systematically studies how trust dynamics and system design are affected by human responses to robot failures. The three-stage survey used in the study provides a thorough understanding of human-robot interactions. While the second stage concentrates on interaction details, such as robot precision and error acknowledgment, the first stage collects demographic data and initial levels of trust. In the last phase, participants' perceptions are examined after the encounter, and trust dynamics, forgiveness, and propensity to suggest robotic technologies are evaluated. Results show that participants' trust in robotic technologies increased significantly when robots acknowledged their errors or limitations to participants and their willingness to suggest robots for activities in the future points to a favorable change in perception, emphasizing the role that direct engagement has in influencing trust dynamics. By providing useful advice for creating more sympathetic, responsive, and reliable robotic systems, the study advances the science of human-robot interaction and promotes a wider adoption of robotic technologies.","sentences":["As robots grow more and more integrated into numerous industries, it is critical to comprehend how humans respond to their failures.","This paper systematically studies how trust dynamics and system design are affected by human responses to robot failures.","The three-stage survey used in the study provides a thorough understanding of human-robot interactions.","While the second stage concentrates on interaction details, such as robot precision and error acknowledgment, the first stage collects demographic data and initial levels of trust.","In the last phase, participants' perceptions are examined after the encounter, and trust dynamics, forgiveness, and propensity to suggest robotic technologies are evaluated.","Results show that participants' trust in robotic technologies increased significantly when robots acknowledged their errors or limitations to participants and their willingness to suggest robots for activities in the future points to a favorable change in perception, emphasizing the role that direct engagement has in influencing trust dynamics.","By providing useful advice for creating more sympathetic, responsive, and reliable robotic systems, the study advances the science of human-robot interaction and promotes a wider adoption of robotic technologies."],"url":"http://arxiv.org/abs/2403.14293v1","category":"cs.RO"}
{"created":"2024-03-21 10:31:11","title":"Is Reference Necessary in the Evaluation of NLG Systems? When and Where?","abstract":"The majority of automatic metrics for evaluating NLG systems are reference-based. However, the challenge of collecting human annotation results in a lack of reliable references in numerous application scenarios. Despite recent advancements in reference-free metrics, it has not been well understood when and where they can be used as an alternative to reference-based metrics. In this study, by employing diverse analytical approaches, we comprehensively assess the performance of both metrics across a wide range of NLG tasks, encompassing eight datasets and eight evaluation models. Based on solid experiments, the results show that reference-free metrics exhibit a higher correlation with human judgment and greater sensitivity to deficiencies in language quality. However, their effectiveness varies across tasks and is influenced by the quality of candidate texts. Therefore, it's important to assess the performance of reference-free metrics before applying them to a new task, especially when inputs are in uncommon form or when the answer space is highly variable. Our study can provide insight into the appropriate application of automatic metrics and the impact of metric choice on evaluation performance.","sentences":["The majority of automatic metrics for evaluating NLG systems are reference-based.","However, the challenge of collecting human annotation results in a lack of reliable references in numerous application scenarios.","Despite recent advancements in reference-free metrics, it has not been well understood when and where they can be used as an alternative to reference-based metrics.","In this study, by employing diverse analytical approaches, we comprehensively assess the performance of both metrics across a wide range of NLG tasks, encompassing eight datasets and eight evaluation models.","Based on solid experiments, the results show that reference-free metrics exhibit a higher correlation with human judgment and greater sensitivity to deficiencies in language quality.","However, their effectiveness varies across tasks and is influenced by the quality of candidate texts.","Therefore, it's important to assess the performance of reference-free metrics before applying them to a new task, especially when inputs are in uncommon form or when the answer space is highly variable.","Our study can provide insight into the appropriate application of automatic metrics and the impact of metric choice on evaluation performance."],"url":"http://arxiv.org/abs/2403.14275v1","category":"cs.CL"}
{"created":"2024-03-21 09:37:11","title":"Minimal covariance realization and system identification algorithm for a class of stochastic linear switched systems with i.i.d. switching","abstract":"In this paper, we work with Linear Switched Systems (LSS). We show the existence of minimality in innovation form for such systems. We also present a realization algorithm to compute a minimal LSS in innovation form, by calculating the covariances of the state-pace matrices. Finally, a system identification algorithm statically consistent is presented. The later, based on the realization algorithm, uses the collected data in order to compute the covariances of the inputs and outputs.","sentences":["In this paper, we work with Linear Switched Systems (LSS).","We show the existence of minimality in innovation form for such systems.","We also present a realization algorithm to compute a minimal LSS in innovation form, by calculating the covariances of the state-pace matrices.","Finally, a system identification algorithm statically consistent is presented.","The later, based on the realization algorithm, uses the collected data in order to compute the covariances of the inputs and outputs."],"url":"http://arxiv.org/abs/2403.14259v1","category":"math.OC"}
{"created":"2024-03-21 07:48:35","title":"Unleashing Unlabeled Data: A Paradigm for Cross-View Geo-Localization","abstract":"This paper investigates the effective utilization of unlabeled data for large-area cross-view geo-localization (CVGL), encompassing both unsupervised and semi-supervised settings. Common approaches to CVGL rely on ground-satellite image pairs and employ label-driven supervised training. However, the cost of collecting precise cross-view image pairs hinders the deployment of CVGL in real-life scenarios. Without the pairs, CVGL will be more challenging to handle the significant imaging and spatial gaps between ground and satellite images. To this end, we propose an unsupervised framework including a cross-view projection to guide the model for retrieving initial pseudo-labels and a fast re-ranking mechanism to refine the pseudo-labels by leveraging the fact that ``the perfectly paired ground-satellite image is located in a unique and identical scene\". The framework exhibits competitive performance compared with supervised works on three open-source benchmarks. Our code and models will be released on https://github.com/liguopeng0923/UCVGL.","sentences":["This paper investigates the effective utilization of unlabeled data for large-area cross-view geo-localization (CVGL), encompassing both unsupervised and semi-supervised settings.","Common approaches to CVGL rely on ground-satellite image pairs and employ label-driven supervised training.","However, the cost of collecting precise cross-view image pairs hinders the deployment of CVGL in real-life scenarios.","Without the pairs, CVGL will be more challenging to handle the significant imaging and spatial gaps between ground and satellite images.","To this end, we propose an unsupervised framework including a cross-view projection to guide the model for retrieving initial pseudo-labels and a fast re-ranking mechanism to refine the pseudo-labels by leveraging the fact that ``the perfectly paired ground-satellite image is located in a unique and identical scene\".","The framework exhibits competitive performance compared with supervised works on three open-source benchmarks.","Our code and models will be released on https://github.com/liguopeng0923/UCVGL."],"url":"http://arxiv.org/abs/2403.14198v1","category":"cs.CV"}
{"created":"2024-03-21 06:55:30","title":"Designing Complexity? The Role of Self-Organization in Urban planning and Design","abstract":"This chapter explores the concept of self-organization in urban planning and design, highlighting its role in shaping the unique characteristics of cities. It examines how various socio-economic, cultural, and political factors contribute to the development of distinct architectural styles, emphasizing the morphological patterns and self-organization principles. The chapter addresses the emergence of scaling laws and fractal geometry in urban forms, using historical and contemporary examples to illustrate these concepts. The discussion also delves into the cognitive aspects of urban design, examining how the physical layout of cities influences cognitive maps and perceptions of urban environments, and how these perceptions, in turn, influence urban design. Through the prism of self-organization, it demonstrates the dynamic interplay between individual and collective actions and the shaping of the urban landscape. This analysis offers insights into the complex, self-organizing systems that define urban spaces, emphasizing the interdependencies among architectural design, urban planning, and human cognition in shaping cityscapes.","sentences":["This chapter explores the concept of self-organization in urban planning and design, highlighting its role in shaping the unique characteristics of cities.","It examines how various socio-economic, cultural, and political factors contribute to the development of distinct architectural styles, emphasizing the morphological patterns and self-organization principles.","The chapter addresses the emergence of scaling laws and fractal geometry in urban forms, using historical and contemporary examples to illustrate these concepts.","The discussion also delves into the cognitive aspects of urban design, examining how the physical layout of cities influences cognitive maps and perceptions of urban environments, and how these perceptions, in turn, influence urban design.","Through the prism of self-organization, it demonstrates the dynamic interplay between individual and collective actions and the shaping of the urban landscape.","This analysis offers insights into the complex, self-organizing systems that define urban spaces, emphasizing the interdependencies among architectural design, urban planning, and human cognition in shaping cityscapes."],"url":"http://arxiv.org/abs/2403.14175v1","category":"physics.soc-ph"}
{"created":"2024-03-21 06:53:20","title":"HCTO: Optimality-Aware LiDAR Inertial Odometry with Hybrid Continuous Time Optimization for Compact Wearable Mapping System","abstract":"Compact wearable mapping system (WMS) has gained significant attention due to their convenience in various applications. Specifically, it provides an efficient way to collect prior maps for 3D structure inspection and robot-based \"last-mile delivery\" in complex environments. However, vibrations in human motion and the uneven distribution of point cloud features in complex environments often lead to rapid drift, which is a prevalent issue when applying existing LiDAR Inertial Odometry (LIO) methods on low-cost WMS. To address these limitations, we propose a novel LIO for WMSs based on Hybrid Continuous Time Optimization (HCTO) considering the optimality of Lidar correspondences. First, HCTO recognizes patterns in human motion (high-frequency part, low-frequency part, and constant velocity part) by analyzing raw IMU measurements. Second, HCTO constructs hybrid IMU factors according to different motion states, which enables robust and accurate estimation against vibration-induced noise in the IMU measurements. Third, the best point correspondences are selected using optimal design to achieve real-time performance and better odometry accuracy. We conduct experiments on head-mounted WMS datasets to evaluate the performance of our system, demonstrating significant advantages over state-of-the-art methods. Video recordings of experiments can be found on the project page of HCTO: \\href{https://github.com/kafeiyin00/HCTO}{https://github.com/kafeiyin00/HCTO}.","sentences":["Compact wearable mapping system (WMS) has gained significant attention due to their convenience in various applications.","Specifically, it provides an efficient way to collect prior maps for 3D structure inspection and robot-based \"last-mile delivery\" in complex environments.","However, vibrations in human motion and the uneven distribution of point cloud features in complex environments often lead to rapid drift, which is a prevalent issue when applying existing LiDAR Inertial Odometry (LIO) methods on low-cost WMS.","To address these limitations, we propose a novel LIO for WMSs based on Hybrid Continuous Time Optimization (HCTO) considering the optimality of Lidar correspondences.","First, HCTO recognizes patterns in human motion (high-frequency part, low-frequency part, and constant velocity part) by analyzing raw IMU measurements.","Second, HCTO constructs hybrid IMU factors according to different motion states, which enables robust and accurate estimation against vibration-induced noise in the IMU measurements.","Third, the best point correspondences are selected using optimal design to achieve real-time performance and better odometry accuracy.","We conduct experiments on head-mounted WMS datasets to evaluate the performance of our system, demonstrating significant advantages over state-of-the-art methods.","Video recordings of experiments can be found on the project page of HCTO: \\href{https://github.com/kafeiyin00/HCTO}{https://github.com/kafeiyin00/HCTO}."],"url":"http://arxiv.org/abs/2403.14173v1","category":"cs.RO"}
{"created":"2024-03-21 06:32:36","title":"Leveraging Large Language Model-based Room-Object Relationships Knowledge for Enhancing Multimodal-Input Object Goal Navigation","abstract":"Object-goal navigation is a crucial engineering task for the community of embodied navigation; it involves navigating to an instance of a specified object category within unseen environments. Although extensive investigations have been conducted on both end-to-end and modular-based, data-driven approaches, fully enabling an agent to comprehend the environment through perceptual knowledge and perform object-goal navigation as efficiently as humans remains a significant challenge. Recently, large language models have shown potential in this task, thanks to their powerful capabilities for knowledge extraction and integration. In this study, we propose a data-driven, modular-based approach, trained on a dataset that incorporates common-sense knowledge of object-to-room relationships extracted from a large language model. We utilize the multi-channel Swin-Unet architecture to conduct multi-task learning incorporating with multimodal inputs. The results in the Habitat simulator demonstrate that our framework outperforms the baseline by an average of 10.6% in the efficiency metric, Success weighted by Path Length (SPL). The real-world demonstration shows that the proposed approach can efficiently conduct this task by traversing several rooms. For more details and real-world demonstrations, please check our project webpage (https://sunleyuan.github.io/ObjectNav).","sentences":["Object-goal navigation is a crucial engineering task for the community of embodied navigation; it involves navigating to an instance of a specified object category within unseen environments.","Although extensive investigations have been conducted on both end-to-end and modular-based, data-driven approaches, fully enabling an agent to comprehend the environment through perceptual knowledge and perform object-goal navigation as efficiently as humans remains a significant challenge.","Recently, large language models have shown potential in this task, thanks to their powerful capabilities for knowledge extraction and integration.","In this study, we propose a data-driven, modular-based approach, trained on a dataset that incorporates common-sense knowledge of object-to-room relationships extracted from a large language model.","We utilize the multi-channel Swin-Unet architecture to conduct multi-task learning incorporating with multimodal inputs.","The results in the Habitat simulator demonstrate that our framework outperforms the baseline by an average of 10.6% in the efficiency metric, Success weighted by Path Length (SPL).","The real-world demonstration shows that the proposed approach can efficiently conduct this task by traversing several rooms.","For more details and real-world demonstrations, please check our project webpage (https://sunleyuan.github.io/ObjectNav)."],"url":"http://arxiv.org/abs/2403.14163v1","category":"cs.RO"}
{"created":"2024-03-21 06:14:46","title":"Volumetric Environment Representation for Vision-Language Navigation","abstract":"Vision-language navigation (VLN) requires an agent to navigate through an 3D environment based on visual observations and natural language instructions. It is clear that the pivotal factor for successful navigation lies in the comprehensive scene understanding. Previous VLN agents employ monocular frameworks to extract 2D features of perspective views directly. Though straightforward, they struggle for capturing 3D geometry and semantics, leading to a partial and incomplete environment representation. To achieve a comprehensive 3D representation with fine-grained details, we introduce a Volumetric Environment Representation (VER), which voxelizes the physical world into structured 3D cells. For each cell, VER aggregates multi-view 2D features into such a unified 3D space via 2D-3D sampling. Through coarse-to-fine feature extraction and multi-task learning for VER, our agent predicts 3D occupancy, 3D room layout, and 3D bounding boxes jointly. Based on online collected VERs, our agent performs volume state estimation and builds episodic memory for predicting the next step. Experimental results show our environment representations from multi-task learning lead to evident performance gains on VLN. Our model achieves state-of-the-art performance across VLN benchmarks (R2R, REVERIE, and R4R).","sentences":["Vision-language navigation (VLN) requires an agent to navigate through an 3D environment based on visual observations and natural language instructions.","It is clear that the pivotal factor for successful navigation lies in the comprehensive scene understanding.","Previous VLN agents employ monocular frameworks to extract 2D features of perspective views directly.","Though straightforward, they struggle for capturing 3D geometry and semantics, leading to a partial and incomplete environment representation.","To achieve a comprehensive 3D representation with fine-grained details, we introduce a Volumetric Environment Representation (VER), which voxelizes the physical world into structured 3D cells.","For each cell, VER aggregates multi-view 2D features into such a unified 3D space via 2D-3D sampling.","Through coarse-to-fine feature extraction and multi-task learning for VER, our agent predicts 3D occupancy, 3D room layout, and 3D bounding boxes jointly.","Based on online collected VERs, our agent performs volume state estimation and builds episodic memory for predicting the next step.","Experimental results show our environment representations from multi-task learning lead to evident performance gains on VLN.","Our model achieves state-of-the-art performance across VLN benchmarks (R2R, REVERIE, and R4R)."],"url":"http://arxiv.org/abs/2403.14158v1","category":"cs.CV"}
{"created":"2024-03-21 06:10:51","title":"Policy Mirror Descent with Lookahead","abstract":"Policy Mirror Descent (PMD) stands as a versatile algorithmic framework encompassing several seminal policy gradient algorithms such as natural policy gradient, with connections with state-of-the-art reinforcement learning (RL) algorithms such as TRPO and PPO. PMD can be seen as a soft Policy Iteration algorithm implementing regularized 1-step greedy policy improvement. However, 1-step greedy policies might not be the best choice and recent remarkable empirical successes in RL such as AlphaGo and AlphaZero have demonstrated that greedy approaches with respect to multiple steps outperform their 1-step counterpart. In this work, we propose a new class of PMD algorithms called $h$-PMD which incorporates multi-step greedy policy improvement with lookahead depth $h$ to the PMD update rule. To solve discounted infinite horizon Markov Decision Processes with discount factor $\\gamma$, we show that $h$-PMD which generalizes the standard PMD enjoys a faster dimension-free $\\gamma^h$-linear convergence rate, contingent on the computation of multi-step greedy policies. We propose an inexact version of $h$-PMD where lookahead action values are estimated. Under a generative model, we establish a sample complexity for $h$-PMD which improves over prior work. Finally, we extend our result to linear function approximation to scale to large state spaces. Under suitable assumptions, our sample complexity only involves dependence on the dimension of the feature map space instead of the state space size.","sentences":["Policy Mirror Descent (PMD) stands as a versatile algorithmic framework encompassing several seminal policy gradient algorithms such as natural policy gradient, with connections with state-of-the-art reinforcement learning (RL) algorithms such as TRPO and PPO.","PMD can be seen as a soft Policy Iteration algorithm implementing regularized 1-step greedy policy improvement.","However, 1-step greedy policies might not be the best choice and recent remarkable empirical successes in RL such as AlphaGo and AlphaZero have demonstrated that greedy approaches with respect to multiple steps outperform their 1-step counterpart.","In this work, we propose a new class of PMD algorithms called $h$-PMD which incorporates multi-step greedy policy improvement with lookahead depth $h$ to the PMD update rule.","To solve discounted infinite horizon Markov Decision Processes with discount factor $\\gamma$, we show that $h$-PMD which generalizes the standard PMD enjoys a faster dimension-free $\\gamma^h$-linear convergence rate, contingent on the computation of multi-step greedy policies.","We propose an inexact version of $h$-PMD where lookahead action values are estimated.","Under a generative model, we establish a sample complexity for $h$-PMD which improves over prior work.","Finally, we extend our result to linear function approximation to scale to large state spaces.","Under suitable assumptions, our sample complexity only involves dependence on the dimension of the feature map space instead of the state space size."],"url":"http://arxiv.org/abs/2403.14156v1","category":"cs.LG"}
{"created":"2024-03-21 05:57:27","title":"Deep Learning for Trajectory Data Management and Mining: A Survey and Beyond","abstract":"Trajectory computing is a pivotal domain encompassing trajectory data management and mining, garnering widespread attention due to its crucial role in various practical applications such as location services, urban traffic, and public safety. Traditional methods, focusing on simplistic spatio-temporal features, face challenges of complex calculations, limited scalability, and inadequate adaptability to real-world complexities. In this paper, we present a comprehensive review of the development and recent advances in deep learning for trajectory computing (DL4Traj). We first define trajectory data and provide a brief overview of widely-used deep learning models. Systematically, we explore deep learning applications in trajectory management (pre-processing, storage, analysis, and visualization) and mining (trajectory-related forecasting, trajectory-related recommendation, trajectory classification, travel time estimation, anomaly detection, and mobility generation). Notably, we encapsulate recent advancements in Large Language Models (LLMs) that hold the potential to augment trajectory computing. Additionally, we summarize application scenarios, public datasets, and toolkits. Finally, we outline current challenges in DL4Traj research and propose future directions. Relevant papers and open-source resources have been collated and are continuously updated at: \\href{https://github.com/yoshall/Awesome-Trajectory-Computing}{DL4Traj Repo}.","sentences":["Trajectory computing is a pivotal domain encompassing trajectory data management and mining, garnering widespread attention due to its crucial role in various practical applications such as location services, urban traffic, and public safety.","Traditional methods, focusing on simplistic spatio-temporal features, face challenges of complex calculations, limited scalability, and inadequate adaptability to real-world complexities.","In this paper, we present a comprehensive review of the development and recent advances in deep learning for trajectory computing (DL4Traj).","We first define trajectory data and provide a brief overview of widely-used deep learning models.","Systematically, we explore deep learning applications in trajectory management (pre-processing, storage, analysis, and visualization) and mining (trajectory-related forecasting, trajectory-related recommendation, trajectory classification, travel time estimation, anomaly detection, and mobility generation).","Notably, we encapsulate recent advancements in Large Language Models (LLMs) that hold the potential to augment trajectory computing.","Additionally, we summarize application scenarios, public datasets, and toolkits.","Finally, we outline current challenges in DL4Traj research and propose future directions.","Relevant papers and open-source resources have been collated and are continuously updated at: \\href{https://github.com/yoshall/Awesome-Trajectory-Computing}{DL4Traj Repo}."],"url":"http://arxiv.org/abs/2403.14151v1","category":"cs.LG"}
{"created":"2024-03-21 05:42:17","title":"Evolving Benchmark Functions to Compare Evolutionary Algorithms via Genetic Programming","abstract":"In this study, we use Genetic Programming (GP) to compose new optimization benchmark functions. Optimization benchmarks have the important role of showing the differences between evolutionary algorithms, making it possible for further analysis and comparisons. We show that the benchmarks generated by GP are able to differentiate algorithms better than human-made benchmark functions. The fitness measure of the GP is the Wasserstein distance of the solutions found by a pair of optimizers. Additionally, we use MAP-Elites to both enhance the search power of the GP and also illustrate how the difference between optimizers changes by various landscape features. Our approach provides a novel way to automate the design of benchmark functions and to compare evolutionary algorithms.","sentences":["In this study, we use Genetic Programming (GP) to compose new optimization benchmark functions.","Optimization benchmarks have the important role of showing the differences between evolutionary algorithms, making it possible for further analysis and comparisons.","We show that the benchmarks generated by GP are able to differentiate algorithms better than human-made benchmark functions.","The fitness measure of the GP is the Wasserstein distance of the solutions found by a pair of optimizers.","Additionally, we use MAP-Elites to both enhance the search power of the GP and also illustrate how the difference between optimizers changes by various landscape features.","Our approach provides a novel way to automate the design of benchmark functions and to compare evolutionary algorithms."],"url":"http://arxiv.org/abs/2403.14146v1","category":"cs.NE"}
{"created":"2024-03-21 04:15:56","title":"Advancing IIoT with Over-the-Air Federated Learning: The Role of Iterative Magnitude Pruning","abstract":"The industrial Internet of Things (IIoT) under Industry 4.0 heralds an era of interconnected smart devices where data-driven insights and machine learning (ML) fuse to revolutionize manufacturing. A noteworthy development in IIoT is the integration of federated learning (FL), which addresses data privacy and security among devices. FL enables edge sensors, also known as peripheral intelligence units (PIUs) to learn and adapt using their data locally, without explicit sharing of confidential data, to facilitate a collaborative yet confidential learning process. However, the lower memory footprint and computational power of PIUs inherently require deep neural network (DNN) models that have a very compact size. Model compression techniques such as pruning can be used to reduce the size of DNN models by removing unnecessary connections that have little impact on the model's performance, thus making the models more suitable for the limited resources of PIUs. Targeting the notion of compact yet robust DNN models, we propose the integration of iterative magnitude pruning (IMP) of the DNN model being trained in an over-the-air FL (OTA-FL) environment for IIoT. We provide a tutorial overview and also present a case study of the effectiveness of IMP in OTA-FL for an IIoT environment. Finally, we present future directions for enhancing and optimizing these deep compression techniques further, aiming to push the boundaries of IIoT capabilities in acquiring compact yet robust and high-performing DNN models.","sentences":["The industrial Internet of Things (IIoT) under Industry 4.0 heralds an era of interconnected smart devices where data-driven insights and machine learning (ML) fuse to revolutionize manufacturing.","A noteworthy development in IIoT is the integration of federated learning (FL), which addresses data privacy and security among devices.","FL enables edge sensors, also known as peripheral intelligence units (PIUs) to learn and adapt using their data locally, without explicit sharing of confidential data, to facilitate a collaborative yet confidential learning process.","However, the lower memory footprint and computational power of PIUs inherently require deep neural network (DNN) models that have a very compact size.","Model compression techniques such as pruning can be used to reduce the size of DNN models by removing unnecessary connections that have little impact on the model's performance, thus making the models more suitable for the limited resources of PIUs.","Targeting the notion of compact yet robust DNN models, we propose the integration of iterative magnitude pruning (IMP) of the DNN model being trained in an over-the-air FL (OTA-FL) environment for IIoT. We provide a tutorial overview and also present a case study of the effectiveness of IMP in OTA-FL for an IIoT environment.","Finally, we present future directions for enhancing and optimizing these deep compression techniques further, aiming to push the boundaries of IIoT capabilities in acquiring compact yet robust and high-performing DNN models."],"url":"http://arxiv.org/abs/2403.14120v1","category":"cs.LG"}
{"created":"2024-03-21 04:08:29","title":"C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion","abstract":"In deep learning, test-time adaptation has gained attention as a method for model fine-tuning without the need for labeled data. A prime exemplification is the recently proposed test-time prompt tuning for large-scale vision-language models such as CLIP. Unfortunately, these prompts have been mainly developed to improve accuracy, overlooking the importance of calibration-a crucial aspect for quantifying prediction uncertainty. However, traditional calibration methods rely on substantial amounts of labeled data, making them impractical for test-time scenarios. To this end, this paper explores calibration during test-time prompt tuning by leveraging the inherent properties of CLIP. Through a series of observations, we find that the prompt choice significantly affects the calibration in CLIP, where the prompts leading to higher text feature dispersion result in better-calibrated predictions. Introducing the Average Text Feature Dispersion (ATFD), we establish its relationship with calibration error and present a novel method, Calibrated Test-time Prompt Tuning (C-TPT), for optimizing prompts during test-time with enhanced calibration. Through extensive experiments on different CLIP architectures and datasets, we show that C-TPT can effectively improve the calibration of test-time prompt tuning without needing labeled data.","sentences":["In deep learning, test-time adaptation has gained attention as a method for model fine-tuning without the need for labeled data.","A prime exemplification is the recently proposed test-time prompt tuning for large-scale vision-language models such as CLIP.","Unfortunately, these prompts have been mainly developed to improve accuracy, overlooking the importance of calibration-a crucial aspect for quantifying prediction uncertainty.","However, traditional calibration methods rely on substantial amounts of labeled data, making them impractical for test-time scenarios.","To this end, this paper explores calibration during test-time prompt tuning by leveraging the inherent properties of CLIP.","Through a series of observations, we find that the prompt choice significantly affects the calibration in CLIP, where the prompts leading to higher text feature dispersion result in better-calibrated predictions.","Introducing the Average Text Feature Dispersion (ATFD), we establish its relationship with calibration error and present a novel method, Calibrated Test-time Prompt Tuning (C-TPT), for optimizing prompts during test-time with enhanced calibration.","Through extensive experiments on different CLIP architectures and datasets, we show that C-TPT can effectively improve the calibration of test-time prompt tuning without needing labeled data."],"url":"http://arxiv.org/abs/2403.14119v1","category":"cs.CV"}
{"created":"2024-03-21 04:03:16","title":"A Design Space for Intelligent and Interactive Writing Assistants","abstract":"In our era of rapid technological advancement, the research landscape for writing assistants has become increasingly fragmented across various research communities. We seek to address this challenge by proposing a design space as a structured way to examine and explore the multidimensional space of intelligent and interactive writing assistants. Through a large community collaboration, we explore five aspects of writing assistants: task, user, technology, interaction, and ecosystem. Within each aspect, we define dimensions (i.e., fundamental components of an aspect) and codes (i.e., potential options for each dimension) by systematically reviewing 115 papers. Our design space aims to offer researchers and designers a practical tool to navigate, comprehend, and compare the various possibilities of writing assistants, and aid in the envisioning and design of new writing assistants.","sentences":["In our era of rapid technological advancement, the research landscape for writing assistants has become increasingly fragmented across various research communities.","We seek to address this challenge by proposing a design space as a structured way to examine and explore the multidimensional space of intelligent and interactive writing assistants.","Through a large community collaboration, we explore five aspects of writing assistants: task, user, technology, interaction, and ecosystem.","Within each aspect, we define dimensions (i.e., fundamental components of an aspect) and codes (i.e., potential options for each dimension) by systematically reviewing 115 papers.","Our design space aims to offer researchers and designers a practical tool to navigate, comprehend, and compare the various possibilities of writing assistants, and aid in the envisioning and design of new writing assistants."],"url":"http://arxiv.org/abs/2403.14117v1","category":"cs.HC"}
{"created":"2024-03-21 04:01:26","title":"Training point-based deep learning networks for forest segmentation with synthetic data","abstract":"Remote sensing through unmanned aerial systems (UAS) has been increasing in forestry in recent years, along with using machine learning for data processing. Deep learning architectures, extensively applied in natural language and image processing, have recently been extended to the point cloud domain. However, the availability of point cloud datasets for training and testing remains limited. Creating forested environment point cloud datasets is expensive, requires high-precision sensors, and is time-consuming as manual point classification is required. Moreover, forest areas could be inaccessible or dangerous for humans, further complicating data collection. Then, a question arises whether it is possible to use synthetic data to train deep learning networks without the need to rely on large volumes of real forest data. To answer this question, we developed a realistic simulator that procedurally generates synthetic forest scenes. Thanks to this, we have conducted a comparative study of different state-of-the-art point-based deep learning networks for forest segmentation. Using created datasets, we determined the feasibility of using synthetic data to train deep learning networks to classify point clouds from real forest datasets. Both the simulator and the datasets are released as part of this work.","sentences":["Remote sensing through unmanned aerial systems (UAS) has been increasing in forestry in recent years, along with using machine learning for data processing.","Deep learning architectures, extensively applied in natural language and image processing, have recently been extended to the point cloud domain.","However, the availability of point cloud datasets for training and testing remains limited.","Creating forested environment point cloud datasets is expensive, requires high-precision sensors, and is time-consuming as manual point classification is required.","Moreover, forest areas could be inaccessible or dangerous for humans, further complicating data collection.","Then, a question arises whether it is possible to use synthetic data to train deep learning networks without the need to rely on large volumes of real forest data.","To answer this question, we developed a realistic simulator that procedurally generates synthetic forest scenes.","Thanks to this, we have conducted a comparative study of different state-of-the-art point-based deep learning networks for forest segmentation.","Using created datasets, we determined the feasibility of using synthetic data to train deep learning networks to classify point clouds from real forest datasets.","Both the simulator and the datasets are released as part of this work."],"url":"http://arxiv.org/abs/2403.14115v1","category":"cs.CV"}
{"created":"2024-03-21 03:42:39","title":"Heuristic Algorithm-based Action Masking Reinforcement Learning (HAAM-RL) with Ensemble Inference Method","abstract":"This paper presents a novel reinforcement learning (RL) approach called HAAM-RL (Heuristic Algorithm-based Action Masking Reinforcement Learning) for optimizing the color batching re-sequencing problem in automobile painting processes. The existing heuristic algorithms have limitations in adequately reflecting real-world constraints and accurately predicting logistics performance. Our methodology incorporates several key techniques including a tailored Markov Decision Process (MDP) formulation, reward setting including Potential-Based Reward Shaping, action masking using heuristic algorithms (HAAM-RL), and an ensemble inference method that combines multiple RL models. The RL agent is trained and evaluated using FlexSim, a commercial 3D simulation software, integrated with our RL MLOps platform BakingSoDA. Experimental results across 30 scenarios demonstrate that HAAM-RL with an ensemble inference method achieves a 16.25% performance improvement over the conventional heuristic algorithm, with stable and consistent results. The proposed approach exhibits superior performance and generalization capability, indicating its effectiveness in optimizing complex manufacturing processes. The study also discusses future research directions, including alternative state representations, incorporating model-based RL methods, and integrating additional real-world constraints.","sentences":["This paper presents a novel reinforcement learning (RL) approach called HAAM-RL (Heuristic Algorithm-based Action Masking Reinforcement Learning) for optimizing the color batching re-sequencing problem in automobile painting processes.","The existing heuristic algorithms have limitations in adequately reflecting real-world constraints and accurately predicting logistics performance.","Our methodology incorporates several key techniques including a tailored Markov Decision Process (MDP) formulation, reward setting including Potential-Based Reward Shaping, action masking using heuristic algorithms (HAAM-RL), and an ensemble inference method that combines multiple RL models.","The RL agent is trained and evaluated using FlexSim, a commercial 3D simulation software, integrated with our RL MLOps platform BakingSoDA.","Experimental results across 30 scenarios demonstrate that HAAM-RL with an ensemble inference method achieves a 16.25% performance improvement over the conventional heuristic algorithm, with stable and consistent results.","The proposed approach exhibits superior performance and generalization capability, indicating its effectiveness in optimizing complex manufacturing processes.","The study also discusses future research directions, including alternative state representations, incorporating model-based RL methods, and integrating additional real-world constraints."],"url":"http://arxiv.org/abs/2403.14110v1","category":"cs.LG"}
{"created":"2024-03-21 03:25:49","title":"DouRN: Improving DouZero by Residual Neural Networks","abstract":"Deep reinforcement learning has made significant progress in games with imperfect information, but its performance in the card game Doudizhu (Chinese Poker/Fight the Landlord) remains unsatisfactory. Doudizhu is different from conventional games as it involves three players and combines elements of cooperation and confrontation, resulting in a large state and action space. In 2021, a Doudizhu program called DouZero\\cite{zha2021douzero} surpassed previous models without prior knowledge by utilizing traditional Monte Carlo methods and multilayer perceptrons. Building on this work, our study incorporates residual networks into the model, explores different architectural designs, and conducts multi-role testing. Our findings demonstrate that this model significantly improves the winning rate within the same training time. Additionally, we introduce a call scoring system to assist the agent in deciding whether to become a landlord. With these enhancements, our model consistently outperforms the existing version of DouZero and even experienced human players. \\footnote{The source code is available at \\url{https://github.com/Yingchaol/Douzero_Resnet.git.}","sentences":["Deep reinforcement learning has made significant progress in games with imperfect information, but its performance in the card game Doudizhu (Chinese Poker/Fight the Landlord) remains unsatisfactory.","Doudizhu is different from conventional games as it involves three players and combines elements of cooperation and confrontation, resulting in a large state and action space.","In 2021, a Doudizhu program called DouZero\\cite{zha2021douzero} surpassed previous models without prior knowledge by utilizing traditional Monte Carlo methods and multilayer perceptrons.","Building on this work, our study incorporates residual networks into the model, explores different architectural designs, and conducts multi-role testing.","Our findings demonstrate that this model significantly improves the winning rate within the same training time.","Additionally, we introduce a call scoring system to assist the agent in deciding whether to become a landlord.","With these enhancements, our model consistently outperforms the existing version of DouZero and even experienced human players.","\\footnote{The source code is available at \\url{https://github.com/Yingchaol/Douzero_Resnet.git.}"],"url":"http://arxiv.org/abs/2403.14102v1","category":"cs.AI"}
{"created":"2024-03-21 03:23:34","title":"Causal knowledge engineering: A case study from COVID-19","abstract":"COVID-19 appeared abruptly in early 2020, requiring a rapid response amid a context of great uncertainty. Good quality data and knowledge was initially lacking, and many early models had to be developed with causal assumptions and estimations built in to supplement limited data, often with no reliable approach for identifying, validating and documenting these causal assumptions. Our team embarked on a knowledge engineering process to develop a causal knowledge base consisting of several causal BNs for diverse aspects of COVID-19. The unique challenges of the setting lead to experiments with the elicitation approach, and what emerged was a knowledge engineering method we call Causal Knowledge Engineering (CKE). The CKE provides a structured approach for building a causal knowledge base that can support the development of a variety of application-specific models. Here we describe the CKE method, and use our COVID-19 work as a case study to provide a detailed discussion and analysis of the method.","sentences":["COVID-19 appeared abruptly in early 2020, requiring a rapid response amid a context of great uncertainty.","Good quality data and knowledge was initially lacking, and many early models had to be developed with causal assumptions and estimations built in to supplement limited data, often with no reliable approach for identifying, validating and documenting these causal assumptions.","Our team embarked on a knowledge engineering process to develop a causal knowledge base consisting of several causal BNs for diverse aspects of COVID-19.","The unique challenges of the setting lead to experiments with the elicitation approach, and what emerged was a knowledge engineering method we call Causal Knowledge Engineering (CKE).","The CKE provides a structured approach for building a causal knowledge base that can support the development of a variety of application-specific models.","Here we describe the CKE method, and use our COVID-19 work as a case study to provide a detailed discussion and analysis of the method."],"url":"http://arxiv.org/abs/2403.14100v1","category":"cs.AI"}
{"created":"2024-03-21 03:01:25","title":"Science based AI model certification for untrained operational environments with application in traffic state estimation","abstract":"The expanding role of Artificial Intelligence (AI) in diverse engineering domains highlights the challenges associated with deploying AI models in new operational environments, involving substantial investments in data collection and model training. Rapid application of AI necessitates evaluating the feasibility of utilizing pre-trained models in unobserved operational settings with minimal or no additional data. However, interpreting the opaque nature of AI's black-box models remains a persistent challenge. Addressing this issue, this paper proposes a science-based certification methodology to assess the viability of employing pre-trained data-driven models in untrained operational environments. The methodology advocates a profound integration of domain knowledge, leveraging theoretical and analytical models from physics and related disciplines, with data-driven AI models. This novel approach introduces tools to facilitate the development of secure engineering systems, providing decision-makers with confidence in the trustworthiness and safety of AI-based models across diverse environments characterized by limited training data and dynamic, uncertain conditions. The paper demonstrates the efficacy of this methodology in real-world safety-critical scenarios, particularly in the context of traffic state estimation. Through simulation results, the study illustrates how the proposed methodology efficiently quantifies physical inconsistencies exhibited by pre-trained AI models. By utilizing analytical models, the methodology offers a means to gauge the applicability of pre-trained AI models in new operational environments. This research contributes to advancing the understanding and deployment of AI models, offering a robust certification framework that enhances confidence in their reliability and safety across a spectrum of operational conditions.","sentences":["The expanding role of Artificial Intelligence (AI) in diverse engineering domains highlights the challenges associated with deploying AI models in new operational environments, involving substantial investments in data collection and model training.","Rapid application of AI necessitates evaluating the feasibility of utilizing pre-trained models in unobserved operational settings with minimal or no additional data.","However, interpreting the opaque nature of AI's black-box models remains a persistent challenge.","Addressing this issue, this paper proposes a science-based certification methodology to assess the viability of employing pre-trained data-driven models in untrained operational environments.","The methodology advocates a profound integration of domain knowledge, leveraging theoretical and analytical models from physics and related disciplines, with data-driven AI models.","This novel approach introduces tools to facilitate the development of secure engineering systems, providing decision-makers with confidence in the trustworthiness and safety of AI-based models across diverse environments characterized by limited training data and dynamic, uncertain conditions.","The paper demonstrates the efficacy of this methodology in real-world safety-critical scenarios, particularly in the context of traffic state estimation.","Through simulation results, the study illustrates how the proposed methodology efficiently quantifies physical inconsistencies exhibited by pre-trained AI models.","By utilizing analytical models, the methodology offers a means to gauge the applicability of pre-trained AI models in new operational environments.","This research contributes to advancing the understanding and deployment of AI models, offering a robust certification framework that enhances confidence in their reliability and safety across a spectrum of operational conditions."],"url":"http://arxiv.org/abs/2403.14093v1","category":"cs.CV"}
{"created":"2024-03-21 02:59:56","title":"Carbon Footprint Reduction for Sustainable Data Centers in Real-Time","abstract":"As machine learning workloads significantly increase energy consumption, sustainable data centers with low carbon emissions are becoming a top priority for governments and corporations worldwide. This requires a paradigm shift in optimizing power consumption in cooling and IT loads, shifting flexible loads based on the availability of renewable energy in the power grid, and leveraging battery storage from the uninterrupted power supply in data centers, using collaborative agents. The complex association between these optimization strategies and their dependencies on variable external factors like weather and the power grid carbon intensity makes this a hard problem. Currently, a real-time controller to optimize all these goals simultaneously in a dynamic real-world setting is lacking. We propose a Data Center Carbon Footprint Reduction (DC-CFR) multi-agent Reinforcement Learning (MARL) framework that optimizes data centers for the multiple objectives of carbon footprint reduction, energy consumption, and energy cost. The results show that the DC-CFR MARL agents effectively resolved the complex interdependencies in optimizing cooling, load shifting, and energy storage in real-time for various locations under real-world dynamic weather and grid carbon intensity conditions. DC-CFR significantly outperformed the industry standard ASHRAE controller with a considerable reduction in carbon emissions (14.5%), energy usage (14.4%), and energy cost (13.7%) when evaluated over one year across multiple geographical regions.","sentences":["As machine learning workloads significantly increase energy consumption, sustainable data centers with low carbon emissions are becoming a top priority for governments and corporations worldwide.","This requires a paradigm shift in optimizing power consumption in cooling and IT loads, shifting flexible loads based on the availability of renewable energy in the power grid, and leveraging battery storage from the uninterrupted power supply in data centers, using collaborative agents.","The complex association between these optimization strategies and their dependencies on variable external factors like weather and the power grid carbon intensity makes this a hard problem.","Currently, a real-time controller to optimize all these goals simultaneously in a dynamic real-world setting is lacking.","We propose a Data Center Carbon Footprint Reduction (DC-CFR) multi-agent Reinforcement Learning (MARL) framework that optimizes data centers for the multiple objectives of carbon footprint reduction, energy consumption, and energy cost.","The results show that the DC-CFR MARL agents effectively resolved the complex interdependencies in optimizing cooling, load shifting, and energy storage in real-time for various locations under real-world dynamic weather and grid carbon intensity conditions.","DC-CFR significantly outperformed the industry standard ASHRAE controller with a considerable reduction in carbon emissions (14.5%), energy usage (14.4%), and energy cost (13.7%) when evaluated over one year across multiple geographical regions."],"url":"http://arxiv.org/abs/2403.14092v1","category":"cs.LG"}
{"created":"2024-03-21 02:43:37","title":"Improved Algorithms for Maximum Coverage in Dynamic and Random Order Streams","abstract":"The maximum coverage problem is to select $k$ sets from a collection of sets such that the cardinality of the union of the selected sets is maximized. We consider $(1-1/e-\\epsilon)$-approximation algorithms for this NP-hard problem in three standard data stream models.   1. {\\em Dynamic Model.} The stream consists of a sequence of sets being inserted and deleted. Our multi-pass algorithm uses $\\epsilon^{-2} k \\cdot \\text{polylog}(n,m)$ space. The best previous result (Assadi and Khanna, SODA 2018) used $(n +\\epsilon^{-4} k) \\text{polylog}(n,m)$ space. While both algorithms use $O(\\epsilon^{-1} \\log n)$ passes, our analysis shows that when $\\epsilon$ is a constant, it is possible to reduce the number of passes by a $1/\\log \\log n$ factor without incurring additional space.   2. {\\em Random Order Model.} In this model, there are no deletions and the sets forming the instance are uniformly randomly permuted to form the input stream. We show that a single pass and $k \\text{polylog}(n,m)$ space suffices for arbitrary small constant $\\epsilon$. The best previous result, by Warneke et al.~(ESA 2023), used $k^2 \\text{polylog}(n,m)$ space.   3. {\\em Insert-Only Model.} Lastly, our results, along with numerous previous results, use a sub-sampling technique introduced by McGregor and Vu (ICDT 2017) to sparsify the input instance. We explain how this technique and others used in the paper can be implemented such that the amortized update time of our algorithm is polylogarithmic. This also implies an improvement of the state-of-the-art insert only algorithms in terms of the update time: $\\text{polylog}(m,n)$ update time suffices whereas the best previous result by Jaud et al.~(SEA 2023) required update time that was linear in $k$.","sentences":["The maximum coverage problem is to select $k$ sets from a collection of sets such that the cardinality of the union of the selected sets is maximized.","We consider $(1-1/e-\\epsilon)$-approximation algorithms for this NP-hard problem in three standard data stream models.   ","1.","{\\em Dynamic Model.}","The stream consists of a sequence of sets being inserted and deleted.","Our multi-pass algorithm uses $\\epsilon^{-2} k \\cdot \\text{polylog}(n,m)$ space.","The best previous result (Assadi and Khanna, SODA 2018) used $(n +\\epsilon^{-4} k) \\text{polylog}(n,m)$ space.","While both algorithms use $O(\\epsilon^{-1} \\log n)$ passes, our analysis shows that when $\\epsilon$ is a constant, it is possible to reduce the number of passes by a $1/\\log \\log n$ factor without incurring additional space.   ","2. {\\em Random Order Model.}","In this model, there are no deletions and the sets forming the instance are uniformly randomly permuted to form the input stream.","We show that a single pass and $k \\text{polylog}(n,m)$ space suffices for arbitrary small constant $\\epsilon$. The best previous result, by Warneke et al.~(ESA 2023), used $k^2 \\text{polylog}(n,m)$ space.   ","3. {\\em Insert-Only Model.}","Lastly, our results, along with numerous previous results, use a sub-sampling technique introduced by McGregor and Vu (ICDT 2017) to sparsify the input instance.","We explain how this technique and others used in the paper can be implemented such that the amortized update time of our algorithm is polylogarithmic.","This also implies an improvement of the state-of-the-art insert only algorithms in terms of the update time: $\\text{polylog}(m,n)$ update time suffices whereas the best previous result by Jaud et al.~(SEA 2023) required update time that was linear in $k$."],"url":"http://arxiv.org/abs/2403.14087v1","category":"cs.DS"}
{"created":"2024-03-21 01:57:30","title":"Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language Models for Media Forensics","abstract":"DeepFakes, which refer to AI-generated media content, have become an increasing concern due to their use as a means for disinformation. Detecting DeepFakes is currently solved with programmed machine learning algorithms. In this work, we investigate the capabilities of multimodal large language models (LLMs) in DeepFake detection. We conducted qualitative and quantitative experiments to demonstrate multimodal LLMs and show that they can expose AI-generated images through careful experimental design and prompt engineering. This is interesting, considering that LLMs are not inherently tailored for media forensic tasks, and the process does not require programming. We discuss the limitations of multimodal LLMs for these tasks and suggest possible improvements.","sentences":["DeepFakes, which refer to AI-generated media content, have become an increasing concern due to their use as a means for disinformation.","Detecting DeepFakes is currently solved with programmed machine learning algorithms.","In this work, we investigate the capabilities of multimodal large language models (LLMs) in DeepFake detection.","We conducted qualitative and quantitative experiments to demonstrate multimodal LLMs and show that they can expose AI-generated images through careful experimental design and prompt engineering.","This is interesting, considering that LLMs are not inherently tailored for media forensic tasks, and the process does not require programming.","We discuss the limitations of multimodal LLMs for these tasks and suggest possible improvements."],"url":"http://arxiv.org/abs/2403.14077v1","category":"cs.AI"}
{"created":"2024-03-21 00:59:35","title":"Semantics from Space: Satellite-Guided Thermal Semantic Segmentation Annotation for Aerial Field Robots","abstract":"We present a new method to automatically generate semantic segmentation annotations for thermal imagery captured from an aerial vehicle by utilizing satellite-derived data products alongside onboard global positioning and attitude estimates. This new capability overcomes the challenge of developing thermal semantic perception algorithms for field robots due to the lack of annotated thermal field datasets and the time and costs of manual annotation, enabling precise and rapid annotation of thermal data from field collection efforts at a massively-parallelizable scale. By incorporating a thermal-conditioned refinement step with visual foundation models, our approach can produce highly-precise semantic segmentation labels using low-resolution satellite land cover data for little-to-no cost. It achieves 98.5% of the performance from using costly high-resolution options and demonstrates between 70-160% improvement over popular zero-shot semantic segmentation methods based on large vision-language models currently used for generating annotations for RGB imagery. Code will be available at: https://github.com/connorlee77/aerial-auto-segment.","sentences":["We present a new method to automatically generate semantic segmentation annotations for thermal imagery captured from an aerial vehicle by utilizing satellite-derived data products alongside onboard global positioning and attitude estimates.","This new capability overcomes the challenge of developing thermal semantic perception algorithms for field robots due to the lack of annotated thermal field datasets and the time and costs of manual annotation, enabling precise and rapid annotation of thermal data from field collection efforts at a massively-parallelizable scale.","By incorporating a thermal-conditioned refinement step with visual foundation models, our approach can produce highly-precise semantic segmentation labels using low-resolution satellite land cover data for little-to-no cost.","It achieves 98.5% of the performance from using costly high-resolution options and demonstrates between 70-160% improvement over popular zero-shot semantic segmentation methods based on large vision-language models currently used for generating annotations for RGB imagery.","Code will be available at: https://github.com/connorlee77/aerial-auto-segment."],"url":"http://arxiv.org/abs/2403.14056v1","category":"cs.CV"}
{"created":"2024-03-21 00:14:53","title":"A Roadmap Towards Automated and Regulated Robotic Systems","abstract":"The rapid development of generative technology opens up possibility for higher level of automation, and artificial intelligence (AI) embodiment in robotic systems is imminent. However, due to the blackbox nature of the generative technology, the generation of the knowledge and workflow scheme is uncontrolled, especially in a dynamic environment and a complex scene. This poses challenges to regulations in safety-demanding applications such as medical scenes. We argue that the unregulated generative processes from AI is fitted for low level end tasks, but intervention in the form of manual or automated regulation should happen post-workflow-generation and pre-robotic-execution. To address this, we propose a roadmap that can lead to fully automated and regulated robotic systems. In this paradigm, the high level policies are generated as structured graph data, enabling regulatory oversight and reusability, while the code base for lower level tasks is generated by generative models. Our approach aims the transitioning from expert knowledge to regulated action, akin to the iterative processes of study, practice, scrutiny, and execution in human tasks. We identify the generative and deterministic processes in a design cycle, where generative processes serve as a text-based world simulator and the deterministic processes generate the executable system. We propose State Machine Seralization Language (SMSL) to be the conversion point between text simulator and executable workflow control. From there, we analyze the modules involved based on the current literature, and discuss human in the loop. As a roadmap, this work identifies the current possible implementation and future work. This work does not provide an implemented system but envisions to inspire the researchers working on the direction in the roadmap. We implement the SMSL and D-SFO paradigm that serve as the starting point of the roadmap.","sentences":["The rapid development of generative technology opens up possibility for higher level of automation, and artificial intelligence (AI) embodiment in robotic systems is imminent.","However, due to the blackbox nature of the generative technology, the generation of the knowledge and workflow scheme is uncontrolled, especially in a dynamic environment and a complex scene.","This poses challenges to regulations in safety-demanding applications such as medical scenes.","We argue that the unregulated generative processes from AI is fitted for low level end tasks, but intervention in the form of manual or automated regulation should happen post-workflow-generation and pre-robotic-execution.","To address this, we propose a roadmap that can lead to fully automated and regulated robotic systems.","In this paradigm, the high level policies are generated as structured graph data, enabling regulatory oversight and reusability, while the code base for lower level tasks is generated by generative models.","Our approach aims the transitioning from expert knowledge to regulated action, akin to the iterative processes of study, practice, scrutiny, and execution in human tasks.","We identify the generative and deterministic processes in a design cycle, where generative processes serve as a text-based world simulator and the deterministic processes generate the executable system.","We propose State Machine Seralization Language (SMSL) to be the conversion point between text simulator and executable workflow control.","From there, we analyze the modules involved based on the current literature, and discuss human in the loop.","As a roadmap, this work identifies the current possible implementation and future work.","This work does not provide an implemented system but envisions to inspire the researchers working on the direction in the roadmap.","We implement the SMSL and D-SFO paradigm that serve as the starting point of the roadmap."],"url":"http://arxiv.org/abs/2403.14049v1","category":"cs.RO"}
{"created":"2024-03-21 00:13:59","title":"The NeurIPS 2023 Machine Learning for Audio Workshop: Affective Audio Benchmarks and Novel Data","abstract":"The NeurIPS 2023 Machine Learning for Audio Workshop brings together machine learning (ML) experts from various audio domains. There are several valuable audio-driven ML tasks, from speech emotion recognition to audio event detection, but the community is sparse compared to other ML areas, e.g., computer vision or natural language processing. A major limitation with audio is the available data; with audio being a time-dependent modality, high-quality data collection is time-consuming and costly, making it challenging for academic groups to apply their often state-of-the-art strategies to a larger, more generalizable dataset. In this short white paper, to encourage researchers with limited access to large-datasets, the organizers first outline several open-source datasets that are available to the community, and for the duration of the workshop are making several propriety datasets available. Namely, three vocal datasets, Hume-Prosody, Hume-VocalBurst, an acted emotional speech dataset Modulate-Sonata, and an in-game streamer dataset Modulate-Stream. We outline the current baselines on these datasets but encourage researchers from across audio to utilize them outside of the initial baseline tasks.","sentences":["The NeurIPS 2023 Machine Learning for Audio Workshop brings together machine learning (ML) experts from various audio domains.","There are several valuable audio-driven ML tasks, from speech emotion recognition to audio event detection, but the community is sparse compared to other ML areas, e.g., computer vision or natural language processing.","A major limitation with audio is the available data; with audio being a time-dependent modality, high-quality data collection is time-consuming and costly, making it challenging for academic groups to apply their often state-of-the-art strategies to a larger, more generalizable dataset.","In this short white paper, to encourage researchers with limited access to large-datasets, the organizers first outline several open-source datasets that are available to the community, and for the duration of the workshop are making several propriety datasets available.","Namely, three vocal datasets, Hume-Prosody, Hume-VocalBurst, an acted emotional speech dataset Modulate-Sonata, and an in-game streamer dataset Modulate-Stream.","We outline the current baselines on these datasets but encourage researchers from across audio to utilize them outside of the initial baseline tasks."],"url":"http://arxiv.org/abs/2403.14048v1","category":"cs.SD"}
{"created":"2024-03-20 23:28:11","title":"Non-evolutionary effects on Period change in Magellanic Cepheids I. New binary systems revealed from Light Travel Time Effect","abstract":"Period change studies give a window to probe into the evolution and dynamics of Cepheids. While evolutionary period changes have been well studied both observationally and theoretically, non-evolutionary period changes lack a systematic and quantitative description. The overall objective is to have a quantitative understanding of the full picture of non-evolutionary period changes in Cepheids, to develop a formalism to disentangle it from the secular evolutionary period change. In the first part of the series of works, we aim to conduct a systematic search for non-evolutionary period changes to search for Cepheids in likely binary configuration and quantify their incidence rates in the Magellanic Clouds. We collect more than decade-long time-series photometry from the publically available survey, Optical Gravitational Lensing Experiment (OGLE), with more than 7200 Cepheids collectively from the Large Magellanic Cloud (LMC) and Small Magellanic Cloud (SMC). Our sample contains both fundamental-mode and first overtone-mode Cepheids. Then we calculate observed minus calculated ($O-C$) diagrams to reveal the light-travel time effect (LTTE). In our search, out of an overall sample of more than 7200 Cepheids, we found 52 candidate Cepheid binary systems in the LMC (30 fundamental and 22 first overtone-mode) and 145 in the SMC (85 fundamental and 60 first overtone-mode). The majority of the sample is characterized by orbital periods of 2000-4000\\,d and eccentricities of 0.2-0.5. Moreover, we report two candidates in each galaxy with the Cepheid likely existing with a giant companion. The incidence rate ratio for SMC to LMC calculated from our sample is in agreement with binary Cepheid population synthesis predictions.","sentences":["Period change studies give a window to probe into the evolution and dynamics of Cepheids.","While evolutionary period changes have been well studied both observationally and theoretically, non-evolutionary period changes lack a systematic and quantitative description.","The overall objective is to have a quantitative understanding of the full picture of non-evolutionary period changes in Cepheids, to develop a formalism to disentangle it from the secular evolutionary period change.","In the first part of the series of works, we aim to conduct a systematic search for non-evolutionary period changes to search for Cepheids in likely binary configuration and quantify their incidence rates in the Magellanic Clouds.","We collect more than decade-long time-series photometry from the publically available survey, Optical Gravitational Lensing Experiment (OGLE), with more than 7200 Cepheids collectively from the Large Magellanic Cloud (LMC) and Small Magellanic Cloud (SMC).","Our sample contains both fundamental-mode and first overtone-mode Cepheids.","Then we calculate observed minus calculated ($O-C$) diagrams to reveal the light-travel time effect (LTTE).","In our search, out of an overall sample of more than 7200 Cepheids, we found 52 candidate Cepheid binary systems in the LMC (30 fundamental and 22 first overtone-mode) and 145 in the SMC (85 fundamental and 60 first overtone-mode).","The majority of the sample is characterized by orbital periods of 2000-4000\\,d and eccentricities of 0.2-0.5.","Moreover, we report two candidates in each galaxy with the Cepheid likely existing with a giant companion.","The incidence rate ratio for SMC to LMC calculated from our sample is in agreement with binary Cepheid population synthesis predictions."],"url":"http://arxiv.org/abs/2403.14039v1","category":"astro-ph.SR"}
{"created":"2024-03-20 23:26:48","title":"PureConnect: A Localized Social Media System to Increase Awareness and Connectedness in Environmental Justice Communities","abstract":"Frequent disruptions like highway constructions are common now-a-days, often impacting environmental justice communities (communities with low socio-economic status with disproportionately high and adverse human health and environmental effects) that live nearby. Based on our interactions via focus groups with the members of four environmental justice communities impacted by a major highway construction, a common concern is a sense of uncertainty about project activities and loss of social connectedness, leading to increased stress, depression, anxiety and diminished well-being. This paper addresses this concern by developing a localized social media system called PureConnect with a goal to raise the level of awareness about the project and increase social connectedness among the community members. PureConnect has been designed using active engagement with four environmental justice communities affected by a major highway construction. It has been deployed in the real world among the members of the four environmental justice communities, and a detailed analysis of the data collected from this deployment as well as surveys show that PureConnect is potentially useful in improving community members' well-being and the members appreciate the functionalities it provides.","sentences":["Frequent disruptions like highway constructions are common now-a-days, often impacting environmental justice communities (communities with low socio-economic status with disproportionately high and adverse human health and environmental effects) that live nearby.","Based on our interactions via focus groups with the members of four environmental justice communities impacted by a major highway construction, a common concern is a sense of uncertainty about project activities and loss of social connectedness, leading to increased stress, depression, anxiety and diminished well-being.","This paper addresses this concern by developing a localized social media system called PureConnect with a goal to raise the level of awareness about the project and increase social connectedness among the community members.","PureConnect has been designed using active engagement with four environmental justice communities affected by a major highway construction.","It has been deployed in the real world among the members of the four environmental justice communities, and a detailed analysis of the data collected from this deployment as well as surveys show that PureConnect is potentially useful in improving community members' well-being and the members appreciate the functionalities it provides."],"url":"http://arxiv.org/abs/2403.14038v1","category":"cs.SI"}
{"created":"2024-03-20 23:21:35","title":"Ax-to-Grind Urdu: Benchmark Dataset for Urdu Fake News Detection","abstract":"Misinformation can seriously impact society, affecting anything from public opinion to institutional confidence and the political horizon of a state. Fake News (FN) proliferation on online websites and Online Social Networks (OSNs) has increased profusely. Various fact-checking websites include news in English and barely provide information about FN in regional languages. Thus the Urdu FN purveyors cannot be discerned using factchecking portals. SOTA approaches for Fake News Detection (FND) count upon appropriately labelled and large datasets. FND in regional and resource-constrained languages lags due to the lack of limited-sized datasets and legitimate lexical resources. The previous datasets for Urdu FND are limited-sized, domain-restricted, publicly unavailable and not manually verified where the news is translated from English into Urdu. In this paper, we curate and contribute the first largest publicly available dataset for Urdu FND, Ax-to-Grind Urdu, to bridge the identified gaps and limitations of existing Urdu datasets in the literature. It constitutes 10,083 fake and real news on fifteen domains collected from leading and authentic Urdu newspapers and news channel websites in Pakistan and India. FN for the Ax-to-Grind dataset is collected from websites and crowdsourcing. The dataset contains news items in Urdu from the year 2017 to the year 2023. Expert journalists annotated the dataset. We benchmark the dataset with an ensemble model of mBERT,XLNet, and XLM RoBERTa. The selected models are originally trained on multilingual large corpora. The results of the proposed model are based on performance metrics, F1-score, accuracy, precision, recall and MCC value.","sentences":["Misinformation can seriously impact society, affecting anything from public opinion to institutional confidence and the political horizon of a state.","Fake News (FN) proliferation on online websites and Online Social Networks (OSNs) has increased profusely.","Various fact-checking websites include news in English and barely provide information about FN in regional languages.","Thus the Urdu FN purveyors cannot be discerned using factchecking portals.","SOTA approaches for Fake News Detection (FND) count upon appropriately labelled and large datasets.","FND in regional and resource-constrained languages lags due to the lack of limited-sized datasets and legitimate lexical resources.","The previous datasets for Urdu FND are limited-sized, domain-restricted, publicly unavailable and not manually verified where the news is translated from English into Urdu.","In this paper, we curate and contribute the first largest publicly available dataset for Urdu FND, Ax-to-Grind Urdu, to bridge the identified gaps and limitations of existing Urdu datasets in the literature.","It constitutes 10,083 fake and real news on fifteen domains collected from leading and authentic Urdu newspapers and news channel websites in Pakistan and India.","FN for the Ax-to-Grind dataset is collected from websites and crowdsourcing.","The dataset contains news items in Urdu from the year 2017 to the year 2023.","Expert journalists annotated the dataset.","We benchmark the dataset with an ensemble model of mBERT,XLNet, and XLM RoBERTa.","The selected models are originally trained on multilingual large corpora.","The results of the proposed model are based on performance metrics, F1-score, accuracy, precision, recall and MCC value."],"url":"http://arxiv.org/abs/2403.14037v1","category":"cs.CL"}
{"created":"2024-03-20 22:52:34","title":"EcoSense: Energy-Efficient Intelligent Sensing for In-Shore Ship Detection through Edge-Cloud Collaboration","abstract":"Detecting marine objects inshore presents challenges owing to algorithmic intricacies and complexities in system deployment. We propose a difficulty-aware edge-cloud collaborative sensing system that splits the task into object localization and fine-grained classification. Objects are classified either at the edge or within the cloud, based on their estimated difficulty. The framework comprises a low-power device-tailored front-end model for object localization, classification, and difficulty estimation, along with a transformer-graph convolutional network-based back-end model for fine-grained classification. Our system demonstrates superior performance (mAP@0.5 +4.3%}) on widely used marine object detection datasets, significantly reducing both data transmission volume (by 95.43%) and energy consumption (by 72.7%}) at the system level. We validate the proposed system across various embedded system platforms and in real-world scenarios involving drone deployment.","sentences":["Detecting marine objects inshore presents challenges owing to algorithmic intricacies and complexities in system deployment.","We propose a difficulty-aware edge-cloud collaborative sensing system that splits the task into object localization and fine-grained classification.","Objects are classified either at the edge or within the cloud, based on their estimated difficulty.","The framework comprises a low-power device-tailored front-end model for object localization, classification, and difficulty estimation, along with a transformer-graph convolutional network-based back-end model for fine-grained classification.","Our system demonstrates superior performance (mAP@0.5 +4.3%}) on widely used marine object detection datasets, significantly reducing both data transmission volume (by 95.43%) and energy consumption (by 72.7%}) at the system level.","We validate the proposed system across various embedded system platforms and in real-world scenarios involving drone deployment."],"url":"http://arxiv.org/abs/2403.14027v1","category":"cs.CV"}
{"created":"2024-03-20 22:49:00","title":"A system capable of verifiably and privately screening global DNA synthesis","abstract":"Printing custom DNA sequences is essential to scientific and biomedical research, but the technology can be used to manufacture plagues as well as cures. Just as ink printers recognize and reject attempts to counterfeit money, DNA synthesizers and assemblers should deny unauthorized requests to make viral DNA that could be used to ignite a pandemic. There are three complications. First, we don't need to quickly update printers to deal with newly discovered currencies, whereas we regularly learn of new viruses and other biological threats. Second, anti-counterfeiting specifications on a local printer can't be extracted and misused by malicious actors, unlike information on biological threats. Finally, any screening must keep the inspected DNA sequences private, as they may constitute valuable trade secrets. Here we describe SecureDNA, a free, privacy-preserving, and fully automated system capable of verifiably screening all DNA synthesis orders of 30+ base pairs against an up-to-date database of hazards, and its operational performance and specificity when applied to 67 million base pairs of DNA synthesized by providers in the United States, Europe, and China.","sentences":["Printing custom DNA sequences is essential to scientific and biomedical research, but the technology can be used to manufacture plagues as well as cures.","Just as ink printers recognize and reject attempts to counterfeit money, DNA synthesizers and assemblers should deny unauthorized requests to make viral DNA that could be used to ignite a pandemic.","There are three complications.","First, we don't need to quickly update printers to deal with newly discovered currencies, whereas we regularly learn of new viruses and other biological threats.","Second, anti-counterfeiting specifications on a local printer can't be extracted and misused by malicious actors, unlike information on biological threats.","Finally, any screening must keep the inspected DNA sequences private, as they may constitute valuable trade secrets.","Here we describe SecureDNA, a free, privacy-preserving, and fully automated system capable of verifiably screening all DNA synthesis orders of 30+ base pairs against an up-to-date database of hazards, and its operational performance and specificity when applied to 67 million base pairs of DNA synthesized by providers in the United States, Europe, and China."],"url":"http://arxiv.org/abs/2403.14023v1","category":"cs.CR"}
{"created":"2024-03-20 22:41:15","title":"Zero-Knowledge Proof of Distinct Identity: a Standard-compatible Sybil-resistant Pseudonym Extension for C-ITS","abstract":"Pseudonyms are widely used in Cooperative Intelligent Transport Systems (C-ITS) to protect the location privacy of vehicles. However, the unlinkability nature of pseudonyms also enables Sybil attacks, where a malicious vehicle can pretend to be multiple vehicles at the same time. In this paper, we propose a novel protocol called zero-knowledge Proof of Distinct Identity (zk-PoDI,) which allows a vehicle to prove that it is not the owner of another pseudonym in the local area, without revealing its actual identity. Zk-PoDI is based on the Diophantine equation and zk-SNARK, and does not rely on any specific pseudonym design or infrastructure assistance. We show that zk-PoDI satisfies all the requirements for a practical Sybil-resistance pseudonym system, and it has low latency, adjustable difficulty, moderate computation overhead, and negligible communication cost. We also discuss the future work of implementing and evaluating zk-PoDI in a realistic city-scale simulation environment.","sentences":["Pseudonyms are widely used in Cooperative Intelligent Transport Systems (C-ITS) to protect the location privacy of vehicles.","However, the unlinkability nature of pseudonyms also enables Sybil attacks, where a malicious vehicle can pretend to be multiple vehicles at the same time.","In this paper, we propose a novel protocol called zero-knowledge Proof of Distinct Identity (zk-PoDI,) which allows a vehicle to prove that it is not the owner of another pseudonym in the local area, without revealing its actual identity.","Zk-PoDI is based on the Diophantine equation and zk-SNARK, and does not rely on any specific pseudonym design or infrastructure assistance.","We show that zk-PoDI satisfies all the requirements for a practical Sybil-resistance pseudonym system, and it has low latency, adjustable difficulty, moderate computation overhead, and negligible communication cost.","We also discuss the future work of implementing and evaluating zk-PoDI in a realistic city-scale simulation environment."],"url":"http://arxiv.org/abs/2403.14020v1","category":"cs.CR"}
{"created":"2024-03-20 22:40:53","title":"Searching Search Spaces: Meta-evolving a Geometric Encoding for Neural Networks","abstract":"In evolutionary policy search, neural networks are usually represented using a direct mapping: each gene encodes one network weight. Indirect encoding methods, where each gene can encode for multiple weights, shorten the genome to reduce the dimensions of the search space and better exploit permutations and symmetries. The Geometric Encoding for Neural network Evolution (GENE) introduced an indirect encoding where the weight of a connection is computed as the (pseudo-)distance between the two linked neurons, leading to a genome size growing linearly with the number of genes instead of quadratically in direct encoding. However GENE still relies on hand-crafted distance functions with no prior optimization. Here we show that better performing distance functions can be found for GENE using Cartesian Genetic Programming (CGP) in a meta-evolution approach, hence optimizing the encoding to create a search space that is easier to exploit. We show that GENE with a learned function can outperform both direct encoding and the hand-crafted distances, generalizing on unseen problems, and we study how the encoding impacts neural network properties.","sentences":["In evolutionary policy search, neural networks are usually represented using a direct mapping: each gene encodes one network weight.","Indirect encoding methods, where each gene can encode for multiple weights, shorten the genome to reduce the dimensions of the search space and better exploit permutations and symmetries.","The Geometric Encoding for Neural network Evolution (GENE) introduced an indirect encoding where the weight of a connection is computed as the (pseudo-)distance between the two linked neurons, leading to a genome size growing linearly with the number of genes instead of quadratically in direct encoding.","However GENE still relies on hand-crafted distance functions with no prior optimization.","Here we show that better performing distance functions can be found for GENE using Cartesian Genetic Programming (CGP) in a meta-evolution approach, hence optimizing the encoding to create a search space that is easier to exploit.","We show that GENE with a learned function can outperform both direct encoding and the hand-crafted distances, generalizing on unseen problems, and we study how the encoding impacts neural network properties."],"url":"http://arxiv.org/abs/2403.14019v1","category":"cs.NE"}
{"created":"2024-03-20 22:24:52","title":"Crowdsourcing Task Traces for Service Robotics","abstract":"Demonstration is an effective end-user development paradigm for teaching robots how to perform new tasks. In this paper, we posit that demonstration is useful not only as a teaching tool, but also as a way to understand and assist end-user developers in thinking about a task at hand. As a first step toward gaining this understanding, we constructed a lightweight web interface to crowdsource step-by-step instructions of common household tasks, leveraging the imaginations and past experiences of potential end-user developers. As evidence of the utility of our interface, we deployed the interface on Amazon Mechanical Turk and collected 207 task traces that span 18 different task categories. We describe our vision for how these task traces can be operationalized as task models within end-user development tools and provide a roadmap for future work.","sentences":["Demonstration is an effective end-user development paradigm for teaching robots how to perform new tasks.","In this paper, we posit that demonstration is useful not only as a teaching tool, but also as a way to understand and assist end-user developers in thinking about a task at hand.","As a first step toward gaining this understanding, we constructed a lightweight web interface to crowdsource step-by-step instructions of common household tasks, leveraging the imaginations and past experiences of potential end-user developers.","As evidence of the utility of our interface, we deployed the interface on Amazon Mechanical Turk and collected 207 task traces that span 18 different task categories.","We describe our vision for how these task traces can be operationalized as task models within end-user development tools and provide a roadmap for future work."],"url":"http://arxiv.org/abs/2403.14014v1","category":"cs.HC"}
{"created":"2024-03-20 22:14:39","title":"A New Massive Multilingual Dataset for High-Performance Language Technologies","abstract":"We present the HPLT (High Performance Language Technologies) language resources, a new massive multilingual dataset including both monolingual and bilingual corpora extracted from CommonCrawl and previously unused web crawls from the Internet Archive. We describe our methods for data acquisition, management and processing of large corpora, which rely on open-source software tools and high-performance computing. Our monolingual collection focuses on low- to medium-resourced languages and covers 75 languages and a total of ~5.6 trillion word tokens de-duplicated on the document level. Our English-centric parallel corpus is derived from its monolingual counterpart and covers 18 language pairs and more than 96 million aligned sentence pairs with roughly 1.4 billion English tokens. The HPLT language resources are one of the largest open text corpora ever released, providing a great resource for language modeling and machine translation training. We publicly release the corpora, the software, and the tools used in this work.","sentences":["We present the HPLT (High Performance Language Technologies) language resources, a new massive multilingual dataset including both monolingual and bilingual corpora extracted from CommonCrawl and previously unused web crawls from the Internet Archive.","We describe our methods for data acquisition, management and processing of large corpora, which rely on open-source software tools and high-performance computing.","Our monolingual collection focuses on low- to medium-resourced languages and covers 75 languages and a total of ~5.6 trillion word tokens de-duplicated on the document level.","Our English-centric parallel corpus is derived from its monolingual counterpart and covers 18 language pairs and more than 96 million aligned sentence pairs with roughly 1.4 billion English tokens.","The HPLT language resources are one of the largest open text corpora ever released, providing a great resource for language modeling and machine translation training.","We publicly release the corpora, the software, and the tools used in this work."],"url":"http://arxiv.org/abs/2403.14009v1","category":"cs.CL"}
{"created":"2024-03-20 22:11:01","title":"On Prompt Sensitivity of ChatGPT in Affective Computing","abstract":"Recent studies have demonstrated the emerging capabilities of foundation models like ChatGPT in several fields, including affective computing. However, accessing these emerging capabilities is facilitated through prompt engineering. Despite the existence of some prompting techniques, the field is still rapidly evolving and many prompting ideas still require investigation. In this work, we introduce a method to evaluate and investigate the sensitivity of the performance of foundation models based on different prompts or generation parameters. We perform our evaluation on ChatGPT within the scope of affective computing on three major problems, namely sentiment analysis, toxicity detection, and sarcasm detection. First, we carry out a sensitivity analysis on pivotal parameters in auto-regressive text generation, specifically the temperature parameter $T$ and the top-$p$ parameter in Nucleus sampling, dictating how conservative or creative the model should be during generation. Furthermore, we explore the efficacy of several prompting ideas, where we explore how giving different incentives or structures affect the performance. Our evaluation takes into consideration performance measures on the affective computing tasks, and the effectiveness of the model to follow the stated instructions, hence generating easy-to-parse responses to be smoothly used in downstream applications.","sentences":["Recent studies have demonstrated the emerging capabilities of foundation models like ChatGPT in several fields, including affective computing.","However, accessing these emerging capabilities is facilitated through prompt engineering.","Despite the existence of some prompting techniques, the field is still rapidly evolving and many prompting ideas still require investigation.","In this work, we introduce a method to evaluate and investigate the sensitivity of the performance of foundation models based on different prompts or generation parameters.","We perform our evaluation on ChatGPT within the scope of affective computing on three major problems, namely sentiment analysis, toxicity detection, and sarcasm detection.","First, we carry out a sensitivity analysis on pivotal parameters in auto-regressive text generation, specifically the temperature parameter $T$ and the top-$p$ parameter in Nucleus sampling, dictating how conservative or creative the model should be during generation.","Furthermore, we explore the efficacy of several prompting ideas, where we explore how giving different incentives or structures affect the performance.","Our evaluation takes into consideration performance measures on the affective computing tasks, and the effectiveness of the model to follow the stated instructions, hence generating easy-to-parse responses to be smoothly used in downstream applications."],"url":"http://arxiv.org/abs/2403.14006v1","category":"cs.CL"}
{"created":"2024-03-20 22:03:40","title":"Uncertainty Driven Active Learning for Image Segmentation in Underwater Inspection","abstract":"Active learning aims to select the minimum amount of data to train a model that performs similarly to a model trained with the entire dataset. We study the potential of active learning for image segmentation in underwater infrastructure inspection tasks, where large amounts of data are typically collected. The pipeline inspection images are usually semantically repetitive but with great variations in quality. We use mutual information as the acquisition function, calculated using Monte Carlo dropout. To assess the effectiveness of the framework, DenseNet and HyperSeg are trained with the CamVid dataset using active learning. In addition, HyperSeg is trained with a pipeline inspection dataset of over 50,000 images. For the pipeline dataset, HyperSeg with active learning achieved 67.5% meanIoU using 12.5% of the data, and 61.4% with the same amount of randomly selected images. This shows that using active learning for segmentation models in underwater inspection tasks can lower the cost significantly.","sentences":["Active learning aims to select the minimum amount of data to train a model that performs similarly to a model trained with the entire dataset.","We study the potential of active learning for image segmentation in underwater infrastructure inspection tasks, where large amounts of data are typically collected.","The pipeline inspection images are usually semantically repetitive but with great variations in quality.","We use mutual information as the acquisition function, calculated using Monte Carlo dropout.","To assess the effectiveness of the framework, DenseNet and HyperSeg are trained with the CamVid dataset using active learning.","In addition, HyperSeg is trained with a pipeline inspection dataset of over 50,000 images.","For the pipeline dataset, HyperSeg with active learning achieved 67.5% meanIoU using 12.5% of the data, and 61.4% with the same amount of randomly selected images.","This shows that using active learning for segmentation models in underwater inspection tasks can lower the cost significantly."],"url":"http://arxiv.org/abs/2403.14002v1","category":"cs.CV"}
{"created":"2024-03-20 21:24:40","title":"Virasoro constraints and representations for quiver moduli spaces","abstract":"We study the Virasoro constraints for moduli spaces of representations of quiver with relations by Joyce's vertex algebras. Using the framed Virasoro constraints, we construct a representation of half of the Virasoro algebra on the cohomology of moduli stacks of quiver representations under smoothness assumption. By exploiting the non-commutative nature of the Virasoro operators, we apply our theory for quivers to del Pezzo surfaces using exceptional collections. In particular, the Virasoro constraints and representations are proven for moduli of sheaves on $\\mathbb{P}^2$, $\\mathbb{P}^1\\times \\mathbb{P}^1$ and $\\text{Bl}_{\\mathsf{pt}}(\\mathbb{P}^2)$. Lastly, we unravel the Virasoro constraints for Grassmannians in terms of symmetric polynomials and Hecke operators.","sentences":["We study the Virasoro constraints for moduli spaces of representations of quiver with relations by Joyce's vertex algebras.","Using the framed Virasoro constraints, we construct a representation of half of the Virasoro algebra on the cohomology of moduli stacks of quiver representations under smoothness assumption.","By exploiting the non-commutative nature of the Virasoro operators, we apply our theory for quivers to del Pezzo surfaces using exceptional collections.","In particular, the Virasoro constraints and representations are proven for moduli of sheaves on $\\mathbb{P}^2$, $\\mathbb{P}^1\\times \\mathbb{P}^1$ and $\\text{Bl}_{\\mathsf{pt}}(\\mathbb{P}^2)$. Lastly, we unravel the Virasoro constraints for Grassmannians in terms of symmetric polynomials and Hecke operators."],"url":"http://arxiv.org/abs/2403.13982v1","category":"math.AG"}
{"created":"2024-03-20 20:54:43","title":"The equational theory of the Weihrauch lattice with multiplication","abstract":"We study the equational theory of the Weihrauch lattice with multiplication, meaning the collection of equations between terms built from variables, the lattice operations $\\sqcup$, $\\sqcap$, the product $\\times$, and the finite parallelization $(-)^*$ which are true however we substitute Weihrauch degrees for the variables. We provide a combinatorial description of these in terms of a reducibility between finite graphs, and moreover, show that deciding which equations are true in this sense is complete for the third level of the polynomial hierarchy.","sentences":["We study the equational theory of the Weihrauch lattice with multiplication, meaning the collection of equations between terms built from variables, the lattice operations $\\sqcup$, $\\sqcap$, the product $\\times$, and the finite parallelization $(-)^*$ which are true however we substitute Weihrauch degrees for the variables.","We provide a combinatorial description of these in terms of a reducibility between finite graphs, and moreover, show that deciding which equations are true in this sense is complete for the third level of the polynomial hierarchy."],"url":"http://arxiv.org/abs/2403.13975v1","category":"cs.LO"}
{"created":"2024-03-20 20:46:41","title":"\"This is not a data problem\": Algorithms and Power in Public Higher Education in Canada","abstract":"Algorithmic decision-making is increasingly being adopted across public higher education. The expansion of data-driven practices by post-secondary institutions has occurred in parallel with the adoption of New Public Management approaches by neoliberal administrations. In this study, we conduct a qualitative analysis of an in-depth ethnographic case study of data and algorithms in use at a public college in Ontario, Canada. We identify the data, algorithms, and outcomes in use at the college. We assess how the college's processes and relationships support those outcomes and the different stakeholders' perceptions of the college's data-driven systems. In addition, we find that the growing reliance on algorithmic decisions leads to increased student surveillance, exacerbation of existing inequities, and the automation of the faculty-student relationship. Finally, we identify a cycle of increased institutional power perpetuated by algorithmic decision-making, and driven by a push towards financial sustainability.","sentences":["Algorithmic decision-making is increasingly being adopted across public higher education.","The expansion of data-driven practices by post-secondary institutions has occurred in parallel with the adoption of New Public Management approaches by neoliberal administrations.","In this study, we conduct a qualitative analysis of an in-depth ethnographic case study of data and algorithms in use at a public college in Ontario, Canada.","We identify the data, algorithms, and outcomes in use at the college.","We assess how the college's processes and relationships support those outcomes and the different stakeholders' perceptions of the college's data-driven systems.","In addition, we find that the growing reliance on algorithmic decisions leads to increased student surveillance, exacerbation of existing inequities, and the automation of the faculty-student relationship.","Finally, we identify a cycle of increased institutional power perpetuated by algorithmic decision-making, and driven by a push towards financial sustainability."],"url":"http://arxiv.org/abs/2403.13969v1","category":"cs.HC"}
{"created":"2024-03-20 20:13:39","title":"Open Access NAO (OAN): a ROS2-based software framework for HRI applications with the NAO robot","abstract":"This paper presents a new software framework for HRI experimentation with the sixth version of the common NAO robot produced by the United Robotics Group. Embracing the common demand of researchers for better performance and new features for NAO, the authors took advantage of the ability to run ROS2 onboard on the NAO to develop a framework independent of the APIs provided by the manufacturer. Such a system provides NAO with not only the basic skills of a humanoid robot such as walking and reproducing movements of interest but also features often used in HRI such as: speech recognition/synthesis, face and object detention, and the use of Generative Pre-trained Transformer (GPT) models for conversation. The developed code is therefore configured as a ready-to-use but also highly expandable and improvable tool thanks to the possibilities provided by the ROS community.","sentences":["This paper presents a new software framework for HRI experimentation with the sixth version of the common NAO robot produced by the United Robotics Group.","Embracing the common demand of researchers for better performance and new features for NAO, the authors took advantage of the ability to run ROS2 onboard on the NAO to develop a framework independent of the APIs provided by the manufacturer.","Such a system provides NAO with not only the basic skills of a humanoid robot such as walking and reproducing movements of interest but also features often used in HRI such as: speech recognition/synthesis, face and object detention, and the use of Generative Pre-trained Transformer (GPT) models for conversation.","The developed code is therefore configured as a ready-to-use but also highly expandable and improvable tool thanks to the possibilities provided by the ROS community."],"url":"http://arxiv.org/abs/2403.13960v1","category":"cs.RO"}
{"created":"2024-03-20 19:49:21","title":"Considerations in the use of ML interaction potentials for free energy calculations","abstract":"Machine learning potentials (MLPs) offer the potential to accurately model the energy and free energy landscapes of molecules with the precision of quantum mechanics and an efficiency similar to classical simulations. This research focuses on using equivariant graph neural networks MLPs due to their proven effectiveness in modeling equilibrium molecular trajectories. A key issue addressed is the capability of MLPs to accurately predict free energies and transition states by considering both the energy and the diversity of molecular configurations. We examined how the distribution of collective variables (CVs) in the training data affects MLP accuracy in determining the free energy surface (FES) of systems, using Metadynamics simulations for butane and alanine dipeptide (ADP). The study involved training forty-three MLPs, half based on classical molecular dynamics data and the rest on ab initio computed energies. The MLPs were trained using different distributions that aim to replicate hypothetical scenarios of sampled CVs obtained if the underlying FES of the system was unknown. Findings for butane revealed that training data coverage of key FES regions ensures model accuracy regardless of CV distribution. However, missing significant FES regions led to correct potential energy predictions but failed free energy reconstruction. For ADP, models trained on classical dynamics data were notably less accurate, while ab initio-based MLPs predicted potential energy well but faltered on free energy predictions. These results emphasize the challenge of assembling an all-encompassing training set for accurate FES prediction and highlight the importance of understanding the FES in preparing training data. The study points out the limitations of MLPs in free energy calculations, stressing the need for comprehensive data that encompasses the system's full FES for effective model training.","sentences":["Machine learning potentials (MLPs) offer the potential to accurately model the energy and free energy landscapes of molecules with the precision of quantum mechanics and an efficiency similar to classical simulations.","This research focuses on using equivariant graph neural networks MLPs due to their proven effectiveness in modeling equilibrium molecular trajectories.","A key issue addressed is the capability of MLPs to accurately predict free energies and transition states by considering both the energy and the diversity of molecular configurations.","We examined how the distribution of collective variables (CVs) in the training data affects MLP accuracy in determining the free energy surface (FES) of systems, using Metadynamics simulations for butane and alanine dipeptide (ADP).","The study involved training forty-three MLPs, half based on classical molecular dynamics data and the rest on ab initio computed energies.","The MLPs were trained using different distributions that aim to replicate hypothetical scenarios of sampled CVs obtained if the underlying FES of the system was unknown.","Findings for butane revealed that training data coverage of key FES regions ensures model accuracy regardless of CV distribution.","However, missing significant FES regions led to correct potential energy predictions but failed free energy reconstruction.","For ADP, models trained on classical dynamics data were notably less accurate, while ab initio-based MLPs predicted potential energy well but faltered on free energy predictions.","These results emphasize the challenge of assembling an all-encompassing training set for accurate FES prediction and highlight the importance of understanding the FES in preparing training data.","The study points out the limitations of MLPs in free energy calculations, stressing the need for comprehensive data that encompasses the system's full FES for effective model training."],"url":"http://arxiv.org/abs/2403.13952v1","category":"physics.chem-ph"}
{"created":"2024-03-20 19:45:06","title":"ACDG-VTON: Accurate and Contained Diffusion Generation for Virtual Try-On","abstract":"Virtual Try-on (VTON) involves generating images of a person wearing selected garments. Diffusion-based methods, in particular, can create high-quality images, but they struggle to maintain the identities of the input garments. We identified this problem stems from the specifics in the training formulation for diffusion. To address this, we propose a unique training scheme that limits the scope in which diffusion is trained. We use a control image that perfectly aligns with the target image during training. In turn, this accurately preserves garment details during inference. We demonstrate our method not only effectively conserves garment details but also allows for layering, styling, and shoe try-on. Our method runs multi-garment try-on in a single inference cycle and can support high-quality zoomed-in generations without training in higher resolutions. Finally, we show our method surpasses prior methods in accuracy and quality.","sentences":["Virtual Try-on (VTON) involves generating images of a person wearing selected garments.","Diffusion-based methods, in particular, can create high-quality images, but they struggle to maintain the identities of the input garments.","We identified this problem stems from the specifics in the training formulation for diffusion.","To address this, we propose a unique training scheme that limits the scope in which diffusion is trained.","We use a control image that perfectly aligns with the target image during training.","In turn, this accurately preserves garment details during inference.","We demonstrate our method not only effectively conserves garment details but also allows for layering, styling, and shoe try-on.","Our method runs multi-garment try-on in a single inference cycle and can support high-quality zoomed-in generations without training in higher resolutions.","Finally, we show our method surpasses prior methods in accuracy and quality."],"url":"http://arxiv.org/abs/2403.13951v1","category":"cs.CV"}
{"created":"2024-03-20 19:42:11","title":"Evo* 2023 -- Late-Breaking Abstracts Volume","abstract":"Volume with the Late-Breaking Abstracts submitted to the Evo* 2023 Conference, held in Brno (Czech Republic), from 12 to 14 of April. These papers present ongoing research and preliminary results investigating on the application of different approaches of Bioinspired Methods (mainly Evolutionary Computation) to different problems, most of them real world ones.","sentences":["Volume with the Late-Breaking Abstracts submitted to the Evo* 2023 Conference, held in Brno (Czech Republic), from 12 to 14 of April.","These papers present ongoing research and preliminary results investigating on the application of different approaches of Bioinspired Methods (mainly Evolutionary Computation) to different problems, most of them real world ones."],"url":"http://arxiv.org/abs/2403.13950v1","category":"cs.NE"}
{"created":"2024-03-20 19:41:05","title":"BlendScape: Enabling Unified and Personalized Video-Conferencing Environments through Generative AI","abstract":"Today's video-conferencing tools support a rich range of professional and social activities, but their generic, grid-based environments cannot be easily adapted to meet the varying needs of distributed collaborators. To enable end-user customization, we developed BlendScape, a system for meeting participants to compose video-conferencing environments tailored to their collaboration context by leveraging AI image generation techniques. BlendScape supports flexible representations of task spaces by blending users' physical or virtual backgrounds into unified environments and implements multimodal interaction techniques to steer the generation. Through an evaluation with 15 end-users, we investigated their customization preferences for work and social scenarios. Participants could rapidly express their design intentions with BlendScape and envisioned using the system to structure collaboration in future meetings, but experienced challenges with preventing distracting elements. We implement scenarios to demonstrate BlendScape's expressiveness in supporting distributed collaboration techniques from prior work and propose composition techniques to improve the quality of environments.","sentences":["Today's video-conferencing tools support a rich range of professional and social activities, but their generic, grid-based environments cannot be easily adapted to meet the varying needs of distributed collaborators.","To enable end-user customization, we developed BlendScape, a system for meeting participants to compose video-conferencing environments tailored to their collaboration context by leveraging AI image generation techniques.","BlendScape supports flexible representations of task spaces by blending users' physical or virtual backgrounds into unified environments and implements multimodal interaction techniques to steer the generation.","Through an evaluation with 15 end-users, we investigated their customization preferences for work and social scenarios.","Participants could rapidly express their design intentions with BlendScape and envisioned using the system to structure collaboration in future meetings, but experienced challenges with preventing distracting elements.","We implement scenarios to demonstrate BlendScape's expressiveness in supporting distributed collaboration techniques from prior work and propose composition techniques to improve the quality of environments."],"url":"http://arxiv.org/abs/2403.13947v1","category":"cs.HC"}
{"created":"2024-03-20 19:26:27","title":"Sensory Glove-Based Surgical Robot User Interface","abstract":"Robotic surgery has reached a high level of maturity and has become an integral part of standard surgical care. However, existing surgeon consoles are bulky and take up valuable space in the operating room, present challenges for surgical team coordination, and their proprietary nature makes it difficult to take advantage of recent technological advances, especially in virtual and augmented reality. One potential area for further improvement is the integration of modern sensory gloves into robotic platforms, allowing surgeons to control robotic arms directly with their hand movements intuitively. We propose one such system that combines an HTC Vive tracker, a Manus Meta Prime 3 XR sensory glove, and God Vision wireless smart glasses. The system controls one arm of a da Vinci surgical robot. In addition to moving the arm, the surgeon can use fingers to control the end-effector of the surgical instrument. Hand gestures are used to implement clutching and similar functions. In particular, we introduce clutching of the instrument orientation, a functionality not available in the da Vinci system. The vibrotactile elements of the glove are used to provide feedback to the user when gesture commands are invoked. A preliminary evaluation of the system shows that it has excellent tracking accuracy and allows surgeons to efficiently perform common surgical training tasks with minimal practice with the new interface; this suggests that the interface is highly intuitive. The proposed system is inexpensive, allows rapid prototyping, and opens opportunities for further innovations in the design of surgical robot interfaces.","sentences":["Robotic surgery has reached a high level of maturity and has become an integral part of standard surgical care.","However, existing surgeon consoles are bulky and take up valuable space in the operating room, present challenges for surgical team coordination, and their proprietary nature makes it difficult to take advantage of recent technological advances, especially in virtual and augmented reality.","One potential area for further improvement is the integration of modern sensory gloves into robotic platforms, allowing surgeons to control robotic arms directly with their hand movements intuitively.","We propose one such system that combines an HTC Vive tracker, a Manus Meta Prime 3 XR sensory glove, and God Vision wireless smart glasses.","The system controls one arm of a da Vinci surgical robot.","In addition to moving the arm, the surgeon can use fingers to control the end-effector of the surgical instrument.","Hand gestures are used to implement clutching and similar functions.","In particular, we introduce clutching of the instrument orientation, a functionality not available in the da Vinci system.","The vibrotactile elements of the glove are used to provide feedback to the user when gesture commands are invoked.","A preliminary evaluation of the system shows that it has excellent tracking accuracy and allows surgeons to efficiently perform common surgical training tasks with minimal practice with the new interface; this suggests that the interface is highly intuitive.","The proposed system is inexpensive, allows rapid prototyping, and opens opportunities for further innovations in the design of surgical robot interfaces."],"url":"http://arxiv.org/abs/2403.13941v1","category":"cs.RO"}
{"created":"2024-03-20 19:25:11","title":"Multi-criteria approach for selecting an explanation from the set of counterfactuals produced by an ensemble of explainers","abstract":"Counterfactuals are widely used to explain ML model predictions by providing alternative scenarios for obtaining the more desired predictions. They can be generated by a variety of methods that optimize different, sometimes conflicting, quality measures and produce quite different solutions. However, choosing the most appropriate explanation method and one of the generated counterfactuals is not an easy task. Instead of forcing the user to test many different explanation methods and analysing conflicting solutions, in this paper, we propose to use a multi-stage ensemble approach that will select single counterfactual based on the multiple-criteria analysis. It offers a compromise solution that scores well on several popular quality measures. This approach exploits the dominance relation and the ideal point decision aid method, which selects one counterfactual from the Pareto front. The conducted experiments demonstrated that the proposed approach generates fully actionable counterfactuals with attractive compromise values of the considered quality measures.","sentences":["Counterfactuals are widely used to explain ML model predictions by providing alternative scenarios for obtaining the more desired predictions.","They can be generated by a variety of methods that optimize different, sometimes conflicting, quality measures and produce quite different solutions.","However, choosing the most appropriate explanation method and one of the generated counterfactuals is not an easy task.","Instead of forcing the user to test many different explanation methods and analysing conflicting solutions, in this paper, we propose to use a multi-stage ensemble approach that will select single counterfactual based on the multiple-criteria analysis.","It offers a compromise solution that scores well on several popular quality measures.","This approach exploits the dominance relation and the ideal point decision aid method, which selects one counterfactual from the Pareto front.","The conducted experiments demonstrated that the proposed approach generates fully actionable counterfactuals with attractive compromise values of the considered quality measures."],"url":"http://arxiv.org/abs/2403.13940v1","category":"cs.LG"}
{"created":"2024-03-20 19:22:01","title":"Systematic approach to measure the performance of microchannel-plate photomultipliers","abstract":"In this paper, we present our approach to systematically measure numerous performance parameters of MCP-PMTs. The experimental setups, the analyses and selected results are discussed. Although the techniques used may be different in other locations, the document is intended as a guide for comparable measurements with other types of MCP-PMTs. Measurements are shown for the following performance parameters: spectral and spatial quantum efficiency, collection efficiency, gain as a function of voltage, position and magnetic field, time resolution, rate capability and lifetime. By using a dedicated 3-axis stepper and an FPGA-based DAQ system, also inner PMT parameters are measured as a function of the active area, such as relative detection efficiency, dark count rate, time resolution, recoil electron and afterpulse distributions, as well as charge sharing and electronic crosstalk. In addition, some of the parameters are investigated inside a strong magnetic field. For many of these measurements, the change of most setup parameters and the subsequent analysis can be controlled semi-automatically by software scripts.","sentences":["In this paper, we present our approach to systematically measure numerous performance parameters of MCP-PMTs.","The experimental setups, the analyses and selected results are discussed.","Although the techniques used may be different in other locations, the document is intended as a guide for comparable measurements with other types of MCP-PMTs.","Measurements are shown for the following performance parameters: spectral and spatial quantum efficiency, collection efficiency, gain as a function of voltage, position and magnetic field, time resolution, rate capability and lifetime.","By using a dedicated 3-axis stepper and an FPGA-based DAQ system, also inner PMT parameters are measured as a function of the active area, such as relative detection efficiency, dark count rate, time resolution, recoil electron and afterpulse distributions, as well as charge sharing and electronic crosstalk.","In addition, some of the parameters are investigated inside a strong magnetic field.","For many of these measurements, the change of most setup parameters and the subsequent analysis can be controlled semi-automatically by software scripts."],"url":"http://arxiv.org/abs/2403.13938v1","category":"physics.ins-det"}
{"created":"2024-03-20 19:01:34","title":"Generalized angularities and differential jet shapes measurements from STAR at $\\sqrt{s} = $ 200 GeV","abstract":"Jets from early stages of heavy-ion collisions undergo modified showering in quark-gluon plasma (QGP) relative to vacuum due to jet-medium interactions, which can be measured using observables like differential jet shape and generalized angularities. Differential jet shape ($\\rho(\\mathbf{R})$) encodes radially differential information about jet broadening and has shown an average migration of charged energy away from the axes of quenched jets from Pb+Pb collisions at the LHC. Measurements of generalized angularities in presence of the medium from Pb+Pb collisions at the LHC show harder, or more quark-like jet fragmentation relative to vacuum. Measuring these distributions in heavy-ion collisions at RHIC will help us further characterize jet-medium interactions in a phase-space region complementary to that of the LHC.   In these proceedings, we present the first fully corrected measurements of $\\rho(\\mathbf{R})$, jet girth (g), momentum dispersion ($p_T^D$) and momentum difference of leading and subleading constituent particles (LeSub) observables, using hard-core jets in $p$+$p$ collisions at $\\sqrt{s} = 200$ GeV, collected by the STAR experiment. Finally, the data are compared with model calculations and the physics implications are discussed.","sentences":["Jets from early stages of heavy-ion collisions undergo modified showering in quark-gluon plasma (QGP) relative to vacuum due to jet-medium interactions, which can be measured using observables like differential jet shape and generalized angularities.","Differential jet shape ($\\rho(\\mathbf{R})$) encodes radially differential information about jet broadening and has shown an average migration of charged energy away from the axes of quenched jets from Pb+Pb collisions at the LHC.","Measurements of generalized angularities in presence of the medium from Pb+Pb collisions at the LHC show harder, or more quark-like jet fragmentation relative to vacuum.","Measuring these distributions in heavy-ion collisions at RHIC will help us further characterize jet-medium interactions in a phase-space region complementary to that of the LHC.   ","In these proceedings, we present the first fully corrected measurements of $\\rho(\\mathbf{R})$, jet girth (g), momentum dispersion ($p_T^D$) and momentum difference of leading and subleading constituent particles (LeSub) observables, using hard-core jets in $p$+$p$ collisions at $\\sqrt{s} = 200$ GeV, collected by the STAR experiment.","Finally, the data are compared with model calculations and the physics implications are discussed."],"url":"http://arxiv.org/abs/2403.13928v1","category":"nucl-ex"}
{"created":"2024-03-20 18:59:18","title":"Reducing Large Language Model Bias with Emphasis on 'Restricted Industries': Automated Dataset Augmentation and Prejudice Quantification","abstract":"Despite the growing capabilities of large language models, there exists concerns about the biases they develop. In this paper, we propose a novel, automated mechanism for debiasing through specified dataset augmentation in the lens of bias producers and in the context of 'restricted industries' with limited data. We additionally create two new additional metrics, the mb-index and db-index, to quantify bias, considering the idea that bias occurs due to both intrinsic model architecture and dataset.","sentences":["Despite the growing capabilities of large language models, there exists concerns about the biases they develop.","In this paper, we propose a novel, automated mechanism for debiasing through specified dataset augmentation in the lens of bias producers and in the context of 'restricted industries' with limited data.","We additionally create two new additional metrics, the mb-index and db-index, to quantify bias, considering the idea that bias occurs due to both intrinsic model architecture and dataset."],"url":"http://arxiv.org/abs/2403.13925v1","category":"cs.CL"}
{"created":"2024-03-20 18:49:50","title":"Generalized angularities measurements from STAR at $\\sqrt{s_{\\rm NN}} = $ 200 GeV","abstract":"Jets are produced in early stages of heavy-ion collisions and undergo modified showering in the quark-gluon plasma (QGP) medium relative to a vacuum case. These modifications can be measured using observables like jet momentum profile and generalized angularities to study the details of jet-medium interactions. Jet momentum profile ($\\rho(r)$) encodes radially differential information about jet broadening and has shown migration of charged energy towards the jet periphery in Pb+Pb collisions at the LHC. Measurements of generalized angularities (girth $g$ and momentum dispersion $p_T^D$) and LeSub (difference between leading and subleading constituents) from Pb+Pb collisions at the LHC show harder, or more quark-like jet fragmentation, in the presence of the medium. Measuring these distributions in heavy-ion collisions at RHIC will help us further characterize the jet-medium interactions in a phase-space region complimentary to that of the LHC. In this contribution, we present the first measurements of fully corrected $g$, $p_T^D$ and LeSub observables using hard-core jets in Au+Au collisions at $\\sqrt{s_{\\rm NN}}=200$ GeV, collected by the STAR experiment at RHIC.","sentences":["Jets are produced in early stages of heavy-ion collisions and undergo modified showering in the quark-gluon plasma (QGP) medium relative to a vacuum case.","These modifications can be measured using observables like jet momentum profile and generalized angularities to study the details of jet-medium interactions.","Jet momentum profile ($\\rho(r)$) encodes radially differential information about jet broadening and has shown migration of charged energy towards the jet periphery in Pb+Pb collisions at the LHC.","Measurements of generalized angularities (girth $g$ and momentum dispersion $p_T^D$) and LeSub (difference between leading and subleading constituents) from Pb+Pb collisions at the LHC show harder, or more quark-like jet fragmentation, in the presence of the medium.","Measuring these distributions in heavy-ion collisions at RHIC will help us further characterize the jet-medium interactions in a phase-space region complimentary to that of the LHC.","In this contribution, we present the first measurements of fully corrected $g$, $p_T^D$ and LeSub observables using hard-core jets in Au+Au collisions at $\\sqrt{s_{\\rm NN}}=200$ GeV, collected by the STAR experiment at RHIC."],"url":"http://arxiv.org/abs/2403.13921v1","category":"nucl-ex"}
{"created":"2024-03-20 18:30:12","title":"Augmented Reality Demonstrations for Scalable Robot Imitation Learning","abstract":"Robot Imitation Learning (IL) is a widely used method for training robots to perform manipulation tasks that involve mimicking human demonstrations to acquire skills. However, its practicality has been limited due to its requirement that users be trained in operating real robot arms to provide demonstrations. This paper presents an innovative solution: an Augmented Reality (AR)-assisted framework for demonstration collection, empowering non-roboticist users to produce demonstrations for robot IL using devices like the HoloLens 2. Our framework facilitates scalable and diverse demonstration collection for real-world tasks. We validate our approach with experiments on three classical robotics tasks: reach, push, and pick-and-place. The real robot performs each task successfully while replaying demonstrations collected via AR.","sentences":["Robot Imitation Learning (IL) is a widely used method for training robots to perform manipulation tasks that involve mimicking human demonstrations to acquire skills.","However, its practicality has been limited due to its requirement that users be trained in operating real robot arms to provide demonstrations.","This paper presents an innovative solution: an Augmented Reality (AR)-assisted framework for demonstration collection, empowering non-roboticist users to produce demonstrations for robot IL using devices like the HoloLens 2.","Our framework facilitates scalable and diverse demonstration collection for real-world tasks.","We validate our approach with experiments on three classical robotics tasks: reach, push, and pick-and-place.","The real robot performs each task successfully while replaying demonstrations collected via AR."],"url":"http://arxiv.org/abs/2403.13910v1","category":"cs.RO"}
{"created":"2024-03-20 18:01:57","title":"Towards Learning Contrast Kinetics with Multi-Condition Latent Diffusion Models","abstract":"Contrast agents in dynamic contrast enhanced magnetic resonance imaging allow to localize tumors and observe their contrast kinetics, which is essential for cancer characterization and respective treatment decision-making. However, contrast agent administration is not only associated with adverse health risks, but also restricted for patients during pregnancy, and for those with kidney malfunction, or other adverse reactions. With contrast uptake as key biomarker for lesion malignancy, cancer recurrence risk, and treatment response, it becomes pivotal to reduce the dependency on intravenous contrast agent administration. To this end, we propose a multi-conditional latent diffusion model capable of acquisition time-conditioned image synthesis of DCE-MRI temporal sequences. To evaluate medical image synthesis, we additionally propose and validate the Fr\\'echet radiomics distance as an image quality measure based on biomarker variability between synthetic and real imaging data. Our results demonstrate our method's ability to generate realistic multi-sequence fat-saturated breast DCE-MRI and uncover the emerging potential of deep learning based contrast kinetics simulation. We publicly share our accessible codebase at https://github.com/RichardObi/ccnet.","sentences":["Contrast agents in dynamic contrast enhanced magnetic resonance imaging allow to localize tumors and observe their contrast kinetics, which is essential for cancer characterization and respective treatment decision-making.","However, contrast agent administration is not only associated with adverse health risks, but also restricted for patients during pregnancy, and for those with kidney malfunction, or other adverse reactions.","With contrast uptake as key biomarker for lesion malignancy, cancer recurrence risk, and treatment response, it becomes pivotal to reduce the dependency on intravenous contrast agent administration.","To this end, we propose a multi-conditional latent diffusion model capable of acquisition time-conditioned image synthesis of DCE-MRI temporal sequences.","To evaluate medical image synthesis, we additionally propose and validate the Fr\\'echet radiomics distance as an image quality measure based on biomarker variability between synthetic and real imaging data.","Our results demonstrate our method's ability to generate realistic multi-sequence fat-saturated breast DCE-MRI and uncover the emerging potential of deep learning based contrast kinetics simulation.","We publicly share our accessible codebase at https://github.com/RichardObi/ccnet."],"url":"http://arxiv.org/abs/2403.13890v1","category":"eess.IV"}
{"created":"2024-03-20 14:54:45","title":"Exact solution for the collective non-Markovian decay of two fully excited quantum emitters","abstract":"The spontaneous decay of a quantum emitter can be significantly affected by both, the state of other emitters and the state of the surrounding radiation. Such instances combine two central topics in quantum optics that are typically treated separately: collective decay and non-Markovian dynamics. Here we access collective non-Markovian decay through the exact solution for two excited quantum emitters located at adjacent unit cells of a one-dimensional single-band waveguide. The transformed field amplitudes, which we express in terms of elementary functions, combine exponential, algebraic, fractional, and mixed algebraic-exponential decay, as well as logarithmic corrections to some of the algebraic parts. We develop analytic methods for multiparticle open quantum systems that shed light on the complexity of non-linear quantum optical phenomena.","sentences":["The spontaneous decay of a quantum emitter can be significantly affected by both, the state of other emitters and the state of the surrounding radiation.","Such instances combine two central topics in quantum optics that are typically treated separately: collective decay and non-Markovian dynamics.","Here we access collective non-Markovian decay through the exact solution for two excited quantum emitters located at adjacent unit cells of a one-dimensional single-band waveguide.","The transformed field amplitudes, which we express in terms of elementary functions, combine exponential, algebraic, fractional, and mixed algebraic-exponential decay, as well as logarithmic corrections to some of the algebraic parts.","We develop analytic methods for multiparticle open quantum systems that shed light on the complexity of non-linear quantum optical phenomena."],"url":"http://arxiv.org/abs/2403.13871v1","category":"quant-ph"}
{"created":"2024-03-20 14:00:29","title":"Accurately Predicting Probabilities of Safety-Critical Rare Events for Intelligent Systems","abstract":"Intelligent systems are increasingly integral to our daily lives, yet rare safety-critical events present significant latent threats to their practical deployment. Addressing this challenge hinges on accurately predicting the probability of safety-critical events occurring within a given time step from the current state, a metric we define as 'criticality'. The complexity of predicting criticality arises from the extreme data imbalance caused by rare events in high dimensional variables associated with the rare events, a challenge we refer to as the curse of rarity. Existing methods tend to be either overly conservative or prone to overlooking safety-critical events, thus struggling to achieve both high precision and recall rates, which severely limits their applicability. This study endeavors to develop a criticality prediction model that excels in both precision and recall rates for evaluating the criticality of safety-critical autonomous systems. We propose a multi-stage learning framework designed to progressively densify the dataset, mitigating the curse of rarity across stages. To validate our approach, we evaluate it in two cases: lunar lander and bipedal walker scenarios. The results demonstrate that our method surpasses traditional approaches, providing a more accurate and dependable assessment of criticality in intelligent systems.","sentences":["Intelligent systems are increasingly integral to our daily lives, yet rare safety-critical events present significant latent threats to their practical deployment.","Addressing this challenge hinges on accurately predicting the probability of safety-critical events occurring within a given time step from the current state, a metric we define as 'criticality'.","The complexity of predicting criticality arises from the extreme data imbalance caused by rare events in high dimensional variables associated with the rare events, a challenge we refer to as the curse of rarity.","Existing methods tend to be either overly conservative or prone to overlooking safety-critical events, thus struggling to achieve both high precision and recall rates, which severely limits their applicability.","This study endeavors to develop a criticality prediction model that excels in both precision and recall rates for evaluating the criticality of safety-critical autonomous systems.","We propose a multi-stage learning framework designed to progressively densify the dataset, mitigating the curse of rarity across stages.","To validate our approach, we evaluate it in two cases: lunar lander and bipedal walker scenarios.","The results demonstrate that our method surpasses traditional approaches, providing a more accurate and dependable assessment of criticality in intelligent systems."],"url":"http://arxiv.org/abs/2403.13869v1","category":"cs.LG"}
{"created":"2024-03-20 11:47:42","title":"The Bid Picture: Auction-Inspired Multi-player Generative Adversarial Networks Training","abstract":"This article proposes auction-inspired multi-player generative adversarial networks training, which mitigates the mode collapse problem of GANs. Mode collapse occurs when an over-fitted generator generates a limited range of samples, often concentrating on a small subset of the data distribution. Despite the restricted diversity of generated samples, the discriminator can still be deceived into distinguishing these samples as real samples from the actual distribution. In the absence of external standards, a model cannot recognize its failure during the training phase. We extend the two-player game of generative adversarial networks to the multi-player game. During the training, the values of each model are determined by the bids submitted by other players in an auction-like process.","sentences":["This article proposes auction-inspired multi-player generative adversarial networks training, which mitigates the mode collapse problem of GANs.","Mode collapse occurs when an over-fitted generator generates a limited range of samples, often concentrating on a small subset of the data distribution.","Despite the restricted diversity of generated samples, the discriminator can still be deceived into distinguishing these samples as real samples from the actual distribution.","In the absence of external standards, a model cannot recognize its failure during the training phase.","We extend the two-player game of generative adversarial networks to the multi-player game.","During the training, the values of each model are determined by the bids submitted by other players in an auction-like process."],"url":"http://arxiv.org/abs/2403.13866v1","category":"cs.LG"}
{"created":"2024-03-20 10:13:54","title":"Graph Neural Network for Crawling Target Nodes in Social Networks","abstract":"Social networks crawling is in the focus of active research the last years. One of the challenging task is to collect target nodes in an initially unknown graph given a budget of crawling steps. Predicting a node property based on its partially known neighbourhood is at the heart of a successful crawler. In this paper we adopt graph neural networks for this purpose and show they are competitive to traditional classifiers and are better for individual cases. Additionally we suggest a training sample boosting technique, which helps to diversify the training set at early stages of crawling and thus improves the predictor quality. The experimental study on three types of target set topology indicates GNN based approach has a potential in crawling task, especially in the case of distributed target nodes.","sentences":["Social networks crawling is in the focus of active research the last years.","One of the challenging task is to collect target nodes in an initially unknown graph given a budget of crawling steps.","Predicting a node property based on its partially known neighbourhood is at the heart of a successful crawler.","In this paper we adopt graph neural networks for this purpose and show they are competitive to traditional classifiers and are better for individual cases.","Additionally we suggest a training sample boosting technique, which helps to diversify the training set at early stages of crawling and thus improves the predictor quality.","The experimental study on three types of target set topology indicates GNN based approach has a potential in crawling task, especially in the case of distributed target nodes."],"url":"http://arxiv.org/abs/2403.13865v1","category":"cs.SI"}
{"created":"2024-03-20 08:45:31","title":"DiffImpute: Tabular Data Imputation With Denoising Diffusion Probabilistic Model","abstract":"Tabular data plays a crucial role in various domains but often suffers from missing values, thereby curtailing its potential utility. Traditional imputation techniques frequently yield suboptimal results and impose substantial computational burdens, leading to inaccuracies in subsequent modeling tasks. To address these challenges, we propose DiffImpute, a novel Denoising Diffusion Probabilistic Model (DDPM). Specifically, DiffImpute is trained on complete tabular datasets, ensuring that it can produce credible imputations for missing entries without undermining the authenticity of the existing data. Innovatively, it can be applied to various settings of Missing Completely At Random (MCAR) and Missing At Random (MAR). To effectively handle the tabular features in DDPM, we tailor four tabular denoising networks, spanning MLP, ResNet, Transformer, and U-Net. We also propose Harmonization to enhance coherence between observed and imputed data by infusing the data back and denoising them multiple times during the sampling stage. To enable efficient inference while maintaining imputation performance, we propose a refined non-Markovian sampling process that works along with Harmonization. Empirical evaluations on seven diverse datasets underscore the prowess of DiffImpute. Specifically, when paired with the Transformer as the denoising network, it consistently outperforms its competitors, boasting an average ranking of 1.7 and the most minimal standard deviation. In contrast, the next best method lags with a ranking of 2.8 and a standard deviation of 0.9. The code is available at https://github.com/Dendiiiii/DiffImpute.","sentences":["Tabular data plays a crucial role in various domains but often suffers from missing values, thereby curtailing its potential utility.","Traditional imputation techniques frequently yield suboptimal results and impose substantial computational burdens, leading to inaccuracies in subsequent modeling tasks.","To address these challenges, we propose DiffImpute, a novel Denoising Diffusion Probabilistic Model (DDPM).","Specifically, DiffImpute is trained on complete tabular datasets, ensuring that it can produce credible imputations for missing entries without undermining the authenticity of the existing data.","Innovatively, it can be applied to various settings of Missing Completely At Random (MCAR) and Missing At Random (MAR).","To effectively handle the tabular features in DDPM, we tailor four tabular denoising networks, spanning MLP, ResNet, Transformer, and U-Net.","We also propose Harmonization to enhance coherence between observed and imputed data by infusing the data back and denoising them multiple times during the sampling stage.","To enable efficient inference while maintaining imputation performance, we propose a refined non-Markovian sampling process that works along with Harmonization.","Empirical evaluations on seven diverse datasets underscore the prowess of DiffImpute.","Specifically, when paired with the Transformer as the denoising network, it consistently outperforms its competitors, boasting an average ranking of 1.7 and the most minimal standard deviation.","In contrast, the next best method lags with a ranking of 2.8 and a standard deviation of 0.9.","The code is available at https://github.com/Dendiiiii/DiffImpute."],"url":"http://arxiv.org/abs/2403.13863v1","category":"cs.LG"}
{"created":"2024-03-21 17:59:55","title":"LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors","abstract":"We present a simple self-supervised method to enhance the performance of ViT features for dense downstream tasks. Our Lightweight Feature Transform (LiFT) is a straightforward and compact postprocessing network that can be applied to enhance the features of any pre-trained ViT backbone. LiFT is fast and easy to train with a self-supervised objective, and it boosts the density of ViT features for minimal extra inference cost. Furthermore, we demonstrate that LiFT can be applied with approaches that use additional task-specific downstream modules, as we integrate LiFT with ViTDet for COCO detection and segmentation. Despite the simplicity of LiFT, we find that it is not simply learning a more complex version of bilinear interpolation. Instead, our LiFT training protocol leads to several desirable emergent properties that benefit ViT features in dense downstream tasks. This includes greater scale invariance for features, and better object boundary maps. By simply training LiFT for a few epochs, we show improved performance on keypoint correspondence, detection, segmentation, and object discovery tasks. Overall, LiFT provides an easy way to unlock the benefits of denser feature arrays for a fraction of the computational cost. For more details, refer to our project page at https://www.cs.umd.edu/~sakshams/LiFT/.","sentences":["We present a simple self-supervised method to enhance the performance of ViT features for dense downstream tasks.","Our Lightweight Feature Transform (LiFT) is a straightforward and compact postprocessing network that can be applied to enhance the features of any pre-trained ViT backbone.","LiFT is fast and easy to train with a self-supervised objective, and it boosts the density of ViT features for minimal extra inference cost.","Furthermore, we demonstrate that LiFT can be applied with approaches that use additional task-specific downstream modules, as we integrate LiFT with ViTDet for COCO detection and segmentation.","Despite the simplicity of LiFT, we find that it is not simply learning a more complex version of bilinear interpolation.","Instead, our LiFT training protocol leads to several desirable emergent properties that benefit ViT features in dense downstream tasks.","This includes greater scale invariance for features, and better object boundary maps.","By simply training LiFT for a few epochs, we show improved performance on keypoint correspondence, detection, segmentation, and object discovery tasks.","Overall, LiFT provides an easy way to unlock the benefits of denser feature arrays for a fraction of the computational cost.","For more details, refer to our project page at https://www.cs.umd.edu/~sakshams/LiFT/."],"url":"http://arxiv.org/abs/2403.14625v1","category":"cs.CV"}
{"created":"2024-03-21 17:58:18","title":"Search for protostellar jets with UWISH2 in the molecular cloud complexes Vulpecula and IRDC G53.2","abstract":"Jets and outflows are the early signposts of stellar birth. Using the UKIRT Wide Field Infrared Survey for H2 (UWISH2) at 2.12 micron, 127 outflows are identified in molecular cloud complexes Vulpecula OB1 and IRDC G53.2 covering 12 square degrees of the Galactic plane. Using multi-wavelength datasets, from 1.2 to 70 micron, 79 young stellar objects (YSOs) are proposed as potential driving sources, where, $\\sim$ 79% are likely Class 0/I protostars, 17% are Class II YSOs and the remaining 4% are Class III YSOs. The outflows are characterized in terms of their length, flux, luminosity and knot-spacing. The identified outflows have a median lobe length of 0.22 pc and 0.17 pc for outflows in Vulpecula OB1 and IRDC G53.2, respectively. Our analysis, from the knot spacing, reveals a typical ejection frequency of $\\sim$ 1.2 kyr suggesting an intermediate type between the FU-Ori and EX-Ori type of eruptions in both cloud complexes. Furthermore, the physical parameters of the driving sources are obtained by performing radiative transfer modelling to the observed spectral energy distributions (SEDs), which suggest that the outflows are driven by intermediate mass stars. Various observed trends between the outflow properties and the corresponding driving sources, and various interesting outflows and star forming sites, including sites of triggered star formation and protocluster forming clump with clusters of jets, are discussed. The obtained results and the identified jet-bearing protostellar sample will pave the way to understand many aspects of outflows with future high-resolution observations.","sentences":["Jets and outflows are the early signposts of stellar birth.","Using the UKIRT Wide Field Infrared Survey for H2 (UWISH2) at 2.12 micron, 127 outflows are identified in molecular cloud complexes Vulpecula OB1 and IRDC G53.2 covering 12 square degrees of the Galactic plane.","Using multi-wavelength datasets, from 1.2 to 70 micron, 79 young stellar objects (YSOs) are proposed as potential driving sources, where, $\\sim$ 79% are likely Class 0/I protostars, 17% are Class II YSOs and the remaining 4% are Class III YSOs.","The outflows are characterized in terms of their length, flux, luminosity and knot-spacing.","The identified outflows have a median lobe length of 0.22 pc and 0.17 pc for outflows in Vulpecula OB1 and IRDC G53.2, respectively.","Our analysis, from the knot spacing, reveals a typical ejection frequency of $\\sim$ 1.2 kyr suggesting an intermediate type between the FU-Ori and EX-Ori type of eruptions in both cloud complexes.","Furthermore, the physical parameters of the driving sources are obtained by performing radiative transfer modelling to the observed spectral energy distributions (SEDs), which suggest that the outflows are driven by intermediate mass stars.","Various observed trends between the outflow properties and the corresponding driving sources, and various interesting outflows and star forming sites, including sites of triggered star formation and protocluster forming clump with clusters of jets, are discussed.","The obtained results and the identified jet-bearing protostellar sample will pave the way to understand many aspects of outflows with future high-resolution observations."],"url":"http://arxiv.org/abs/2403.14615v1","category":"astro-ph.GA"}
{"created":"2024-03-21 17:55:50","title":"Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey","abstract":"Large models represent a groundbreaking advancement in multiple application fields, enabling remarkable achievements across various tasks. However, their unprecedented scale comes with significant computational costs. These models, often consisting of billions of parameters, require vast amounts of computational resources for execution. Especially, the expansive scale and computational demands pose considerable challenges when customizing them for particular downstream tasks, particularly over the hardware platforms constrained by computational capabilities. Parameter Efficient Fine-Tuning (PEFT) provides a practical solution by efficiently adapt the large models over the various downstream tasks. In particular, PEFT refers to the process of adjusting the parameters of a pre-trained large models to adapt it to a specific task while minimizing the number of additional parameters introduced or computational resources required. This approach is particularly important when dealing with large language models with high parameter counts, as fine-tuning these models from scratch can be computationally expensive and resource-intensive, posing considerable challenges in the supporting system platform design. In this survey, we present comprehensive studies of various PEFT algorithms, examining their performance and computational overhead. Moreover, we provide an overview of applications developed using different PEFT algorithms and discuss common techniques employed to mitigate computation costs for PEFT. In addition to the algorithmic perspective, we overview various real-world system designs to investigate the implementation costs associated with different PEFT algorithms. This survey serves as an indispensable resource for researchers aiming to understand both the PEFT algorithm and its system implementation, offering detailed insights into recent advancements and practical applications.","sentences":["Large models represent a groundbreaking advancement in multiple application fields, enabling remarkable achievements across various tasks.","However, their unprecedented scale comes with significant computational costs.","These models, often consisting of billions of parameters, require vast amounts of computational resources for execution.","Especially, the expansive scale and computational demands pose considerable challenges when customizing them for particular downstream tasks, particularly over the hardware platforms constrained by computational capabilities.","Parameter Efficient Fine-Tuning (PEFT) provides a practical solution by efficiently adapt the large models over the various downstream tasks.","In particular, PEFT refers to the process of adjusting the parameters of a pre-trained large models to adapt it to a specific task while minimizing the number of additional parameters introduced or computational resources required.","This approach is particularly important when dealing with large language models with high parameter counts, as fine-tuning these models from scratch can be computationally expensive and resource-intensive, posing considerable challenges in the supporting system platform design.","In this survey, we present comprehensive studies of various PEFT algorithms, examining their performance and computational overhead.","Moreover, we provide an overview of applications developed using different PEFT algorithms and discuss common techniques employed to mitigate computation costs for PEFT.","In addition to the algorithmic perspective, we overview various real-world system designs to investigate the implementation costs associated with different PEFT algorithms.","This survey serves as an indispensable resource for researchers aiming to understand both the PEFT algorithm and its system implementation, offering detailed insights into recent advancements and practical applications."],"url":"http://arxiv.org/abs/2403.14608v1","category":"cs.LG"}
{"created":"2024-03-21 17:55:26","title":"Polynomial-Time Classical Simulation of Noisy IQP Circuits with Constant Depth","abstract":"Sampling from the output distributions of quantum computations comprising only commuting gates, known as instantaneous quantum polynomial (IQP) computations, is believed to be intractable for classical computers, and hence this task has become a leading candidate for testing the capabilities of quantum devices. Here we demonstrate that for an arbitrary IQP circuit undergoing dephasing or depolarizing noise, whose depth is greater than a critical $O(1)$ threshold, the output distribution can be efficiently sampled by a classical computer. Unlike other simulation algorithms for quantum supremacy tasks, we do not require assumptions on the circuit's architecture, on anti-concentration properties, nor do we require $\\Omega(\\log(n))$ circuit depth. We take advantage of the fact that IQP circuits have deep sections of diagonal gates, which allows the noise to build up predictably and induce a large-scale breakdown of entanglement within the circuit. Our results suggest that quantum supremacy experiments based on IQP circuits may be more susceptible to classical simulation than previously thought.","sentences":["Sampling from the output distributions of quantum computations comprising only commuting gates, known as instantaneous quantum polynomial (IQP) computations, is believed to be intractable for classical computers, and hence this task has become a leading candidate for testing the capabilities of quantum devices.","Here we demonstrate that for an arbitrary IQP circuit undergoing dephasing or depolarizing noise, whose depth is greater than a critical $O(1)$ threshold, the output distribution can be efficiently sampled by a classical computer.","Unlike other simulation algorithms for quantum supremacy tasks, we do not require assumptions on the circuit's architecture, on anti-concentration properties, nor do we require $\\Omega(\\log(n))$ circuit depth.","We take advantage of the fact that IQP circuits have deep sections of diagonal gates, which allows the noise to build up predictably and induce a large-scale breakdown of entanglement within the circuit.","Our results suggest that quantum supremacy experiments based on IQP circuits may be more susceptible to classical simulation than previously thought."],"url":"http://arxiv.org/abs/2403.14607v1","category":"quant-ph"}
{"created":"2024-03-21 17:51:45","title":"Miscibility-Immiscibility transition of strongly interacting bosonic mixtures in optical lattices","abstract":"Interaction plays key role in the mixing properties of a multi-component system. The miscibility-immiscibility transition (MIT) in a weakly interacting mixture of Bose gases is predominantly determined by the strengths of the intra and inter-component two-body contact interactions. On the other hand, in the strongly interacting regime interaction induced processes become relevant. Despite previous studies on bosonic mixtures in optical lattices, the effects of the interaction induced processes on the MIT remains unexplored. In this work, we investigate the MIT in the strongly interacting phases of two-component bosonic mixture trapped in a homogeneous two-dimensional square optical lattice. Particularly we examine the transition when both the components are in superfluid (SF), one-body staggered superfluid (OSSF) or supersolid (SS) phases. Our study prevails that, similar to the contact interactions, the MIT can be influenced by competing intra and inter-component density induced tunnelings and off-site interactions. To probe the MIT in the strongly interacting regime, we study the extended version of the Bose-Hubbard model with the density induced tunneling and nearest-neighbouring interaction terms, and focus in the regime where the hopping processes are considerably weaker than the on-site interaction. We solve this model through site-decoupling mean-field theory with Gutzwiller ansatz and characterize the miscibility through the site-wise co-existence of the two-component across the lattice. Our study contributes to the better understanding of miscibility properties of multi-component systems in the strongly interacting regime.","sentences":["Interaction plays key role in the mixing properties of a multi-component system.","The miscibility-immiscibility transition (MIT) in a weakly interacting mixture of Bose gases is predominantly determined by the strengths of the intra and inter-component two-body contact interactions.","On the other hand, in the strongly interacting regime interaction induced processes become relevant.","Despite previous studies on bosonic mixtures in optical lattices, the effects of the interaction induced processes on the MIT remains unexplored.","In this work, we investigate the MIT in the strongly interacting phases of two-component bosonic mixture trapped in a homogeneous two-dimensional square optical lattice.","Particularly we examine the transition when both the components are in superfluid (SF), one-body staggered superfluid (OSSF) or supersolid (SS) phases.","Our study prevails that, similar to the contact interactions, the MIT can be influenced by competing intra and inter-component density induced tunnelings and off-site interactions.","To probe the MIT in the strongly interacting regime, we study the extended version of the Bose-Hubbard model with the density induced tunneling and nearest-neighbouring interaction terms, and focus in the regime where the hopping processes are considerably weaker than the on-site interaction.","We solve this model through site-decoupling mean-field theory with Gutzwiller ansatz and characterize the miscibility through the site-wise co-existence of the two-component across the lattice.","Our study contributes to the better understanding of miscibility properties of multi-component systems in the strongly interacting regime."],"url":"http://arxiv.org/abs/2403.14601v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-21 17:49:34","title":"Mutation of signed valued quivers and presentations of simple complex Lie algebras","abstract":"We define a mutation procedure for mutation Dynkin signed valued quivers which generalises Fomin-Zelevinsky mutation of skew-symmetrizable matrices appearing in the theory of cluster algebras. On forgetting the signs, this reduces to Fomin-Zelevinsky mutation. Each mutation Dynkin signed valued quiver gives a realization of the root system which is related to work of Parsons on companion bases, and it gives presentations of Lie algebras, following work of Barot-Rivera and P\\'erez-Rivera.","sentences":["We define a mutation procedure for mutation Dynkin signed valued quivers which generalises Fomin-Zelevinsky mutation of skew-symmetrizable matrices appearing in the theory of cluster algebras.","On forgetting the signs, this reduces to Fomin-Zelevinsky mutation.","Each mutation Dynkin signed valued quiver gives a realization of the root system which is related to work of Parsons on companion bases, and it gives presentations of Lie algebras, following work of Barot-Rivera and P\\'erez-Rivera."],"url":"http://arxiv.org/abs/2403.14595v1","category":"math.RT"}
{"created":"2024-03-21 17:45:44","title":"When faster rotation is harmful: the competition of alliances with inner blocking mechanism","abstract":"Competitors in an intransitive loop of dominance can form a defensive alliance against an external species. The vitality of this super-structure, however, is jeopardized if we modify the original rock-scissors-paper-like rule and allow that the vicinity of a predator blocks stochastically the invasion success of its neighboring prey towards a third actor. To explore the potential consequences of this multi-point interaction we introduce a minimal model where two three-member alliances are fighting but one of them suffers from this inner blocking mechanism. We demonstrate that this weakness can be compensated by a faster inner rotation which is in agreement with previous findings. This broadly valid principle, however, is not always true here because the increase of rotation speed could be harmful and results in series of reentrant phase transitions on the parameter plane. This unexpected behavior can be explained by the relation of the blocked triplet and a neutral pair formed by a triplet member with an external species. Our results provide novel aspects to the fundamental laws which determine the evolutionary process in multi-strategy ecological systems.","sentences":["Competitors in an intransitive loop of dominance can form a defensive alliance against an external species.","The vitality of this super-structure, however, is jeopardized if we modify the original rock-scissors-paper-like rule and allow that the vicinity of a predator blocks stochastically the invasion success of its neighboring prey towards a third actor.","To explore the potential consequences of this multi-point interaction we introduce a minimal model where two three-member alliances are fighting but one of them suffers from this inner blocking mechanism.","We demonstrate that this weakness can be compensated by a faster inner rotation which is in agreement with previous findings.","This broadly valid principle, however, is not always true here because the increase of rotation speed could be harmful and results in series of reentrant phase transitions on the parameter plane.","This unexpected behavior can be explained by the relation of the blocked triplet and a neutral pair formed by a triplet member with an external species.","Our results provide novel aspects to the fundamental laws which determine the evolutionary process in multi-strategy ecological systems."],"url":"http://arxiv.org/abs/2403.14590v1","category":"q-bio.PE"}
{"created":"2024-03-21 17:37:43","title":"Co-Optimization of Environment and Policies for Decentralized Multi-Agent Navigation","abstract":"This work views the multi-agent system and its surrounding environment as a co-evolving system, where the behavior of one affects the other. The goal is to take both agent actions and environment configurations as decision variables, and optimize these two components in a coordinated manner to improve some measure of interest. Towards this end, we consider the problem of decentralized multi-agent navigation in cluttered environments. By introducing two sub-objectives of multi-agent navigation and environment optimization, we propose an $\\textit{agent-environment co-optimization}$ problem and develop a $\\textit{coordinated algorithm}$ that alternates between these sub-objectives to search for an optimal synthesis of agent actions and obstacle configurations in the environment; ultimately, improving the navigation performance. Due to the challenge of explicitly modeling the relation between agents, environment and performance, we leverage policy gradient to formulate a model-free learning mechanism within the coordinated framework. A formal convergence analysis shows that our coordinated algorithm tracks the local minimum trajectory of an associated time-varying non-convex optimization problem. Extensive numerical results corroborate theoretical findings and show the benefits of co-optimization over baselines. Interestingly, the results also indicate that optimized environment configurations are able to offer structural guidance that is key to de-conflicting agents in motion.","sentences":["This work views the multi-agent system and its surrounding environment as a co-evolving system, where the behavior of one affects the other.","The goal is to take both agent actions and environment configurations as decision variables, and optimize these two components in a coordinated manner to improve some measure of interest.","Towards this end, we consider the problem of decentralized multi-agent navigation in cluttered environments.","By introducing two sub-objectives of multi-agent navigation and environment optimization, we propose an $\\textit{agent-environment co-optimization}$ problem and develop a $\\textit{coordinated algorithm}$ that alternates between these sub-objectives to search for an optimal synthesis of agent actions and obstacle configurations in the environment; ultimately, improving the navigation performance.","Due to the challenge of explicitly modeling the relation between agents, environment and performance, we leverage policy gradient to formulate a model-free learning mechanism within the coordinated framework.","A formal convergence analysis shows that our coordinated algorithm tracks the local minimum trajectory of an associated time-varying non-convex optimization problem.","Extensive numerical results corroborate theoretical findings and show the benefits of co-optimization over baselines.","Interestingly, the results also indicate that optimized environment configurations are able to offer structural guidance that is key to de-conflicting agents in motion."],"url":"http://arxiv.org/abs/2403.14583v1","category":"cs.RO"}
{"created":"2024-03-21 17:15:21","title":"Pressure-induced enhancement of superconductivity in a non-centrosymmetric compound LaPtGe","abstract":"We report a pressure-induced enhancement of the superconducting transition temperature (Tc) in a non-centrosymmetric (NCS) compound, LaPtGe. With pressure, till 3 GPa, we observed a modest enhancement of the Tc with a rate of 0.071 K/GPa. However, above this pressure, the rate showed a ~2.5 times increase, 0.183 K/GPa. We observed a Tc of 3.94 K at 6 GPa, the highest pressure value used in our transport study. Synchrotron high-pressure x-ray powder diffraction (HP-XRPD) measurements do not reveal any structural phase transition in this system in this pressure range. However, it showed an apparent change of slope in the pressure dependence of lattice parameters above and below 3 GPa. Pressure dependence of the unit-cell volume also followed a distinct trend below and above 3 GPa, with the Birch-Murnaghan equation of state fit providing a bulk modulus (B0) value of ~144 and ~162 GPa, respectively, for two pressure regions. Further, the magnetotransport measurement under pressure up to 2.45 GPa reveals the enhancement of the upper critical field (Hc2(0)) from 0.7 T (0 GPa) to 0.92 T (2.45 GPa). In addition, the upward curvature in Hc2(T) becomes stronger with increasing pressure, suggesting a change of the underlying Fermi surface topology with pressure. The signature for the inducible Hc2(T) with pressure, the distinct enhancement of Tc around 3 GPa, and the noticeable change in lattice parameters around 3 GPa suggests the possibility of multi-gap superconductivity in LaPtGe similar to identical structure NCS compound, LaPtSi. The enhancement of Tc by pressure can be correlated with the possible underlying lattice modulation by compression and the change in the density of states at the Fermi level. Also, the bare change in pressure-dependent activation energy U0 up to 2.45 GPa calculated using the Arrhenius relation clearly shows that the shift in Tc does not arise from grain boundaries.","sentences":["We report a pressure-induced enhancement of the superconducting transition temperature (Tc) in a non-centrosymmetric (NCS) compound, LaPtGe.","With pressure, till 3 GPa, we observed a modest enhancement of the Tc with a rate of 0.071 K/GPa.","However, above this pressure, the rate showed a ~2.5 times increase, 0.183 K/GPa.","We observed a Tc of 3.94 K at 6 GPa, the highest pressure value used in our transport study.","Synchrotron high-pressure x-ray powder diffraction (HP-XRPD) measurements do not reveal any structural phase transition in this system in this pressure range.","However, it showed an apparent change of slope in the pressure dependence of lattice parameters above and below 3 GPa.","Pressure dependence of the unit-cell volume also followed a distinct trend below and above 3 GPa, with the Birch-Murnaghan equation of state fit providing a bulk modulus (B0) value of ~144 and ~162 GPa, respectively, for two pressure regions.","Further, the magnetotransport measurement under pressure up to 2.45 GPa reveals the enhancement of the upper critical field (Hc2(0)) from 0.7 T (0 GPa) to 0.92 T (2.45 GPa).","In addition, the upward curvature in Hc2(T) becomes stronger with increasing pressure, suggesting a change of the underlying Fermi surface topology with pressure.","The signature for the inducible Hc2(T) with pressure, the distinct enhancement of Tc around 3 GPa, and the noticeable change in lattice parameters around 3 GPa suggests the possibility of multi-gap superconductivity in LaPtGe similar to identical structure NCS compound, LaPtSi.","The enhancement of Tc by pressure can be correlated with the possible underlying lattice modulation by compression and the change in the density of states at the Fermi level.","Also, the bare change in pressure-dependent activation energy U0 up to 2.45 GPa calculated using the Arrhenius relation clearly shows that the shift in Tc does not arise from grain boundaries."],"url":"http://arxiv.org/abs/2403.14568v1","category":"cond-mat.supr-con"}
{"created":"2024-03-21 17:13:45","title":"Global Solutions to the 3D Half-Wave Maps Equation with Angular Regularity","abstract":"The half-wave maps equation is a nonlocal geometric equation arising in the continuum dynamics of Haldane-Shashtry and Calogero-Moser spin systems. In high dimensions $n\\geq4$, global wellposedness for data which is small in the critical Besov space $\\dot{B}^{n/2}_{2,1}$ is known since the works of Krieger, Sire and Kiesenhofer [13,9]. There is a major obstruction in extending these results to three dimensions due to the loss of the crucial $L^2_tL^\\infty_x$ Strichartz estimate. In this work, we make progress on this case by proving that the equation is \"weakly\" globally well-posed (in the sense of Tao [28]) for initial data which is not only small in $\\dot{B}^{3/2}_{2,1}$ but also possesses some angular regularity and weighted decay of derivatives. We use Sterbenz's improved Strichartz estimates in conjunction with certain commuting vector fields to develop trilinear estimates in weighted Strichartz spaces which avoid the use of the $L^2_tL^\\infty_x$ endpoint.","sentences":["The half-wave maps equation is a nonlocal geometric equation arising in the continuum dynamics of Haldane-Shashtry and Calogero-Moser spin systems.","In high dimensions $n\\geq4$, global wellposedness for data which is small in the critical Besov space $\\dot{B}^{n/2}_{2,1}$ is known since the works of Krieger, Sire and Kiesenhofer","[13,9].","There is a major obstruction in extending these results to three dimensions due to the loss of the crucial $L^2_tL^\\infty_x$ Strichartz estimate.","In this work, we make progress on this case by proving that the equation is \"weakly\" globally well-posed (in the sense of Tao [28]) for initial data which is not only small in $\\dot{B}^{3/2}_{2,1}$ but also possesses some angular regularity and weighted decay of derivatives.","We use Sterbenz's improved Strichartz estimates in conjunction with certain commuting vector fields to develop trilinear estimates in weighted Strichartz spaces which avoid the use of the $L^2_tL^\\infty_x$ endpoint."],"url":"http://arxiv.org/abs/2403.14567v1","category":"math.AP"}
{"created":"2024-03-21 16:53:17","title":"A Joint Optimization Approach for Power-Efficient Heterogeneous OFDMA Radio Access Networks","abstract":"Heterogeneous networks have emerged as a popular solution for accommodating the growing number of connected devices and increasing traffic demands in cellular networks. While offering broader coverage, higher capacity, and lower latency, the escalating energy consumption poses sustainability challenges. In this paper a novel optimization approach for OFDMA heterogeneous networks is proposed to minimize transmission power while respecting individual users throughput constraints. The problem is formulated as a mixed integer geometric program, and optimizes at once multiple system variables such as user association, working bandwidth, and base stations transmission powers. Crucially, the proposed approach becomes a convex optimization problem when user-base station associations are provided. Evaluations in multiple realistic scenarios from the production mobile network of a major European operator and based on precise channel gains and throughput requirements from measured data validate the effectiveness of the proposed approach. Overall, our original solution paves the road for greener connectivity by reducing the energy footprint of heterogeneous mobile networks, hence fostering more sustainable communication systems.","sentences":["Heterogeneous networks have emerged as a popular solution for accommodating the growing number of connected devices and increasing traffic demands in cellular networks.","While offering broader coverage, higher capacity, and lower latency, the escalating energy consumption poses sustainability challenges.","In this paper a novel optimization approach for OFDMA heterogeneous networks is proposed to minimize transmission power while respecting individual users throughput constraints.","The problem is formulated as a mixed integer geometric program, and optimizes at once multiple system variables such as user association, working bandwidth, and base stations transmission powers.","Crucially, the proposed approach becomes a convex optimization problem when user-base station associations are provided.","Evaluations in multiple realistic scenarios from the production mobile network of a major European operator and based on precise channel gains and throughput requirements from measured data validate the effectiveness of the proposed approach.","Overall, our original solution paves the road for greener connectivity by reducing the energy footprint of heterogeneous mobile networks, hence fostering more sustainable communication systems."],"url":"http://arxiv.org/abs/2403.14555v1","category":"math.OC"}
{"created":"2024-03-21 16:53:03","title":"Gaussian Frosting: Editable Complex Radiance Fields with Real-Time Rendering","abstract":"We propose Gaussian Frosting, a novel mesh-based representation for high-quality rendering and editing of complex 3D effects in real-time. Our approach builds on the recent 3D Gaussian Splatting framework, which optimizes a set of 3D Gaussians to approximate a radiance field from images. We propose first extracting a base mesh from Gaussians during optimization, then building and refining an adaptive layer of Gaussians with a variable thickness around the mesh to better capture the fine details and volumetric effects near the surface, such as hair or grass. We call this layer Gaussian Frosting, as it resembles a coating of frosting on a cake. The fuzzier the material, the thicker the frosting. We also introduce a parameterization of the Gaussians to enforce them to stay inside the frosting layer and automatically adjust their parameters when deforming, rescaling, editing or animating the mesh. Our representation allows for efficient rendering using Gaussian splatting, as well as editing and animation by modifying the base mesh. We demonstrate the effectiveness of our method on various synthetic and real scenes, and show that it outperforms existing surface-based approaches. We will release our code and a web-based viewer as additional contributions. Our project page is the following: https://anttwo.github.io/frosting/","sentences":["We propose Gaussian Frosting, a novel mesh-based representation for high-quality rendering and editing of complex 3D effects in real-time.","Our approach builds on the recent 3D Gaussian Splatting framework, which optimizes a set of 3D Gaussians to approximate a radiance field from images.","We propose first extracting a base mesh from Gaussians during optimization, then building and refining an adaptive layer of Gaussians with a variable thickness around the mesh to better capture the fine details and volumetric effects near the surface, such as hair or grass.","We call this layer Gaussian Frosting, as it resembles a coating of frosting on a cake.","The fuzzier the material, the thicker the frosting.","We also introduce a parameterization of the Gaussians to enforce them to stay inside the frosting layer and automatically adjust their parameters when deforming, rescaling, editing or animating the mesh.","Our representation allows for efficient rendering using Gaussian splatting, as well as editing and animation by modifying the base mesh.","We demonstrate the effectiveness of our method on various synthetic and real scenes, and show that it outperforms existing surface-based approaches.","We will release our code and a web-based viewer as additional contributions.","Our project page is the following: https://anttwo.github.io/frosting/"],"url":"http://arxiv.org/abs/2403.14554v1","category":"cs.CV"}
{"created":"2024-03-21 16:49:39","title":"Resonant Ion Radiation Scattering and the Integrated Atomic Cross-Section as applied to Binary Star Shock Fronts","abstract":"The current literature is rather vague regarding how to calculate the exact numerical value of the resonant ion scattering cross-section that should be used for a specific bandpass of finite width. Such a value was needed in order to calculate the ion and mass densities in the shock fronts of hot, close binary star systems. This was done based on a modeling of ultraviolet wind-line profiles, using IUE spectra. Therefore, a numerical integration has been carried out, in wavelength-space, of the exact expression for the cross-section over two band-passes of astrophysical interest. The exact expression employed was that derived from a solution of the Abraham-Lorentz equation. The numerical results depend on the resonant wavelength, which is taken to be at the center of the bandpass. Most texts on the subject derive an expression for the scattering cross-section in frequency-space, based on the assumption that the radiation reaction term in the Abraham-Lorentz equation may be approximated by a resistive term. The integral of this cross-section over the entire spectrum is independent of the resonant frequency, except for the transition probability. This has limited practical use when dealing with fluxes measured in a bandpass of finite width expressed in wavelength units and scattering is the only mechanism for producing the observed fluxes. Such is the case when dealing with the low densities encountered in stellar winds and shock fronts.   Integrated cross-sections that depend on the resonant wavelength are used to determine the number and mass densities of C IV and N V ions in the shock fronts found in some hot, eclipsing binary star systems, for which several IUE spectra have been obtained over a Keplerian orbital period. This then leans to a determination of the mass density and total mass in the shock once the volume of the shock is determined.","sentences":["The current literature is rather vague regarding how to calculate the exact numerical value of the resonant ion scattering cross-section that should be used for a specific bandpass of finite width.","Such a value was needed in order to calculate the ion and mass densities in the shock fronts of hot, close binary star systems.","This was done based on a modeling of ultraviolet wind-line profiles, using IUE spectra.","Therefore, a numerical integration has been carried out, in wavelength-space, of the exact expression for the cross-section over two band-passes of astrophysical interest.","The exact expression employed was that derived from a solution of the Abraham-Lorentz equation.","The numerical results depend on the resonant wavelength, which is taken to be at the center of the bandpass.","Most texts on the subject derive an expression for the scattering cross-section in frequency-space, based on the assumption that the radiation reaction term in the Abraham-Lorentz equation may be approximated by a resistive term.","The integral of this cross-section over the entire spectrum is independent of the resonant frequency, except for the transition probability.","This has limited practical use when dealing with fluxes measured in a bandpass of finite width expressed in wavelength units and scattering is the only mechanism for producing the observed fluxes.","Such is the case when dealing with the low densities encountered in stellar winds and shock fronts.   ","Integrated cross-sections that depend on the resonant wavelength are used to determine the number and mass densities of C IV and N V ions in the shock fronts found in some hot, eclipsing binary star systems, for which several IUE spectra have been obtained over a Keplerian orbital period.","This then leans to a determination of the mass density and total mass in the shock once the volume of the shock is determined."],"url":"http://arxiv.org/abs/2403.14549v1","category":"astro-ph.SR"}
{"created":"2024-03-21 16:38:10","title":"Learning Hierarchical Control Systems for Autonomous Systems with Energy Constraints","abstract":"This paper focuses on the design of hierarchical control architectures for autonomous systems with energy constraints. We focus on systems where energy storage limitations and slow recharge rates drastically affect the way the autonomous systems are operated. Using examples from space robotics and public transportation, we motivate the need for formally designed learning hierarchical control systems. We propose a learning control architecture which incorporates learning mechanisms at various levels of the control hierarchy to improve performance and resource utilization. The proposed hierarchical control scheme relies on high-level energy-aware task planning and assignment, complemented by a low-level predictive control mechanism responsible for the autonomous execution of tasks, including motion control and energy management. Simulation examples show the benefits and the limitations of the proposed architecture when learning is used to obtain a more energy-efficient task allocation.","sentences":["This paper focuses on the design of hierarchical control architectures for autonomous systems with energy constraints.","We focus on systems where energy storage limitations and slow recharge rates drastically affect the way the autonomous systems are operated.","Using examples from space robotics and public transportation, we motivate the need for formally designed learning hierarchical control systems.","We propose a learning control architecture which incorporates learning mechanisms at various levels of the control hierarchy to improve performance and resource utilization.","The proposed hierarchical control scheme relies on high-level energy-aware task planning and assignment, complemented by a low-level predictive control mechanism responsible for the autonomous execution of tasks, including motion control and energy management.","Simulation examples show the benefits and the limitations of the proposed architecture when learning is used to obtain a more energy-efficient task allocation."],"url":"http://arxiv.org/abs/2403.14536v1","category":"eess.SY"}
{"created":"2024-03-21 16:27:57","title":"Polynomial convexity with degree bounds","abstract":"We introduce different notions of polynomial convexity with bounds on degrees of polynomials in $\\mathbb C^n$. We provide some examples in higher dimensions and show necessary and sufficient conditions for polynomial convexity with degree bounds for certain sets of points in $\\mathbb C$ and for certain arcs in the unit circle.","sentences":["We introduce different notions of polynomial convexity with bounds on degrees of polynomials in $\\mathbb C^n$. We provide some examples in higher dimensions and show necessary and sufficient conditions for polynomial convexity with degree bounds for certain sets of points in $\\mathbb C$ and for certain arcs in the unit circle."],"url":"http://arxiv.org/abs/2403.14529v1","category":"math.CV"}
{"created":"2024-03-21 16:26:45","title":"The Iwahori--Matsumoto dual for tempered representations of Lusztig's geometric Hecke algebras","abstract":"The Iwahori--Matsumoto involution $\\mathsf{IM}$ is an algebra involution on an affine Hecke algebra. To a connected complex reductive group $G$, Lusztig associated various geometric graded Hecke algebras. These graded Hecke algebras are also associated to certain affine Hecke algebras. Let $\\mathbb{H}$ be such a graded Hecke algebra associated to an affine Hecke algebra $\\mathcal{H}$. We obtain an involution $\\mathbb{IM}$ on $\\mathbb{H}$ induced by $\\mathsf{IM}$ on $\\mathcal{H}$ via Lusztig's reduction theorems. We also denote by $\\mathbb{IM}$ the involution on the Grothendieck group of complex finite-dimensional representations of $\\mathbb{H}$ induced by $\\mathbb{IM}$. The irreducible representations of $\\mathbb{H}$ are parametrised by the set $\\mathcal{M}$ of $G$-conjugacy classes of quadruples $(e,s,r_0,\\psi)$ where $r_0 \\in \\mathbb{C}$, $e \\in \\mathrm{Lie}(G)$ is nilpotent, $s \\in \\mathrm{Lie}(G)$ is semisimple, and $\\psi$ is an irreducible representation of the group of components of the simultaneous centraliser of $(e,s)$ in $G$. Let $\\bar Y$ be an irreducible tempered representation of $\\mathbb{H}$ with real infinitesimal character. In this paper, we give an explicit algorithm that computes the $G$-orbit of the nilpotent element in the quadruple in $\\mathcal{M}$ that parametrises the irreducible representation $\\mathbb{IM}(\\bar Y)$ for $G = \\mathrm{Sp}(2n,\\mathbb{C})$ and $G = \\mathrm{SO}(N,\\mathbb{C})$.","sentences":["The Iwahori--Matsumoto involution $\\mathsf{IM}$ is an algebra involution on an affine Hecke algebra.","To a connected complex reductive group $G$, Lusztig associated various geometric graded Hecke algebras.","These graded Hecke algebras are also associated to certain affine Hecke algebras.","Let $\\mathbb{H}$ be such a graded Hecke algebra associated to an affine Hecke algebra $\\mathcal{H}$. We obtain an involution $\\mathbb{IM}$ on $\\mathbb{H}$ induced by $\\mathsf{IM}$ on $\\mathcal{H}$ via Lusztig's reduction theorems.","We also denote by $\\mathbb{IM}$ the involution on the Grothendieck group of complex finite-dimensional representations of $\\mathbb{H}$ induced by $\\mathbb{IM}$. The irreducible representations of $\\mathbb{H}$ are parametrised by the set $\\mathcal{M}$ of $G$-conjugacy classes of quadruples $(e,s,r_0,\\psi)$ where $r_0 \\in \\mathbb{C}$, $e \\in \\mathrm{Lie}(G)$ is nilpotent, $s \\in \\mathrm{Lie}(G)$ is semisimple, and $\\psi$ is an irreducible representation of the group of components of the simultaneous centraliser of $(e,s)$ in $G$. Let $\\bar Y$ be an irreducible tempered representation of $\\mathbb{H}$ with real infinitesimal character.","In this paper, we give an explicit algorithm that computes the $G$-orbit of the nilpotent element in the quadruple in $\\mathcal{M}$ that parametrises the irreducible representation $\\mathbb{IM}(\\bar Y)$ for $G = \\mathrm{Sp}(2n,\\mathbb{C})$ and $G = \\mathrm{SO}(N,\\mathbb{C})$."],"url":"http://arxiv.org/abs/2403.14528v1","category":"math.RT"}
{"created":"2024-03-21 16:17:57","title":"Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference","abstract":"In recent years, the application of multimodal large language models (MLLM) in various fields has achieved remarkable success. However, as the foundation model for many downstream tasks, current MLLMs are composed of the well-known Transformer network, which has a less efficient quadratic computation complexity. To improve the efficiency of such basic models, we propose Cobra, a linear computational complexity MLLM. Specifically, Cobra integrates the efficient Mamba language model into the visual modality. Moreover, we explore and study various modal fusion schemes to create an effective multi-modal Mamba. Extensive experiments demonstrate that (1) Cobra achieves extremely competitive performance with current computationally efficient state-of-the-art methods, \\textit{e.g.}, LLaVA-Phi, TinyLLaVA, and MobileVLM v2, and has faster speed due to Cobra's linear sequential modeling. (2) Interestingly, the results of closed-set challenging prediction benchmarks show that Cobra performs well in overcoming visual illusions and spatial relationship judgments. (3) Notably, Cobra even achieves comparable performance to LLaVA with about 43% of the number of parameters. We will make all codes of Cobra open-source and hope that the proposed method can facilitate future research on complexity problems in MLLM. Our project page is available at: https://sites.google.com/view/cobravlm.","sentences":["In recent years, the application of multimodal large language models (MLLM) in various fields has achieved remarkable success.","However, as the foundation model for many downstream tasks, current MLLMs are composed of the well-known Transformer network, which has a less efficient quadratic computation complexity.","To improve the efficiency of such basic models, we propose Cobra, a linear computational complexity MLLM.","Specifically, Cobra integrates the efficient Mamba language model into the visual modality.","Moreover, we explore and study various modal fusion schemes to create an effective multi-modal Mamba.","Extensive experiments demonstrate that (1) Cobra achieves extremely competitive performance with current computationally efficient state-of-the-art methods, \\textit{e.g.}, LLaVA-Phi, TinyLLaVA, and MobileVLM v2, and has faster speed due to Cobra's linear sequential modeling.","(2) Interestingly, the results of closed-set challenging prediction benchmarks show that Cobra performs well in overcoming visual illusions and spatial relationship judgments.","(3) Notably, Cobra even achieves comparable performance to LLaVA with about 43% of the number of parameters.","We will make all codes of Cobra open-source and hope that the proposed method can facilitate future research on complexity problems in MLLM.","Our project page is available at: https://sites.google.com/view/cobravlm."],"url":"http://arxiv.org/abs/2403.14520v1","category":"cs.CV"}
{"created":"2024-03-21 16:17:39","title":"Designing Robust Linear Output Feedback Controller based on CLF-CBF framework via Linear~Programming(LP-CLF-CBF)","abstract":"We consider the problem of designing output feedback controllers that use measurements from a set of landmarks to navigate through a cell-decomposable environment using duality, Control Lyapunov and Barrier Functions (CLF, CBF), and Linear Programming. We propose two objectives for navigating in an environment, one to traverse the environment by making loops and one by converging to a stabilization point while smoothing the transition between consecutive cells. We test our algorithms in a simulation environment, evaluating the robustness of the approach to practical conditions, such as bearing-only measurements, and measurements acquired with a camera with a limited field of view.","sentences":["We consider the problem of designing output feedback controllers that use measurements from a set of landmarks to navigate through a cell-decomposable environment using duality, Control Lyapunov and Barrier Functions (CLF, CBF), and Linear Programming.","We propose two objectives for navigating in an environment, one to traverse the environment by making loops and one by converging to a stabilization point while smoothing the transition between consecutive cells.","We test our algorithms in a simulation environment, evaluating the robustness of the approach to practical conditions, such as bearing-only measurements, and measurements acquired with a camera with a limited field of view."],"url":"http://arxiv.org/abs/2403.14519v1","category":"eess.SY"}
{"created":"2024-03-21 16:10:42","title":"Machine-learning invariant foliations in forced systems for reduced order modelling","abstract":"We identify reduced order models (ROM) of forced systems from data using invariant foliations. The forcing can be external, parametric, periodic or quasi-periodic. The process has four steps: 1. identify an approximate invariant torus and the linear dynamics about the torus; 2. identify a globally defined invariant foliation about the torus; 3. identify a local foliation about an invariant manifold that complements the global foliation 4. extract the invariant manifold as the leaf going through the torus and interpret the result. We combine steps 2 and 3, so that we can track the location of the invariant torus and scale the invariance equations appropriately. We highlight some fundamental limitations of invariant manifolds and foliations when fitting them to data, that require further mathematics to resolve.","sentences":["We identify reduced order models (ROM) of forced systems from data using invariant foliations.","The forcing can be external, parametric, periodic or quasi-periodic.","The process has four steps: 1. identify an approximate invariant torus and the linear dynamics about the torus; 2. identify a globally defined invariant foliation about the torus; 3. identify a local foliation about an invariant manifold that complements the global foliation 4. extract the invariant manifold as the leaf going through the torus and interpret the result.","We combine steps 2 and 3, so that we can track the location of the invariant torus and scale the invariance equations appropriately.","We highlight some fundamental limitations of invariant manifolds and foliations when fitting them to data, that require further mathematics to resolve."],"url":"http://arxiv.org/abs/2403.14514v1","category":"math.DS"}
{"created":"2024-03-21 16:08:21","title":"View-decoupled Transformer for Person Re-identification under Aerial-ground Camera Network","abstract":"Existing person re-identification methods have achieved remarkable advances in appearance-based identity association across homogeneous cameras, such as ground-ground matching. However, as a more practical scenario, aerial-ground person re-identification (AGPReID) among heterogeneous cameras has received minimal attention. To alleviate the disruption of discriminative identity representation by dramatic view discrepancy as the most significant challenge in AGPReID, the view-decoupled transformer (VDT) is proposed as a simple yet effective framework. Two major components are designed in VDT to decouple view-related and view-unrelated features, namely hierarchical subtractive separation and orthogonal loss, where the former separates these two features inside the VDT, and the latter constrains these two to be independent. In addition, we contribute a large-scale AGPReID dataset called CARGO, consisting of five/eight aerial/ground cameras, 5,000 identities, and 108,563 images. Experiments on two datasets show that VDT is a feasible and effective solution for AGPReID, surpassing the previous method on mAP/Rank1 by up to 5.0%/2.7% on CARGO and 3.7%/5.2% on AG-ReID, keeping the same magnitude of computational complexity. Our project is available at https://github.com/LinlyAC/VDT-AGPReID","sentences":["Existing person re-identification methods have achieved remarkable advances in appearance-based identity association across homogeneous cameras, such as ground-ground matching.","However, as a more practical scenario, aerial-ground person re-identification (AGPReID) among heterogeneous cameras has received minimal attention.","To alleviate the disruption of discriminative identity representation by dramatic view discrepancy as the most significant challenge in AGPReID, the view-decoupled transformer (VDT) is proposed as a simple yet effective framework.","Two major components are designed in VDT to decouple view-related and view-unrelated features, namely hierarchical subtractive separation and orthogonal loss, where the former separates these two features inside the VDT, and the latter constrains these two to be independent.","In addition, we contribute a large-scale AGPReID dataset called CARGO, consisting of five/eight aerial/ground cameras, 5,000 identities, and 108,563 images.","Experiments on two datasets show that VDT is a feasible and effective solution for AGPReID, surpassing the previous method on mAP/Rank1 by up to 5.0%/2.7% on CARGO and 3.7%/5.2% on AG-ReID, keeping the same magnitude of computational complexity.","Our project is available at https://github.com/LinlyAC/VDT-AGPReID"],"url":"http://arxiv.org/abs/2403.14513v1","category":"cs.CV"}
{"created":"2024-03-21 15:56:19","title":"Electrostatic wave interaction via asymmetric vector solitons as precursor to rogue wave formation in non-Maxwellian plasmas","abstract":"An asymmetric pair of coupled nonlinear Schr{\\\"o}dinger (CNLS) equations has been derived through a multiscale perturbation method applied to a plasma fluid model, in which two wavepackets of distinct carrier wavenumbers and amplitudes are allowed to co-propagate and interact. The original fluid model was set up for a non-magnetized plasma consisting of cold inertial ions evolving against a $\\kappa-$distributed electron background in 1D. The reduction procedure resulting in the CNLS equations has provided analytical expressions for the dispersion, self-modulation and cross-coupling coefficients in terms of the carrier wavenumbers.   The system admits various types of vector solitons (VSs), physically representing nonlinear localized electrostatic plasma modes. The possibility for either bright (B) or dark (D) type excitations for either of the two waves provides four combinations for the envelope pair (BB, BD, DB, DD). Moreover, the soliton parameters are also calculated for each type of VS in its respective area of existence. The dependence of the VS characteristics on the carrier wavenumbers and the spectral index $\\kappa$ has been explored. In certain cases, the amplitude of one component may exceed its counterpart (second amplitude) by a factor 2.5 or higher, indicating that extremely asymmetric waves may be formed due to modulational interactions among the wavepackets.   As $\\kappa$ decreases from large values, modulational instability (MI) occurs in larger areas of the parameter plane(s) and with higher growth rates. The distribution of different types of VSs on the parameter plane(s) also varies significantly with decreasing $\\kappa$, and in fact dramatically for $\\kappa$ between $3$ and $2$. Deviation from the Maxwell-Boltzmann picture therefore seems to favor MI as a precursor to the formation of bright (predominantly) type envelope excitations and freak waves.","sentences":["An asymmetric pair of coupled nonlinear Schr{\\\"o}dinger (CNLS) equations has been derived through a multiscale perturbation method applied to a plasma fluid model, in which two wavepackets of distinct carrier wavenumbers and amplitudes are allowed to co-propagate and interact.","The original fluid model was set up for a non-magnetized plasma consisting of cold inertial ions evolving against a $\\kappa-$distributed electron background in 1D.","The reduction procedure resulting in the CNLS equations has provided analytical expressions for the dispersion, self-modulation and cross-coupling coefficients in terms of the carrier wavenumbers.   ","The system admits various types of vector solitons (VSs), physically representing nonlinear localized electrostatic plasma modes.","The possibility for either bright (B) or dark (D) type excitations for either of the two waves provides four combinations for the envelope pair (BB, BD, DB, DD).","Moreover, the soliton parameters are also calculated for each type of VS in its respective area of existence.","The dependence of the VS characteristics on the carrier wavenumbers and the spectral index $\\kappa$ has been explored.","In certain cases, the amplitude of one component may exceed its counterpart (second amplitude) by a factor 2.5 or higher, indicating that extremely asymmetric waves may be formed due to modulational interactions among the wavepackets.   ","As $\\kappa$ decreases from large values, modulational instability (MI) occurs in larger areas of the parameter plane(s) and with higher growth rates.","The distribution of different types of VSs on the parameter plane(s) also varies significantly with decreasing $\\kappa$, and in fact dramatically for $\\kappa$ between $3$ and $2$. Deviation from the Maxwell-Boltzmann picture therefore seems to favor MI as a precursor to the formation of bright (predominantly) type envelope excitations and freak waves."],"url":"http://arxiv.org/abs/2403.14505v1","category":"physics.plasm-ph"}
{"created":"2024-03-21 15:52:49","title":"Meta-learning of data-driven controllers with automatic model reference tuning: theory and experimental case study","abstract":"Data-driven control offers a viable option for control scenarios where constructing a system model is expensive or time-consuming. Nonetheless, many of these algorithms are not entirely automated, often necessitating the adjustment of multiple hyperparameters through cumbersome trial-and-error processes and demanding significant amounts of data. In this paper, we explore a meta-learning approach to leverage potentially existing prior knowledge about analogous (though not identical) systems, aiming to reduce both the experimental workload and ease the tuning of the available degrees of freedom. We validate this methodology through an experimental case study involving the tuning of proportional, integral (PI) controllers for brushless DC (BLDC) motors with variable loads and architectures.","sentences":["Data-driven control offers a viable option for control scenarios where constructing a system model is expensive or time-consuming.","Nonetheless, many of these algorithms are not entirely automated, often necessitating the adjustment of multiple hyperparameters through cumbersome trial-and-error processes and demanding significant amounts of data.","In this paper, we explore a meta-learning approach to leverage potentially existing prior knowledge about analogous (though not identical) systems, aiming to reduce both the experimental workload and ease the tuning of the available degrees of freedom.","We validate this methodology through an experimental case study involving the tuning of proportional, integral (PI) controllers for brushless DC (BLDC) motors with variable loads and architectures."],"url":"http://arxiv.org/abs/2403.14500v1","category":"eess.SY"}
{"created":"2024-03-21 15:49:19","title":"Tilt or twist-competing synclinic and anticlinic interactions in SmC phases of bent-core mesogens","abstract":"Recent liquid-crystalline (LC) research is focused on structurally new molecular systems distinct from simple nematic or smectic phases. Sophisticated molecular shape may reveal structural complexity, combining helicity and polarity. Achiral symmetry-breaking in bent-core molecules leads to propensity for synclinic and anticlinic molecular structures within consecutive smectic layers. Moreover, despite their achiral character, dimers readily adopt helical phases. In our study, we investigated a hybrid molecular structure incorporating both characteristics, namely a rigid-bent core and an attached bulky polar group via a flexible spacer. To perform phase identification, we enriched the standard experimental methods with the sophisticated resonant soft x-ray scattering. Notably, we have observed a distinct preference for specific phase types depending on the length of the homologue. Longer homologues exhibit a predisposition towards the formation of tilted smectic phases, characterized by complex sequences of synclinic and anticlinic interfaces. Conversely, shorter homologues manifest a propensity for helical smectic structures. For intermediate homologues, the frustration is alleviated through the formation of several modulated smectic phases. Based on the presented research, we describe the preconditions for high level structures in relation with conflicting constraints.","sentences":["Recent liquid-crystalline (LC) research is focused on structurally new molecular systems distinct from simple nematic or smectic phases.","Sophisticated molecular shape may reveal structural complexity, combining helicity and polarity.","Achiral symmetry-breaking in bent-core molecules leads to propensity for synclinic and anticlinic molecular structures within consecutive smectic layers.","Moreover, despite their achiral character, dimers readily adopt helical phases.","In our study, we investigated a hybrid molecular structure incorporating both characteristics, namely a rigid-bent core and an attached bulky polar group via a flexible spacer.","To perform phase identification, we enriched the standard experimental methods with the sophisticated resonant soft x-ray scattering.","Notably, we have observed a distinct preference for specific phase types depending on the length of the homologue.","Longer homologues exhibit a predisposition towards the formation of tilted smectic phases, characterized by complex sequences of synclinic and anticlinic interfaces.","Conversely, shorter homologues manifest a propensity for helical smectic structures.","For intermediate homologues, the frustration is alleviated through the formation of several modulated smectic phases.","Based on the presented research, we describe the preconditions for high level structures in relation with conflicting constraints."],"url":"http://arxiv.org/abs/2403.14498v1","category":"cond-mat.soft"}
{"created":"2024-03-21 15:41:13","title":"Real forms of Mori fiber spaces with many symmetries","abstract":"We determine the rational real forms of the complex Mori fiber spaces for which the identity component of the automorphism group is a maximal connected algebraic subgroup of $\\mathrm{Bir}(\\mathbb{P}_{\\mathbb{C}}^{3})$. This yields a list of maximal connected algebraic subgroup of $\\mathrm{Bir}(\\mathbb{P}_{\\mathbb{R}}^{3})$. We furthermore determine the equivariant Sarkisov links starting from these rational real forms. This article is the first step towards classifying all the maximal connected algebraic subgroups of $\\mathrm{Bir}(\\mathbb{P}_{\\mathbb{R}}^{3})$.","sentences":["We determine the rational real forms of the complex Mori fiber spaces for which the identity component of the automorphism group is a maximal connected algebraic subgroup of $\\mathrm{Bir}(\\mathbb{P}_{\\mathbb{C}}^{3})$. This yields a list of maximal connected algebraic subgroup of $\\mathrm{Bir}(\\mathbb{P}_{\\mathbb{R}}^{3})$. We furthermore determine the equivariant Sarkisov links starting from these rational real forms.","This article is the first step towards classifying all the maximal connected algebraic subgroups of $\\mathrm{Bir}(\\mathbb{P}_{\\mathbb{R}}^{3})$."],"url":"http://arxiv.org/abs/2403.14493v1","category":"math.AG"}
{"created":"2024-03-21 15:39:05","title":"Induced Subforests and Superforests","abstract":"Graph isomorphism, subgraph isomorphism, and maximum common subgraphs are classical well-investigated objects. Their (parameterized) complexity and efficiently tractable cases have been studied. In the present paper, for a given set of forests, we study maximum common induced subforests and minimum common induced superforests. We show that finding a maximum subforest is NP-hard already for two subdivided stars while finding a minimum superforest is tractable for two trees but NP-hard for three trees. For a given set of $k$ trees, we present an efficient greedy $\\left(\\frac{k}{2}-\\frac{1}{2}+\\frac{1}{k}\\right)$-approximation algorithm for the minimum superforest problem. Finally, we present a polynomial time approximation scheme for the maximum subforest problem for any given set of forests.","sentences":["Graph isomorphism, subgraph isomorphism, and maximum common subgraphs are classical well-investigated objects.","Their (parameterized) complexity and efficiently tractable cases have been studied.","In the present paper, for a given set of forests, we study maximum common induced subforests and minimum common induced superforests.","We show that finding a maximum subforest is NP-hard already for two subdivided stars while finding a minimum superforest is tractable for two trees but NP-hard for three trees.","For a given set of $k$ trees, we present an efficient greedy $\\left(\\frac{k}{2}-\\frac{1}{2}+\\frac{1}{k}\\right)$-approximation algorithm for the minimum superforest problem.","Finally, we present a polynomial time approximation scheme for the maximum subforest problem for any given set of forests."],"url":"http://arxiv.org/abs/2403.14492v1","category":"cs.DS"}
{"created":"2024-03-21 15:38:47","title":"Counting cherry reduction sequences is counting linear extensions (in phylogenetic tree-child networks)","abstract":"Orchard and tree-child networks share an important property with phylogenetic trees: they can be completely reduced to a single node by iteratively deleting cherries and reticulated cherries. As it is the case with phylogenetic trees, the number of ways in which this can be done gives information about the topology of the network. Here, we show that the problem of computing this number in tree-child networks is akin to that of finding the number of linear extensions of the poset induced by each network, and give an algorithm based on this reduction whose complexity is bounded in terms of the level of the network.","sentences":["Orchard and tree-child networks share an important property with phylogenetic trees: they can be completely reduced to a single node by iteratively deleting cherries and reticulated cherries.","As it is the case with phylogenetic trees, the number of ways in which this can be done gives information about the topology of the network.","Here, we show that the problem of computing this number in tree-child networks is akin to that of finding the number of linear extensions of the poset induced by each network, and give an algorithm based on this reduction whose complexity is bounded in terms of the level of the network."],"url":"http://arxiv.org/abs/2403.14491v1","category":"q-bio.PE"}
{"created":"2024-03-21 15:38:34","title":"Bistatic Doppler Frequency Estimation with Asynchronous Moving Devices for Integrated Sensing and Communications","abstract":"In this letter, we present for the first time a method to estimate the bistatic Doppler frequency of a target with clock asynchronous and mobile Integrated Sensing And Communication (ISAC) devices. Existing approaches have separately tackled the presence of phase offsets due to clock asynchrony or the additional Doppler shift due to device movement. However, in real ISAC scenarios, these two sources of phase nuisance are concurrently present, making the estimation of the target's Doppler frequency particularly challenging. Our method solves the problem using the sole wireless signal at the receiver, by computing Channel Impulse Response (CIR) phase differences across different multipath components and subsequent time instants. In this way, we cancel out phase offsets. Then, we construct a system of equations that allows disentangling the target's Doppler frequency from that of the moving device. The proposed method is validated via simulation, exploring the impact of different system parameters. Numerical results show that our approach is a viable way of estimating Doppler frequency in bistatic asynchronous ISAC scenarios with mobile devices.","sentences":["In this letter, we present for the first time a method to estimate the bistatic Doppler frequency of a target with clock asynchronous and mobile Integrated Sensing And Communication (ISAC) devices.","Existing approaches have separately tackled the presence of phase offsets due to clock asynchrony or the additional Doppler shift due to device movement.","However, in real ISAC scenarios, these two sources of phase nuisance are concurrently present, making the estimation of the target's Doppler frequency particularly challenging.","Our method solves the problem using the sole wireless signal at the receiver, by computing Channel Impulse Response (CIR) phase differences across different multipath components and subsequent time instants.","In this way, we cancel out phase offsets.","Then, we construct a system of equations that allows disentangling the target's Doppler frequency from that of the moving device.","The proposed method is validated via simulation, exploring the impact of different system parameters.","Numerical results show that our approach is a viable way of estimating Doppler frequency in bistatic asynchronous ISAC scenarios with mobile devices."],"url":"http://arxiv.org/abs/2403.14490v1","category":"eess.SP"}
{"created":"2024-03-21 15:37:37","title":"Adversary-Robust Graph-Based Learning of WSIs","abstract":"Enhancing the robustness of deep learning models against adversarial attacks is crucial, especially in critical domains like healthcare where significant financial interests heighten the risk of such attacks. Whole slide images (WSIs) are high-resolution, digitized versions of tissue samples mounted on glass slides, scanned using sophisticated imaging equipment. The digital analysis of WSIs presents unique challenges due to their gigapixel size and multi-resolution storage format. In this work, we aim at improving the robustness of cancer Gleason grading classification systems against adversarial attacks, addressing challenges at both the image and graph levels. As regards the proposed algorithm, we develop a novel and innovative graph-based model which utilizes GNN to extract features from the graph representation of WSIs. A denoising module, along with a pooling layer is incorporated to manage the impact of adversarial attacks on the WSIs. The process concludes with a transformer module that classifies various grades of prostate cancer based on the processed data. To assess the effectiveness of the proposed method, we conducted a comparative analysis using two scenarios. Initially, we trained and tested the model without the denoiser using WSIs that had not been exposed to any attack. We then introduced a range of attacks at either the image or graph level and processed them through the proposed network. The performance of the model was evaluated in terms of accuracy and kappa scores. The results from this comparison showed a significant improvement in cancer diagnosis accuracy, highlighting the robustness and efficiency of the proposed method in handling adversarial challenges in the context of medical imaging.","sentences":["Enhancing the robustness of deep learning models against adversarial attacks is crucial, especially in critical domains like healthcare where significant financial interests heighten the risk of such attacks.","Whole slide images (WSIs) are high-resolution, digitized versions of tissue samples mounted on glass slides, scanned using sophisticated imaging equipment.","The digital analysis of WSIs presents unique challenges due to their gigapixel size and multi-resolution storage format.","In this work, we aim at improving the robustness of cancer Gleason grading classification systems against adversarial attacks, addressing challenges at both the image and graph levels.","As regards the proposed algorithm, we develop a novel and innovative graph-based model which utilizes GNN to extract features from the graph representation of WSIs.","A denoising module, along with a pooling layer is incorporated to manage the impact of adversarial attacks on the WSIs.","The process concludes with a transformer module that classifies various grades of prostate cancer based on the processed data.","To assess the effectiveness of the proposed method, we conducted a comparative analysis using two scenarios.","Initially, we trained and tested the model without the denoiser using WSIs that had not been exposed to any attack.","We then introduced a range of attacks at either the image or graph level and processed them through the proposed network.","The performance of the model was evaluated in terms of accuracy and kappa scores.","The results from this comparison showed a significant improvement in cancer diagnosis accuracy, highlighting the robustness and efficiency of the proposed method in handling adversarial challenges in the context of medical imaging."],"url":"http://arxiv.org/abs/2403.14489v1","category":"cs.CV"}
{"created":"2024-03-21 15:32:49","title":"Incorrect Resonance Escape Probability in Monte Carlo Codes due to the Threshold Approximation of Temperature-Dependent Scattering","abstract":"Monte Carlo-transport codes are designed to simulate the complex neutron transport physics associated with nuclear systems. These codes are tasked with simulating phenomena such as temperature effects on cross-sections, thermo-physical effects, reaction rates, and kinematics. It is not computationally possible to simulate the physics of a system exactly. However, many of the approximations made by modern simulation codes have been well validated. This article investigates an impactful simulation error caused by an approximation made in many Monte Carlo-transport codes. The approximation that target-at-rest is valid for neutrons at energies 400 times that of the thermal energy of the target particle is found to be inaccurate in certain scenarios. This paper identifies such cases, notably TRISO [1] fuel and instances where fuel infiltrates the pores of graphite in Molten Salt Reactors. The breakdown of this approximation occurs particularly when there exists a small length scale between fuel, a material with absorption resonances, and moderator, a scattering material. When threshold values are too small, resonance escape probabilities can deviate by as much as 1% per resonance, forming a baseline defect. Furthermore, two distinct anomalies were observed upon temperature variation, directly attributed to the transition between target-at-rest and target-in-motion physics. Equations provided in this study offer predictions for the temperature ranges within which these anomalies occur, based on system temperature and threshold value. The recommendations put forth in this paper advocate for incorporating the threshold value as a user-defined variable in transport Monte Carlo codes employing this approximation. Additionally, users are advised to conduct convergence studies to ensure that the chosen threshold value is sufficiently high to mitigate the influence of baseline defects and anomalies.","sentences":["Monte Carlo-transport codes are designed to simulate the complex neutron transport physics associated with nuclear systems.","These codes are tasked with simulating phenomena such as temperature effects on cross-sections, thermo-physical effects, reaction rates, and kinematics.","It is not computationally possible to simulate the physics of a system exactly.","However, many of the approximations made by modern simulation codes have been well validated.","This article investigates an impactful simulation error caused by an approximation made in many Monte Carlo-transport codes.","The approximation that target-at-rest is valid for neutrons at energies 400 times that of the thermal energy of the target particle is found to be inaccurate in certain scenarios.","This paper identifies such cases, notably TRISO [1] fuel and instances where fuel infiltrates the pores of graphite in Molten Salt Reactors.","The breakdown of this approximation occurs particularly when there exists a small length scale between fuel, a material with absorption resonances, and moderator, a scattering material.","When threshold values are too small, resonance escape probabilities can deviate by as much as 1% per resonance, forming a baseline defect.","Furthermore, two distinct anomalies were observed upon temperature variation, directly attributed to the transition between target-at-rest and target-in-motion physics.","Equations provided in this study offer predictions for the temperature ranges within which these anomalies occur, based on system temperature and threshold value.","The recommendations put forth in this paper advocate for incorporating the threshold value as a user-defined variable in transport Monte Carlo codes employing this approximation.","Additionally, users are advised to conduct convergence studies to ensure that the chosen threshold value is sufficiently high to mitigate the influence of baseline defects and anomalies."],"url":"http://arxiv.org/abs/2403.14486v1","category":"physics.comp-ph"}
{"created":"2024-03-21 15:31:28","title":"Decorating the gauge/YBE correspondence","abstract":"In this paper, we aim to study the three-dimensional $\\mathcal N=2$ supersymmetric dual gauge theories on $S_b^3/\\mathbb{Z}_r$ in the context of the gauge/YBE correspondence. We consider hyperbolic hypergeometric integral identities acquired via the equality of supersymmetric lens partition functions as solutions to the decoration transformation and the flipping relation in statistical mechanics. The solutions of those transformations aim at investigating various decorated lattice models possessing the Boltzmann weights of integrable Ising-like models obtained via the gauge/YBE correspondence. We also constructed The Bailey pairs for the decoration transformation and the flipping relation.","sentences":["In this paper, we aim to study the three-dimensional $\\mathcal N=2$ supersymmetric dual gauge theories on $S_b^3/\\mathbb{Z}_r$ in the context of the gauge/YBE correspondence.","We consider hyperbolic hypergeometric integral identities acquired via the equality of supersymmetric lens partition functions as solutions to the decoration transformation and the flipping relation in statistical mechanics.","The solutions of those transformations aim at investigating various decorated lattice models possessing the Boltzmann weights of integrable Ising-like models obtained via the gauge/YBE correspondence.","We also constructed The Bailey pairs for the decoration transformation and the flipping relation."],"url":"http://arxiv.org/abs/2403.14485v1","category":"hep-th"}
{"created":"2024-03-21 15:29:01","title":"covSTATIS: a multi-table technique for network neuroscience","abstract":"Similarity analyses between multiple correlation or covariance tables constitute the cornerstone of network neuroscience. Here, we introduce covSTATIS, a versatile, linear, unsupervised multi-table method designed to identify structured patterns in multi-table data, and allow for the simultaneous extraction and interpretation of both individual and group-level features. With covSTATIS, multiple similarity tables can now be easily integrated, without requiring a priori data simplification, complex black-box implementations, user-dependent specifications, or supervised frameworks. Applications of covSTATIS and source code are provided. covSTATIS offers a promising avenue for advancing the theoretical and analytic landscape of network neuroscience.","sentences":["Similarity analyses between multiple correlation or covariance tables constitute the cornerstone of network neuroscience.","Here, we introduce covSTATIS, a versatile, linear, unsupervised multi-table method designed to identify structured patterns in multi-table data, and allow for the simultaneous extraction and interpretation of both individual and group-level features.","With covSTATIS, multiple similarity tables can now be easily integrated, without requiring a priori data simplification, complex black-box implementations, user-dependent specifications, or supervised frameworks.","Applications of covSTATIS and source code are provided.","covSTATIS offers a promising avenue for advancing the theoretical and analytic landscape of network neuroscience."],"url":"http://arxiv.org/abs/2403.14481v1","category":"q-bio.QM"}
{"created":"2024-03-21 15:26:11","title":"Periodicity from X-ray sources within the inner Galactic disk","abstract":"For many years, it has been claimed that the Galactic ridge X-ray emission at the Galactic Center (GC) is truly diffuse in nature. However, with the advancement of modern X-ray satellites, it has been found that most of the diffuse emission is actually comprised of thousands of previously unresolved X-ray point sources. Further, many studies suggest that a vast majority of these X-ray point sources are magnetic cataclysmic variables (mCVs) and active binaries. One unambiguous way to identify these mCVs and other sources is by detecting their X-ray periodicity. Therefore, we systematically searched for periodic X-ray sources in the inner Galactic disk, including the GC region. We have used data from our ongoing XMM-Newton Heritage survey of the inner Galactic disk ($350^{\\circ}\\lesssim l\\lesssim+7^{\\circ}$ and $-1^{\\circ}\\lesssim b\\lesssim +1^{\\circ}$) plus the XMM-Newton archival observations of the GC. We computed the Lomb-Scargle periodogram of the light curves for the periodicity search. We fitted the energy spectra of the sources using a simple power-law model plus three Gaussians at 6.4, 6.7, and 6.9 keV for the iron $K$ emission complex. We detected periodicity in 26 sources. For 14 of them, this is the first discovery of periodicity. For the other 12 sources, we found periods similar to those already known, indicating no significant period evolution. We also searched for the Gaia counterparts of the periodic sources to estimate their distances using the Gaia parallax. We found a likely Gaia counterpart for seven sources. We have classified the sources into four categories based on the periodicity, hardness ratio, and the equivalent width of Fe $K$ line emission. Of the 14 sources where we detect the periodicity for the first time, four are likely to be intermediate polars, five are likely to be polars, two are neutron star X-ray binaries, and three are of unknown nature.","sentences":["For many years, it has been claimed that the Galactic ridge X-ray emission at the Galactic Center (GC) is truly diffuse in nature.","However, with the advancement of modern X-ray satellites, it has been found that most of the diffuse emission is actually comprised of thousands of previously unresolved X-ray point sources.","Further, many studies suggest that a vast majority of these X-ray point sources are magnetic cataclysmic variables (mCVs) and active binaries.","One unambiguous way to identify these mCVs and other sources is by detecting their X-ray periodicity.","Therefore, we systematically searched for periodic X-ray sources in the inner Galactic disk, including the GC region.","We have used data from our ongoing XMM-Newton Heritage survey of the inner Galactic disk ($350^{\\circ}\\lesssim l\\lesssim+7^{\\circ}$ and $-1^{\\circ}\\lesssim b\\lesssim +1^{\\circ}$) plus the XMM-Newton archival observations of the GC.","We computed the Lomb-Scargle periodogram of the light curves for the periodicity search.","We fitted the energy spectra of the sources using a simple power-law model plus three Gaussians at 6.4, 6.7, and 6.9 keV for the iron $K$ emission complex.","We detected periodicity in 26 sources.","For 14 of them, this is the first discovery of periodicity.","For the other 12 sources, we found periods similar to those already known, indicating no significant period evolution.","We also searched for the Gaia counterparts of the periodic sources to estimate their distances using the Gaia parallax.","We found a likely Gaia counterpart for seven sources.","We have classified the sources into four categories based on the periodicity, hardness ratio, and the equivalent width of Fe $K$ line emission.","Of the 14 sources where we detect the periodicity for the first time, four are likely to be intermediate polars, five are likely to be polars, two are neutron star X-ray binaries, and three are of unknown nature."],"url":"http://arxiv.org/abs/2403.14480v1","category":"astro-ph.HE"}
{"created":"2024-03-21 15:13:05","title":"Synthesizing Controller for Safe Navigation using Control Density Function","abstract":"We consider the problem of navigating a nonlinear dynamical system from some initial set to some target set while avoiding collision with an unsafe set. We extend the concept of density function to control density function (CDF) for solving navigation problems with safety constraints. The occupancy-based interpretation of the measure associated with the density function is instrumental in imposing the safety constraints. The navigation problem with safety constraints is formulated as a quadratic program (QP) using CDF. The existing approach using the control barrier function (CBF) also formulates the navigation problem with safety constraints as QP. One of the main advantages of the proposed QP using CDF compared to QP formulated using CBF is that both the convergence/stability and safety can be combined and imposed using the CDF. Simulation results involving the Duffing oscillator and safe navigation of Dubin car models are provided to verify the main findings of the paper.","sentences":["We consider the problem of navigating a nonlinear dynamical system from some initial set to some target set while avoiding collision with an unsafe set.","We extend the concept of density function to control density function (CDF) for solving navigation problems with safety constraints.","The occupancy-based interpretation of the measure associated with the density function is instrumental in imposing the safety constraints.","The navigation problem with safety constraints is formulated as a quadratic program (QP) using CDF.","The existing approach using the control barrier function (CBF) also formulates the navigation problem with safety constraints as QP.","One of the main advantages of the proposed QP using CDF compared to QP formulated using CBF is that both the convergence/stability and safety can be combined and imposed using the CDF.","Simulation results involving the Duffing oscillator and safe navigation of Dubin car models are provided to verify the main findings of the paper."],"url":"http://arxiv.org/abs/2403.14464v1","category":"eess.SY"}
{"created":"2024-03-21 15:12:17","title":"Modeling of high-pressure transient gas-liquid flow in M-shaped jumpers of subsea gas production systems","abstract":"Two-phase flow with low liquid loads is common in high-pressure natural gas offshore gathering and transmission pipelines. During gas production slowdowns or shutdowns, an accumulation of liquid in the lower sections of subsea pipelines may occur. This phenomenon is observed in jumpers that connect different units in deep-water subsea gas production facilities. The displacement of the accumulated liquid during production ramp-up induces temporal variations in pressure drop across the jumper and forces on its elbows, resulting in flow-induced vibrations (FIV) that pose potential risks to the structural integrity of the jumper. To bridge the gap between laboratory experiments and field conditions, transient 3D numerical simulations were conducted using the OpenFOAM software. These simulations facilitated the development of a mechanistic model to elucidate the factors contributing to increased pressure and forces during the liquid purging process. The study examined the influence of gas pressure level, pipe diameter, initially accumulated liquid amount, liquid properties, and gas mass flow rate on the transient pressure drop and the forces acting on the jumper's elbows. The critical gas production rate required for complete liquid removal of the accumulated liquid was determined, and scaling rules were proposed to predict the effects of gas pressure and pipe diameter on this critical value. The dominant frequencies of pressure and force fluctuations were identified, with low-pressure systems exhibiting frequencies associated with two-phase flow phenomena and high-pressure systems showing frequencies attributed to acoustic waves.","sentences":["Two-phase flow with low liquid loads is common in high-pressure natural gas offshore gathering and transmission pipelines.","During gas production slowdowns or shutdowns, an accumulation of liquid in the lower sections of subsea pipelines may occur.","This phenomenon is observed in jumpers that connect different units in deep-water subsea gas production facilities.","The displacement of the accumulated liquid during production ramp-up induces temporal variations in pressure drop across the jumper and forces on its elbows, resulting in flow-induced vibrations (FIV) that pose potential risks to the structural integrity of the jumper.","To bridge the gap between laboratory experiments and field conditions, transient 3D numerical simulations were conducted using the OpenFOAM software.","These simulations facilitated the development of a mechanistic model to elucidate the factors contributing to increased pressure and forces during the liquid purging process.","The study examined the influence of gas pressure level, pipe diameter, initially accumulated liquid amount, liquid properties, and gas mass flow rate on the transient pressure drop and the forces acting on the jumper's elbows.","The critical gas production rate required for complete liquid removal of the accumulated liquid was determined, and scaling rules were proposed to predict the effects of gas pressure and pipe diameter on this critical value.","The dominant frequencies of pressure and force fluctuations were identified, with low-pressure systems exhibiting frequencies associated with two-phase flow phenomena and high-pressure systems showing frequencies attributed to acoustic waves."],"url":"http://arxiv.org/abs/2403.14463v1","category":"physics.flu-dyn"}
{"created":"2024-03-21 15:02:50","title":"Non-Markovian skin effect","abstract":"The Liouvillian skin effect and the non-Hermitian skin effect have both been used to explain the localization of eigenmodes near system boundaries, though the former is arguably more accurate in some regimes due to its incorporation of quantum jumps. However, these frameworks predominantly focus on weak Markovian interactions, neglecting the potentially crucial role of memory effects. To address this, we investigate, utilizing the powerful hierarchical equations of motion method, how a non-Markovian environment can modify the Liouvillian skin effect. We demonstrate that a non-Markovian environment can induce not only a ``thick skin effect\", where the skin mode broadens and shifts into the bulk, but also skin-mode coherence, leading to the coherence-delocalization and oscillatory relaxation with a characteristic linear scaling with system size. Remarkably, both the skin-mode and steady-state coherence exhibit resistance to decoherence from additional environmental noise. These findings highlight the profound impact of system-bath correlations on relaxation and localization, revealing unique phenomena beyond conventional Markovian approximations.","sentences":["The Liouvillian skin effect and the non-Hermitian skin effect have both been used to explain the localization of eigenmodes near system boundaries, though the former is arguably more accurate in some regimes due to its incorporation of quantum jumps.","However, these frameworks predominantly focus on weak Markovian interactions, neglecting the potentially crucial role of memory effects.","To address this, we investigate, utilizing the powerful hierarchical equations of motion method, how a non-Markovian environment can modify the Liouvillian skin effect.","We demonstrate that a non-Markovian environment can induce not only a ``thick skin effect\", where the skin mode broadens and shifts into the bulk, but also skin-mode coherence, leading to the coherence-delocalization and oscillatory relaxation with a characteristic linear scaling with system size.","Remarkably, both the skin-mode and steady-state coherence exhibit resistance to decoherence from additional environmental noise.","These findings highlight the profound impact of system-bath correlations on relaxation and localization, revealing unique phenomena beyond conventional Markovian approximations."],"url":"http://arxiv.org/abs/2403.14455v1","category":"quant-ph"}
{"created":"2024-03-21 14:53:50","title":"Exploring 3D Human Pose Estimation and Forecasting from the Robot's Perspective: The HARPER Dataset","abstract":"We introduce HARPER, a novel dataset for 3D body pose estimation and forecast in dyadic interactions between users and \\spot, the quadruped robot manufactured by Boston Dynamics. The key-novelty is the focus on the robot's perspective, i.e., on the data captured by the robot's sensors. These make 3D body pose analysis challenging because being close to the ground captures humans only partially. The scenario underlying HARPER includes 15 actions, of which 10 involve physical contact between the robot and users. The Corpus contains not only the recordings of the built-in stereo cameras of Spot, but also those of a 6-camera OptiTrack system (all recordings are synchronized). This leads to ground-truth skeletal representations with a precision lower than a millimeter. In addition, the Corpus includes reproducible benchmarks on 3D Human Pose Estimation, Human Pose Forecasting, and Collision Prediction, all based on publicly available baseline approaches. This enables future HARPER users to rigorously compare their results with those we provide in this work.","sentences":["We introduce HARPER, a novel dataset for 3D body pose estimation and forecast in dyadic interactions between users and \\spot, the quadruped robot manufactured by Boston Dynamics.","The key-novelty is the focus on the robot's perspective, i.e., on the data captured by the robot's sensors.","These make 3D body pose analysis challenging because being close to the ground captures humans only partially.","The scenario underlying HARPER includes 15 actions, of which 10 involve physical contact between the robot and users.","The Corpus contains not only the recordings of the built-in stereo cameras of Spot, but also those of a 6-camera OptiTrack system (all recordings are synchronized).","This leads to ground-truth skeletal representations with a precision lower than a millimeter.","In addition, the Corpus includes reproducible benchmarks on 3D Human Pose Estimation, Human Pose Forecasting, and Collision Prediction, all based on publicly available baseline approaches.","This enables future HARPER users to rigorously compare their results with those we provide in this work."],"url":"http://arxiv.org/abs/2403.14447v1","category":"cs.CV"}
{"created":"2024-03-21 14:52:03","title":"History-Independent Concurrent Objects","abstract":"A data structure is called history independent if its internal memory representation does not reveal the history of operations applied to it, only its current state. In this paper we study history independence for concurrent data structures, and establish foundational possibility and impossibility results. We show that a large class of concurrent objects cannot be implemented from smaller base objects in a manner that is both wait-free and history independent; but if we settle for either lock-freedom instead of wait-freedom or for a weak notion of history independence, then at least one object in the class, multi-valued single-reader single-writer registers, can be implemented from smaller base objects, binary registers.   On the other hand, using large base objects, we give a strong possibility result in the form of a universal construction: an object with $s$ possible states can be implemented in a wait-free, history-independent manner from compare-and-swap base objects that each have $O(s + 2^n)$ possible memory states, where $n$ is the number of processes in the system.","sentences":["A data structure is called history independent if its internal memory representation does not reveal the history of operations applied to it, only its current state.","In this paper we study history independence for concurrent data structures, and establish foundational possibility and impossibility results.","We show that a large class of concurrent objects cannot be implemented from smaller base objects in a manner that is both wait-free and history independent; but if we settle for either lock-freedom instead of wait-freedom or for a weak notion of history independence, then at least one object in the class, multi-valued single-reader single-writer registers, can be implemented from smaller base objects, binary registers.   ","On the other hand, using large base objects, we give a strong possibility result in the form of a universal construction: an object with $s$ possible states can be implemented in a wait-free, history-independent manner from compare-and-swap base objects that each have $O(s + 2^n)$ possible memory states, where $n$ is the number of processes in the system."],"url":"http://arxiv.org/abs/2403.14445v1","category":"cs.DC"}
{"created":"2024-03-21 14:44:03","title":"A Multimodal Approach to Device-Directed Speech Detection with Large Language Models","abstract":"Interactions with virtual assistants typically start with a predefined trigger phrase followed by the user command. To make interactions with the assistant more intuitive, we explore whether it is feasible to drop the requirement that users must begin each command with a trigger phrase. We explore this task in three ways: First, we train classifiers using only acoustic information obtained from the audio waveform. Second, we take the decoder outputs of an automatic speech recognition (ASR) system, such as 1-best hypotheses, as input features to a large language model (LLM). Finally, we explore a multimodal system that combines acoustic and lexical features, as well as ASR decoder signals in an LLM. Using multimodal information yields relative equal-error-rate improvements over text-only and audio-only models of up to 39% and 61%. Increasing the size of the LLM and training with low-rank adaption leads to further relative EER reductions of up to 18% on our dataset.","sentences":["Interactions with virtual assistants typically start with a predefined trigger phrase followed by the user command.","To make interactions with the assistant more intuitive, we explore whether it is feasible to drop the requirement that users must begin each command with a trigger phrase.","We explore this task in three ways: First, we train classifiers using only acoustic information obtained from the audio waveform.","Second, we take the decoder outputs of an automatic speech recognition (ASR) system, such as 1-best hypotheses, as input features to a large language model (LLM).","Finally, we explore a multimodal system that combines acoustic and lexical features, as well as ASR decoder signals in an LLM.","Using multimodal information yields relative equal-error-rate improvements over text-only and audio-only models of up to 39% and 61%.","Increasing the size of the LLM and training with low-rank adaption leads to further relative EER reductions of up to 18% on our dataset."],"url":"http://arxiv.org/abs/2403.14438v1","category":"cs.CL"}
{"created":"2024-03-21 14:43:25","title":"An Efficient Rate Splitting Precoding Approach in Multi-User MISO FDD Systems","abstract":"In this work, we develop an efficient precoding strategy for a multi-user multiple-input-single output (MU MISO) system operating in frequency-division-duplex (FDD) mode, where rate splitting multiple access (RSMA) is implemented. To this end, we consider one-layer RS and show its significant impact on the system performance, specifically in the case where the channel state information (CSI) is incomplete at the transmitter. Based on a lower bound on the achievable rate that takes into account the CSI errors, we establish an augmented weighted average mean squared error (AWAMSE) algorithm for the RS setup denoted by AWAMSE-RS, where even the updates for the common and the private precoders are computed via analytical expressions, hence circumventing the need for interior-point methods. Simulation results validate the efficiency of our approach in terms of computational time and its competitiveness in terms of the achievable system throughput compared to state-of-the-art methods and non-RS setups.","sentences":["In this work, we develop an efficient precoding strategy for a multi-user multiple-input-single output (MU MISO) system operating in frequency-division-duplex (FDD) mode, where rate splitting multiple access (RSMA) is implemented.","To this end, we consider one-layer RS and show its significant impact on the system performance, specifically in the case where the channel state information (CSI) is incomplete at the transmitter.","Based on a lower bound on the achievable rate that takes into account the CSI errors, we establish an augmented weighted average mean squared error (AWAMSE) algorithm for the RS setup denoted by AWAMSE-RS, where even the updates for the common and the private precoders are computed via analytical expressions, hence circumventing the need for interior-point methods.","Simulation results validate the efficiency of our approach in terms of computational time and its competitiveness in terms of the achievable system throughput compared to state-of-the-art methods and non-RS setups."],"url":"http://arxiv.org/abs/2403.14437v1","category":"eess.SP"}
{"created":"2024-03-21 14:37:53","title":"Breaking consensus in kinetic opinion formation models on graphons","abstract":"In this work we propose and investigate a strategy to prevent consensus in kinetic models for opinion formation. We consider a large interacting agent system, and assume that agent interactions are driven by compromise as well as self-thinking dynamics and also modulated by an underlying static social network. This network structure is included using so-called graphons, which modulate the interaction frequency in the corresponding kinetic formulation. We then derive the corresponding limiting Fokker Planck equation, and analyze its large time behavior. This microscopic setting serves as a starting point for the proposed control strategy, which steers agents away from mean opinion and is characterised by a suitable penalization depending on the properties of the graphon. We show that this minimalist approach is very effective by analyzing the quasi-stationary solutions mean-field model in a plurality of graphon structures. Several numerical experiments are also provided the show the effectiveness of the approach in preventing the formation of consensus steering the system towards a declustered state.","sentences":["In this work we propose and investigate a strategy to prevent consensus in kinetic models for opinion formation.","We consider a large interacting agent system, and assume that agent interactions are driven by compromise as well as self-thinking dynamics and also modulated by an underlying static social network.","This network structure is included using so-called graphons, which modulate the interaction frequency in the corresponding kinetic formulation.","We then derive the corresponding limiting Fokker Planck equation, and analyze its large time behavior.","This microscopic setting serves as a starting point for the proposed control strategy, which steers agents away from mean opinion and is characterised by a suitable penalization depending on the properties of the graphon.","We show that this minimalist approach is very effective by analyzing the quasi-stationary solutions mean-field model in a plurality of graphon structures.","Several numerical experiments are also provided the show the effectiveness of the approach in preventing the formation of consensus steering the system towards a declustered state."],"url":"http://arxiv.org/abs/2403.14431v1","category":"math-ph"}
{"created":"2024-03-21 14:36:55","title":"FHAUC: Privacy Preserving AUC Calculation for Federated Learning using Fully Homomorphic Encryption","abstract":"Ensuring data privacy is a significant challenge for machine learning applications, not only during model training but also during evaluation. Federated learning has gained significant research interest in recent years as a result. Current research on federated learning primarily focuses on preserving privacy during the training phase. However, model evaluation has not been adequately addressed, despite the potential for significant privacy leaks during this phase as well. In this paper, we demonstrate that the state-of-the-art AUC computation method for federated learning systems, which utilizes differential privacy, still leaks sensitive information about the test data while also requiring a trusted central entity to perform the computations. More importantly, we show that the performance of this method becomes completely unusable as the data size decreases. In this context, we propose an efficient, accurate, robust, and more secure evaluation algorithm capable of computing the AUC in horizontal federated learning systems. Our approach not only enhances security compared to the current state-of-the-art but also surpasses the state-of-the-art AUC computation method in both approximation performance and computational robustness, as demonstrated by experimental results. To illustrate, our approach can efficiently calculate the AUC of a federated learning system involving 100 parties, achieving 99.93% accuracy in just 0.68 seconds, regardless of data size, while providing complete data privacy.","sentences":["Ensuring data privacy is a significant challenge for machine learning applications, not only during model training but also during evaluation.","Federated learning has gained significant research interest in recent years as a result.","Current research on federated learning primarily focuses on preserving privacy during the training phase.","However, model evaluation has not been adequately addressed, despite the potential for significant privacy leaks during this phase as well.","In this paper, we demonstrate that the state-of-the-art AUC computation method for federated learning systems, which utilizes differential privacy, still leaks sensitive information about the test data while also requiring a trusted central entity to perform the computations.","More importantly, we show that the performance of this method becomes completely unusable as the data size decreases.","In this context, we propose an efficient, accurate, robust, and more secure evaluation algorithm capable of computing the AUC in horizontal federated learning systems.","Our approach not only enhances security compared to the current state-of-the-art but also surpasses the state-of-the-art AUC computation method in both approximation performance and computational robustness, as demonstrated by experimental results.","To illustrate, our approach can efficiently calculate the AUC of a federated learning system involving 100 parties, achieving 99.93% accuracy in just 0.68 seconds, regardless of data size, while providing complete data privacy."],"url":"http://arxiv.org/abs/2403.14428v1","category":"cs.CR"}
{"created":"2024-03-21 14:33:34","title":"Emergent communication and learning pressures in language models: a language evolution perspective","abstract":"Language models and humans are two types of learning systems. Finding or facilitating commonalities could enable major breakthroughs in our understanding of the acquisition and evolution of language. Many theories of language evolution rely heavily on learning biases and learning pressures. Yet due to substantial differences in learning pressures, it is questionable whether the similarity between humans and machines is sufficient for insights to carry over and to be worth testing with human participants. Here, we review the emergent communication literature, a subfield of multi-agent reinforcement learning, from a language evolution perspective. We find that the emergent communication literature excels at designing and adapting models to recover initially absent linguistic phenomena of natural languages. Based on a short literature review, we identify key pressures that have recovered initially absent human patterns in emergent communication models: communicative success, efficiency, learnability, and other psycho-/sociolinguistic factors. We argue that this may serve as inspiration for how to design language models for language acquisition and language evolution research.","sentences":["Language models and humans are two types of learning systems.","Finding or facilitating commonalities could enable major breakthroughs in our understanding of the acquisition and evolution of language.","Many theories of language evolution rely heavily on learning biases and learning pressures.","Yet due to substantial differences in learning pressures, it is questionable whether the similarity between humans and machines is sufficient for insights to carry over and to be worth testing with human participants.","Here, we review the emergent communication literature, a subfield of multi-agent reinforcement learning, from a language evolution perspective.","We find that the emergent communication literature excels at designing and adapting models to recover initially absent linguistic phenomena of natural languages.","Based on a short literature review, we identify key pressures that have recovered initially absent human patterns in emergent communication models: communicative success, efficiency, learnability, and other psycho-/sociolinguistic factors.","We argue that this may serve as inspiration for how to design language models for language acquisition and language evolution research."],"url":"http://arxiv.org/abs/2403.14427v1","category":"cs.CL"}
{"created":"2024-03-21 14:02:04","title":"Random Graph Modeling: A survey of the concepts","abstract":"Random graph (RG) models play a central role in the complex networks analysis. They help to understand, control, and predict phenomena occurring, for instance, in social networks, biological networks, the Internet, etc.   Despite a large number of RG models presented in the literature, there are few concepts underlying them. Instead of trying to classify a wide variety of very dispersed models, we capture and describe concepts they exploit considering preferential attachment, copying principle, hyperbolic geometry, recursively defined structure, edge switching, Monte Carlo sampling, etc. We analyze RG models, extract their basic principles, and build a taxonomy of concepts they are based on. We also discuss how these concepts are combined in RG models and how they work in typical applications like benchmarks, null models, and data anonymization.","sentences":["Random graph (RG) models play a central role in the complex networks analysis.","They help to understand, control, and predict phenomena occurring, for instance, in social networks, biological networks, the Internet, etc.   ","Despite a large number of RG models presented in the literature, there are few concepts underlying them.","Instead of trying to classify a wide variety of very dispersed models, we capture and describe concepts they exploit considering preferential attachment, copying principle, hyperbolic geometry, recursively defined structure, edge switching, Monte Carlo sampling, etc.","We analyze RG models, extract their basic principles, and build a taxonomy of concepts they are based on.","We also discuss how these concepts are combined in RG models and how they work in typical applications like benchmarks, null models, and data anonymization."],"url":"http://arxiv.org/abs/2403.14415v1","category":"cs.SI"}
{"created":"2024-03-21 13:57:04","title":"New bounds for heat transport in internally heated convection at infinite Prandtl number","abstract":"We prove new bounds on the heat flux out of the bottom boundary, $\\mathcal{F}_B$, for a fluid at infinite Prandtl number, heated internally between isothermal parallel plates under two kinematic boundary conditions. In uniform internally heated convection, the supply of heat equally leaves the domain by conduction when there is no flow. When the heating, quantified by the Rayleigh number, $R$, is sufficiently large, turbulent convection ensues and decreases the heat leaving the domain through the bottom boundary. In the case of no-slip boundary conditions, with the background field method, we prove that $\\mathcal{F}_B \\gtrsim R^{-2/3} - R^{-1/2}\\log{(1-R^{-2/3})}$ up to a positive constant independent of the Rayleigh and Prandtl numbers. Whereas between stress-free boundaries we prove, $\\mathcal{F}_B \\gtrsim R^{-40/29} - R^{-35/29}\\log{(1-R^{-40/29})}$. We perform a numerical study of the system in two dimensions up to a Rayleigh number of $5\\times10^9$ with the spectral solver Dedalus. The numerical investigations indicate that $\\mathcal{F}_B \\sim R^{-0.092} $ and $\\mathcal{F}_B \\sim R^{-0.12}$ for the two kinematic boundary conditions respectively. The gap between the bounds and simulations, and our constructions in the proofs highlight that there still exists room for optimisation of bounds for $\\mathcal{F}_B$.","sentences":["We prove new bounds on the heat flux out of the bottom boundary, $\\mathcal{F}_B$, for a fluid at infinite Prandtl number, heated internally between isothermal parallel plates under two kinematic boundary conditions.","In uniform internally heated convection, the supply of heat equally leaves the domain by conduction when there is no flow.","When the heating, quantified by the Rayleigh number, $R$, is sufficiently large, turbulent convection ensues and decreases the heat leaving the domain through the bottom boundary.","In the case of no-slip boundary conditions, with the background field method, we prove that $\\mathcal{F}_B \\gtrsim R^{-2/3} - R^{-1/2}\\log{(1-R^{-2/3})}$ up to a positive constant independent of the Rayleigh and Prandtl numbers.","Whereas between stress-free boundaries we prove,","$\\mathcal{F}_B \\gtrsim R^{-40/29} - R^{-35/29}\\log{(1-R^{-40/29})}$.","We perform a numerical study of the system in two dimensions up to a Rayleigh number of $5\\times10^9$ with the spectral solver Dedalus.","The numerical investigations indicate that $\\mathcal{F}_B \\sim R^{-0.092} $ and $\\mathcal{F}_B \\sim R^{-0.12}$ for the two kinematic boundary conditions respectively.","The gap between the bounds and simulations, and our constructions in the proofs highlight that there still exists room for optimisation of bounds for $\\mathcal{F}_B$."],"url":"http://arxiv.org/abs/2403.14407v1","category":"physics.flu-dyn"}
{"created":"2024-03-21 13:52:17","title":"XLAVS-R: Cross-Lingual Audio-Visual Speech Representation Learning for Noise-Robust Speech Perception","abstract":"Speech recognition and translation systems perform poorly on noisy inputs, which are frequent in realistic environments. Augmenting these systems with visual signals has the potential to improve robustness to noise. However, audio-visual (AV) data is only available in limited amounts and for fewer languages than audio-only resources. To address this gap, we present XLAVS-R, a cross-lingual audio-visual speech representation model for noise-robust speech recognition and translation in over 100 languages. It is designed to maximize the benefits of limited multilingual AV pre-training data, by building on top of audio-only multilingual pre-training and simplifying existing pre-training schemes. Extensive evaluation on the MuAViC benchmark shows the strength of XLAVS-R on downstream audio-visual speech recognition and translation tasks, where it outperforms the previous state of the art by up to 18.5% WER and 4.7 BLEU given noisy AV inputs, and enables strong zero-shot audio-visual ability with audio-only fine-tuning.","sentences":["Speech recognition and translation systems perform poorly on noisy inputs, which are frequent in realistic environments.","Augmenting these systems with visual signals has the potential to improve robustness to noise.","However, audio-visual (AV) data is only available in limited amounts and for fewer languages than audio-only resources.","To address this gap, we present XLAVS-R, a cross-lingual audio-visual speech representation model for noise-robust speech recognition and translation in over 100 languages.","It is designed to maximize the benefits of limited multilingual AV pre-training data, by building on top of audio-only multilingual pre-training and simplifying existing pre-training schemes.","Extensive evaluation on the MuAViC benchmark shows the strength of XLAVS-R on downstream audio-visual speech recognition and translation tasks, where it outperforms the previous state of the art by up to 18.5% WER and 4.7 BLEU given noisy AV inputs, and enables strong zero-shot audio-visual ability with audio-only fine-tuning."],"url":"http://arxiv.org/abs/2403.14402v1","category":"cs.SD"}
{"created":"2024-03-21 13:39:15","title":"Assimilation of SWOT Altimetry and Sentinel-1 Flood Extent Observations for Flood Reanalysis -- A Proof-of-Concept","abstract":"In spite of astonishing advances and developments in remote sensing technologies, meeting the spatio-temporal requirements for flood hydrodynamic modeling remains a great challenge for Earth Observation. The assimilation of multi-source remote sensing data in 2D hydrodynamic models participates to overcome such a challenge. The recently launched Surface Water and Ocean Topography (SWOT) wide-swath altimetry satellite provides a global coverage of water surface elevation at a high resolution. SWOT provides complementary observation to radar and optical images, increasing the opportunity to observe and monitor flood events. This research work focuses on the assimilation of 2D flood extent maps derived from Sentinel-1 C-SAR imagery data, and water surface elevation from SWOT as well as in-situ water level measurements. An Ensemble Kalman Filter (EnKF) with a joint state-parameter analysis is implemented on top of a 2D hydrodynamic TELEMAC-2D model to account for errors in roughness, input forcing and water depth in floodplain subdomains. The proposed strategy is carried out in an Observing System Simulation Experiment based on the 2021 flood event over the Garonne Marmandaise catchment. This work makes the most of the large volume of heterogeneous data from space for flood prediction in hindcast mode paves the way for nowcasting.","sentences":["In spite of astonishing advances and developments in remote sensing technologies, meeting the spatio-temporal requirements for flood hydrodynamic modeling remains a great challenge for Earth Observation.","The assimilation of multi-source remote sensing data in 2D hydrodynamic models participates to overcome such a challenge.","The recently launched Surface Water and Ocean Topography (SWOT) wide-swath altimetry satellite provides a global coverage of water surface elevation at a high resolution.","SWOT provides complementary observation to radar and optical images, increasing the opportunity to observe and monitor flood events.","This research work focuses on the assimilation of 2D flood extent maps derived from Sentinel-1 C-SAR imagery data, and water surface elevation from SWOT as well as in-situ water level measurements.","An Ensemble Kalman Filter (EnKF) with a joint state-parameter analysis is implemented on top of a 2D hydrodynamic TELEMAC-2D model to account for errors in roughness, input forcing and water depth in floodplain subdomains.","The proposed strategy is carried out in an Observing System Simulation Experiment based on the 2021 flood event over the Garonne Marmandaise catchment.","This work makes the most of the large volume of heterogeneous data from space for flood prediction in hindcast mode paves the way for nowcasting."],"url":"http://arxiv.org/abs/2403.14394v1","category":"eess.IV"}
{"created":"2024-03-21 13:24:24","title":"Exploiting Over-The-Air Consensus for Collision Avoidance and Formation Control in Multi-Agent Systems","abstract":"This paper introduces a distributed control method for multi-agent robotic systems employing Over the Air Consensus (OTA-Consensus). Designed for agents with decoupled single-integrator dynamics, this approach aims at efficient formation achievement and collision avoidance. As a distinctive feature, it leverages OTA's ability to exploit interference in wireless channels, a property traditionally considered a drawback, thus enhancing communication efficiency among robots. An analytical proof of asymptotic convergence is established for systems with time-varying communication topologies represented by sequences of strongly connected directed graphs. Comparative evaluations demonstrate significant efficiency improvements over current state-of-the-art methods, especially in scenarios with a large number of agents.","sentences":["This paper introduces a distributed control method for multi-agent robotic systems employing Over the Air Consensus (OTA-Consensus).","Designed for agents with decoupled single-integrator dynamics, this approach aims at efficient formation achievement and collision avoidance.","As a distinctive feature, it leverages OTA's ability to exploit interference in wireless channels, a property traditionally considered a drawback, thus enhancing communication efficiency among robots.","An analytical proof of asymptotic convergence is established for systems with time-varying communication topologies represented by sequences of strongly connected directed graphs.","Comparative evaluations demonstrate significant efficiency improvements over current state-of-the-art methods, especially in scenarios with a large number of agents."],"url":"http://arxiv.org/abs/2403.14386v1","category":"eess.SY"}
{"created":"2024-03-21 13:20:52","title":"Krylov localization as a probe for ergodicity breaking","abstract":"Krylov complexity has recently gained attention where the growth of operator complexity in time is measured in terms of the off-diagonal operator Lanczos coefficients. The operator Lanczos algorithm reduces the problem of complexity growth to a single-particle semi-infinite tight-binding chain (known as the Krylov chain). Employing the phenomenon of Anderson localization, we propose the inverse localization length on the Krylov chain as a probe to detect weak ergodicity-breaking. On the Krylov chain we find delocalization in an ergodic regime, as we show for the SYK model, and localization in case of a weakly ergodicity-broken regime. Considering the dynamics beyond scrambling, we find a collapse across different system sizes at the point of weak ergodicity-breaking leading to a quantitative prediction. We further show universal traits of different operators in the ergodic regime beyond the scrambling dynamics. We test for two settings: (1) the coupled SYK model, and (2) the quantum East model. Our findings open avenues for mapping ergodicity/weak ergodicity-breaking transitions to delocalization/localization phenomenology on the Krylov chain.","sentences":["Krylov complexity has recently gained attention where the growth of operator complexity in time is measured in terms of the off-diagonal operator Lanczos coefficients.","The operator Lanczos algorithm reduces the problem of complexity growth to a single-particle semi-infinite tight-binding chain (known as the Krylov chain).","Employing the phenomenon of Anderson localization, we propose the inverse localization length on the Krylov chain as a probe to detect weak ergodicity-breaking.","On the Krylov chain we find delocalization in an ergodic regime, as we show for the SYK model, and localization in case of a weakly ergodicity-broken regime.","Considering the dynamics beyond scrambling, we find a collapse across different system sizes at the point of weak ergodicity-breaking leading to a quantitative prediction.","We further show universal traits of different operators in the ergodic regime beyond the scrambling dynamics.","We test for two settings: (1) the coupled SYK model, and (2) the quantum East model.","Our findings open avenues for mapping ergodicity/weak ergodicity-breaking transitions to delocalization/localization phenomenology on the Krylov chain."],"url":"http://arxiv.org/abs/2403.14384v1","category":"quant-ph"}
{"created":"2024-03-21 13:16:36","title":"Space-time quasicrystals in Bose-Einstein condensates","abstract":"An autoresonant approach for exciting space-time quasicrystals in Bose-Einstein condensates is proposed by employing two-component chirped frequency parametric driving or modulation of the interaction strength within Gross-Pitaevskii equation. A weakly nonlinear theory of the process is developed using Whitham's averaged variational principle yielding reduction to a two-degrees-of-freedom dynamical system in action-angle variables. Additionally, the theory also delineates permissible driving parameters and establishes thresholds on the driving amplitudes required for autoresonant excitation.","sentences":["An autoresonant approach for exciting space-time quasicrystals in Bose-Einstein condensates is proposed by employing two-component chirped frequency parametric driving or modulation of the interaction strength within Gross-Pitaevskii equation.","A weakly nonlinear theory of the process is developed using Whitham's averaged variational principle yielding reduction to a two-degrees-of-freedom dynamical system in action-angle variables.","Additionally, the theory also delineates permissible driving parameters and establishes thresholds on the driving amplitudes required for autoresonant excitation."],"url":"http://arxiv.org/abs/2403.14382v1","category":"nlin.PS"}
{"created":"2024-03-21 13:06:57","title":"InfNeRF: Towards Infinite Scale NeRF Rendering with O(log n) Space Complexity","abstract":"The conventional mesh-based Level of Detail (LoD) technique, exemplified by applications such as Google Earth and many game engines, exhibits the capability to holistically represent a large scene even the Earth, and achieves rendering with a space complexity of O(log n). This constrained data requirement not only enhances rendering efficiency but also facilitates dynamic data fetching, thereby enabling a seamless 3D navigation experience for users. In this work, we extend this proven LoD technique to Neural Radiance Fields (NeRF) by introducing an octree structure to represent the scenes in different scales. This innovative approach provides a mathematically simple and elegant representation with a rendering space complexity of O(log n), aligned with the efficiency of mesh-based LoD techniques. We also present a novel training strategy that maintains a complexity of O(n). This strategy allows for parallel training with minimal overhead, ensuring the scalability and efficiency of our proposed method. Our contribution is not only in extending the capabilities of existing techniques but also in establishing a foundation for scalable and efficient large-scale scene representation using NeRF and octree structures.","sentences":["The conventional mesh-based Level of Detail (LoD) technique, exemplified by applications such as Google Earth and many game engines, exhibits the capability to holistically represent a large scene even the Earth, and achieves rendering with a space complexity of O(log n).","This constrained data requirement not only enhances rendering efficiency but also facilitates dynamic data fetching, thereby enabling a seamless 3D navigation experience for users.","In this work, we extend this proven LoD technique to Neural Radiance Fields (NeRF) by introducing an octree structure to represent the scenes in different scales.","This innovative approach provides a mathematically simple and elegant representation with a rendering space complexity of O(log n), aligned with the efficiency of mesh-based LoD techniques.","We also present a novel training strategy that maintains a complexity of O(n).","This strategy allows for parallel training with minimal overhead, ensuring the scalability and efficiency of our proposed method.","Our contribution is not only in extending the capabilities of existing techniques but also in establishing a foundation for scalable and efficient large-scale scene representation using NeRF and octree structures."],"url":"http://arxiv.org/abs/2403.14376v1","category":"cs.CV"}
{"created":"2024-03-21 13:05:16","title":"A new control-oriented METANET model to encompass service stations on highways","abstract":"In this paper, we propose the METANET with service station (METANET-s) model, a second-order macroscopic traffic model that, compared to the classical METANET, incorporates the dynamics of service stations on highways. Specifically, we employ the (so-called) store-and-forward links to model the stop of vehicles and the possible queue forming in the process of merging back into the highway mainstream. We explore the capability of the METANET-s to capture well both traffic back propagation and capacity drops, which are typically caused by the presence of vehicles joining again the mainstream traffic from the service station. Therefore, capturing these effects is crucial to improving the model's predictive capabilities. Finally, we perform a comparative analysis with the Cell Transmission Model with service station (CTM-s), showcasing that the METANET-s describes the traffic evolution much better than its first-order counterpart.","sentences":["In this paper, we propose the METANET with service station (METANET-s) model, a second-order macroscopic traffic model that, compared to the classical METANET, incorporates the dynamics of service stations on highways.","Specifically, we employ the (so-called) store-and-forward links to model the stop of vehicles and the possible queue forming in the process of merging back into the highway mainstream.","We explore the capability of the METANET-s to capture well both traffic back propagation and capacity drops, which are typically caused by the presence of vehicles joining again the mainstream traffic from the service station.","Therefore, capturing these effects is crucial to improving the model's predictive capabilities.","Finally, we perform a comparative analysis with the Cell Transmission Model with service station (CTM-s), showcasing that the METANET-s describes the traffic evolution much better than its first-order counterpart."],"url":"http://arxiv.org/abs/2403.14373v1","category":"eess.SY"}
{"created":"2024-03-21 13:02:14","title":"A Benchmark for the Application of Distributed Control Techniques to the Electricity Network of the European Economic Area","abstract":"The European Economic Area Electricity Network Benchmark (EEA-ENB) is a multi-area power system representing the European network of transmission systems for electricity to facilitate the application of distributed control techniques. In the EEA-ENB we consider the Load Frequency Control (LFC) problem in the presence of renewable energy sources (RESs), and energy storage systems (ESSs). RESs are known to cause instability in power networks due to their inertia-less and intermittent characteristics, while ESSs are introduced as a resource to mitigate the problem. In the EEA-ENB, particular attention is dedicated to Distributed Model Predictive Control (DMPC), whose application is often limited to small and homogeneous test cases due to the lack of standardized large-scale scenarios for testing, and due to the large computation time required to obtain a centralized MPC action for performance comparison with DMPC strategies under consideration. The second problem is exacerbated when the scale of the system grows. To address these challenges and to provide a real-world-based and control-independent benchmark, the EEA-ENB has been developed. The benchmark includes a centralized MPC strategy providing performance and computation time metrics to compare distributed control within a repeatable and realistic simulation environment.","sentences":["The European Economic Area Electricity Network Benchmark (EEA-ENB) is a multi-area power system representing the European network of transmission systems for electricity to facilitate the application of distributed control techniques.","In the EEA-ENB we consider the Load Frequency Control (LFC) problem in the presence of renewable energy sources (RESs), and energy storage systems (ESSs).","RESs are known to cause instability in power networks due to their inertia-less and intermittent characteristics, while ESSs are introduced as a resource to mitigate the problem.","In the EEA-ENB, particular attention is dedicated to Distributed Model Predictive Control (DMPC), whose application is often limited to small and homogeneous test cases due to the lack of standardized large-scale scenarios for testing, and due to the large computation time required to obtain a centralized MPC action for performance comparison with DMPC strategies under consideration.","The second problem is exacerbated when the scale of the system grows.","To address these challenges and to provide a real-world-based and control-independent benchmark, the EEA-ENB has been developed.","The benchmark includes a centralized MPC strategy providing performance and computation time metrics to compare distributed control within a repeatable and realistic simulation environment."],"url":"http://arxiv.org/abs/2403.14372v1","category":"eess.SY"}
{"created":"2024-03-21 12:56:11","title":"A Control Barrier Function Composition Approach for Multi-Agent Systems in Marine Applications","abstract":"The agents within a multi-agent system (MAS) operating in marine environments often need to utilize task payloads and avoid collisions in coordination, necessitating adherence to a set of relative-pose constraints, which may include field-of-view, line-of-sight, collision-avoidance, and range constraints. A nominal controller designed for reference tracking may not guarantee the marine MAS stays safe w.r.t. these constraints. To modify the nominal input as one that enforces safety, we introduce a framework to systematically encode the relative-pose constraints as nonsmooth control barrier functions (NCBFs) and combine them as a single NCBF using Boolean composition, which enables a simplified verification process compared to using the NCBFs individually. While other relative-pose constraint functions have explicit derivatives, the challenging line-of-sight constraint is encoded with the minimum distance function between the line-of-sight set and other agents, whose derivative is not explicit. Hence, existing safe control design methods that consider composite NCBFs cannot be applied. To address this challenge, we propose a novel quadratic program formulation based on the dual of the minimum distance problem and develop a new theory to ensure the resulting control input guarantees constraint satisfaction. Lastly, we validate the effectiveness of our proposed framework on a simulated large-scale marine MAS and a real-world marine MAS comprising one Unmanned Surface Vehicle and two Unmanned Underwater Vehicles.","sentences":["The agents within a multi-agent system (MAS) operating in marine environments often need to utilize task payloads and avoid collisions in coordination, necessitating adherence to a set of relative-pose constraints, which may include field-of-view, line-of-sight, collision-avoidance, and range constraints.","A nominal controller designed for reference tracking may not guarantee the marine MAS stays safe w.r.t.","these constraints.","To modify the nominal input as one that enforces safety, we introduce a framework to systematically encode the relative-pose constraints as nonsmooth control barrier functions (NCBFs) and combine them as a single NCBF using Boolean composition, which enables a simplified verification process compared to using the NCBFs individually.","While other relative-pose constraint functions have explicit derivatives, the challenging line-of-sight constraint is encoded with the minimum distance function between the line-of-sight set and other agents, whose derivative is not explicit.","Hence, existing safe control design methods that consider composite NCBFs cannot be applied.","To address this challenge, we propose a novel quadratic program formulation based on the dual of the minimum distance problem and develop a new theory to ensure the resulting control input guarantees constraint satisfaction.","Lastly, we validate the effectiveness of our proposed framework on a simulated large-scale marine MAS and a real-world marine MAS comprising one Unmanned Surface Vehicle and two Unmanned Underwater Vehicles."],"url":"http://arxiv.org/abs/2403.14369v1","category":"eess.SY"}
{"created":"2024-03-21 12:49:32","title":"SurroundSDF: Implicit 3D Scene Understanding Based on Signed Distance Field","abstract":"Vision-centric 3D environment understanding is both vital and challenging for autonomous driving systems. Recently, object-free methods have attracted considerable attention. Such methods perceive the world by predicting the semantics of discrete voxel grids but fail to construct continuous and accurate obstacle surfaces. To this end, in this paper, we propose SurroundSDF to implicitly predict the signed distance field (SDF) and semantic field for the continuous perception from surround images. Specifically, we introduce a query-based approach and utilize SDF constrained by the Eikonal formulation to accurately describe the surfaces of obstacles. Furthermore, considering the absence of precise SDF ground truth, we propose a novel weakly supervised paradigm for SDF, referred to as the Sandwich Eikonal formulation, which emphasizes applying correct and dense constraints on both sides of the surface, thereby enhancing the perceptual accuracy of the surface. Experiments suggest that our method achieves SOTA for both occupancy prediction and 3D scene reconstruction tasks on the nuScenes dataset.","sentences":["Vision-centric 3D environment understanding is both vital and challenging for autonomous driving systems.","Recently, object-free methods have attracted considerable attention.","Such methods perceive the world by predicting the semantics of discrete voxel grids but fail to construct continuous and accurate obstacle surfaces.","To this end, in this paper, we propose SurroundSDF to implicitly predict the signed distance field (SDF) and semantic field for the continuous perception from surround images.","Specifically, we introduce a query-based approach and utilize SDF constrained by the Eikonal formulation to accurately describe the surfaces of obstacles.","Furthermore, considering the absence of precise SDF ground truth, we propose a novel weakly supervised paradigm for SDF, referred to as the Sandwich Eikonal formulation, which emphasizes applying correct and dense constraints on both sides of the surface, thereby enhancing the perceptual accuracy of the surface.","Experiments suggest that our method achieves SOTA for both occupancy prediction and 3D scene reconstruction tasks on the nuScenes dataset."],"url":"http://arxiv.org/abs/2403.14366v1","category":"cs.CV"}
{"created":"2024-03-21 12:29:26","title":"LDTR: Transformer-based Lane Detection with Anchor-chain Representation","abstract":"Despite recent advances in lane detection methods, scenarios with limited- or no-visual-clue of lanes due to factors such as lighting conditions and occlusion remain challenging and crucial for automated driving. Moreover, current lane representations require complex post-processing and struggle with specific instances. Inspired by the DETR architecture, we propose LDTR, a transformer-based model to address these issues. Lanes are modeled with a novel anchor-chain, regarding a lane as a whole from the beginning, which enables LDTR to handle special lanes inherently. To enhance lane instance perception, LDTR incorporates a novel multi-referenced deformable attention module to distribute attention around the object. Additionally, LDTR incorporates two line IoU algorithms to improve convergence efficiency and employs a Gaussian heatmap auxiliary branch to enhance model representation capability during training. To evaluate lane detection models, we rely on Frechet distance, parameterized F1-score, and additional synthetic metrics. Experimental results demonstrate that LDTR achieves state-of-the-art performance on well-known datasets.","sentences":["Despite recent advances in lane detection methods, scenarios with limited- or no-visual-clue of lanes due to factors such as lighting conditions and occlusion remain challenging and crucial for automated driving.","Moreover, current lane representations require complex post-processing and struggle with specific instances.","Inspired by the DETR architecture, we propose LDTR, a transformer-based model to address these issues.","Lanes are modeled with a novel anchor-chain, regarding a lane as a whole from the beginning, which enables LDTR to handle special lanes inherently.","To enhance lane instance perception, LDTR incorporates a novel multi-referenced deformable attention module to distribute attention around the object.","Additionally, LDTR incorporates two line IoU algorithms to improve convergence efficiency and employs a Gaussian heatmap auxiliary branch to enhance model representation capability during training.","To evaluate lane detection models, we rely on Frechet distance, parameterized F1-score, and additional synthetic metrics.","Experimental results demonstrate that LDTR achieves state-of-the-art performance on well-known datasets."],"url":"http://arxiv.org/abs/2403.14354v1","category":"cs.CV"}
{"created":"2024-03-21 12:28:44","title":"DaCapo: Accelerating Continuous Learning in Autonomous Systems for Video Analytics","abstract":"Deep neural network (DNN) video analytics is crucial for autonomous systems such as self-driving vehicles, unmanned aerial vehicles (UAVs), and security robots. However, real-world deployment faces challenges due to their limited computational resources and battery power. To tackle these challenges, continuous learning exploits a lightweight \"student\" model at deployment (inference), leverages a larger \"teacher\" model for labeling sampled data (labeling), and continuously retrains the student model to adapt to changing scenarios (retraining). This paper highlights the limitations in state-of-the-art continuous learning systems: (1) they focus on computations for retraining, while overlooking the compute needs for inference and labeling, (2) they rely on power-hungry GPUs, unsuitable for battery-operated autonomous systems, and (3) they are located on a remote centralized server, intended for multi-tenant scenarios, again unsuitable for autonomous systems due to privacy, network availability, and latency concerns. We propose a hardware-algorithm co-designed solution for continuous learning, DaCapo, that enables autonomous systems to perform concurrent executions of inference, labeling, and training in a performant and energy-efficient manner. DaCapo comprises (1) a spatially-partitionable and precision-flexible accelerator enabling parallel execution of kernels on sub-accelerators at their respective precisions, and (2) a spatiotemporal resource allocation algorithm that strategically navigates the resource-accuracy tradeoff space, facilitating optimal decisions for resource allocation to achieve maximal accuracy. Our evaluation shows that DaCapo achieves 6.5% and 5.5% higher accuracy than a state-of-the-art GPU-based continuous learning systems, Ekya and EOMU, respectively, while consuming 254x less power.","sentences":["Deep neural network (DNN) video analytics is crucial for autonomous systems such as self-driving vehicles, unmanned aerial vehicles (UAVs), and security robots.","However, real-world deployment faces challenges due to their limited computational resources and battery power.","To tackle these challenges, continuous learning exploits a lightweight \"student\" model at deployment (inference), leverages a larger \"teacher\" model for labeling sampled data (labeling), and continuously retrains the student model to adapt to changing scenarios (retraining).","This paper highlights the limitations in state-of-the-art continuous learning systems: (1) they focus on computations for retraining, while overlooking the compute needs for inference and labeling, (2) they rely on power-hungry GPUs, unsuitable for battery-operated autonomous systems, and (3) they are located on a remote centralized server, intended for multi-tenant scenarios, again unsuitable for autonomous systems due to privacy, network availability, and latency concerns.","We propose a hardware-algorithm co-designed solution for continuous learning, DaCapo, that enables autonomous systems to perform concurrent executions of inference, labeling, and training in a performant and energy-efficient manner.","DaCapo comprises (1) a spatially-partitionable and precision-flexible accelerator enabling parallel execution of kernels on sub-accelerators at their respective precisions, and (2) a spatiotemporal resource allocation algorithm that strategically navigates the resource-accuracy tradeoff space, facilitating optimal decisions for resource allocation to achieve maximal accuracy.","Our evaluation shows that DaCapo achieves 6.5% and 5.5% higher accuracy than a state-of-the-art GPU-based continuous learning systems, Ekya and EOMU, respectively, while consuming 254x less power."],"url":"http://arxiv.org/abs/2403.14353v1","category":"cs.AR"}
{"created":"2024-03-21 12:28:24","title":"Accelerating Time-to-Science by Streaming Detector Data Directly into Perlmutter Compute Nodes","abstract":"Recent advancements in detector technology have significantly increased the size and complexity of experimental data, and high-performance computing (HPC) provides a path towards more efficient and timely data processing. However, movement of large data sets from acquisition systems to HPC centers introduces bottlenecks owing to storage I/O at both ends. This manuscript introduces a streaming workflow designed for an high data rate electron detector that streams data directly to compute node memory at the National Energy Research Scientific Computing Center (NERSC), thereby avoiding storage I/O. The new workflow deploys ZeroMQ-based services for data production, aggregation, and distribution for on-the-fly processing, all coordinated through a distributed key-value store. The system is integrated with the detector's science gateway and utilizes the NERSC Superfacility API to initiate streaming jobs through a web-based frontend. Our approach achieves up to a 14-fold increase in data throughput and enhances predictability and reliability compared to a I/O-heavy file-based transfer workflow. Our work highlights the transformative potential of streaming workflows to expedite data analysis for time-sensitive experiments.","sentences":["Recent advancements in detector technology have significantly increased the size and complexity of experimental data, and high-performance computing (HPC) provides a path towards more efficient and timely data processing.","However, movement of large data sets from acquisition systems to HPC centers introduces bottlenecks owing to storage I/O at both ends.","This manuscript introduces a streaming workflow designed for an high data rate electron detector that streams data directly to compute node memory at the National Energy Research Scientific Computing Center (NERSC), thereby avoiding storage I/O.","The new workflow deploys ZeroMQ-based services for data production, aggregation, and distribution for on-the-fly processing, all coordinated through a distributed key-value store.","The system is integrated with the detector's science gateway and utilizes the NERSC Superfacility API to initiate streaming jobs through a web-based frontend.","Our approach achieves up to a 14-fold increase in data throughput and enhances predictability and reliability compared to a I/O-heavy file-based transfer workflow.","Our work highlights the transformative potential of streaming workflows to expedite data analysis for time-sensitive experiments."],"url":"http://arxiv.org/abs/2403.14352v1","category":"cs.NI"}
{"created":"2024-03-21 12:20:36","title":"Adversary-Augmented Simulation to evaluate client-fairness on HyperLedger Fabric","abstract":"This paper presents a novel adversary model specifically tailored to distributed systems, with the aim to asses the security of blockchain technologies. Building upon literature on adversarial assumptions and capabilities, we include classical notions of failure and communication models to classify and bind the use of adversarial actions. We focus on the effect of these actions on properties of distributed protocols. A significant effort of our research is the integration of this model into the Multi-Agent eXperimenter (MAX) framework. This integration enables realistic simulations of adversarial attacks on blockchain systems. In particular, we have simulated attacks violating a form of client-fairness on HyperLedger Fabric.","sentences":["This paper presents a novel adversary model specifically tailored to distributed systems, with the aim to asses the security of blockchain technologies.","Building upon literature on adversarial assumptions and capabilities, we include classical notions of failure and communication models to classify and bind the use of adversarial actions.","We focus on the effect of these actions on properties of distributed protocols.","A significant effort of our research is the integration of this model into the Multi-Agent eXperimenter (MAX) framework.","This integration enables realistic simulations of adversarial attacks on blockchain systems.","In particular, we have simulated attacks violating a form of client-fairness on HyperLedger Fabric."],"url":"http://arxiv.org/abs/2403.14342v1","category":"cs.CR"}
{"created":"2024-03-21 12:06:30","title":"Optimal Second-Order Rates for Quantum Information Decoupling","abstract":"In this paper, we consider the standard quantum information decoupling, in which Alice aims to decouple her system from the environment by local operations and discarding some of her systems. To achieve an $\\varepsilon$-decoupling with trace distance as the error criterion, we establish a near-optimal one-shot characterization for the largest dimension of the remainder system in terms of the conditional $(1-\\varepsilon)$-hypothesis-testing entropy. When the underlying system is independent and identically prepared, our result leads to the matched second-order rate as well as the matched moderate deviation rate. As an application, we find an achievability bound in entanglement distillation protocol, where the objective is for Alice and Bob to transform their quantum state to maximally entangled state with largest possible dimension using only local operations and one-way classical communications.","sentences":["In this paper, we consider the standard quantum information decoupling, in which Alice aims to decouple her system from the environment by local operations and discarding some of her systems.","To achieve an $\\varepsilon$-decoupling with trace distance as the error criterion, we establish a near-optimal one-shot characterization for the largest dimension of the remainder system in terms of the conditional $(1-\\varepsilon)$-hypothesis-testing entropy.","When the underlying system is independent and identically prepared, our result leads to the matched second-order rate as well as the matched moderate deviation rate.","As an application, we find an achievability bound in entanglement distillation protocol, where the objective is for Alice and Bob to transform their quantum state to maximally entangled state with largest possible dimension using only local operations and one-way classical communications."],"url":"http://arxiv.org/abs/2403.14338v1","category":"quant-ph"}
{"created":"2024-03-21 11:55:28","title":"Singularities and growth of higher order discrete equations","abstract":"We study the link between the degree growth of integrable birational mappings of order higher than two and their singularity structures. The higher order mappings we use in this study are all obtained by coupling mappings that are integrable through spectral methods, typically belonging to the QRT family, to a variety of linearisable ones. We show that by judiciously choosing these linearisable mappings, it is possible to obtain higher order mappings that exhibit the maximal degree growth compatible with integrability, i.e. for which the degree grows as a polynomial of order equal to the order of the mapping. In all the cases we analysed, we found that maximal degree growth was associated with the existence of an unconfining singularity pattern. Several cases with submaximal growth but which still possess unconfining singularity patterns are also presented. In many cases the exact degrees of the iterates of the mappings were obtained by applying a method due to Halburd, based on the preimages of specific values that appear in the singularity patterns of the mapping, but we also present some examples where such a calculation appears to be impossible.","sentences":["We study the link between the degree growth of integrable birational mappings of order higher than two and their singularity structures.","The higher order mappings we use in this study are all obtained by coupling mappings that are integrable through spectral methods, typically belonging to the QRT family, to a variety of linearisable ones.","We show that by judiciously choosing these linearisable mappings, it is possible to obtain higher order mappings that exhibit the maximal degree growth compatible with integrability, i.e. for which the degree grows as a polynomial of order equal to the order of the mapping.","In all the cases we analysed, we found that maximal degree growth was associated with the existence of an unconfining singularity pattern.","Several cases with submaximal growth but which still possess unconfining singularity patterns are also presented.","In many cases the exact degrees of the iterates of the mappings were obtained by applying a method due to Halburd, based on the preimages of specific values that appear in the singularity patterns of the mapping, but we also present some examples where such a calculation appears to be impossible."],"url":"http://arxiv.org/abs/2403.14329v1","category":"nlin.SI"}
{"created":"2024-03-21 11:51:42","title":"Investigating the validity of structure learning algorithms in identifying risk factors for intervention in patients with diabetes","abstract":"Diabetes, a pervasive and enduring health challenge, imposes significant global implications on health, financial healthcare systems, and societal well-being. This study undertakes a comprehensive exploration of various structural learning algorithms to discern causal pathways amongst potential risk factors influencing diabetes progression. The methodology involves the application of these algorithms to relevant diabetes data, followed by the conversion of their output graphs into Causal Bayesian Networks (CBNs), enabling predictive analysis and the evaluation of discrepancies in the effect of hypothetical interventions within our context-specific case study.   This study highlights the substantial impact of algorithm selection on intervention outcomes. To consolidate insights from diverse algorithms, we employ a model-averaging technique that helps us obtain a unique causal model for diabetes derived from a varied set of structural learning algorithms. We also investigate how each of those individual graphs, as well as the average graph, compare to the structures elicited by a domain expert who categorised graph edges into high confidence, moderate, and low confidence types, leading into three individual graphs corresponding to the three levels of confidence.   The resulting causal model and data are made available online, and serve as a valuable resource and a guide for informed decision-making by healthcare practitioners, offering a comprehensive understanding of the interactions between relevant risk factors and the effect of hypothetical interventions. Therefore, this research not only contributes to the academic discussion on diabetes, but also provides practical guidance for healthcare professionals in developing efficient intervention and risk management strategies.","sentences":["Diabetes, a pervasive and enduring health challenge, imposes significant global implications on health, financial healthcare systems, and societal well-being.","This study undertakes a comprehensive exploration of various structural learning algorithms to discern causal pathways amongst potential risk factors influencing diabetes progression.","The methodology involves the application of these algorithms to relevant diabetes data, followed by the conversion of their output graphs into Causal Bayesian Networks (CBNs), enabling predictive analysis and the evaluation of discrepancies in the effect of hypothetical interventions within our context-specific case study.   ","This study highlights the substantial impact of algorithm selection on intervention outcomes.","To consolidate insights from diverse algorithms, we employ a model-averaging technique that helps us obtain a unique causal model for diabetes derived from a varied set of structural learning algorithms.","We also investigate how each of those individual graphs, as well as the average graph, compare to the structures elicited by a domain expert who categorised graph edges into high confidence, moderate, and low confidence types, leading into three individual graphs corresponding to the three levels of confidence.   ","The resulting causal model and data are made available online, and serve as a valuable resource and a guide for informed decision-making by healthcare practitioners, offering a comprehensive understanding of the interactions between relevant risk factors and the effect of hypothetical interventions.","Therefore, this research not only contributes to the academic discussion on diabetes, but also provides practical guidance for healthcare professionals in developing efficient intervention and risk management strategies."],"url":"http://arxiv.org/abs/2403.14327v1","category":"cs.LG"}
{"created":"2024-03-21 11:44:25","title":"Neural Network-Based Processing and Reconstruction of Compromised Biophotonic Image Data","abstract":"The integration of deep learning techniques with biophotonic setups has opened new horizons in bioimaging. A compelling trend in this field involves deliberately compromising certain measurement metrics to engineer better bioimaging tools in terms of cost, speed, and form-factor, followed by compensating for the resulting defects through the utilization of deep learning models trained on a large amount of ideal, superior or alternative data. This strategic approach has found increasing popularity due to its potential to enhance various aspects of biophotonic imaging. One of the primary motivations for employing this strategy is the pursuit of higher temporal resolution or increased imaging speed, critical for capturing fine dynamic biological processes. This approach also offers the prospect of simplifying hardware requirements/complexities, thereby making advanced imaging standards more accessible in terms of cost and/or size. This article provides an in-depth review of the diverse measurement aspects that researchers intentionally impair in their biophotonic setups, including the point spread function, signal-to-noise ratio, sampling density, and pixel resolution. By deliberately compromising these metrics, researchers aim to not only recuperate them through the application of deep learning networks, but also bolster in return other crucial parameters, such as the field-of-view, depth-of-field, and space-bandwidth product. Here, we discuss various biophotonic methods that have successfully employed this strategic approach. These techniques span broad applications and showcase the versatility and effectiveness of deep learning in the context of compromised biophotonic data. Finally, by offering our perspectives on the future possibilities of this rapidly evolving concept, we hope to motivate our readers to explore novel ways of balancing hardware compromises with compensation via AI.","sentences":["The integration of deep learning techniques with biophotonic setups has opened new horizons in bioimaging.","A compelling trend in this field involves deliberately compromising certain measurement metrics to engineer better bioimaging tools in terms of cost, speed, and form-factor, followed by compensating for the resulting defects through the utilization of deep learning models trained on a large amount of ideal, superior or alternative data.","This strategic approach has found increasing popularity due to its potential to enhance various aspects of biophotonic imaging.","One of the primary motivations for employing this strategy is the pursuit of higher temporal resolution or increased imaging speed, critical for capturing fine dynamic biological processes.","This approach also offers the prospect of simplifying hardware requirements/complexities, thereby making advanced imaging standards more accessible in terms of cost and/or size.","This article provides an in-depth review of the diverse measurement aspects that researchers intentionally impair in their biophotonic setups, including the point spread function, signal-to-noise ratio, sampling density, and pixel resolution.","By deliberately compromising these metrics, researchers aim to not only recuperate them through the application of deep learning networks, but also bolster in return other crucial parameters, such as the field-of-view, depth-of-field, and space-bandwidth product.","Here, we discuss various biophotonic methods that have successfully employed this strategic approach.","These techniques span broad applications and showcase the versatility and effectiveness of deep learning in the context of compromised biophotonic data.","Finally, by offering our perspectives on the future possibilities of this rapidly evolving concept, we hope to motivate our readers to explore novel ways of balancing hardware compromises with compensation via AI."],"url":"http://arxiv.org/abs/2403.14324v1","category":"physics.optics"}
{"created":"2024-03-21 11:41:02","title":"Integrable geodesic flows with simultaneously diagonalisable quadratic integrals","abstract":"We show that if $n$ functionally independent commutative quadratic in momenta integrals for the geodesic flow of a Riemannian or pseudo-Riemannian metric on an $n$-dimensional manifold are simultaneously diagonalisable at the tangent space to every point, then they come from the St\\\"ackel construction, so the metric admits orthogonal separation of variables.","sentences":["We show that if $n$ functionally independent commutative quadratic in momenta integrals for the geodesic flow of a Riemannian or pseudo-Riemannian metric on an $n$-dimensional manifold are simultaneously diagonalisable at the tangent space to every point, then they come from the St\\\"ackel construction, so the metric admits orthogonal separation of variables."],"url":"http://arxiv.org/abs/2403.14319v1","category":"math.DG"}
{"created":"2024-03-21 11:36:13","title":"Subspace restricted thermalization in a correlated-hopping model with strong Hilbert space fragmentation characterized by irreducible strings","abstract":"We introduce a one-dimensional correlated-hopping model of spinless fermions in which a particle can hop between two neighboring sites only if the sites to the left and right of those two sites have different particle numbers. Using a bond to site mapping, this model involving four-site terms can be mapped to an assisted pair-flipping model involving only three-site terms. This model shows strong Hilbert space fragmentation (HSF). We define irreducible strings (IS) to label the different fragments, determine the number of fragments, and the sizes of fragments corresponding to some special IS. In some classes of fragments, the Hamiltonian can be diagonalized completely, and in others it can be seen to have a structure characteristic of models which are not fully integrable. In the largest fragment in our model, the number of states grows exponentially with the system size, but the ratio of this number to the total Hilbert space dimension tends to zero exponentially in the thermodynamic limit. Within this fragment, we provide numerical evidence that only a weaker version of eigenstate thermalization hypothesis (ETH) remains valid; we call this the subspace-restricted ETH. This is a modification of the usual ETH which combines the strong and weak versions of ETH and is also applicable to fragments of all dimensions. To understand the out-of-equilibrium dynamics of the model, we study the infinite-temperature time-dependent autocorrelation functions starting from a random initial state; we find that these exhibit a different behavior near the boundary compared to the bulk. We finally propose an experimental setup to realize our correlated-hopping model.","sentences":["We introduce a one-dimensional correlated-hopping model of spinless fermions in which a particle can hop between two neighboring sites only if the sites to the left and right of those two sites have different particle numbers.","Using a bond to site mapping, this model involving four-site terms can be mapped to an assisted pair-flipping model involving only three-site terms.","This model shows strong Hilbert space fragmentation (HSF).","We define irreducible strings (IS) to label the different fragments, determine the number of fragments, and the sizes of fragments corresponding to some special IS.","In some classes of fragments, the Hamiltonian can be diagonalized completely, and in others it can be seen to have a structure characteristic of models which are not fully integrable.","In the largest fragment in our model, the number of states grows exponentially with the system size, but the ratio of this number to the total Hilbert space dimension tends to zero exponentially in the thermodynamic limit.","Within this fragment, we provide numerical evidence that only a weaker version of eigenstate thermalization hypothesis (ETH) remains valid; we call this the subspace-restricted ETH.","This is a modification of the usual ETH which combines the strong and weak versions of ETH and is also applicable to fragments of all dimensions.","To understand the out-of-equilibrium dynamics of the model, we study the infinite-temperature time-dependent autocorrelation functions starting from a random initial state; we find that these exhibit a different behavior near the boundary compared to the bulk.","We finally propose an experimental setup to realize our correlated-hopping model."],"url":"http://arxiv.org/abs/2403.14314v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-21 11:35:13","title":"Gaia23ckh: Symbiotic outburst of the assumed Mira variable V390 Sco","abstract":"The poorly studied variable star V390 Sco, previously classified as a Mira pulsator, was detected in a brightening event by the ESA Gaia satellite in September 2023. This work presents an analysis of available archival multifrequency photometric data of this target, along with our spectroscopic observations. Our findings lead to the conclusion that V390 Sco is a new symbiotic star identified by Gaia, currently undergoing a classical symbiotic outburst. Additionally, we uncovered three prior outbursts of this system through archival photometry. The outbursts recur approximately every 2330 - 2400 days, and we hypothesize the periastron passage in an eccentric orbit may trigger them, similarly to the case of BX Mon, DD Mic, or MWC 560. A detailed investigation into the nature of the donor star suggested that V390 Sco is an S-type symbiotic star, likely hosting a less evolved, semiregularly pulsating giant donor, but not a Mira variable.","sentences":["The poorly studied variable star V390 Sco, previously classified as a Mira pulsator, was detected in a brightening event by the ESA Gaia satellite in September 2023.","This work presents an analysis of available archival multifrequency photometric data of this target, along with our spectroscopic observations.","Our findings lead to the conclusion that V390 Sco is a new symbiotic star identified by Gaia, currently undergoing a classical symbiotic outburst.","Additionally, we uncovered three prior outbursts of this system through archival photometry.","The outbursts recur approximately every 2330 - 2400 days, and we hypothesize the periastron passage in an eccentric orbit may trigger them, similarly to the case of BX Mon, DD Mic, or MWC 560.","A detailed investigation into the nature of the donor star suggested that V390 Sco is an S-type symbiotic star, likely hosting a less evolved, semiregularly pulsating giant donor, but not a Mira variable."],"url":"http://arxiv.org/abs/2403.14313v1","category":"astro-ph.SR"}
{"created":"2024-03-21 11:16:31","title":"Picotesla-sensitivity microcavity optomechanical magnetometry","abstract":"Cavity optomechanical systems have enabled precision sensing of magnetic fields, by leveraging the optical resonance-enhanced readout and mechanical resonance-enhanced response. Previous studies have successfully achieved scalable and reproducible microcavity optomechanical magnetometry (MCOM) by incorporating Terfenol-D thin films into high-quality ($Q$) factor whispering gallery mode (WGM) microcavities. However, the sensitivity was limited to 585 pT/Hz$^{1/2}$, over 20 times inferior to those using Terfenol-D particles. In this work, we propose and demonstrate a high-sensitivity and scalable MCOM approach by sputtering a FeGaB thin film onto a high-$Q$ SiO$_2$ WGM microdisk. Theoretical studies are conducted to explore the magnetic actuation constant and noise-limited sensitivity by varying the parameters of the FeGaB film and SiO$_2$ microdisk. Multiple magnetometers with different radii are fabricated and characterized. By utilizing a microdisk with a radius of 355 $\\mu$m and a thickness of 1 $\\mu$m, along with a FeGaB film with a radius of 330 $\\mu$m and a thickness of 1.3 $\\mu$m, we have achieved a remarkable peak sensitivity of 1.68 pT/Hz$^{1/2}$ at 9.52 MHz. This represents a significant improvement of over two orders of magnitude compared with previous studies employing sputtered Terfenol-D film. Notably, the magnetometer operates without a bias magnetic field, thanks to the remarkable soft magnetic properties of the FeGaB film. Furthermore, as a proof-of-concept, we have demonstrated the real-time measurement of a pulsed magnetic field simulating the corona current in a high-voltage transmission line using our developed magnetometer. These high-sensitivity magnetometers hold great potential for various applications, such as magnetic induction tomography and corona current monitoring.","sentences":["Cavity optomechanical systems have enabled precision sensing of magnetic fields, by leveraging the optical resonance-enhanced readout and mechanical resonance-enhanced response.","Previous studies have successfully achieved scalable and reproducible microcavity optomechanical magnetometry (MCOM) by incorporating Terfenol-D thin films into high-quality ($Q$) factor whispering gallery mode (WGM) microcavities.","However, the sensitivity was limited to 585 pT/Hz$^{1/2}$, over 20 times inferior to those using Terfenol-D particles.","In this work, we propose and demonstrate a high-sensitivity and scalable MCOM approach by sputtering a FeGaB thin film onto a high-$Q$ SiO$_2$ WGM microdisk.","Theoretical studies are conducted to explore the magnetic actuation constant and noise-limited sensitivity by varying the parameters of the FeGaB film and SiO$_2$ microdisk.","Multiple magnetometers with different radii are fabricated and characterized.","By utilizing a microdisk with a radius of 355 $\\mu$m and a thickness of 1 $\\mu$m, along with a FeGaB film with a radius of 330 $\\mu$m and a thickness of 1.3 $\\mu$m, we have achieved a remarkable peak sensitivity of 1.68 pT/Hz$^{1/2}$ at 9.52 MHz.","This represents a significant improvement of over two orders of magnitude compared with previous studies employing sputtered Terfenol-D film.","Notably, the magnetometer operates without a bias magnetic field, thanks to the remarkable soft magnetic properties of the FeGaB film.","Furthermore, as a proof-of-concept, we have demonstrated the real-time measurement of a pulsed magnetic field simulating the corona current in a high-voltage transmission line using our developed magnetometer.","These high-sensitivity magnetometers hold great potential for various applications, such as magnetic induction tomography and corona current monitoring."],"url":"http://arxiv.org/abs/2403.14301v1","category":"physics.optics"}
{"created":"2024-03-21 10:48:30","title":"A LSTM-enhanced surrogate model to simulate the dynamics of particle-laden fluid systems","abstract":"The numerical treatment of fluid-particle systems is a very challenging problem because of the complex coupling phenomena occurring between the two phases. Although accurate mathematical modelling is available to address this kind of application, the computational cost of the numerical simulations is very expensive. The use of the most modern high-performance computing infrastructures could help to mitigate such an issue but not completely fix it. In this work, we develop a non-intrusive data-driven reduced order model (ROM) for Computational Fluid Dynamics (CFD) - Discrete Element Method (DEM) simulations. The ROM is built using the proper orthogonal decomposition (POD) for the computation of the reduced basis space and the Long Short-Term Memory (LSTM) network for the computation of the reduced coefficients. We are interested in dealing both with system identification and prediction. The most relevant novelties rely on (i) a filtering procedure of the full-order snapshots to reduce the dimensionality of the reduced problem and (ii) a preliminary treatment of the particle phase. The accuracy of our ROM approach is assessed against the classic Goldschmidt fluidized bed benchmark problem. Finally, we also provide some insights about the efficiency of our ROM approach.","sentences":["The numerical treatment of fluid-particle systems is a very challenging problem because of the complex coupling phenomena occurring between the two phases.","Although accurate mathematical modelling is available to address this kind of application, the computational cost of the numerical simulations is very expensive.","The use of the most modern high-performance computing infrastructures could help to mitigate such an issue but not completely fix it.","In this work, we develop a non-intrusive data-driven reduced order model (ROM) for Computational Fluid Dynamics (CFD) - Discrete Element Method (DEM) simulations.","The ROM is built using the proper orthogonal decomposition (POD) for the computation of the reduced basis space and the Long Short-Term Memory (LSTM) network for the computation of the reduced coefficients.","We are interested in dealing both with system identification and prediction.","The most relevant novelties rely on (i) a filtering procedure of the full-order snapshots to reduce the dimensionality of the reduced problem and (ii) a preliminary treatment of the particle phase.","The accuracy of our ROM approach is assessed against the classic Goldschmidt fluidized bed benchmark problem.","Finally, we also provide some insights about the efficiency of our ROM approach."],"url":"http://arxiv.org/abs/2403.14283v1","category":"math.NA"}
{"created":"2024-03-21 10:39:44","title":"Large Language Models for Blockchain Security: A Systematic Literature Review","abstract":"Large Language Models (LLMs) have emerged as powerful tools in various domains involving blockchain security (BS). Several recent studies are exploring LLMs applied to BS. However, there remains a gap in our understanding regarding the full scope of applications, impacts, and potential constraints of LLMs on blockchain security. To fill this gap, we conduct a literature review on LLM4BS.   As the first review of LLM's application on blockchain security, our study aims to comprehensively analyze existing research and elucidate how LLMs contribute to enhancing the security of blockchain systems. Through a thorough examination of scholarly works, we delve into the integration of LLMs into various aspects of blockchain security. We explore the mechanisms through which LLMs can bolster blockchain security, including their applications in smart contract auditing, identity verification, anomaly detection, vulnerable repair, and so on. Furthermore, we critically assess the challenges and limitations associated with leveraging LLMs for blockchain security, considering factors such as scalability, privacy concerns, and adversarial attacks. Our review sheds light on the opportunities and potential risks inherent in this convergence, providing valuable insights for researchers, practitioners, and policymakers alike.","sentences":["Large Language Models (LLMs) have emerged as powerful tools in various domains involving blockchain security (BS).","Several recent studies are exploring LLMs applied to BS.","However, there remains a gap in our understanding regarding the full scope of applications, impacts, and potential constraints of LLMs on blockchain security.","To fill this gap, we conduct a literature review on LLM4BS.   ","As the first review of LLM's application on blockchain security, our study aims to comprehensively analyze existing research and elucidate how LLMs contribute to enhancing the security of blockchain systems.","Through a thorough examination of scholarly works, we delve into the integration of LLMs into various aspects of blockchain security.","We explore the mechanisms through which LLMs can bolster blockchain security, including their applications in smart contract auditing, identity verification, anomaly detection, vulnerable repair, and so on.","Furthermore, we critically assess the challenges and limitations associated with leveraging LLMs for blockchain security, considering factors such as scalability, privacy concerns, and adversarial attacks.","Our review sheds light on the opportunities and potential risks inherent in this convergence, providing valuable insights for researchers, practitioners, and policymakers alike."],"url":"http://arxiv.org/abs/2403.14280v1","category":"cs.CR"}
{"created":"2024-03-21 10:34:39","title":"Discovery of the magnetic cataclysmic variable XMM J152737.4-205305.9 with a deep eclipse-like feature","abstract":"In this study, we report a discovery from XMM-Newton, which involves the identification and subsequent examination of a newly discovered polar-type cataclysmic variable named XMM J152737.4-205305.9. The discovery was made by matching the XMM-Newton data archive with the cataclysmic variable candidate catalog provided by Gaia Data Release 3. The utilization of X-ray photometry has led to the identification of two distinct dips that exhibit a recurring pattern with a precise period of 112.4(1) minutes in two XMM-Newton observations that are one year apart. The data obtained from the photometry of Zwicky Transient Facility (ZTF) and ATLAS surveys consistently indicate the presence of the different mass accretion states of up to 2 mag. Following the optical data, the \\textit{SRG}(Spectrum Roentgen Gamma)/eROSITA All Sky Survey observed the system in two different X-ray levels which may imply different accretion states. Following these observations, the low-resolution spectrum obtained using SALT spectroscopy exposes the prominent hydrogen Balmer and helium emission lines, strongly supporting that the system belongs to the category of polar-type magnetic cataclysmic variable. The XMM-Newton observations, conducted under various conditions of X-ray levels, reveal a consistent pattern of a deep dip-like feature with a width of $\\approx 9.1$ min. This feature implies the presence of an eclipse in both observations. According to Gaia data, the object is located at a distance of $1156^{+720}_{-339}$,pc, and its X-ray luminosity lies within the $L_{\\rm X}$= (3-6)$\\times10^{31}$ \\lergs range.","sentences":["In this study, we report a discovery from XMM-Newton, which involves the identification and subsequent examination of a newly discovered polar-type cataclysmic variable named XMM J152737.4-205305.9.","The discovery was made by matching the XMM-Newton data archive with the cataclysmic variable candidate catalog provided by Gaia Data Release 3.","The utilization of X-ray photometry has led to the identification of two distinct dips that exhibit a recurring pattern with a precise period of 112.4(1) minutes in two XMM-Newton observations that are one year apart.","The data obtained from the photometry of Zwicky Transient Facility (ZTF) and ATLAS surveys consistently indicate the presence of the different mass accretion states of up to 2 mag.","Following the optical data, the \\textit{SRG}(Spectrum Roentgen Gamma)/eROSITA All Sky Survey observed the system in two different X-ray levels which may imply different accretion states.","Following these observations, the low-resolution spectrum obtained using SALT spectroscopy exposes the prominent hydrogen Balmer and helium emission lines, strongly supporting that the system belongs to the category of polar-type magnetic cataclysmic variable.","The XMM-Newton observations, conducted under various conditions of X-ray levels, reveal a consistent pattern of a deep dip-like feature with a width of $\\approx 9.1$ min.","This feature implies the presence of an eclipse in both observations.","According to Gaia data, the object is located at a distance of $1156^{+720}_{-339}$,pc, and its X-ray luminosity lies within the $L_{\\rm X}$= (3-6)$\\times10^{31}$ \\lergs range."],"url":"http://arxiv.org/abs/2403.14278v1","category":"astro-ph.HE"}
{"created":"2024-03-21 10:33:12","title":"Optical Bias and Cryogenic Laser Readout of a Multipixel Superconducting Nanowire Single Photon Detector","abstract":"Cryogenic opto-electronic interconnects are gaining increasing interest as a means to control and read out cryogenic electronic components. The challenge is to achieve sufficient signal integrity with low heat load processing. In this context, we demonstrate the opto-electronic bias and readout of a commercial four-pixel superconducting nanowire single-photon detector array using a cryogenic photodiode and laser. We show that this approach has a similar system detection efficiency to a conventional bias. Furthermore, multi-pixel detection events are faithfully converted between the optical and electrical domain, which allows reliable extraction of amplitude multiplexed photon statistics. Our device has a passive heat dissipation of 2.6mW, maintains the signal rise time of 3ns, and operates in free-running (self-resetting) mode at a repetition rate of 600kHz. This demonstrates the potential of high-bandwidth, low noise, and low heat load opto-electronic interconnects for scalable cryogenic signal processing and transmission.","sentences":["Cryogenic opto-electronic interconnects are gaining increasing interest as a means to control and read out cryogenic electronic components.","The challenge is to achieve sufficient signal integrity with low heat load processing.","In this context, we demonstrate the opto-electronic bias and readout of a commercial four-pixel superconducting nanowire single-photon detector array using a cryogenic photodiode and laser.","We show that this approach has a similar system detection efficiency to a conventional bias.","Furthermore, multi-pixel detection events are faithfully converted between the optical and electrical domain, which allows reliable extraction of amplitude multiplexed photon statistics.","Our device has a passive heat dissipation of 2.6mW, maintains the signal rise time of 3ns, and operates in free-running (self-resetting) mode at a repetition rate of 600kHz.","This demonstrates the potential of high-bandwidth, low noise, and low heat load opto-electronic interconnects for scalable cryogenic signal processing and transmission."],"url":"http://arxiv.org/abs/2403.14276v1","category":"physics.optics"}
{"created":"2024-03-21 10:20:34","title":"Resonances in nonlinear systems with a decaying chirped-frequency excitation and noise","abstract":"The influence of multiplicative white noise on the resonance capture of strongly nonlinear oscillatory systems under chirped-frequency excitations is investigated. It is assumed that the intensity of the perturbation decays polynomially with time, and its frequency grows according to a power low. Resonant solutions with a growing amplitude and phase, synchronized with the excitation, are considered. The persistence of such a regime in the presence of stochastic perturbations is discussed. In particular, conditions are described that guarantee the stochastic stability of the resonant modes on infinite or asymptotically large time intervals. The technique used is based on a combination of the averaging method, stability analysis and construction of stochastic Lyapunov functions. The proposed theory is applied to the Duffing oscillator with a chirped-frequency excitation and noise.","sentences":["The influence of multiplicative white noise on the resonance capture of strongly nonlinear oscillatory systems under chirped-frequency excitations is investigated.","It is assumed that the intensity of the perturbation decays polynomially with time, and its frequency grows according to a power low.","Resonant solutions with a growing amplitude and phase, synchronized with the excitation, are considered.","The persistence of such a regime in the presence of stochastic perturbations is discussed.","In particular, conditions are described that guarantee the stochastic stability of the resonant modes on infinite or asymptotically large time intervals.","The technique used is based on a combination of the averaging method, stability analysis and construction of stochastic Lyapunov functions.","The proposed theory is applied to the Duffing oscillator with a chirped-frequency excitation and noise."],"url":"http://arxiv.org/abs/2403.14271v1","category":"math.DS"}
{"created":"2024-03-21 10:15:57","title":"Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship Detection","abstract":"Visual relationship detection aims to identify objects and their relationships in images. Prior methods approach this task by adding separate relationship modules or decoders to existing object detection architectures. This separation increases complexity and hinders end-to-end training, which limits performance. We propose a simple and highly efficient decoder-free architecture for open-vocabulary visual relationship detection. Our model consists of a Transformer-based image encoder that represents objects as tokens and models their relationships implicitly. To extract relationship information, we introduce an attention mechanism that selects object pairs likely to form a relationship. We provide a single-stage recipe to train this model on a mixture of object and relationship detection data. Our approach achieves state-of-the-art relationship detection performance on Visual Genome and on the large-vocabulary GQA benchmark at real-time inference speeds. We provide analyses of zero-shot performance, ablations, and real-world qualitative examples.","sentences":["Visual relationship detection aims to identify objects and their relationships in images.","Prior methods approach this task by adding separate relationship modules or decoders to existing object detection architectures.","This separation increases complexity and hinders end-to-end training, which limits performance.","We propose a simple and highly efficient decoder-free architecture for open-vocabulary visual relationship detection.","Our model consists of a Transformer-based image encoder that represents objects as tokens and models their relationships implicitly.","To extract relationship information, we introduce an attention mechanism that selects object pairs likely to form a relationship.","We provide a single-stage recipe to train this model on a mixture of object and relationship detection data.","Our approach achieves state-of-the-art relationship detection performance on Visual Genome and on the large-vocabulary GQA benchmark at real-time inference speeds.","We provide analyses of zero-shot performance, ablations, and real-world qualitative examples."],"url":"http://arxiv.org/abs/2403.14270v1","category":"cs.CV"}
{"created":"2024-03-21 10:00:17","title":"Exotic Fusion System on a Subgroup of the Monster","abstract":"We prove that an exotic fusion system described by Grazian on a subgroup of the Monster group is block-exotic, thus proving that exotic and block-exotic fusion systems are the same for all $p$-groups with sectional rank 3, where $p \\geq 5$.","sentences":["We prove that an exotic fusion system described by Grazian on a subgroup of the Monster group is block-exotic, thus proving that exotic and block-exotic fusion systems are the same for all $p$-groups with sectional rank 3, where $p \\geq 5$."],"url":"http://arxiv.org/abs/2403.14265v1","category":"math.GR"}
{"created":"2024-03-21 09:48:03","title":"Complexity of the Model Checking problem for inquisitive propositional and modal logic","abstract":"The aim of this paper is to study the complexity of the model checking problem MC for inquisitive propositional logic InqB and for inquisitive modal logic InqM, that is, the problem of deciding whether a given finite structure for the logic satisfies a given formula. In recent years, this problem has been thoroughly investigated for several variations of dependence and teams logics, systems closely related to inquisitive logic. Building upon some ideas presented by Yang, we prove that the model checking problems for InqB and InqM are both AP-complete.","sentences":["The aim of this paper is to study the complexity of the model checking problem MC for inquisitive propositional logic InqB and for inquisitive modal logic InqM, that is, the problem of deciding whether a given finite structure for the logic satisfies a given formula.","In recent years, this problem has been thoroughly investigated for several variations of dependence and teams logics, systems closely related to inquisitive logic.","Building upon some ideas presented by Yang, we prove that the model checking problems for InqB and InqM are both AP-complete."],"url":"http://arxiv.org/abs/2403.14260v1","category":"cs.LO"}
{"created":"2024-03-21 09:30:36","title":"Locally homogeneous Axiom A flows I: projective Anosov subgroups and exponential mixing","abstract":"By constructing a non-empty domain of discontinuity in a suitable homogeneous space, we prove that every torsion-free projective Anosov subgroup is the monodromy group of a locally homogeneous contact Axiom A dynamical system with a unique basic hyperbolic set on which the flow is conjugate to the refraction flow of Sambarino. Under the assumption of irreducibility, we utilize the work of Stoyanov to establish spectral estimates for the associated complex Ruelle transfer operators, and by way of corollary: exponential mixing, exponentially decaying error term in the prime orbit theorem, and a spectral gap for the Ruelle zeta function. With no irreducibility assumption, results of Dyatlov-Guillarmou imply the global meromorphic continuation of zeta functions with smooth weights, as well as the existence of a discrete spectrum of Ruelle-Pollicott resonances and (co)-resonant states. We apply our results to space-like geodesic flows for the convex cocompact pseudo-Riemannian manifolds of Danciger-Gu\\'eritaud-Kassel, and the Benoist-Hilbert geodesic flow for strictly convex real projective manifolds.","sentences":["By constructing a non-empty domain of discontinuity in a suitable homogeneous space, we prove that every torsion-free projective Anosov subgroup is the monodromy group of a locally homogeneous contact Axiom A dynamical system with a unique basic hyperbolic set on which the flow is conjugate to the refraction flow of Sambarino.","Under the assumption of irreducibility, we utilize the work of Stoyanov to establish spectral estimates for the associated complex Ruelle transfer operators, and by way of corollary: exponential mixing, exponentially decaying error term in the prime orbit theorem, and a spectral gap for the Ruelle zeta function.","With no irreducibility assumption, results of Dyatlov-Guillarmou imply the global meromorphic continuation of zeta functions with smooth weights, as well as the existence of a discrete spectrum of Ruelle-Pollicott resonances and (co)-resonant states.","We apply our results to space-like geodesic flows for the convex cocompact pseudo-Riemannian manifolds of Danciger-Gu\\'eritaud-Kassel, and the Benoist-Hilbert geodesic flow for strictly convex real projective manifolds."],"url":"http://arxiv.org/abs/2403.14257v1","category":"math.DG"}
{"created":"2024-03-21 09:28:05","title":"Developments in quasihydrodynamics","abstract":"At its core, hydrodynamics is a many-body low-energy effective theory for the long-wavelength, long-timescale dynamics of conserved charges in systems close to thermodynamic equilibrium. It has a wide range of applications spanning from nuclear physics, astrophysics, cosmology, and more recently strongly-interacting electronic phases of matter. In solid-state systems, however, symmetries are often only approximate, and softly broken by the presence of the lattice, impurities, and defects, or because the symmetry is accidental. Therefore, the hydrodynamic regime must be expanded to include weak non-conservation effects, which lead to a theory known as quasihydrodynamics. In this thesis we make progress in understanding the theory of (quasi) hydrodynamics, with a specific focus on applications to condensed matter systems and their holographic description.","sentences":["At its core, hydrodynamics is a many-body low-energy effective theory for the long-wavelength, long-timescale dynamics of conserved charges in systems close to thermodynamic equilibrium.","It has a wide range of applications spanning from nuclear physics, astrophysics, cosmology, and more recently strongly-interacting electronic phases of matter.","In solid-state systems, however, symmetries are often only approximate, and softly broken by the presence of the lattice, impurities, and defects, or because the symmetry is accidental.","Therefore, the hydrodynamic regime must be expanded to include weak non-conservation effects, which lead to a theory known as quasihydrodynamics.","In this thesis we make progress in understanding the theory of (quasi) hydrodynamics, with a specific focus on applications to condensed matter systems and their holographic description."],"url":"http://arxiv.org/abs/2403.14254v1","category":"hep-th"}
{"created":"2024-03-21 09:24:56","title":"Polynomial Volterra processes","abstract":"We study the class of continuous polynomial Volterra processes, which we define as solutions to stochastic Volterra equations driven by a continuous semimartingale with affine drift and quadratic diffusion matrix in the state of the Volterra process. To demonstrate the versatility of possible state spaces within our framework, we construct polynomial Volterra processes on the unit ball. This construction is based on a stochastic invariance principle for stochastic Volterra equations with possibly singular kernels. Similarly to classical polynomial processes, polynomial Volterra processes allow for tractable expressions of the moments in terms of the unique solution to a system of deterministic integral equations, which reduce to a system of ODEs in the classical case. By applying this observation to the moments of the finite-dimensional distributions we derive a uniqueness result for polynomial Volterra processes. Moreover, we prove that the moments are polynomials with respect to the initial condition, another crucial property shared by classical polynomial processes. The corresponding coefficients can be interpreted as a deterministic dual process and solve integral equations dual to those verified by the moments themselves. Additionally, we obtain a representation of the moments in terms of a pure jump process with killing, which corresponds to another non-deterministic dual process.","sentences":["We study the class of continuous polynomial Volterra processes, which we define as solutions to stochastic Volterra equations driven by a continuous semimartingale with affine drift and quadratic diffusion matrix in the state of the Volterra process.","To demonstrate the versatility of possible state spaces within our framework, we construct polynomial Volterra processes on the unit ball.","This construction is based on a stochastic invariance principle for stochastic Volterra equations with possibly singular kernels.","Similarly to classical polynomial processes, polynomial Volterra processes allow for tractable expressions of the moments in terms of the unique solution to a system of deterministic integral equations, which reduce to a system of ODEs in the classical case.","By applying this observation to the moments of the finite-dimensional distributions we derive a uniqueness result for polynomial Volterra processes.","Moreover, we prove that the moments are polynomials with respect to the initial condition, another crucial property shared by classical polynomial processes.","The corresponding coefficients can be interpreted as a deterministic dual process and solve integral equations dual to those verified by the moments themselves.","Additionally, we obtain a representation of the moments in terms of a pure jump process with killing, which corresponds to another non-deterministic dual process."],"url":"http://arxiv.org/abs/2403.14251v1","category":"math.PR"}
{"created":"2024-03-21 09:18:16","title":"Direct Probe of Topology and Geometry of Quantum States on IBM Q","abstract":"The concepts of topology and geometry are of critical importance in exploring exotic phases of quantum matter. Though they have been investigated on various experimental platforms, to date a direct probe of topological and geometric properties on a universal quantum computer even for a minimum model is still in vain. In this work, we first show that a density matrix form of the quantum geometric tensor (QGT) can be explicitly re-constructed from Pauli operator measurements on a quantum circuit. We then propose two algorithms, suitable for IBM quantum computers, to directly probe QGT. The first algorithm is a variational quantum algorithm particularly suitable for Noisy Intermediate-Scale Quantum (NISQ)-era devices, whereas the second one is a pure quantum algorithm based on quantum imaginary time evolution. Explicit results obtained from IBM Q simulating a Chern insulator model are presented and analysed. Our results indicate that transmon qubit-based universal quantum computers have the potential to directly simulate and investigate topological and geometric properties of a quantum system.","sentences":["The concepts of topology and geometry are of critical importance in exploring exotic phases of quantum matter.","Though they have been investigated on various experimental platforms, to date a direct probe of topological and geometric properties on a universal quantum computer even for a minimum model is still in vain.","In this work, we first show that a density matrix form of the quantum geometric tensor (QGT) can be explicitly re-constructed from Pauli operator measurements on a quantum circuit.","We then propose two algorithms, suitable for IBM quantum computers, to directly probe QGT.","The first algorithm is a variational quantum algorithm particularly suitable for Noisy Intermediate-Scale Quantum (NISQ)-era devices, whereas the second one is a pure quantum algorithm based on quantum imaginary time evolution.","Explicit results obtained from IBM Q simulating a Chern insulator model are presented and analysed.","Our results indicate that transmon qubit-based universal quantum computers have the potential to directly simulate and investigate topological and geometric properties of a quantum system."],"url":"http://arxiv.org/abs/2403.14249v1","category":"quant-ph"}
{"created":"2024-03-21 09:01:50","title":"Magnetocrystalline anisotropy in metallic systems: fast and stable estimation in Green`s functions formalism","abstract":"In this work we suggest a theoretical approach, that allows to study the effects of magnetocrystalline anisotropy (MCA) in metallic systems using the Green`s functions formalism. We demonstrate that employment of the reciprocal space resolution instead of its reduction in the inter-site variant essentially improves the numerical stability of MCA energy by means of Monkhorst-Pack grid density and spatial convergence. The latter problem is able to be completely removed due to rigorous analytical replacement of pairwise atomic summation by simple composition of sublattices contributions, calculated as a whole. The approach is validated on the effective model of single atom, which nevertheless inherits the qualitative MCA picture of Co monolayer and Au/Co/Au sandwiched material. The numerical convergence is confirmed using the model of atomic chain in the strong metallic regime. For cobalt monoxide, described by ab initio calculations using GGA+U, the MCA energy angular profile reveals the prevailing role of ferromagnetically aligned Co sublattices in forming of the easy axis.","sentences":["In this work we suggest a theoretical approach, that allows to study the effects of magnetocrystalline anisotropy (MCA) in metallic systems using the Green`s functions formalism.","We demonstrate that employment of the reciprocal space resolution instead of its reduction in the inter-site variant essentially improves the numerical stability of MCA energy by means of Monkhorst-Pack grid density and spatial convergence.","The latter problem is able to be completely removed due to rigorous analytical replacement of pairwise atomic summation by simple composition of sublattices contributions, calculated as a whole.","The approach is validated on the effective model of single atom, which nevertheless inherits the qualitative MCA picture of Co monolayer and Au/Co/Au sandwiched material.","The numerical convergence is confirmed using the model of atomic chain in the strong metallic regime.","For cobalt monoxide, described by ab initio calculations using GGA+U, the MCA energy angular profile reveals the prevailing role of ferromagnetically aligned Co sublattices in forming of the easy axis."],"url":"http://arxiv.org/abs/2403.14241v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-21 08:59:26","title":"Physical insights from the aspect ratio dependence of turbulence in negative triangularity plasmas","abstract":"In this work, we study the impact of aspect ratio A = R0 /r (the ratio of major radius R0 to minor radius r) on the confinement benefits of Negative Triangularity (NT) plasma shaping. We use high-fidelity flux tube gyrokinetic GENE simulations and consider several different scenarios: four of them inspired by TCV experimental data, a scenario inspired by DIII-D experimental data and a scenario expected in the new SMART spherical tokamak. The present study reveals a surprising and non-trivial dependence. NT improves confinement at any value of A for ITG turbulence, while for TEM turbulence confinement is improved only in the case of large and conventional aspect ratios. Additionally, through a detailed study of a large aspect ratio case with pure ITG drive, we develop an intuitive physical picture that explains the beneficial effect of NT at large and conventional aspect ratios. This picture does not hold in TEM-dominated regimes, where a complex synergistic effect of many factors is found. Finally, we performed the first linear gyrokinetic simulations of SMART, finding that both NT and PT scenarios are dominated by micro-tearing-mode (MTM) turbulence and that NT is more susceptible to MTMs at tight aspect ratio. However, we found that a regime where ITGs dominate in SMART can be found, and in this regime NT is more linearly stable.","sentences":["In this work, we study the impact of aspect ratio A = R0 /r (the ratio of major radius R0 to minor radius r) on the confinement benefits of Negative Triangularity (NT) plasma shaping.","We use high-fidelity flux tube gyrokinetic GENE simulations and consider several different scenarios: four of them inspired by TCV experimental data, a scenario inspired by DIII-D experimental data and a scenario expected in the new SMART spherical tokamak.","The present study reveals a surprising and non-trivial dependence.","NT improves confinement at any value of A for ITG turbulence, while for TEM turbulence confinement is improved only in the case of large and conventional aspect ratios.","Additionally, through a detailed study of a large aspect ratio case with pure ITG drive, we develop an intuitive physical picture that explains the beneficial effect of NT at large and conventional aspect ratios.","This picture does not hold in TEM-dominated regimes, where a complex synergistic effect of many factors is found.","Finally, we performed the first linear gyrokinetic simulations of SMART, finding that both NT and PT scenarios are dominated by micro-tearing-mode (MTM) turbulence and that NT is more susceptible to MTMs at tight aspect ratio.","However, we found that a regime where ITGs dominate in SMART can be found, and in this regime NT is more linearly stable."],"url":"http://arxiv.org/abs/2403.14239v1","category":"physics.plasm-ph"}
{"created":"2024-03-21 08:41:00","title":"Hausdorff dimension of the parameters for $(\u03b1,\u03b2)$-transformations with the specification property","abstract":"In this paper we consider the specification property for $(\\alpha,\\beta)$-shifts. When $\\alpha=0$, Schmeling shows that the set of $\\beta>1$ for which the $\\beta$-shift has the specification property has the Lebesgue measure zero but has the full Hausdorff dimension\\cite{Schmeling}. So it is natural to ask what happens when $\\alpha>0$. Buzzi shows that for fixed $\\alpha$ the set of $\\beta >1$ for which the $(\\alpha,\\beta)$-shift has the specification property has Lebesgue measure zero. Hence we consider the Hausdorff dimension of the parameter space of $(\\alpha,\\beta)$-shifts.","sentences":["In this paper we consider the specification property for $(\\alpha,\\beta)$-shifts.","When $\\alpha=0$, Schmeling shows that the set of $\\beta>1$ for which the $\\beta$-shift has the specification property has the Lebesgue measure zero but has the full Hausdorff dimension\\cite{Schmeling}.","So it is natural to ask what happens when $\\alpha>0$. Buzzi shows that for fixed $\\alpha$ the set of $\\beta >1$ for which the $(\\alpha,\\beta)$-shift has the specification property has Lebesgue measure zero.","Hence we consider the Hausdorff dimension of the parameter space of $(\\alpha,\\beta)$-shifts."],"url":"http://arxiv.org/abs/2403.14230v1","category":"math.DS"}
{"created":"2024-03-21 08:37:05","title":"On unifying control barrier and Lyapunov functions using QP and Sontag's formula with an application to tumor dynamics","abstract":"A common tool in system theory for formulating control laws that achieve local asymptotic stability are Control Lyapunov functions (CLFs), while Control Barrier functions (CBFs) are typically employed to enforce safety constraints. Combining these two types of functions is of interest, because it leads to stabilizing controllers with safety guarantees. A common approach to merge CLFs and CBFs is to solve an optimization problem where both CLF and CBF inequalities are imposed as constraints. In this paper, we show via an example from the literature that this approach can lead to undesirable behavior (i.e., slow convergence and oscillating inputs). Then, we propose a novel cost function that penalizes the deviation from Sontag's formula by using a state-dependent weighting matrix. We show that by minimizing the developed cost function subject to a CBF constraint, local asymptotic stability is obtained with an explicit domain of attraction, without using a CLF constraint. To deal with vanishing properties of the weight matrix as the state approaches the equilibrium, we introduce a hybrid continuous control law that recovers Sontag's formula locally. The effectiveness of the developed hybrid stabilizing control law based on CLFs and CBFs is illustrated in stabilization of a 3D tumor model, subject to physiological constraints (i.e., all states must be positive), which yields useful insights into optimal cancer treatment design.","sentences":["A common tool in system theory for formulating control laws that achieve local asymptotic stability are Control Lyapunov functions (CLFs), while Control Barrier functions (CBFs) are typically employed to enforce safety constraints.","Combining these two types of functions is of interest, because it leads to stabilizing controllers with safety guarantees.","A common approach to merge CLFs and CBFs is to solve an optimization problem where both CLF and CBF inequalities are imposed as constraints.","In this paper, we show via an example from the literature that this approach can lead to undesirable behavior (i.e., slow convergence and oscillating inputs).","Then, we propose a novel cost function that penalizes the deviation from Sontag's formula by using a state-dependent weighting matrix.","We show that by minimizing the developed cost function subject to a CBF constraint, local asymptotic stability is obtained with an explicit domain of attraction, without using a CLF constraint.","To deal with vanishing properties of the weight matrix as the state approaches the equilibrium, we introduce a hybrid continuous control law that recovers Sontag's formula locally.","The effectiveness of the developed hybrid stabilizing control law based on CLFs and CBFs is illustrated in stabilization of a 3D tumor model, subject to physiological constraints (i.e., all states must be positive), which yields useful insights into optimal cancer treatment design."],"url":"http://arxiv.org/abs/2403.14226v1","category":"math.OC"}
{"created":"2024-03-21 08:20:31","title":"Analysis of a map-based neuronal model","abstract":"Subthreshold oscillations in neurons are those oscillations which do not attain the critical value of the membrane's voltage needed for triggering an action potential (a spike). Their contribution to the forming of action potentials in neurons is a current field of research in biology. The present work approaches this subject using tools from mathematical modeling, more exactly, a neuronal non-smooth map-based model is proposed and studied. The behavior of the model in a noisy medium is also studied.","sentences":["Subthreshold oscillations in neurons are those oscillations which do not attain the critical value of the membrane's voltage needed for triggering an action potential (a spike).","Their contribution to the forming of action potentials in neurons is a current field of research in biology.","The present work approaches this subject using tools from mathematical modeling, more exactly, a neuronal non-smooth map-based model is proposed and studied.","The behavior of the model in a noisy medium is also studied."],"url":"http://arxiv.org/abs/2403.14219v1","category":"math.DS"}
{"created":"2024-03-21 08:13:56","title":"Maximizing Phylogenetic Diversity under Time Pressure: Planning with Extinctions Ahead","abstract":"Phylogenetic Diversity (PD) is a measure of the overall biodiversity of a set of present-day species (taxa) within a phylogenetic tree. In Maximize Phylogenetic Diversity (MPD) one is asked to find a set of taxa (of bounded size/cost) for which this measure is maximized. MPD is a relevant problem in conservation planning, where there are not enough resources to preserve all taxa and minimizing the overall loss of biodiversity is critical. We consider an extension of this problem, motivated by real-world concerns, in which each taxon not only requires a certain amount of time to save, but also has an extinction time after which it can no longer be saved. In addition there may be multiple teams available to work on preservation efforts in parallel; we consider two variants of the problem based on whether teams are allowed to collaborate on the same taxa. These problems have much in common with machine scheduling problems, (with taxa corresponding to tasks and teams corresponding to machines), but with the objective function (the phylogenetic diversity) inspired by biological considerations. Our extensions are, in contrast to the original MPD, NP-hard, even in very restricted cases. We provide several algorithms and hardness-results and thereby show that the problems are fixed-parameter tractable (FPT) when parameterized the target phylogenetic diversity, and that the problem where teams are allowed to collaborate is FPT when parameterized the acceptable loss of diversity.","sentences":["Phylogenetic Diversity (PD) is a measure of the overall biodiversity of a set of present-day species (taxa) within a phylogenetic tree.","In Maximize Phylogenetic Diversity (MPD) one is asked to find a set of taxa (of bounded size/cost) for which this measure is maximized.","MPD is a relevant problem in conservation planning, where there are not enough resources to preserve all taxa and minimizing the overall loss of biodiversity is critical.","We consider an extension of this problem, motivated by real-world concerns, in which each taxon not only requires a certain amount of time to save, but also has an extinction time after which it can no longer be saved.","In addition there may be multiple teams available to work on preservation efforts in parallel; we consider two variants of the problem based on whether teams are allowed to collaborate on the same taxa.","These problems have much in common with machine scheduling problems, (with taxa corresponding to tasks and teams corresponding to machines), but with the objective function (the phylogenetic diversity) inspired by biological considerations.","Our extensions are, in contrast to the original MPD, NP-hard, even in very restricted cases.","We provide several algorithms and hardness-results and thereby show that the problems are fixed-parameter tractable (FPT) when parameterized the target phylogenetic diversity, and that the problem where teams are allowed to collaborate is FPT when parameterized the acceptable loss of diversity."],"url":"http://arxiv.org/abs/2403.14217v1","category":"cs.CC"}
{"created":"2024-03-21 08:13:07","title":"A Gaussian smooth transition vector autoregressive model: An application to the macroeconomic effects of severe weather shocks","abstract":"We introduce a new smooth transition vector autoregressive model with a Gaussian conditional distribution and transition weights that, for a $p$th order model, depend on the full distribution of the preceding $p$ observations. Specifically, the transition weight of each regime increases in its relative weighted likelihood. This data-driven approach facilitates capturing complex switching dynamics, enhancing the identification of gradual regime shifts. In an empirical application to the macroeconomic effects of a severe weather shock, we find that in monthly U.S. data from 1961:1 to 2022:3, the impacts of the shock are stronger in the regime prevailing in the early part of the sample and in certain crisis periods than in the regime dominating the latter part of the sample. This suggests overall adaptation of the U.S. economy to increased severe weather over time.","sentences":["We introduce a new smooth transition vector autoregressive model with a Gaussian conditional distribution and transition weights that, for a $p$th order model, depend on the full distribution of the preceding $p$ observations.","Specifically, the transition weight of each regime increases in its relative weighted likelihood.","This data-driven approach facilitates capturing complex switching dynamics, enhancing the identification of gradual regime shifts.","In an empirical application to the macroeconomic effects of a severe weather shock, we find that in monthly U.S. data from 1961:1 to 2022:3, the impacts of the shock are stronger in the regime prevailing in the early part of the sample and in certain crisis periods than in the regime dominating the latter part of the sample.","This suggests overall adaptation of the U.S. economy to increased severe weather over time."],"url":"http://arxiv.org/abs/2403.14216v1","category":"econ.EM"}
{"created":"2024-03-21 08:04:17","title":"Bifurcation diagrams in a class of Kolmogorov systems","abstract":"We study a two-dimensional Kolmogorov system when its two parameters vary in a small neighbourhood of the value $0.$ The local behavior of the system is described in terms of bifurcation diagrams.","sentences":["We study a two-dimensional Kolmogorov system when its two parameters vary in a small neighbourhood of the value $0.$ The local behavior of the system is described in terms of bifurcation diagrams."],"url":"http://arxiv.org/abs/2403.14211v1","category":"math.DS"}
{"created":"2024-03-21 08:01:31","title":"Multi Methods of Matrix Analysis Use for Control and Optimization system in Control Engineering","abstract":"Matrix analysis plays a crucial role in the field of control engineering, providing a powerful mathematical framework for the analysis and design of control systems. This research report explores various applications of matrix analysis in control engineering, focusing on its contributions to system modeling, stability analysis, controllablity, observability, and optimization. The report also discusses specific examples and case studies to illustrate the practical significance of matrix analysis in addressing real-world control engineering challenges Analyze controllability. Informally, a system is controllable if we can construct a set of inputs that will drive the system to any given state. Analyze observability. Informally, observability means that by controlling the inputs and watching the outputs of a system we can determine what the states were. Optimal Control is a control method that aims to find the optimal control input to achieve the best performance of the system under certain constraints. This performance index can be the system output, energy consumption, time, etc.","sentences":["Matrix analysis plays a crucial role in the field of control engineering, providing a powerful mathematical framework for the analysis and design of control systems.","This research report explores various applications of matrix analysis in control engineering, focusing on its contributions to system modeling, stability analysis, controllablity, observability, and optimization.","The report also discusses specific examples and case studies to illustrate the practical significance of matrix analysis in addressing real-world control engineering challenges Analyze controllability.","Informally, a system is controllable if we can construct a set of inputs that will drive the system to any given state.","Analyze observability.","Informally, observability means that by controlling the inputs and watching the outputs of a system we can determine what the states were.","Optimal Control is a control method that aims to find the optimal control input to achieve the best performance of the system under certain constraints.","This performance index can be the system output, energy consumption, time, etc."],"url":"http://arxiv.org/abs/2403.14209v1","category":"math.OC"}
{"created":"2024-03-21 07:57:28","title":"VL-DNA: Enhance DNA Storage Capacity with Variable Payload (Strand) Lengths","abstract":"DNA storage is a promising archival data storage solution to today's big data problem. A DNA storage system encodes and stores digital data with synthetic DNA sequences and decodes DNA sequences back to digital data via sequencing. For efficient target data retrieving, existing Polymerase Chain Reaction PCR based DNA storage systems apply primers as specific identifier to tag different set of DNA strands. However, the PCR based DNA storage system suffers from primer-payload collisions, causing a significant reduction of storage capacity. This paper proposes using variable strand length, which takes advantage of the inherent payload-cutting process, to split collisions and recover primers. The executing time of our scheme is linear to the number of primer-payload collisions. The scheme serves as a post-processing method to any DNA encoding scheme. The evaluation of three state-of-the-art encoding schemes shows that the scheme can recover thousands of usable primers and improve tube capacity ranging from 18.27% to 19x.","sentences":["DNA storage is a promising archival data storage solution to today's big data problem.","A DNA storage system encodes and stores digital data with synthetic DNA sequences and decodes DNA sequences back to digital data via sequencing.","For efficient target data retrieving, existing Polymerase Chain Reaction PCR based DNA storage systems apply primers as specific identifier to tag different set of DNA strands.","However, the PCR based DNA storage system suffers from primer-payload collisions, causing a significant reduction of storage capacity.","This paper proposes using variable strand length, which takes advantage of the inherent payload-cutting process, to split collisions and recover primers.","The executing time of our scheme is linear to the number of primer-payload collisions.","The scheme serves as a post-processing method to any DNA encoding scheme.","The evaluation of three state-of-the-art encoding schemes shows that the scheme can recover thousands of usable primers and improve tube capacity ranging from 18.27% to 19x."],"url":"http://arxiv.org/abs/2403.14204v1","category":"cs.ET"}
{"created":"2024-03-21 07:50:44","title":"A survey on Bernstein-type theorems for entire graphical surfaces","abstract":"We survey Bernstein-type theorems of graphical surfaces in the Euclidean space and the Lorentz-Minkowski space. More specifically, we explain several proofs of the Bernstein theorem for minimal graphs in the Euclidean 3-space. Furthermore, we show the Heinz-type mean curvature estimates for graphs in the Euclidean 3-space and space-like graphs in the Lorentz-Minkowski 3-space. As an application of these estimates, we give Bernstein-type theorems for constant mean curvature graphs in the Euclidean 3-space and constant mean curvature space-like graphs in the Lorentz-Minkowski 3-space, respectively. We also study Bernstein-type results for minimal graphs in the Euclidean 4-space and the Calabi-Bernstein theorem in the Lorentz-Minkowski 3-space.","sentences":["We survey Bernstein-type theorems of graphical surfaces in the Euclidean space and the Lorentz-Minkowski space.","More specifically, we explain several proofs of the Bernstein theorem for minimal graphs in the Euclidean 3-space.","Furthermore, we show the Heinz-type mean curvature estimates for graphs in the Euclidean 3-space and space-like graphs in the Lorentz-Minkowski 3-space.","As an application of these estimates, we give Bernstein-type theorems for constant mean curvature graphs in the Euclidean 3-space and constant mean curvature space-like graphs in the Lorentz-Minkowski 3-space, respectively.","We also study Bernstein-type results for minimal graphs in the Euclidean 4-space and the Calabi-Bernstein theorem in the Lorentz-Minkowski 3-space."],"url":"http://arxiv.org/abs/2403.14199v1","category":"math.DG"}
{"created":"2024-03-21 07:42:07","title":"An Agnostic Biosignature Based on Modeling Panspermia and Terraformation","abstract":"A fundamental goal of astrobiology is to detect life outside of Earth. This proves to be an exceptional challenge outside of our solar system, where strong assumptions must be made about how life would manifest and interact with its planet. Such assumptions are required because of the lack of a consensus theory of living systems, or an understanding of the possible extent of planetary dynamics. Here we explore a model of life spreading between planetary systems via panspermia and terraformation. Our model shows that as life propagates across the galaxy, correlations emerge between planetary characteristics and location, and can function as a population-scale agnostic biosignature. This biosignature is agnostic because it is independent of strong assumptions about any particular instantiation of life or planetary characteristic--by focusing on a specific hypothesis of what life may do, rather than what life may be. By clustering planets based on their observed characteristics, and examining the spatial extent of these clusters, we demonstrate (and evaluate) a way to prioritize specific planets for further observation--based on their potential for containing life. We consider obstacles that must be overcome to practically implement our approach, including identifying specific ways in which better understanding astrophysical and planetary processes would improve our ability to detect life. Finally, we consider how this model leads us to think in novel ways about hierarchies of life and planetary scale replication.","sentences":["A fundamental goal of astrobiology is to detect life outside of Earth.","This proves to be an exceptional challenge outside of our solar system, where strong assumptions must be made about how life would manifest and interact with its planet.","Such assumptions are required because of the lack of a consensus theory of living systems, or an understanding of the possible extent of planetary dynamics.","Here we explore a model of life spreading between planetary systems via panspermia and terraformation.","Our model shows that as life propagates across the galaxy, correlations emerge between planetary characteristics and location, and can function as a population-scale agnostic biosignature.","This biosignature is agnostic because it is independent of strong assumptions about any particular instantiation of life or planetary characteristic--by focusing on a specific hypothesis of what life may do, rather than what life may be.","By clustering planets based on their observed characteristics, and examining the spatial extent of these clusters, we demonstrate (and evaluate) a way to prioritize specific planets for further observation--based on their potential for containing life.","We consider obstacles that must be overcome to practically implement our approach, including identifying specific ways in which better understanding astrophysical and planetary processes would improve our ability to detect life.","Finally, we consider how this model leads us to think in novel ways about hierarchies of life and planetary scale replication."],"url":"http://arxiv.org/abs/2403.14195v1","category":"astro-ph.EP"}
{"created":"2024-03-21 07:40:52","title":"Event-triggered Boundary Control of Mixed-autonomy Traffic","abstract":"Control problems of mixed-autonomy traffic system consisting of both Human-driven Vehicles (HV) and Autonomous Vehicles (AV) have gained increasing attention. This paper is focused on suppressing traffic oscillations of the mixed-autonomy traffic system using boundary control design. The mixed traffic dynamics are described by a 4 x 4 hyperbolic partial differential equations (PDE) which governs propagation of four properties in traffic including density of HV, density of AV, friction between two classes of vehicles from driving interactions, and averaged velocity. We propose event-triggered boundary control design since control signal of traffic light on ramp or varying speed limit cannot be updated in a continuous time fashion. We apply event-triggered mechanism for a PDE backstepping controller and obtain dynamic triggering condition. Lyapunov analysis is conducted to prove the exponential stability of the closed loop system with the event-triggered controller. Numerical simulation demonstrates how car-following spacing of AV affects event-triggering mechanism of control input in mixed-autonomy traffic.","sentences":["Control problems of mixed-autonomy traffic system consisting of both Human-driven Vehicles (HV) and Autonomous Vehicles (AV) have gained increasing attention.","This paper is focused on suppressing traffic oscillations of the mixed-autonomy traffic system using boundary control design.","The mixed traffic dynamics are described by a 4 x 4 hyperbolic partial differential equations (PDE) which governs propagation of four properties in traffic including density of HV, density of AV, friction between two classes of vehicles from driving interactions, and averaged velocity.","We propose event-triggered boundary control design since control signal of traffic light on ramp or varying speed limit cannot be updated in a continuous time fashion.","We apply event-triggered mechanism for a PDE backstepping controller and obtain dynamic triggering condition.","Lyapunov analysis is conducted to prove the exponential stability of the closed loop system with the event-triggered controller.","Numerical simulation demonstrates how car-following spacing of AV affects event-triggering mechanism of control input in mixed-autonomy traffic."],"url":"http://arxiv.org/abs/2403.14194v1","category":"eess.SY"}
{"created":"2024-03-21 07:26:17","title":"Optimal Scheduling of Uplink-Downlink Networked Control Systems with Energy Harvesting Sensor","abstract":"In this work, we consider a wireless networked control system (WNCS) consisting of a plant, a battery-operated sensor, a controller, and an actuator. The battery in the sensor harvests energy from the environment. The sensor then uses this energy for packet transmissions. There are two types of wireless communication channels, (i) sensor--controller channel (also called uplink channel), and (ii) controller--actuator channel (also called downlink channel). The controller is \\emph{half-duplex}, and this prevents it from simultaneously receiving an update from the sensor, and also transmitting a control packet to the actuator. Though frequent transmissions via uplink channel improve controller's estimate of the plant state, but this also reduces the timely control of the plant. Hence, in order to strike a balance between these two, we consider the problem of designing an optimal scheduling policy that minimizes the expected cumulative infinite horizon discounted cost, where the instantaneous cost is equal to the square of the plant state. At each time $t$, the scheduler at the sensor has to decide whether it should activate the uplink channel, or downlink. We pose this dynamic optimization problem as a Markov decision process (MDP), in which the state at time $t$ is composed of (i) the plant state $x(t)$, (ii) the age of the data packet available at the controller, denoted by $\\tau(t)$, (iii) a binary variable $y(t)$ which indicates the availability of a control packet at the controller, and (iv) the energy level of the battery at the sensor $b(t)$. We show that there exists an optimal scheduling policy that exhibits a threshold structure, meaning that for each time $t$, if there is a control packet available with the controller, then the sensor activates the downlink channel in case $|x(t)|$ exceeds a threshold $x\\ust(\\tau(t),b(t))$.","sentences":["In this work, we consider a wireless networked control system (WNCS) consisting of a plant, a battery-operated sensor, a controller, and an actuator.","The battery in the sensor harvests energy from the environment.","The sensor then uses this energy for packet transmissions.","There are two types of wireless communication channels, (i) sensor--controller channel (also called uplink channel), and (ii) controller--actuator channel (also called downlink channel).","The controller is \\emph{half-duplex}, and this prevents it from simultaneously receiving an update from the sensor, and also transmitting a control packet to the actuator.","Though frequent transmissions via uplink channel improve controller's estimate of the plant state, but this also reduces the timely control of the plant.","Hence, in order to strike a balance between these two, we consider the problem of designing an optimal scheduling policy that minimizes the expected cumulative infinite horizon discounted cost, where the instantaneous cost is equal to the square of the plant state.","At each time $t$, the scheduler at the sensor has to decide whether it should activate the uplink channel, or downlink.","We pose this dynamic optimization problem as a Markov decision process (MDP), in which the state at time $t$ is composed of (i) the plant state $x(t)$, (ii) the age of the data packet available at the controller, denoted by $\\tau(t)$, (iii) a binary variable $y(t)$ which indicates the availability of a control packet at the controller, and (iv) the energy level of the battery at the sensor $b(t)$. We show that there exists an optimal scheduling policy that exhibits a threshold structure, meaning that for each time $t$, if there is a control packet available with the controller, then the sensor activates the downlink channel in case $|x(t)|$ exceeds a threshold $x\\ust(\\tau(t),b(t))$."],"url":"http://arxiv.org/abs/2403.14189v1","category":"math.OC"}
{"created":"2024-03-21 07:23:37","title":"Stability analysis of the incompressible porous media equation and the Stokes transport system via energy structure","abstract":"Inthispaper,werevisitasymptoticstabilityforthetwo-dimensionalincompressibleporous media equation and the Stokes transport system in a periodic channel. It is well-known that a stratified density, which strictly decreases in the vertical direction, is asymptotically stable under sufficiently small and smooth perturbations. We provide improvements in the regularity assumptions on the perturbation and in the convergence rate. Unlike the standard approach for stability analysis relying on linearized equations, we directly address the nonlinear problem by exploiting the energy structure of each system. While it is widely known that the potential energy is a Lyapunov functional in both systems, our key observation is that the second derivative of the potential energy reveals a (degenerate) coercive structure, which arises from the fact that the solution converges to the minimizer of the energy.","sentences":["Inthispaper,werevisitasymptoticstabilityforthetwo-dimensionalincompressibleporous media equation and the Stokes transport system in a periodic channel.","It is well-known that a stratified density, which strictly decreases in the vertical direction, is asymptotically stable under sufficiently small and smooth perturbations.","We provide improvements in the regularity assumptions on the perturbation and in the convergence rate.","Unlike the standard approach for stability analysis relying on linearized equations, we directly address the nonlinear problem by exploiting the energy structure of each system.","While it is widely known that the potential energy is a Lyapunov functional in both systems, our key observation is that the second derivative of the potential energy reveals a (degenerate) coercive structure, which arises from the fact that the solution converges to the minimizer of the energy."],"url":"http://arxiv.org/abs/2403.14187v1","category":"math.AP"}
{"created":"2024-03-21 07:15:09","title":"A theoretic analysis of magnetoactive GES-based turbulent solar plasma instability","abstract":"A recently reported gravito-electrostatic sheath (GES) model is procedurally applied to study the turbumagnetoactive helioseismic oscillation features on the entire bi-fluidic solar plasma system. The bounded solar interior plasma (SIP, internally self-gravitating) and the unbounded solar wind plasma (SWP, externally point-gravitating) are coupled through the interfacial diffused solar surface boundary (SSB) due to an exact gravito-electrostatic interplay. A numerical platform on the developed theoretic formalism reveals the evolution of both dispersive and non-dispersive features of the modified GES mode fluctuations in new parametric windows. Different colourspectral profiles exhibit important features of the GES-based SIP-SWP perturbations elaborately. It is illustratively shown that the thermostatistical GES stability depends mainly on the radial distance, magnetic field, equilibrium plasma density, and plasma temperature. We see that their dispersive features are more pertinently pronounced in the self-gravitational domains (SIP) than the electrostatic ones (SWP). Besides, different characteristic parameters with accelerating (or decelerating) and stabilizing (or destabilizing) effects influencing the entire solar plasma stability are illustratively portrayed. We speculate that, in the SIP, the long-wave (gravitational-like) helioseismic fluctuations become highly dispersive showing more propagatory nature than the shorter ones (acoustic-like). The short waves show more propagatory propensity than the longer ones in the SSB and SWP regime. The reliability of our proposed investigation is bolstered along with the tentative applicability and future scope in light of the current solar observational scenarios, such as SOHO, STEREO, SDO, PSP, and SolO.","sentences":["A recently reported gravito-electrostatic sheath (GES) model is procedurally applied to study the turbumagnetoactive helioseismic oscillation features on the entire bi-fluidic solar plasma system.","The bounded solar interior plasma (SIP, internally self-gravitating) and the unbounded solar wind plasma (SWP, externally point-gravitating) are coupled through the interfacial diffused solar surface boundary (SSB) due to an exact gravito-electrostatic interplay.","A numerical platform on the developed theoretic formalism reveals the evolution of both dispersive and non-dispersive features of the modified GES mode fluctuations in new parametric windows.","Different colourspectral profiles exhibit important features of the GES-based SIP-SWP perturbations elaborately.","It is illustratively shown that the thermostatistical GES stability depends mainly on the radial distance, magnetic field, equilibrium plasma density, and plasma temperature.","We see that their dispersive features are more pertinently pronounced in the self-gravitational domains (SIP) than the electrostatic ones (SWP).","Besides, different characteristic parameters with accelerating (or decelerating) and stabilizing (or destabilizing) effects influencing the entire solar plasma stability are illustratively portrayed.","We speculate that, in the SIP, the long-wave (gravitational-like) helioseismic fluctuations become highly dispersive showing more propagatory nature than the shorter ones (acoustic-like).","The short waves show more propagatory propensity than the longer ones in the SSB and SWP regime.","The reliability of our proposed investigation is bolstered along with the tentative applicability and future scope in light of the current solar observational scenarios, such as SOHO, STEREO, SDO, PSP, and SolO."],"url":"http://arxiv.org/abs/2403.14182v1","category":"astro-ph.SR"}
{"created":"2024-03-21 06:50:41","title":"Lane level joint control of off-ramp and main line speed guidance on expressway in rainy weather","abstract":"In the upstream of the exit ramp of the expressway, the speed limit difference leads to a significant deceleration of the vehicle in the area adjacent to the off-ramp. The friction coefficient of the road surface decreases under rainy weather, and the above deceleration process can easily lead to sideslip and rollover of the vehicle. Dynamic speed guidance is an effective way to improve the status quo. Currently, there is an emerging trend to utilize I2V technology and high-precision map technology for lane level speed guidance control. This paper presents an optimized joint control strategy for main line-off-ramp speed guidance, which can adjust the guidance speed in real time according to the rainfall intensity. At the same time, this paper designs a progressive deceleration strategy, which works together with the speed guidance control to ensure the safe deceleration of vehicles. The simulation results show that the proposed control strategy outperforms the fixed speed limit control in terms of improving the total traveled time (TTT), total traveled distance (TTD) and standard deviation of speed (SD). Sensitivity analysis shows that the proposed control strategy can improve performance with the increase of the compliance rate of drivers. The speed guidance control method established in this paper can improve the vehicle operation efficiency in the off-ramp area of the expressway and reduce the speed difference of each vehicle in rainy weather, which guarantee the safety of expressway driving in the rainy day.","sentences":["In the upstream of the exit ramp of the expressway, the speed limit difference leads to a significant deceleration of the vehicle in the area adjacent to the off-ramp.","The friction coefficient of the road surface decreases under rainy weather, and the above deceleration process can easily lead to sideslip and rollover of the vehicle.","Dynamic speed guidance is an effective way to improve the status quo.","Currently, there is an emerging trend to utilize I2V technology and high-precision map technology for lane level speed guidance control.","This paper presents an optimized joint control strategy for main line-off-ramp speed guidance, which can adjust the guidance speed in real time according to the rainfall intensity.","At the same time, this paper designs a progressive deceleration strategy, which works together with the speed guidance control to ensure the safe deceleration of vehicles.","The simulation results show that the proposed control strategy outperforms the fixed speed limit control in terms of improving the total traveled time (TTT), total traveled distance (TTD) and standard deviation of speed (SD).","Sensitivity analysis shows that the proposed control strategy can improve performance with the increase of the compliance rate of drivers.","The speed guidance control method established in this paper can improve the vehicle operation efficiency in the off-ramp area of the expressway and reduce the speed difference of each vehicle in rainy weather, which guarantee the safety of expressway driving in the rainy day."],"url":"http://arxiv.org/abs/2403.14172v1","category":"eess.SY"}
{"created":"2024-03-21 06:47:18","title":"Solutions of Pascali systems attached to convex boundaries","abstract":"Given a bounded strictly convex domain $\\Omega\\Subset \\mathbb{C}$ and a point $q\\in \\Omega$ we construct a continuous solution of the Pascali-type elliptic system of differential equations that is centered in $q$, maps the unit disc into $\\Omega$ and the unit circle into $\\partial \\Omega$.","sentences":["Given a bounded strictly convex domain $\\Omega\\Subset \\mathbb{C}$ and a point $q\\in \\Omega$ we construct a continuous solution of the Pascali-type elliptic system of differential equations that is centered in $q$, maps the unit disc into $\\Omega$ and the unit circle into $\\partial \\Omega$."],"url":"http://arxiv.org/abs/2403.14170v1","category":"math.CV"}
{"created":"2024-03-21 06:42:26","title":"Thermodynamical Topology of Quantum BTZ Black Hole","abstract":"Among the study of black hole thermodynamics, topology offers a novel approach and perspective for classifying black hole systems. In this work, we explore the thermodynamical topology of the quantum BTZ black hole by employing the concept of the generalized free energy. To fully characterize the thermodynamics, we introduce two distinct topological numbers. The first one is determined by an expression, denoted by $z$, derived from the free energy. Although it can provide us with some local physical explanations, sufficient physical significance still lacks from a global perspective. On the other hand, the second topological number is based on the entropy expression of the generalized free energy, leading to a more meaningful interpretation of its physical implications. This result highlights the natural choice of entropy as the domain variable for the generalized free energy. Regarding the second topological number, our analysis reveals a topological transition that is associated with the thermodynamical stability of the ``cold\" black hole state of the quantum BTZ black hole. Furthermore, our study suggests the existence of topological numbers beyond the conventional values of $\\pm 1,0$.","sentences":["Among the study of black hole thermodynamics, topology offers a novel approach and perspective for classifying black hole systems.","In this work, we explore the thermodynamical topology of the quantum BTZ black hole by employing the concept of the generalized free energy.","To fully characterize the thermodynamics, we introduce two distinct topological numbers.","The first one is determined by an expression, denoted by $z$, derived from the free energy.","Although it can provide us with some local physical explanations, sufficient physical significance still lacks from a global perspective.","On the other hand, the second topological number is based on the entropy expression of the generalized free energy, leading to a more meaningful interpretation of its physical implications.","This result highlights the natural choice of entropy as the domain variable for the generalized free energy.","Regarding the second topological number, our analysis reveals a topological transition that is associated with the thermodynamical stability of the ``cold\" black hole state of the quantum BTZ black hole.","Furthermore, our study suggests the existence of topological numbers beyond the conventional values of $\\pm 1,0$."],"url":"http://arxiv.org/abs/2403.14167v1","category":"gr-qc"}
{"created":"2024-03-21 06:27:24","title":"On Starlike Functions Associated with a Bean Shaped Domain","abstract":"In this paper, we introduce and explore a new class of starlike functions denoted by $\\mathcal{S}^*_{\\mathfrak{B}}$, defined as follows:   $$\\mathcal{S}^*_{\\mathfrak{B}}=\\{f\\in \\mathcal{A}:zf'(z)/f(z)\\prec \\sqrt{1+\\tanh{z}}=:\\mathfrak{B}(z)\\}.$$   Here, $\\mathfrak{B}(z)$ represents a mapping from the unit disk onto a bean-shaped domain. Our study focuses on understanding the characteristic properties of both $\\mathfrak{B}(z)$ and the functions in $\\mathcal{S}^*_{\\mathfrak{B}}$. We derive sharp conditions under which $\\psi(p)\\prec\\sqrt{1+\\tanh(z)}$ implies $p(z)\\prec ((1+A z)/(1+B z))^\\gamma$, where $\\psi(p)$ is defined as:   \\begin{equation*} (1-\\alpha)p(z)+\\alpha p^2(z)+\\beta \\frac{zp'(z)}{p^k(z)}\\quad \\text{and}\\quad (p(z))^\\delta+\\beta \\frac{zp'(z)}{(p(z))^k}. \\end{equation*} Additionally, we establish inclusion relations involving $\\mathcal{S}^*_{\\mathfrak{B}}$ and derive precise estimates for the sharp radii constants of $\\mathcal{S}^*_{\\mathfrak{B}}$.","sentences":["In this paper, we introduce and explore a new class of starlike functions denoted by $\\mathcal{S}^*_{\\mathfrak{B}}$, defined as follows:   $$\\mathcal{S}^*_{\\mathfrak{B}}=\\{f\\in \\mathcal{A}:zf'(z)/f(z)\\prec \\sqrt{1+\\tanh{z}}=:\\mathfrak{B}(z)\\}.$$   Here, $\\mathfrak{B}(z)$ represents a mapping from the unit disk onto a bean-shaped domain.","Our study focuses on understanding the characteristic properties of both $\\mathfrak{B}(z)$ and the functions in $\\mathcal{S}^*_{\\mathfrak{B}}$.","We derive sharp conditions under which $\\psi(p)\\prec\\sqrt{1+\\tanh(z)}$ implies $p(z)\\prec ((1+A z)/(1+B z))^\\gamma$, where $\\psi(p)$ is defined as:   \\begin{equation*} (1-\\alpha)p(z)+\\alpha p^2(z)+\\beta \\frac{zp'(z)}{p^k(z)}\\quad \\text{and}\\quad (p(z))^\\delta+\\beta \\frac{zp'(z)}{(p(z))^k}.","\\end{equation*} Additionally, we establish inclusion relations involving $\\mathcal{S}^*_{\\mathfrak{B}}$ and derive precise estimates for the sharp radii constants of $\\mathcal{S}^*_{\\mathfrak{B}}$."],"url":"http://arxiv.org/abs/2403.14162v1","category":"math.CV"}
{"created":"2024-03-21 06:23:38","title":"Robust Locomotion via Zero-order Stochastic Nonlinear Model Predictive Control with Guard Saltation Matrix","abstract":"This paper presents a stochastic/robust nonlinear model predictive control (NMPC) to enhance the robustness of legged locomotion against contact uncertainties. We integrate the contact uncertainties into the covariance propagation of stochastic/robust NMPC framework by leveraging the guard saltation matrix and an extended Kalman filter-like covariance update. We achieve fast stochastic/robust NMPC computation by utilizing the zero-order stochastic/robust NMPC algorithm with additional improvements in computational efficiency concerning the feedback gains. We conducted numerical experiments and demonstrate that the proposed method can accurately forecast future state covariance and generate trajectories that satisfies constraints even in the presence of the contact uncertainties. Hardware experiments on the perceptive locomotion of a wheeled-legged robot were also carried out, validating the feasibility of the proposed method in a real-world system with limited on-board computation.","sentences":["This paper presents a stochastic/robust nonlinear model predictive control (NMPC) to enhance the robustness of legged locomotion against contact uncertainties.","We integrate the contact uncertainties into the covariance propagation of stochastic/robust NMPC framework by leveraging the guard saltation matrix and an extended Kalman filter-like covariance update.","We achieve fast stochastic/robust NMPC computation by utilizing the zero-order stochastic/robust NMPC algorithm with additional improvements in computational efficiency concerning the feedback gains.","We conducted numerical experiments and demonstrate that the proposed method can accurately forecast future state covariance and generate trajectories that satisfies constraints even in the presence of the contact uncertainties.","Hardware experiments on the perceptive locomotion of a wheeled-legged robot were also carried out, validating the feasibility of the proposed method in a real-world system with limited on-board computation."],"url":"http://arxiv.org/abs/2403.14159v1","category":"cs.RO"}
{"created":"2024-03-21 06:11:40","title":"Semiclassical asymptotics for Bergman projections with Gevrey weights","abstract":"We extend the direct approach to the semiclassical asymptotics for Bergman projections, developed by Deleporte--Hitrik--Sj\\\"ostrand for real analytic exponential weights and Hitrik--Stone for smooth exponential weights, to the case of Gevrey weights. We prove that the amplitude of the asymptotic Bergman projection forms a Gevrey symbol whose asymptotic coefficients obey certain Gevrey-type growth rate, and it is constructed by an asymptotic inversion of an explicit Fourier integral operator up to a Gevrey-type small remainder.","sentences":["We extend the direct approach to the semiclassical asymptotics for Bergman projections, developed by Deleporte--Hitrik--Sj\\\"ostrand for real analytic exponential weights and Hitrik--Stone for smooth exponential weights, to the case of Gevrey weights.","We prove that the amplitude of the asymptotic Bergman projection forms a Gevrey symbol whose asymptotic coefficients obey certain Gevrey-type growth rate, and it is constructed by an asymptotic inversion of an explicit Fourier integral operator up to a Gevrey-type small remainder."],"url":"http://arxiv.org/abs/2403.14157v1","category":"math.AP"}
{"created":"2024-03-21 06:01:02","title":"LR-FHSS Transceiver for Direct-to-Satellite IoT Communications: Design, Implementation, and Verification","abstract":"This paper proposes a long range-frequency hopping spread spectrum (LR-FHSS) transceiver design for the Direct-to-Satellite Internet of Things (DtS-IoT) communication system. The DtS-IoT system has recently attracted attention as a promising nonterrestrial network (NTN) solution to provide high-traffic and low-latency data transfer services to IoT devices in global coverage. In particular, this study provides guidelines for the overall DtS-IoT system architecture and design details that conform to the Long Range Wide-Area Network (LoRaWAN). Furthermore, we also detail various DtS-IoT use cases. Considering the multiple low-Earth orbit (LEO) satellites, we developed the LR-FHSS transceiver to improve system efficiency, which is the first attempt in real satellite communication systems using LR-FHSS. Moreover, as an extension of our previous work with perfect synchronization, we applied a robust synchronization scheme against the Doppler effect and co-channel interference (CCI) caused by LEO satellite channel environments, including signal detection for the simultaneous reception of numerous frequency hopping signals and an enhanced soft-output-Viterbi-algorithm (SOVA) for the header and payload receptions. Lastly, we present proof-of-concept implementation and testbeds using an application-specific integrated circuit (ASIC) chipset and a field-programmable gate array (FPGA) that verify the performance of the proposed LR-FHSS transceiver design of DtS-IoT communication systems. The laboratory test results reveal that the proposed LR-FHSS-based framework with the robust synchronization technique can provide wide coverage, seamless connectivity, and high throughput communication links for the realization of future sixth-generation (6G) networks.","sentences":["This paper proposes a long range-frequency hopping spread spectrum (LR-FHSS) transceiver design for the Direct-to-Satellite Internet of Things (DtS-IoT) communication system.","The DtS-IoT system has recently attracted attention as a promising nonterrestrial network (NTN) solution to provide high-traffic and low-latency data transfer services to IoT devices in global coverage.","In particular, this study provides guidelines for the overall DtS-IoT system architecture and design details that conform to the Long Range Wide-Area Network (LoRaWAN).","Furthermore, we also detail various DtS-IoT use cases.","Considering the multiple low-Earth orbit (LEO) satellites, we developed the LR-FHSS transceiver to improve system efficiency, which is the first attempt in real satellite communication systems using LR-FHSS.","Moreover, as an extension of our previous work with perfect synchronization, we applied a robust synchronization scheme against the Doppler effect and co-channel interference (CCI) caused by LEO satellite channel environments, including signal detection for the simultaneous reception of numerous frequency hopping signals and an enhanced soft-output-Viterbi-algorithm (SOVA) for the header and payload receptions.","Lastly, we present proof-of-concept implementation and testbeds using an application-specific integrated circuit (ASIC) chipset and a field-programmable gate array (FPGA) that verify the performance of the proposed LR-FHSS transceiver design of DtS-IoT communication systems.","The laboratory test results reveal that the proposed LR-FHSS-based framework with the robust synchronization technique can provide wide coverage, seamless connectivity, and high throughput communication links for the realization of future sixth-generation (6G) networks."],"url":"http://arxiv.org/abs/2403.14154v1","category":"eess.SY"}
{"created":"2024-03-21 05:55:27","title":"A combinatorial view of Holant problems on higher domains","abstract":"On the Boolean domain, there is a class of symmetric signatures called ``Fibonacci gates\" for which a beautiful P-time combinatorial algorithm has been designed for the corresponding $\\operatorname{Holant}^*$ problems.   In this work, we give a combinatorial view for $\\operatorname{Holant}^*(\\mathcal{F})$ problems on a domain of size 3 where $\\mathcal{F}$ is a set of arity 3 functions with inputs taking values on the domain of size 3 and the functions share some common properties. The combinatorial view can also be extended to the domain of size 4.   Specifically, we extend the definition of \"Fibonacci gates\" to the domain of size 3 and the domain of size 4. Moreover, we give the corresponding combinatorial algorithms.","sentences":["On the Boolean domain, there is a class of symmetric signatures called ``Fibonacci gates\" for which a beautiful P-time combinatorial algorithm has been designed for the corresponding $\\operatorname{Holant}^*$ problems.   ","In this work, we give a combinatorial view for $\\operatorname{Holant}^*(\\mathcal{F})$ problems on a domain of size 3 where $\\mathcal{F}$ is a set of arity 3 functions with inputs taking values on the domain of size 3 and the functions share some common properties.","The combinatorial view can also be extended to the domain of size 4.   ","Specifically, we extend the definition of \"Fibonacci gates\" to the domain of size 3 and the domain of size 4.","Moreover, we give the corresponding combinatorial algorithms."],"url":"http://arxiv.org/abs/2403.14150v1","category":"cs.CC"}
{"created":"2024-03-21 05:50:52","title":"Exact analytic expressions for discrete first-passage time probability distributions in Markov networks","abstract":"The first-passage time (FPT) is the time it takes a system variable to cross a given boundary for the first time. In the context of Markov networks, the FPT is the time a random walker takes to reach a particular node (target) by hopping from one node to another. If the walker pauses at each node for a period of time drawn from a continuous distribution, the FPT will be a continuous variable; if the pauses last exactly one unit of time, the FPT will be discrete and equal to the number of hops. We derive an exact analytical expression for the discrete first-passage time (DFPT) in Markov networks. Our approach is as follows: first, we divide each edge (connection between two nodes) of the network into $h$ unidirectional edges connecting a cascade of $h$ fictitious nodes and compute the continuous FPT (CFPT). Second, we set the transition rates along the edges to $h$, and show that as $h\\to\\infty$, the distribution of travel times between any two nodes of the original network approaches a delta function centered at 1, which is equivalent to pauses lasting 1 unit of time. Using this approach, we also compute the joint-probability distributions for the DFPT, the target node, and the node from which the target node was reached. A comparison with simulation confirms the validity of our approach.","sentences":["The first-passage time (FPT) is the time it takes a system variable to cross a given boundary for the first time.","In the context of Markov networks, the FPT is the time a random walker takes to reach a particular node (target) by hopping from one node to another.","If the walker pauses at each node for a period of time drawn from a continuous distribution, the FPT will be a continuous variable; if the pauses last exactly one unit of time, the FPT will be discrete and equal to the number of hops.","We derive an exact analytical expression for the discrete first-passage time (DFPT) in Markov networks.","Our approach is as follows: first, we divide each edge (connection between two nodes) of the network into $h$ unidirectional edges connecting a cascade of $h$ fictitious nodes and compute the continuous FPT (CFPT).","Second, we set the transition rates along the edges to $h$, and show that as $h\\to\\infty$, the distribution of travel times between any two nodes of the original network approaches a delta function centered at 1, which is equivalent to pauses lasting 1 unit of time.","Using this approach, we also compute the joint-probability distributions for the DFPT, the target node, and the node from which the target node was reached.","A comparison with simulation confirms the validity of our approach."],"url":"http://arxiv.org/abs/2403.14149v1","category":"q-bio.MN"}
{"created":"2024-03-21 05:42:42","title":"The transcritical Bogdanov Takens bifurcation with boundary due to the risk perception on a recruitment epidemiological model","abstract":"We analyze an epidemiological model with treatment and recruitment considering the risk perception. In this model, we consider an exponential function as a recruitment rate. We have found that this model undergoes the transcritical Bogdanov-Takens bifurcation with boundary, where the system experiences the transcritical bifurcation between the disease-free equilibrium point and the endemic equilibrium point. The Hopf bifurcation also arises at the endemic equilibrium point, this is, the appearance or disappearance of a limit cycle, and finally, the Homoclinic bifurcation which transforms the limit cycle into a homoclinic cycle, starting and ending at the disease-free equilibrium point.","sentences":["We analyze an epidemiological model with treatment and recruitment considering the risk perception.","In this model, we consider an exponential function as a recruitment rate.","We have found that this model undergoes the transcritical Bogdanov-Takens bifurcation with boundary, where the system experiences the transcritical bifurcation between the disease-free equilibrium point and the endemic equilibrium point.","The Hopf bifurcation also arises at the endemic equilibrium point, this is, the appearance or disappearance of a limit cycle, and finally, the Homoclinic bifurcation which transforms the limit cycle into a homoclinic cycle, starting and ending at the disease-free equilibrium point."],"url":"http://arxiv.org/abs/2403.14147v1","category":"math.DS"}
{"created":"2024-03-21 05:41:20","title":"Realizing topological quantum magnets with atomic spins on surfaces","abstract":"Artificial quantum systems have emerged as indispensable platforms to realize exotic topological matter in a well-controlled manner. Here, we demonstrate topological quantum Heisenberg spin lattices, engineered with spin chains and two-dimensional spin arrays using spin 1/2 atoms on insulating films in a scanning tunnelling microscope (STM). We engineered with atomic precision both topological and trivial phases of the quantum spin model, realizing first- and second-order topological quantum magnets. Their many-body excitations were probed by single-atom electron spin resonance with ultrahigh energy resolution. The atomically-localized magnetic field of the STM tip allows us to directly visualize various topological bound modes including topological edge states, topological defects, and higher-order corner modes. Our results provide an important bottom-up approach to simulating exotic quantum many-body phases of interacting spins.","sentences":["Artificial quantum systems have emerged as indispensable platforms to realize exotic topological matter in a well-controlled manner.","Here, we demonstrate topological quantum Heisenberg spin lattices, engineered with spin chains and two-dimensional spin arrays using spin 1/2 atoms on insulating films in a scanning tunnelling microscope (STM).","We engineered with atomic precision both topological and trivial phases of the quantum spin model, realizing first- and second-order topological quantum magnets.","Their many-body excitations were probed by single-atom electron spin resonance with ultrahigh energy resolution.","The atomically-localized magnetic field of the STM tip allows us to directly visualize various topological bound modes including topological edge states, topological defects, and higher-order corner modes.","Our results provide an important bottom-up approach to simulating exotic quantum many-body phases of interacting spins."],"url":"http://arxiv.org/abs/2403.14145v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-21 05:40:01","title":"Understanding the Ranking Loss for Recommendation with Sparse User Feedback","abstract":"Click-through rate (CTR) prediction holds significant importance in the realm of online advertising. While many existing approaches treat it as a binary classification problem and utilize binary cross entropy (BCE) as the optimization objective, recent advancements have indicated that combining BCE loss with ranking loss yields substantial performance improvements. However, the full efficacy of this combination loss remains incompletely understood. In this paper, we uncover a new challenge associated with BCE loss in scenarios with sparse positive feedback, such as CTR prediction: the gradient vanishing for negative samples. Subsequently, we introduce a novel perspective on the effectiveness of ranking loss in CTR prediction, highlighting its ability to generate larger gradients on negative samples, thereby mitigating their optimization issues and resulting in improved classification ability. Our perspective is supported by extensive theoretical analysis and empirical evaluation conducted on publicly available datasets. Furthermore, we successfully deployed the ranking loss in Tencent's online advertising system, achieving notable lifts of 0.70% and 1.26% in Gross Merchandise Value (GMV) for two main scenarios. The code for our approach is openly accessible at the following GitHub repository: https://github.com/SkylerLinn/Understanding-the-Ranking-Loss.","sentences":["Click-through rate (CTR) prediction holds significant importance in the realm of online advertising.","While many existing approaches treat it as a binary classification problem and utilize binary cross entropy (BCE) as the optimization objective, recent advancements have indicated that combining BCE loss with ranking loss yields substantial performance improvements.","However, the full efficacy of this combination loss remains incompletely understood.","In this paper, we uncover a new challenge associated with BCE loss in scenarios with sparse positive feedback, such as CTR prediction: the gradient vanishing for negative samples.","Subsequently, we introduce a novel perspective on the effectiveness of ranking loss in CTR prediction, highlighting its ability to generate larger gradients on negative samples, thereby mitigating their optimization issues and resulting in improved classification ability.","Our perspective is supported by extensive theoretical analysis and empirical evaluation conducted on publicly available datasets.","Furthermore, we successfully deployed the ranking loss in Tencent's online advertising system, achieving notable lifts of 0.70% and 1.26% in Gross Merchandise Value (GMV) for two main scenarios.","The code for our approach is openly accessible at the following GitHub repository: https://github.com/SkylerLinn/Understanding-the-Ranking-Loss."],"url":"http://arxiv.org/abs/2403.14144v1","category":"cs.IR"}
{"created":"2024-03-21 05:39:58","title":"Early Planet Formation in Embedded Disks (eDisk) XIII: Aligned Disks with Non-Settled Dust Around the Newly Resolved Class 0 Protobinary R CrA IRAS 32","abstract":"Young protostellar binary systems, with expected ages less than $\\sim$10$^5$ years, are little modified since birth, providing key clues to binary formation and evolution. We present a first look at the young, Class 0 binary protostellar system R CrA IRAS 32 from the Early Planet Formation in Embedded Disks (eDisk) ALMA large program, which observed the system in the 1.3 mm continuum emission, $^{12}$CO (2-1), $^{13}$CO (2-1), C$^{18}$O (2-1), SO (6$_5$-5$_4$), and nine other molecular lines that trace disk, envelope, shocks, and outflows. With a continuum resolution of $\\sim$0.03$^{\\prime\\prime}$ ($\\sim$5 au, at a distance of 150 pc), we characterize the newly discovered binary system with a separation of 207 au, their circumstellar disks, and a circumbinary disk-like structure. The circumstellar disk radii are 26.9$\\pm$0.3 and 22.8$\\pm$0.3 au for sources A and B, respectively, and their circumstellar disk dust masses are estimated as 22.5$\\pm$1.1 and 12.4$\\pm$0.6 M$_{\\Earth}$. The circumstellar disks and the circumbinary structure have well aligned position angles and inclinations, indicating formation in a smooth, ordered process such as disk fragmentation. In addition, the circumstellar disks have a near/far-side asymmetry in the continuum emission suggesting that the dust has yet to settle into a thin layer near the midplane. Spectral analysis of CO isotopologues reveals outflows that originate from both of the sources and possibly from the circumbinary disk-like structure. Furthermore, we detect Keplerian rotation in the $^{13}$CO isotopologues toward both circumstellar disks and likely Keplerian rotation in the circumbinary structure; the latter suggests that it is probably a circumbinary disk.","sentences":["Young protostellar binary systems, with expected ages less than $\\sim$10$^5$ years, are little modified since birth, providing key clues to binary formation and evolution.","We present a first look at the young, Class 0 binary protostellar system R CrA IRAS 32 from the Early Planet Formation in Embedded Disks (eDisk) ALMA large program, which observed the system in the 1.3 mm continuum emission, $^{12}$CO (2-1), $^{13}$CO (2-1), C$^{18}$O (2-1), SO (6$_5$-5$_4$), and nine other molecular lines that trace disk, envelope, shocks, and outflows.","With a continuum resolution of $\\sim$0.03$^{\\prime\\prime}$ ($\\sim$5 au, at a distance of 150 pc), we characterize the newly discovered binary system with a separation of 207 au, their circumstellar disks, and a circumbinary disk-like structure.","The circumstellar disk radii are 26.9$\\pm$0.3 and 22.8$\\pm$0.3 au for sources A and B, respectively, and their circumstellar disk dust masses are estimated as 22.5$\\pm$1.1 and 12.4$\\pm$0.6","M$_{\\Earth}$.","The circumstellar disks and the circumbinary structure have well aligned position angles and inclinations, indicating formation in a smooth, ordered process such as disk fragmentation.","In addition, the circumstellar disks have a near/far-side asymmetry in the continuum emission suggesting that the dust has yet to settle into a thin layer near the midplane.","Spectral analysis of CO isotopologues reveals outflows that originate from both of the sources and possibly from the circumbinary disk-like structure.","Furthermore, we detect Keplerian rotation in the $^{13}$CO isotopologues toward both circumstellar disks and likely Keplerian rotation in the circumbinary structure; the latter suggests that it is probably a circumbinary disk."],"url":"http://arxiv.org/abs/2403.14143v1","category":"astro-ph.SR"}
{"created":"2024-03-21 05:36:25","title":"Empowering Segmentation Ability to Multi-modal Large Language Models","abstract":"Multi-modal large language models (MLLMs) can understand image-language prompts and demonstrate impressive reasoning ability. In this paper, we extend MLLMs' output by empowering MLLMs with the segmentation ability. The extended MLLMs can both output language responses to the image-language prompts and segment the regions that the complex question or query in the language prompts focuses on. To this end, the existing work, LISA, enlarges the original word embeddings with an additional segment token and fine-tunes dialogue generation and query-focused segmentation together, where the feature of the segment token is used to prompt the segment-anything model. Although they achieve superior segmentation performance, we observe that the dialogue ability decreases by a large margin compared to the original MLLMs. To maintain the original MLLMs' dialogue ability, we propose a novel MLLMs framework, coined as LLaVASeg, which leverages a chain-of-thought prompting strategy to instruct the MLLMs to segment the target region queried by the user. The MLLMs are first prompted to reason about the simple description of the target region from the complicated user query, then extract the visual attributes of the target region according to the understanding of MLLMs to the image. These visual attributes, such as color and relative locations, are utilized to prompt the downstream segmentation model. Experiments show that the proposed method keeps the original dialogue ability and equips the MLLMs' model with strong reasoning segmentation ability. The code is available at https://github.com/YuqiYang213/LLaVASeg.","sentences":["Multi-modal large language models (MLLMs) can understand image-language prompts and demonstrate impressive reasoning ability.","In this paper, we extend MLLMs' output by empowering MLLMs with the segmentation ability.","The extended MLLMs can both output language responses to the image-language prompts and segment the regions that the complex question or query in the language prompts focuses on.","To this end, the existing work, LISA, enlarges the original word embeddings with an additional segment token and fine-tunes dialogue generation and query-focused segmentation together, where the feature of the segment token is used to prompt the segment-anything model.","Although they achieve superior segmentation performance, we observe that the dialogue ability decreases by a large margin compared to the original MLLMs.","To maintain the original MLLMs' dialogue ability, we propose a novel MLLMs framework, coined as LLaVASeg, which leverages a chain-of-thought prompting strategy to instruct the MLLMs to segment the target region queried by the user.","The MLLMs are first prompted to reason about the simple description of the target region from the complicated user query, then extract the visual attributes of the target region according to the understanding of MLLMs to the image.","These visual attributes, such as color and relative locations, are utilized to prompt the downstream segmentation model.","Experiments show that the proposed method keeps the original dialogue ability and equips the MLLMs' model with strong reasoning segmentation ability.","The code is available at https://github.com/YuqiYang213/LLaVASeg."],"url":"http://arxiv.org/abs/2403.14141v1","category":"cs.CV"}
{"created":"2024-03-21 05:17:22","title":"Genetic Programming for Explainable Manifold Learning","abstract":"Manifold learning techniques play a pivotal role in machine learning by revealing lower-dimensional embeddings within high-dimensional data, thus enhancing both the efficiency and interpretability of data analysis by transforming the data into a lower-dimensional representation. However, a notable challenge with current manifold learning methods is their lack of explicit functional mappings, crucial for explainability in many real-world applications. Genetic programming, known for its interpretable functional tree-based models, has emerged as a promising approach to address this challenge. Previous research leveraged multi-objective GP to balance manifold quality against embedding dimensionality, producing functional mappings across a range of embedding sizes. Yet, these mapping trees often became complex, hindering explainability. In response, in this paper, we introduce Genetic Programming for Explainable Manifold Learning (GP-EMaL), a novel approach that directly penalises tree complexity. Our new method is able to maintain high manifold quality while significantly enhancing explainability and also allows customisation of complexity measures, such as symmetry balancing, scaling, and node complexity, catering to diverse application needs. Our experimental analysis demonstrates that GP-EMaL is able to match the performance of the existing approach in most cases, while using simpler, smaller, and more interpretable tree structures. This advancement marks a significant step towards achieving interpretable manifold learning.","sentences":["Manifold learning techniques play a pivotal role in machine learning by revealing lower-dimensional embeddings within high-dimensional data, thus enhancing both the efficiency and interpretability of data analysis by transforming the data into a lower-dimensional representation.","However, a notable challenge with current manifold learning methods is their lack of explicit functional mappings, crucial for explainability in many real-world applications.","Genetic programming, known for its interpretable functional tree-based models, has emerged as a promising approach to address this challenge.","Previous research leveraged multi-objective GP to balance manifold quality against embedding dimensionality, producing functional mappings across a range of embedding sizes.","Yet, these mapping trees often became complex, hindering explainability.","In response, in this paper, we introduce Genetic Programming for Explainable Manifold Learning (GP-EMaL), a novel approach that directly penalises tree complexity.","Our new method is able to maintain high manifold quality while significantly enhancing explainability and also allows customisation of complexity measures, such as symmetry balancing, scaling, and node complexity, catering to diverse application needs.","Our experimental analysis demonstrates that GP-EMaL is able to match the performance of the existing approach in most cases, while using simpler, smaller, and more interpretable tree structures.","This advancement marks a significant step towards achieving interpretable manifold learning."],"url":"http://arxiv.org/abs/2403.14139v1","category":"cs.NE"}
{"created":"2024-03-21 04:55:37","title":"Efficient Learning Strategy for Predicting Glass Forming Ability in Imbalanced Datasets of Bulk Metallic Glasses","abstract":"The prediction of glass forming ability (GFA) and various properties in bulk metallic glasses (BMGs) pose a challenge due to the unique disordered atomic structure in this type of materials. Machine learning shows the potential ability to find a way out. However, the training set from the experimental data of BMGs faces the issue of data imbalance, including the distribution of data related to elements, the range of performance data, and the distribution of sparse and dense data area in each specific system. In this work, the origin of the data imbalance and its impact on the GFA prediction ability of machine learning models are analyzed. We propose the solutions by training the model using the pruned dataset to mitigate the imbalance and by performing an active experimental iterative learning to compensate for the information loss resulting from data reduction. The strategy is proved in Zr-Al-Cu system, and the automated workflow has been established. It effectively avoids the prediction results from trapping into the intensive training data area or from inducing by the data distribution of similar element systems. This approach will expedite the development of new BMGs compositions especially for unexplored systems.","sentences":["The prediction of glass forming ability (GFA) and various properties in bulk metallic glasses (BMGs) pose a challenge due to the unique disordered atomic structure in this type of materials.","Machine learning shows the potential ability to find a way out.","However, the training set from the experimental data of BMGs faces the issue of data imbalance, including the distribution of data related to elements, the range of performance data, and the distribution of sparse and dense data area in each specific system.","In this work, the origin of the data imbalance and its impact on the GFA prediction ability of machine learning models are analyzed.","We propose the solutions by training the model using the pruned dataset to mitigate the imbalance and by performing an active experimental iterative learning to compensate for the information loss resulting from data reduction.","The strategy is proved in Zr-Al-Cu system, and the automated workflow has been established.","It effectively avoids the prediction results from trapping into the intensive training data area or from inducing by the data distribution of similar element systems.","This approach will expedite the development of new BMGs compositions especially for unexplored systems."],"url":"http://arxiv.org/abs/2403.14131v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-21 04:53:48","title":"Thermal Conductivity Calculation using Homogeneous Non-equilibrium Molecular Dynamics Simulation with Allegro","abstract":"In this study, we derive the heat flux formula for the Allegro model, one of machine-learning interatomic potentials using the equivariant deep neural network, to calculate lattice thermal conductivity using the homogeneous non-equilibrium molecular dynamics (HNEMD) method based on the Green-Kubo formula. Allegro can construct more advanced atomic descriptors than conventional ones, and can be applied to multicomponent and large-scale systems, providing a significant advantage in estimating the thermal conductivity of anharmonic materials, such as thermoelectric materials. In addition, the spectral heat current (SHC) method, recently developed for the HNEMD framework (HNEMD-SHC), allows the calculation of not only the total thermal conductivity but also its frequency components. The verification of the heat flux and the demonstration of HNEMD-SHC method are performed for the extremely anharmonic low-temperature phase of Ag$_2$Se.","sentences":["In this study, we derive the heat flux formula for the Allegro model, one of machine-learning interatomic potentials using the equivariant deep neural network, to calculate lattice thermal conductivity using the homogeneous non-equilibrium molecular dynamics (HNEMD) method based on the Green-Kubo formula.","Allegro can construct more advanced atomic descriptors than conventional ones, and can be applied to multicomponent and large-scale systems, providing a significant advantage in estimating the thermal conductivity of anharmonic materials, such as thermoelectric materials.","In addition, the spectral heat current (SHC) method, recently developed for the HNEMD framework (HNEMD-SHC), allows the calculation of not only the total thermal conductivity but also its frequency components.","The verification of the heat flux and the demonstration of HNEMD-SHC method are performed for the extremely anharmonic low-temperature phase of Ag$_2$Se."],"url":"http://arxiv.org/abs/2403.14130v1","category":"physics.comp-ph"}
{"created":"2024-03-21 04:52:22","title":"Effect of Substitution Group on Intramolecular Hydrogen Bond of Amino Alcohols from Raman spectroscopy","abstract":"Due to the simultaneous presence of two polar functional groups and flexible spatial structure, Aminoethanol (AE) is a model system for investigating the relationship between intramolecular hydrogen bonding and conformational equlibrium. In addition, Aminoethanol and their derivatives exhibit remarkable efficacy in the reversible capture of carbon dioxide. The intramoleculr hydrogen bond of 2-AE is determined by a subtle balance between electrostatic interactions, Van der Waals interactions, and steric effects. Changing the polarity of functional groups can regulate the strength of intramolecular hydrogen bonds. In this work, using spontaneous Raman spectroscopy combined with theoretical calculations, we investigated the effect of N-terminated substitution group on intramolecular hydrogen bond. When the H atom of NH2 functional group is replaced by electron-donating groups such as methyl and ethyl, it was observed experimentally that the red-shift of OH stretching vibration frequency caused by O-H... N intramolecular hydrogen bonding increases significantly and then the corresponding peak intensity increases. This indicates that with the introduction of substitutions on the N atom, the O-H... N intramolecular hydrogen bond in 2-AE is enhanced and the corresponding conformational population increases. The results of AIM and NCI analysis are consistent with experimental observations. These results provide insights for regulating the strength of intramolecular hydrogen bonds and also contribute to the strategy of CO2 capture.","sentences":["Due to the simultaneous presence of two polar functional groups and flexible spatial structure, Aminoethanol (AE) is a model system for investigating the relationship between intramolecular hydrogen bonding and conformational equlibrium.","In addition, Aminoethanol and their derivatives exhibit remarkable efficacy in the reversible capture of carbon dioxide.","The intramoleculr hydrogen bond of 2-AE is determined by a subtle balance between electrostatic interactions, Van der Waals interactions, and steric effects.","Changing the polarity of functional groups can regulate the strength of intramolecular hydrogen bonds.","In this work, using spontaneous Raman spectroscopy combined with theoretical calculations, we investigated the effect of N-terminated substitution group on intramolecular hydrogen bond.","When the H atom of NH2 functional group is replaced by electron-donating groups such as methyl and ethyl, it was observed experimentally that the red-shift of OH stretching vibration frequency caused by O-H... N intramolecular hydrogen bonding increases significantly and then the corresponding peak intensity increases.","This indicates that with the introduction of substitutions on the N atom, the O-H... N intramolecular hydrogen bond in 2-AE is enhanced and the corresponding conformational population increases.","The results of AIM and NCI analysis are consistent with experimental observations.","These results provide insights for regulating the strength of intramolecular hydrogen bonds and also contribute to the strategy of CO2 capture."],"url":"http://arxiv.org/abs/2403.14129v1","category":"physics.chem-ph"}
{"created":"2024-03-21 04:42:04","title":"Learning causal graphs using variable grouping according to ancestral relationship","abstract":"Several causal discovery algorithms have been proposed. However, when the sample size is small relative to the number of variables, the accuracy of estimating causal graphs using existing methods decreases. And some methods are not feasible when the sample size is smaller than the number of variables. To circumvent these problems, some researchers proposed causal structure learning algorithms using divide-and-conquer approaches. For learning the entire causal graph, the approaches first split variables into several subsets according to the conditional independence relationships among the variables, then apply a conventional causal discovery algorithm to each subset and merge the estimated results. Since the divide-and-conquer approach reduces the number of variables to which a causal structure learning algorithm is applied, it is expected to improve the estimation accuracy of causal graphs, especially when the sample size is small relative to the number of variables and the model is sparse. However, existing methods are either computationally expensive or do not provide sufficient accuracy when the sample size is small. This paper proposes a new algorithm for grouping variables based the ancestral relationships among the variables, under the LiNGAM assumption, where the causal relationships are linear, and the mutually independent noise are distributed as continuous non-Gaussian distributions. We call the proposed algorithm CAG. The time complexity of the ancestor finding in CAG is shown to be cubic to the number of variables. Extensive computer experiments confirm that the proposed method outperforms the original DirectLiNGAM without grouping variables and other divide-and-conquer approaches not only in estimation accuracy but also in computation time when the sample size is small relative to the number of variables and the model is sparse.","sentences":["Several causal discovery algorithms have been proposed.","However, when the sample size is small relative to the number of variables, the accuracy of estimating causal graphs using existing methods decreases.","And some methods are not feasible when the sample size is smaller than the number of variables.","To circumvent these problems, some researchers proposed causal structure learning algorithms using divide-and-conquer approaches.","For learning the entire causal graph, the approaches first split variables into several subsets according to the conditional independence relationships among the variables, then apply a conventional causal discovery algorithm to each subset and merge the estimated results.","Since the divide-and-conquer approach reduces the number of variables to which a causal structure learning algorithm is applied, it is expected to improve the estimation accuracy of causal graphs, especially when the sample size is small relative to the number of variables and the model is sparse.","However, existing methods are either computationally expensive or do not provide sufficient accuracy when the sample size is small.","This paper proposes a new algorithm for grouping variables based the ancestral relationships among the variables, under the LiNGAM assumption, where the causal relationships are linear, and the mutually independent noise are distributed as continuous non-Gaussian distributions.","We call the proposed algorithm CAG.","The time complexity of the ancestor finding in CAG is shown to be cubic to the number of variables.","Extensive computer experiments confirm that the proposed method outperforms the original DirectLiNGAM without grouping variables and other divide-and-conquer approaches not only in estimation accuracy but also in computation time when the sample size is small relative to the number of variables and the model is sparse."],"url":"http://arxiv.org/abs/2403.14125v1","category":"stat.ML"}
{"created":"2024-03-21 04:42:04","title":"Sub-Nyquist Sampling OFDM Radar With a Time-Frequency Phase-Coded Waveform","abstract":"This paper presents a time-frequency phase-coded sub-Nyquist sampling orthogonal frequency division multiplexing (PC-SNS-OFDM) radar system to reduce the analog-to-digital converter (ADC) sampling rate without any additional hardware or signal processing. The proposed radar divides the transmitted OFDM signal into multiple sub-bands along the frequency axis and provides orthogonality to these sub-bands by multiplying phase codes in both the time and frequency domains. Although the sampling rate is reduced by the factor of the number of sub-bands, the sub-bands above the sampling rate are folded into the lowest one due to aliasing. In the process of restoring the signals in folded sub-bands to those in full signal bands, the proposed PC-SNS-OFDM radar effectively eliminates symbol-mismatch noise while introducing trade-offs in the range and Doppler ambiguities. The utilization of phase codes in both the frequency and time domains provides flexible control of the range and Doppler ambiguities. It also improves the signal-to-noise ratio (SNR) of detected targets compared to an earlier sub-Nyquist sampling OFDM radar system. This is validated with simulations and experiments under various sub-Nyquist sampling rates.","sentences":["This paper presents a time-frequency phase-coded sub-Nyquist sampling orthogonal frequency division multiplexing (PC-SNS-OFDM) radar system to reduce the analog-to-digital converter (ADC) sampling rate without any additional hardware or signal processing.","The proposed radar divides the transmitted OFDM signal into multiple sub-bands along the frequency axis and provides orthogonality to these sub-bands by multiplying phase codes in both the time and frequency domains.","Although the sampling rate is reduced by the factor of the number of sub-bands, the sub-bands above the sampling rate are folded into the lowest one due to aliasing.","In the process of restoring the signals in folded sub-bands to those in full signal bands, the proposed PC-SNS-OFDM radar effectively eliminates symbol-mismatch noise while introducing trade-offs in the range and Doppler ambiguities.","The utilization of phase codes in both the frequency and time domains provides flexible control of the range and Doppler ambiguities.","It also improves the signal-to-noise ratio (SNR) of detected targets compared to an earlier sub-Nyquist sampling OFDM radar system.","This is validated with simulations and experiments under various sub-Nyquist sampling rates."],"url":"http://arxiv.org/abs/2403.14126v1","category":"eess.SP"}
{"created":"2024-03-21 04:34:24","title":"Soft Masked Transformer for Point Cloud Processing with Skip Attention-Based Upsampling","abstract":"Point cloud processing methods leverage local and global point features %at the feature level to cater to downstream tasks, yet they often overlook the task-level context inherent in point clouds during the encoding stage. We argue that integrating task-level information into the encoding stage significantly enhances performance. To that end, we propose SMTransformer which incorporates task-level information into a vector-based transformer by utilizing a soft mask generated from task-level queries and keys to learn the attention weights. Additionally, to facilitate effective communication between features from the encoding and decoding layers in high-level tasks such as segmentation, we introduce a skip-attention-based up-sampling block. This block dynamically fuses features from various resolution points across the encoding and decoding layers. To mitigate the increase in network parameters and training time resulting from the complexity of the aforementioned blocks, we propose a novel shared position encoding strategy. This strategy allows various transformer blocks to share the same position information over the same resolution points, thereby reducing network parameters and training time without compromising accuracy.Experimental comparisons with existing methods on multiple datasets demonstrate the efficacy of SMTransformer and skip-attention-based up-sampling for point cloud processing tasks, including semantic segmentation and classification. In particular, we achieve state-of-the-art semantic segmentation results of 73.4% mIoU on S3DIS Area 5 and 62.4% mIoU on SWAN dataset","sentences":["Point cloud processing methods leverage local and global point features %at the feature level to cater to downstream tasks, yet they often overlook the task-level context inherent in point clouds during the encoding stage.","We argue that integrating task-level information into the encoding stage significantly enhances performance.","To that end, we propose SMTransformer which incorporates task-level information into a vector-based transformer by utilizing a soft mask generated from task-level queries and keys to learn the attention weights.","Additionally, to facilitate effective communication between features from the encoding and decoding layers in high-level tasks such as segmentation, we introduce a skip-attention-based up-sampling block.","This block dynamically fuses features from various resolution points across the encoding and decoding layers.","To mitigate the increase in network parameters and training time resulting from the complexity of the aforementioned blocks, we propose a novel shared position encoding strategy.","This strategy allows various transformer blocks to share the same position information over the same resolution points, thereby reducing network parameters and training time without compromising accuracy.","Experimental comparisons with existing methods on multiple datasets demonstrate the efficacy of SMTransformer and skip-attention-based up-sampling for point cloud processing tasks, including semantic segmentation and classification.","In particular, we achieve state-of-the-art semantic segmentation results of 73.4% mIoU on S3DIS Area 5 and 62.4% mIoU on SWAN dataset"],"url":"http://arxiv.org/abs/2403.14124v1","category":"cs.CV"}
{"created":"2024-03-21 04:24:49","title":"External Knowledge Enhanced 3D Scene Generation from Sketch","abstract":"Generating realistic 3D scenes is challenging due to the complexity of room layouts and object geometries.We propose a sketch based knowledge enhanced diffusion architecture (SEK) for generating customized, diverse, and plausible 3D scenes. SEK conditions the denoising process with a hand-drawn sketch of the target scene and cues from an object relationship knowledge base. We first construct an external knowledge base containing object relationships and then leverage knowledge enhanced graph reasoning to assist our model in understanding hand-drawn sketches. A scene is represented as a combination of 3D objects and their relationships, and then incrementally diffused to reach a Gaussian distribution.We propose a 3D denoising scene transformer that learns to reverse the diffusion process, conditioned by a hand-drawn sketch along with knowledge cues, to regressively generate the scene including the 3D object instances as well as their layout. Experiments on the 3D-FRONT dataset show that our model improves FID, CKL by 17.41%, 37.18% in 3D scene generation and FID, KID by 19.12%, 20.06% in 3D scene completion compared to the nearest competitor DiffuScene.","sentences":["Generating realistic 3D scenes is challenging due to the complexity of room layouts and object geometries.","We propose a sketch based knowledge enhanced diffusion architecture (SEK) for generating customized, diverse, and plausible 3D scenes.","SEK conditions the denoising process with a hand-drawn sketch of the target scene and cues from an object relationship knowledge base.","We first construct an external knowledge base containing object relationships and then leverage knowledge enhanced graph reasoning to assist our model in understanding hand-drawn sketches.","A scene is represented as a combination of 3D objects and their relationships, and then incrementally diffused to reach a Gaussian distribution.","We propose a 3D denoising scene transformer that learns to reverse the diffusion process, conditioned by a hand-drawn sketch along with knowledge cues, to regressively generate the scene including the 3D object instances as well as their layout.","Experiments on the 3D-FRONT dataset show that our model improves FID, CKL by 17.41%, 37.18% in 3D scene generation and FID, KID by 19.12%, 20.06% in 3D scene completion compared to the nearest competitor DiffuScene."],"url":"http://arxiv.org/abs/2403.14121v1","category":"cs.CV"}
{"created":"2024-03-21 17:52:44","title":"Alfv\u00e9n Pulse Driven Spicule-like Jets in the Presence of Thermal Conduction and Ion-Neutral Collision in Two-Fluid Regime","abstract":"We present the formation of quasi-periodic cool spicule-like jets in the solar atmosphere using 2.5-D numerical simulation in two-fluid regime (ions+neutrals) under the presence of thermal conduction and ion-neutral collision. The non-linear, impulsive Alfv\\'enic perturbations at the top of the photosphere trigger field aligned magnetoacoustic perturbations due to ponderomotive force. The transport of energy from Alfv\\'en pulse to such vertical velocity perturbations due to ponderomotive force is considered as an initial trigger mechanism. Thereafter, these velocity perturbations steepen into the shocks followed by quasi-periodic rise and fall of the cool jets transporting mass in the overlying corona.","sentences":["We present the formation of quasi-periodic cool spicule-like jets in the solar atmosphere using 2.5-D numerical simulation in two-fluid regime (ions+neutrals) under the presence of thermal conduction and ion-neutral collision.","The non-linear, impulsive Alfv\\'enic perturbations at the top of the photosphere trigger field aligned magnetoacoustic perturbations due to ponderomotive force.","The transport of energy from Alfv\\'en pulse to such vertical velocity perturbations due to ponderomotive force is considered as an initial trigger mechanism.","Thereafter, these velocity perturbations steepen into the shocks followed by quasi-periodic rise and fall of the cool jets transporting mass in the overlying corona."],"url":"http://arxiv.org/abs/2403.14603v1","category":"astro-ph.SR"}
{"created":"2024-03-21 17:48:38","title":"Rethinking Adversarial Inverse Reinforcement Learning: From the Angles of Policy Imitation and Transferable Reward Recovery","abstract":"Adversarial inverse reinforcement learning (AIRL) stands as a cornerstone approach in imitation learning. This paper rethinks the two different angles of AIRL: policy imitation and transferable reward recovery. We begin with substituting the built-in algorithm in AIRL with soft actor-critic (SAC) during the policy optimization process to enhance sample efficiency, thanks to the off-policy formulation of SAC and identifiable Markov decision process (MDP) models with respect to AIRL. It indeed exhibits a significant improvement in policy imitation but accidentally brings drawbacks to transferable reward recovery. To learn this issue, we illustrate that the SAC algorithm itself is not feasible to disentangle the reward function comprehensively during the AIRL training process, and propose a hybrid framework, PPO-AIRL + SAC, for satisfactory transfer effect. Additionally, we analyze the capability of environments to extract disentangled rewards from an algebraic theory perspective.","sentences":["Adversarial inverse reinforcement learning (AIRL) stands as a cornerstone approach in imitation learning.","This paper rethinks the two different angles of AIRL: policy imitation and transferable reward recovery.","We begin with substituting the built-in algorithm in AIRL with soft actor-critic (SAC) during the policy optimization process to enhance sample efficiency, thanks to the off-policy formulation of SAC and identifiable Markov decision process (MDP) models with respect to AIRL.","It indeed exhibits a significant improvement in policy imitation but accidentally brings drawbacks to transferable reward recovery.","To learn this issue, we illustrate that the SAC algorithm itself is not feasible to disentangle the reward function comprehensively during the AIRL training process, and propose a hybrid framework, PPO-AIRL + SAC, for satisfactory transfer effect.","Additionally, we analyze the capability of environments to extract disentangled rewards from an algebraic theory perspective."],"url":"http://arxiv.org/abs/2403.14593v1","category":"cs.LG"}
{"created":"2024-03-21 17:42:45","title":"An Analysis of Linear Time Series Forecasting Models","abstract":"Despite their simplicity, linear models perform well at time series forecasting, even when pitted against deeper and more expensive models. A number of variations to the linear model have been proposed, often including some form of feature normalisation that improves model generalisation. In this paper we analyse the sets of functions expressible using these linear model architectures. In so doing we show that several popular variants of linear models for time series forecasting are equivalent and functionally indistinguishable from standard, unconstrained linear regression. We characterise the model classes for each linear variant. We demonstrate that each model can be reinterpreted as unconstrained linear regression over a suitably augmented feature set, and therefore admit closed-form solutions when using a mean-squared loss function. We provide experimental evidence that the models under inspection learn nearly identical solutions, and finally demonstrate that the simpler closed form solutions are superior forecasters across 72% of test settings.","sentences":["Despite their simplicity, linear models perform well at time series forecasting, even when pitted against deeper and more expensive models.","A number of variations to the linear model have been proposed, often including some form of feature normalisation that improves model generalisation.","In this paper we analyse the sets of functions expressible using these linear model architectures.","In so doing we show that several popular variants of linear models for time series forecasting are equivalent and functionally indistinguishable from standard, unconstrained linear regression.","We characterise the model classes for each linear variant.","We demonstrate that each model can be reinterpreted as unconstrained linear regression over a suitably augmented feature set, and therefore admit closed-form solutions when using a mean-squared loss function.","We provide experimental evidence that the models under inspection learn nearly identical solutions, and finally demonstrate that the simpler closed form solutions are superior forecasters across 72% of test settings."],"url":"http://arxiv.org/abs/2403.14587v1","category":"cs.LG"}
{"created":"2024-03-21 17:33:35","title":"Tomographic redshift dipole: Testing the cosmological principle","abstract":"The cosmological principle posits that the universe is statistically homogeneous and isotropic on large scales, implying all matter share the same rest frame. This principle suggests that velocity estimates of our motion from various sources should agree with the cosmic microwave background (CMB) dipole's inferred velocity of 370 km/s. Yet, for over two decades, analyses of different radio galaxy and quasar catalogs have reported velocities with amplitudes in notable tension with the CMB dipole. In a blind analysis of BOSS and eBOSS spectroscopic data from galaxies and quasars across $0.2<z<2.2$, we applied a novel dipole estimator for a tomographic approach, robustly correcting biases and quantifying uncertainties with state-of-the-art mock catalogs. Our results, indicating a velocity of $v = 353^{+123}_{-111}$ km/s, closely align with the CMB dipole, demonstrating a $1.4\\sigma$ agreement. This finding provides significant empirical support for the cosmological principle, affirming our motion's consistency with the CMB across vast cosmic distances.","sentences":["The cosmological principle posits that the universe is statistically homogeneous and isotropic on large scales, implying all matter share the same rest frame.","This principle suggests that velocity estimates of our motion from various sources should agree with the cosmic microwave background (CMB) dipole's inferred velocity of 370 km/s. Yet, for over two decades, analyses of different radio galaxy and quasar catalogs have reported velocities with amplitudes in notable tension with the CMB dipole.","In a blind analysis of BOSS and eBOSS spectroscopic data from galaxies and quasars across $0.2<z<2.2$, we applied a novel dipole estimator for a tomographic approach, robustly correcting biases and quantifying uncertainties with state-of-the-art mock catalogs.","Our results, indicating a velocity of $v = 353^{+123}_{-111}$ km/s, closely align with the CMB dipole, demonstrating a $1.4\\sigma$ agreement.","This finding provides significant empirical support for the cosmological principle, affirming our motion's consistency with the CMB across vast cosmic distances."],"url":"http://arxiv.org/abs/2403.14580v1","category":"astro-ph.CO"}
{"created":"2024-03-21 17:31:57","title":"On the minimization of the Willmore energy under a constraint on total mean curvature and area","abstract":"Motivated by a model for lipid bilayer cell membranes, we study the minimization of the Willmore functional in the class of oriented closed surfaces with prescribed total mean curvature, prescribed area, and prescribed genus. Adapting methods previously developed by Keller-Mondino-Rivi\\`ere, Bauer-Kuwert, and Ndiaye-Sch\\\"atzle, we prove existence of smooth minimizers for a large class of constraints. Moreover, we analyze the asymptotic behaviour of the energy profile close to the unit sphere and consider the total mean curvature of axisymmetric surfaces.","sentences":["Motivated by a model for lipid bilayer cell membranes, we study the minimization of the Willmore functional in the class of oriented closed surfaces with prescribed total mean curvature, prescribed area, and prescribed genus.","Adapting methods previously developed by Keller-Mondino-Rivi\\`ere, Bauer-Kuwert, and Ndiaye-Sch\\\"atzle, we prove existence of smooth minimizers for a large class of constraints.","Moreover, we analyze the asymptotic behaviour of the energy profile close to the unit sphere and consider the total mean curvature of axisymmetric surfaces."],"url":"http://arxiv.org/abs/2403.14579v1","category":"math.DG"}
{"created":"2024-03-21 17:30:59","title":"RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants in the Biomedical Domain","abstract":"Large Language Models (LLMs) increasingly support applications in a wide range of domains, some with potential high societal impact such as biomedicine, yet their reliability in realistic use cases is under-researched. In this work we introduce the Reliability AssesMent for Biomedical LLM Assistants (RAmBLA) framework and evaluate whether four state-of-the-art foundation LLMs can serve as reliable assistants in the biomedical domain. We identify prompt robustness, high recall, and a lack of hallucinations as necessary criteria for this use case. We design shortform tasks and tasks requiring LLM freeform responses mimicking real-world user interactions. We evaluate LLM performance using semantic similarity with a ground truth response, through an evaluator LLM.","sentences":["Large Language Models (LLMs) increasingly support applications in a wide range of domains, some with potential high societal impact such as biomedicine, yet their reliability in realistic use cases is under-researched.","In this work we introduce the Reliability AssesMent for Biomedical LLM Assistants (RAmBLA) framework and evaluate whether four state-of-the-art foundation LLMs can serve as reliable assistants in the biomedical domain.","We identify prompt robustness, high recall, and a lack of hallucinations as necessary criteria for this use case.","We design shortform tasks and tasks requiring LLM freeform responses mimicking real-world user interactions.","We evaluate LLM performance using semantic similarity with a ground truth response, through an evaluator LLM."],"url":"http://arxiv.org/abs/2403.14578v1","category":"cs.LG"}
{"created":"2024-03-21 16:49:20","title":"DINO-Tracker: Taming DINO for Self-Supervised Point Tracking in a Single Video","abstract":"We present DINO-Tracker -- a new framework for long-term dense tracking in video. The pillar of our approach is combining test-time training on a single video, with the powerful localized semantic features learned by a pre-trained DINO-ViT model. Specifically, our framework simultaneously adopts DINO's features to fit to the motion observations of the test video, while training a tracker that directly leverages the refined features. The entire framework is trained end-to-end using a combination of self-supervised losses, and regularization that allows us to retain and benefit from DINO's semantic prior. Extensive evaluation demonstrates that our method achieves state-of-the-art results on known benchmarks. DINO-tracker significantly outperforms self-supervised methods and is competitive with state-of-the-art supervised trackers, while outperforming them in challenging cases of tracking under long-term occlusions.","sentences":["We present DINO-Tracker -- a new framework for long-term dense tracking in video.","The pillar of our approach is combining test-time training on a single video, with the powerful localized semantic features learned by a pre-trained DINO-ViT model.","Specifically, our framework simultaneously adopts DINO's features to fit to the motion observations of the test video, while training a tracker that directly leverages the refined features.","The entire framework is trained end-to-end using a combination of self-supervised losses, and regularization that allows us to retain and benefit from DINO's semantic prior.","Extensive evaluation demonstrates that our method achieves state-of-the-art results on known benchmarks.","DINO-tracker significantly outperforms self-supervised methods and is competitive with state-of-the-art supervised trackers, while outperforming them in challenging cases of tracking under long-term occlusions."],"url":"http://arxiv.org/abs/2403.14548v1","category":"cs.CV"}
{"created":"2024-03-21 16:42:17","title":"A Note on the Balance Laws of Nonlinear Hyperelasticity","abstract":"It is known that the balance laws of hyperelasticity (Green elasticity), i.e., conservation of mass and balance of linear and angular momenta, can be derived using the first law of thermodynamics by postulating its invariance under rigid body motions of the Euclidean ambient space -- the Green-Naghdi-Rivlin theorem. In the case of a non-Euclidean ambient space, covariance of the energy balance -- its invariance under arbitrary diffeomorphisms of the ambient space -- gives all the balance laws and the Doyle-Ericksen formula -- the Marsden-Hughes theorem. In this note, we show that the constitutive equations as well as the balance laws of hyperelasticity can be derived using the first and second laws of thermodynamics without assuming any invariance.","sentences":["It is known that the balance laws of hyperelasticity (Green elasticity), i.e., conservation of mass and balance of linear and angular momenta, can be derived using the first law of thermodynamics by postulating its invariance under rigid body motions of the Euclidean ambient space -- the Green-Naghdi-Rivlin theorem.","In the case of a non-Euclidean ambient space, covariance of the energy balance -- its invariance under arbitrary diffeomorphisms of the ambient space -- gives all the balance laws and the Doyle-Ericksen formula -- the Marsden-Hughes theorem.","In this note, we show that the constitutive equations as well as the balance laws of hyperelasticity can be derived using the first and second laws of thermodynamics without assuming any invariance."],"url":"http://arxiv.org/abs/2403.14543v1","category":"physics.class-ph"}
{"created":"2024-03-21 16:36:40","title":"Transfer Learning for Cross-dataset Isolated Sign Language Recognition in Under-Resourced Datasets","abstract":"Sign language recognition (SLR) has recently achieved a breakthrough in performance thanks to deep neural networks trained on large annotated sign datasets. Of the many different sign languages, these annotated datasets are only available for a select few. Since acquiring gloss-level labels on sign language videos is difficult, learning by transferring knowledge from existing annotated sources is useful for recognition in under-resourced sign languages. This study provides a publicly available cross-dataset transfer learning benchmark from two existing public Turkish SLR datasets. We use a temporal graph convolution-based sign language recognition approach to evaluate five supervised transfer learning approaches and experiment with closed-set and partial-set cross-dataset transfer learning. Experiments demonstrate that improvement over finetuning based transfer learning is possible with specialized supervised transfer learning methods.","sentences":["Sign language recognition (SLR) has recently achieved a breakthrough in performance thanks to deep neural networks trained on large annotated sign datasets.","Of the many different sign languages, these annotated datasets are only available for a select few.","Since acquiring gloss-level labels on sign language videos is difficult, learning by transferring knowledge from existing annotated sources is useful for recognition in under-resourced sign languages.","This study provides a publicly available cross-dataset transfer learning benchmark from two existing public Turkish SLR datasets.","We use a temporal graph convolution-based sign language recognition approach to evaluate five supervised transfer learning approaches and experiment with closed-set and partial-set cross-dataset transfer learning.","Experiments demonstrate that improvement over finetuning based transfer learning is possible with specialized supervised transfer learning methods."],"url":"http://arxiv.org/abs/2403.14534v1","category":"cs.CV"}
{"created":"2024-03-21 16:33:55","title":"The dependence of the magnetism of a near-limb sunspot on height","abstract":"The physical parameters of the sunspot are not fully understood, especially the height dependence of the magnetic field. So far, it is also an open question as to which heights the He I 1083 nm spectral line is formed at. Our aim is to investigate the magnetic and dynamical properties in the atmosphere above a sunspot, from the photosphere to the chromosphere. We analyzed the photospheric and chromospheric magnetic field properties of a stable sunspot in AR 12553 on June 20, 2016 using spectropolarimetric observations obtained with GRIS at GREGOR. A spectral-line inversion technique was used to infer the magnetic field vector and Doppler velocities from the full Stokes profiles. In total, three spectral lines were inverted and the variation of the magnetic properties was qualified using the average values of the radial circles. The sunspot is located close to the solar limb, and thus this allows us to make a geometrical determination of the height of the spectral line He I 1083 nm. We find the height of helium spectral line to be 970 km above the photospheric spectral lines directly from observation at a stable sunspot. The total magnetic field strength decreases with height over the sunspot; the rates are -0.34 G/km for the umbra and -0.28 G/km for the penumbra. The inclination increases with increasing height in the umbra, but decreases in the penumbra. In the umbra, the vertical component ($B_z$) decreases with height, while the horizontal component ($B_{hor}$) remains almost constant. In the penumbra this is reversed, as $B_z$ remains nearly constant over height, while $B_{hor}$ decreases. We also observe fast velocities with 30 km/s in small chromospheric patches on the central side of the spot. The key parameters depending on height in the sunspot are the $B_{z}$ component of the magnetic field for the umbra and the $B_{hor}$ component of the magnetic field for the penumbra.","sentences":["The physical parameters of the sunspot are not fully understood, especially the height dependence of the magnetic field.","So far, it is also an open question as to which heights the He I 1083","nm spectral line is formed at.","Our aim is to investigate the magnetic and dynamical properties in the atmosphere above a sunspot, from the photosphere to the chromosphere.","We analyzed the photospheric and chromospheric magnetic field properties of a stable sunspot in AR 12553 on June 20, 2016 using spectropolarimetric observations obtained with GRIS at GREGOR.","A spectral-line inversion technique was used to infer the magnetic field vector and Doppler velocities from the full Stokes profiles.","In total, three spectral lines were inverted and the variation of the magnetic properties was qualified using the average values of the radial circles.","The sunspot is located close to the solar limb, and thus this allows us to make a geometrical determination of the height of the spectral line He I 1083 nm.","We find the height of helium spectral line to be 970 km above the photospheric spectral lines directly from observation at a stable sunspot.","The total magnetic field strength decreases with height over the sunspot; the rates are -0.34 G/km for the umbra and -0.28 G/km for the penumbra.","The inclination increases with increasing height in the umbra, but decreases in the penumbra.","In the umbra, the vertical component ($B_z$) decreases with height, while the horizontal component ($B_{hor}$) remains almost constant.","In the penumbra this is reversed, as $B_z$ remains nearly constant over height, while $B_{hor}$ decreases.","We also observe fast velocities with 30 km/s in small chromospheric patches on the central side of the spot.","The key parameters depending on height in the sunspot are the $B_{z}$ component of the magnetic field for the umbra and the $B_{hor}$ component of the magnetic field for the penumbra."],"url":"http://arxiv.org/abs/2403.14532v1","category":"astro-ph.SR"}
{"created":"2024-03-21 16:28:58","title":"HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression","abstract":"3D Gaussian Splatting (3DGS) has emerged as a promising framework for novel view synthesis, boasting rapid rendering speed with high fidelity. However, the substantial Gaussians and their associated attributes necessitate effective compression techniques. Nevertheless, the sparse and unorganized nature of the point cloud of Gaussians (or anchors in our paper) presents challenges for compression. To address this, we make use of the relations between the unorganized anchors and the structured hash grid, leveraging their mutual information for context modeling, and propose a Hash-grid Assisted Context (HAC) framework for highly compact 3DGS representation. Our approach introduces a binary hash grid to establish continuous spatial consistencies, allowing us to unveil the inherent spatial relations of anchors through a carefully designed context model. To facilitate entropy coding, we utilize Gaussian distributions to accurately estimate the probability of each quantized attribute, where an adaptive quantization module is proposed to enable high-precision quantization of these attributes for improved fidelity restoration. Additionally, we incorporate an adaptive masking strategy to eliminate invalid Gaussians and anchors. Importantly, our work is the pioneer to explore context-based compression for 3DGS representation, resulting in a remarkable size reduction of over $75\\times$ compared to vanilla 3DGS, while simultaneously improving fidelity, and achieving over $11\\times$ size reduction over SOTA 3DGS compression approach Scaffold-GS. Our code is available here: https://github.com/YihangChen-ee/HAC","sentences":["3D Gaussian Splatting (3DGS) has emerged as a promising framework for novel view synthesis, boasting rapid rendering speed with high fidelity.","However, the substantial Gaussians and their associated attributes necessitate effective compression techniques.","Nevertheless, the sparse and unorganized nature of the point cloud of Gaussians (or anchors in our paper) presents challenges for compression.","To address this, we make use of the relations between the unorganized anchors and the structured hash grid, leveraging their mutual information for context modeling, and propose a Hash-grid Assisted Context (HAC) framework for highly compact 3DGS representation.","Our approach introduces a binary hash grid to establish continuous spatial consistencies, allowing us to unveil the inherent spatial relations of anchors through a carefully designed context model.","To facilitate entropy coding, we utilize Gaussian distributions to accurately estimate the probability of each quantized attribute, where an adaptive quantization module is proposed to enable high-precision quantization of these attributes for improved fidelity restoration.","Additionally, we incorporate an adaptive masking strategy to eliminate invalid Gaussians and anchors.","Importantly, our work is the pioneer to explore context-based compression for 3DGS representation, resulting in a remarkable size reduction of over $75\\times$ compared to vanilla 3DGS, while simultaneously improving fidelity, and achieving over $11\\times$ size reduction over SOTA 3DGS compression approach Scaffold-GS.","Our code is available here: https://github.com/YihangChen-ee/HAC"],"url":"http://arxiv.org/abs/2403.14530v1","category":"cs.CV"}
{"created":"2024-03-21 15:53:55","title":"Healing Regimes for Microscopic Wounds in the Vertex Model of Cell Tissues","abstract":"Wounds in epithelial tissues compromise their vital role in homeostasis. A rapid and efficient wound healing encompasses different mechanisms, which includes the formation of a contractile actin-myosin cable around its edge, known as the purse-string mechanism. We combine mean-field calculations and numerical simulations of the Vertex model to study the interplay between tissue properties and the purse-string mechanism and its impact on the healing process. We find different regimes, where the wound opens, closes partially or completely. We also derive an analytic expression for the closure time which is validated by numerical simulations. This study establishes under which conditions the purse-string mechanism suffices for closure, providing an analytical mean-field expression for the respective thresholds.","sentences":["Wounds in epithelial tissues compromise their vital role in homeostasis.","A rapid and efficient wound healing encompasses different mechanisms, which includes the formation of a contractile actin-myosin cable around its edge, known as the purse-string mechanism.","We combine mean-field calculations and numerical simulations of the Vertex model to study the interplay between tissue properties and the purse-string mechanism and its impact on the healing process.","We find different regimes, where the wound opens, closes partially or completely.","We also derive an analytic expression for the closure time which is validated by numerical simulations.","This study establishes under which conditions the purse-string mechanism suffices for closure, providing an analytical mean-field expression for the respective thresholds."],"url":"http://arxiv.org/abs/2403.14501v1","category":"cond-mat.soft"}
{"created":"2024-03-21 15:24:41","title":"Magnetic corrections to the QCD coupling: strong field approximation","abstract":"We compute the 1-loop vertex function of the QCD coupling in the presence of an ultra intense magnetic field. From the vertex function, we extract the effective coupling and show that it grows with increasing magnetic field. We consider the quark-gluon vertex and the three-gluon vertex, accounting for the propagators of charged particles within the loops using the lowest Landau level approximation in order to satisfy the condition where the magnetic field is the largest energy scale. Under this approximation, we find that the contribution from the three-gluon vertex vanishes. Therefore, this result arises from the competition between the color charge associated to gluons and to quarks as well, with the former being larger than the latter. The behavior of the QCD coupling as a function of the magnetic field strength is analogous to that exhibited by the light-quark condensate, indicating the magnetic catalysis occurs. This increasing behavior stems from the dominant contribution of color charge associated to gluons in the vertex function.","sentences":["We compute the 1-loop vertex function of the QCD coupling in the presence of an ultra intense magnetic field.","From the vertex function, we extract the effective coupling and show that it grows with increasing magnetic field.","We consider the quark-gluon vertex and the three-gluon vertex, accounting for the propagators of charged particles within the loops using the lowest Landau level approximation in order to satisfy the condition where the magnetic field is the largest energy scale.","Under this approximation, we find that the contribution from the three-gluon vertex vanishes.","Therefore, this result arises from the competition between the color charge associated to gluons and to quarks as well, with the former being larger than the latter.","The behavior of the QCD coupling as a function of the magnetic field strength is analogous to that exhibited by the light-quark condensate, indicating the magnetic catalysis occurs.","This increasing behavior stems from the dominant contribution of color charge associated to gluons in the vertex function."],"url":"http://arxiv.org/abs/2403.14478v1","category":"hep-ph"}
{"created":"2024-03-21 15:20:07","title":"The Ethics of ChatGPT in Medicine and Healthcare: A Systematic Review on Large Language Models (LLMs)","abstract":"With the introduction of ChatGPT, Large Language Models (LLMs) have received enormous attention in healthcare. Despite their potential benefits, researchers have underscored various ethical implications. While individual instances have drawn much attention, the debate lacks a systematic overview of practical applications currently researched and ethical issues connected to them. Against this background, this work aims to map the ethical landscape surrounding the current stage of deployment of LLMs in medicine and healthcare. Electronic databases and preprint servers were queried using a comprehensive search strategy. Studies were screened and extracted following a modified rapid review approach. Methodological quality was assessed using a hybrid approach. For 53 records, a meta-aggregative synthesis was performed. Four fields of applications emerged and testify to a vivid exploration phase. Advantages of using LLMs are attributed to their capacity in data analysis, personalized information provisioning, support in decision-making, mitigating information loss and enhancing information accessibility. However, we also identifies recurrent ethical concerns connected to fairness, bias, non-maleficence, transparency, and privacy. A distinctive concern is the tendency to produce harmful misinformation or convincingly but inaccurate content. A recurrent plea for ethical guidance and human oversight is evident. Given the variety of use cases, it is suggested that the ethical guidance debate be reframed to focus on defining what constitutes acceptable human oversight across the spectrum of applications. This involves considering diverse settings, varying potentials for harm, and different acceptable thresholds for performance and certainty in healthcare. In addition, a critical inquiry is necessary to determine the extent to which the current experimental use of LLMs is necessary and justified.","sentences":["With the introduction of ChatGPT, Large Language Models (LLMs) have received enormous attention in healthcare.","Despite their potential benefits, researchers have underscored various ethical implications.","While individual instances have drawn much attention, the debate lacks a systematic overview of practical applications currently researched and ethical issues connected to them.","Against this background, this work aims to map the ethical landscape surrounding the current stage of deployment of LLMs in medicine and healthcare.","Electronic databases and preprint servers were queried using a comprehensive search strategy.","Studies were screened and extracted following a modified rapid review approach.","Methodological quality was assessed using a hybrid approach.","For 53 records, a meta-aggregative synthesis was performed.","Four fields of applications emerged and testify to a vivid exploration phase.","Advantages of using LLMs are attributed to their capacity in data analysis, personalized information provisioning, support in decision-making, mitigating information loss and enhancing information accessibility.","However, we also identifies recurrent ethical concerns connected to fairness, bias, non-maleficence, transparency, and privacy.","A distinctive concern is the tendency to produce harmful misinformation or convincingly but inaccurate content.","A recurrent plea for ethical guidance and human oversight is evident.","Given the variety of use cases, it is suggested that the ethical guidance debate be reframed to focus on defining what constitutes acceptable human oversight across the spectrum of applications.","This involves considering diverse settings, varying potentials for harm, and different acceptable thresholds for performance and certainty in healthcare.","In addition, a critical inquiry is necessary to determine the extent to which the current experimental use of LLMs is necessary and justified."],"url":"http://arxiv.org/abs/2403.14473v1","category":"cs.CY"}
{"created":"2024-03-21 14:46:45","title":"Quantifying Semantic Query Similarity for Automated Linear SQL Grading: A Graph-based Approach","abstract":"Quantifying the semantic similarity between database queries is a critical challenge with broad applications, ranging from query log analysis to automated educational assessment of SQL skills. Traditional methods often rely solely on syntactic comparisons or are limited to checking for semantic equivalence.   This paper introduces a novel graph-based approach to measure the semantic dissimilarity between SQL queries. Queries are represented as nodes in an implicit graph, while the transitions between nodes are called edits, which are weighted by semantic dissimilarity. We employ shortest path algorithms to identify the lowest-cost edit sequence between two given queries, thereby defining a quantifiable measure of semantic distance.   A prototype implementation of this technique has been evaluated through an empirical study, which strongly suggests that our method provides more accurate and comprehensible grading compared to existing techniques. Moreover, the results indicate that our approach comes close to the quality of manual grading, making it a robust tool for diverse database query comparison tasks.","sentences":["Quantifying the semantic similarity between database queries is a critical challenge with broad applications, ranging from query log analysis to automated educational assessment of SQL skills.","Traditional methods often rely solely on syntactic comparisons or are limited to checking for semantic equivalence.   ","This paper introduces a novel graph-based approach to measure the semantic dissimilarity between SQL queries.","Queries are represented as nodes in an implicit graph, while the transitions between nodes are called edits, which are weighted by semantic dissimilarity.","We employ shortest path algorithms to identify the lowest-cost edit sequence between two given queries, thereby defining a quantifiable measure of semantic distance.   ","A prototype implementation of this technique has been evaluated through an empirical study, which strongly suggests that our method provides more accurate and comprehensible grading compared to existing techniques.","Moreover, the results indicate that our approach comes close to the quality of manual grading, making it a robust tool for diverse database query comparison tasks."],"url":"http://arxiv.org/abs/2403.14441v1","category":"cs.DB"}
{"created":"2024-03-21 14:22:38","title":"$^{171}$Yb$^+$ optical clock with $2.2\\times 10^{-18}$ systematic uncertainty and absolute frequency measurements","abstract":"A full evaluation of the uncertainty budget for the ytterbium ion optical clock at the National Physical Laboratory (NPL) was performed on the electric octupole (E3) $^2\\mathrm{S}_{1/2}\\,\\rightarrow\\, ^2\\mathrm{F}_{7/2}$ transition. The total systematic frequency shift was measured with a fractional standard systematic uncertainty of $2.2\\times 10^{-18}$. Furthermore, the absolute frequency of the E3 transition of the $^{171}$Yb$^+$ ion was measured between 2019 and 2023 via a link to International Atomic Time (TAI) and against the local caesium fountain NPL-CsF2. The absolute frequencies were measured with fractional standard uncertainties between $3.7 \\times 10^{-16}$ and $1.1 \\times 10^{-15}$, and all were in agreement with the 2021 BIPM recommended frequency.","sentences":["A full evaluation of the uncertainty budget for the ytterbium ion optical clock at the National Physical Laboratory (NPL) was performed on the electric octupole (E3) $^2\\mathrm{S}_{1/2}\\,\\rightarrow\\, ^2\\mathrm{F}_{7/2}$ transition.","The total systematic frequency shift was measured with a fractional standard systematic uncertainty of $2.2\\times 10^{-18}$.","Furthermore, the absolute frequency of the E3 transition of the $^{171}$Yb$^+$ ion was measured between 2019 and 2023 via a link to International Atomic Time (TAI) and against the local caesium fountain NPL-CsF2.","The absolute frequencies were measured with fractional standard uncertainties between $3.7 \\times 10^{-16}$ and $1.1 \\times 10^{-15}$, and all were in agreement with the 2021 BIPM recommended frequency."],"url":"http://arxiv.org/abs/2403.14423v1","category":"physics.atom-ph"}
{"created":"2024-03-21 14:05:32","title":"Quantum Channel Simulation under Purified Distance is no more difficult than State Splitting","abstract":"Characterizing the minimal communication needed for the quantum channel simulation is a fundamental task in the quantum information theory. In this paper, we show that, under the purified distance, the quantum channel simulation can be directly achieved via quantum state splitting without using a technique known as the de Finetti reduction, and thus provide a pair of tighter one-shot bounds. Using the bounds, we also recover the quantum reverse Shannon theorem in a much simpler way.","sentences":["Characterizing the minimal communication needed for the quantum channel simulation is a fundamental task in the quantum information theory.","In this paper, we show that, under the purified distance, the quantum channel simulation can be directly achieved via quantum state splitting without using a technique known as the de Finetti reduction, and thus provide a pair of tighter one-shot bounds.","Using the bounds, we also recover the quantum reverse Shannon theorem in a much simpler way."],"url":"http://arxiv.org/abs/2403.14416v1","category":"quant-ph"}
{"created":"2024-03-21 13:58:00","title":"Rational approximation of operator semigroups via the $\\mathcal B$-calculus","abstract":"We improve the classical results by Brenner and Thom\\'ee on rational approximations of operator semigroups. In the setting of Hilbert spaces, we introduce a finer regularity scale for initial data, provide sharper stability estimates, and obtain optimal approximation rates. Moreover, we strengthen a result due to Egert-Rozendaal on subdiagonal Pad\\'e approximations of operator semigroups. Our approach is direct and based on the theory of the $\\mathcal B$- functional calculus developed recently. On the way, we elaborate a new and simple approach to construction of the $\\mathcal B$-calculus thus making the paper essentially self-contai","sentences":["We improve the classical results by Brenner and Thom\\'ee on rational approximations of operator semigroups.","In the setting of Hilbert spaces, we introduce a finer regularity scale for initial data, provide sharper stability estimates, and obtain optimal approximation rates.","Moreover, we strengthen a result due to Egert-Rozendaal on subdiagonal Pad\\'e approximations of operator semigroups.","Our approach is direct and based on the theory of the $\\mathcal B$- functional calculus developed recently.","On the way, we elaborate a new and simple approach to construction of the $\\mathcal B$-calculus thus making the paper essentially self-contai"],"url":"http://arxiv.org/abs/2403.14411v1","category":"math.FA"}
{"created":"2024-03-21 13:54:03","title":"A reinforcement learning guided hybrid evolutionary algorithm for the latency location routing problem","abstract":"The latency location routing problem integrates the facility location problem and the multi-depot cumulative capacitated vehicle routing problem. This problem involves making simultaneous decisions about depot locations and vehicle routes to serve customers while aiming to minimize the sum of waiting (arriving) times for all customers. To address this computationally challenging problem, we propose a reinforcement learning guided hybrid evolutionary algorithm following the framework of the memetic algorithm. The proposed algorithm relies on a diversity-enhanced multi-parent edge assembly crossover to build promising offspring and a reinforcement learning guided variable neighborhood descent to determine the exploration order of multiple neighborhoods. Additionally, strategic oscillation is used to achieve a balanced exploration of both feasible and infeasible solutions. The competitiveness of the algorithm against state-of-the-art methods is demonstrated by experimental results on the three sets of 76 popular instances, including 51 improved best solutions (new upper bounds) for the 59 instances with unknown optima and equal best results for the remaining instances. We also conduct additional experiments to shed light on the key components of the algorithm.","sentences":["The latency location routing problem integrates the facility location problem and the multi-depot cumulative capacitated vehicle routing problem.","This problem involves making simultaneous decisions about depot locations and vehicle routes to serve customers while aiming to minimize the sum of waiting (arriving) times for all customers.","To address this computationally challenging problem, we propose a reinforcement learning guided hybrid evolutionary algorithm following the framework of the memetic algorithm.","The proposed algorithm relies on a diversity-enhanced multi-parent edge assembly crossover to build promising offspring and a reinforcement learning guided variable neighborhood descent to determine the exploration order of multiple neighborhoods.","Additionally, strategic oscillation is used to achieve a balanced exploration of both feasible and infeasible solutions.","The competitiveness of the algorithm against state-of-the-art methods is demonstrated by experimental results on the three sets of 76 popular instances, including 51 improved best solutions (new upper bounds) for the 59 instances with unknown optima and equal best results for the remaining instances.","We also conduct additional experiments to shed light on the key components of the algorithm."],"url":"http://arxiv.org/abs/2403.14405v1","category":"cs.NE"}
{"created":"2024-03-21 13:48:04","title":"Absence of phonon-mediated superconductivity in La$_3$Ni$_2$O$_7$ under pressure","abstract":"A recent experimental study announced the emergence of superconductivity in La$_3$Ni$_2$O$_7$ under pressure, with the highest observed superconducting transition temperature ($T_c$) reaching approximately 80 K beyond 14 GPa. While extensive studies have been devoted to the electronic correlations and potential superconducting pairing mechanisms, there lack investigations into the phonon properties and electron phonon coupling. Using density functional theory in conjunction with Wannier interpolation techniques, we study the phonon properties and electron phonon interactions in La$_3$Ni$_2$O$_7$ under 29.5 GPa. Our findings reveal that the electron phonon coupling is insufficient to solely explain the observed high superconducting $T_c$ $\\sim$ 80 K in La$_3$Ni$_2$O$_7$. And the calculated strong Fermi surface nesting may explain the experimental observed charge density wave transition in La$_3$Ni$_2$O$_7$. Our calculations substantiate La$_3$Ni$_2$O$_7$ is an unconventional superconductor.","sentences":["A recent experimental study announced the emergence of superconductivity in La$_3$Ni$_2$O$_7$ under pressure, with the highest observed superconducting transition temperature ($T_c$) reaching approximately 80 K beyond 14 GPa.","While extensive studies have been devoted to the electronic correlations and potential superconducting pairing mechanisms, there lack investigations into the phonon properties and electron phonon coupling.","Using density functional theory in conjunction with Wannier interpolation techniques, we study the phonon properties and electron phonon interactions in La$_3$Ni$_2$O$_7$ under 29.5 GPa.","Our findings reveal that the electron phonon coupling is insufficient to solely explain the observed high superconducting $T_c$ $\\sim$ 80 K in La$_3$Ni$_2$O$_7$. And the calculated strong Fermi surface nesting may explain the experimental observed charge density wave transition in La$_3$Ni$_2$O$_7$. Our calculations substantiate La$_3$Ni$_2$O$_7$ is an unconventional superconductor."],"url":"http://arxiv.org/abs/2403.14400v1","category":"cond-mat.supr-con"}
{"created":"2024-03-21 13:33:00","title":"A Bag of Tricks for Few-Shot Class-Incremental Learning","abstract":"We present a bag of tricks framework for few-shot class-incremental learning (FSCIL), which is a challenging form of continual learning that involves continuous adaptation to new tasks with limited samples. FSCIL requires both stability and adaptability, i.e., preserving proficiency in previously learned tasks while learning new ones. Our proposed bag of tricks brings together eight key and highly influential techniques that improve stability, adaptability, and overall performance under a unified framework for FSCIL. We organize these tricks into three categories: stability tricks, adaptability tricks, and training tricks. Stability tricks aim to mitigate the forgetting of previously learned classes by enhancing the separation between the embeddings of learned classes and minimizing interference when learning new ones. On the other hand, adaptability tricks focus on the effective learning of new classes. Finally, training tricks improve the overall performance without compromising stability or adaptability. We perform extensive experiments on three benchmark datasets, CIFAR-100, CUB-200, and miniIMageNet, to evaluate the impact of our proposed framework. Our detailed analysis shows that our approach substantially improves both stability and adaptability, establishing a new state-of-the-art by outperforming prior works in the area. We believe our method provides a go-to solution and establishes a robust baseline for future research in this area.","sentences":["We present a bag of tricks framework for few-shot class-incremental learning (FSCIL), which is a challenging form of continual learning that involves continuous adaptation to new tasks with limited samples.","FSCIL requires both stability and adaptability, i.e., preserving proficiency in previously learned tasks while learning new ones.","Our proposed bag of tricks brings together eight key and highly influential techniques that improve stability, adaptability, and overall performance under a unified framework for FSCIL.","We organize these tricks into three categories: stability tricks, adaptability tricks, and training tricks.","Stability tricks aim to mitigate the forgetting of previously learned classes by enhancing the separation between the embeddings of learned classes and minimizing interference when learning new ones.","On the other hand, adaptability tricks focus on the effective learning of new classes.","Finally, training tricks improve the overall performance without compromising stability or adaptability.","We perform extensive experiments on three benchmark datasets, CIFAR-100, CUB-200, and miniIMageNet, to evaluate the impact of our proposed framework.","Our detailed analysis shows that our approach substantially improves both stability and adaptability, establishing a new state-of-the-art by outperforming prior works in the area.","We believe our method provides a go-to solution and establishes a robust baseline for future research in this area."],"url":"http://arxiv.org/abs/2403.14392v1","category":"cs.CV"}
{"created":"2024-03-21 13:25:20","title":"Tunable band gap in twisted bilayer graphene","abstract":"At large commensurate angles, twisted bilayer graphene which holds even parity under sublattice exchange exhibits a tiny gap. Here, we point out a way to tune this tiny gap into a large gap. We start from comprehensive understanding of the physical origin of gap opening by density functional theory calculations. We reveal that the effective inter-layer hopping, intra-layer CDW, or inter-layer charge imbalance favors a gap. Then, on the basis of tight-binding calculations, we suggest that a periodic transverse inhomogeneous pressure, which can tune inter-layer hoppings in specific regions of the moi$\\rm\\acute{r}$e supercell, may open a gap of over $100$~meV, which is further confirmed by first-principles calculations. Our results provide a theoretical guidance for experiments to open a large gap in twisted bilayer graphene.","sentences":["At large commensurate angles, twisted bilayer graphene which holds even parity under sublattice exchange exhibits a tiny gap.","Here, we point out a way to tune this tiny gap into a large gap.","We start from comprehensive understanding of the physical origin of gap opening by density functional theory calculations.","We reveal that the effective inter-layer hopping, intra-layer CDW, or inter-layer charge imbalance favors a gap.","Then, on the basis of tight-binding calculations, we suggest that a periodic transverse inhomogeneous pressure, which can tune inter-layer hoppings in specific regions of the moi$\\rm\\acute{r}$e supercell, may open a gap of over $100$~meV, which is further confirmed by first-principles calculations.","Our results provide a theoretical guidance for experiments to open a large gap in twisted bilayer graphene."],"url":"http://arxiv.org/abs/2403.14387v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-21 13:21:33","title":"Estimating Causal Effects with Double Machine Learning -- A Method Evaluation","abstract":"The estimation of causal effects with observational data continues to be a very active research area. In recent years, researchers have developed new frameworks which use machine learning to relax classical assumptions necessary for the estimation of causal effects. In this paper, we review one of the most prominent methods - \"double/debiased machine learning\" (DML) - and empirically evaluate it by comparing its performance on simulated data relative to more traditional statistical methods, before applying it to real-world data. Our findings indicate that the application of a suitably flexible machine learning algorithm within DML improves the adjustment for various nonlinear confounding relationships. This advantage enables a departure from traditional functional form assumptions typically necessary in causal effect estimation. However, we demonstrate that the method continues to critically depend on standard assumptions about causal structure and identification. When estimating the effects of air pollution on housing prices in our application, we find that DML estimates are consistently larger than estimates of less flexible methods. From our overall results, we provide actionable recommendations for specific choices researchers must make when applying DML in practice.","sentences":["The estimation of causal effects with observational data continues to be a very active research area.","In recent years, researchers have developed new frameworks which use machine learning to relax classical assumptions necessary for the estimation of causal effects.","In this paper, we review one of the most prominent methods - \"double/debiased machine learning\" (DML) - and empirically evaluate it by comparing its performance on simulated data relative to more traditional statistical methods, before applying it to real-world data.","Our findings indicate that the application of a suitably flexible machine learning algorithm within DML improves the adjustment for various nonlinear confounding relationships.","This advantage enables a departure from traditional functional form assumptions typically necessary in causal effect estimation.","However, we demonstrate that the method continues to critically depend on standard assumptions about causal structure and identification.","When estimating the effects of air pollution on housing prices in our application, we find that DML estimates are consistently larger than estimates of less flexible methods.","From our overall results, we provide actionable recommendations for specific choices researchers must make when applying DML in practice."],"url":"http://arxiv.org/abs/2403.14385v1","category":"stat.ML"}
{"created":"2024-03-21 11:39:51","title":"On $C^1$ Whitney extension theorem in Banach spaces","abstract":"Our note is a complement to recent articles \\cite{JS1} (2011) and \\cite{JS2} (2013) by M. Jim\\'enez-Sevilla and L. S\\'anchez-Gonz\\'alez which generalise (the basic statement of) the classical Whitney extension theorem for $C^1$-smooth real functions on $\\mathbb R^n$ to the case of real functions on $X$ (\\cite{JS1}) and to the case of mappings from $X$ to $Y$ (\\cite{JS2}) for some Banach spaces $X$ and $Y$. Since the proof from \\cite{JS2} contains a serious flaw, we supply a different more transparent detailed proof under (probably) slightly stronger assumptions on $X$ and $Y$. Our proof gives also extensions results from special sets (e.g. Lipschitz submanifolds or closed convex bodies) under substantially weaker assumptions on $X$ and $Y$. Further, we observe that the mapping $F\\in C^1(X;Y)$ which extends $f$ given on a closed set $A\\subset X$ can be, in some cases, $C^\\infty$-smooth (or $C^k$-smooth with $k>1$) on $X\\setminus A$. Of course, also this improved result is weaker than Whitney's result (for $X=\\mathbb R^n$, $Y=\\mathbb R$) which asserts that $F$ is even analytic on $X\\setminus A$. Further, following another Whitney's article and using the above results, we prove results on extensions of $C^1$-smooth mappings from open (``weakly'') quasiconvex subsets of $X$. Following the above mentioned articles we also consider the question concerning the Lipschitz constant of $F$ is $f$ is a Lipschitz mapping. We also present a new observation on the limitation of the possible validity of Whitney's extension theorem for $C^2$-smooth functions.","sentences":["Our note is a complement to recent articles \\cite{JS1} (2011) and \\cite{JS2} (2013) by M. Jim\\'enez-Sevilla and L. S\\'anchez-Gonz\\'alez which generalise (the basic statement of) the classical Whitney extension theorem for $C^1$-smooth real functions on $\\mathbb R^n$ to the case of real functions on $X$ (\\cite{JS1}) and to the case of mappings from $X$ to $Y$ (\\cite{JS2}) for some Banach spaces $X$ and $Y$. Since the proof from \\cite{JS2} contains a serious flaw, we supply a different more transparent detailed proof under (probably) slightly stronger assumptions on $X$ and $Y$. Our proof gives also extensions results from special sets (e.g. Lipschitz submanifolds or closed convex bodies) under substantially weaker assumptions on $X$ and $Y$. Further, we observe that the mapping $F\\in C^1(X;Y)$ which extends $f$ given on a closed set $A\\subset X$ can be, in some cases, $C^\\infty$-smooth (or $C^k$-smooth with $k>1$) on $X\\setminus A$.","Of course, also this improved result is weaker than Whitney's result (for $X=\\mathbb R^n$, $Y=\\mathbb R$) which asserts that $F$ is even analytic on $X\\setminus A$.","Further, following another Whitney's article and using the above results, we prove results on extensions of $C^1$-smooth mappings from open (``weakly'') quasiconvex subsets of $X$. Following the above mentioned articles we also consider the question concerning the Lipschitz constant of $F$ is $f$ is a Lipschitz mapping.","We also present a new observation on the limitation of the possible validity of Whitney's extension theorem for $C^2$-smooth functions."],"url":"http://arxiv.org/abs/2403.14317v1","category":"math.FA"}
{"created":"2024-03-21 10:54:21","title":"Exploring Green AI for Audio Deepfake Detection","abstract":"The state-of-the-art audio deepfake detectors leveraging deep neural networks exhibit impressive recognition performance. Nonetheless, this advantage is accompanied by a significant carbon footprint. This is mainly due to the use of high-performance computing with accelerators and high training time. Studies show that average deep NLP model produces around 626k lbs of CO\\textsubscript{2} which is equivalent to five times of average US car emission at its lifetime. This is certainly a massive threat to the environment. To tackle this challenge, this study presents a novel framework for audio deepfake detection that can be seamlessly trained using standard CPU resources. Our proposed framework utilizes off-the-shelve self-supervised learning (SSL) based models which are pre-trained and available in public repositories. In contrast to existing methods that fine-tune SSL models and employ additional deep neural networks for downstream tasks, we exploit classical machine learning algorithms such as logistic regression and shallow neural networks using the SSL embeddings extracted using the pre-trained model. Our approach shows competitive results compared to the commonly used high-carbon footprint approaches. In experiments with the ASVspoof 2019 LA dataset, we achieve a 0.90\\% equal error rate (EER) with less than 1k trainable model parameters. To encourage further research in this direction and support reproducible results, the Python code will be made publicly accessible following acceptance. Github: https://github.com/sahasubhajit/Speech-Spoofing-","sentences":["The state-of-the-art audio deepfake detectors leveraging deep neural networks exhibit impressive recognition performance.","Nonetheless, this advantage is accompanied by a significant carbon footprint.","This is mainly due to the use of high-performance computing with accelerators and high training time.","Studies show that average deep NLP model produces around 626k lbs of CO\\textsubscript{2} which is equivalent to five times of average US car emission at its lifetime.","This is certainly a massive threat to the environment.","To tackle this challenge, this study presents a novel framework for audio deepfake detection that can be seamlessly trained using standard CPU resources.","Our proposed framework utilizes off-the-shelve self-supervised learning (SSL) based models which are pre-trained and available in public repositories.","In contrast to existing methods that fine-tune SSL models and employ additional deep neural networks for downstream tasks, we exploit classical machine learning algorithms such as logistic regression and shallow neural networks using the SSL embeddings extracted using the pre-trained model.","Our approach shows competitive results compared to the commonly used high-carbon footprint approaches.","In experiments with the ASVspoof 2019 LA dataset, we achieve a 0.90\\% equal error rate (EER) with less than 1k trainable model parameters.","To encourage further research in this direction and support reproducible results, the Python code will be made publicly accessible following acceptance.","Github: https://github.com/sahasubhajit/Speech-Spoofing-"],"url":"http://arxiv.org/abs/2403.14290v1","category":"cs.SD"}
{"created":"2024-03-21 10:49:54","title":"Assessing the Robustness of Spectral Clustering for Deep Speaker Diarization","abstract":"Clustering speaker embeddings is crucial in speaker diarization but hasn't received as much focus as other components. Moreover, the robustness of speaker diarization across various datasets hasn't been explored when the development and evaluation data are from different domains. To bridge this gap, this study thoroughly examines spectral clustering for both same-domain and cross-domain speaker diarization. Our extensive experiments on two widely used corpora, AMI and DIHARD, reveal the performance trend of speaker diarization in the presence of domain mismatch. We observe that the performance difference between two different domain conditions can be attributed to the role of spectral clustering. In particular, keeping other modules unchanged, we show that differences in optimal tuning parameters as well as speaker count estimation originates due to the mismatch. This study opens several future directions for speaker diarization research.","sentences":["Clustering speaker embeddings is crucial in speaker diarization but hasn't received as much focus as other components.","Moreover, the robustness of speaker diarization across various datasets hasn't been explored when the development and evaluation data are from different domains.","To bridge this gap, this study thoroughly examines spectral clustering for both same-domain and cross-domain speaker diarization.","Our extensive experiments on two widely used corpora, AMI and DIHARD, reveal the performance trend of speaker diarization in the presence of domain mismatch.","We observe that the performance difference between two different domain conditions can be attributed to the role of spectral clustering.","In particular, keeping other modules unchanged, we show that differences in optimal tuning parameters as well as speaker count estimation originates due to the mismatch.","This study opens several future directions for speaker diarization research."],"url":"http://arxiv.org/abs/2403.14286v1","category":"cs.SD"}
{"created":"2024-03-21 10:49:13","title":"Gauge Field Dynamics in a Multilayer Kitaev Spin Liquid","abstract":"The Kitaev honeycomb model hosts a quantum spin liquid in its ground state, where the excitations are gapless Majorana fermions and static $\\mathbb Z_2$ gauge fluxes called visons. We consider Kitaev models stacked on top of each other, weakly coupled by Heisenberg interaction $\\propto J_\\perp$. This inter-layer coupling breaks the integrability of the model and makes the gauge fields dynamic. While single visons stay static in this model, an inter-layer pair of visons can hop with a hopping amplitude $\\propto J_{\\perp}$, but remains confined to a single plane. An intra-layer vison-pair, in contrast, is constrained to move along the stacking direction only. Depending on the anisotropy of the Kitaev couplings $K_x, K_y, K_z$, the intra-layer vison pairs show completely different dynamical behaviours. While coherent intra-layer tunnelling is possible for sufficiently strong anisotropies, only incoherent processes are possible in the isotropic case. When a magnetic field opens a gap for Majorana fermions, one can identify two types of intra-layer vison pairs, one bosonic and one fermionic. Only the bosonic pair obtains a hopping rate linear in $J_\\perp$. We argue that our results can be used to identify leading instabilities of the Kitaev phase induced by the inter-layer coupling.","sentences":["The Kitaev honeycomb model hosts a quantum spin liquid in its ground state, where the excitations are gapless Majorana fermions and static $\\mathbb Z_2$ gauge fluxes called visons.","We consider Kitaev models stacked on top of each other, weakly coupled by Heisenberg interaction $\\propto J_\\perp$. This inter-layer coupling breaks the integrability of the model and makes the gauge fields dynamic.","While single visons stay static in this model, an inter-layer pair of visons can hop with a hopping amplitude $\\propto J_{\\perp}$, but remains confined to a single plane.","An intra-layer vison-pair, in contrast, is constrained to move along the stacking direction only.","Depending on the anisotropy of the Kitaev couplings $K_x, K_y, K_z$, the intra-layer vison pairs show completely different dynamical behaviours.","While coherent intra-layer tunnelling is possible for sufficiently strong anisotropies, only incoherent processes are possible in the isotropic case.","When a magnetic field opens a gap for Majorana fermions, one can identify two types of intra-layer vison pairs, one bosonic and one fermionic.","Only the bosonic pair obtains a hopping rate linear in $J_\\perp$. We argue that our results can be used to identify leading instabilities of the Kitaev phase induced by the inter-layer coupling."],"url":"http://arxiv.org/abs/2403.14284v1","category":"cond-mat.str-el"}
{"created":"2024-03-21 10:41:31","title":"UAV-Assisted Maritime Search and Rescue: A Holistic Approach","abstract":"In this paper, we explore the application of Unmanned Aerial Vehicles (UAVs) in maritime search and rescue (mSAR) missions, focusing on medium-sized fixed-wing drones and quadcopters. We address the challenges and limitations inherent in operating some of the different classes of UAVs, particularly in search operations. Our research includes the development of a comprehensive software framework designed to enhance the efficiency and efficacy of SAR operations. This framework combines preliminary detection onboard UAVs with advanced object detection at ground stations, aiming to reduce visual strain and improve decision-making for operators. It will be made publicly available upon publication. We conduct experiments to evaluate various Region of Interest (RoI) proposal methods, especially by imposing simulated limited bandwidth on them, an important consideration when flying remote or offshore operations. This forces the algorithm to prioritize some predictions over others.","sentences":["In this paper, we explore the application of Unmanned Aerial Vehicles (UAVs) in maritime search and rescue (mSAR) missions, focusing on medium-sized fixed-wing drones and quadcopters.","We address the challenges and limitations inherent in operating some of the different classes of UAVs, particularly in search operations.","Our research includes the development of a comprehensive software framework designed to enhance the efficiency and efficacy of SAR operations.","This framework combines preliminary detection onboard UAVs with advanced object detection at ground stations, aiming to reduce visual strain and improve decision-making for operators.","It will be made publicly available upon publication.","We conduct experiments to evaluate various Region of Interest (RoI) proposal methods, especially by imposing simulated limited bandwidth on them, an important consideration when flying remote or offshore operations.","This forces the algorithm to prioritize some predictions over others."],"url":"http://arxiv.org/abs/2403.14281v1","category":"cs.RO"}
{"created":"2024-03-21 10:38:18","title":"Zero123-6D: Zero-shot Novel View Synthesis for RGB Category-level 6D Pose Estimation","abstract":"Estimating the pose of objects through vision is essential to make robotic platforms interact with the environment. Yet, it presents many challenges, often related to the lack of flexibility and generalizability of state-of-the-art solutions. Diffusion models are a cutting-edge neural architecture transforming 2D and 3D computer vision, outlining remarkable performances in zero-shot novel-view synthesis. Such a use case is particularly intriguing for reconstructing 3D objects. However, localizing objects in unstructured environments is rather unexplored. To this end, this work presents Zero123-6D to demonstrate the utility of Diffusion Model-based novel-view-synthesizers in enhancing RGB 6D pose estimation at category-level by integrating them with feature extraction techniques. The outlined method exploits such a novel view synthesizer to expand a sparse set of RGB-only reference views for the zero-shot 6D pose estimation task. Experiments are quantitatively analyzed on the CO3D dataset, showcasing increased performance over baselines, a substantial reduction in data requirements, and the removal of the necessity of depth information.","sentences":["Estimating the pose of objects through vision is essential to make robotic platforms interact with the environment.","Yet, it presents many challenges, often related to the lack of flexibility and generalizability of state-of-the-art solutions.","Diffusion models are a cutting-edge neural architecture transforming 2D and 3D computer vision, outlining remarkable performances in zero-shot novel-view synthesis.","Such a use case is particularly intriguing for reconstructing 3D objects.","However, localizing objects in unstructured environments is rather unexplored.","To this end, this work presents Zero123-6D to demonstrate the utility of Diffusion Model-based novel-view-synthesizers in enhancing RGB 6D pose estimation at category-level by integrating them with feature extraction techniques.","The outlined method exploits such a novel view synthesizer to expand a sparse set of RGB-only reference views for the zero-shot 6D pose estimation task.","Experiments are quantitatively analyzed on the CO3D dataset, showcasing increased performance over baselines, a substantial reduction in data requirements, and the removal of the necessity of depth information."],"url":"http://arxiv.org/abs/2403.14279v1","category":"cs.CV"}
{"created":"2024-03-21 10:08:56","title":"Construction and analysis of symmetry breaking operators for the pair $(\\operatorname{GL}(n+1,\\mathbb{R}),\\operatorname{GL}(n,\\mathbb{R}))$","abstract":"The pair of real reductive groups $(G,H)=(\\operatorname{GL}(n+1,\\mathbb{R}),\\operatorname{GL}(n,\\mathbb{R}))$ is a strong Gelfand pair, i.e. the multiplicities $\\dim\\operatorname{Hom}_H(\\pi|_H,\\tau)$ are either $0$ or $1$ for all irreducible Casselman-Wallach representations $\\pi$ of $G$ and $\\tau$ of $H$. This paper is concerned with the construction of explicit intertwining operators in $\\operatorname{Hom}_H(\\pi|_H,\\tau)$, so-called symmetry breaking operators, in the case where both $\\pi$ and $\\tau$ are principal series representations. Such operators come in families that depend meromorphically on the induction parameters, and we show how to normalize them in order to make the parameter dependence holomorphic. This is done by establishing explicit Bernstein-Sato identities for their distribution kernels as well as explicit functional identities for the composition of symmetry breaking operators with standard Knapp-Stein intertwining operators for $G$ and $H$. We also show that the obtained normalization is optimal and identify a subset of parameters for which the family of operators vanishes. Finally, we relate the operators to the local archimedean Rankin-Selberg integrals and use this relation to evaluate them on the spherical vectors.","sentences":["The pair of real reductive groups $(G,H)=(\\operatorname{GL}(n+1,\\mathbb{R}),\\operatorname{GL}(n,\\mathbb{R}))$ is a strong Gelfand pair, i.e. the multiplicities $\\dim\\operatorname{Hom}_H(\\pi|_H,\\tau)$ are either $0$ or $1$ for all irreducible Casselman-Wallach representations $\\pi$ of $G$ and $\\tau$ of $H$. This paper is concerned with the construction of explicit intertwining operators in $\\operatorname{Hom}_H(\\pi|_H,\\tau)$, so-called symmetry breaking operators, in the case where both $\\pi$ and $\\tau$ are principal series representations.","Such operators come in families that depend meromorphically on the induction parameters, and we show how to normalize them in order to make the parameter dependence holomorphic.","This is done by establishing explicit Bernstein-Sato identities for their distribution kernels as well as explicit functional identities for the composition of symmetry breaking operators with standard Knapp-Stein intertwining operators for $G$ and $H$. We also show that the obtained normalization is optimal and identify a subset of parameters for which the family of operators vanishes.","Finally, we relate the operators to the local archimedean Rankin-Selberg integrals and use this relation to evaluate them on the spherical vectors."],"url":"http://arxiv.org/abs/2403.14267v1","category":"math.RT"}
{"created":"2024-03-21 09:29:18","title":"Space-Efficient Indexes for Uncertain Strings","abstract":"Strings in the real world are often encoded with some level of uncertainty. In the character-level uncertainty model, an uncertain string $X$ of length $n$ on an alphabet $\\Sigma$ is a sequence of $n$ probability distributions over $\\Sigma$. Given an uncertain string $X$ and a weight threshold $\\frac{1}{z}\\in(0,1]$, we say that pattern $P$ occurs in $X$ at position $i$, if the product of probabilities of the letters of $P$ at positions $i,\\ldots,i+|P|-1$ is at least $\\frac{1}{z}$. While indexing standard strings for online pattern searches can be performed in linear time and space, indexing uncertain strings is much more challenging. Specifically, the state-of-the-art index for uncertain strings has $\\mathcal{O}(nz)$ size, requires $\\mathcal{O}(nz)$ time and $\\mathcal{O}(nz)$ space to be constructed, and answers pattern matching queries in the optimal $\\mathcal{O}(m+|\\text{Occ}|)$ time, where $m$ is the length of $P$ and $|\\text{Occ}|$ is the total number of occurrences of $P$ in $X$. For large $n$ and (moderate) $z$ values, this index is completely impractical to construct, which outweighs the benefit of the supported optimal pattern matching queries. We were thus motivated to design a space-efficient index at the expense of slower yet competitive pattern matching queries. We propose an index of $\\mathcal{O}(\\frac{nz}{\\ell}\\log z)$ expected size, which can be constructed using $\\mathcal{O}(\\frac{nz}{\\ell}\\log z)$ expected space, and supports very fast pattern matching queries in expectation, for patterns of length $m\\geq \\ell$. We have implemented and evaluated several versions of our index. The best-performing version of our index is up to two orders of magnitude smaller than the state of the art in terms of both index size and construction space, while offering faster or very competitive query and construction times.","sentences":["Strings in the real world are often encoded with some level of uncertainty.","In the character-level uncertainty model, an uncertain string $X$ of length $n$ on an alphabet $\\Sigma$ is a sequence of $n$ probability distributions over $\\Sigma$. Given an uncertain string $X$ and a weight threshold $\\frac{1}{z}\\in(0,1]$, we say that pattern $P$ occurs in $X$ at position $i$, if the product of probabilities of the letters of $P$ at positions $i,\\ldots,i+|P|-1$ is at least $\\frac{1}{z}$. While indexing standard strings for online pattern searches can be performed in linear time and space, indexing uncertain strings is much more challenging.","Specifically, the state-of-the-art index for uncertain strings has $\\mathcal{O}(nz)$ size, requires $\\mathcal{O}(nz)$ time and $\\mathcal{O}(nz)$ space to be constructed, and answers pattern matching queries in the optimal $\\mathcal{O}(m+|\\text{Occ}|)$ time, where $m$ is the length of $P$ and $|\\text{Occ}|$ is the total number of occurrences of $P$ in $X$. For large $n$ and (moderate) $z$ values, this index is completely impractical to construct, which outweighs the benefit of the supported optimal pattern matching queries.","We were thus motivated to design a space-efficient index at the expense of slower yet competitive pattern matching queries.","We propose an index of $\\mathcal{O}(\\frac{nz}{\\ell}\\log z)$ expected size, which can be constructed using $\\mathcal{O}(\\frac{nz}{\\ell}\\log z)$ expected space, and supports very fast pattern matching queries in expectation, for patterns of length $m\\geq \\ell$. We have implemented and evaluated several versions of our index.","The best-performing version of our index is up to two orders of magnitude smaller than the state of the art in terms of both index size and construction space, while offering faster or very competitive query and construction times."],"url":"http://arxiv.org/abs/2403.14256v1","category":"cs.DS"}
{"created":"2024-03-21 09:01:54","title":"E-Syn: E-Graph Rewriting with Technology-Aware Cost Functions for Logic Synthesis","abstract":"Logic synthesis plays a crucial role in the digital design flow. It has a decisive influence on the final Quality of Results (QoR) of the circuit implementations. However, existing multi-level logic optimization algorithms often employ greedy approaches with a series of local optimization steps. Each step breaks the circuit into small pieces (e.g., k-feasible cuts) and applies incremental changes to individual pieces separately. These local optimization steps could limit the exploration space and may miss opportunities for significant improvements. To address the limitation, this paper proposes using e-graph in logic synthesis. The new workflow, named Esyn, makes use of the well-established e-graph infrastructure to efficiently perform logic rewriting. It explores a diverse set of equivalent Boolean representations while allowing technology-aware cost functions to better support delay-oriented and area-oriented logic synthesis. Experiments over a wide range of benchmark designs show our proposed logic optimization approach reaches a wider design space compared to the commonly used AIG-based logic synthesis flow. It achieves on average 15.29% delay saving in delay-oriented synthesis and 6.42% area saving for area-oriented synthesis.","sentences":["Logic synthesis plays a crucial role in the digital design flow.","It has a decisive influence on the final Quality of Results (QoR) of the circuit implementations.","However, existing multi-level logic optimization algorithms often employ greedy approaches with a series of local optimization steps.","Each step breaks the circuit into small pieces (e.g., k-feasible cuts) and applies incremental changes to individual pieces separately.","These local optimization steps could limit the exploration space and may miss opportunities for significant improvements.","To address the limitation, this paper proposes using e-graph in logic synthesis.","The new workflow, named Esyn, makes use of the well-established e-graph infrastructure to efficiently perform logic rewriting.","It explores a diverse set of equivalent Boolean representations while allowing technology-aware cost functions to better support delay-oriented and area-oriented logic synthesis.","Experiments over a wide range of benchmark designs show our proposed logic optimization approach reaches a wider design space compared to the commonly used AIG-based logic synthesis flow.","It achieves on average 15.29% delay saving in delay-oriented synthesis and 6.42% area saving for area-oriented synthesis."],"url":"http://arxiv.org/abs/2403.14242v1","category":"cs.AR"}
{"created":"2024-03-21 08:41:53","title":"Contrastive Balancing Representation Learning for Heterogeneous Dose-Response Curves Estimation","abstract":"Estimating the individuals' potential response to varying treatment doses is crucial for decision-making in areas such as precision medicine and management science. Most recent studies predict counterfactual outcomes by learning a covariate representation that is independent of the treatment variable. However, such independence constraints neglect much of the covariate information that is useful for counterfactual prediction, especially when the treatment variables are continuous. To tackle the above issue, in this paper, we first theoretically demonstrate the importance of the balancing and prognostic representations for unbiased estimation of the heterogeneous dose-response curves, that is, the learned representations are constrained to satisfy the conditional independence between the covariates and both of the treatment variables and the potential responses. Based on this, we propose a novel Contrastive balancing Representation learning Network using a partial distance measure, called CRNet, for estimating the heterogeneous dose-response curves without losing the continuity of treatments. Extensive experiments are conducted on synthetic and real-world datasets demonstrating that our proposal significantly outperforms previous methods.","sentences":["Estimating the individuals' potential response to varying treatment doses is crucial for decision-making in areas such as precision medicine and management science.","Most recent studies predict counterfactual outcomes by learning a covariate representation that is independent of the treatment variable.","However, such independence constraints neglect much of the covariate information that is useful for counterfactual prediction, especially when the treatment variables are continuous.","To tackle the above issue, in this paper, we first theoretically demonstrate the importance of the balancing and prognostic representations for unbiased estimation of the heterogeneous dose-response curves, that is, the learned representations are constrained to satisfy the conditional independence between the covariates and both of the treatment variables and the potential responses.","Based on this, we propose a novel Contrastive balancing Representation learning Network using a partial distance measure, called CRNet, for estimating the heterogeneous dose-response curves without losing the continuity of treatments.","Extensive experiments are conducted on synthetic and real-world datasets demonstrating that our proposal significantly outperforms previous methods."],"url":"http://arxiv.org/abs/2403.14232v1","category":"cs.LG"}
{"created":"2024-03-21 06:47:28","title":"MMIDR: Teaching Large Language Model to Interpret Multimodal Misinformation via Knowledge Distillation","abstract":"Automatic detection of multimodal misinformation has gained a widespread attention recently. However, the potential of powerful Large Language Models (LLMs) for multimodal misinformation detection remains underexplored. Besides, how to teach LLMs to interpret multimodal misinformation in cost-effective and accessible way is still an open question. To address that, we propose MMIDR, a framework designed to teach LLMs in providing fluent and high-quality textual explanations for their decision-making process of multimodal misinformation. To convert multimodal misinformation into an appropriate instruction-following format, we present a data augmentation perspective and pipeline. This pipeline consists of a visual information processing module and an evidence retrieval module. Subsequently, we prompt the proprietary LLMs with processed contents to extract rationales for interpreting the authenticity of multimodal misinformation. Furthermore, we design an efficient knowledge distillation approach to distill the capability of proprietary LLMs in explaining multimodal misinformation into open-source LLMs. To explore several research questions regarding the performance of LLMs in multimodal misinformation detection tasks, we construct an instruction-following multimodal misinformation dataset and conduct comprehensive experiments. The experimental findings reveal that our MMIDR exhibits sufficient detection performance and possesses the capacity to provide compelling rationales to support its assessments.","sentences":["Automatic detection of multimodal misinformation has gained a widespread attention recently.","However, the potential of powerful Large Language Models (LLMs) for multimodal misinformation detection remains underexplored.","Besides, how to teach LLMs to interpret multimodal misinformation in cost-effective and accessible way is still an open question.","To address that, we propose MMIDR, a framework designed to teach LLMs in providing fluent and high-quality textual explanations for their decision-making process of multimodal misinformation.","To convert multimodal misinformation into an appropriate instruction-following format, we present a data augmentation perspective and pipeline.","This pipeline consists of a visual information processing module and an evidence retrieval module.","Subsequently, we prompt the proprietary LLMs with processed contents to extract rationales for interpreting the authenticity of multimodal misinformation.","Furthermore, we design an efficient knowledge distillation approach to distill the capability of proprietary LLMs in explaining multimodal misinformation into open-source LLMs.","To explore several research questions regarding the performance of LLMs in multimodal misinformation detection tasks, we construct an instruction-following multimodal misinformation dataset and conduct comprehensive experiments.","The experimental findings reveal that our MMIDR exhibits sufficient detection performance and possesses the capacity to provide compelling rationales to support its assessments."],"url":"http://arxiv.org/abs/2403.14171v1","category":"cs.CL"}
{"created":"2024-03-21 05:58:10","title":"The properties of FR0 radio galaxies as intermediate objects in the evolution of radio galaxies","abstract":"The counter-rotation between black holes and accretion disk configuration was introduced over a decade ago to elucidate the nature of the radio loud/radio-quiet dichotomy and the jet-disk connection, but has since been applied to a plethora of observations across space and time. We briefly review the paradigm in which counter-rotation is key for the triggering of radio galaxies and its observational support, then apply it to a series of observations concerning FR0 radio galaxies. FR0 radio galaxies appear to be radio galaxies in transition, with low-spinning black holes and thus weaker but tilted jets with respect to an earlier radio quasar phase. As a result, FR0 radio galaxies are prescribed to be in an earlier phase of star formation suppression in radio galaxies, compared to a later phase that is unlikely to be less than tens of millions of years in the future if they have enough accretion fuel to evolve into more powerful FRI radio galaxies. FR0 radio galaxies will have a greater or lesser star formation suppression feedback effect depending on how long they live. Tilted jets also enhance stellar velocities in the bulge. Because FR0 jet lengths are of the same order of magnitude as the radius of the stellar bulge, FR0 jets are prescribed to have begun, more or less recently depending on their age, to affect stellar velocity dispersions as well. As a result, they will be associated with dispersion values that tend to be larger than for characteristically non-jetted active galaxies, but smaller than giant radio galaxies such as M87 that have experienced a long-term tilted and more powerful FRI jet. With these ideas it is possible to make a coarse-grained prediction for the slope of the M-{\\sigma} plane for FR0 radio galaxies with values between 4 and 8.","sentences":["The counter-rotation between black holes and accretion disk configuration was introduced over a decade ago to elucidate the nature of the radio loud/radio-quiet dichotomy and the jet-disk connection, but has since been applied to a plethora of observations across space and time.","We briefly review the paradigm in which counter-rotation is key for the triggering of radio galaxies and its observational support, then apply it to a series of observations concerning FR0 radio galaxies.","FR0 radio galaxies appear to be radio galaxies in transition, with low-spinning black holes and thus weaker but tilted jets with respect to an earlier radio quasar phase.","As a result, FR0 radio galaxies are prescribed to be in an earlier phase of star formation suppression in radio galaxies, compared to a later phase that is unlikely to be less than tens of millions of years in the future if they have enough accretion fuel to evolve into more powerful FRI radio galaxies.","FR0 radio galaxies will have a greater or lesser star formation suppression feedback effect depending on how long they live.","Tilted jets also enhance stellar velocities in the bulge.","Because FR0 jet lengths are of the same order of magnitude as the radius of the stellar bulge, FR0 jets are prescribed to have begun, more or less recently depending on their age, to affect stellar velocity dispersions as well.","As a result, they will be associated with dispersion values that tend to be larger than for characteristically non-jetted active galaxies, but smaller than giant radio galaxies such as M87 that have experienced a long-term tilted and more powerful FRI jet.","With these ideas it is possible to make a coarse-grained prediction for the slope of the M-{\\sigma} plane for FR0 radio galaxies with values between 4 and 8."],"url":"http://arxiv.org/abs/2403.14153v1","category":"astro-ph.HE"}
{"created":"2024-03-21 05:57:43","title":"Generalized Rosenbaum Bounds Sensitivity Analysis for Matched Observational Studies with Treatment Doses: Sufficiency, Consistency, and Efficiency","abstract":"In matched observational studies with binary treatments, the Rosenbaum bounds framework is arguably the most widely used sensitivity analysis framework for assessing sensitivity to unobserved covariates. Unlike the binary treatment case, although widely needed in practice, sensitivity analysis for matched observational studies with treatment doses (i.e., non-binary treatments such as ordinal treatments or continuous treatments) still lacks solid foundations and valid methodologies. We fill in this blank by establishing theoretical foundations and novel methodologies under a generalized Rosenbaum bounds sensitivity analysis framework. First, we present a criterion for assessing the validity of sensitivity analysis in matched observational studies with treatment doses and use that criterion to justify the necessity of incorporating the treatment dose information into sensitivity analysis through generalized Rosenbaum sensitivity bounds. We also generalize Rosenbaum's classic sensitivity parameter $\\Gamma$ to the non-binary treatment case and prove its sufficiency. Second, we study the asymptotic properties of sensitivity analysis by generalizing Rosenbaum's classic design sensitivity and Bahadur efficiency for testing Fisher's sharp null to the non-binary treatment case and deriving novel formulas for them. Our theoretical results showed the importance of appropriately incorporating the treatment dose into a test, which is an intrinsic distinction with the binary treatment case. Third, for testing Neyman's weak null (i.e., null sample average treatment effect), we propose the first valid sensitivity analysis procedure for matching with treatment dose through generalizing an existing optimization-based sensitivity analysis for the binary treatment case, built on the generalized Rosenbaum sensitivity bounds and large-scale mixed integer programming.","sentences":["In matched observational studies with binary treatments, the Rosenbaum bounds framework is arguably the most widely used sensitivity analysis framework for assessing sensitivity to unobserved covariates.","Unlike the binary treatment case, although widely needed in practice, sensitivity analysis for matched observational studies with treatment doses (i.e., non-binary treatments such as ordinal treatments or continuous treatments) still lacks solid foundations and valid methodologies.","We fill in this blank by establishing theoretical foundations and novel methodologies under a generalized Rosenbaum bounds sensitivity analysis framework.","First, we present a criterion for assessing the validity of sensitivity analysis in matched observational studies with treatment doses and use that criterion to justify the necessity of incorporating the treatment dose information into sensitivity analysis through generalized Rosenbaum sensitivity bounds.","We also generalize Rosenbaum's classic sensitivity parameter $\\Gamma$ to the non-binary treatment case and prove its sufficiency.","Second, we study the asymptotic properties of sensitivity analysis by generalizing Rosenbaum's classic design sensitivity and Bahadur efficiency for testing Fisher's sharp null to the non-binary treatment case and deriving novel formulas for them.","Our theoretical results showed the importance of appropriately incorporating the treatment dose into a test, which is an intrinsic distinction with the binary treatment case.","Third, for testing Neyman's weak null (i.e., null sample average treatment effect), we propose the first valid sensitivity analysis procedure for matching with treatment dose through generalizing an existing optimization-based sensitivity analysis for the binary treatment case, built on the generalized Rosenbaum sensitivity bounds and large-scale mixed integer programming."],"url":"http://arxiv.org/abs/2403.14152v1","category":"stat.ME"}
{"created":"2024-03-21 05:13:34","title":"Evidential Semantic Mapping in Off-road Environments with Uncertainty-aware Bayesian Kernel Inference","abstract":"Robotic mapping with Bayesian Kernel Inference (BKI) has shown promise in creating semantic maps by effectively leveraging local spatial information. However, existing semantic mapping methods face challenges in constructing reliable maps in unstructured outdoor scenarios due to unreliable semantic predictions. To address this issue, we propose an evidential semantic mapping, which can enhance reliability in perceptually challenging off-road environments. We integrate Evidential Deep Learning into the semantic segmentation network to obtain the uncertainty estimate of semantic prediction. Subsequently, this semantic uncertainty is incorporated into an uncertainty-aware BKI, tailored to prioritize more confident semantic predictions when accumulating semantic information. By adaptively handling semantic uncertainties, the proposed framework constructs robust representations of the surroundings even in previously unseen environments. Comprehensive experiments across various off-road datasets demonstrate that our framework enhances accuracy and robustness, consistently outperforming existing methods in scenes with high perceptual uncertainties.","sentences":["Robotic mapping with Bayesian Kernel Inference (BKI) has shown promise in creating semantic maps by effectively leveraging local spatial information.","However, existing semantic mapping methods face challenges in constructing reliable maps in unstructured outdoor scenarios due to unreliable semantic predictions.","To address this issue, we propose an evidential semantic mapping, which can enhance reliability in perceptually challenging off-road environments.","We integrate Evidential Deep Learning into the semantic segmentation network to obtain the uncertainty estimate of semantic prediction.","Subsequently, this semantic uncertainty is incorporated into an uncertainty-aware BKI, tailored to prioritize more confident semantic predictions when accumulating semantic information.","By adaptively handling semantic uncertainties, the proposed framework constructs robust representations of the surroundings even in previously unseen environments.","Comprehensive experiments across various off-road datasets demonstrate that our framework enhances accuracy and robustness, consistently outperforming existing methods in scenes with high perceptual uncertainties."],"url":"http://arxiv.org/abs/2403.14138v1","category":"cs.RO"}
{"created":"2024-03-21 05:10:26","title":"Powerful Lossy Compression for Noisy Images","abstract":"Image compression and denoising represent fundamental challenges in image processing with many real-world applications. To address practical demands, current solutions can be categorized into two main strategies: 1) sequential method; and 2) joint method. However, sequential methods have the disadvantage of error accumulation as there is information loss between multiple individual models. Recently, the academic community began to make some attempts to tackle this problem through end-to-end joint methods. Most of them ignore that different regions of noisy images have different characteristics. To solve these problems, in this paper, our proposed signal-to-noise ratio~(SNR) aware joint solution exploits local and non-local features for image compression and denoising simultaneously. We design an end-to-end trainable network, which includes the main encoder branch, the guidance branch, and the signal-to-noise ratio~(SNR) aware branch. We conducted extensive experiments on both synthetic and real-world datasets, demonstrating that our joint solution outperforms existing state-of-the-art methods.","sentences":["Image compression and denoising represent fundamental challenges in image processing with many real-world applications.","To address practical demands, current solutions can be categorized into two main strategies: 1) sequential method; and 2) joint method.","However, sequential methods have the disadvantage of error accumulation as there is information loss between multiple individual models.","Recently, the academic community began to make some attempts to tackle this problem through end-to-end joint methods.","Most of them ignore that different regions of noisy images have different characteristics.","To solve these problems, in this paper, our proposed signal-to-noise ratio~(SNR) aware joint solution exploits local and non-local features for image compression and denoising simultaneously.","We design an end-to-end trainable network, which includes the main encoder branch, the guidance branch, and the signal-to-noise ratio~(SNR) aware branch.","We conducted extensive experiments on both synthetic and real-world datasets, demonstrating that our joint solution outperforms existing state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.14135v1","category":"eess.IV"}
{"created":"2024-03-21 05:08:14","title":"Mutation of Brauer configuration algebras","abstract":"For Brauer graph algebras, tilting mutation is compatible with flip of Brauer graphs. The aim of this paper is to generalize this result to the class of Brauer configuration algebras introduced by Green and Schroll recently. More precisely, under a certain condition, we introduce flip of Brauer configurations and prove that it is compatible with tilting mutation of the corresponding Brauer configuration algebras.","sentences":["For Brauer graph algebras, tilting mutation is compatible with flip of Brauer graphs.","The aim of this paper is to generalize this result to the class of Brauer configuration algebras introduced by Green and Schroll recently.","More precisely, under a certain condition, we introduce flip of Brauer configurations and prove that it is compatible with tilting mutation of the corresponding Brauer configuration algebras."],"url":"http://arxiv.org/abs/2403.14134v1","category":"math.RT"}
{"created":"2024-03-21 04:42:42","title":"Spin injection and detection in a Si-based ferromagnetic tunnel junction: A theoretical model based on the band diagram and experimental demonstration","abstract":"We have experimentally and theoretically investigated the spin injection/detection polarization in a Si-based ferromagnetic tunnel junction with an amorphous MgO layer, and demonstrated that the experimental features of the spin polarization in a wide bias range can be well explained using our theoretical model based on the band diagram of the junction and the direct tunneling mechanism. It is shown that the spin polarization originates from the band diagrams of the ferromagnetic Fe layer and n+-Si channel in the junction, while the spin selectivity of the MgO tunnel barrier is not necessary. Besides, we clarified the mechanism of the reduction in spin polarization when the bias is high and nonlinear properties are prominent, where the widely-used spin injection/detection model proposed by Valet and Fert is not applicable. The dominant mechanism of such reduction is found to be spin accumulation saturation (SAS) at the n+-Si interface in contact with the MgO layer as the bias is increased in the spin extraction geometry, which is inevitable in semiconductor-based ferromagnetic tunnel junctions. We performed numerical calculations on a two-terminal spin transport device with a n+-Si channel using the junction properties extracted from the experiments, and revealed that the magnetoresistance (MR) ratio is suppressed mainly by SAS in a higher bias range. Furthermore, we proposed methods for improving the MR ratio in two-terminal spin transport devices. Our experiments and theoretical model provide a deep understanding of the spin injection/detection phenomena in semiconductor-based spin transport devices, toward the realization of high performance under reasonably high bias conditions for practical use.","sentences":["We have experimentally and theoretically investigated the spin injection/detection polarization in a Si-based ferromagnetic tunnel junction with an amorphous MgO layer, and demonstrated that the experimental features of the spin polarization in a wide bias range can be well explained using our theoretical model based on the band diagram of the junction and the direct tunneling mechanism.","It is shown that the spin polarization originates from the band diagrams of the ferromagnetic Fe layer and n+-Si channel in the junction, while the spin selectivity of the MgO tunnel barrier is not necessary.","Besides, we clarified the mechanism of the reduction in spin polarization when the bias is high and nonlinear properties are prominent, where the widely-used spin injection/detection model proposed by Valet and Fert is not applicable.","The dominant mechanism of such reduction is found to be spin accumulation saturation (SAS) at the n+-Si interface in contact with the MgO layer as the bias is increased in the spin extraction geometry, which is inevitable in semiconductor-based ferromagnetic tunnel junctions.","We performed numerical calculations on a two-terminal spin transport device with a n+-Si channel using the junction properties extracted from the experiments, and revealed that the magnetoresistance (MR) ratio is suppressed mainly by SAS in a higher bias range.","Furthermore, we proposed methods for improving the MR ratio in two-terminal spin transport devices.","Our experiments and theoretical model provide a deep understanding of the spin injection/detection phenomena in semiconductor-based spin transport devices, toward the realization of high performance under reasonably high bias conditions for practical use."],"url":"http://arxiv.org/abs/2403.14127v1","category":"physics.app-ph"}
{"created":"2024-03-21 04:02:40","title":"Mechanistic Insights into Temperature Effects for Ionic Conductivity in Li6PS5Cl","abstract":"Ensuring solid-state lithium batteries perform well across a wide temperature range is crucial for their practical use. Molecular dynamics (MD) simulations can provide valuable insights into the temperature dependence of the battery materials, however, the high computational cost of ab initio MD poses challenges for simulating ion migration dynamics at low temperatures. To address this issue, accurate machine-learning interatomic potentials were trained, which enable efficient and reliable simulations of the ionic diffusion processes in Li6PS5Cl over a large temperature range for long-time evolution. Our study revealed the significant impact of subtle lattice parameter variations on Li+ diffusion at low temperatures and identified the increasing influence of surface contributions as the temperature decreases. Our findings elucidate the factors influencing low temperature performance and present strategic guidance towards improving the performance of solid-state lithium batteries under these conditions.","sentences":["Ensuring solid-state lithium batteries perform well across a wide temperature range is crucial for their practical use.","Molecular dynamics (MD) simulations can provide valuable insights into the temperature dependence of the battery materials, however, the high computational cost of ab initio MD poses challenges for simulating ion migration dynamics at low temperatures.","To address this issue, accurate machine-learning interatomic potentials were trained, which enable efficient and reliable simulations of the ionic diffusion processes in Li6PS5Cl over a large temperature range for long-time evolution.","Our study revealed the significant impact of subtle lattice parameter variations on Li+ diffusion at low temperatures and identified the increasing influence of surface contributions as the temperature decreases.","Our findings elucidate the factors influencing low temperature performance and present strategic guidance towards improving the performance of solid-state lithium batteries under these conditions."],"url":"http://arxiv.org/abs/2403.14116v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-21 03:58:27","title":"Test-time Similarity Modification for Person Re-identification toward Temporal Distribution Shift","abstract":"Person re-identification (re-id), which aims to retrieve images of the same person in a given image from a database, is one of the most practical image recognition applications. In the real world, however, the environments that the images are taken from change over time. This causes a distribution shift between training and testing and degrades the performance of re-id. To maintain re-id performance, models should continue adapting to the test environment's temporal changes. Test-time adaptation (TTA), which aims to adapt models to the test environment with only unlabeled test data, is a promising way to handle this problem because TTA can adapt models instantly in the test environment. However, the previous TTA methods are designed for classification and cannot be directly applied to re-id. This is because the set of people's identities in the dataset differs between training and testing in re-id, whereas the set of classes is fixed in the current TTA methods designed for classification. To improve re-id performance in changing test environments, we propose TEst-time similarity Modification for Person re-identification (TEMP), a novel TTA method for re-id. TEMP is the first fully TTA method for re-id, which does not require any modification to pre-training. Inspired by TTA methods that refine the prediction uncertainty in classification, we aim to refine the uncertainty in re-id. However, the uncertainty cannot be computed in the same way as classification in re-id since it is an open-set task, which does not share person labels between training and testing. Hence, we propose re-id entropy, an alternative uncertainty measure for re-id computed based on the similarity between the feature vectors. Experiments show that the re-id entropy can measure the uncertainty on re-id and TEMP improves the performance of re-id in online settings where the distribution changes over time.","sentences":["Person re-identification (re-id), which aims to retrieve images of the same person in a given image from a database, is one of the most practical image recognition applications.","In the real world, however, the environments that the images are taken from change over time.","This causes a distribution shift between training and testing and degrades the performance of re-id.","To maintain re-id performance, models should continue adapting to the test environment's temporal changes.","Test-time adaptation (TTA), which aims to adapt models to the test environment with only unlabeled test data, is a promising way to handle this problem because TTA can adapt models instantly in the test environment.","However, the previous TTA methods are designed for classification and cannot be directly applied to re-id.","This is because the set of people's identities in the dataset differs between training and testing in re-id, whereas the set of classes is fixed in the current TTA methods designed for classification.","To improve re-id performance in changing test environments, we propose TEst-time similarity Modification for Person re-identification (TEMP), a novel TTA method for re-id.","TEMP is the first fully TTA method for re-id, which does not require any modification to pre-training.","Inspired by TTA methods that refine the prediction uncertainty in classification, we aim to refine the uncertainty in re-id.","However, the uncertainty cannot be computed in the same way as classification in re-id since it is an open-set task, which does not share person labels between training and testing.","Hence, we propose re-id entropy, an alternative uncertainty measure for re-id computed based on the similarity between the feature vectors.","Experiments show that the re-id entropy can measure the uncertainty on re-id and TEMP improves the performance of re-id in online settings where the distribution changes over time."],"url":"http://arxiv.org/abs/2403.14114v1","category":"cs.CV"}
{"created":"2024-03-21 03:41:10","title":"Reinforcement Learning Design for Quickest Change Detection","abstract":"The field of quickest change detection (QCD) concerns design and analysis of algorithms to estimate in real time the time at which an important event takes place, and identify properties of the post-change behavior. It is shown in this paper that approaches based on reinforcement learning (RL) can be adapted based on any \"surrogate information state\" that is adapted to the observations. Hence we are left to choose both the surrogate information state process and the algorithm. For the former, it is argued that there are many choices available, based on a rich theory of asymptotic statistics for QCD. Two approaches to RL design are considered: (i) Stochastic gradient descent based on an actor-critic formulation. Theory is largely complete for this approach: the algorithm is unbiased, and will converge to a local minimum. However, it is shown that variance of stochastic gradients can be very large, necessitating the need for commensurately long run times; (ii) Q-learning algorithms based on a version of the projected Bellman equation. It is shown that the algorithm is stable, in the sense of bounded sample paths, and that a solution to the projected Bellman equation exists under mild conditions. Numerical experiments illustrate these findings, and provide a roadmap for algorithm design in more general settings.","sentences":["The field of quickest change detection (QCD) concerns design and analysis of algorithms to estimate in real time the time at which an important event takes place, and identify properties of the post-change behavior.","It is shown in this paper that approaches based on reinforcement learning (RL) can be adapted based on any \"surrogate information state\" that is adapted to the observations.","Hence we are left to choose both the surrogate information state process and the algorithm.","For the former, it is argued that there are many choices available, based on a rich theory of asymptotic statistics for QCD.","Two approaches to RL design are considered: (i) Stochastic gradient descent based on an actor-critic formulation.","Theory is largely complete for this approach: the algorithm is unbiased, and will converge to a local minimum.","However, it is shown that variance of stochastic gradients can be very large, necessitating the need for commensurately long run times; (ii) Q-learning algorithms based on a version of the projected Bellman equation.","It is shown that the algorithm is stable, in the sense of bounded sample paths, and that a solution to the projected Bellman equation exists under mild conditions.","Numerical experiments illustrate these findings, and provide a roadmap for algorithm design in more general settings."],"url":"http://arxiv.org/abs/2403.14109v1","category":"math.OC"}
{"created":"2024-03-21 03:40:46","title":"On the Power of Quantum Distributed Proofs","abstract":"Quantum nondeterministic distributed computing was recently introduced as dQMA (distributed quantum Merlin-Arthur) protocols by Fraigniaud, Le Gall, Nishimura and Paz (ITCS 2021). In dQMA protocols, with the help of quantum proofs and local communication, nodes on a network verify a global property of the network. Fraigniaud et al. showed that, when the network size is small, there exists an exponential separation in proof size between distributed classical and quantum verification protocols, for the equality problem, where the verifiers check if all the data owned by a subset of them are identical. In this paper, we further investigate and characterize the power of the dQMA protocols for various decision problems.   First, we give a more efficient dQMA protocol for the equality problem with a simpler analysis. This is done by adding a symmetrization step on each node and exploiting properties of the permutation test, which is a generalization of the SWAP test. We also show a quantum advantage for the equality problem on path networks still persists even when the network size is large, by considering ``relay points'' between extreme nodes.   Second, we show that even in a general network, there exist efficient dQMA protocols for the ranking verification problem, the Hamming distance problem, and more problems that derive from efficient quantum one-way communication protocols. Third, in a line network, we construct an efficient dQMA protocol for a problem that has an efficient two-party QMA communication protocol.   Finally, we obtain the first lower bounds on the proof and communication cost of dQMA protocols. To prove a lower bound on the equality problem, we show any dQMA protocol with an entangled proof between nodes can be simulated with a dQMA protocol with a separable proof between nodes by using a QMA communication-complete problem introduced by Raz and Shpilka (CCC 2004).","sentences":["Quantum nondeterministic distributed computing was recently introduced as dQMA (distributed quantum Merlin-Arthur) protocols by Fraigniaud, Le Gall, Nishimura and Paz (ITCS 2021).","In dQMA protocols, with the help of quantum proofs and local communication, nodes on a network verify a global property of the network.","Fraigniaud et al. showed that, when the network size is small, there exists an exponential separation in proof size between distributed classical and quantum verification protocols, for the equality problem, where the verifiers check if all the data owned by a subset of them are identical.","In this paper, we further investigate and characterize the power of the dQMA protocols for various decision problems.   ","First, we give a more efficient dQMA protocol for the equality problem with a simpler analysis.","This is done by adding a symmetrization step on each node and exploiting properties of the permutation test, which is a generalization of the SWAP test.","We also show a quantum advantage for the equality problem on path networks still persists even when the network size is large, by considering ``relay points'' between extreme nodes.   ","Second, we show that even in a general network, there exist efficient dQMA protocols for the ranking verification problem, the Hamming distance problem, and more problems that derive from efficient quantum one-way communication protocols.","Third, in a line network, we construct an efficient dQMA protocol for a problem that has an efficient two-party QMA communication protocol.   ","Finally, we obtain the first lower bounds on the proof and communication cost of dQMA protocols.","To prove a lower bound on the equality problem, we show any dQMA protocol with an entangled proof between nodes can be simulated with a dQMA protocol with a separable proof between nodes by using a QMA communication-complete problem introduced by Raz and Shpilka (CCC 2004)."],"url":"http://arxiv.org/abs/2403.14108v1","category":"quant-ph"}
{"created":"2024-03-21 03:34:18","title":"Existence Is Chaos: Enhancing 3D Human Motion Prediction with Uncertainty Consideration","abstract":"Human motion prediction is consisting in forecasting future body poses from historically observed sequences. It is a longstanding challenge due to motion's complex dynamics and uncertainty. Existing methods focus on building up complicated neural networks to model the motion dynamics. The predicted results are required to be strictly similar to the training samples with L2 loss in current training pipeline. However, little attention has been paid to the uncertainty property which is crucial to the prediction task. We argue that the recorded motion in training data could be an observation of possible future, rather than a predetermined result. In addition, existing works calculate the predicted error on each future frame equally during training, while recent work indicated that different frames could play different roles. In this work, a novel computationally efficient encoder-decoder model with uncertainty consideration is proposed, which could learn proper characteristics for future frames by a dynamic function. Experimental results on benchmark datasets demonstrate that our uncertainty consideration approach has obvious advantages both in quantity and quality. Moreover, the proposed method could produce motion sequences with much better quality that avoids the intractable shaking artefacts. We believe our work could provide a novel perspective to consider the uncertainty quality for the general motion prediction task and encourage the studies in this field. The code will be available in https://github.com/Motionpre/Adaptive-Salient-Loss-SAGGB.","sentences":["Human motion prediction is consisting in forecasting future body poses from historically observed sequences.","It is a longstanding challenge due to motion's complex dynamics and uncertainty.","Existing methods focus on building up complicated neural networks to model the motion dynamics.","The predicted results are required to be strictly similar to the training samples with L2 loss in current training pipeline.","However, little attention has been paid to the uncertainty property which is crucial to the prediction task.","We argue that the recorded motion in training data could be an observation of possible future, rather than a predetermined result.","In addition, existing works calculate the predicted error on each future frame equally during training, while recent work indicated that different frames could play different roles.","In this work, a novel computationally efficient encoder-decoder model with uncertainty consideration is proposed, which could learn proper characteristics for future frames by a dynamic function.","Experimental results on benchmark datasets demonstrate that our uncertainty consideration approach has obvious advantages both in quantity and quality.","Moreover, the proposed method could produce motion sequences with much better quality that avoids the intractable shaking artefacts.","We believe our work could provide a novel perspective to consider the uncertainty quality for the general motion prediction task and encourage the studies in this field.","The code will be available in https://github.com/Motionpre/Adaptive-Salient-Loss-SAGGB."],"url":"http://arxiv.org/abs/2403.14104v1","category":"cs.CV"}
{"created":"2024-03-21 03:08:03","title":"Parcae: Proactive, Liveput-Optimized DNN Training on Preemptible Instances","abstract":"Deep neural networks (DNNs) are becoming progressively large and costly to train. This paper aims to reduce DNN training costs by leveraging preemptible instances on modern clouds, which can be allocated at a much lower price when idle but may be preempted by the cloud provider at any time. Prior work that supports DNN training on preemptive instances employs a reactive approach to handling instance preemptions and allocations after their occurrence, which only achieves limited performance and scalability.   We present Parcae, a system that enables cheap, fast, and scalable DNN training on preemptible instances by proactively adjusting the parallelization strategy of a DNN training job to adapt to predicted resource changes before instance preemptions and allocations really happen, which significantly reduces the cost of handling these events. Parcae optimizes liveput, a novel metric that measures the expected training throughput of a DNN job under various possible preemption scenarios. Compared to existing reactive, throughput-optimized systems, Parcae's proactive, live-optimized solution considers both the throughput of a job and its robustness under preemptions. To optimize liveput, Parcae supports lightweight instance migration and uses an availability predictor to forecast future preemptions. It then uses a liveput optimizer to discover an optimal strategy to parallelize DNN training under predicted preemptions. We evaluate Parcae on a variety of DNNs and preemption traces and show that Parcae outperforms existing spot-instance DNN training systems by up to 10$\\times$. More importantly, Parcae achieves near-optimal performance for training large DNNs under frequent preemptions, in which case existing approaches cannot make any progress.","sentences":["Deep neural networks (DNNs) are becoming progressively large and costly to train.","This paper aims to reduce DNN training costs by leveraging preemptible instances on modern clouds, which can be allocated at a much lower price when idle but may be preempted by the cloud provider at any time.","Prior work that supports DNN training on preemptive instances employs a reactive approach to handling instance preemptions and allocations after their occurrence, which only achieves limited performance and scalability.   ","We present Parcae, a system that enables cheap, fast, and scalable DNN training on preemptible instances by proactively adjusting the parallelization strategy of a DNN training job to adapt to predicted resource changes before instance preemptions and allocations really happen, which significantly reduces the cost of handling these events.","Parcae optimizes liveput, a novel metric that measures the expected training throughput of a DNN job under various possible preemption scenarios.","Compared to existing reactive, throughput-optimized systems, Parcae's proactive, live-optimized solution considers both the throughput of a job and its robustness under preemptions.","To optimize liveput, Parcae supports lightweight instance migration and uses an availability predictor to forecast future preemptions.","It then uses a liveput optimizer to discover an optimal strategy to parallelize DNN training under predicted preemptions.","We evaluate Parcae on a variety of DNNs and preemption traces and show that Parcae outperforms existing spot-instance DNN training systems by up to 10$\\times$. More importantly, Parcae achieves near-optimal performance for training large DNNs under frequent preemptions, in which case existing approaches cannot make any progress."],"url":"http://arxiv.org/abs/2403.14097v1","category":"cs.DC"}
{"created":"2024-03-21 03:02:46","title":"Thrust Generation by Shark Denticles","abstract":"DNS is performed for flow separation over a bump in a turbulent channel. Comparisons are made between a smooth bump configuration and one where the lee side is covered with replicas of complete shark denticles. As flow over the bump is under an adverse pressure gradient (APG), a reversed pore flow (RPF) is formed in the porous cavities underneath the crowns of the denticle array. Remarkable thrust is generated by the RPF as denticle necks accelerate the fluid passing between them in the upstream streamwise direction. Several geometrical features of shark denticle, including some that had not previously been considered hydrodynamically functional, are identified to form an anisotropic permeable porous media that enables and sustains the RPF and thrust generation. The RPF is activated by the APG before massive flow reversal. The results indicate a proactive, on-demand drag reduction mechanism that leverages and transforms the APG into a favorable outcome.","sentences":["DNS is performed for flow separation over a bump in a turbulent channel.","Comparisons are made between a smooth bump configuration and one where the lee side is covered with replicas of complete shark denticles.","As flow over the bump is under an adverse pressure gradient (APG), a reversed pore flow (RPF) is formed in the porous cavities underneath the crowns of the denticle array.","Remarkable thrust is generated by the RPF as denticle necks accelerate the fluid passing between them in the upstream streamwise direction.","Several geometrical features of shark denticle, including some that had not previously been considered hydrodynamically functional, are identified to form an anisotropic permeable porous media that enables and sustains the RPF and thrust generation.","The RPF is activated by the APG before massive flow reversal.","The results indicate a proactive, on-demand drag reduction mechanism that leverages and transforms the APG into a favorable outcome."],"url":"http://arxiv.org/abs/2403.14095v1","category":"physics.flu-dyn"}
{"created":"2024-03-21 02:35:53","title":"Structure-preserving, weighted implicit-explicit schemes for multi-phase incompressible Navier-Stokes/Darcy coupled nonlocal Allen-Cahn model","abstract":"A multitude of substances exist as mixtures comprising multiple chemical components in the natural world. These substances undergo morphological changes under external influences. the phase field model coupled with fluid flow, the dynamic movement and evolution of the phase interface intricately interact with the fluid motion. This article focuses on the N-component models that couple the conservative Allen-Cahn equation with two types of incompressible fluid flow systems: the Navier-Stokes equation and the Darcy equation. By utilizing the scalar auxiliary variable method and the projection method, we innovatively construct two types of structure-preserving weighted implicit-explicit schemes for the coupled models, resulting in fully decoupled linear systems and second-order accuracy in time. The schemes are proved to be mass-conservative. In addition, with the application of $G$-norm inspired by the idea of $G$-stability, we rigorously establish its unconditional energy stability. Finally, the performance of the proposed scheme is verified by some numerical simulations.","sentences":["A multitude of substances exist as mixtures comprising multiple chemical components in the natural world.","These substances undergo morphological changes under external influences.","the phase field model coupled with fluid flow, the dynamic movement and evolution of the phase interface intricately interact with the fluid motion.","This article focuses on the N-component models that couple the conservative Allen-Cahn equation with two types of incompressible fluid flow systems: the Navier-Stokes equation and the Darcy equation.","By utilizing the scalar auxiliary variable method and the projection method, we innovatively construct two types of structure-preserving weighted implicit-explicit schemes for the coupled models, resulting in fully decoupled linear systems and second-order accuracy in time.","The schemes are proved to be mass-conservative.","In addition, with the application of $G$-norm inspired by the idea of $G$-stability, we rigorously establish its unconditional energy stability.","Finally, the performance of the proposed scheme is verified by some numerical simulations."],"url":"http://arxiv.org/abs/2403.14086v1","category":"math.NA"}
{"created":"2024-03-21 02:19:54","title":"EventDance: Unsupervised Source-free Cross-modal Adaptation for Event-based Object Recognition","abstract":"In this paper, we make the first attempt at achieving the cross-modal (i.e., image-to-events) adaptation for event-based object recognition without accessing any labeled source image data owning to privacy and commercial issues. Tackling this novel problem is non-trivial due to the novelty of event cameras and the distinct modality gap between images and events. In particular, as only the source model is available, a hurdle is how to extract the knowledge from the source model by only using the unlabeled target event data while achieving knowledge transfer. To this end, we propose a novel framework, dubbed EventDance for this unsupervised source-free cross-modal adaptation problem. Importantly, inspired by event-to-video reconstruction methods, we propose a reconstruction-based modality bridging (RMB) module, which reconstructs intensity frames from events in a self-supervised manner. This makes it possible to build up the surrogate images to extract the knowledge (i.e., labels) from the source model. We then propose a multi-representation knowledge adaptation (MKA) module that transfers the knowledge to target models learning events with multiple representation types for fully exploring the spatiotemporal information of events. The two modules connecting the source and target models are mutually updated so as to achieve the best performance. Experiments on three benchmark datasets with two adaption settings show that EventDance is on par with prior methods utilizing the source data.","sentences":["In this paper, we make the first attempt at achieving the cross-modal (i.e., image-to-events) adaptation for event-based object recognition without accessing any labeled source image data owning to privacy and commercial issues.","Tackling this novel problem is non-trivial due to the novelty of event cameras and the distinct modality gap between images and events.","In particular, as only the source model is available, a hurdle is how to extract the knowledge from the source model by only using the unlabeled target event data while achieving knowledge transfer.","To this end, we propose a novel framework, dubbed EventDance for this unsupervised source-free cross-modal adaptation problem.","Importantly, inspired by event-to-video reconstruction methods, we propose a reconstruction-based modality bridging (RMB) module, which reconstructs intensity frames from events in a self-supervised manner.","This makes it possible to build up the surrogate images to extract the knowledge (i.e., labels) from the source model.","We then propose a multi-representation knowledge adaptation (MKA) module that transfers the knowledge to target models learning events with multiple representation types for fully exploring the spatiotemporal information of events.","The two modules connecting the source and target models are mutually updated so as to achieve the best performance.","Experiments on three benchmark datasets with two adaption settings show that EventDance is on par with prior methods utilizing the source data."],"url":"http://arxiv.org/abs/2403.14082v1","category":"cs.CV"}
{"created":"2024-03-21 01:53:42","title":"On the lineshapes of temperature-dependent transport measurements of superconductors under pressure","abstract":"Recent reports of superconductivity in the vicinity of room temperature have been the subject of discussion by the community. Specifically, features in the resistance-temperature (R-T) relations have raised questions. We show that many of these features can arise from previously unaccounted-for dynamic effects associated with the AC transport techniques often used in high-pressure experiments. These dynamic AC effects can can cause the apparent resistance (Rapparent) to diverge from the DC resistance (RDC), sharpen measured superconducting transitions, and produce other features in the measured R-T response. We also show that utilizing the full output of phase-sensitive transport measurements provides a valuable probe of superconducting samples in difficult to measure systems","sentences":["Recent reports of superconductivity in the vicinity of room temperature have been the subject of discussion by the community.","Specifically, features in the resistance-temperature (R-T) relations have raised questions.","We show that many of these features can arise from previously unaccounted-for dynamic effects associated with the AC transport techniques often used in high-pressure experiments.","These dynamic AC effects can can cause the apparent resistance (Rapparent) to diverge from the DC resistance (RDC), sharpen measured superconducting transitions, and produce other features in the measured R-T response.","We also show that utilizing the full output of phase-sensitive transport measurements provides a valuable probe of superconducting samples in difficult to measure systems"],"url":"http://arxiv.org/abs/2403.14075v1","category":"cond-mat.supr-con"}
{"created":"2024-03-21 01:47:22","title":"A Taxonomy of Ambiguity Types for NLP","abstract":"Ambiguity is an critical component of language that allows for more effective communication between speakers, but is often ignored in NLP. Recent work suggests that NLP systems may struggle to grasp certain elements of human language understanding because they may not handle ambiguities at the level that humans naturally do in communication. Additionally, different types of ambiguity may serve different purposes and require different approaches for resolution, and we aim to investigate how language models' abilities vary across types. We propose a taxonomy of ambiguity types as seen in English to facilitate NLP analysis. Our taxonomy can help make meaningful splits in language ambiguity data, allowing for more fine-grained assessments of both datasets and model performance.","sentences":["Ambiguity is an critical component of language that allows for more effective communication between speakers, but is often ignored in NLP.","Recent work suggests that NLP systems may struggle to grasp certain elements of human language understanding because they may not handle ambiguities at the level that humans naturally do in communication.","Additionally, different types of ambiguity may serve different purposes and require different approaches for resolution, and we aim to investigate how language models' abilities vary across types.","We propose a taxonomy of ambiguity types as seen in English to facilitate NLP analysis.","Our taxonomy can help make meaningful splits in language ambiguity data, allowing for more fine-grained assessments of both datasets and model performance."],"url":"http://arxiv.org/abs/2403.14072v1","category":"cs.CL"}
{"created":"2024-03-21 01:30:24","title":"Automatic Outlier Rectification via Optimal Transport","abstract":"In this paper, we propose a novel conceptual framework to detect outliers using optimal transport with a concave cost function. Conventional outlier detection approaches typically use a two-stage procedure: first, outliers are detected and removed, and then estimation is performed on the cleaned data. However, this approach does not inform outlier removal with the estimation task, leaving room for improvement. To address this limitation, we propose an automatic outlier rectification mechanism that integrates rectification and estimation within a joint optimization framework. We take the first step to utilize an optimal transport distance with a concave cost function to construct a rectification set in the space of probability distributions. Then, we select the best distribution within the rectification set to perform the estimation task. Notably, the concave cost function we introduced in this paper is the key to making our estimator effectively identify the outlier during the optimization process. We discuss the fundamental differences between our estimator and optimal transport-based distributionally robust optimization estimator. finally, we demonstrate the effectiveness and superiority of our approach over conventional approaches in extensive simulation and empirical analyses for mean estimation, least absolute regression, and the fitting of option implied volatility surfaces.","sentences":["In this paper, we propose a novel conceptual framework to detect outliers using optimal transport with a concave cost function.","Conventional outlier detection approaches typically use a two-stage procedure: first, outliers are detected and removed, and then estimation is performed on the cleaned data.","However, this approach does not inform outlier removal with the estimation task, leaving room for improvement.","To address this limitation, we propose an automatic outlier rectification mechanism that integrates rectification and estimation within a joint optimization framework.","We take the first step to utilize an optimal transport distance with a concave cost function to construct a rectification set in the space of probability distributions.","Then, we select the best distribution within the rectification set to perform the estimation task.","Notably, the concave cost function we introduced in this paper is the key to making our estimator effectively identify the outlier during the optimization process.","We discuss the fundamental differences between our estimator and optimal transport-based distributionally robust optimization estimator.","finally, we demonstrate the effectiveness and superiority of our approach over conventional approaches in extensive simulation and empirical analyses for mean estimation, least absolute regression, and the fitting of option implied volatility surfaces."],"url":"http://arxiv.org/abs/2403.14067v1","category":"stat.ML"}
{"created":"2024-03-21 01:25:39","title":"LeFusion: Synthesizing Myocardial Pathology on Cardiac MRI via Lesion-Focus Diffusion Models","abstract":"Data generated in clinical practice often exhibits biases, such as long-tail imbalance and algorithmic unfairness. This study aims to mitigate these challenges through data synthesis. Previous efforts in medical imaging synthesis have struggled with separating lesion information from background context, leading to difficulties in generating high-quality backgrounds and limited control over the synthetic output. Inspired by diffusion-based image inpainting, we propose LeFusion, lesion-focused diffusion models. By redesigning the diffusion learning objectives to concentrate on lesion areas, it simplifies the model learning process and enhance the controllability of the synthetic output, while preserving background by integrating forward-diffused background contexts into the reverse diffusion process. Furthermore, we generalize it to jointly handle multi-class lesions, and further introduce a generative model for lesion masks to increase synthesis diversity. Validated on the DE-MRI cardiac lesion segmentation dataset (Emidec), our methodology employs the popular nnUNet to demonstrate that the synthetic data make it possible to effectively enhance a state-of-the-art model. Code and model are available at https://github.com/M3DV/LeFusion.","sentences":["Data generated in clinical practice often exhibits biases, such as long-tail imbalance and algorithmic unfairness.","This study aims to mitigate these challenges through data synthesis.","Previous efforts in medical imaging synthesis have struggled with separating lesion information from background context, leading to difficulties in generating high-quality backgrounds and limited control over the synthetic output.","Inspired by diffusion-based image inpainting, we propose LeFusion, lesion-focused diffusion models.","By redesigning the diffusion learning objectives to concentrate on lesion areas, it simplifies the model learning process and enhance the controllability of the synthetic output, while preserving background by integrating forward-diffused background contexts into the reverse diffusion process.","Furthermore, we generalize it to jointly handle multi-class lesions, and further introduce a generative model for lesion masks to increase synthesis diversity.","Validated on the DE-MRI cardiac lesion segmentation dataset (Emidec), our methodology employs the popular nnUNet to demonstrate that the synthetic data make it possible to effectively enhance a state-of-the-art model.","Code and model are available at https://github.com/M3DV/LeFusion."],"url":"http://arxiv.org/abs/2403.14066v1","category":"eess.IV"}
{"created":"2024-03-21 01:20:32","title":"DiffSTOCK: Probabilistic relational Stock Market Predictions using Diffusion Models","abstract":"In this work, we propose an approach to generalize denoising diffusion probabilistic models for stock market predictions and portfolio management. Present works have demonstrated the efficacy of modeling interstock relations for market time-series forecasting and utilized Graph-based learning models for value prediction and portfolio management. Though convincing, these deterministic approaches still fall short of handling uncertainties i.e., due to the low signal-to-noise ratio of the financial data, it is quite challenging to learn effective deterministic models. Since the probabilistic methods have shown to effectively emulate higher uncertainties for time-series predictions. To this end, we showcase effective utilisation of Denoising Diffusion Probabilistic Models (DDPM), to develop an architecture for providing better market predictions conditioned on the historical financial indicators and inter-stock relations. Additionally, we also provide a novel deterministic architecture MaTCHS which uses Masked Relational Transformer(MRT) to exploit inter-stock relations along with historical stock features. We demonstrate that our model achieves SOTA performance for movement predication and Portfolio management.","sentences":["In this work, we propose an approach to generalize denoising diffusion probabilistic models for stock market predictions and portfolio management.","Present works have demonstrated the efficacy of modeling interstock relations for market time-series forecasting and utilized Graph-based learning models for value prediction and portfolio management.","Though convincing, these deterministic approaches still fall short of handling uncertainties i.e., due to the low signal-to-noise ratio of the financial data, it is quite challenging to learn effective deterministic models.","Since the probabilistic methods have shown to effectively emulate higher uncertainties for time-series predictions.","To this end, we showcase effective utilisation of Denoising Diffusion Probabilistic Models (DDPM), to develop an architecture for providing better market predictions conditioned on the historical financial indicators and inter-stock relations.","Additionally, we also provide a novel deterministic architecture MaTCHS which uses Masked Relational Transformer(MRT) to exploit inter-stock relations along with historical stock features.","We demonstrate that our model achieves SOTA performance for movement predication and Portfolio management."],"url":"http://arxiv.org/abs/2403.14063v1","category":"cs.LG"}
{"created":"2024-03-21 01:11:55","title":"Exploring the role of the halo mass function for inferring astrophysical parameters during reionisation","abstract":"The detection of the 21-cm signal at $z\\gtrsim6$ will reveal insights into the properties of the first galaxies responsible for driving reionisation. To extract this information, we perform parameter inference which requires embedding 3D simulations of the 21-cm signal within a Bayesian inference pipeline. Presently, when performing inference we must choose which sources of uncertainty to sample and which to hold fixed. Since the astrophysics of galaxies are much more uncertain than those of the underlying halo-mass function (HMF), we usually parameterise and model the former while fixing the latter. However, in doing so we may bias our inference of the properties of these first galaxies. In this work, we explore the consequences of assuming an incorrect choice of HMF and quantify the relative biases in our inferred astrophysical model parameters when considering the wrong HMF. We then relax this assumption by constructing a generalised five parameter model for the HMF and simultaneously recover these parameters along with our underlying astrophysical model. For this analysis, we use 21cmFAST and perform Simulation-Based Inference by applying marginal neural ratio estimation to learn the likelihood-to-evidence ratio using Swyft. Using a mock 1000 hour observation of the 21-cm power spectrum from the forthcoming Square Kilometre Array, conservatively assuming foreground wedge avoidance, we find assuming the incorrect HMF can bias the recovered astrophysical parameters by up to $\\sim3-4\\sigma$ even when including independent information from observed luminosity functions. When considering our generalised HMF model, we recover constraints on our astrophysical parameters with a factor of $\\sim2-4$ larger marginalised uncertainties. Importantly, these constraints are unbiased, agnostic to the underlying HMF and therefore more conservative.","sentences":["The detection of the 21-cm signal at $z\\gtrsim6$ will reveal insights into the properties of the first galaxies responsible for driving reionisation.","To extract this information, we perform parameter inference which requires embedding 3D simulations of the 21-cm signal within a Bayesian inference pipeline.","Presently, when performing inference we must choose which sources of uncertainty to sample and which to hold fixed.","Since the astrophysics of galaxies are much more uncertain than those of the underlying halo-mass function (HMF), we usually parameterise and model the former while fixing the latter.","However, in doing so we may bias our inference of the properties of these first galaxies.","In this work, we explore the consequences of assuming an incorrect choice of HMF and quantify the relative biases in our inferred astrophysical model parameters when considering the wrong HMF.","We then relax this assumption by constructing a generalised five parameter model for the HMF and simultaneously recover these parameters along with our underlying astrophysical model.","For this analysis, we use 21cmFAST and perform Simulation-Based Inference by applying marginal neural ratio estimation to learn the likelihood-to-evidence ratio using Swyft.","Using a mock 1000 hour observation of the 21-cm power spectrum from the forthcoming Square Kilometre Array, conservatively assuming foreground wedge avoidance, we find assuming the incorrect HMF can bias the recovered astrophysical parameters by up to $\\sim3-4\\sigma$ even when including independent information from observed luminosity functions.","When considering our generalised HMF model, we recover constraints on our astrophysical parameters with a factor of $\\sim2-4$ larger marginalised uncertainties.","Importantly, these constraints are unbiased, agnostic to the underlying HMF and therefore more conservative."],"url":"http://arxiv.org/abs/2403.14061v1","category":"astro-ph.CO"}
{"created":"2024-03-21 01:11:01","title":"Inferring astrophysical parameters using the 2D cylindrical power spectrum from reionisation","abstract":"Enlightening our understanding of the first galaxies responsible for driving reionisation requires detecting the 21-cm signal from neutral hydrogen. Interpreting the wealth of information embedded in this signal requires Bayesian inference. Parameter inference from the 21-cm signal is primarily restricted to the spherically averaged power spectrum (1D PS) owing to its relatively straightforward derivation of an analytic likelihood function enabling traditional Monte-Carlo Markov-Chain (MCMC) approaches. However, in recent years, simulation-based inference (SBI) has become feasible which removes the necessity of having an analytic likelihood, enabling more complex summary statistics of the 21-cm signal to be used for Bayesian inference. In this work, we use SBI, specifically marginal neural ratio estimation to learn the likelihood-to-evidence ratio with Swyft, to explore parameter inference using the cylindrically averaged 2D PS. Since the 21-cm signal is anisotropic, the 2D PS should yield more constraining information compared to the 1D PS which isotropically averages the signal. For this, we consider a mock 1000 hr observation of the 21-cm signal using the SKA and compare the performance of the 2D PS relative to the 1D PS. Additionally, we explore two separate foreground mitigation strategies, perfect foreground removal and wedge avoidance. We find the 2D PS outperforms the 1D PS by improving the marginalised uncertainties on individual astrophysical parameters by up to $\\sim30-40$ per cent irrespective of the foreground mitigation strategy. Primarily, these improvements stem from how the 2D PS distinguishes between the transverse, $k_{\\perp}$, and redshift dependent, $k_{\\parallel}$ information which enables greater sensitivity to the complex reionisation morphology.","sentences":["Enlightening our understanding of the first galaxies responsible for driving reionisation requires detecting the 21-cm signal from neutral hydrogen.","Interpreting the wealth of information embedded in this signal requires Bayesian inference.","Parameter inference from the 21-cm signal is primarily restricted to the spherically averaged power spectrum (1D PS) owing to its relatively straightforward derivation of an analytic likelihood function enabling traditional Monte-Carlo Markov-Chain (MCMC) approaches.","However, in recent years, simulation-based inference (SBI) has become feasible which removes the necessity of having an analytic likelihood, enabling more complex summary statistics of the 21-cm signal to be used for Bayesian inference.","In this work, we use SBI, specifically marginal neural ratio estimation to learn the likelihood-to-evidence ratio with Swyft, to explore parameter inference using the cylindrically averaged 2D PS.","Since the 21-cm signal is anisotropic, the 2D PS should yield more constraining information compared to the 1D PS which isotropically averages the signal.","For this, we consider a mock 1000 hr observation of the 21-cm signal using the SKA and compare the performance of the 2D PS relative to the 1D PS.","Additionally, we explore two separate foreground mitigation strategies, perfect foreground removal and wedge avoidance.","We find the 2D PS outperforms the 1D PS by improving the marginalised uncertainties on individual astrophysical parameters by up to $\\sim30-40$ per cent irrespective of the foreground mitigation strategy.","Primarily, these improvements stem from how the 2D PS distinguishes between the transverse, $k_{\\perp}$, and redshift dependent, $k_{\\parallel}$ information which enables greater sensitivity to the complex reionisation morphology."],"url":"http://arxiv.org/abs/2403.14060v1","category":"astro-ph.CO"}
{"created":"2024-03-21 01:06:47","title":"Hypothesis-Driven Deep Learning for Out of Distribution Detection","abstract":"Predictions of opaque black-box systems are frequently deployed in high-stakes applications such as healthcare. For such applications, it is crucial to assess how models handle samples beyond the domain of training data. While several metrics and tests exist to detect out-of-distribution (OoD) data from in-distribution (InD) data to a deep neural network (DNN), their performance varies significantly across datasets, models, and tasks, which limits their practical use. In this paper, we propose a hypothesis-driven approach to quantify whether a new sample is InD or OoD. Given a trained DNN and some input, we first feed the input through the DNN and compute an ensemble of OoD metrics, which we term latent responses. We then formulate the OoD detection problem as a hypothesis test between latent responses of different groups, and use permutation-based resampling to infer the significance of the observed latent responses under a null hypothesis. We adapt our method to detect an unseen sample of bacteria to a trained deep learning model, and show that it reveals interpretable differences between InD and OoD latent responses. Our work has implications for systematic novelty detection and informed decision-making from classifiers trained on a subset of labels.","sentences":["Predictions of opaque black-box systems are frequently deployed in high-stakes applications such as healthcare.","For such applications, it is crucial to assess how models handle samples beyond the domain of training data.","While several metrics and tests exist to detect out-of-distribution (OoD) data from in-distribution (InD) data to a deep neural network (DNN), their performance varies significantly across datasets, models, and tasks, which limits their practical use.","In this paper, we propose a hypothesis-driven approach to quantify whether a new sample is InD or OoD.","Given a trained DNN and some input, we first feed the input through the DNN and compute an ensemble of OoD metrics, which we term latent responses.","We then formulate the OoD detection problem as a hypothesis test between latent responses of different groups, and use permutation-based resampling to infer the significance of the observed latent responses under a null hypothesis.","We adapt our method to detect an unseen sample of bacteria to a trained deep learning model, and show that it reveals interpretable differences between InD and OoD latent responses.","Our work has implications for systematic novelty detection and informed decision-making from classifiers trained on a subset of labels."],"url":"http://arxiv.org/abs/2403.14058v1","category":"cs.LG"}
{"created":"2024-03-21 00:45:38","title":"Triboelectrically mediated self-assembly and manipulation of drops at an interface","abstract":"The fluid-fluid interface is a complex environment for a floating object where the statics and dynamics may be governed by capillarity, gravity, inertia, and other external body forces. Yet, the alignment of these forces in intricate ways might result in beautiful pattern formation and self-assembly of these objects, as in the case of bubble rafts or colloidal particles. While interfacial self-assembly has been explored widely, controlled manipulation of floating objects, e.g. drops, at the fluid-fluid interface still remains a challenge largely unexplored. In this work, we reveal the self-assembly and manipulation of water drops floating at an oil-air interface. We show that the assembly occurs due to electrostatic interactions between the drops and their environment. We highlight the role of the boundary surrounding the system by showing that even drops with a net zero electric charge can self-assemble under certain conditions. Using experiments and theory, we show that the depth of the oil bath plays an important role in setting the distance between the self-assembled drops. Furthermore, we demonstrate ways to manipulate the drops actively and passively at the interface.","sentences":["The fluid-fluid interface is a complex environment for a floating object where the statics and dynamics may be governed by capillarity, gravity, inertia, and other external body forces.","Yet, the alignment of these forces in intricate ways might result in beautiful pattern formation and self-assembly of these objects, as in the case of bubble rafts or colloidal particles.","While interfacial self-assembly has been explored widely, controlled manipulation of floating objects, e.g. drops, at the fluid-fluid interface still remains a challenge largely unexplored.","In this work, we reveal the self-assembly and manipulation of water drops floating at an oil-air interface.","We show that the assembly occurs due to electrostatic interactions between the drops and their environment.","We highlight the role of the boundary surrounding the system by showing that even drops with a net zero electric charge can self-assemble under certain conditions.","Using experiments and theory, we show that the depth of the oil bath plays an important role in setting the distance between the self-assembled drops.","Furthermore, we demonstrate ways to manipulate the drops actively and passively at the interface."],"url":"http://arxiv.org/abs/2403.14055v1","category":"physics.flu-dyn"}
{"created":"2024-03-21 00:07:02","title":"Desiderata of evidence for representation in neuroscience","abstract":"This paper develops a systematic framework for the evidence neuroscientists use to establish whether a neural response represents a feature. Researchers try to establish that the neural response is (1) sensitive and (2) specific to the feature, (3) invariant to other features, and (4) functional, which means that it is used downstream in the brain. We formalize these desiderata in information-theoretic terms. This formalism allows us to precisely state the desiderata while unifying the different analysis methods used in neuroscience under one framework. We discuss how common methods such as correlational analyses, decoding and encoding models, representational similarity analysis, and tests of statistical dependence are used to evaluate the desiderata. In doing so, we provide a common terminology to researchers that helps to clarify disagreements, to compare and integrate results across studies and research groups, and to identify when evidence might be missing and when evidence for some representational conclusion is strong. We illustrate the framework with several canonical examples, including the representation of orientation, numerosity, faces, and spatial location. We end by discussing how the framework can be extended to cover models of the neural code, multi-stage models, and other domains.","sentences":["This paper develops a systematic framework for the evidence neuroscientists use to establish whether a neural response represents a feature.","Researchers try to establish that the neural response is (1) sensitive and (2) specific to the feature, (3) invariant to other features, and (4) functional, which means that it is used downstream in the brain.","We formalize these desiderata in information-theoretic terms.","This formalism allows us to precisely state the desiderata while unifying the different analysis methods used in neuroscience under one framework.","We discuss how common methods such as correlational analyses, decoding and encoding models, representational similarity analysis, and tests of statistical dependence are used to evaluate the desiderata.","In doing so, we provide a common terminology to researchers that helps to clarify disagreements, to compare and integrate results across studies and research groups, and to identify when evidence might be missing and when evidence for some representational conclusion is strong.","We illustrate the framework with several canonical examples, including the representation of orientation, numerosity, faces, and spatial location.","We end by discussing how the framework can be extended to cover models of the neural code, multi-stage models, and other domains."],"url":"http://arxiv.org/abs/2403.14046v1","category":"q-bio.NC"}
{"created":"2024-03-20 23:36:06","title":"Spatial Fairness: The Case for its Importance, Limitations of Existing Work, and Guidelines for Future Research","abstract":"Despite location being increasingly used in decision-making systems employed in many sensitive domains such as mortgages and insurance, astonishingly little attention has been paid to unfairness that may seep in due to the correlation of location with characteristics considered protected under anti-discrimination law, such as race or national origin. This position paper argues for the urgent need to consider fairness with respect to location, termed \\textit{spatial fairness}, by outlining the harms that continue to be perpetuated due to location's correlation with protected characteristics. This interdisciplinary work connects knowledge from fields such as public policy, economic development, and geography to highlight how fair-AI research currently falls short of correcting for spatial biases, and does not consider challenges unique to spatial data. Furthermore, we identify limitations of the handful of spatial fairness work proposed so far, and finally, detail guidelines for future research so subsequent work may avoid such issues and help correct spatial biases.","sentences":["Despite location being increasingly used in decision-making systems employed in many sensitive domains such as mortgages and insurance, astonishingly little attention has been paid to unfairness that may seep in due to the correlation of location with characteristics considered protected under anti-discrimination law, such as race or national origin.","This position paper argues for the urgent need to consider fairness with respect to location, termed \\textit{spatial fairness}, by outlining the harms that continue to be perpetuated due to location's correlation with protected characteristics.","This interdisciplinary work connects knowledge from fields such as public policy, economic development, and geography to highlight how fair-AI research currently falls short of correcting for spatial biases, and does not consider challenges unique to spatial data.","Furthermore, we identify limitations of the handful of spatial fairness work proposed so far, and finally, detail guidelines for future research so subsequent work may avoid such issues and help correct spatial biases."],"url":"http://arxiv.org/abs/2403.14040v1","category":"cs.CY"}
{"created":"2024-03-20 23:14:43","title":"Approximate SU(5), Fine Structure Constants","abstract":"We fit the three finestructure constants of the Standard Model with three, in first approximation theoretically estimable parameters, 1) a \"unifiedscale\",turning out not equal to the Planck scale and thus only estimable by a very speculative story, 2) a \"number of layers\" being a priori the number of families, and 3) a unified coupling related to a critical coupling on a lattice. So formally we postdict the three fine structure constants! In the philosophy of our model there is a physically lattice theory with link variables taking values in a (or in the various) \"small\" representations of the Standard Model Group. We argue for that these representations functio in first approximation as were the theory a genuine $SU(5)$ theory. Next we take into account fluctuation of the gauge fields in the lattice and obtain a correction to the a priori $SU(5)$ approximation, because of course the link fluctuations not corresponding any Standard model Lie algebra, but only to the SU(5), do not exist. The model is a development of our old anti-grand-unification model having as its genuine gauge group, close to fundamental scale, a cross product of the standard model group S(U(3)x U(2)) with itself, there being one Cartesian product factor for each family. In these old works we included the hypotesis of \"multiple point criticallity principle\" which here effectively means the coupling constants be critical on the lattice. Counted relative to the Higgs scale we suggest the in our sense \"unified scale\" (where the deviations between the inverse fine structure constants deviate by quantum fluctuations being only from standard model groups, not SU(5)) makes up the 2/3 th power of the Planck scale relative to the Higgs scale, or better the top quark mass scale.","sentences":["We fit the three finestructure constants of the Standard Model with three, in first approximation theoretically estimable parameters, 1) a \"unifiedscale\",turning out not equal to the Planck scale and thus only estimable by a very speculative story, 2) a \"number of layers\" being a priori the number of families, and 3) a unified coupling related to a critical coupling on a lattice.","So formally we postdict the three fine structure constants!","In the philosophy of our model there is a physically lattice theory with link variables taking values in a (or in the various) \"small\" representations of the Standard Model Group.","We argue for that these representations functio in first approximation as were the theory a genuine $SU(5)$ theory.","Next we take into account fluctuation of the gauge fields in the lattice and obtain a correction to the a priori $SU(5)$ approximation, because of course the link fluctuations not corresponding any Standard model Lie algebra, but only to the SU(5), do not exist.","The model is a development of our old anti-grand-unification model having as its genuine gauge group, close to fundamental scale, a cross product of the standard model group S(U(3)x U(2)) with itself, there being one Cartesian product factor for each family.","In these old works we included the hypotesis of \"multiple point criticallity principle\" which here effectively means the coupling constants be critical on the lattice.","Counted relative to the Higgs scale we suggest the in our sense \"unified scale\" (where the deviations between the inverse fine structure constants deviate by quantum fluctuations being only from standard model groups, not SU(5)) makes up the 2/3 th power of the Planck scale relative to the Higgs scale, or better the top quark mass scale."],"url":"http://arxiv.org/abs/2403.14034v1","category":"hep-ph"}
{"created":"2024-03-20 22:45:07","title":"Predicting the electroporated tissue area trajectory in Electroporation-based protocol optimization","abstract":"Electroporation (EP), the temporary or permanent permeabilization of the cell membrane induced by an electric field, is the basis of various applications in medicine and food processing. In EP-based protocol optimization in terms of pulse number, such as in electrochemotherapy (ECT), irreversible electroporation (IRE), and gene electrotransfer (GET), it is essential to reach an optimal dose-response, that is, the pulse dose that maximizes the electroporated tissue area while minimizing damage. Predicting the electroporated tissue area variation in time, i.e., its trajectory, requires measuring the EP threshold trajectory and understanding the interaction between both trajectories and the damaged tissue area trajectory. Here we introduce a new methodology, based on the analysis of the nonlinear dynamic interaction of the EP threshold and damaged tissue area trajectories, that shows that the EP threshold trajectory is the time gradient of the electric field. This allows predicting the electroporated area trajectory with a single electroporated area measurement at the last pulse, thus avoiding the need to measure the EP threshold trajectory, a rather cumbersome task. Also, it makes it possible to explain at the macroscopic level why the EP threshold trajectory has an approximate exponential time decrease while the EP threshold isoline trajectory, aka the electroporated tissue area trajectory, has an approximate logarithmic time increase. Further, it permits predicting an optimal dose response in terms of pulse number in an EP-based protocol, with a single electroporated tissue area measurement. Examples of its application to an in vitro vegetal model, and to an in vivo skinfold chamber shed new light on the nonlinear dynamic behavior of electroporated tissue area, EP threshold, and damaged tissue area trajectories, paving the way for optimal treatment planning in EP-based protocols.","sentences":["Electroporation (EP), the temporary or permanent permeabilization of the cell membrane induced by an electric field, is the basis of various applications in medicine and food processing.","In EP-based protocol optimization in terms of pulse number, such as in electrochemotherapy (ECT), irreversible electroporation (IRE), and gene electrotransfer (GET), it is essential to reach an optimal dose-response, that is, the pulse dose that maximizes the electroporated tissue area while minimizing damage.","Predicting the electroporated tissue area variation in time, i.e., its trajectory, requires measuring the EP threshold trajectory and understanding the interaction between both trajectories and the damaged tissue area trajectory.","Here we introduce a new methodology, based on the analysis of the nonlinear dynamic interaction of the EP threshold and damaged tissue area trajectories, that shows that the EP threshold trajectory is the time gradient of the electric field.","This allows predicting the electroporated area trajectory with a single electroporated area measurement at the last pulse, thus avoiding the need to measure the EP threshold trajectory, a rather cumbersome task.","Also, it makes it possible to explain at the macroscopic level why the EP threshold trajectory has an approximate exponential time decrease while the EP threshold isoline trajectory, aka the electroporated tissue area trajectory, has an approximate logarithmic time increase.","Further, it permits predicting an optimal dose response in terms of pulse number in an EP-based protocol, with a single electroporated tissue area measurement.","Examples of its application to an in vitro vegetal model, and to an in vivo skinfold chamber shed new light on the nonlinear dynamic behavior of electroporated tissue area, EP threshold, and damaged tissue area trajectories, paving the way for optimal treatment planning in EP-based protocols."],"url":"http://arxiv.org/abs/2403.14022v1","category":"physics.med-ph"}
{"created":"2024-03-20 22:43:29","title":"Prospects for the determination of fundamental constants with beyond-state-of-the-art uncertainty using molecular hydrogen ion spectroscopy","abstract":"The proton, deuteron and triton masses can be determined relative to the electron mass via rovibrational spectroscopy of molecular hydrogen ions. This has to occur via comparison of the experimentally measured transition frequencies and the ab initio calculated frequencies, whose dependence on the mass ratios can be calculated precisely. In precision experiments to date (on HD$^+$ and H$_2^+$), the transitions have involved the ground vibrational level $v=0$ and excited vibrational levels with quantum numbers up to $v'=9$. For these transitions, the sensitivity of the ab initio frequency on the high-order-QED contributions is correlated with that on the mass ratios. This prevents an efficient simultaneous determination of these quantities from experimental data, so that the accuracy of the mass ratios is essentially limited by the theoretical uncertainty. Here we analyze how the accuracy of mass ratios may be improved by providing experimental transition frequencies between levels with larger quantum numbers, whose sensitivity on the mass ratio is positive rather than negative, or close to zero. This allows the unknown QED contributions and involved fundamental constants to be much more efficiently determined from a joint analysis of several measurements. We also consider scenarios where transitions of D$_2^+$ are included. We find these to be powerful approaches, allowing in principle to reach uncertainties for the mass ratios two orders smaller than CODATA 2018, and without using muonic hydrogen data. For the Rydberg constant and the charge radii, improvements by factors of 4 to 11 are projected.","sentences":["The proton, deuteron and triton masses can be determined relative to the electron mass via rovibrational spectroscopy of molecular hydrogen ions.","This has to occur via comparison of the experimentally measured transition frequencies and the ab initio calculated frequencies, whose dependence on the mass ratios can be calculated precisely.","In precision experiments to date (on HD$^+$ and H$_2^+$), the transitions have involved the ground vibrational level $v=0$ and excited vibrational levels with quantum numbers up to $v'=9$. For these transitions, the sensitivity of the ab initio frequency on the high-order-QED contributions is correlated with that on the mass ratios.","This prevents an efficient simultaneous determination of these quantities from experimental data, so that the accuracy of the mass ratios is essentially limited by the theoretical uncertainty.","Here we analyze how the accuracy of mass ratios may be improved by providing experimental transition frequencies between levels with larger quantum numbers, whose sensitivity on the mass ratio is positive rather than negative, or close to zero.","This allows the unknown QED contributions and involved fundamental constants to be much more efficiently determined from a joint analysis of several measurements.","We also consider scenarios where transitions of D$_2^+$ are included.","We find these to be powerful approaches, allowing in principle to reach uncertainties for the mass ratios two orders smaller than CODATA 2018, and without using muonic hydrogen data.","For the Rydberg constant and the charge radii, improvements by factors of 4 to 11 are projected."],"url":"http://arxiv.org/abs/2403.14021v1","category":"physics.atom-ph"}
{"created":"2024-03-20 22:35:42","title":"Linearized analysis of dissipative Two Axis Counter Twisting (TACT) squeezing for Metrology","abstract":"In this work we analyze two axis twisting in the presence of depolarizing channel dissipation. We find that spin squeezing is only possible if the dissipation is parametrically weaker than the squeezing coupling. Squeezing may be used for meteorologically useful decrease of spin noise but only in the case where the squeezing occurs before measurement, in the case one squeezes as one measures one also squeezes the signal thereby making spin squeezing ineffective for metrological gain. The key mathematical advance made in this work is the observation that TACT in the presence of depolarizing noise is equivalent to TACT with reduced polarization and no noise. We find an exponential gain in signal to noise with the exponent proportional to the ratio between the squeezing strength and the depolarization rate.","sentences":["In this work we analyze two axis twisting in the presence of depolarizing channel dissipation.","We find that spin squeezing is only possible if the dissipation is parametrically weaker than the squeezing coupling.","Squeezing may be used for meteorologically useful decrease of spin noise but only in the case where the squeezing occurs before measurement, in the case one squeezes as one measures one also squeezes the signal thereby making spin squeezing ineffective for metrological gain.","The key mathematical advance made in this work is the observation that TACT in the presence of depolarizing noise is equivalent to TACT with reduced polarization and no noise.","We find an exponential gain in signal to noise with the exponent proportional to the ratio between the squeezing strength and the depolarization rate."],"url":"http://arxiv.org/abs/2403.14017v1","category":"quant-ph"}
{"created":"2024-03-20 22:31:30","title":"Data-Driven Modeling of Dislocation Mobility from Atomistics using Physics-Informed Machine Learning","abstract":"Dislocation mobility, which dictates the response of dislocations to an applied stress, is a fundamental property of crystalline materials that governs the evolution of plastic deformation. Traditional approaches for deriving mobility laws rely on phenomenological models of the underlying physics, whose free parameters are in turn fitted to a small number of intuition-driven atomic scale simulations under varying conditions of temperature and stress. This tedious and time-consuming approach becomes particularly cumbersome for materials with complex dependencies on stress, temperature, and local environment, such as body-centered cubic crystals (BCC) metals and alloys. In this paper, we present a novel, uncertainty quantification-driven active learning paradigm for learning dislocation mobility laws from automated high-throughput large-scale molecular dynamics simulations, using Graph Neural Networks (GNN) with a physics-informed architecture. We demonstrate that this Physics-informed Graph Neural Network (PI-GNN) framework captures the underlying physics more accurately compared to existing phenomenological mobility laws in BCC metals.","sentences":["Dislocation mobility, which dictates the response of dislocations to an applied stress, is a fundamental property of crystalline materials that governs the evolution of plastic deformation.","Traditional approaches for deriving mobility laws rely on phenomenological models of the underlying physics, whose free parameters are in turn fitted to a small number of intuition-driven atomic scale simulations under varying conditions of temperature and stress.","This tedious and time-consuming approach becomes particularly cumbersome for materials with complex dependencies on stress, temperature, and local environment, such as body-centered cubic crystals (BCC) metals and alloys.","In this paper, we present a novel, uncertainty quantification-driven active learning paradigm for learning dislocation mobility laws from automated high-throughput large-scale molecular dynamics simulations, using Graph Neural Networks (GNN) with a physics-informed architecture.","We demonstrate that this Physics-informed Graph Neural Network (PI-GNN) framework captures the underlying physics more accurately compared to existing phenomenological mobility laws in BCC metals."],"url":"http://arxiv.org/abs/2403.14015v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-20 22:11:58","title":"Pricing-driven Development and Operation of SaaS : Challenges and Opportunities","abstract":"As the Software as a Service (SaaS) paradigm continues to reshape the software industry, a nuanced understanding of its operational dynamics becomes increasingly crucial. This paper delves into the intricate relationship between pricing strategies and software development within the SaaS model. Using PetClinic as a case study, we explore the implications of a Pricing-driven Development and Operation approach of SaaS systems, highlighting the delicate balance between business-driven decision-making and technical implementation challenges, shedding light on how pricing plans can shape software features and deployment. Our discussion aims to provide strategic insights for the community to navigate the complexities of this integrated approach, fostering a better alignment between business models and technological capabilities for effective cloud-based services.","sentences":["As the Software as a Service (SaaS) paradigm continues to reshape the software industry, a nuanced understanding of its operational dynamics becomes increasingly crucial.","This paper delves into the intricate relationship between pricing strategies and software development within the SaaS model.","Using PetClinic as a case study, we explore the implications of a Pricing-driven Development and Operation approach of SaaS systems, highlighting the delicate balance between business-driven decision-making and technical implementation challenges, shedding light on how pricing plans can shape software features and deployment.","Our discussion aims to provide strategic insights for the community to navigate the complexities of this integrated approach, fostering a better alignment between business models and technological capabilities for effective cloud-based services."],"url":"http://arxiv.org/abs/2403.14007v1","category":"cs.SE"}
{"created":"2024-03-20 21:51:20","title":"Real groups, symmetric varieties and Langlands duality","abstract":"Let $G_\\mathbb R$ be a connected real reductive group and let $X$ be the corresponding complex symmetric variety under the Cartan bijection. We construct a canonical equivalence between the relative Satake category of $G(\\mathcal O)$-equivariant $\\mathbb C$-constructible complexes on the loop space of $X$ and the real Satake category of $G_\\mathbb R(\\mathcal O_\\mathbb R)$-equivariant $\\mathbb C$-constructible complexes on the real affine Grassmannian. We show that the equivalence is $t$-exact with respect to the natural perverse $t$-structures and is compatible with the fusion products and Hecke actions. We further show that the relative Satake category is equivalent to the category of $\\mathbb C$-constructible complexes on the moduli stack of $G_\\mathbb R$-bundles on the real projective line $\\mathbb P^1(\\mathbb R)$ and hence provides a connection between the relative Langlands program and the geometric Langlands program for real groups. We provide numerous applications of the main theorems to real and relative Langlands duality including the formality and commutativity conjectures for the real and relative Satake categories and an identification of the dual groups for $G_\\mathbb R$ and $X$.","sentences":["Let $G_\\mathbb R$ be a connected real reductive group and let $X$ be the corresponding complex symmetric variety under the Cartan bijection.","We construct a canonical equivalence between the relative Satake category of $G(\\mathcal O)$-equivariant $\\mathbb C$-constructible complexes on the loop space of $X$ and the real Satake category of $G_\\mathbb R(\\mathcal O_\\mathbb R)$-equivariant $\\mathbb C$-constructible complexes on the real affine Grassmannian.","We show that the equivalence is $t$-exact with respect to the natural perverse $t$-structures and is compatible with the fusion products and Hecke actions.","We further show that the relative Satake category is equivalent to the category of $\\mathbb C$-constructible complexes on the moduli stack of $G_\\mathbb R$-bundles on the real projective line $\\mathbb P^1(\\mathbb R)$ and hence provides a connection between the relative Langlands program and the geometric Langlands program for real groups.","We provide numerous applications of the main theorems to real and relative Langlands duality including the formality and commutativity conjectures for the real and relative Satake categories and an identification of the dual groups for $G_\\mathbb R$ and $X$."],"url":"http://arxiv.org/abs/2403.13995v1","category":"math.RT"}
{"created":"2024-03-20 21:48:42","title":"Stationary neutron star envelopes at high accretion rates","abstract":"In this work we model stationary neutron star envelopes at high accretion rates and describe our new code for such studies. As a first step we put special emphasis on the rp-process which results in the synthesis of heavy elements and study in detail how this synthesis depends on the mass accretion rate and the chemical composition of the accreted matter. We show that at very low accretion rate, $\\dot{M} \\sim 0.01 \\dot{M}_{\\text{Edd}}$, mostly low mass ($A\\leq$ 24) elements are synthesized with a few heavier ones below the $^{40}$Ca bottleneck. However, once $\\dot{M}$ is above ${\\buildrel \\sim \\over >} 0.1 \\dot{M}_{\\text{Edd}}$ this bottleneck is surpassed and nuclei in the iron peak region ($A\\sim$ 56) are abundantly produced. At higher mass accretion rates progressively heavier nuclei are generated, reaching $A \\sim 70$ at $\\dot{M}_{\\text{Edd}}$ and $A \\sim 90$ at $5 \\dot{M}_{\\text{Edd}}$. We find that when the rp-process is efficient, the nucleosynthesis it generates is independent of the accreted abundance of CNO elements as these are directly and copiously generated once the $3\\alpha$-reaction is operating. We also explore the efficiency of the rp-process under variations of the relative abundances of H and He. Simultaneously, we put special emphasis on the density profiles of the energy generation rate particularly at high density beyond the hydrogen exhaustion point. Our results are of importance for the study of neutron stars in systems in which X-ray bursts are absent but are also of relevance for other systems in describing the low density region, mostly below $10^6$ g cm\\mmm, inbetween bursts.","sentences":["In this work we model stationary neutron star envelopes at high accretion rates and describe our new code for such studies.","As a first step we put special emphasis on the rp-process which results in the synthesis of heavy elements and study in detail how this synthesis depends on the mass accretion rate and the chemical composition of the accreted matter.","We show that at very low accretion rate, $\\dot{M} \\sim 0.01 \\dot{M}_{\\text{Edd}}$, mostly low mass ($A\\leq$ 24) elements are synthesized with a few heavier ones below the $^{40}$Ca bottleneck.","However, once $\\dot{M}$ is above ${\\buildrel \\sim \\over >} 0.1 \\dot{M}_{\\text{Edd}}$ this bottleneck is surpassed and nuclei in the iron peak region ($A\\sim$ 56) are abundantly produced.","At higher mass accretion rates progressively heavier nuclei are generated, reaching $A \\sim 70$ at $\\dot{M}_{\\text{Edd}}$ and $A \\sim 90$ at $5 \\dot{M}_{\\text{Edd}}$.","We find that when the rp-process is efficient, the nucleosynthesis it generates is independent of the accreted abundance of CNO elements as these are directly and copiously generated once the $3\\alpha$-reaction is operating.","We also explore the efficiency of the rp-process under variations of the relative abundances of H and He.","Simultaneously, we put special emphasis on the density profiles of the energy generation rate particularly at high density beyond the hydrogen exhaustion point.","Our results are of importance for the study of neutron stars in systems in which X-ray bursts are absent but are also of relevance for other systems in describing the low density region, mostly below $10^6$ g cm\\mmm, inbetween bursts."],"url":"http://arxiv.org/abs/2403.13994v1","category":"astro-ph.HE"}
{"created":"2024-03-20 21:42:29","title":"Anomalous Neutron Nuclear-Magnetic Interference Spectroscopy","abstract":"The electron-phonon interaction plays a critical role in materials electrical, thermal, optical, and superconducting properties. However, measuring the phonon mode-resolved electron-phonon interaction has been challenging. Here we propose neutron-scattering-based Anomalous Neutron nUclear-Magnetic Interference Spectroscopy (ANUBIS), where the co-existence of neutron nuclear scattering and magnetic scattering leads to anomalous dynamical structure factor under the presence of the electron-phonon interaction. Such anomalous structure factor is linear in electron-phonon coupling constant at the phonon wavevector, and is directly proportional to the momentum and energy-resolved dielectric function. The experimental configuration can be achieved using existing polarized inelastic neutron scattering setup, and an order-of-magnitude estimate shows the viability to observe the anomalous scattering signal is around $10^{-4}$ to $10^{-3}$ relative to phonon scattering, which is achievable at emerging neutron facilities. Our proposal offers an alternative neutron-based metrology to probe the crucial electronic properties.","sentences":["The electron-phonon interaction plays a critical role in materials electrical, thermal, optical, and superconducting properties.","However, measuring the phonon mode-resolved electron-phonon interaction has been challenging.","Here we propose neutron-scattering-based Anomalous Neutron nUclear-Magnetic Interference Spectroscopy (ANUBIS), where the co-existence of neutron nuclear scattering and magnetic scattering leads to anomalous dynamical structure factor under the presence of the electron-phonon interaction.","Such anomalous structure factor is linear in electron-phonon coupling constant at the phonon wavevector, and is directly proportional to the momentum and energy-resolved dielectric function.","The experimental configuration can be achieved using existing polarized inelastic neutron scattering setup, and an order-of-magnitude estimate shows the viability to observe the anomalous scattering signal is around $10^{-4}$ to $10^{-3}$ relative to phonon scattering, which is achievable at emerging neutron facilities.","Our proposal offers an alternative neutron-based metrology to probe the crucial electronic properties."],"url":"http://arxiv.org/abs/2403.13990v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-20 20:47:15","title":"Following marginal stability manifolds in quasilinear dynamical reductions of multiscale flows in two space dimensions","abstract":"A two-dimensional extension of a recently developed formalism for slow-fast quasilinear (QL) systems subject to fast instabilities is derived. Prior work has demonstrated that the emergent dynamics of these systems is characterized by a slow evolution of mean fields coupled to marginally stable, fast fluctuation fields. By exploiting this emergent behavior, an efficient fast-eigenvalue/slow-initial-value solution algorithm can be developed in which the amplitude of the fast fluctuations is slaved to the slowly evolving mean fields to ensure marginal stability (and temporal scale separation) is maintained. For 2D systems that are spatially-extended in one direction, the fluctuation eigenfunctions are labeled by their wavenumbers characterizing spatial variability in that direction, and the marginal mode(s) also must coincide with the fastest-growing mode(s) over all admissible wavenumbers. Here, we introduce two equivalent procedures for deriving an ordinary differential equation governing the slow evolution of the wavenumber of the fastest-growing fluctuation mode that simultaneously must be slaved to the mean dynamics to ensure the mode has zero growth rate. We illustrate the procedure in the context of a 2D model partial differential equation that shares certain attributes with the equations governing strongly stratified shear flows. The slaved evolution follows one or more marginal stability manifolds, which constitute select state-space structures that are not invariant under the full flow dynamics yet capture quasi-coherent states in physical space in a manner analogous to invariant solutions identified in, e.g., transitionally-turbulent shear flows. Accordingly, we propose that marginal stability manifolds are central organizing structures in a dynamical systems description of certain classes of multiscale flows where scale separation justifies a QL approximation of the dynamics.","sentences":["A two-dimensional extension of a recently developed formalism for slow-fast quasilinear (QL) systems subject to fast instabilities is derived.","Prior work has demonstrated that the emergent dynamics of these systems is characterized by a slow evolution of mean fields coupled to marginally stable, fast fluctuation fields.","By exploiting this emergent behavior, an efficient fast-eigenvalue/slow-initial-value solution algorithm can be developed in which the amplitude of the fast fluctuations is slaved to the slowly evolving mean fields to ensure marginal stability (and temporal scale separation) is maintained.","For 2D systems that are spatially-extended in one direction, the fluctuation eigenfunctions are labeled by their wavenumbers characterizing spatial variability in that direction, and the marginal mode(s) also must coincide with the fastest-growing mode(s) over all admissible wavenumbers.","Here, we introduce two equivalent procedures for deriving an ordinary differential equation governing the slow evolution of the wavenumber of the fastest-growing fluctuation mode that simultaneously must be slaved to the mean dynamics to ensure the mode has zero growth rate.","We illustrate the procedure in the context of a 2D model partial differential equation that shares certain attributes with the equations governing strongly stratified shear flows.","The slaved evolution follows one or more marginal stability manifolds, which constitute select state-space structures that are not invariant under the full flow dynamics yet capture quasi-coherent states in physical space in a manner analogous to invariant solutions identified in, e.g., transitionally-turbulent shear flows.","Accordingly, we propose that marginal stability manifolds are central organizing structures in a dynamical systems description of certain classes of multiscale flows where scale separation justifies a QL approximation of the dynamics."],"url":"http://arxiv.org/abs/2403.13971v1","category":"physics.flu-dyn"}
{"created":"2024-03-20 20:31:23","title":"Modal analysis of the triadic interactions in the dynamics of a transitional shock wave boundary layer interaction","abstract":"This work is a numerical study of a transitional shock wave boundary layer interaction (SWBLI). The main goal is to improve our understanding of the well known low-frequency SWBLI unsteadiness and especially the suspected role of triadic interactions in the underlying physical mechanism. To this end, a Direct Numerical Simulation (DNS) is performed using high-order finite volume scheme equipped with a suitable shock capture procedure. The resulting database is then extensively post-processed in order to extract the main dynamical features of the interaction zone (involved characteristic frequencies, characteristics of the vortical structures, etc.). The dynamical organisation and space-time evolution of the flow at dominant frequencies are then further characterised by mean of a Spectral Proper Orthogonal Decomposition (SPOD) analysis. In order to study the role of triadic interactions occurring in the interaction region, a BiSpectral Mode Decomposition (BSMD) analysis is applied to the data base. It allows us to extract the significant triadic interactions, their location and the resulting physical spatial modes. Strong triadic interactions are detected in the downstream part of the separation bubble whose role on the low-frequency unsteadiness is characterised. All the results of the various analyses are then discussed and integrated to formulate a possible mechanism fuelling low-frequency SWBLI unsteadiness.","sentences":["This work is a numerical study of a transitional shock wave boundary layer interaction (SWBLI).","The main goal is to improve our understanding of the well known low-frequency SWBLI unsteadiness and especially the suspected role of triadic interactions in the underlying physical mechanism.","To this end, a Direct Numerical Simulation (DNS) is performed using high-order finite volume scheme equipped with a suitable shock capture procedure.","The resulting database is then extensively post-processed in order to extract the main dynamical features of the interaction zone (involved characteristic frequencies, characteristics of the vortical structures, etc.).","The dynamical organisation and space-time evolution of the flow at dominant frequencies are then further characterised by mean of a Spectral Proper Orthogonal Decomposition (SPOD) analysis.","In order to study the role of triadic interactions occurring in the interaction region, a BiSpectral Mode Decomposition (BSMD) analysis is applied to the data base.","It allows us to extract the significant triadic interactions, their location and the resulting physical spatial modes.","Strong triadic interactions are detected in the downstream part of the separation bubble whose role on the low-frequency unsteadiness is characterised.","All the results of the various analyses are then discussed and integrated to formulate a possible mechanism fuelling low-frequency SWBLI unsteadiness."],"url":"http://arxiv.org/abs/2403.13963v1","category":"physics.flu-dyn"}
{"created":"2024-03-20 20:22:07","title":"What is isotropic turbulence and why is it important?","abstract":"This article begins with an overview, then gives the precise definition of isotropic turbulence, and follows that with the basic conservation equations, in both real space and wavenumber space. These provide the foundations of all theoretical approaches, both fundamental and phenomenological. After that, my intention is to try to highlight the main unresolved issues and give some indication of what progress there has been over decades (in all cases), and what still needs to be done. I should emphasise that I am not trying to provide either a conventional review or even a pedagogical treatment. Instead I am giving concise summaries, supplemented (where I can) by my own observations, which make substantial points that I believe are original, and which have not been made in the literature. To take just one example, it is known by some people that Kolmogorov's 1962 theory is not correctly described as a 'refinement' of his 1941 theory. This was pointed out by Kraichnan in 1974. However, what does not appear to have been recognized is that the 1962 theory is physically invalid, and also that a plausible implementation of it destroys the Kolmogorov (1941) scaling of energy spectra which has been widely observed over many years. This is discussed in Section 4 below. Lastly, I have tried to give an informal treatment in order to make everything easily accessible, to reach the widest possible audience. In particular, the section on renormalization methods is written without giving the equations of the various theories, merely stating in words what has been done, what are the different methods and also what still needs to be done.","sentences":["This article begins with an overview, then gives the precise definition of isotropic turbulence, and follows that with the basic conservation equations, in both real space and wavenumber space.","These provide the foundations of all theoretical approaches, both fundamental and phenomenological.","After that, my intention is to try to highlight the main unresolved issues and give some indication of what progress there has been over decades (in all cases), and what still needs to be done.","I should emphasise that I am not trying to provide either a conventional review or even a pedagogical treatment.","Instead I am giving concise summaries, supplemented (where I can) by my own observations, which make substantial points that I believe are original, and which have not been made in the literature.","To take just one example, it is known by some people that Kolmogorov's 1962 theory is not correctly described as a 'refinement' of his 1941 theory.","This was pointed out by Kraichnan in 1974.","However, what does not appear to have been recognized is that the 1962 theory is physically invalid, and also that a plausible implementation of it destroys the Kolmogorov (1941) scaling of energy spectra which has been widely observed over many years.","This is discussed in Section 4 below.","Lastly, I have tried to give an informal treatment in order to make everything easily accessible, to reach the widest possible audience.","In particular, the section on renormalization methods is written without giving the equations of the various theories, merely stating in words what has been done, what are the different methods and also what still needs to be done."],"url":"http://arxiv.org/abs/2403.13962v1","category":"math-ph"}
{"created":"2024-03-20 19:23:43","title":"On Regular Fusible Modules","abstract":"In this article, we introduce the notion of regular fusible modules. Let $R$ be a ring with an identity and $M$ an $R$-module. An element $0\\neq m\\in M$ is said to be regular fusible if there exists $r\\in R$, a non zero-divisor of $M$, such that $mr$ can be written as the sum of a torsion element and a torsion free element in $M$. $M$ is called regular fusible if every nonzero element of $M$ is regular fusible. We characterize regular fusible modules in terms of fusible modules. In addition, we show that a regular fusible module over a right duo ring is reduced and nonsingular. Moreover, we study the regular fusible property under Cartesian product, trivial extension ring, and module of a fractions. Also, we characterize division rings in terms of fusible modules.","sentences":["In this article, we introduce the notion of regular fusible modules.","Let $R$ be a ring with an identity and $M$ an $R$-module.","An element $0\\neq m\\in M$ is said to be regular fusible if there exists $r\\in R$, a non zero-divisor of $M$, such that $mr$ can be written as the sum of a torsion element and a torsion free element in $M$. $M$ is called regular fusible if every nonzero element of $M$ is regular fusible.","We characterize regular fusible modules in terms of fusible modules.","In addition, we show that a regular fusible module over a right duo ring is reduced and nonsingular.","Moreover, we study the regular fusible property under Cartesian product, trivial extension ring, and module of a fractions.","Also, we characterize division rings in terms of fusible modules."],"url":"http://arxiv.org/abs/2403.13939v1","category":"math.RA"}
{"created":"2024-03-20 19:18:03","title":"Joint Deconvolution of Astronomical Images in the Presence of Poisson Noise","abstract":"We present a new method for joint likelihood deconvolution (Jolideco) of a set of astronomical observations of the same sky region in the presence of Poisson noise. The observations may be obtained from different instruments with different resolution, and different point spread functions. Jolideco reconstructs a single flux image by optimizing the posterior distribution based on the joint Poisson likelihood of all observations under a patch-based image prior. The patch prior is parameterised via a Gaussian Mixture model which we train on high-signal-to-noise astronomical images, including data from the James Webb Telescope and the GLEAM radio survey. This prior favors correlation structures among the reconstructed pixel intensities that are characteristic of those observed in the training images. It is, however, not informative for the mean or scale of the reconstruction. By applying the method to simulated data we show that the combination of multiple observations and the patch-based prior leads to much improved reconstruction quality in many different source scenarios and signal to noise regimes. We demonstrate that with the patch prior Jolideco yields superior reconstruction quality relative to alternative standard methods such as the Richardson-Lucy method. We illustrate the results of Jolideco applied to example data from the Chandra X-ray Observatory and the Fermi-LAT Gamma-ray Space Telescope. By comparing the measured width of a counts based and the corresponding Jolideco flux profile of an X-ray filament in SNR 1E 0102.2-721} we find the deconvolved width of 0.58+- 0.02 arcsec to be consistent with the theoretical expectation derived from the known width of the PSF.","sentences":["We present a new method for joint likelihood deconvolution (Jolideco) of a set of astronomical observations of the same sky region in the presence of Poisson noise.","The observations may be obtained from different instruments with different resolution, and different point spread functions.","Jolideco reconstructs a single flux image by optimizing the posterior distribution based on the joint Poisson likelihood of all observations under a patch-based image prior.","The patch prior is parameterised via a Gaussian Mixture model which we train on high-signal-to-noise astronomical images, including data from the James Webb Telescope and the GLEAM radio survey.","This prior favors correlation structures among the reconstructed pixel intensities that are characteristic of those observed in the training images.","It is, however, not informative for the mean or scale of the reconstruction.","By applying the method to simulated data we show that the combination of multiple observations and the patch-based prior leads to much improved reconstruction quality in many different source scenarios and signal to noise regimes.","We demonstrate that with the patch prior Jolideco yields superior reconstruction quality relative to alternative standard methods such as the Richardson-Lucy method.","We illustrate the results of Jolideco applied to example data from the Chandra X-ray Observatory and the Fermi-LAT Gamma-ray Space Telescope.","By comparing the measured width of a counts based and the corresponding Jolideco flux profile of an X-ray filament in SNR 1E 0102.2-721} we find the deconvolved width of 0.58+- 0.02 arcsec to be consistent with the theoretical expectation derived from the known width of the PSF."],"url":"http://arxiv.org/abs/2403.13933v1","category":"astro-ph.IM"}
{"created":"2024-03-20 19:12:26","title":"High Accuracy Numerical Optimal Control for Rigid Bodies with Patch Contacts through Equivalent Contact Points -- Extended Version","abstract":"This paper extends the Finite Elements with Switch Detection and Jumps (FESD-J) [1] method to problems of rigid body dynamics involving patch contacts. The FESD-J method is a high accuracy discretization scheme suitable for use in direct optimal control of nonsmooth mechanical systems. It detects dynamic switches exactly in time and, thereby, maintains the integration order of the underlying Runge- Kutta (RK) method. This is in contrast to commonly used time-stepping methods which only achieve first-order accuracy. Considering rigid bodies with possible patch contacts results in nondifferentiable signed distance functions (SDF), which introduces additional nonsmoothness into the dynamical system. In this work, we utilize so-called equivalent contact points (ECP), which parameterize force and impulse distributions on contact patches by evaluation at single points. We embed a nondifferentiable SDF into a complementarity Lagrangian system (CLS) and show that the determined ECP are well-defined. We then extend the FESD-J discretization to the considered CLS such that its integration accuracy is maintained. The functionality of the method is illustrated for both a simulation and an optimal control example.","sentences":["This paper extends the Finite Elements with Switch Detection and Jumps (FESD-J)","[1] method to problems of rigid body dynamics involving patch contacts.","The FESD-J method is a high accuracy discretization scheme suitable for use in direct optimal control of nonsmooth mechanical systems.","It detects dynamic switches exactly in time and, thereby, maintains the integration order of the underlying Runge- Kutta (RK) method.","This is in contrast to commonly used time-stepping methods which only achieve first-order accuracy.","Considering rigid bodies with possible patch contacts results in nondifferentiable signed distance functions (SDF), which introduces additional nonsmoothness into the dynamical system.","In this work, we utilize so-called equivalent contact points (ECP), which parameterize force and impulse distributions on contact patches by evaluation at single points.","We embed a nondifferentiable SDF into a complementarity Lagrangian system (CLS) and show that the determined ECP are well-defined.","We then extend the FESD-J discretization to the considered CLS such that its integration accuracy is maintained.","The functionality of the method is illustrated for both a simulation and an optimal control example."],"url":"http://arxiv.org/abs/2403.13931v1","category":"math.OC"}
{"created":"2024-03-20 19:00:49","title":"Noise-induced shallow circuits and absence of barren plateaus","abstract":"Motivated by realistic hardware considerations of the pre-fault-tolerant era, we comprehensively study the impact of uncorrected noise on quantum circuits. We first show that any noise `truncates' most quantum circuits to effectively logarithmic depth, in the task of computing Pauli expectation values. We then prove that quantum circuits under any non-unital noise exhibit lack of barren plateaus for cost functions composed of local observables. But, by leveraging the effective shallowness, we also design a classical algorithm to estimate Pauli expectation values within inverse-polynomial additive error with high probability over the ensemble. Its runtime is independent of circuit depth and it operates in polynomial time in the number of qubits for one-dimensional architectures and quasi-polynomial time for higher-dimensional ones. Taken together, our results showcase that, unless we carefully engineer the circuits to take advantage of the noise, it is unlikely that noisy quantum circuits are preferable over shallow quantum circuits for algorithms that output Pauli expectation value estimates, like many variational quantum machine learning proposals. Moreover, we anticipate that our work could provide valuable insights into the fundamental open question about the complexity of sampling from (possibly non-unital) noisy random circuits.","sentences":["Motivated by realistic hardware considerations of the pre-fault-tolerant era, we comprehensively study the impact of uncorrected noise on quantum circuits.","We first show that any noise `truncates' most quantum circuits to effectively logarithmic depth, in the task of computing Pauli expectation values.","We then prove that quantum circuits under any non-unital noise exhibit lack of barren plateaus for cost functions composed of local observables.","But, by leveraging the effective shallowness, we also design a classical algorithm to estimate Pauli expectation values within inverse-polynomial additive error with high probability over the ensemble.","Its runtime is independent of circuit depth and it operates in polynomial time in the number of qubits for one-dimensional architectures and quasi-polynomial time for higher-dimensional ones.","Taken together, our results showcase that, unless we carefully engineer the circuits to take advantage of the noise, it is unlikely that noisy quantum circuits are preferable over shallow quantum circuits for algorithms that output Pauli expectation value estimates, like many variational quantum machine learning proposals.","Moreover, we anticipate that our work could provide valuable insights into the fundamental open question about the complexity of sampling from (possibly non-unital) noisy random circuits."],"url":"http://arxiv.org/abs/2403.13927v1","category":"quant-ph"}
{"created":"2024-03-20 18:54:52","title":"LFS-Aware Surface Reconstruction from Unoriented 3D Point Clouds","abstract":"We present a novel approach for generating isotropic surface triangle meshes directly from unoriented 3D point clouds, with mesh density adapting to the estimated local feature size (LFS). The popular reconstruction pipelines first reconstruct a dense mesh from the input point cloud and then apply remeshing to obtain the isotropic mesh. The sequential pipeline makes it hard to find a lower-density mesh while preserving more details. Instead, our approach reconstructs both an implicit function and an LFS-aware mesh sizing function directly from the input point cloud, which is then used to produce the final LFS-aware mesh without remeshing. We combine local curvature radius and shape diameter to estimate the LFS directly from the input point clouds. Also, we propose a new mesh solver to solve an implicit function whose zero level set delineates the surface without requiring normal orientation. The added value of our approach is generating isotropic meshes directly from 3D point clouds with an LFS-aware density, thus enabling flexible mesh quality control. Our experiments demonstrate the robustness of our method to noise, outliers, and missing data. Our method is also capable of preserving sharp features for CAD point clouds.","sentences":["We present a novel approach for generating isotropic surface triangle meshes directly from unoriented 3D point clouds, with mesh density adapting to the estimated local feature size (LFS).","The popular reconstruction pipelines first reconstruct a dense mesh from the input point cloud and then apply remeshing to obtain the isotropic mesh.","The sequential pipeline makes it hard to find a lower-density mesh while preserving more details.","Instead, our approach reconstructs both an implicit function and an LFS-aware mesh sizing function directly from the input point cloud, which is then used to produce the final LFS-aware mesh without remeshing.","We combine local curvature radius and shape diameter to estimate the LFS directly from the input point clouds.","Also, we propose a new mesh solver to solve an implicit function whose zero level set delineates the surface without requiring normal orientation.","The added value of our approach is generating isotropic meshes directly from 3D point clouds with an LFS-aware density, thus enabling flexible mesh quality control.","Our experiments demonstrate the robustness of our method to noise, outliers, and missing data.","Our method is also capable of preserving sharp features for CAD point clouds."],"url":"http://arxiv.org/abs/2403.13924v1","category":"cs.GR"}
{"created":"2024-03-20 18:53:22","title":"Credit vs. Discount-Based Congestion Pricing: A Comparison Study","abstract":"Tolling, or congestion pricing, offers a promising traffic management policy for regulating congestion, but has also attracted criticism for placing outsized financial burdens on low-income users. Credit-based congestion pricing (CBCP) and discount-based congestion pricing (DBCP) policies, which respectively provide travel credits and toll discounts to low-income users on tolled roads, have emerged as promising mechanisms for reducing traffic congestion without worsening societal inequities. However, the optimal design of CBCP and DBCP policies, as well as their relative advantages and disadvantages, remain poorly understood. To address this, we study the effects of implementing CBCP and DBCP policies to route users on a network of multi-lane highways with tolled express lanes. We formulate a non-atomic routing game framework in which a subset of eligible users is granted toll relief in the form of a fixed budget or toll discount, while the remaining ineligible users must pay out-of-pocket. We prove the existence of Nash equilibrium traffic flow patterns corresponding to any given CBCP or DBCP policy. Under the additional assumption that eligible users have time-invariant VoTs, we provide a convex program to efficiently compute these equilibria. For networks consisting of a single edge, we identify conditions under which CBCP policies outperform DBCP policies (and vice versa), in the sense of improving eligible users' access to the express lane. Finally, we present empirical results from a CBCP pilot study of the San Mateo 101 Express Lane Project in California. Our empirical results corroborate our theoretical analysis of the impact of deploying credit-based and discount-based policies, and lend insights into the sensitivity of their impact with respect to the travel demand and users' VoTs.","sentences":["Tolling, or congestion pricing, offers a promising traffic management policy for regulating congestion, but has also attracted criticism for placing outsized financial burdens on low-income users.","Credit-based congestion pricing (CBCP) and discount-based congestion pricing (DBCP) policies, which respectively provide travel credits and toll discounts to low-income users on tolled roads, have emerged as promising mechanisms for reducing traffic congestion without worsening societal inequities.","However, the optimal design of CBCP and DBCP policies, as well as their relative advantages and disadvantages, remain poorly understood.","To address this, we study the effects of implementing CBCP and DBCP policies to route users on a network of multi-lane highways with tolled express lanes.","We formulate a non-atomic routing game framework in which a subset of eligible users is granted toll relief in the form of a fixed budget or toll discount, while the remaining ineligible users must pay out-of-pocket.","We prove the existence of Nash equilibrium traffic flow patterns corresponding to any given CBCP or DBCP policy.","Under the additional assumption that eligible users have time-invariant VoTs, we provide a convex program to efficiently compute these equilibria.","For networks consisting of a single edge, we identify conditions under which CBCP policies outperform DBCP policies (and vice versa), in the sense of improving eligible users' access to the express lane.","Finally, we present empirical results from a CBCP pilot study of the San Mateo 101 Express Lane Project in California.","Our empirical results corroborate our theoretical analysis of the impact of deploying credit-based and discount-based policies, and lend insights into the sensitivity of their impact with respect to the travel demand and users' VoTs."],"url":"http://arxiv.org/abs/2403.13923v1","category":"eess.SY"}
{"created":"2024-03-20 18:39:47","title":"Automated Calibration of Parallel and Distributed Computing Simulators: A Case Study","abstract":"Many parallel and distributed computing research results are obtained in simulation, using simulators that mimic real-world executions on some target system. Each such simulator is configured by picking values for parameters that define the behavior of the underlying simulation models it implements. The main concern for a simulator is accuracy: simulated behaviors should be as close as possible to those observed in the real-world target system. This requires that values for each of the simulator's parameters be carefully picked, or \"calibrated,\" based on ground-truth real-world executions. Examining the current state of the art shows that simulator calibration, at least in the field of parallel and distributed computing, is often undocumented (and thus perhaps often not performed) and, when documented, is described as a labor-intensive, manual process. In this work we evaluate the benefit of automating simulation calibration using simple algorithms. Specifically, we use a real-world case study from the field of High Energy Physics and compare automated calibration to calibration performed by a domain scientist. Our main finding is that automated calibration is on par with or significantly outperforms the calibration performed by the domain scientist. Furthermore, automated calibration makes it straightforward to operate desirable trade-offs between simulation accuracy and simulation speed.","sentences":["Many parallel and distributed computing research results are obtained in simulation, using simulators that mimic real-world executions on some target system.","Each such simulator is configured by picking values for parameters that define the behavior of the underlying simulation models it implements.","The main concern for a simulator is accuracy: simulated behaviors should be as close as possible to those observed in the real-world target system.","This requires that values for each of the simulator's parameters be carefully picked, or \"calibrated,\" based on ground-truth real-world executions.","Examining the current state of the art shows that simulator calibration, at least in the field of parallel and distributed computing, is often undocumented (and thus perhaps often not performed) and, when documented, is described as a labor-intensive, manual process.","In this work we evaluate the benefit of automating simulation calibration using simple algorithms.","Specifically, we use a real-world case study from the field of High Energy Physics and compare automated calibration to calibration performed by a domain scientist.","Our main finding is that automated calibration is on par with or significantly outperforms the calibration performed by the domain scientist.","Furthermore, automated calibration makes it straightforward to operate desirable trade-offs between simulation accuracy and simulation speed."],"url":"http://arxiv.org/abs/2403.13918v1","category":"cs.DC"}
{"created":"2024-03-20 18:29:55","title":"Sequential Modeling of Complex Marine Navigation: Case Study on a Passenger Vessel (Student Abstract)","abstract":"The maritime industry's continuous commitment to sustainability has led to a dedicated exploration of methods to reduce vessel fuel consumption. This paper undertakes this challenge through a machine learning approach, leveraging a real-world dataset spanning two years of a ferry in west coast Canada. Our focus centers on the creation of a time series forecasting model given the dynamic and static states, actions, and disturbances. This model is designed to predict dynamic states based on the actions provided, subsequently serving as an evaluative tool to assess the proficiency of the ferry's operation under the captain's guidance. Additionally, it lays the foundation for future optimization algorithms, providing valuable feedback on decision-making processes. To facilitate future studies, our code is available at \\url{https://github.com/pagand/model_optimze_vessel/tree/AAAI}","sentences":["The maritime industry's continuous commitment to sustainability has led to a dedicated exploration of methods to reduce vessel fuel consumption.","This paper undertakes this challenge through a machine learning approach, leveraging a real-world dataset spanning two years of a ferry in west coast Canada.","Our focus centers on the creation of a time series forecasting model given the dynamic and static states, actions, and disturbances.","This model is designed to predict dynamic states based on the actions provided, subsequently serving as an evaluative tool to assess the proficiency of the ferry's operation under the captain's guidance.","Additionally, it lays the foundation for future optimization algorithms, providing valuable feedback on decision-making processes.","To facilitate future studies, our code is available at \\url{https://github.com/pagand/model_optimze_vessel/tree/AAAI}"],"url":"http://arxiv.org/abs/2403.13909v1","category":"cs.LG"}
{"created":"2024-03-20 18:29:16","title":"PHANGS-HST: Globular Cluster Systems in 17 Nearby Spiral Galaxies","abstract":"We present new catalogs of likely globular clusters (GCs) in 17 nearby spiral galaxies studied as part of the PHANGS-HST Treasury Survey. The galaxies were imaged in five broad-band filters from the near-ultraviolet through the $I$ band. PHANGS-HST has produced catalogs of stellar clusters of all ages by selecting extended sources (from multiple concentration index measurements) followed by morphological classification (centrally concentrated and symmetric or asymmetric, multiple peaks, contaminant) by visually examining the V-band image and separately by a machine-learning algorithm which classified larger samples to reach fainter limits. From both cluster catalogs, we select an initial list of candidate GCs to have $B-V \\geq 0.5$ and $V-I \\geq 0.73$~mag, then remove likely contaminants (including reddened young clusters, background galaxies misclassified by the neural network, and chance superpositions/blends of stars) after a careful visual inspection. We find that $\\approx86$ % of the color-selected candidates classified as spherically symmetric, and $\\approx68$ of those classified as centrally concentrated but asymmetric are likely to be GCs. The luminosity functions of the GC candidates in 2 of our 17 galaxies, NGC 628 and NGC 3627, are atypical, and continue to rise at least 1~mag fainter than the expected turnover near $M_V \\sim -7.4$. These faint candidate GCs have more extended spatial distributions than their bright counterparts, and may reside in the disk rather than the bulge/halo, similar to faint GCs previously discovered in M101. These faint clusters may be somewhat younger since the age-metallicity degeneracy makes it difficult to determine precise cluster ages from integrated colors once they reach $\\approx1$~Gyr.","sentences":["We present new catalogs of likely globular clusters (GCs) in 17 nearby spiral galaxies studied as part of the PHANGS-HST Treasury Survey.","The galaxies were imaged in five broad-band filters from the near-ultraviolet through the $I$ band.","PHANGS-HST has produced catalogs of stellar clusters of all ages by selecting extended sources (from multiple concentration index measurements) followed by morphological classification (centrally concentrated and symmetric or asymmetric, multiple peaks, contaminant) by visually examining the V-band image and separately by a machine-learning algorithm which classified larger samples to reach fainter limits.","From both cluster catalogs, we select an initial list of candidate GCs to have $B-V \\geq 0.5$ and $V-I \\geq 0.73$~mag, then remove likely contaminants (including reddened young clusters, background galaxies misclassified by the neural network, and chance superpositions/blends of stars) after a careful visual inspection.","We find that $\\approx86$ % of the color-selected candidates classified as spherically symmetric, and $\\approx68$ of those classified as centrally concentrated but asymmetric are likely to be GCs.","The luminosity functions of the GC candidates in 2 of our 17 galaxies, NGC 628 and NGC 3627, are atypical, and continue to rise at least 1~mag fainter than the expected turnover near $M_V \\sim -7.4$.","These faint candidate GCs have more extended spatial distributions than their bright counterparts, and may reside in the disk rather than the bulge/halo, similar to faint GCs previously discovered in M101.","These faint clusters may be somewhat younger since the age-metallicity degeneracy makes it difficult to determine precise cluster ages from integrated colors once they reach $\\approx1$~Gyr."],"url":"http://arxiv.org/abs/2403.13908v1","category":"astro-ph.GA"}
{"created":"2024-03-20 18:10:19","title":"PINNferring the Hubble Function with Uncertainties","abstract":"The Hubble function characterizes a given Friedmann-Robertson-Walker spacetime and can be related to the densities of the cosmological fluids and their equations of state. We show how physics-informed neural networks (PINNs) emulate this dynamical system and provide fast predictions of the luminosity distance for a given choice of densities and equations of state, as needed for the analysis of supernova data. We use this emulator to perform a model-independent and parameter-free reconstruction of the Hubble function on the basis of supernova data. As part of this study, we develop and validate an uncertainty treatment for PINNs using a heteroscedastic loss and repulsive ensembles.","sentences":["The Hubble function characterizes a given Friedmann-Robertson-Walker spacetime and can be related to the densities of the cosmological fluids and their equations of state.","We show how physics-informed neural networks (PINNs) emulate this dynamical system and provide fast predictions of the luminosity distance for a given choice of densities and equations of state, as needed for the analysis of supernova data.","We use this emulator to perform a model-independent and parameter-free reconstruction of the Hubble function on the basis of supernova data.","As part of this study, we develop and validate an uncertainty treatment for PINNs using a heteroscedastic loss and repulsive ensembles."],"url":"http://arxiv.org/abs/2403.13899v1","category":"astro-ph.CO"}
{"created":"2024-03-20 18:09:57","title":"Optimal Risk-Sensitive Scheduling Policies for Remote Estimation of Autoregressive Markov Processes","abstract":"We design scheduling policies that minimize a risk-sensitive cost criterion for a remote estimation setup. Since risk-sensitive cost objective takes into account not just the mean value of the cost, but also higher order moments of its probability distribution, the resulting policy is robust to changes in the underlying system's parameters. The setup consists of a sensor that observes a discrete-time autoregressive Markov process, and at each time $t$ decides whether or not to transmit its observations to a remote estimator using an unreliable wireless communication channel after encoding these observations into data packets. We model the communication channel as a Gilbert-Elliott channel \\cite{10384144}. Sensor probes the channel \\cite{laourine2010betting} and hence knows the channel state at each time $t$ before making scheduling decision. The scheduler has to minimize the expected value of the exponential of the finite horizon cumulative cost that is sum of the following two quantities (i) the cumulative transmission power consumed, (ii) the cumulative squared estimator error. We pose this dynamic optimization problem as a Markov decision process (MDP), in which the system state at time $t$ is composed of (i) the instantaneous error $\\Delta(t):= x(t)-a\\hat{x}(t-1)$, where $x(t),\\hat{x}(t-1)$ are the system state and the estimate at time $t,t-1$ respectively, and (ii) the channel state $c(t)$. We show that there exists an optimal policy that has a threshold structure, i.e., at each time $t$, for each possible channel state $c$, there is a threshold $\\D\\ust(c)$ such that if the current channel state is $c$, then it transmits only when the error $\\D(t)$ exceeds $\\D\\ust(c)$.","sentences":["We design scheduling policies that minimize a risk-sensitive cost criterion for a remote estimation setup.","Since risk-sensitive cost objective takes into account not just the mean value of the cost, but also higher order moments of its probability distribution, the resulting policy is robust to changes in the underlying system's parameters.","The setup consists of a sensor that observes a discrete-time autoregressive Markov process, and at each time $t$ decides whether or not to transmit its observations to a remote estimator using an unreliable wireless communication channel after encoding these observations into data packets.","We model the communication channel as a Gilbert-Elliott channel \\cite{10384144}.","Sensor probes the channel \\cite{laourine2010betting} and hence knows the channel state at each time $t$ before making scheduling decision.","The scheduler has to minimize the expected value of the exponential of the finite horizon cumulative cost that is sum of the following two quantities (i) the cumulative transmission power consumed, (ii) the cumulative squared estimator error.","We pose this dynamic optimization problem as a Markov decision process (MDP), in which the system state at time $t$ is composed of (i) the instantaneous error $\\Delta(t):= x(t)-a\\hat{x}(t-1)$, where $x(t),\\hat{x}(t-1)$ are the system state and the estimate at time $t,t-1$ respectively, and (ii) the channel state $c(t)$. We show that there exists an optimal policy that has a threshold structure, i.e., at each time $t$, for each possible channel state $c$, there is a threshold $\\D\\ust(c)$ such that if the current channel state is $c$, then it transmits only when the error $\\D(t)$ exceeds $\\D\\ust(c)$."],"url":"http://arxiv.org/abs/2403.13898v1","category":"math.OC"}
{"created":"2024-03-20 18:04:29","title":"The Distance to the S147 Supernova Remnant","abstract":"In the absence of a parallax distance to a pulsar or a surviving binary in a supernova remnant (SNR), distances to Galactic SNRs are generally very uncertain. However, by combining Gaia data with wide field, multi-fiber echelle spectroscopy, it is now possible to obtain accurate distances to many SNRs with limited extinction by searching for the appearance of high velocity CaII or NaI absorption lines in hot stars as a function of distance. We demonstrate this for the SNR S147 using the spectra of 259 luminous, blue stars. We obtain a median distance of 1.37 kpc (1.30 to 1.47 kpc at 90% confidence) that is consistent with the median parallax distance to the pulsar of 1.46 kpc (1.12 to 2.10 kpc at 90% confidence), but with significantly smaller uncertainties. Our distance is also consistent with the distance to the candidate unbound binary companion in this SNR, HD37424. The presence of high velocity absorption lines is correlated with the emission line flux of the SNR but not with the radio flux.","sentences":["In the absence of a parallax distance to a pulsar or a surviving binary in a supernova remnant (SNR), distances to Galactic SNRs are generally very uncertain.","However, by combining Gaia data with wide field, multi-fiber echelle spectroscopy, it is now possible to obtain accurate distances to many SNRs with limited extinction by searching for the appearance of high velocity CaII or NaI absorption lines in hot stars as a function of distance.","We demonstrate this for the SNR S147 using the spectra of 259 luminous, blue stars.","We obtain a median distance of 1.37 kpc (1.30 to 1.47 kpc at 90% confidence) that is consistent with the median parallax distance to the pulsar of 1.46 kpc (1.12 to 2.10 kpc at 90% confidence), but with significantly smaller uncertainties.","Our distance is also consistent with the distance to the candidate unbound binary companion in this SNR, HD37424.","The presence of high velocity absorption lines is correlated with the emission line flux of the SNR but not with the radio flux."],"url":"http://arxiv.org/abs/2403.13892v1","category":"astro-ph.HE"}
{"created":"2024-03-20 18:00:03","title":"Continuous Gravitational Waves: A New Window to Look for Heavy Non-annihilating Dark Matter","abstract":"Sun-like stars can transmute into comparable mass black holes by steadily accumulating heavy non-annihilating dark matter particles over the course of their lives. If such stars form in binary systems, they could give rise to quasi-monochromatic, persistent gravitational waves, commonly known as continuous gravitational waves, as they inspiral towards one another. We demonstrate that next-generation space-based detectors, e.g., Laser Interferometer Space Antenna (LISA) and Big Bang Observer (BBO), can provide novel constraints on dark matter parameters (dark matter mass and its interaction cross-section with the nucleons) by probing gravitational waves from transmuted Sun-like stars that are in close binaries. Our projected constraints depend on several astrophysical uncertainties, nevertheless, are competitive with the existing constraints obtained from cosmological measurements as well as terrestrial direct searches, demonstrating a notable science-case for these space-based gravitational wave detectors as probes of particle dark matter.","sentences":["Sun-like stars can transmute into comparable mass black holes by steadily accumulating heavy non-annihilating dark matter particles over the course of their lives.","If such stars form in binary systems, they could give rise to quasi-monochromatic, persistent gravitational waves, commonly known as continuous gravitational waves, as they inspiral towards one another.","We demonstrate that next-generation space-based detectors, e.g., Laser Interferometer Space Antenna (LISA) and Big Bang Observer (BBO), can provide novel constraints on dark matter parameters (dark matter mass and its interaction cross-section with the nucleons) by probing gravitational waves from transmuted Sun-like stars that are in close binaries.","Our projected constraints depend on several astrophysical uncertainties, nevertheless, are competitive with the existing constraints obtained from cosmological measurements as well as terrestrial direct searches, demonstrating a notable science-case for these space-based gravitational wave detectors as probes of particle dark matter."],"url":"http://arxiv.org/abs/2403.13886v1","category":"hep-ph"}
{"created":"2024-03-20 18:00:02","title":"Topological Dipole Insulator","abstract":"We expand the concept of two-dimensional topological insulators to encompass a novel category known as topological dipole insulators (TDIs), characterized by conserved dipole moments along the $x$-direction in addition to charge conservation. By generalizing Laughlin's flux insertion argument, we prove a no-go theorem and predict possible edge patterns and anomalies in a TDI with both charge $U^e(1)$ and dipole $U^d(1)$ symmetries. The edge of a TDI is characterized as a quadrupolar channel that displays a dipole $U^d(1)$ anomaly. A quantized amount of dipole gets transferred between the edges under the dipolar flux insertion, manifesting as `quantized quadrupolar Hall effect' in TDIs. A microscopic coupled-wire Hamiltonian realizing the TDI is constructed by introducing a mutually commuting pair-hopping terms between wires to gap out all the bulk modes while preserving the dipole moment. The effective action at the quadrupolar edge can be derived from the wire model, with the corresponding bulk dipolar Chern-Simons response theory delineating the topological electromagnetic response in TDIs. Finally, we enrich our exploration of topological dipole insulators to the spinful case and construct a dipolar version of the quantum spin Hall effect, whose boundary evidences a mixed anomaly between spin and dipole symmetry. Effective bulk and the edge action for the dipolar quantum spin Hall insulator are constructed as well.","sentences":["We expand the concept of two-dimensional topological insulators to encompass a novel category known as topological dipole insulators (TDIs), characterized by conserved dipole moments along the $x$-direction in addition to charge conservation.","By generalizing Laughlin's flux insertion argument, we prove a no-go theorem and predict possible edge patterns and anomalies in a TDI with both charge $U^e(1)$ and dipole $U^d(1)$ symmetries.","The edge of a TDI is characterized as a quadrupolar channel that displays a dipole $U^d(1)$ anomaly.","A quantized amount of dipole gets transferred between the edges under the dipolar flux insertion, manifesting as `quantized quadrupolar Hall effect' in TDIs.","A microscopic coupled-wire Hamiltonian realizing the TDI is constructed by introducing a mutually commuting pair-hopping terms between wires to gap out all the bulk modes while preserving the dipole moment.","The effective action at the quadrupolar edge can be derived from the wire model, with the corresponding bulk dipolar Chern-Simons response theory delineating the topological electromagnetic response in TDIs.","Finally, we enrich our exploration of topological dipole insulators to the spinful case and construct a dipolar version of the quantum spin Hall effect, whose boundary evidences a mixed anomaly between spin and dipole symmetry.","Effective bulk and the edge action for the dipolar quantum spin Hall insulator are constructed as well."],"url":"http://arxiv.org/abs/2403.13880v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-20 18:00:02","title":"Measuring the Substructure Mass Power Spectrum of 23 SLACS Strong Galaxy-Galaxy Lenses with Convolutional Neural Networks","abstract":"Strong gravitational lensing can be used as a tool for constraining the substructure in the mass distribution of galaxies. In this study we investigate the power spectrum of dark matter perturbations in a population of 23 Hubble Space Telescope images of strong galaxy-galaxy lenses selected from The Sloan Lens ACS (SLACS) survey. We model the dark matter substructure as a Gaussian Random Field perturbation on a smooth lens mass potential, characterized by power-law statistics. We expand upon the previously developed machine learning framework to predict the power-law statistics by using a convolutional neural network (CNN) that accounts for both epistemic and aleatoric uncertainties. For the training sets, we use the smooth lens mass potentials and reconstructed source galaxies that have been previously modelled through traditional fits of analytical and shapelet profiles as a starting point. We train three CNNs with different training set: the first using standard data augmentation on the best-fitting reconstructed sources, the second using different reconstructed sources spaced throughout the posterior distribution, and the third using a combination of the two data sets. We apply the trained CNNs to the SLACS data and find agreement in their predictions. Our results suggest a significant substructure perturbation favoring a high frequency power spectrum across our lens population.","sentences":["Strong gravitational lensing can be used as a tool for constraining the substructure in the mass distribution of galaxies.","In this study we investigate the power spectrum of dark matter perturbations in a population of 23 Hubble Space Telescope images of strong galaxy-galaxy lenses selected from The Sloan Lens ACS (SLACS) survey.","We model the dark matter substructure as a Gaussian Random Field perturbation on a smooth lens mass potential, characterized by power-law statistics.","We expand upon the previously developed machine learning framework to predict the power-law statistics by using a convolutional neural network (CNN) that accounts for both epistemic and aleatoric uncertainties.","For the training sets, we use the smooth lens mass potentials and reconstructed source galaxies that have been previously modelled through traditional fits of analytical and shapelet profiles as a starting point.","We train three CNNs with different training set: the first using standard data augmentation on the best-fitting reconstructed sources, the second using different reconstructed sources spaced throughout the posterior distribution, and the third using a combination of the two data sets.","We apply the trained CNNs to the SLACS data and find agreement in their predictions.","Our results suggest a significant substructure perturbation favoring a high frequency power spectrum across our lens population."],"url":"http://arxiv.org/abs/2403.13881v1","category":"astro-ph.CO"}
{"created":"2024-03-20 18:00:01","title":"A Noisy Approach to Intrinsically Mixed-State Topological Order","abstract":"We propose a general framework for studying two-dimensional (2D) topologically ordered states subject to local correlated errors and show that the resulting mixed-state can display intrinsically mixed-state topological order (imTO) -- topological order which is not expected to occur in the ground state of 2D local gapped Hamiltonians. Specifically, we show that decoherence, previously interpreted as anyon condensation in a doubled Hilbert space, is more naturally phrased as, and provides a physical mechanism for, \"gauging out\" anyons in the original Hilbert space. We find that gauging out anyons generically results in imTO, with the decohered mixed-state strongly symmetric under certain anomalous 1-form symmetries. This framework lays bare a striking connection between the decohered density matrix and topological subsystem codes, which can appear as anomalous surface states of 3D topological orders. Through a series of examples, we show that the decohered state can display a classical memory, encode logical qubits (i.e., exhibit a quantum memory), and even host chiral or non-modular topological order. We argue that the decohered states represent genuine mixed-state quantum phases of matter and that a partial classification of imTO is given in terms of braided fusion categories.","sentences":["We propose a general framework for studying two-dimensional (2D) topologically ordered states subject to local correlated errors and show that the resulting mixed-state can display intrinsically mixed-state topological order (imTO) -- topological order which is not expected to occur in the ground state of 2D local gapped Hamiltonians.","Specifically, we show that decoherence, previously interpreted as anyon condensation in a doubled Hilbert space, is more naturally phrased as, and provides a physical mechanism for, \"gauging out\" anyons in the original Hilbert space.","We find that gauging out anyons generically results in imTO, with the decohered mixed-state strongly symmetric under certain anomalous 1-form symmetries.","This framework lays bare a striking connection between the decohered density matrix and topological subsystem codes, which can appear as anomalous surface states of 3D topological orders.","Through a series of examples, we show that the decohered state can display a classical memory, encode logical qubits (i.e., exhibit a quantum memory), and even host chiral or non-modular topological order.","We argue that the decohered states represent genuine mixed-state quantum phases of matter and that a partial classification of imTO is given in terms of braided fusion categories."],"url":"http://arxiv.org/abs/2403.13879v1","category":"cond-mat.str-el"}
{"created":"2024-03-21 17:58:14","title":"AdaIR: Adaptive All-in-One Image Restoration via Frequency Mining and Modulation","abstract":"In the image acquisition process, various forms of degradation, including noise, haze, and rain, are frequently introduced. These degradations typically arise from the inherent limitations of cameras or unfavorable ambient conditions. To recover clean images from degraded versions, numerous specialized restoration methods have been developed, each targeting a specific type of degradation. Recently, all-in-one algorithms have garnered significant attention by addressing different types of degradations within a single model without requiring prior information of the input degradation type. However, these methods purely operate in the spatial domain and do not delve into the distinct frequency variations inherent to different degradation types. To address this gap, we propose an adaptive all-in-one image restoration network based on frequency mining and modulation. Our approach is motivated by the observation that different degradation types impact the image content on different frequency subbands, thereby requiring different treatments for each restoration task. Specifically, we first mine low- and high-frequency information from the input features, guided by the adaptively decoupled spectra of the degraded image. The extracted features are then modulated by a bidirectional operator to facilitate interactions between different frequency components. Finally, the modulated features are merged into the original input for a progressively guided restoration. With this approach, the model achieves adaptive reconstruction by accentuating the informative frequency subbands according to different input degradations. Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance on different image restoration tasks, including denoising, dehazing, deraining, motion deblurring, and low-light image enhancement. Our code is available at https://github.com/c-yn/AdaIR.","sentences":["In the image acquisition process, various forms of degradation, including noise, haze, and rain, are frequently introduced.","These degradations typically arise from the inherent limitations of cameras or unfavorable ambient conditions.","To recover clean images from degraded versions, numerous specialized restoration methods have been developed, each targeting a specific type of degradation.","Recently, all-in-one algorithms have garnered significant attention by addressing different types of degradations within a single model without requiring prior information of the input degradation type.","However, these methods purely operate in the spatial domain and do not delve into the distinct frequency variations inherent to different degradation types.","To address this gap, we propose an adaptive all-in-one image restoration network based on frequency mining and modulation.","Our approach is motivated by the observation that different degradation types impact the image content on different frequency subbands, thereby requiring different treatments for each restoration task.","Specifically, we first mine low-","and high-frequency information from the input features, guided by the adaptively decoupled spectra of the degraded image.","The extracted features are then modulated by a bidirectional operator to facilitate interactions between different frequency components.","Finally, the modulated features are merged into the original input for a progressively guided restoration.","With this approach, the model achieves adaptive reconstruction by accentuating the informative frequency subbands according to different input degradations.","Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance on different image restoration tasks, including denoising, dehazing, deraining, motion deblurring, and low-light image enhancement.","Our code is available at https://github.com/c-yn/AdaIR."],"url":"http://arxiv.org/abs/2403.14614v1","category":"cs.CV"}
{"created":"2024-03-21 17:56:46","title":"Fast and accurate log-determinant approximations","abstract":"We consider the problem of estimating log-determinants of large, sparse, positive definite matrices. A key focus of our algorithm is to reduce computational cost, and it is based on sparse approximate inverses. The algorithm can be implemented to be adaptive, and it uses graph spline approximation to improve accuracy. We illustrate our approach on classes of large sparse matrices.","sentences":["We consider the problem of estimating log-determinants of large, sparse, positive definite matrices.","A key focus of our algorithm is to reduce computational cost, and it is based on sparse approximate inverses.","The algorithm can be implemented to be adaptive, and it uses graph spline approximation to improve accuracy.","We illustrate our approach on classes of large sparse matrices."],"url":"http://arxiv.org/abs/2403.14609v1","category":"math.NA"}
{"created":"2024-03-21 17:47:22","title":"New variants of arithmetic quantum ergodicity","abstract":"We establish two new variants of arithmetic quantum ergodicity. The first is for self-dual $\\mathrm{GL}_2$ Hecke-Maass newforms over $\\mathbb{Q}$ as the level and Laplace eigenvalue vary jointly. The second is a nonsplit analogue wherein almost all restrictions of Hilbert (respectively Bianchi) Hecke-Maass cusp forms to the modular surface dissipate as their Laplace eigenvalues grow.","sentences":["We establish two new variants of arithmetic quantum ergodicity.","The first is for self-dual $\\mathrm{GL}_2$ Hecke-Maass newforms over $\\mathbb{Q}$ as the level and Laplace eigenvalue vary jointly.","The second is a nonsplit analogue wherein almost all restrictions of Hilbert (respectively Bianchi) Hecke-Maass cusp forms to the modular surface dissipate as their Laplace eigenvalues grow."],"url":"http://arxiv.org/abs/2403.14591v1","category":"math.NT"}
{"created":"2024-03-21 15:18:21","title":"S2LIC: Learned Image Compression with the SwinV2 Block, Adaptive Channel-wise and Global-inter Attention Context","abstract":"Recently, deep learning technology has been successfully applied in the field of image compression, leading to superior rate-distortion performance. It is crucial to design an effective and efficient entropy model to estimate the probability distribution of the latent representation. However, the majority of entropy models primarily focus on one-dimensional correlation processing between channel and spatial information. In this paper, we propose an Adaptive Channel-wise and Global-inter attention Context (ACGC) entropy model, which can efficiently achieve dual feature aggregation in both inter-slice and intraslice contexts. Specifically, we divide the latent representation into different slices and then apply the ACGC model in a parallel checkerboard context to achieve faster decoding speed and higher rate-distortion performance. In order to capture redundant global features across different slices, we utilize deformable attention in adaptive global-inter attention to dynamically refine the attention weights based on the actual spatial relationships and context. Furthermore, in the main transformation structure, we propose a high-performance S2LIC model. We introduce the residual SwinV2 Transformer model to capture global feature information and utilize a dense block network as the feature enhancement module to improve the nonlinear representation of the image within the transformation structure. Experimental results demonstrate that our method achieves faster encoding and decoding speeds and outperforms VTM-17.1 and some recent learned image compression methods in both PSNR and MS-SSIM metrics.","sentences":["Recently, deep learning technology has been successfully applied in the field of image compression, leading to superior rate-distortion performance.","It is crucial to design an effective and efficient entropy model to estimate the probability distribution of the latent representation.","However, the majority of entropy models primarily focus on one-dimensional correlation processing between channel and spatial information.","In this paper, we propose an Adaptive Channel-wise and Global-inter attention Context (ACGC) entropy model, which can efficiently achieve dual feature aggregation in both inter-slice and intraslice contexts.","Specifically, we divide the latent representation into different slices and then apply the ACGC model in a parallel checkerboard context to achieve faster decoding speed and higher rate-distortion performance.","In order to capture redundant global features across different slices, we utilize deformable attention in adaptive global-inter attention to dynamically refine the attention weights based on the actual spatial relationships and context.","Furthermore, in the main transformation structure, we propose a high-performance S2LIC model.","We introduce the residual SwinV2 Transformer model to capture global feature information and utilize a dense block network as the feature enhancement module to improve the nonlinear representation of the image within the transformation structure.","Experimental results demonstrate that our method achieves faster encoding and decoding speeds and outperforms VTM-17.1 and some recent learned image compression methods in both PSNR and MS-SSIM metrics."],"url":"http://arxiv.org/abs/2403.14471v1","category":"eess.IV"}
{"created":"2024-03-21 14:59:48","title":"Phenology curve estimation via a mixed model representation of functional principal components: Characterizing time series of satellite-derived vegetation indices","abstract":"Vegetation phenology consists of studying synchronous stationary events, such as the vegetation green up and leaves senescence, that can be construed as adaptive responses to climatic constraints. In this paper, we propose a method to estimate the annual phenology curve from multi-annual observations of time series of vegetation indices derived from satellite images. We fitted the classical harmonic regression model to annual-based time series in order to construe the original data set as realizations of a functional process. Hierarchical clustering was applied to define a nearly homogeneous group of annual (smoothed) time series from which a representative and idealized phenology curve was estimated at the pixel level. This curve resulted from fitting a mixed model, based on functional principal components, to the homogeneous group of time series. Leveraging the idealized phenology curve, we employed standard calculus criteria to estimate the following phenological parameters (stationary events): green up, start of season, maturity, senescence, end of season and dormancy. By applying the proposed methodology to four different data cubes (time series from 2000 to 2023 of a popular satellite-derived vegetation index) recorded across grasslands, forests, and annual rainfed agricultural zones of a Flora and Fauna Protected Area in northern Mexico, we verified that our approach characterizes properly the phenological cycle in vegetation with nearly periodic dynamics, such as grasslands and agricultural areas. The R package sephora was used for all computations in this paper.","sentences":["Vegetation phenology consists of studying synchronous stationary events, such as the vegetation green up and leaves senescence, that can be construed as adaptive responses to climatic constraints.","In this paper, we propose a method to estimate the annual phenology curve from multi-annual observations of time series of vegetation indices derived from satellite images.","We fitted the classical harmonic regression model to annual-based time series in order to construe the original data set as realizations of a functional process.","Hierarchical clustering was applied to define a nearly homogeneous group of annual (smoothed) time series from which a representative and idealized phenology curve was estimated at the pixel level.","This curve resulted from fitting a mixed model, based on functional principal components, to the homogeneous group of time series.","Leveraging the idealized phenology curve, we employed standard calculus criteria to estimate the following phenological parameters (stationary events): green up, start of season, maturity, senescence, end of season and dormancy.","By applying the proposed methodology to four different data cubes (time series from 2000 to 2023 of a popular satellite-derived vegetation index) recorded across grasslands, forests, and annual rainfed agricultural zones of a Flora and Fauna Protected Area in northern Mexico, we verified that our approach characterizes properly the phenological cycle in vegetation with nearly periodic dynamics, such as grasslands and agricultural areas.","The R package sephora was used for all computations in this paper."],"url":"http://arxiv.org/abs/2403.14451v1","category":"stat.ME"}
{"created":"2024-03-21 14:06:38","title":"OA-CNNs: Omni-Adaptive Sparse CNNs for 3D Semantic Segmentation","abstract":"The booming of 3D recognition in the 2020s began with the introduction of point cloud transformers. They quickly overwhelmed sparse CNNs and became state-of-the-art models, especially in 3D semantic segmentation. However, sparse CNNs are still valuable networks, due to their efficiency treasure, and ease of application. In this work, we reexamine the design distinctions and test the limits of what a sparse CNN can achieve. We discover that the key credit to the performance difference is adaptivity. Specifically, we propose two key components, i.e., adaptive receptive fields (spatially) and adaptive relation, to bridge the gap. This exploration led to the creation of Omni-Adaptive 3D CNNs (OA-CNNs), a family of networks that integrates a lightweight module to greatly enhance the adaptivity of sparse CNNs at minimal computational cost. Without any self-attention modules, OA-CNNs favorably surpass point transformers in terms of accuracy in both indoor and outdoor scenes, with much less latency and memory cost. Notably, it achieves 76.1%, 78.9%, and 70.6% mIoU on ScanNet v2, nuScenes, and SemanticKITTI validation benchmarks respectively, while maintaining at most 5x better speed than transformer counterparts. This revelation highlights the potential of pure sparse CNNs to outperform transformer-related networks.","sentences":["The booming of 3D recognition in the 2020s began with the introduction of point cloud transformers.","They quickly overwhelmed sparse CNNs and became state-of-the-art models, especially in 3D semantic segmentation.","However, sparse CNNs are still valuable networks, due to their efficiency treasure, and ease of application.","In this work, we reexamine the design distinctions and test the limits of what a sparse CNN can achieve.","We discover that the key credit to the performance difference is adaptivity.","Specifically, we propose two key components, i.e., adaptive receptive fields (spatially) and adaptive relation, to bridge the gap.","This exploration led to the creation of Omni-Adaptive 3D CNNs (OA-CNNs), a family of networks that integrates a lightweight module to greatly enhance the adaptivity of sparse CNNs at minimal computational cost.","Without any self-attention modules, OA-CNNs favorably surpass point transformers in terms of accuracy in both indoor and outdoor scenes, with much less latency and memory cost.","Notably, it achieves 76.1%, 78.9%, and 70.6% mIoU on ScanNet v2, nuScenes, and SemanticKITTI validation benchmarks respectively, while maintaining at most 5x better speed than transformer counterparts.","This revelation highlights the potential of pure sparse CNNs to outperform transformer-related networks."],"url":"http://arxiv.org/abs/2403.14418v1","category":"cs.CV"}
{"created":"2024-03-21 13:28:27","title":"Quarklet Characterizations for bivariate Bessel-Potential Spaces on the Unit Square via Tensor Products","abstract":"In this paper we deduce new characterizations for bivariate Bessel-Potential spaces defined on the unit square via B-spline quarklets. For that purpose in a first step we use univariate boundary adapted quarklets to describe univariate Bessel-Potential spaces on intervals. To obtain the bivariate characterizations a recent result of Hansen and Sickel is applied. It yields that each bivariate Bessel-Potential space on a square can be written as an intersection of function spaces which have a tensor product structure. Hence our main result is a characterization of bivariate Bessel-Potential spaces on squares in terms of quarklets that are tensor products of univariate quarklets on intervals.","sentences":["In this paper we deduce new characterizations for bivariate Bessel-Potential spaces defined on the unit square via B-spline quarklets.","For that purpose in a first step we use univariate boundary adapted quarklets to describe univariate Bessel-Potential spaces on intervals.","To obtain the bivariate characterizations a recent result of Hansen and Sickel is applied.","It yields that each bivariate Bessel-Potential space on a square can be written as an intersection of function spaces which have a tensor product structure.","Hence our main result is a characterization of bivariate Bessel-Potential spaces on squares in terms of quarklets that are tensor products of univariate quarklets on intervals."],"url":"http://arxiv.org/abs/2403.14388v1","category":"math.FA"}
{"created":"2024-03-21 12:24:48","title":"Statistical modeling to adjust for time trends in adaptive platform trials utilizing non-concurrent controls","abstract":"Utilizing non-concurrent controls in the analysis of late-entering experimental arms in platform trials has recently received considerable attention, both on academic and regulatory levels. While incorporating this data can lead to increased power and lower required sample sizes, it might also introduce bias to the effect estimators if temporal drifts are present in the trial. Aiming to mitigate the potential calendar time bias, we propose various frequentist model-based approaches that leverage the non-concurrent control data, while adjusting for time trends. One of the currently available frequentist models incorporates time as a categorical fixed effect, separating the duration of the trial into periods, defined as time intervals bounded by any treatment arm entering or leaving the platform. In this work, we propose two extensions of this model. First, we consider an alternative definition of the time covariate by dividing the trial into fixed-length calendar time intervals. Second, we propose alternative methods to adjust for time trends. In particular, we investigate adjusting for autocorrelated random effects to account for dependency between closer time intervals and employing spline regression to model time with a smooth polynomial function. We evaluate the performance of the proposed approaches in a simulation study and illustrate their use by means of a case study.","sentences":["Utilizing non-concurrent controls in the analysis of late-entering experimental arms in platform trials has recently received considerable attention, both on academic and regulatory levels.","While incorporating this data can lead to increased power and lower required sample sizes, it might also introduce bias to the effect estimators if temporal drifts are present in the trial.","Aiming to mitigate the potential calendar time bias, we propose various frequentist model-based approaches that leverage the non-concurrent control data, while adjusting for time trends.","One of the currently available frequentist models incorporates time as a categorical fixed effect, separating the duration of the trial into periods, defined as time intervals bounded by any treatment arm entering or leaving the platform.","In this work, we propose two extensions of this model.","First, we consider an alternative definition of the time covariate by dividing the trial into fixed-length calendar time intervals.","Second, we propose alternative methods to adjust for time trends.","In particular, we investigate adjusting for autocorrelated random effects to account for dependency between closer time intervals and employing spline regression to model time with a smooth polynomial function.","We evaluate the performance of the proposed approaches in a simulation study and illustrate their use by means of a case study."],"url":"http://arxiv.org/abs/2403.14348v1","category":"stat.ME"}
{"created":"2024-03-21 09:50:39","title":"Diffusion Models with Ensembled Structure-Based Anomaly Scoring for Unsupervised Anomaly Detection","abstract":"Supervised deep learning techniques show promise in medical image analysis. However, they require comprehensive annotated data sets, which poses challenges, particularly for rare diseases. Consequently, unsupervised anomaly detection (UAD) emerges as a viable alternative for pathology segmentation, as only healthy data is required for training. However, recent UAD anomaly scoring functions often focus on intensity only and neglect structural differences, which impedes the segmentation performance. This work investigates the potential of Structural Similarity (SSIM) to bridge this gap. SSIM captures both intensity and structural disparities and can be advantageous over the classical $l1$ error. However, we show that there is more than one optimal kernel size for the SSIM calculation for different pathologies. Therefore, we investigate an adaptive ensembling strategy for various kernel sizes to offer a more pathology-agnostic scoring mechanism. We demonstrate that this ensembling strategy can enhance the performance of DMs and mitigate the sensitivity to different kernel sizes across varying pathologies, highlighting its promise for brain MRI anomaly detection.","sentences":["Supervised deep learning techniques show promise in medical image analysis.","However, they require comprehensive annotated data sets, which poses challenges, particularly for rare diseases.","Consequently, unsupervised anomaly detection (UAD) emerges as a viable alternative for pathology segmentation, as only healthy data is required for training.","However, recent UAD anomaly scoring functions often focus on intensity only and neglect structural differences, which impedes the segmentation performance.","This work investigates the potential of Structural Similarity (SSIM) to bridge this gap.","SSIM captures both intensity and structural disparities and can be advantageous over the classical $l1$ error.","However, we show that there is more than one optimal kernel size for the SSIM calculation for different pathologies.","Therefore, we investigate an adaptive ensembling strategy for various kernel sizes to offer a more pathology-agnostic scoring mechanism.","We demonstrate that this ensembling strategy can enhance the performance of DMs and mitigate the sensitivity to different kernel sizes across varying pathologies, highlighting its promise for brain MRI anomaly detection."],"url":"http://arxiv.org/abs/2403.14262v1","category":"eess.IV"}
{"created":"2024-03-21 07:06:30","title":"AdaProj: Adaptively Scaled Angular Margin Subspace Projections for Anomalous Sound Detection with Auxiliary Classification Tasks","abstract":"The state-of-the-art approach for semi-supervised anomalous sound detection is to first learn an embedding space by using auxiliary classification tasks based on meta information or self-supervised learning and then estimate the distribution of normal data. In this work, AdaProj a novel loss function is presented. In contrast to commonly used angular margin losses, which project data of each class as close as possible to their corresponding class centers, AdaProj learns to project data onto class-specific subspaces. By doing so, the resulting distributions of embeddings belonging to normal data are not required to be as restrictive as other loss functions allowing a more detailed view on the data. In experiments conducted on the DCASE2022 and DCASE2023 datasets, it is shown that using AdaProj to learn an embedding space significantly outperforms other commonly used loss functions and results in a state-of-the-art performance on the DCASE2023 dataset.","sentences":["The state-of-the-art approach for semi-supervised anomalous sound detection is to first learn an embedding space by using auxiliary classification tasks based on meta information or self-supervised learning and then estimate the distribution of normal data.","In this work, AdaProj a novel loss function is presented.","In contrast to commonly used angular margin losses, which project data of each class as close as possible to their corresponding class centers, AdaProj learns to project data onto class-specific subspaces.","By doing so, the resulting distributions of embeddings belonging to normal data are not required to be as restrictive as other loss functions allowing a more detailed view on the data.","In experiments conducted on the DCASE2022 and DCASE2023 datasets, it is shown that using AdaProj to learn an embedding space significantly outperforms other commonly used loss functions and results in a state-of-the-art performance on the DCASE2023 dataset."],"url":"http://arxiv.org/abs/2403.14179v1","category":"eess.AS"}
{"created":"2024-03-21 03:28:24","title":"MaskSAM: Towards Auto-prompt SAM with Mask Classification for Medical Image Segmentation","abstract":"Segment Anything Model~(SAM), a prompt-driven foundation model for natural image segmentation, has demonstrated impressive zero-shot performance. However, SAM does not work when directly applied to medical image segmentation tasks, since SAM lacks the functionality to predict semantic labels for predicted masks and needs to provide extra prompts, such as points or boxes, to segment target regions. Meanwhile, there is a huge gap between 2D natural images and 3D medical images, so the performance of SAM is imperfect for medical image segmentation tasks. Following the above issues, we propose MaskSAM, a novel mask classification prompt-free SAM adaptation framework for medical image segmentation. We design a prompt generator combined with the image encoder in SAM to generate a set of auxiliary classifier tokens, auxiliary binary masks, and auxiliary bounding boxes. Each pair of auxiliary mask and box prompts, which can solve the requirements of extra prompts, is associated with class label predictions by the sum of the auxiliary classifier token and the learnable global classifier tokens in the mask decoder of SAM to solve the predictions of semantic labels. Meanwhile, we design a 3D depth-convolution adapter for image embeddings and a 3D depth-MLP adapter for prompt embeddings. We inject one of them into each transformer block in the image encoder and mask decoder to enable pre-trained 2D SAM models to extract 3D information and adapt to 3D medical images. Our method achieves state-of-the-art performance on AMOS2022, 90.52% Dice, which improved by 2.7% compared to nnUNet. Our method surpasses nnUNet by 1.7% on ACDC and 1.0% on Synapse datasets.","sentences":["Segment Anything Model~(SAM), a prompt-driven foundation model for natural image segmentation, has demonstrated impressive zero-shot performance.","However, SAM does not work when directly applied to medical image segmentation tasks, since SAM lacks the functionality to predict semantic labels for predicted masks and needs to provide extra prompts, such as points or boxes, to segment target regions.","Meanwhile, there is a huge gap between 2D natural images and 3D medical images, so the performance of SAM is imperfect for medical image segmentation tasks.","Following the above issues, we propose MaskSAM, a novel mask classification prompt-free SAM adaptation framework for medical image segmentation.","We design a prompt generator combined with the image encoder in SAM to generate a set of auxiliary classifier tokens, auxiliary binary masks, and auxiliary bounding boxes.","Each pair of auxiliary mask and box prompts, which can solve the requirements of extra prompts, is associated with class label predictions by the sum of the auxiliary classifier token and the learnable global classifier tokens in the mask decoder of SAM to solve the predictions of semantic labels.","Meanwhile, we design a 3D depth-convolution adapter for image embeddings and a 3D depth-MLP adapter for prompt embeddings.","We inject one of them into each transformer block in the image encoder and mask decoder to enable pre-trained 2D SAM models to extract 3D information and adapt to 3D medical images.","Our method achieves state-of-the-art performance on AMOS2022, 90.52% Dice, which improved by 2.7% compared to nnUNet.","Our method surpasses nnUNet by 1.7% on ACDC and 1.0% on Synapse datasets."],"url":"http://arxiv.org/abs/2403.14103v1","category":"cs.CV"}
{"created":"2024-03-21 01:54:00","title":"Improving $\u039b$ Signal Extraction with Domain Adaptation via Normalizing Flows","abstract":"The present study presents a novel application for normalizing flows for domain adaptation. The study investigates the ability of flow based neural networks to improve signal extraction of $\\Lambda$ Hyperons at CLAS12. Normalizing Flows can help model complex probability density functions that describe physics processes, enabling uses such as event generation. $\\Lambda$ signal extraction has been improved through the use of classifier networks, but differences in simulation and data domains limit classifier performance; this study utilizes the flows for domain adaptation between Monte Carlo simulation and data. We were successful in training a flow network to transform between the latent physics space and a normal distribution. We also found that applying the flows lessened the dependence of the figure of merit on the cut on the classifier output, meaning that there was a broader range where the cut results in a similar figure of merit.","sentences":["The present study presents a novel application for normalizing flows for domain adaptation.","The study investigates the ability of flow based neural networks to improve signal extraction of $\\Lambda$ Hyperons at CLAS12.","Normalizing Flows can help model complex probability density functions that describe physics processes, enabling uses such as event generation.","$\\Lambda$ signal extraction has been improved through the use of classifier networks, but differences in simulation and data domains limit classifier performance; this study utilizes the flows for domain adaptation between Monte Carlo simulation and data.","We were successful in training a flow network to transform between the latent physics space and a normal distribution.","We also found that applying the flows lessened the dependence of the figure of merit on the cut on the classifier output, meaning that there was a broader range where the cut results in a similar figure of merit."],"url":"http://arxiv.org/abs/2403.14076v1","category":"hep-ex"}
{"created":"2024-03-21 01:10:56","title":"PE-GPT: A Physics-Informed Interactive Large Language Model for Power Converter Modulation Design","abstract":"This paper proposes PE-GPT, a custom-tailored large language model uniquely adapted for power converter modulation design. By harnessing in-context learning and specialized tiered physics-informed neural networks, PE-GPT guides users through text-based dialogues, recommending actionable modulation parameters. The effectiveness of PE-GPT is validated through a practical design case involving dual active bridge converters, supported by hardware experimentation. This research underscores the transformative potential of large language models in power converter modulation design, offering enhanced accessibility, explainability, and efficiency, thereby setting a new paradigm in the field.","sentences":["This paper proposes PE-GPT, a custom-tailored large language model uniquely adapted for power converter modulation design.","By harnessing in-context learning and specialized tiered physics-informed neural networks, PE-GPT guides users through text-based dialogues, recommending actionable modulation parameters.","The effectiveness of PE-GPT is validated through a practical design case involving dual active bridge converters, supported by hardware experimentation.","This research underscores the transformative potential of large language models in power converter modulation design, offering enhanced accessibility, explainability, and efficiency, thereby setting a new paradigm in the field."],"url":"http://arxiv.org/abs/2403.14059v1","category":"eess.SY"}
{"created":"2024-03-21 00:35:39","title":"Adaptive Finite Element Interpolated Neural Networks","abstract":"The use of neural networks to approximate partial differential equations (PDEs) has gained significant attention in recent years. However, the approximation of PDEs with localised phenomena, e.g., sharp gradients and singularities, remains a challenge, due to ill-defined cost functions in terms of pointwise residual sampling or poor numerical integration. In this work, we introduce $h$-adaptive finite element interpolated neural networks. The method relies on the interpolation of a neural network onto a finite element space that is gradually adapted to the solution during the training process to equidistribute a posteriori error indicator. The use of adaptive interpolation is essential in preserving the non-linear approximation capabilities of the neural networks to effectively tackle problems with localised features. The training relies on a gradient-based optimisation of a loss function based on the (dual) norm of the finite element residual of the interpolated neural network. Automatic mesh adaptation (i.e., refinement and coarsening) is performed based on a posteriori error indicators till a certain level of accuracy is reached. The proposed methodology can be applied to indefinite and nonsymmetric problems. We carry out a detailed numerical analysis of the scheme and prove several a priori error estimates, depending on the expressiveness of the neural network compared to the interpolation mesh. Our numerical experiments confirm the effectiveness of the method in capturing sharp gradients and singularities for forward PDE problems, both in 2D and 3D scenarios. We also show that the proposed preconditioning strategy (i.e., using a dual residual norm of the residual as a cost function) enhances training robustness and accelerates convergence.","sentences":["The use of neural networks to approximate partial differential equations (PDEs) has gained significant attention in recent years.","However, the approximation of PDEs with localised phenomena, e.g., sharp gradients and singularities, remains a challenge, due to ill-defined cost functions in terms of pointwise residual sampling or poor numerical integration.","In this work, we introduce $h$-adaptive finite element interpolated neural networks.","The method relies on the interpolation of a neural network onto a finite element space that is gradually adapted to the solution during the training process to equidistribute a posteriori error indicator.","The use of adaptive interpolation is essential in preserving the non-linear approximation capabilities of the neural networks to effectively tackle problems with localised features.","The training relies on a gradient-based optimisation of a loss function based on the (dual) norm of the finite element residual of the interpolated neural network.","Automatic mesh adaptation (i.e., refinement and coarsening) is performed based on a posteriori error indicators till a certain level of accuracy is reached.","The proposed methodology can be applied to indefinite and nonsymmetric problems.","We carry out a detailed numerical analysis of the scheme and prove several a priori error estimates, depending on the expressiveness of the neural network compared to the interpolation mesh.","Our numerical experiments confirm the effectiveness of the method in capturing sharp gradients and singularities for forward PDE problems, both in 2D and 3D scenarios.","We also show that the proposed preconditioning strategy (i.e., using a dual residual norm of the residual as a cost function) enhances training robustness and accelerates convergence."],"url":"http://arxiv.org/abs/2403.14054v1","category":"math.NA"}
{"created":"2024-03-20 22:51:29","title":"HRI Curriculum for a Liberal Arts Education","abstract":"In this paper, we discuss the opportunities and challenges of teaching a human-robot interaction course at an undergraduate liberal arts college. We provide a sample syllabus adapted from a previous version of a course.","sentences":["In this paper, we discuss the opportunities and challenges of teaching a human-robot interaction course at an undergraduate liberal arts college.","We provide a sample syllabus adapted from a previous version of a course."],"url":"http://arxiv.org/abs/2403.14025v1","category":"cs.CY"}
{"created":"2024-03-20 21:41:10","title":"Observations of Locally Excited Waves in the Low Solar Atmosphere Using the Daniel K. Inouye Solar Telescope (DKIST)","abstract":"We present an interpretation of the recent Daniel K. Inouye Solar Telescope (DKIST) observations of propagating wavefronts in the lower solar atmosphere. Using MPS/University of Chicago MHD (MURaM) radiative magnetohydrodynamic simulations spanning solar photosphere, overshoot region, and lower chromosphere, we identify three acoustic-wave source mechanisms, each occurring at a different atmospheric height. We synthesize the DKIST Visible Broadband Imager (VBI) G-band, blue-continuum, and CaIIK signatures of these waves at high spatial and temporal resolution, and conclude that the wavefronts observed by DKIST likely originate from acoustic sources at the top of the solar photosphere overshoot region and in the chromosphere proper. The overall importance of these local sources to the atmospheric energy and momentum budget of the solar atmosphere is unknown, but one of the excitation mechanism identified (upward propagating shock interaction with down-welling chromospheric plasma resulting in acoustic radiation) appears to be an important shock dissipation mechanism. Additionally, the observed wavefronts may prove useful for ultra-local helioseismological inversions and promise to play an important diagnostic role at multiple atmospheric heights.","sentences":["We present an interpretation of the recent Daniel K. Inouye Solar Telescope (DKIST) observations of propagating wavefronts in the lower solar atmosphere.","Using MPS/University of Chicago MHD (MURaM) radiative magnetohydrodynamic simulations spanning solar photosphere, overshoot region, and lower chromosphere, we identify three acoustic-wave source mechanisms, each occurring at a different atmospheric height.","We synthesize the DKIST Visible Broadband Imager (VBI) G-band, blue-continuum, and CaIIK signatures of these waves at high spatial and temporal resolution, and conclude that the wavefronts observed by DKIST likely originate from acoustic sources at the top of the solar photosphere overshoot region and in the chromosphere proper.","The overall importance of these local sources to the atmospheric energy and momentum budget of the solar atmosphere is unknown, but one of the excitation mechanism identified (upward propagating shock interaction with down-welling chromospheric plasma resulting in acoustic radiation) appears to be an important shock dissipation mechanism.","Additionally, the observed wavefronts may prove useful for ultra-local helioseismological inversions and promise to play an important diagnostic role at multiple atmospheric heights."],"url":"http://arxiv.org/abs/2403.13987v1","category":"astro-ph.SR"}
{"created":"2024-03-20 20:43:52","title":"Meta-plasticity and memory in multi-level recurrent feed-forward networks","abstract":"Network systems can exhibit memory effects in which the interactions between different pairs of nodes adapt in time, leading to the emergence of preferred connections, patterns, and sub-networks. To a first approximation, this memory can be modelled through a ``plastic'' Hebbian or homophily mechanism, in which edges get reinforced proportionally to the amount of information flowing through them. However, recent studies on glia-neuron networks have highlighted how memory can evolve due to more complex dynamics, including multi-level network structures and ``meta-plastic'' effects that modulate reinforcement. Inspired by those systems, here we develop a simple and general model for the dynamics of an adaptive network with an additional meta-plastic mechanism that varies the rate of Hebbian strengthening of its edge connections. Specifically, we consider a biased random walk on a cyclic feed-forward network. The random walk chooses its steps according to the weights of the network edges. The weights evolve through a Hebbian mechanism modulated by a meta-plastic reinforcement, biasing the walker to prefer edges that have been already explored. We study the dynamical emergence (memorization) of preferred paths and their retrieval and identify three regimes: one dominated by the Hebbian term, one in which the meta-reinforcement drives memory formation, and a balanced one. We show that, in the latter two regimes, meta-reinforcement allows for the retrieval of a previously stored path even after weights have been reset to zero to erase Hebbian memory.","sentences":["Network systems can exhibit memory effects in which the interactions between different pairs of nodes adapt in time, leading to the emergence of preferred connections, patterns, and sub-networks.","To a first approximation, this memory can be modelled through a ``plastic'' Hebbian or homophily mechanism, in which edges get reinforced proportionally to the amount of information flowing through them.","However, recent studies on glia-neuron networks have highlighted how memory can evolve due to more complex dynamics, including multi-level network structures and ``meta-plastic'' effects that modulate reinforcement.","Inspired by those systems, here we develop a simple and general model for the dynamics of an adaptive network with an additional meta-plastic mechanism that varies the rate of Hebbian strengthening of its edge connections.","Specifically, we consider a biased random walk on a cyclic feed-forward network.","The random walk chooses its steps according to the weights of the network edges.","The weights evolve through a Hebbian mechanism modulated by a meta-plastic reinforcement, biasing the walker to prefer edges that have been already explored.","We study the dynamical emergence (memorization) of preferred paths and their retrieval and identify three regimes: one dominated by the Hebbian term, one in which the meta-reinforcement drives memory formation, and a balanced one.","We show that, in the latter two regimes, meta-reinforcement allows for the retrieval of a previously stored path even after weights have been reset to zero to erase Hebbian memory."],"url":"http://arxiv.org/abs/2403.13967v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-20 19:34:07","title":"$N$-player game formulation of the majority-vote model of opinion dynamics","abstract":"From a self-centered perspective, it can be assumed that people only hold opinions that can benefit them. If opinions have no intrinsic value, and acquire their value when held by the majority of individuals in a discussion group, then we have a situation that can be modeled as an $N$-player game. Here we explore the dynamics of (binary) opinion formation using a game-theoretic framework to study an $N$-player game version of Galam's local majority-vote model. The opinion dynamics is modeled by a stochastic imitation dynamics in which the individuals copy the opinion of more successful peers. In the infinite population limit, this dynamics is described by the classical replicator equation of evolutionary game theory. The equilibrium solution shows a threshold separating the initial frequencies that lead to the fixation of one opinion or the other. A comparison with Galam's deterministic model reveals contrasting results, especially in the presence of inflexible individuals, who never change their opinions. In particular, the $N$-player game predicts a polarized equilibrium consisting only of extremists. Using finite-size scaling analysis, we evaluate the critical exponents that determine the population size dependence of the opinion's fixation probability and mean fixation times near the threshold. The results underscore the usefulness of combining evolutionary game theory with opinion dynamics and the importance of statistical physics tools to summarize the results of Monte Carlo simulations.","sentences":["From a self-centered perspective, it can be assumed that people only hold opinions that can benefit them.","If opinions have no intrinsic value, and acquire their value when held by the majority of individuals in a discussion group, then we have a situation that can be modeled as an $N$-player game.","Here we explore the dynamics of (binary) opinion formation using a game-theoretic framework to study an $N$-player game version of Galam's local majority-vote model.","The opinion dynamics is modeled by a stochastic imitation dynamics in which the individuals copy the opinion of more successful peers.","In the infinite population limit, this dynamics is described by the classical replicator equation of evolutionary game theory.","The equilibrium solution shows a threshold separating the initial frequencies that lead to the fixation of one opinion or the other.","A comparison with Galam's deterministic model reveals contrasting results, especially in the presence of inflexible individuals, who never change their opinions.","In particular, the $N$-player game predicts a polarized equilibrium consisting only of extremists.","Using finite-size scaling analysis, we evaluate the critical exponents that determine the population size dependence of the opinion's fixation probability and mean fixation times near the threshold.","The results underscore the usefulness of combining evolutionary game theory with opinion dynamics and the importance of statistical physics tools to summarize the results of Monte Carlo simulations."],"url":"http://arxiv.org/abs/2403.13945v1","category":"physics.soc-ph"}
{"created":"2024-03-20 18:20:12","title":"Nonequilibrium quantum heat transport between structured environments","abstract":"We apply the hierarchical equations of motion technique to analyzing nonequilibrium heat transport in a spin-boson type model, whereby heat transfer through a central spin is mediated by an intermediate pair of coupled harmonic oscillators. The coupling between each pair of oscillators is shown to introduce a localized gap into the effective spectral densities characterizing the system-oscillator-reservoir interactions. Compared to the case of a single mediating oscillator, we find the heat current to be drastically modified at weak system-bath coupling. In particular, a second-order treatment fails to capture the correct steady-state behavior in this regime, which stems from the $\\lambda^4$-scaling of the energy transfer rate to lowest order in the coupling strength $\\lambda$. This leads naturally to a strong suppression in the steady-state current in the asymptotically weak coupling limit. On the other hand, the current noise follows the same scaling as in the single oscillator case in accordance with the fluctuation-dissipation theorem. Additionally, we find the heat current to be consistent with Fourier's law even at large temperature bias. Our analysis highlights a novel mechanism for controlling heat transport in nanoscale systems based on tailoring the spectral properties of thermal environments.","sentences":["We apply the hierarchical equations of motion technique to analyzing nonequilibrium heat transport in a spin-boson type model, whereby heat transfer through a central spin is mediated by an intermediate pair of coupled harmonic oscillators.","The coupling between each pair of oscillators is shown to introduce a localized gap into the effective spectral densities characterizing the system-oscillator-reservoir interactions.","Compared to the case of a single mediating oscillator, we find the heat current to be drastically modified at weak system-bath coupling.","In particular, a second-order treatment fails to capture the correct steady-state behavior in this regime, which stems from the $\\lambda^4$-scaling of the energy transfer rate to lowest order in the coupling strength $\\lambda$.","This leads naturally to a strong suppression in the steady-state current in the asymptotically weak coupling limit.","On the other hand, the current noise follows the same scaling as in the single oscillator case in accordance with the fluctuation-dissipation theorem.","Additionally, we find the heat current to be consistent with Fourier's law even at large temperature bias.","Our analysis highlights a novel mechanism for controlling heat transport in nanoscale systems based on tailoring the spectral properties of thermal environments."],"url":"http://arxiv.org/abs/2403.13904v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-20 18:05:57","title":"Background-Induced Forces from Dark Relics","abstract":"Light particles quadratically coupled to nucleons induce macroscopic forces in matter. While a quantum effect always exists, an additional force occurs in the presence of a finite density of the light particles. We compute and classify such background-induced forces for particles of spin $0,\\frac{1}{2},1$ in the framework of effective field theory. We show that, at short distance, the background-induced forces exhibit a universal behavior that depends solely on the moments of the phase space distribution function of the light particles. We compute the forces in the case of thermal-like densities of dark particles that may realistically occur in cosmology. All the background-induced forces remain, unlike the quantum ones, exponentially unsuppressed at large distance, implying that large scale fifth force experiments are highly sensitive to dark relics. Moreover at zero mass the forces from dark bosons are generically enhanced with respect to their quantum counterpart due to Bose-Einstein distribution. Overall, we find that the resulting fifth force bounds can compete with those from quantum forces. A thorough adaptation of the results from the E\\\"ot-Wash experiment may produce powerful additional bounds.","sentences":["Light particles quadratically coupled to nucleons induce macroscopic forces in matter.","While a quantum effect always exists, an additional force occurs in the presence of a finite density of the light particles.","We compute and classify such background-induced forces for particles of spin $0,\\frac{1}{2},1$ in the framework of effective field theory.","We show that, at short distance, the background-induced forces exhibit a universal behavior that depends solely on the moments of the phase space distribution function of the light particles.","We compute the forces in the case of thermal-like densities of dark particles that may realistically occur in cosmology.","All the background-induced forces remain, unlike the quantum ones, exponentially unsuppressed at large distance, implying that large scale fifth force experiments are highly sensitive to dark relics.","Moreover at zero mass the forces from dark bosons are generically enhanced with respect to their quantum counterpart due to Bose-Einstein distribution.","Overall, we find that the resulting fifth force bounds can compete with those from quantum forces.","A thorough adaptation of the results from the E\\\"ot-Wash experiment may produce powerful additional bounds."],"url":"http://arxiv.org/abs/2403.13894v1","category":"hep-ph"}
{"created":"2024-03-19 17:59:39","title":"Negative Yields Positive: Unified Dual-Path Adapter for Vision-Language Models","abstract":"Recently, large-scale pre-trained Vision-Language Models (VLMs) have demonstrated great potential in learning open-world visual representations, and exhibit remarkable performance across a wide range of downstream tasks through efficient fine-tuning. In this work, we innovatively introduce the concept of dual learning into fine-tuning VLMs, i.e., we not only learn what an image is, but also what an image isn't. Building on this concept, we introduce a novel DualAdapter approach to enable dual-path adaptation of VLMs from both positive and negative perspectives with only limited annotated samples. In the inference stage, our DualAdapter performs unified predictions by simultaneously conducting complementary positive selection and negative exclusion across target classes, thereby enhancing the overall recognition accuracy of VLMs in downstream tasks. Our extensive experimental results across 15 datasets validate that the proposed DualAdapter outperforms existing state-of-the-art methods on both few-shot learning and domain generalization tasks while achieving competitive computational efficiency. Code is available at https://github.com/zhangce01/DualAdapter.","sentences":["Recently, large-scale pre-trained Vision-Language Models (VLMs) have demonstrated great potential in learning open-world visual representations, and exhibit remarkable performance across a wide range of downstream tasks through efficient fine-tuning.","In this work, we innovatively introduce the concept of dual learning into fine-tuning VLMs, i.e., we not only learn what an image is, but also what an image isn't.","Building on this concept, we introduce a novel DualAdapter approach to enable dual-path adaptation of VLMs from both positive and negative perspectives with only limited annotated samples.","In the inference stage, our DualAdapter performs unified predictions by simultaneously conducting complementary positive selection and negative exclusion across target classes, thereby enhancing the overall recognition accuracy of VLMs in downstream tasks.","Our extensive experimental results across 15 datasets validate that the proposed DualAdapter outperforms existing state-of-the-art methods on both few-shot learning and domain generalization tasks while achieving competitive computational efficiency.","Code is available at https://github.com/zhangce01/DualAdapter."],"url":"http://arxiv.org/abs/2403.12964v1","category":"cs.CV"}
{"created":"2024-03-19 17:55:22","title":"FutureDepth: Learning to Predict the Future Improves Video Depth Estimation","abstract":"In this paper, we propose a novel video depth estimation approach, FutureDepth, which enables the model to implicitly leverage multi-frame and motion cues to improve depth estimation by making it learn to predict the future at training. More specifically, we propose a future prediction network, F-Net, which takes the features of multiple consecutive frames and is trained to predict multi-frame features one time step ahead iteratively. In this way, F-Net learns the underlying motion and correspondence information, and we incorporate its features into the depth decoding process. Additionally, to enrich the learning of multiframe correspondence cues, we further leverage a reconstruction network, R-Net, which is trained via adaptively masked auto-encoding of multiframe feature volumes. At inference time, both F-Net and R-Net are used to produce queries to work with the depth decoder, as well as a final refinement network. Through extensive experiments on several benchmarks, i.e., NYUDv2, KITTI, DDAD, and Sintel, which cover indoor, driving, and open-domain scenarios, we show that FutureDepth significantly improves upon baseline models, outperforms existing video depth estimation methods, and sets new state-of-the-art (SOTA) accuracy. Furthermore, FutureDepth is more efficient than existing SOTA video depth estimation models and has similar latencies when comparing to monocular models","sentences":["In this paper, we propose a novel video depth estimation approach, FutureDepth, which enables the model to implicitly leverage multi-frame and motion cues to improve depth estimation by making it learn to predict the future at training.","More specifically, we propose a future prediction network, F-Net, which takes the features of multiple consecutive frames and is trained to predict multi-frame features one time step ahead iteratively.","In this way, F-Net learns the underlying motion and correspondence information, and we incorporate its features into the depth decoding process.","Additionally, to enrich the learning of multiframe correspondence cues, we further leverage a reconstruction network, R-Net, which is trained via adaptively masked auto-encoding of multiframe feature volumes.","At inference time, both F-Net and R-Net are used to produce queries to work with the depth decoder, as well as a final refinement network.","Through extensive experiments on several benchmarks, i.e., NYUDv2, KITTI, DDAD, and Sintel, which cover indoor, driving, and open-domain scenarios, we show that FutureDepth significantly improves upon baseline models, outperforms existing video depth estimation methods, and sets new state-of-the-art (SOTA) accuracy.","Furthermore, FutureDepth is more efficient than existing SOTA video depth estimation models and has similar latencies when comparing to monocular models"],"url":"http://arxiv.org/abs/2403.12953v1","category":"cs.CV"}
{"created":"2024-03-19 17:54:34","title":"Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization with Vision-Language Models","abstract":"Advancements in vision-language models (VLMs) have propelled the field of computer vision, particularly in the zero-shot learning setting. Despite their promise, the effectiveness of these models often diminishes due to domain shifts in test environments. To address this, we introduce the Test-Time Prototype Shifting (TPS) framework, a pioneering approach designed to adapt VLMs to test datasets using unlabeled test inputs. Our method is based on the notion of modulating per-class prototypes in the shared embedding space. By pre-computing and caching prototypes generated with the pre-trained text encoder, TPS not only facilitates optimization-free prototype reuse for subsequent predictions but also enables seamless integration with current advancements in prompt engineering. At test-time, TPS dynamically learns shift vectors for each prototype based solely on the given test sample, effectively bridging the domain gap and enhancing classification accuracy. A notable aspect of our framework is its significantly reduced memory and computational demands when compared to conventional text-prompt tuning methods. Extensive evaluations across 15 datasets involving natural distribution shifts and cross-dataset generalization demonstrate TPS's superior performance, achieving state-of-the-art results while reducing resource requirements.","sentences":["Advancements in vision-language models (VLMs) have propelled the field of computer vision, particularly in the zero-shot learning setting.","Despite their promise, the effectiveness of these models often diminishes due to domain shifts in test environments.","To address this, we introduce the Test-Time Prototype Shifting (TPS) framework, a pioneering approach designed to adapt VLMs to test datasets using unlabeled test inputs.","Our method is based on the notion of modulating per-class prototypes in the shared embedding space.","By pre-computing and caching prototypes generated with the pre-trained text encoder, TPS not only facilitates optimization-free prototype reuse for subsequent predictions but also enables seamless integration with current advancements in prompt engineering.","At test-time, TPS dynamically learns shift vectors for each prototype based solely on the given test sample, effectively bridging the domain gap and enhancing classification accuracy.","A notable aspect of our framework is its significantly reduced memory and computational demands when compared to conventional text-prompt tuning methods.","Extensive evaluations across 15 datasets involving natural distribution shifts and cross-dataset generalization demonstrate TPS's superior performance, achieving state-of-the-art results while reducing resource requirements."],"url":"http://arxiv.org/abs/2403.12952v1","category":"cs.CV"}
{"created":"2024-03-19 17:50:55","title":"Optimal and Adaptive Non-Stationary Dueling Bandits Under a Generalized Borda Criterion","abstract":"In dueling bandits, the learner receives preference feedback between arms, and the regret of an arm is defined in terms of its suboptimality to a winner arm. The more challenging and practically motivated non-stationary variant of dueling bandits, where preferences change over time, has been the focus of several recent works (Saha and Gupta, 2022; Buening and Saha, 2023; Suk and Agarwal, 2023). The goal is to design algorithms without foreknowledge of the amount of change.   The bulk of known results here studies the Condorcet winner setting, where an arm preferred over any other exists at all times. Yet, such a winner may not exist and, to contrast, the Borda version of this problem (which is always well-defined) has received little attention. In this work, we establish the first optimal and adaptive Borda dynamic regret upper bound, which highlights fundamental differences in the learnability of severe non-stationarity between Condorcet vs. Borda regret objectives in dueling bandits.   Surprisingly, our techniques for non-stationary Borda dueling bandits also yield improved rates within the Condorcet winner setting, and reveal new preference models where tighter notions of non-stationarity are adaptively learnable. This is accomplished through a novel generalized Borda score framework which unites the Borda and Condorcet problems, thus allowing reduction of Condorcet regret to a Borda-like task. Such a generalization was not previously known and is likely to be of independent interest.","sentences":["In dueling bandits, the learner receives preference feedback between arms, and the regret of an arm is defined in terms of its suboptimality to a winner arm.","The more challenging and practically motivated non-stationary variant of dueling bandits, where preferences change over time, has been the focus of several recent works (Saha and Gupta, 2022; Buening and Saha, 2023; Suk and Agarwal, 2023).","The goal is to design algorithms without foreknowledge of the amount of change.   ","The bulk of known results here studies the Condorcet winner setting, where an arm preferred over any other exists at all times.","Yet, such a winner may not exist and, to contrast, the Borda version of this problem (which is always well-defined) has received little attention.","In this work, we establish the first optimal and adaptive Borda dynamic regret upper bound, which highlights fundamental differences in the learnability of severe non-stationarity between Condorcet vs. Borda regret objectives in dueling bandits.   ","Surprisingly, our techniques for non-stationary Borda dueling bandits also yield improved rates within the Condorcet winner setting, and reveal new preference models where tighter notions of non-stationarity are adaptively learnable.","This is accomplished through a novel generalized Borda score framework which unites the Borda and Condorcet problems, thus allowing reduction of Condorcet regret to a Borda-like task.","Such a generalization was not previously known and is likely to be of independent interest."],"url":"http://arxiv.org/abs/2403.12950v1","category":"cs.LG"}
{"created":"2024-03-19 17:50:40","title":"A Novel Energy-Efficient Cross-Layer Design for Scheduling and Routing in 6TiSCH Networks","abstract":"The 6TiSCH protocol stack was proposed to ensure high-performance communications in the Industrial Internet of Things (IIoT). However, the lack of sufficient time slots for nodes outside the 6TiSCH's Destination Oriented Directed Acyclic Graph (DODAG) to transmit their Destination Advertisement Object (DAO) messages and cell reservation requests significantly hinders their integration into the DODAG. This oversight not only prolongs the device's join time but also increases energy consumption during the network formation phase. Moreover, challenges emerge due to the substantial number of control packets employed by both the 6TiSCH Scheduling Function (SF) and routing protocol (RPL), thus draining more energy resources, increasing medium contention, and decreasing spatial reuse. Furthermore, an SF that overlooks previously allocated slots when assigning new ones to the same node may increase jitter, and more complications ensue when it neglects the state of the TSCH queue, thus leading to packet dropping due to queue saturation. Additional complexity arises when the RPL disregards the new parent's schedule saturation during parent switching, which results in inefficient energy and time usage. To address these issues, we introduce in this paper novel mechanisms, strategically situated at the intersection of SF and RPL that are designed to balance the control packet distribution and adaptively manage parent switching. Our proposal, implemented within the 6TiSCH simulator, demonstrates significant improvements across vital performance metrics, such as node's joining time, jitter, latency, energy consumption, and amount of traffic, in comparison to the conventional 6TiSCH benchmark.","sentences":["The 6TiSCH protocol stack was proposed to ensure high-performance communications in the Industrial Internet of Things (IIoT).","However, the lack of sufficient time slots for nodes outside the 6TiSCH's Destination Oriented Directed Acyclic Graph (DODAG) to transmit their Destination Advertisement Object (DAO) messages and cell reservation requests significantly hinders their integration into the DODAG.","This oversight not only prolongs the device's join time but also increases energy consumption during the network formation phase.","Moreover, challenges emerge due to the substantial number of control packets employed by both the 6TiSCH Scheduling Function (SF) and routing protocol (RPL), thus draining more energy resources, increasing medium contention, and decreasing spatial reuse.","Furthermore, an SF that overlooks previously allocated slots when assigning new ones to the same node may increase jitter, and more complications ensue when it neglects the state of the TSCH queue, thus leading to packet dropping due to queue saturation.","Additional complexity arises when the RPL disregards the new parent's schedule saturation during parent switching, which results in inefficient energy and time usage.","To address these issues, we introduce in this paper novel mechanisms, strategically situated at the intersection of SF and RPL that are designed to balance the control packet distribution and adaptively manage parent switching.","Our proposal, implemented within the 6TiSCH simulator, demonstrates significant improvements across vital performance metrics, such as node's joining time, jitter, latency, energy consumption, and amount of traffic, in comparison to the conventional 6TiSCH benchmark."],"url":"http://arxiv.org/abs/2403.12949v1","category":"cs.NI"}
{"created":"2024-03-19 17:34:27","title":"You Only Sample Once: Taming One-Step Text-To-Image Synthesis by Self-Cooperative Diffusion GANs","abstract":"We introduce YOSO, a novel generative model designed for rapid, scalable, and high-fidelity one-step image synthesis. This is achieved by integrating the diffusion process with GANs. Specifically, we smooth the distribution by the denoising generator itself, performing self-cooperative learning. We show that our method can serve as a one-step generation model training from scratch with competitive performance. Moreover, we show that our method can be extended to finetune pre-trained text-to-image diffusion for high-quality one-step text-to-image synthesis even with LoRA fine-tuning. In particular, we provide the first diffusion transformer that can generate images in one step trained on 512 resolution, with the capability of adapting to 1024 resolution without explicit training. Our code is provided at https://github.com/Luo-Yihong/YOSO.","sentences":["We introduce YOSO, a novel generative model designed for rapid, scalable, and high-fidelity one-step image synthesis.","This is achieved by integrating the diffusion process with GANs.","Specifically, we smooth the distribution by the denoising generator itself, performing self-cooperative learning.","We show that our method can serve as a one-step generation model training from scratch with competitive performance.","Moreover, we show that our method can be extended to finetune pre-trained text-to-image diffusion for high-quality one-step text-to-image synthesis even with LoRA fine-tuning.","In particular, we provide the first diffusion transformer that can generate images in one step trained on 512 resolution, with the capability of adapting to 1024 resolution without explicit training.","Our code is provided at https://github.com/Luo-Yihong/YOSO."],"url":"http://arxiv.org/abs/2403.12931v1","category":"cs.CV"}
{"created":"2024-03-19 17:10:52","title":"Strangers in a foreign land: 'Yeastizing' plant enzymes","abstract":"Expressing plant metabolic pathways in microbial platforms is an efficient, cost-effective solution for producing many desired plant compounds. As eukaryotic organisms, yeasts are often the preferred platform. However, expression of plant enzymes in a yeast frequently leads to failure because the enzymes are poorly adapted to the foreign yeast cellular environment. Here we first summarize current engineering approaches for optimizing performance of plant enzymes in yeast. A critical limitation of these approaches is that they are labor-intensive and must be customized for each individual enzyme, which significantly hinders the establishment of plant pathways in cellular factories. In response to this challenge, we propose the development of a cost-effective computational pipeline to redesign plant enzymes for better adaptation to the yeast cellular milieu. This proposition is underpinned by compelling evidence that plant and yeast enzymes exhibit distinct sequence features that are generalizable across enzyme families. Consequently, we introduce a data-driven machine learning framework designed to extract 'yeastizing' rules from natural protein sequence variations, which can be broadly applied to all enzymes. Additionally, we discuss the potential to integrate the machine learning model into a full design-build-test-cycle.","sentences":["Expressing plant metabolic pathways in microbial platforms is an efficient, cost-effective solution for producing many desired plant compounds.","As eukaryotic organisms, yeasts are often the preferred platform.","However, expression of plant enzymes in a yeast frequently leads to failure because the enzymes are poorly adapted to the foreign yeast cellular environment.","Here we first summarize current engineering approaches for optimizing performance of plant enzymes in yeast.","A critical limitation of these approaches is that they are labor-intensive and must be customized for each individual enzyme, which significantly hinders the establishment of plant pathways in cellular factories.","In response to this challenge, we propose the development of a cost-effective computational pipeline to redesign plant enzymes for better adaptation to the yeast cellular milieu.","This proposition is underpinned by compelling evidence that plant and yeast enzymes exhibit distinct sequence features that are generalizable across enzyme families.","Consequently, we introduce a data-driven machine learning framework designed to extract 'yeastizing' rules from natural protein sequence variations, which can be broadly applied to all enzymes.","Additionally, we discuss the potential to integrate the machine learning model into a full design-build-test-cycle."],"url":"http://arxiv.org/abs/2403.12912v2","category":"q-bio.BM"}
{"created":"2024-03-19 17:08:24","title":"Yell At Your Robot: Improving On-the-Fly from Language Corrections","abstract":"Hierarchical policies that combine language and low-level control have been shown to perform impressively long-horizon robotic tasks, by leveraging either zero-shot high-level planners like pretrained language and vision-language models (LLMs/VLMs) or models trained on annotated robotic demonstrations. However, for complex and dexterous skills, attaining high success rates on long-horizon tasks still represents a major challenge -- the longer the task is, the more likely it is that some stage will fail. Can humans help the robot to continuously improve its long-horizon task performance through intuitive and natural feedback? In this paper, we make the following observation: high-level policies that index into sufficiently rich and expressive low-level language-conditioned skills can be readily supervised with human feedback in the form of language corrections. We show that even fine-grained corrections, such as small movements (\"move a bit to the left\"), can be effectively incorporated into high-level policies, and that such corrections can be readily obtained from humans observing the robot and making occasional suggestions. This framework enables robots not only to rapidly adapt to real-time language feedback, but also incorporate this feedback into an iterative training scheme that improves the high-level policy's ability to correct errors in both low-level execution and high-level decision-making purely from verbal feedback. Our evaluation on real hardware shows that this leads to significant performance improvement in long-horizon, dexterous manipulation tasks without the need for any additional teleoperation. Videos and code are available at https://yay-robot.github.io/.","sentences":["Hierarchical policies that combine language and low-level control have been shown to perform impressively long-horizon robotic tasks, by leveraging either zero-shot high-level planners like pretrained language and vision-language models (LLMs/VLMs) or models trained on annotated robotic demonstrations.","However, for complex and dexterous skills, attaining high success rates on long-horizon tasks still represents a major challenge -- the longer the task is, the more likely it is that some stage will fail.","Can humans help the robot to continuously improve its long-horizon task performance through intuitive and natural feedback?","In this paper, we make the following observation: high-level policies that index into sufficiently rich and expressive low-level language-conditioned skills can be readily supervised with human feedback in the form of language corrections.","We show that even fine-grained corrections, such as small movements (\"move a bit to the left\"), can be effectively incorporated into high-level policies, and that such corrections can be readily obtained from humans observing the robot and making occasional suggestions.","This framework enables robots not only to rapidly adapt to real-time language feedback, but also incorporate this feedback into an iterative training scheme that improves the high-level policy's ability to correct errors in both low-level execution and high-level decision-making purely from verbal feedback.","Our evaluation on real hardware shows that this leads to significant performance improvement in long-horizon, dexterous manipulation tasks without the need for any additional teleoperation.","Videos and code are available at https://yay-robot.github.io/."],"url":"http://arxiv.org/abs/2403.12910v1","category":"cs.RO"}
{"created":"2024-03-19 17:02:07","title":"TexDreamer: Towards Zero-Shot High-Fidelity 3D Human Texture Generation","abstract":"Texturing 3D humans with semantic UV maps remains a challenge due to the difficulty of acquiring reasonably unfolded UV. Despite recent text-to-3D advancements in supervising multi-view renderings using large text-to-image (T2I) models, issues persist with generation speed, text consistency, and texture quality, resulting in data scarcity among existing datasets. We present TexDreamer, the first zero-shot multimodal high-fidelity 3D human texture generation model. Utilizing an efficient texture adaptation finetuning strategy, we adapt large T2I model to a semantic UV structure while preserving its original generalization capability. Leveraging a novel feature translator module, the trained model is capable of generating high-fidelity 3D human textures from either text or image within seconds. Furthermore, we introduce ArTicuLated humAn textureS (ATLAS), the largest high-resolution (1024 X 1024) 3D human texture dataset which contains 50k high-fidelity textures with text descriptions.","sentences":["Texturing 3D humans with semantic UV maps remains a challenge due to the difficulty of acquiring reasonably unfolded UV.","Despite recent text-to-3D advancements in supervising multi-view renderings using large text-to-image (T2I) models, issues persist with generation speed, text consistency, and texture quality, resulting in data scarcity among existing datasets.","We present TexDreamer, the first zero-shot multimodal high-fidelity 3D human texture generation model.","Utilizing an efficient texture adaptation finetuning strategy, we adapt large T2I model to a semantic UV structure while preserving its original generalization capability.","Leveraging a novel feature translator module, the trained model is capable of generating high-fidelity 3D human textures from either text or image within seconds.","Furthermore, we introduce ArTicuLated humAn textureS (ATLAS), the largest high-resolution (1024 X 1024) 3D human texture dataset which contains 50k high-fidelity textures with text descriptions."],"url":"http://arxiv.org/abs/2403.12906v1","category":"cs.CV"}
{"created":"2024-03-19 16:40:57","title":"Adaptive Visual Imitation Learning for Robotic Assisted Feeding Across Varied Bowl Configurations and Food Types","abstract":"In this study, we introduce a novel visual imitation network with a spatial attention module for robotic assisted feeding (RAF). The goal is to acquire (i.e., scoop) food items from a bowl. However, achieving robust and adaptive food manipulation is particularly challenging. To deal with this, we propose a framework that integrates visual perception with imitation learning to enable the robot to handle diverse scenarios during scooping. Our approach, named AVIL (adaptive visual imitation learning), exhibits adaptability and robustness across different bowl configurations in terms of material, size, and position, as well as diverse food types including granular, semi-solid, and liquid, even in the presence of distractors. We validate the effectiveness of our approach by conducting experiments on a real robot. We also compare its performance with a baseline. The results demonstrate improvement over the baseline across all scenarios, with an enhancement of up to 2.5 times in terms of a success metric. Notably, our model, trained solely on data from a transparent glass bowl containing granular cereals, showcases generalization ability when tested zero-shot on other bowl configurations with different types of food.","sentences":["In this study, we introduce a novel visual imitation network with a spatial attention module for robotic assisted feeding (RAF).","The goal is to acquire (i.e., scoop) food items from a bowl.","However, achieving robust and adaptive food manipulation is particularly challenging.","To deal with this, we propose a framework that integrates visual perception with imitation learning to enable the robot to handle diverse scenarios during scooping.","Our approach, named AVIL (adaptive visual imitation learning), exhibits adaptability and robustness across different bowl configurations in terms of material, size, and position, as well as diverse food types including granular, semi-solid, and liquid, even in the presence of distractors.","We validate the effectiveness of our approach by conducting experiments on a real robot.","We also compare its performance with a baseline.","The results demonstrate improvement over the baseline across all scenarios, with an enhancement of up to 2.5 times in terms of a success metric.","Notably, our model, trained solely on data from a transparent glass bowl containing granular cereals, showcases generalization ability when tested zero-shot on other bowl configurations with different types of food."],"url":"http://arxiv.org/abs/2403.12891v1","category":"cs.RO"}
{"created":"2024-03-19 16:31:30","title":"HYDRA: A Hyper Agent for Dynamic Compositional Visual Reasoning","abstract":"Recent advances in visual reasoning (VR), particularly with the aid of Large Vision-Language Models (VLMs), show promise but require access to large-scale datasets and face challenges such as high computational costs and limited generalization capabilities. Compositional visual reasoning approaches have emerged as effective strategies; however, they heavily rely on the commonsense knowledge encoded in Large Language Models (LLMs) to perform planning, reasoning, or both, without considering the effect of their decisions on the visual reasoning process, which can lead to errors or failed procedures. To address these challenges, we introduce HYDRA, a multi-stage dynamic compositional visual reasoning framework designed for reliable and incrementally progressive general reasoning. HYDRA integrates three essential modules: a planner, a Reinforcement Learning (RL) agent serving as a cognitive controller, and a reasoner. The planner and reasoner modules utilize an LLM to generate instruction samples and executable code from the selected instruction, respectively, while the RL agent dynamically interacts with these modules, making high-level decisions on selection of the best instruction sample given information from the historical state stored through a feedback loop. This adaptable design enables HYDRA to adjust its actions based on previous feedback received during the reasoning process, leading to more reliable reasoning outputs and ultimately enhancing its overall effectiveness. Our framework demonstrates state-of-the-art performance in various VR tasks on four different widely-used datasets.","sentences":["Recent advances in visual reasoning (VR), particularly with the aid of Large Vision-Language Models (VLMs), show promise but require access to large-scale datasets and face challenges such as high computational costs and limited generalization capabilities.","Compositional visual reasoning approaches have emerged as effective strategies; however, they heavily rely on the commonsense knowledge encoded in Large Language Models (LLMs) to perform planning, reasoning, or both, without considering the effect of their decisions on the visual reasoning process, which can lead to errors or failed procedures.","To address these challenges, we introduce HYDRA, a multi-stage dynamic compositional visual reasoning framework designed for reliable and incrementally progressive general reasoning.","HYDRA integrates three essential modules: a planner, a Reinforcement Learning (RL) agent serving as a cognitive controller, and a reasoner.","The planner and reasoner modules utilize an LLM to generate instruction samples and executable code from the selected instruction, respectively, while the RL agent dynamically interacts with these modules, making high-level decisions on selection of the best instruction sample given information from the historical state stored through a feedback loop.","This adaptable design enables HYDRA to adjust its actions based on previous feedback received during the reasoning process, leading to more reliable reasoning outputs and ultimately enhancing its overall effectiveness.","Our framework demonstrates state-of-the-art performance in various VR tasks on four different widely-used datasets."],"url":"http://arxiv.org/abs/2403.12884v1","category":"cs.CV"}
{"created":"2024-03-19 16:29:59","title":"Confusing Pair Correction Based on Category Prototype for Domain Adaptation under Noisy Environments","abstract":"In this paper, we address unsupervised domain adaptation under noisy environments, which is more challenging and practical than traditional domain adaptation. In this scenario, the model is prone to overfitting noisy labels, resulting in a more pronounced domain shift and a notable decline in the overall model performance. Previous methods employed prototype methods for domain adaptation on robust feature spaces. However, these approaches struggle to effectively classify classes with similar features under noisy environments. To address this issue, we propose a new method to detect and correct confusing class pair. We first divide classes into easy and hard classes based on the small loss criterion. We then leverage the top-2 predictions for each sample after aligning the source and target domain to find the confusing pair in the hard classes. We apply label correction to the noisy samples within the confusing pair. With the proposed label correction method, we can train our model with more accurate labels. Extensive experiments confirm the effectiveness of our method and demonstrate its favorable performance compared with existing state-of-the-art methods. Our codes are publicly available at https://github.com/Hehxcf/CPC/.","sentences":["In this paper, we address unsupervised domain adaptation under noisy environments, which is more challenging and practical than traditional domain adaptation.","In this scenario, the model is prone to overfitting noisy labels, resulting in a more pronounced domain shift and a notable decline in the overall model performance.","Previous methods employed prototype methods for domain adaptation on robust feature spaces.","However, these approaches struggle to effectively classify classes with similar features under noisy environments.","To address this issue, we propose a new method to detect and correct confusing class pair.","We first divide classes into easy and hard classes based on the small loss criterion.","We then leverage the top-2 predictions for each sample after aligning the source and target domain to find the confusing pair in the hard classes.","We apply label correction to the noisy samples within the confusing pair.","With the proposed label correction method, we can train our model with more accurate labels.","Extensive experiments confirm the effectiveness of our method and demonstrate its favorable performance compared with existing state-of-the-art methods.","Our codes are publicly available at https://github.com/Hehxcf/CPC/."],"url":"http://arxiv.org/abs/2403.12883v1","category":"cs.CV"}
{"created":"2024-03-19 16:09:30","title":"PE-Planner: A Performance-Enhanced Quadrotor Motion Planner for Autonomous Flight in Complex and Dynamic Environments","abstract":"The role of a motion planner is pivotal in quadrotor applications, yet existing methods often struggle to adapt to complex environments, limiting their ability to achieve fast, safe, and robust flight. In this letter, we introduce a performance-enhanced quadrotor motion planner designed for autonomous flight in complex environments including dense obstacles, dynamic obstacles, and unknown disturbances. The global planner generates an initial trajectory through kinodynamic path searching and refines it using B-spline trajectory optimization. Subsequently, the local planner takes into account the quadrotor dynamics, estimated disturbance, global reference trajectory, control cost, time cost, and safety constraints to generate real-time control inputs, utilizing the framework of model predictive contouring control. Both simulations and real-world experiments corroborate the heightened robustness, safety, and speed of the proposed motion planner. Additionally, our motion planner achieves flights at more than 6.8 m/s in a challenging and complex racing scenario.","sentences":["The role of a motion planner is pivotal in quadrotor applications, yet existing methods often struggle to adapt to complex environments, limiting their ability to achieve fast, safe, and robust flight.","In this letter, we introduce a performance-enhanced quadrotor motion planner designed for autonomous flight in complex environments including dense obstacles, dynamic obstacles, and unknown disturbances.","The global planner generates an initial trajectory through kinodynamic path searching and refines it using B-spline trajectory optimization.","Subsequently, the local planner takes into account the quadrotor dynamics, estimated disturbance, global reference trajectory, control cost, time cost, and safety constraints to generate real-time control inputs, utilizing the framework of model predictive contouring control.","Both simulations and real-world experiments corroborate the heightened robustness, safety, and speed of the proposed motion planner.","Additionally, our motion planner achieves flights at more than 6.8 m/s in a challenging and complex racing scenario."],"url":"http://arxiv.org/abs/2403.12865v1","category":"cs.RO"}
{"created":"2024-03-19 15:57:32","title":"RASP: A Drone-based Reconfigurable Actuation and Sensing Platform Towards Ambient Intelligent Systems","abstract":"Realizing consumer-grade drones that are as useful as robot vacuums throughout our homes or personal smartphones in our daily lives requires drones to sense, actuate, and respond to general scenarios that may arise. Towards this vision, we propose RASP, a modular and reconfigurable sensing and actuation platform that allows drones to autonomously swap onboard sensors and actuators in only 25 seconds, allowing a single drone to quickly adapt to a diverse range of tasks. RASP consists of a mechanical layer to physically swap sensor modules, an electrical layer to maintain power and communication lines to the sensor/actuator, and a software layer to maintain a common interface between the drone and any sensor module in our platform. Leveraging recent advances in large language and visual language models, we further introduce the architecture, implementation, and real-world deployments of a personal assistant system utilizing RASP. We demonstrate that RASP can enable a diverse range of useful tasks in home, office, lab, and other indoor settings.","sentences":["Realizing consumer-grade drones that are as useful as robot vacuums throughout our homes or personal smartphones in our daily lives requires drones to sense, actuate, and respond to general scenarios that may arise.","Towards this vision, we propose RASP, a modular and reconfigurable sensing and actuation platform that allows drones to autonomously swap onboard sensors and actuators in only 25 seconds, allowing a single drone to quickly adapt to a diverse range of tasks.","RASP consists of a mechanical layer to physically swap sensor modules, an electrical layer to maintain power and communication lines to the sensor/actuator, and a software layer to maintain a common interface between the drone and any sensor module in our platform.","Leveraging recent advances in large language and visual language models, we further introduce the architecture, implementation, and real-world deployments of a personal assistant system utilizing RASP.","We demonstrate that RASP can enable a diverse range of useful tasks in home, office, lab, and other indoor settings."],"url":"http://arxiv.org/abs/2403.12853v1","category":"cs.RO"}
{"created":"2024-03-19 15:45:54","title":"Global-guided Focal Neural Radiance Field for Large-scale Scene Rendering","abstract":"Neural radiance fields~(NeRF) have recently been applied to render large-scale scenes. However, their limited model capacity typically results in blurred rendering results. Existing large-scale NeRFs primarily address this limitation by partitioning the scene into blocks, which are subsequently handled by separate sub-NeRFs. These sub-NeRFs, trained from scratch and processed independently, lead to inconsistencies in geometry and appearance across the scene. Consequently, the rendering quality fails to exhibit significant improvement despite the expansion of model capacity. In this work, we present global-guided focal neural radiance field (GF-NeRF) that achieves high-fidelity rendering of large-scale scenes. Our proposed GF-NeRF utilizes a two-stage (Global and Focal) architecture and a global-guided training strategy. The global stage obtains a continuous representation of the entire scene while the focal stage decomposes the scene into multiple blocks and further processes them with distinct sub-encoders. Leveraging this two-stage architecture, sub-encoders only need fine-tuning based on the global encoder, thus reducing training complexity in the focal stage while maintaining scene-wide consistency. Spatial information and error information from the global stage also benefit the sub-encoders to focus on crucial areas and effectively capture more details of large-scale scenes. Notably, our approach does not rely on any prior knowledge about the target scene, attributing GF-NeRF adaptable to various large-scale scene types, including street-view and aerial-view scenes. We demonstrate that our method achieves high-fidelity, natural rendering results on various types of large-scale datasets. Our project page: https://shaomq2187.github.io/GF-NeRF/","sentences":["Neural radiance fields~(NeRF) have recently been applied to render large-scale scenes.","However, their limited model capacity typically results in blurred rendering results.","Existing large-scale NeRFs primarily address this limitation by partitioning the scene into blocks, which are subsequently handled by separate sub-NeRFs.","These sub-NeRFs, trained from scratch and processed independently, lead to inconsistencies in geometry and appearance across the scene.","Consequently, the rendering quality fails to exhibit significant improvement despite the expansion of model capacity.","In this work, we present global-guided focal neural radiance field (GF-NeRF) that achieves high-fidelity rendering of large-scale scenes.","Our proposed GF-NeRF utilizes a two-stage (Global and Focal) architecture and a global-guided training strategy.","The global stage obtains a continuous representation of the entire scene while the focal stage decomposes the scene into multiple blocks and further processes them with distinct sub-encoders.","Leveraging this two-stage architecture, sub-encoders only need fine-tuning based on the global encoder, thus reducing training complexity in the focal stage while maintaining scene-wide consistency.","Spatial information and error information from the global stage also benefit the sub-encoders to focus on crucial areas and effectively capture more details of large-scale scenes.","Notably, our approach does not rely on any prior knowledge about the target scene, attributing GF-NeRF adaptable to various large-scale scene types, including street-view and aerial-view scenes.","We demonstrate that our method achieves high-fidelity, natural rendering results on various types of large-scale datasets.","Our project page: https://shaomq2187.github.io/GF-NeRF/"],"url":"http://arxiv.org/abs/2403.12839v1","category":"cs.CV"}
{"created":"2024-03-19 15:41:39","title":"AnySkill: Learning Open-Vocabulary Physical Skill for Interactive Agents","abstract":"Traditional approaches in physics-based motion generation, centered around imitation learning and reward shaping, often struggle to adapt to new scenarios. To tackle this limitation, we propose AnySkill, a novel hierarchical method that learns physically plausible interactions following open-vocabulary instructions. Our approach begins by developing a set of atomic actions via a low-level controller trained via imitation learning. Upon receiving an open-vocabulary textual instruction, AnySkill employs a high-level policy that selects and integrates these atomic actions to maximize the CLIP similarity between the agent's rendered images and the text. An important feature of our method is the use of image-based rewards for the high-level policy, which allows the agent to learn interactions with objects without manual reward engineering. We demonstrate AnySkill's capability to generate realistic and natural motion sequences in response to unseen instructions of varying lengths, marking it the first method capable of open-vocabulary physical skill learning for interactive humanoid agents.","sentences":["Traditional approaches in physics-based motion generation, centered around imitation learning and reward shaping, often struggle to adapt to new scenarios.","To tackle this limitation, we propose AnySkill, a novel hierarchical method that learns physically plausible interactions following open-vocabulary instructions.","Our approach begins by developing a set of atomic actions via a low-level controller trained via imitation learning.","Upon receiving an open-vocabulary textual instruction, AnySkill employs a high-level policy that selects and integrates these atomic actions to maximize the CLIP similarity between the agent's rendered images and the text.","An important feature of our method is the use of image-based rewards for the high-level policy, which allows the agent to learn interactions with objects without manual reward engineering.","We demonstrate AnySkill's capability to generate realistic and natural motion sequences in response to unseen instructions of varying lengths, marking it the first method capable of open-vocabulary physical skill learning for interactive humanoid agents."],"url":"http://arxiv.org/abs/2403.12835v1","category":"cs.CV"}
{"created":"2024-03-19 15:17:23","title":"Dynamic Survival Analysis for Early Event Prediction","abstract":"This study advances Early Event Prediction (EEP) in healthcare through Dynamic Survival Analysis (DSA), offering a novel approach by integrating risk localization into alarm policies to enhance clinical event metrics. By adapting and evaluating DSA models against traditional EEP benchmarks, our research demonstrates their ability to match EEP models on a time-step level and significantly improve event-level metrics through a new alarm prioritization scheme (up to 11% AuPRC difference). This approach represents a significant step forward in predictive healthcare, providing a more nuanced and actionable framework for early event prediction and management.","sentences":["This study advances Early Event Prediction (EEP) in healthcare through Dynamic Survival Analysis (DSA), offering a novel approach by integrating risk localization into alarm policies to enhance clinical event metrics.","By adapting and evaluating DSA models against traditional EEP benchmarks, our research demonstrates their ability to match EEP models on a time-step level and significantly improve event-level metrics through a new alarm prioritization scheme (up to 11% AuPRC difference).","This approach represents a significant step forward in predictive healthcare, providing a more nuanced and actionable framework for early event prediction and management."],"url":"http://arxiv.org/abs/2403.12818v1","category":"cs.LG"}
{"created":"2024-03-19 15:07:08","title":"VisualCritic: Making LMMs Perceive Visual Quality Like Humans","abstract":"At present, large multimodal models (LMMs) have exhibited impressive generalization capabilities in understanding and generating visual signals. However, they currently still lack sufficient capability to perceive low-level visual quality akin to human perception. Can LMMs achieve this and show the same degree of generalization in this regard? If so, not only could the versatility of LMMs be further enhanced, but also the challenge of poor cross-dataset performance in the field of visual quality assessment could be addressed. In this paper, we explore this question and provide the answer \"Yes!\". As the result of this initial exploration, we present VisualCritic, the first LMM for broad-spectrum image subjective quality assessment. VisualCritic can be used across diverse data right out of box, without any requirements of dataset-specific adaptation operations like conventional specialist models. As an instruction-following LMM, VisualCritic enables new capabilities of (1) quantitatively measuring the perceptual quality of given images in terms of their Mean Opinion Score (MOS), noisiness, colorfulness, sharpness, and other numerical indicators, (2) qualitatively evaluating visual quality and providing explainable descriptions, (3) discerning whether a given image is AI-generated or photographic. Extensive experiments demonstrate the efficacy of VisualCritic by comparing it with other open-source LMMs and conventional specialist models over both AI-generated and photographic images.","sentences":["At present, large multimodal models (LMMs) have exhibited impressive generalization capabilities in understanding and generating visual signals.","However, they currently still lack sufficient capability to perceive low-level visual quality akin to human perception.","Can LMMs achieve this and show the same degree of generalization in this regard?","If so, not only could the versatility of LMMs be further enhanced, but also the challenge of poor cross-dataset performance in the field of visual quality assessment could be addressed.","In this paper, we explore this question and provide the answer \"Yes!\".","As the result of this initial exploration, we present VisualCritic, the first LMM for broad-spectrum image subjective quality assessment.","VisualCritic can be used across diverse data right out of box, without any requirements of dataset-specific adaptation operations like conventional specialist models.","As an instruction-following LMM, VisualCritic enables new capabilities of (1) quantitatively measuring the perceptual quality of given images in terms of their Mean Opinion Score (MOS), noisiness, colorfulness, sharpness, and other numerical indicators, (2) qualitatively evaluating visual quality and providing explainable descriptions, (3) discerning whether a given image is AI-generated or photographic.","Extensive experiments demonstrate the efficacy of VisualCritic by comparing it with other open-source LMMs and conventional specialist models over both AI-generated and photographic images."],"url":"http://arxiv.org/abs/2403.12806v1","category":"cs.CV"}
{"created":"2024-03-19 15:06:53","title":"Contextual Moral Value Alignment Through Context-Based Aggregation","abstract":"Developing value-aligned AI agents is a complex undertaking and an ongoing challenge in the field of AI. Specifically within the domain of Large Language Models (LLMs), the capability to consolidate multiple independently trained dialogue agents, each aligned with a distinct moral value, into a unified system that can adapt to and be aligned with multiple moral values is of paramount importance. In this paper, we propose a system that does contextual moral value alignment based on contextual aggregation. Here, aggregation is defined as the process of integrating a subset of LLM responses that are best suited to respond to a user input, taking into account features extracted from the user's input. The proposed system shows better results in term of alignment to human value compared to the state of the art.","sentences":["Developing value-aligned AI agents is a complex undertaking and an ongoing challenge in the field of AI.","Specifically within the domain of Large Language Models (LLMs), the capability to consolidate multiple independently trained dialogue agents, each aligned with a distinct moral value, into a unified system that can adapt to and be aligned with multiple moral values is of paramount importance.","In this paper, we propose a system that does contextual moral value alignment based on contextual aggregation.","Here, aggregation is defined as the process of integrating a subset of LLM responses that are best suited to respond to a user input, taking into account features extracted from the user's input.","The proposed system shows better results in term of alignment to human value compared to the state of the art."],"url":"http://arxiv.org/abs/2403.12805v1","category":"cs.AI"}
{"created":"2024-03-19 14:52:51","title":"Bivariate temporal dependence via mixtures of rotated copulas","abstract":"Parametric bivariate copula families have been known to flexibly capture enough various dependence patterns, e.g., either positive or negative dependence in either the lower or upper tails of bivariate distributions. However, to the best of our knowledge, there is not a single parametric model adaptable enough to capture several of these features simultaneously. To address this, we propose a mixture of 4-way rotations of a parametric copula that is able to capture all these features. We illustrate the construction using the Clayton family but the concept is general and can be applied to other families. In order to include dynamic dependence regimes, the approach is extended to a time-dependent sequence of mixture copulas in which the mixture probabilities are allowed to evolve in time via a moving average type of relationship. The properties of the proposed model and its performance are examined using simulated and real data sets.","sentences":["Parametric bivariate copula families have been known to flexibly capture enough various dependence patterns, e.g., either positive or negative dependence in either the lower or upper tails of bivariate distributions.","However, to the best of our knowledge, there is not a single parametric model adaptable enough to capture several of these features simultaneously.","To address this, we propose a mixture of 4-way rotations of a parametric copula that is able to capture all these features.","We illustrate the construction using the Clayton family but the concept is general and can be applied to other families.","In order to include dynamic dependence regimes, the approach is extended to a time-dependent sequence of mixture copulas in which the mixture probabilities are allowed to evolve in time via a moving average type of relationship.","The properties of the proposed model and its performance are examined using simulated and real data sets."],"url":"http://arxiv.org/abs/2403.12789v1","category":"stat.ME"}
{"created":"2024-03-19 14:34:44","title":"Multispectral Image Restoration by Generalized Opponent Transformation Total Variation","abstract":"Multispectral images (MSI) contain light information in different wavelengths of objects, which convey spectral-spatial information and help improve the performance of various image processing tasks. Numerous techniques have been created to extend the application of total variation regularization in restoring multispectral images, for example, based on channel coupling and adaptive total variation regularization. The primary contribution of this paper is to propose and develop a new multispectral total variation regularization in a generalized opponent transformation domain instead of the original multispectral image domain. Here opponent transformations for multispectral images are generalized from a well-known opponent transformation for color images. We will explore the properties of generalized opponent transformation total variation (GOTTV) regularization and the corresponding optimization formula for multispectral image restoration. To evaluate the effectiveness of the new GOTTV method, we provide numerical examples that showcase its superior performance compared to existing multispectral image total variation methods, using criteria such as MPSNR and MSSIM.","sentences":["Multispectral images (MSI) contain light information in different wavelengths of objects, which convey spectral-spatial information and help improve the performance of various image processing tasks.","Numerous techniques have been created to extend the application of total variation regularization in restoring multispectral images, for example, based on channel coupling and adaptive total variation regularization.","The primary contribution of this paper is to propose and develop a new multispectral total variation regularization in a generalized opponent transformation domain instead of the original multispectral image domain.","Here opponent transformations for multispectral images are generalized from a well-known opponent transformation for color images.","We will explore the properties of generalized opponent transformation total variation (GOTTV) regularization and the corresponding optimization formula for multispectral image restoration.","To evaluate the effectiveness of the new GOTTV method, we provide numerical examples that showcase its superior performance compared to existing multispectral image total variation methods, using criteria such as MPSNR and MSSIM."],"url":"http://arxiv.org/abs/2403.12770v1","category":"cs.CV"}
{"created":"2024-03-19 14:30:56","title":"Neural Parameter Regression for Explicit Representations of PDE Solution Operators","abstract":"We introduce Neural Parameter Regression (NPR), a novel framework specifically developed for learning solution operators in Partial Differential Equations (PDEs). Tailored for operator learning, this approach surpasses traditional DeepONets (Lu et al., 2021) by employing Physics-Informed Neural Network (PINN, Raissi et al., 2019) techniques to regress Neural Network (NN) parameters. By parametrizing each solution based on specific initial conditions, it effectively approximates a mapping between function spaces. Our method enhances parameter efficiency by incorporating low-rank matrices, thereby boosting computational efficiency and scalability. The framework shows remarkable adaptability to new initial and boundary conditions, allowing for rapid fine-tuning and inference, even in cases of out-of-distribution examples.","sentences":["We introduce Neural Parameter Regression (NPR), a novel framework specifically developed for learning solution operators in Partial Differential Equations (PDEs).","Tailored for operator learning, this approach surpasses traditional DeepONets (Lu et al., 2021) by employing Physics-Informed Neural Network (PINN, Raissi et al., 2019) techniques to regress Neural Network (NN) parameters.","By parametrizing each solution based on specific initial conditions, it effectively approximates a mapping between function spaces.","Our method enhances parameter efficiency by incorporating low-rank matrices, thereby boosting computational efficiency and scalability.","The framework shows remarkable adaptability to new initial and boundary conditions, allowing for rapid fine-tuning and inference, even in cases of out-of-distribution examples."],"url":"http://arxiv.org/abs/2403.12764v1","category":"cs.LG"}
{"created":"2024-03-19 14:12:54","title":"Sebastian, Basti, Wastl?! Recognizing Named Entities in Bavarian Dialectal Data","abstract":"Named Entity Recognition (NER) is a fundamental task to extract key information from texts, but annotated resources are scarce for dialects. This paper introduces the first dialectal NER dataset for German, BarNER, with 161K tokens annotated on Bavarian Wikipedia articles (bar-wiki) and tweets (bar-tweet), using a schema adapted from German CoNLL 2006 and GermEval. The Bavarian dialect differs from standard German in lexical distribution, syntactic construction, and entity information. We conduct in-domain, cross-domain, sequential, and joint experiments on two Bavarian and three German corpora and present the first comprehensive NER results on Bavarian. Incorporating knowledge from the larger German NER (sub-)datasets notably improves on bar-wiki and moderately on bar-tweet. Inversely, training first on Bavarian contributes slightly to the seminal German CoNLL 2006 corpus. Moreover, with gold dialect labels on Bavarian tweets, we assess multi-task learning between five NER and two Bavarian-German dialect identification tasks and achieve NER SOTA on bar-wiki. We substantiate the necessity of our low-resource BarNER corpus and the importance of diversity in dialects, genres, and topics in enhancing model performance.","sentences":["Named Entity Recognition (NER) is a fundamental task to extract key information from texts, but annotated resources are scarce for dialects.","This paper introduces the first dialectal NER dataset for German, BarNER, with 161K tokens annotated on Bavarian Wikipedia articles (bar-wiki) and tweets (bar-tweet), using a schema adapted from German CoNLL 2006 and GermEval.","The Bavarian dialect differs from standard German in lexical distribution, syntactic construction, and entity information.","We conduct in-domain, cross-domain, sequential, and joint experiments on two Bavarian and three German corpora and present the first comprehensive NER results on Bavarian.","Incorporating knowledge from the larger German NER (sub-)datasets notably improves on bar-wiki and moderately on bar-tweet.","Inversely, training first on Bavarian contributes slightly to the seminal German CoNLL 2006 corpus.","Moreover, with gold dialect labels on Bavarian tweets, we assess multi-task learning between five NER and two Bavarian-German dialect identification tasks and achieve NER SOTA on bar-wiki.","We substantiate the necessity of our low-resource BarNER corpus and the importance of diversity in dialects, genres, and topics in enhancing model performance."],"url":"http://arxiv.org/abs/2403.12749v1","category":"cs.CL"}
{"created":"2024-03-19 13:52:33","title":"To blow-up or not to blow-up for a granular kinetic equation","abstract":"A simplified kinetic description of rapid granular media leads to a nonlocal Vlasov-type equation with a convolution integral operator that is of the same form as the continuity equations for aggregation-diffusion macroscopic dynamics. While the singular behavior of these nonlinear continuity equations is well studied in the literature, the extension to the corresponding granular kinetic equation is highly nontrivial. The main question is whether the singularity formed in velocity direction will be enhanced or mitigated by the shear in phase space due to free transport. We present a preliminary study through a meticulous numerical investigation and heuristic arguments. We have numerically developed a structure-preserving method with adaptive mesh refinement that can effectively capture potential blow-up behavior in the solution for granular kinetic equations. We have analytically constructed a finite-time blow-up infinite mass solution and discussed how this can provide insights into the finite mass scenario.","sentences":["A simplified kinetic description of rapid granular media leads to a nonlocal Vlasov-type equation with a convolution integral operator that is of the same form as the continuity equations for aggregation-diffusion macroscopic dynamics.","While the singular behavior of these nonlinear continuity equations is well studied in the literature, the extension to the corresponding granular kinetic equation is highly nontrivial.","The main question is whether the singularity formed in velocity direction will be enhanced or mitigated by the shear in phase space due to free transport.","We present a preliminary study through a meticulous numerical investigation and heuristic arguments.","We have numerically developed a structure-preserving method with adaptive mesh refinement that can effectively capture potential blow-up behavior in the solution for granular kinetic equations.","We have analytically constructed a finite-time blow-up infinite mass solution and discussed how this can provide insights into the finite mass scenario."],"url":"http://arxiv.org/abs/2403.12735v1","category":"math.NA"}
{"created":"2024-03-19 13:29:44","title":"Shared Autonomy via Variable Impedance Control and Virtual Potential Fields for Encoding Human Demonstration","abstract":"This article introduces a framework for complex human-robot collaboration tasks, such as the co-manufacturing of furniture. For these tasks, it is essential to encode tasks from human demonstration and reproduce these skills in a compliant and safe manner. Therefore, two key components are addressed in this work: motion generation and shared autonomy. We propose a motion generator based on a time-invariant potential field, capable of encoding wrench profiles, complex and closed-loop trajectories, and additionally incorporates obstacle avoidance. Additionally, the paper addresses shared autonomy (SA) which enables synergetic collaboration between human operators and robots by dynamically allocating authority. Variable impedance control (VIC) and force control are employed, where impedance and wrench are adapted based on the human-robot autonomy factor derived from interaction forces. System passivity is ensured by an energy-tank based task passivation strategy. The framework's efficacy is validated through simulations and an experimental study employing a Franka Emika Research 3 robot.","sentences":["This article introduces a framework for complex human-robot collaboration tasks, such as the co-manufacturing of furniture.","For these tasks, it is essential to encode tasks from human demonstration and reproduce these skills in a compliant and safe manner.","Therefore, two key components are addressed in this work: motion generation and shared autonomy.","We propose a motion generator based on a time-invariant potential field, capable of encoding wrench profiles, complex and closed-loop trajectories, and additionally incorporates obstacle avoidance.","Additionally, the paper addresses shared autonomy (SA) which enables synergetic collaboration between human operators and robots by dynamically allocating authority.","Variable impedance control (VIC) and force control are employed, where impedance and wrench are adapted based on the human-robot autonomy factor derived from interaction forces.","System passivity is ensured by an energy-tank based task passivation strategy.","The framework's efficacy is validated through simulations and an experimental study employing a Franka Emika Research 3 robot."],"url":"http://arxiv.org/abs/2403.12720v1","category":"cs.RO"}
{"created":"2024-03-19 13:27:38","title":"Emergence of dynamical networks in termites","abstract":"Termites form complex dynamical trail networks from simple individual rules when exploring their environment. To help identify those simple rules, we reconstructed trail networks from time-lapse images of roaming termites. We quantified the trails' frequentations over time and compared them to the ones obtained by a null model. Arena borders were preferred in both simulated and observed data. Yet, the amplification phenomenon was higher with real termites, underlining the role of pheromones.","sentences":["Termites form complex dynamical trail networks from simple individual rules when exploring their environment.","To help identify those simple rules, we reconstructed trail networks from time-lapse images of roaming termites.","We quantified the trails' frequentations over time and compared them to the ones obtained by a null model.","Arena borders were preferred in both simulated and observed data.","Yet, the amplification phenomenon was higher with real termites, underlining the role of pheromones."],"url":"http://arxiv.org/abs/2403.12718v1","category":"nlin.AO"}
{"created":"2024-03-19 13:19:41","title":"Addressing Source Scale Bias via Image Warping for Domain Adaptation","abstract":"In visual recognition, scale bias is a key challenge due to the imbalance of object and image size distribution inherent in real scene datasets. Conventional solutions involve injecting scale invariance priors, oversampling the dataset at different scales during training, or adjusting scale at inference. While these strategies mitigate scale bias to some extent, their ability to adapt across diverse datasets is limited. Besides, they increase computational load during training and latency during inference. In this work, we use adaptive attentional processing -- oversampling salient object regions by warping images in-place during training. Discovering that shifting the source scale distribution improves backbone features, we developed a instance-level warping guidance aimed at object region sampling to mitigate source scale bias in domain adaptation. Our approach improves adaptation across geographies, lighting and weather conditions, is agnostic to the task, domain adaptation algorithm, saliency guidance, and underlying model architecture. Highlights include +6.1 mAP50 for BDD100K Clear $\\rightarrow$ DENSE Foggy, +3.7 mAP50 for BDD100K Day $\\rightarrow$ Night, +3.0 mAP50 for BDD100K Clear $\\rightarrow$ Rainy, and +6.3 mIoU for Cityscapes $\\rightarrow$ ACDC. Our approach adds minimal memory during training and has no additional latency at inference time. Please see Appendix for more results and analysis.","sentences":["In visual recognition, scale bias is a key challenge due to the imbalance of object and image size distribution inherent in real scene datasets.","Conventional solutions involve injecting scale invariance priors, oversampling the dataset at different scales during training, or adjusting scale at inference.","While these strategies mitigate scale bias to some extent, their ability to adapt across diverse datasets is limited.","Besides, they increase computational load during training and latency during inference.","In this work, we use adaptive attentional processing -- oversampling salient object regions by warping images in-place during training.","Discovering that shifting the source scale distribution improves backbone features, we developed a instance-level warping guidance aimed at object region sampling to mitigate source scale bias in domain adaptation.","Our approach improves adaptation across geographies, lighting and weather conditions, is agnostic to the task, domain adaptation algorithm, saliency guidance, and underlying model architecture.","Highlights include +6.1 mAP50 for BDD100K Clear $\\rightarrow$ DENSE Foggy, +3.7 mAP50 for BDD100K Day $\\rightarrow$ Night, +3.0 mAP50 for BDD100K Clear $\\rightarrow$ Rainy, and +6.3 mIoU for Cityscapes $\\rightarrow$ ACDC.","Our approach adds minimal memory during training and has no additional latency at inference time.","Please see Appendix for more results and analysis."],"url":"http://arxiv.org/abs/2403.12712v1","category":"cs.CV"}
{"created":"2024-03-19 13:08:54","title":"AnimateDiff-Lightning: Cross-Model Diffusion Distillation","abstract":"We present AnimateDiff-Lightning for lightning-fast video generation. Our model uses progressive adversarial diffusion distillation to achieve new state-of-the-art in few-step video generation. We discuss our modifications to adapt it for the video modality. Furthermore, we propose to simultaneously distill the probability flow of multiple base diffusion models, resulting in a single distilled motion module with broader style compatibility. We are pleased to release our distilled AnimateDiff-Lightning model for the community's use.","sentences":["We present AnimateDiff-Lightning for lightning-fast video generation.","Our model uses progressive adversarial diffusion distillation to achieve new state-of-the-art in few-step video generation.","We discuss our modifications to adapt it for the video modality.","Furthermore, we propose to simultaneously distill the probability flow of multiple base diffusion models, resulting in a single distilled motion module with broader style compatibility.","We are pleased to release our distilled AnimateDiff-Lightning model for the community's use."],"url":"http://arxiv.org/abs/2403.12706v1","category":"cs.CV"}
{"created":"2024-03-19 13:01:58","title":"ReProbes: An Architecture for Reconfigurable and Adaptive Probes","abstract":"Modern distributed systems are highly dynamic and scalable, requiring monitoring solutions that can adapt to rapid changes. Monitoring systems that rely on external probes can only achieve adaptation through expensive operations such as deployment, undeployment, and reconfiguration. This poster paper introduces ReProbes, a class of adaptive monitoring probes that can handle rapid changes in data collection strategies. ReProbe offers controllable and configurable self-adaptive capabilities for data transmission, collection, and analysis methods. The resulting architecture can effectively enhance probe adaptability when qualitatively compared to state-of-the-art monitoring solutions.","sentences":["Modern distributed systems are highly dynamic and scalable, requiring monitoring solutions that can adapt to rapid changes.","Monitoring systems that rely on external probes can only achieve adaptation through expensive operations such as deployment, undeployment, and reconfiguration.","This poster paper introduces ReProbes, a class of adaptive monitoring probes that can handle rapid changes in data collection strategies.","ReProbe offers controllable and configurable self-adaptive capabilities for data transmission, collection, and analysis methods.","The resulting architecture can effectively enhance probe adaptability when qualitatively compared to state-of-the-art monitoring solutions."],"url":"http://arxiv.org/abs/2403.12703v1","category":"cs.SE"}
{"created":"2024-03-19 13:01:57","title":"Learning Cross-view Visual Geo-localization without Ground Truth","abstract":"Cross-View Geo-Localization (CVGL) involves determining the geographical location of a query image by matching it with a corresponding GPS-tagged reference image. Current state-of-the-art methods predominantly rely on training models with labeled paired images, incurring substantial annotation costs and training burdens. In this study, we investigate the adaptation of frozen models for CVGL without requiring ground truth pair labels. We observe that training on unlabeled cross-view images presents significant challenges, including the need to establish relationships within unlabeled data and reconcile view discrepancies between uncertain queries and references. To address these challenges, we propose a self-supervised learning framework to train a learnable adapter for a frozen Foundation Model (FM). This adapter is designed to map feature distributions from diverse views into a uniform space using unlabeled data exclusively. To establish relationships within unlabeled data, we introduce an Expectation-Maximization-based Pseudo-labeling module, which iteratively estimates associations between cross-view features and optimizes the adapter. To maintain the robustness of the FM's representation, we incorporate an information consistency module with a reconstruction loss, ensuring that adapted features retain strong discriminative ability across views. Experimental results demonstrate that our proposed method achieves significant improvements over vanilla FMs and competitive accuracy compared to supervised methods, while necessitating fewer training parameters and relying solely on unlabeled data. Evaluation of our adaptation for task-specific models further highlights its broad applicability.","sentences":["Cross-View Geo-Localization (CVGL) involves determining the geographical location of a query image by matching it with a corresponding GPS-tagged reference image.","Current state-of-the-art methods predominantly rely on training models with labeled paired images, incurring substantial annotation costs and training burdens.","In this study, we investigate the adaptation of frozen models for CVGL without requiring ground truth pair labels.","We observe that training on unlabeled cross-view images presents significant challenges, including the need to establish relationships within unlabeled data and reconcile view discrepancies between uncertain queries and references.","To address these challenges, we propose a self-supervised learning framework to train a learnable adapter for a frozen Foundation Model (FM).","This adapter is designed to map feature distributions from diverse views into a uniform space using unlabeled data exclusively.","To establish relationships within unlabeled data, we introduce an Expectation-Maximization-based Pseudo-labeling module, which iteratively estimates associations between cross-view features and optimizes the adapter.","To maintain the robustness of the FM's representation, we incorporate an information consistency module with a reconstruction loss, ensuring that adapted features retain strong discriminative ability across views.","Experimental results demonstrate that our proposed method achieves significant improvements over vanilla FMs and competitive accuracy compared to supervised methods, while necessitating fewer training parameters and relying solely on unlabeled data.","Evaluation of our adaptation for task-specific models further highlights its broad applicability."],"url":"http://arxiv.org/abs/2403.12702v1","category":"cs.CV"}
{"created":"2024-03-19 12:56:02","title":"System Support for Environmentally Sustainable Computing in Data Centers","abstract":"Modern data centers suffer from a growing carbon footprint due to insufficient support for environmental sustainability. While hardware accelerators and renewable energy have been utilized to enhance sustainability, addressing Quality of Service (QoS) degradation caused by renewable energy supply and hardware recycling remains challenging: (1) prior accelerators exhibit significant carbon footprints due to limited reconfigurability and inability to adapt to renewable energy fluctuations; (2) integrating recycled NAND flash chips in data centers poses challenges due to their short lifetime, increasing energy consumption; (3) the absence of a sustainability estimator impedes data centers and users in evaluating and improving their environmental impact. This study aims to improve system support for environmentally sustainable data centers by proposing a reconfigurable hardware accelerator for intensive computing primitives and developing a fractional NAND flash cell to extend the lifetime of recycled flash chips while supporting graceful capacity degradation. We also introduce a sustainability estimator to evaluate user task energy consumption and promote sustainable practices. We present our preliminary results and recognize this as an ongoing initiative with significant potential to advance environmentally sustainable computing in data centers and stimulate further exploration in this critical research domain.","sentences":["Modern data centers suffer from a growing carbon footprint due to insufficient support for environmental sustainability.","While hardware accelerators and renewable energy have been utilized to enhance sustainability, addressing Quality of Service (QoS) degradation caused by renewable energy supply and hardware recycling remains challenging: (1) prior accelerators exhibit significant carbon footprints due to limited reconfigurability and inability to adapt to renewable energy fluctuations; (2) integrating recycled NAND flash chips in data centers poses challenges due to their short lifetime, increasing energy consumption; (3) the absence of a sustainability estimator impedes data centers and users in evaluating and improving their environmental impact.","This study aims to improve system support for environmentally sustainable data centers by proposing a reconfigurable hardware accelerator for intensive computing primitives and developing a fractional NAND flash cell to extend the lifetime of recycled flash chips while supporting graceful capacity degradation.","We also introduce a sustainability estimator to evaluate user task energy consumption and promote sustainable practices.","We present our preliminary results and recognize this as an ongoing initiative with significant potential to advance environmentally sustainable computing in data centers and stimulate further exploration in this critical research domain."],"url":"http://arxiv.org/abs/2403.12698v1","category":"cs.AR"}
{"created":"2024-03-19 12:49:25","title":"Efficient thermalization and universal quantum computing with quantum Gibbs samplers","abstract":"The preparation of thermal states of matter is a crucial task in quantum simulation. In this work, we prove that a recently introduced, efficiently implementable dissipative evolution thermalizes to the Gibbs state in time scaling polynomially with system size at high enough temperatures, and for any Hamiltonian that satisfies a Lieb-Robinson bound, such as local Hamiltonians on a lattice. Furthermore, we show the efficient adiabatic preparation of the associated purifications or ``thermofield double'' states. To the best of our knowledge, these are the first results rigorously establishing the efficient preparation of high-temperature Gibbs states and their purifications. In the low-temperature regime, we show that implementing this family of dissipative evolutions for inverse temperatures logarithmic in the system's size is polynomially equivalent to standard quantum computation. On a technical level, for high temperatures, our proof makes use of the mapping of the generator of the evolution into a Hamiltonian, and then analysing it as perturbation of the Hamiltonian corresponding to infinite temperature. For low temperature, we instead perform a perturbation at zero temperature of the Laplace transform of the energy observable at fixed runtime, and resort to circuit-to-Hamiltonian mappings akin to the proof of universality of quantum adiabatic computing. Taken together, our results show that a family of quasi-local dissipative evolutions efficiently prepares a large class of quantum many-body states of interest, and has the potential to mirror the success of classical Monte Carlo methods for quantum many-body systems.","sentences":["The preparation of thermal states of matter is a crucial task in quantum simulation.","In this work, we prove that a recently introduced, efficiently implementable dissipative evolution thermalizes to the Gibbs state in time scaling polynomially with system size at high enough temperatures, and for any Hamiltonian that satisfies a Lieb-Robinson bound, such as local Hamiltonians on a lattice.","Furthermore, we show the efficient adiabatic preparation of the associated purifications or ``thermofield double'' states.","To the best of our knowledge, these are the first results rigorously establishing the efficient preparation of high-temperature Gibbs states and their purifications.","In the low-temperature regime, we show that implementing this family of dissipative evolutions for inverse temperatures logarithmic in the system's size is polynomially equivalent to standard quantum computation.","On a technical level, for high temperatures, our proof makes use of the mapping of the generator of the evolution into a Hamiltonian, and then analysing it as perturbation of the Hamiltonian corresponding to infinite temperature.","For low temperature, we instead perform a perturbation at zero temperature of the Laplace transform of the energy observable at fixed runtime, and resort to circuit-to-Hamiltonian mappings akin to the proof of universality of quantum adiabatic computing.","Taken together, our results show that a family of quasi-local dissipative evolutions efficiently prepares a large class of quantum many-body states of interest, and has the potential to mirror the success of classical Monte Carlo methods for quantum many-body systems."],"url":"http://arxiv.org/abs/2403.12691v1","category":"quant-ph"}
{"created":"2024-03-19 12:47:55","title":"Stabilizing DG Methods Using Dafermos' Entropy Rate Criterion: III -- Unstructured Grids","abstract":"The approach presented in the second installment of this series is extended to multidimensional systems of conservation laws that are approximated via a Discontinuous Galerkin method on unstructured (triangular) grids. Special attention is paid to predicting the entropy dissipation from boundaries. The resulting schemes are free of tunable viscosity parameters and tested on the Euler equations. The trinity of testcases is the spreading of thermal energy from a point source, transsonic and supersonic flows around airfoils, and supersonic air inlets.","sentences":["The approach presented in the second installment of this series is extended to multidimensional systems of conservation laws that are approximated via a Discontinuous Galerkin method on unstructured (triangular) grids.","Special attention is paid to predicting the entropy dissipation from boundaries.","The resulting schemes are free of tunable viscosity parameters and tested on the Euler equations.","The trinity of testcases is the spreading of thermal energy from a point source, transsonic and supersonic flows around airfoils, and supersonic air inlets."],"url":"http://arxiv.org/abs/2403.12689v1","category":"math.NA"}
{"created":"2024-03-19 12:45:18","title":"WaterVG: Waterway Visual Grounding based on Text-Guided Vision and mmWave Radar","abstract":"The perception of waterways based on human intent holds significant importance for autonomous navigation and operations of Unmanned Surface Vehicles (USVs) in water environments. Inspired by visual grounding, in this paper, we introduce WaterVG, the first visual grounding dataset designed for USV-based waterway perception based on human intention prompts. WaterVG encompasses prompts describing multiple targets, with annotations at the instance level including bounding boxes and masks. Notably, WaterVG includes 11,568 samples with 34,950 referred targets, which integrates both visual and radar characteristics captured by monocular camera and millimeter-wave (mmWave) radar, enabling a finer granularity of text prompts. Furthermore, we propose a novel multi-modal visual grounding model, Potamoi, which is a multi-modal and multi-task model based on the one-stage paradigm with a designed Phased Heterogeneous Modality Fusion (PHMF) structure, including Adaptive Radar Weighting (ARW) and Multi-Head Slim Cross Attention (MHSCA). In specific, MHSCA is a low-cost and efficient fusion module with a remarkably small parameter count and FLOPs, elegantly aligning and fusing scenario context information captured by two sensors with linguistic features, which can effectively address tasks of referring expression comprehension and segmentation based on fine-grained prompts. Comprehensive experiments and evaluations have been conducted on WaterVG, where our Potamoi archives state-of-the-art performances compared with counterparts.","sentences":["The perception of waterways based on human intent holds significant importance for autonomous navigation and operations of Unmanned Surface Vehicles (USVs) in water environments.","Inspired by visual grounding, in this paper, we introduce WaterVG, the first visual grounding dataset designed for USV-based waterway perception based on human intention prompts.","WaterVG encompasses prompts describing multiple targets, with annotations at the instance level including bounding boxes and masks.","Notably, WaterVG includes 11,568 samples with 34,950 referred targets, which integrates both visual and radar characteristics captured by monocular camera and millimeter-wave (mmWave) radar, enabling a finer granularity of text prompts.","Furthermore, we propose a novel multi-modal visual grounding model, Potamoi, which is a multi-modal and multi-task model based on the one-stage paradigm with a designed Phased Heterogeneous Modality Fusion (PHMF) structure, including Adaptive Radar Weighting (ARW) and Multi-Head Slim Cross Attention (MHSCA).","In specific, MHSCA is a low-cost and efficient fusion module with a remarkably small parameter count and FLOPs, elegantly aligning and fusing scenario context information captured by two sensors with linguistic features, which can effectively address tasks of referring expression comprehension and segmentation based on fine-grained prompts.","Comprehensive experiments and evaluations have been conducted on WaterVG, where our Potamoi archives state-of-the-art performances compared with counterparts."],"url":"http://arxiv.org/abs/2403.12686v1","category":"cs.CV"}
{"created":"2024-03-19 12:45:00","title":"Dynamic Manipulation of Deformable Objects using Imitation Learning with Adaptation to Hardware Constraints","abstract":"Imitation Learning (IL) is a promising paradigm for learning dynamic manipulation of deformable objects since it does not depend on difficult-to-create accurate simulations of such objects. However, the translation of motions demonstrated by a human to a robot is a challenge for IL, due to differences in the embodiments and the robot's physical limits. These limits are especially relevant in dynamic manipulation where high velocities and accelerations are typical. To address this problem, we propose a framework that first maps a dynamic demonstration into a motion that respects the robot's constraints using a constrained Dynamic Movement Primitive. Second, the resulting object state is further optimized by quasi-static refinement motions to optimize task performance metrics. This allows both efficiently altering the object state by dynamic motions and stable small-scale refinements. We evaluate the framework in the challenging task of bag opening, designing the system BILBO: Bimanual dynamic manipulation using Imitation Learning for Bag Opening. Our results show that BILBO can successfully open a wide range of crumpled bags, using a demonstration with a single bag. See supplementary material at https://sites.google.com/view/bilbo-bag.","sentences":["Imitation Learning (IL) is a promising paradigm for learning dynamic manipulation of deformable objects since it does not depend on difficult-to-create accurate simulations of such objects.","However, the translation of motions demonstrated by a human to a robot is a challenge for IL, due to differences in the embodiments and the robot's physical limits.","These limits are especially relevant in dynamic manipulation where high velocities and accelerations are typical.","To address this problem, we propose a framework that first maps a dynamic demonstration into a motion that respects the robot's constraints using a constrained Dynamic Movement Primitive.","Second, the resulting object state is further optimized by quasi-static refinement motions to optimize task performance metrics.","This allows both efficiently altering the object state by dynamic motions and stable small-scale refinements.","We evaluate the framework in the challenging task of bag opening, designing the system BILBO: Bimanual dynamic manipulation using Imitation Learning for Bag Opening.","Our results show that BILBO can successfully open a wide range of crumpled bags, using a demonstration with a single bag.","See supplementary material at https://sites.google.com/view/bilbo-bag."],"url":"http://arxiv.org/abs/2403.12685v1","category":"cs.RO"}
{"created":"2024-03-19 12:39:37","title":"Concepts and methods for predicting viral evolution","abstract":"The seasonal human influenza virus undergoes rapid evolution, leading to significant changes in circulating viral strains from year to year. These changes are typically driven by adaptive mutations, particularly in the antigenic epitopes, the regions of the viral surface protein haemagglutinin targeted by human antibodies. Here we describe a consistent set of methods for data-driven predictive analysis of viral evolution. Our pipeline integrates four types of data: (1) sequence data of viral isolates collected on a worldwide scale, (2) epidemiological data on incidences, (3) antigenic characterization of circulating viruses, and (4) intrinsic viral phenotypes. From the combined analysis of these data, we obtain estimates of relative fitness for circulating strains and predictions of clade frequencies for periods of up to one year. Furthermore, we obtain comparative estimates of protection against future viral populations for candidate vaccine strains, providing a basis for pre-emptive vaccine strain selection. Continuously updated predictions obtained from the prediction pipeline for influenza and SARS-CoV-2 are available on the website https://previr.app.","sentences":["The seasonal human influenza virus undergoes rapid evolution, leading to significant changes in circulating viral strains from year to year.","These changes are typically driven by adaptive mutations, particularly in the antigenic epitopes, the regions of the viral surface protein haemagglutinin targeted by human antibodies.","Here we describe a consistent set of methods for data-driven predictive analysis of viral evolution.","Our pipeline integrates four types of data: (1) sequence data of viral isolates collected on a worldwide scale, (2) epidemiological data on incidences, (3) antigenic characterization of circulating viruses, and (4) intrinsic viral phenotypes.","From the combined analysis of these data, we obtain estimates of relative fitness for circulating strains and predictions of clade frequencies for periods of up to one year.","Furthermore, we obtain comparative estimates of protection against future viral populations for candidate vaccine strains, providing a basis for pre-emptive vaccine strain selection.","Continuously updated predictions obtained from the prediction pipeline for influenza and SARS-CoV-2 are available on the website https://previr.app."],"url":"http://arxiv.org/abs/2403.12684v1","category":"q-bio.PE"}
{"created":"2024-03-19 12:21:20","title":"Pragmatic Competence Evaluation of Large Language Models for Korean","abstract":"The current evaluation of Large Language Models (LLMs) predominantly relies on benchmarks focusing on their embedded knowledge by testing through multiple-choice questions (MCQs), a format inherently suited for automated evaluation. Our study extends this evaluation to explore LLMs' pragmatic competence--a facet previously underexamined before the advent of sophisticated LLMs, specifically in the context of Korean. We employ two distinct evaluation setups: the conventional MCQ format, adapted for automatic evaluation, and Open-Ended Questions (OEQs), assessed by human experts, to examine LLMs' narrative response capabilities without predefined options. Our findings reveal that GPT-4 excels, scoring 81.11 and 85.69 in the MCQ and OEQ setups, respectively, with HyperCLOVA X, an LLM optimized for Korean, closely following, especially in the OEQ setup, demonstrating a score of 81.56 with a marginal difference of 4.13 points compared to GPT-4. Furthermore, while few-shot learning strategies generally enhance LLM performance, Chain-of-Thought (CoT) prompting introduces a bias toward literal interpretations, hindering accurate pragmatic inference. Considering the growing expectation for LLMs to understand and produce language that aligns with human communicative norms, our findings emphasize the importance for advancing LLMs' abilities to grasp and convey sophisticated meanings beyond mere literal interpretations.","sentences":["The current evaluation of Large Language Models (LLMs) predominantly relies on benchmarks focusing on their embedded knowledge by testing through multiple-choice questions (MCQs), a format inherently suited for automated evaluation.","Our study extends this evaluation to explore LLMs' pragmatic competence--a facet previously underexamined before the advent of sophisticated LLMs, specifically in the context of Korean.","We employ two distinct evaluation setups: the conventional MCQ format, adapted for automatic evaluation, and Open-Ended Questions (OEQs), assessed by human experts, to examine LLMs' narrative response capabilities without predefined options.","Our findings reveal that GPT-4 excels, scoring 81.11 and 85.69 in the MCQ and OEQ setups, respectively, with HyperCLOVA X, an LLM optimized for Korean, closely following, especially in the OEQ setup, demonstrating a score of 81.56 with a marginal difference of 4.13 points compared to GPT-4.","Furthermore, while few-shot learning strategies generally enhance LLM performance, Chain-of-Thought (CoT) prompting introduces a bias toward literal interpretations, hindering accurate pragmatic inference.","Considering the growing expectation for LLMs to understand and produce language that aligns with human communicative norms, our findings emphasize the importance for advancing LLMs' abilities to grasp and convey sophisticated meanings beyond mere literal interpretations."],"url":"http://arxiv.org/abs/2403.12675v1","category":"cs.CL"}
{"created":"2024-03-19 11:34:40","title":"Adaptive Multilevel Neural Networks for Parametric PDEs with Error Estimation","abstract":"To solve high-dimensional parameter-dependent partial differential equations (pPDEs), a neural network architecture is presented. It is constructed to map parameters of the model data to corresponding finite element solutions. To improve training efficiency and to enable control of the approximation error, the network mimics an adaptive finite element method (AFEM). It outputs a coarse grid solution and a series of corrections as produced in an AFEM, allowing a tracking of the error decay over successive layers of the network. The observed errors are measured by a reliable residual based a posteriori error estimator, enabling the reduction to only few parameters for the approximation in the output of the network. This leads to a problem adapted representation of the solution on locally refined grids. Furthermore, each solution of the AFEM is discretized in a hierarchical basis. For the architecture, convolutional neural networks (CNNs) are chosen. The hierarchical basis then allows to handle sparse images for finely discretized meshes. Additionally, as corrections on finer levels decrease in amplitude, i.e., importance for the overall approximation, the accuracy of the network approximation is allowed to decrease successively. This can either be incorporated in the number of generated high fidelity samples used for training or the size of the network components responsible for the fine grid outputs. The architecture is described and preliminary numerical examples are presented.","sentences":["To solve high-dimensional parameter-dependent partial differential equations (pPDEs), a neural network architecture is presented.","It is constructed to map parameters of the model data to corresponding finite element solutions.","To improve training efficiency and to enable control of the approximation error, the network mimics an adaptive finite element method (AFEM).","It outputs a coarse grid solution and a series of corrections as produced in an AFEM, allowing a tracking of the error decay over successive layers of the network.","The observed errors are measured by a reliable residual based a posteriori error estimator, enabling the reduction to only few parameters for the approximation in the output of the network.","This leads to a problem adapted representation of the solution on locally refined grids.","Furthermore, each solution of the AFEM is discretized in a hierarchical basis.","For the architecture, convolutional neural networks (CNNs) are chosen.","The hierarchical basis then allows to handle sparse images for finely discretized meshes.","Additionally, as corrections on finer levels decrease in amplitude, i.e., importance for the overall approximation, the accuracy of the network approximation is allowed to decrease successively.","This can either be incorporated in the number of generated high fidelity samples used for training or the size of the network components responsible for the fine grid outputs.","The architecture is described and preliminary numerical examples are presented."],"url":"http://arxiv.org/abs/2403.12650v1","category":"math.NA"}
{"created":"2024-03-19 10:50:34","title":"Large-scale metric objects filtering for binary classification with application to abnormal brain connectivity detection","abstract":"The classification of random objects within metric spaces without a vector structure has attracted increasing attention. However, the complexity inherent in such non-Euclidean data often restricts existing models to handle only a limited number of features, leaving a gap in real-world applications. To address this, we propose a data-adaptive filtering procedure to identify informative features from large-scale random objects, leveraging a novel Kolmogorov-Smirnov-type statistic defined on the metric space. Our method, applicable to data in general metric spaces with binary labels, exhibits remarkable flexibility. It enjoys a model-free property, as its implementation does not rely on any specified classifier. Theoretically, it controls the false discovery rate while guaranteeing the sure screening property. Empirically, equipped with a Wasserstein metric, it demonstrates superior sample performance compared to Euclidean competitors. When applied to analyze a dataset on autism, our method identifies significant brain regions associated with the condition. Moreover, it reveals distinct interaction patterns among these regions between individuals with and without autism, achieved by filtering hundreds of thousands of covariance matrices representing various brain connectivities.","sentences":["The classification of random objects within metric spaces without a vector structure has attracted increasing attention.","However, the complexity inherent in such non-Euclidean data often restricts existing models to handle only a limited number of features, leaving a gap in real-world applications.","To address this, we propose a data-adaptive filtering procedure to identify informative features from large-scale random objects, leveraging a novel Kolmogorov-Smirnov-type statistic defined on the metric space.","Our method, applicable to data in general metric spaces with binary labels, exhibits remarkable flexibility.","It enjoys a model-free property, as its implementation does not rely on any specified classifier.","Theoretically, it controls the false discovery rate while guaranteeing the sure screening property.","Empirically, equipped with a Wasserstein metric, it demonstrates superior sample performance compared to Euclidean competitors.","When applied to analyze a dataset on autism, our method identifies significant brain regions associated with the condition.","Moreover, it reveals distinct interaction patterns among these regions between individuals with and without autism, achieved by filtering hundreds of thousands of covariance matrices representing various brain connectivities."],"url":"http://arxiv.org/abs/2403.12624v1","category":"stat.ME"}
{"created":"2024-03-19 10:31:12","title":"Surfactant-laden liquid thread breakup driven by thermal fluctuations","abstract":"The breakup of liquid threads into droplets is crucial in various applications, such as nanoprinting, nanomanufacturing, and inkjet printing, where a detailed understanding of the thinning neck dynamics allows for a precise droplet control. Here, the role of surfactant in the breakup process is studied by many-body dissipative particle dynamics, in particular, the various regime transitions and thread profiles, shedding light on molecular-level intricacies of this process hitherto inaccessible to continuum theory and experiments. Moreover, the role of surfactant in the most unstable perturbation, the formed droplet size, and surfactant distributions have been unraveled. As surfactant concentration rises, both the wavelength and time to breakup steadily increase due to the lowering of surface tension below the critical micelle concentration (CMC) and viscous effects introduced by micelles above the CMC. These changes prior to the breakup lead to larger droplets being formed in cases with higher surfactant concentration. We also compared the thinning dynamics to existing theoretical predictions, revealing that the surfactant-laden breakup starts at the inertial regime and transitions into the thermal fluctuation regime when the concentration is increased. Thus, we illuminate the hitherto poorly investigated and intricate breakup process of surfactant-laden liquid threads driven by thermal fluctuations, contributing to a deeper understanding of this process at molecular scales.","sentences":["The breakup of liquid threads into droplets is crucial in various applications, such as nanoprinting, nanomanufacturing, and inkjet printing, where a detailed understanding of the thinning neck dynamics allows for a precise droplet control.","Here, the role of surfactant in the breakup process is studied by many-body dissipative particle dynamics, in particular, the various regime transitions and thread profiles, shedding light on molecular-level intricacies of this process hitherto inaccessible to continuum theory and experiments.","Moreover, the role of surfactant in the most unstable perturbation, the formed droplet size, and surfactant distributions have been unraveled.","As surfactant concentration rises, both the wavelength and time to breakup steadily increase due to the lowering of surface tension below the critical micelle concentration (CMC) and viscous effects introduced by micelles above the CMC.","These changes prior to the breakup lead to larger droplets being formed in cases with higher surfactant concentration.","We also compared the thinning dynamics to existing theoretical predictions, revealing that the surfactant-laden breakup starts at the inertial regime and transitions into the thermal fluctuation regime when the concentration is increased.","Thus, we illuminate the hitherto poorly investigated and intricate breakup process of surfactant-laden liquid threads driven by thermal fluctuations, contributing to a deeper understanding of this process at molecular scales."],"url":"http://arxiv.org/abs/2403.12614v1","category":"physics.flu-dyn"}
{"created":"2024-03-19 09:51:20","title":"Simulation of the Wave Turbulence of a Liquid Surface Using the Dynamic Conformal Transformation Method","abstract":"The dynamic conformal transformation method has been generalized for the first time to numerically simulate the capillary wave turbulence of a liquid surface in the plane symmetric anisotropic geometry. The model is strongly nonlinear and involves effects of surface tension, as well as energy dissipation and pumping. Simulation results have shown that the system of nonlinear capillary waves can pass to the quasistationary chaotic motion regime (wave turbulence). The calculated exponents of spectra do not coincide with those for the classical Zakharov-Filonenko spectrum for isotropic capillary turbulence but are in good agreement with the estimate obtained under the assumption of the dominant effect of five-wave resonant interactions.","sentences":["The dynamic conformal transformation method has been generalized for the first time to numerically simulate the capillary wave turbulence of a liquid surface in the plane symmetric anisotropic geometry.","The model is strongly nonlinear and involves effects of surface tension, as well as energy dissipation and pumping.","Simulation results have shown that the system of nonlinear capillary waves can pass to the quasistationary chaotic motion regime (wave turbulence).","The calculated exponents of spectra do not coincide with those for the classical Zakharov-Filonenko spectrum for isotropic capillary turbulence but are in good agreement with the estimate obtained under the assumption of the dominant effect of five-wave resonant interactions."],"url":"http://arxiv.org/abs/2403.12592v1","category":"physics.flu-dyn"}
{"created":"2024-03-19 09:47:16","title":"An Adaptive feature mode decomposition based on a novel health indicator for bearing fault diagnosis","abstract":"The vibration analysis of the bearing is very crucial because of its non-stationary nature and low signal-to-noise ratio. Therefore, a novel scheme for detecting bearing defects is put forward based on the extraction of single-valued neutrosophic cross-entropy (SVNCE) to address this issue. Initially, the artificial hummingbird algorithm (AHA) is used to make the feature mode decomposition (FMD) adaptive by optimizing its parameter based on a newly developed health indicator (HI) i.e. sparsity impact measure index (SIMI). This HI ensures full sparsity and impact properties simultaneously. The raw signals are disintegrated into different modes by adaptive FMD at optimal values of its parameters. The energy of these modes is calculated for different health conditions. The energy interval range has been decided based on energy eigen which are then transformed into single-valued neutrosophic sets (SVNSs) for unknown defect conditions. The minimum argument principle employs the least SVNCE values between SVNSs of testing samples (obtained from unknown bearing conditions) and SVNSs of training samples (obtained from known bearing conditions) to recognize the different defects in the bearing. It has been discovered that the suggested methodology is more adept at identifying the various bearing defects.","sentences":["The vibration analysis of the bearing is very crucial because of its non-stationary nature and low signal-to-noise ratio.","Therefore, a novel scheme for detecting bearing defects is put forward based on the extraction of single-valued neutrosophic cross-entropy (SVNCE) to address this issue.","Initially, the artificial hummingbird algorithm (AHA) is used to make the feature mode decomposition (FMD) adaptive by optimizing its parameter based on a newly developed health indicator (HI) i.e. sparsity impact measure index (SIMI).","This HI ensures full sparsity and impact properties simultaneously.","The raw signals are disintegrated into different modes by adaptive FMD at optimal values of its parameters.","The energy of these modes is calculated for different health conditions.","The energy interval range has been decided based on energy eigen which are then transformed into single-valued neutrosophic sets (SVNSs) for unknown defect conditions.","The minimum argument principle employs the least SVNCE values between SVNSs of testing samples (obtained from unknown bearing conditions) and SVNSs of training samples (obtained from known bearing conditions) to recognize the different defects in the bearing.","It has been discovered that the suggested methodology is more adept at identifying the various bearing defects."],"url":"http://arxiv.org/abs/2403.12586v1","category":"eess.SP"}
{"created":"2024-03-19 09:36:08","title":"Rate-optimal higher-order adaptive conforming FEM for biharmonic eigenvalue problems on polygonal domains","abstract":"The a posteriori error analysis of the classical Argyris finite element methods dates back to 1996, while the optimal convergence rates of associated adaptive finite element schemes are established only very recently in 2021. It took a long time to realise the necessity of an extension of the classical finite element spaces to make them hierarchical. This paper establishes the novel adaptive schemes for the biharmonic eigenvalue problems and provides a mathematical proof of optimal convergence rates towards a simple eigenvalue and numerical evidence thereof. This makes the suggested algorithm highly competitive and clearly justifies the higher computational and implementational costs compared to low-order nonconforming schemes. The numerical experiments provide overwhelming evidence that higher polynomial degrees pay off with higher convergence rates and underline that adaptive mesh-refining is mandatory. Five computational benchmarks display accurate reference eigenvalues up to 30 digits.","sentences":["The a posteriori error analysis of the classical Argyris finite element methods dates back to 1996, while the optimal convergence rates of associated adaptive finite element schemes are established only very recently in 2021.","It took a long time to realise the necessity of an extension of the classical finite element spaces to make them hierarchical.","This paper establishes the novel adaptive schemes for the biharmonic eigenvalue problems and provides a mathematical proof of optimal convergence rates towards a simple eigenvalue and numerical evidence thereof.","This makes the suggested algorithm highly competitive and clearly justifies the higher computational and implementational costs compared to low-order nonconforming schemes.","The numerical experiments provide overwhelming evidence that higher polynomial degrees pay off with higher convergence rates and underline that adaptive mesh-refining is mandatory.","Five computational benchmarks display accurate reference eigenvalues up to 30 digits."],"url":"http://arxiv.org/abs/2403.12577v1","category":"math.NA"}
{"created":"2024-03-19 09:34:11","title":"EAS-SNN: End-to-End Adaptive Sampling and Representation for Event-based Detection with Recurrent Spiking Neural Networks","abstract":"Event cameras, with their high dynamic range and temporal resolution, are ideally suited for object detection, especially under scenarios with motion blur and challenging lighting conditions. However, while most existing approaches prioritize optimizing spatiotemporal representations with advanced detection backbones and early aggregation functions, the crucial issue of adaptive event sampling remains largely unaddressed. Spiking Neural Networks (SNNs), which operate on an event-driven paradigm through sparse spike communication, emerge as a natural fit for addressing this challenge. In this study, we discover that the neural dynamics of spiking neurons align closely with the behavior of an ideal temporal event sampler. Motivated by this insight, we propose a novel adaptive sampling module that leverages recurrent convolutional SNNs enhanced with temporal memory, facilitating a fully end-to-end learnable framework for event-based detection. Additionally, we introduce Residual Potential Dropout (RPD) and Spike-Aware Training (SAT) to regulate potential distribution and address performance degradation encountered in spike-based sampling modules. Through rigorous testing on neuromorphic datasets for event-based detection, our approach demonstrably surpasses existing state-of-the-art spike-based methods, achieving superior performance with significantly fewer parameters and time steps. For instance, our method achieves a 4.4\\% mAP improvement on the Gen1 dataset, while requiring 38\\% fewer parameters and three time steps. Moreover, the applicability and effectiveness of our adaptive sampling methodology extend beyond SNNs, as demonstrated through further validation on conventional non-spiking detection models.","sentences":["Event cameras, with their high dynamic range and temporal resolution, are ideally suited for object detection, especially under scenarios with motion blur and challenging lighting conditions.","However, while most existing approaches prioritize optimizing spatiotemporal representations with advanced detection backbones and early aggregation functions, the crucial issue of adaptive event sampling remains largely unaddressed.","Spiking Neural Networks (SNNs), which operate on an event-driven paradigm through sparse spike communication, emerge as a natural fit for addressing this challenge.","In this study, we discover that the neural dynamics of spiking neurons align closely with the behavior of an ideal temporal event sampler.","Motivated by this insight, we propose a novel adaptive sampling module that leverages recurrent convolutional SNNs enhanced with temporal memory, facilitating a fully end-to-end learnable framework for event-based detection.","Additionally, we introduce Residual Potential Dropout (RPD) and Spike-Aware Training (SAT) to regulate potential distribution and address performance degradation encountered in spike-based sampling modules.","Through rigorous testing on neuromorphic datasets for event-based detection, our approach demonstrably surpasses existing state-of-the-art spike-based methods, achieving superior performance with significantly fewer parameters and time steps.","For instance, our method achieves a 4.4\\% mAP improvement on the Gen1 dataset, while requiring 38\\% fewer parameters and three time steps.","Moreover, the applicability and effectiveness of our adaptive sampling methodology extend beyond SNNs, as demonstrated through further validation on conventional non-spiking detection models."],"url":"http://arxiv.org/abs/2403.12574v1","category":"cs.CV"}
{"created":"2024-03-19 09:28:19","title":"Adapting Visual-Language Models for Generalizable Anomaly Detection in Medical Images","abstract":"Recent advancements in large-scale visual-language pre-trained models have led to significant progress in zero-/few-shot anomaly detection within natural image domains. However, the substantial domain divergence between natural and medical images limits the effectiveness of these methodologies in medical anomaly detection. This paper introduces a novel lightweight multi-level adaptation and comparison framework to repurpose the CLIP model for medical anomaly detection. Our approach integrates multiple residual adapters into the pre-trained visual encoder, enabling a stepwise enhancement of visual features across different levels. This multi-level adaptation is guided by multi-level, pixel-wise visual-language feature alignment loss functions, which recalibrate the model's focus from object semantics in natural imagery to anomaly identification in medical images. The adapted features exhibit improved generalization across various medical data types, even in zero-shot scenarios where the model encounters unseen medical modalities and anatomical regions during training. Our experiments on medical anomaly detection benchmarks demonstrate that our method significantly surpasses current state-of-the-art models, with an average AUC improvement of 6.24% and 7.33% for anomaly classification, 2.03% and 2.37% for anomaly segmentation, under the zero-shot and few-shot settings, respectively. Source code is available at: https://github.com/MediaBrain-SJTU/MVFA-AD","sentences":["Recent advancements in large-scale visual-language pre-trained models have led to significant progress in zero-/few-shot anomaly detection within natural image domains.","However, the substantial domain divergence between natural and medical images limits the effectiveness of these methodologies in medical anomaly detection.","This paper introduces a novel lightweight multi-level adaptation and comparison framework to repurpose the CLIP model for medical anomaly detection.","Our approach integrates multiple residual adapters into the pre-trained visual encoder, enabling a stepwise enhancement of visual features across different levels.","This multi-level adaptation is guided by multi-level, pixel-wise visual-language feature alignment loss functions, which recalibrate the model's focus from object semantics in natural imagery to anomaly identification in medical images.","The adapted features exhibit improved generalization across various medical data types, even in zero-shot scenarios where the model encounters unseen medical modalities and anatomical regions during training.","Our experiments on medical anomaly detection benchmarks demonstrate that our method significantly surpasses current state-of-the-art models, with an average AUC improvement of 6.24% and 7.33% for anomaly classification, 2.03% and 2.37% for anomaly segmentation, under the zero-shot and few-shot settings, respectively.","Source code is available at: https://github.com/MediaBrain-SJTU/MVFA-AD"],"url":"http://arxiv.org/abs/2403.12570v1","category":"cs.CV"}
{"created":"2024-03-19 08:48:09","title":"Kinetically constrained models constructed from dissipative quantum dynamics","abstract":"We propose a construction of kinetically constrained models using the Markovian quantum dynamics under strong dissipation. Using the Gorini-Kossakowski-Sudarshan-Lindblad (GKSL) formalism, we show that strong dissipation leads to the emergent decoherence-free subspaces, within which constrained quantum many-body unitary dynamics can take place. We argue that the unitary dynamics constructed by the GKSL dynamics is more tightly constrained than that constructed by the strongly interacting Hamiltonian, where the interactions have the same form with the GKSL jump operators. As an example, we demonstrate that a one-dimensional spin system with two-site dissipation leads to the kinetically constrained ``PXQ\" model, which exhibits the free domain-wall motion with an additional frozen-block structure. Under the uniform magnetic field, the PXQ model shows the domain-wall localization, similar to the Wannier-Stark localization. We then couple two PXQ chains with the magnetic field and inter-chain interaction. We discover that, while localization of the domain walls persists despite the interactions for typical parameter regimes, a non-trivial partial delocalization appears for a certain parameter line.","sentences":["We propose a construction of kinetically constrained models using the Markovian quantum dynamics under strong dissipation.","Using the Gorini-Kossakowski-Sudarshan-Lindblad (GKSL) formalism, we show that strong dissipation leads to the emergent decoherence-free subspaces, within which constrained quantum many-body unitary dynamics can take place.","We argue that the unitary dynamics constructed by the GKSL dynamics is more tightly constrained than that constructed by the strongly interacting Hamiltonian, where the interactions have the same form with the GKSL jump operators.","As an example, we demonstrate that a one-dimensional spin system with two-site dissipation leads to the kinetically constrained ``PXQ\" model, which exhibits the free domain-wall motion with an additional frozen-block structure.","Under the uniform magnetic field, the PXQ model shows the domain-wall localization, similar to the Wannier-Stark localization.","We then couple two PXQ chains with the magnetic field and inter-chain interaction.","We discover that, while localization of the domain walls persists despite the interactions for typical parameter regimes, a non-trivial partial delocalization appears for a certain parameter line."],"url":"http://arxiv.org/abs/2403.12548v1","category":"quant-ph"}
{"created":"2024-03-19 08:37:57","title":"Attitude Tracking of Uncertain Flexible Spacecraft Systems Subject to Unknown External Disturbances","abstract":"In this paper, we investigate the attitude tracking problem of uncertain flexible spacecraft systems subject to external disturbances. In sharp contrast to existing results, the dynamics of flexible spacecraft systems and external disturbances are allowed to be unknown. To deal with the challenges by these unknown factors, we develop a class of nonlinear internal models which converts the attitude tracking problem of uncertain flexible spacecraft systems into a regulation problem of an augmented system. Furthermore, to overcome the difficulties caused by the unmeasurable modal variable, the uncertainty introduced by the internal model, and the cross-coupling of the uncertainties with the system state, we design an auxiliary dynamic system for auxiliary stabilization, a dynamic compensator for dynamic compensation, and a linearly parameterized transformation for adaptive regulation in sequence. By introducing a series of coordinate and input transformations, we propose an adaptive dynamic control law to achieve regulation of the augmented system and thus leading to the solution to the attitude tracking problem. In addition, we analyze the convergence issue of the estimated parameter to its true value by the persistently exciting condition. Finally, the effec tiveness of the developed approach is verified by its application to the attitude manoeuvre of a flexible spacecraft system in the presence of external disturbances.","sentences":["In this paper, we investigate the attitude tracking problem of uncertain flexible spacecraft systems subject to external disturbances.","In sharp contrast to existing results, the dynamics of flexible spacecraft systems and external disturbances are allowed to be unknown.","To deal with the challenges by these unknown factors, we develop a class of nonlinear internal models which converts the attitude tracking problem of uncertain flexible spacecraft systems into a regulation problem of an augmented system.","Furthermore, to overcome the difficulties caused by the unmeasurable modal variable, the uncertainty introduced by the internal model, and the cross-coupling of the uncertainties with the system state, we design an auxiliary dynamic system for auxiliary stabilization, a dynamic compensator for dynamic compensation, and a linearly parameterized transformation for adaptive regulation in sequence.","By introducing a series of coordinate and input transformations, we propose an adaptive dynamic control law to achieve regulation of the augmented system and thus leading to the solution to the attitude tracking problem.","In addition, we analyze the convergence issue of the estimated parameter to its true value by the persistently exciting condition.","Finally, the effec tiveness of the developed approach is verified by its application to the attitude manoeuvre of a flexible spacecraft system in the presence of external disturbances."],"url":"http://arxiv.org/abs/2403.12542v1","category":"math.OC"}
{"created":"2024-03-19 08:23:12","title":"Prompt-Guided Adaptive Model Transformation for Whole Slide Image Classification","abstract":"Multiple instance learning (MIL) has emerged as a popular method for classifying histopathology whole slide images (WSIs). Existing approaches typically rely on frozen pre-trained models to extract instance features, neglecting the substantial domain shift between pre-training natural and histopathological images. To address this issue, we propose PAMT, a novel Prompt-guided Adaptive Model Transformation framework that enhances MIL classification performance by seamlessly adapting pre-trained models to the specific characteristics of histopathology data. To capture the intricate histopathology distribution, we introduce Representative Patch Sampling (RPS) and Prototypical Visual Prompt (PVP) to reform the input data, building a compact while informative representation. Furthermore, to narrow the domain gap, we introduce Adaptive Model Transformation (AMT) that integrates adapter blocks within the feature extraction pipeline, enabling the pre-trained models to learn domain-specific features. We rigorously evaluate our approach on two publicly available datasets, Camelyon16 and TCGA-NSCLC, showcasing substantial improvements across various MIL models. Our findings affirm the potential of PAMT to set a new benchmark in WSI classification, underscoring the value of a targeted reprogramming approach.","sentences":["Multiple instance learning (MIL) has emerged as a popular method for classifying histopathology whole slide images (WSIs).","Existing approaches typically rely on frozen pre-trained models to extract instance features, neglecting the substantial domain shift between pre-training natural and histopathological images.","To address this issue, we propose PAMT, a novel Prompt-guided Adaptive Model Transformation framework that enhances MIL classification performance by seamlessly adapting pre-trained models to the specific characteristics of histopathology data.","To capture the intricate histopathology distribution, we introduce Representative Patch Sampling (RPS) and Prototypical Visual Prompt (PVP) to reform the input data, building a compact while informative representation.","Furthermore, to narrow the domain gap, we introduce Adaptive Model Transformation (AMT) that integrates adapter blocks within the feature extraction pipeline, enabling the pre-trained models to learn domain-specific features.","We rigorously evaluate our approach on two publicly available datasets, Camelyon16 and TCGA-NSCLC, showcasing substantial improvements across various MIL models.","Our findings affirm the potential of PAMT to set a new benchmark in WSI classification, underscoring the value of a targeted reprogramming approach."],"url":"http://arxiv.org/abs/2403.12537v1","category":"cs.CV"}
{"created":"2024-03-21 16:08:07","title":"Some Results on the Strict Fr\u00e9chet Differentiability of the Metric Projection Operator in Hilbert Spaces","abstract":"In this paper, we first present a simpler proof of a result on the strict Fr\\'echet differentiability of the metric projection operator onto closed balls centered at the origin in Hilbert spaces, which given by Li in \\cite{Li24}. Then, based on this result, we prove the strict Fr\\'echet differentiability of the metric projection operator onto closed balls with center at arbitrarily given point in Hilbert spaces. Finally, we study the strict Fr\\'echet differentiability of the metric projection operator onto the second-order cones in Euclidean spaces.","sentences":["In this paper, we first present a simpler proof of a result on the strict Fr\\'echet differentiability of the metric projection operator onto closed balls centered at the origin in Hilbert spaces, which given by Li in \\cite{Li24}.","Then, based on this result, we prove the strict Fr\\'echet differentiability of the metric projection operator onto closed balls with center at arbitrarily given point in Hilbert spaces.","Finally, we study the strict Fr\\'echet differentiability of the metric projection operator onto the second-order cones in Euclidean spaces."],"url":"http://arxiv.org/abs/2403.14512v1","category":"math.FA"}
{"created":"2024-03-21 15:46:19","title":"MULDE: Multiscale Log-Density Estimation via Denoising Score Matching for Video Anomaly Detection","abstract":"We propose a novel approach to video anomaly detection: we treat feature vectors extracted from videos as realizations of a random variable with a fixed distribution and model this distribution with a neural network. This lets us estimate the likelihood of test videos and detect video anomalies by thresholding the likelihood estimates. We train our video anomaly detector using a modification of denoising score matching, a method that injects training data with noise to facilitate modeling its distribution. To eliminate hyperparameter selection, we model the distribution of noisy video features across a range of noise levels and introduce a regularizer that tends to align the models for different levels of noise. At test time, we combine anomaly indications at multiple noise scales with a Gaussian mixture model. Running our video anomaly detector induces minimal delays as inference requires merely extracting the features and forward-propagating them through a shallow neural network and a Gaussian mixture model. Our experiments on five popular video anomaly detection benchmarks demonstrate state-of-the-art performance, both in the object-centric and in the frame-centric setup.","sentences":["We propose a novel approach to video anomaly detection: we treat feature vectors extracted from videos as realizations of a random variable with a fixed distribution and model this distribution with a neural network.","This lets us estimate the likelihood of test videos and detect video anomalies by thresholding the likelihood estimates.","We train our video anomaly detector using a modification of denoising score matching, a method that injects training data with noise to facilitate modeling its distribution.","To eliminate hyperparameter selection, we model the distribution of noisy video features across a range of noise levels and introduce a regularizer that tends to align the models for different levels of noise.","At test time, we combine anomaly indications at multiple noise scales with a Gaussian mixture model.","Running our video anomaly detector induces minimal delays as inference requires merely extracting the features and forward-propagating them through a shallow neural network and a Gaussian mixture model.","Our experiments on five popular video anomaly detection benchmarks demonstrate state-of-the-art performance, both in the object-centric and in the frame-centric setup."],"url":"http://arxiv.org/abs/2403.14497v1","category":"cs.CV"}
{"created":"2024-03-21 14:28:43","title":"Task-optimal data-driven surrogate models for eNMPC via differentiable simulation and optimization","abstract":"We present a method for end-to-end learning of Koopman surrogate models for optimal performance in control. In contrast to previous contributions that employ standard reinforcement learning (RL) algorithms, we use a training algorithm that exploits the potential differentiability of environments based on mechanistic simulation models. We evaluate the performance of our method by comparing it to that of other controller type and training algorithm combinations on a literature known eNMPC case study. Our method exhibits superior performance on this problem, thereby constituting a promising avenue towards more capable controllers that employ dynamic surrogate models.","sentences":["We present a method for end-to-end learning of Koopman surrogate models for optimal performance in control.","In contrast to previous contributions that employ standard reinforcement learning (RL) algorithms, we use a training algorithm that exploits the potential differentiability of environments based on mechanistic simulation models.","We evaluate the performance of our method by comparing it to that of other controller type and training algorithm combinations on a literature known eNMPC case study.","Our method exhibits superior performance on this problem, thereby constituting a promising avenue towards more capable controllers that employ dynamic surrogate models."],"url":"http://arxiv.org/abs/2403.14425v1","category":"cs.LG"}
{"created":"2024-03-21 13:57:25","title":"Spin-orbit interaction with large spin in the semi-classical regime","abstract":"We consider the time dependent Schr\\\"odinger equation with a coupling spin-orbit in the semi-classical regime $\\hbar\\searrow 0$ and large spin number $\\spin\\rightarrow +\\infty$ such that $\\hbar^\\delta\\spin=c$ where $c>0$ and $\\delta>0$ are constant. The initial state $\\Psi(0)$ is a product of an orbital coherent state in $L^2(\\R^d)$ and a spin coherent state in a spin irreducible representation space ${\\mathcal H}_{2\\spin +1}$. For $\\delta <1$, at the leading order in $\\hbar$, the time evolution $\\Psi(t)$ of $ \\Psi(0)$ is well approximated by the product of an orbital and a spin coherent state. Nevertheless for $1/2<\\delta<1$ the quantum orbital leaves the classical orbital. For $\\delta=1$ we prove that this last claim is no more true when the interaction depends on the orbital variables.   For the Dicke model, we prove that the orbital partial trace of the projector on $\\Psi(t)$ is a mixed state in $L^2(\\R)$ for small $t>0$.","sentences":["We consider the time dependent Schr\\\"odinger equation with a coupling spin-orbit in the semi-classical regime $\\hbar\\searrow 0$ and large spin number $\\spin\\rightarrow +\\infty$ such that $\\hbar^\\delta\\spin=c$ where $c>0$ and $\\delta>0$ are constant.","The initial state $\\Psi(0)$ is a product of an orbital coherent state in $L^2(\\R^d)$ and a spin coherent state in a spin irreducible representation space ${\\mathcal H}_{2\\spin +1}$. For $\\delta <1$, at the leading order in $\\hbar$, the time evolution $\\Psi(t)$ of $ \\Psi(0)$ is well approximated by the product of an orbital and a spin coherent state.","Nevertheless for $1/2<\\delta<1$ the quantum orbital leaves the classical orbital.","For $\\delta=1$ we prove that this last claim is no more true when the interaction depends on the orbital variables.   ","For the Dicke model, we prove that the orbital partial trace of the projector on $\\Psi(t)$ is a mixed state in $L^2(\\R)$ for small $t>0$."],"url":"http://arxiv.org/abs/2403.14408v1","category":"math-ph"}
{"created":"2024-03-21 11:57:16","title":"A Differentially Private Clustering Algorithm for Well-Clustered Graphs","abstract":"We study differentially private (DP) algorithms for recovering clusters in well-clustered graphs, which are graphs whose vertex set can be partitioned into a small number of sets, each inducing a subgraph of high inner conductance and small outer conductance. Such graphs have widespread application as a benchmark in the theoretical analysis of spectral clustering. We provide an efficient ($\\epsilon$,$\\delta$)-DP algorithm tailored specifically for such graphs. Our algorithm draws inspiration from the recent work of Chen et al., who developed DP algorithms for recovery of stochastic block models in cases where the graph comprises exactly two nearly-balanced clusters. Our algorithm works for well-clustered graphs with $k$ nearly-balanced clusters, and the misclassification ratio almost matches the one of the best-known non-private algorithms. We conduct experimental evaluations on datasets with known ground truth clusters to substantiate the prowess of our algorithm. We also show that any (pure) $\\epsilon$-DP algorithm would result in substantial error.","sentences":["We study differentially private (DP) algorithms for recovering clusters in well-clustered graphs, which are graphs whose vertex set can be partitioned into a small number of sets, each inducing a subgraph of high inner conductance and small outer conductance.","Such graphs have widespread application as a benchmark in the theoretical analysis of spectral clustering.","We provide an efficient ($\\epsilon$,$\\delta$)-DP algorithm tailored specifically for such graphs.","Our algorithm draws inspiration from the recent work of Chen et al., who developed DP algorithms for recovery of stochastic block models in cases where the graph comprises exactly two nearly-balanced clusters.","Our algorithm works for well-clustered graphs with $k$ nearly-balanced clusters, and the misclassification ratio almost matches the one of the best-known non-private algorithms.","We conduct experimental evaluations on datasets with known ground truth clusters to substantiate the prowess of our algorithm.","We also show that any (pure) $\\epsilon$-DP algorithm would result in substantial error."],"url":"http://arxiv.org/abs/2403.14332v1","category":"cs.DS"}
{"created":"2024-03-21 11:42:31","title":"Large deviation principle for stochastic differential equations driven by stochastic integrals","abstract":"In this paper, we prove the large deviation principle (LDP) for stochastic differential equations driven by stochastic integrals in one dimension. The result can be proved with a minimal use of rough path theory, and this implies the LDP for many class of rough volatility models, and it characterizes the asymptotic behavior of implied volatility. First, we introduce a new concept called $\\alpha$-Uniformly Exponentially Tightness, and prove the LDP for stochastic integrals on H\\\"{o}lder spaces. Second, we apply this type of LDP to deduce the LDP for stochastic differential equations driven by stochastic integrals in one dimension.","sentences":["In this paper, we prove the large deviation principle (LDP) for stochastic differential equations driven by stochastic integrals in one dimension.","The result can be proved with a minimal use of rough path theory, and this implies the LDP for many class of rough volatility models, and it characterizes the asymptotic behavior of implied volatility.","First, we introduce a new concept called $\\alpha$-Uniformly Exponentially Tightness, and prove the LDP for stochastic integrals on H\\\"{o}lder spaces.","Second, we apply this type of LDP to deduce the LDP for stochastic differential equations driven by stochastic integrals in one dimension."],"url":"http://arxiv.org/abs/2403.14321v1","category":"math.PR"}
{"created":"2024-03-21 11:32:54","title":"On Intermediate Exceptional Series","abstract":"The Freudenthal--Tits magic square $\\mathfrak{m}(\\mathbb{A}_1,\\mathbb{A}_2)$ for $\\mathbb{A}=\\mathbb{R},\\mathbb{C},\\mathbb{H},\\mathbb{O}$ of semi-simple Lie algebras can be extended by including the sextonions $\\mathbb{S}$. A series of non-reductive Lie algebras naturally appear in the new row associated with the sextonions, which we will call the \\textit{intermediate exceptional series}, with the largest one as the intermediate Lie algebra $E_{7+1/2}$ constructed by Landsberg--Manivel. We study various aspects of the intermediate vertex operator (super)algebras associated with the intermediate exceptional series, including rationality, coset constructions, irreducible modules, (super)characters and modular linear differential equations. For all $\\mathfrak{g}_I$ belonging to the intermediate exceptional series, the intermediate VOA $L_1(\\mathfrak{g}_I)$ has characters of irreducible modules coinciding with those of the simple rational $C_2$-cofinite $W$-algebra $W_{-h^\\vee/6}(\\mathfrak{g},f_\\theta)$ studied by Kawasetsu, with $\\mathfrak{g} $ belonging to the Cvitanovi\\'c--Deligne exceptional series. We propose some new intermediate VOA $L_k(\\mathfrak{g}_I)$ with integer level $k$ and investigate their properties. For example, for the intermediate Lie algebra $D_{6+1/2}$ between $D_6$ and $E_7$ in the subexceptional series and also in Vogel's projective plane, we find that the intermediate VOA $L_2(D_{6+1/2})$ has a simple current extension to a SVOA with four irreducible Neveu--Schwarz modules. We also provide some (super) coset constructions such as $L_2(E_7)/L_2(D_{6+1/2})$ and $L_1(D_{6+1/2})^{\\otimes2}\\!/L_2(D_{6+1/2})$. In the end, we find that the theta blocks associated with the intermediate exceptional series produce some new holomorphic Jacobi forms of critical weight and lattice index.","sentences":["The Freudenthal--Tits magic square $\\mathfrak{m}(\\mathbb{A}_1,\\mathbb{A}_2)$ for $\\mathbb{A}=\\mathbb{R},\\mathbb{C},\\mathbb{H},\\mathbb{O}$ of semi-simple Lie algebras can be extended by including the sextonions $\\mathbb{S}$. A series of non-reductive Lie algebras naturally appear in the new row associated with the sextonions, which we will call the \\textit{intermediate exceptional series}, with the largest one as the intermediate Lie algebra $E_{7+1/2}$ constructed by Landsberg--Manivel.","We study various aspects of the intermediate vertex operator (super)algebras associated with the intermediate exceptional series, including rationality, coset constructions, irreducible modules, (super)characters and modular linear differential equations.","For all $\\mathfrak{g}_I$ belonging to the intermediate exceptional series, the intermediate VOA $L_1(\\mathfrak{g}_I)$ has characters of irreducible modules coinciding with those of the simple rational $C_2$-cofinite $W$-algebra $W_{-h^\\vee/6}(\\mathfrak{g},f_\\theta)$ studied by Kawasetsu, with $\\mathfrak{g} $ belonging to the Cvitanovi\\'c--Deligne exceptional series.","We propose some new intermediate VOA $L_k(\\mathfrak{g}_I)$ with integer level $k$ and investigate their properties.","For example, for the intermediate Lie algebra $D_{6+1/2}$ between $D_6$ and $E_7$ in the subexceptional series and also in Vogel's projective plane, we find that the intermediate VOA $L_2(D_{6+1/2})$ has a simple current extension to a SVOA with four irreducible Neveu--Schwarz modules.","We also provide some (super) coset constructions such as $L_2(E_7)/L_2(D_{6+1/2})$ and $L_1(D_{6+1/2})^{\\otimes2}\\!/L_2(D_{6+1/2})$. In the end, we find that the theta blocks associated with the intermediate exceptional series produce some new holomorphic Jacobi forms of critical weight and lattice index."],"url":"http://arxiv.org/abs/2403.14311v1","category":"math-ph"}
{"created":"2024-03-21 11:17:03","title":"Critical temperatures of two- and three-dimensional Ferromagnetic Ising models- A hierarchy","abstract":"A unified algebraic structure is shown to exist among various equations for the critical temperatures pertaining to diverse two- and three-dimensional lattices. This isomorphism is a pointer to the straight-forward extension of two-dimensional results to corresponding three dimensional analogues.","sentences":["A unified algebraic structure is shown to exist among various equations for the critical temperatures pertaining to diverse two-","and three-dimensional lattices.","This isomorphism is a pointer to the straight-forward extension of two-dimensional results to corresponding three dimensional analogues."],"url":"http://arxiv.org/abs/2403.14303v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-21 10:53:35","title":"Effects of non-continuous inverse Compton cooling in blazars","abstract":"Context. Blazar flares provide a window into the extreme physical processes occurring in relativistic outflows. Most numerical codes used for modeling blazar emission during flares utilize a simplified continuous-loss description of particle cooling due to the inverse Compton (IC) process, neglecting non-continuous (discrete) effects that arise in the Klein-Nishina (KN) regime. The significance of such effects has not been explored in detail yet. Aims. In this study, we investigate the importance of non-continuous Compton cooling losses and their impact on the electron spectrum and spectral energy distribution (SED) of blazars during high flux states (flares), as well as in the low state. Methods. We solve numerically the full transport equation accounting for large relative jumps in energy, by extending our existing blazar flare modeling code EMBLEM. We perform a detailed physical modeling of the brightest gamma-ray flare of the archetypal Flat Spectrum Radio Quasar (FSRQ) 3C 279 detected in June 2015. We then compare results obtained using the full cooling term and using the continuous-loss approximation. Results. We show that during flaring states of FSRQs characterized by high Compton dominance, the non-continuous cooling can lead to a significant modification of the electron spectrum, introducing a range of distinct features, such as low-energy tails, hardening/softening, narrow and extended particle excesses, and shifts in the cooling break position. Such distortion translates to differences in the associated SED up to 50%. This highlights the importance of non-continuous effects and the need to consider them in blazar emission models, particularly applied to extreme gamma-ray flares.","sentences":["Context.","Blazar flares provide a window into the extreme physical processes occurring in relativistic outflows.","Most numerical codes used for modeling blazar emission during flares utilize a simplified continuous-loss description of particle cooling due to the inverse Compton (IC) process, neglecting non-continuous (discrete) effects that arise in the Klein-Nishina (KN) regime.","The significance of such effects has not been explored in detail yet.","Aims.","In this study, we investigate the importance of non-continuous Compton cooling losses and their impact on the electron spectrum and spectral energy distribution (SED) of blazars during high flux states (flares), as well as in the low state.","Methods.","We solve numerically the full transport equation accounting for large relative jumps in energy, by extending our existing blazar flare modeling code EMBLEM.","We perform a detailed physical modeling of the brightest gamma-ray flare of the archetypal Flat Spectrum Radio Quasar (FSRQ) 3C 279 detected in June 2015.","We then compare results obtained using the full cooling term and using the continuous-loss approximation.","Results.","We show that during flaring states of FSRQs characterized by high Compton dominance, the non-continuous cooling can lead to a significant modification of the electron spectrum, introducing a range of distinct features, such as low-energy tails, hardening/softening, narrow and extended particle excesses, and shifts in the cooling break position.","Such distortion translates to differences in the associated SED up to 50%.","This highlights the importance of non-continuous effects and the need to consider them in blazar emission models, particularly applied to extreme gamma-ray flares."],"url":"http://arxiv.org/abs/2403.14289v1","category":"astro-ph.HE"}
{"created":"2024-03-21 09:07:28","title":"ResNet101 and DAE for Enhance Quality and Classification Accuracy in Skin Cancer Imaging","abstract":"Skin cancer is a crucial health issue that requires timely detection for higher survival rates. Traditional computer vision techniques face challenges in addressing the advanced variability of skin lesion features, a gap partially bridged by convolutional neural networks (CNNs). To overcome the existing issues, we introduce an innovative convolutional ensemble network approach named deep autoencoder (DAE) with ResNet101. This method utilizes convolution-based deep neural networks for the detection of skin cancer. The ISIC-2018 public data taken from the source is used for experimental results, which demonstrate remarkable performance with the different in terms of performance metrics. The methods result in 96.03% of accuracy, 95.40 % of precision, 96.05% of recall, 0.9576 of F-measure, 0.98 of AUC.","sentences":["Skin cancer is a crucial health issue that requires timely detection for higher survival rates.","Traditional computer vision techniques face challenges in addressing the advanced variability of skin lesion features, a gap partially bridged by convolutional neural networks (CNNs).","To overcome the existing issues, we introduce an innovative convolutional ensemble network approach named deep autoencoder (DAE) with ResNet101.","This method utilizes convolution-based deep neural networks for the detection of skin cancer.","The ISIC-2018 public data taken from the source is used for experimental results, which demonstrate remarkable performance with the different in terms of performance metrics.","The methods result in 96.03% of accuracy, 95.40 % of precision, 96.05% of recall, 0.9576 of F-measure, 0.98 of AUC."],"url":"http://arxiv.org/abs/2403.14248v1","category":"eess.IV"}
{"created":"2024-03-21 08:41:44","title":"Spanning Multi-Asset Payoffs With ReLUs","abstract":"We propose a distributional formulation of the spanning problem of a multi-asset payoff by vanilla basket options. This problem is shown to have a unique solution if and only if the payoff function is even and absolutely homogeneous, and we establish a Fourier-based formula to calculate the solution. Financial payoffs are typically piecewise linear, resulting in a solution that may be derived explicitly, yet may also be hard to numerically exploit. One-hidden-layer feedforward neural networks instead provide a natural and efficient numerical alternative for discrete spanning. We test this approach for a selection of archetypal payoffs and obtain better hedging results with vanilla basket options compared to industry-favored approaches based on single-asset vanilla hedges.","sentences":["We propose a distributional formulation of the spanning problem of a multi-asset payoff by vanilla basket options.","This problem is shown to have a unique solution if and only if the payoff function is even and absolutely homogeneous, and we establish a Fourier-based formula to calculate the solution.","Financial payoffs are typically piecewise linear, resulting in a solution that may be derived explicitly, yet may also be hard to numerically exploit.","One-hidden-layer feedforward neural networks instead provide a natural and efficient numerical alternative for discrete spanning.","We test this approach for a selection of archetypal payoffs and obtain better hedging results with vanilla basket options compared to industry-favored approaches based on single-asset vanilla hedges."],"url":"http://arxiv.org/abs/2403.14231v1","category":"q-fin.RM"}
{"created":"2024-03-21 08:40:16","title":"Low-rank tensor product Richardson iteration for radiative transfer in plane-parallel geometry","abstract":"The radiative transfer equation (RTE) has been established as a fundamental tool for the description of energy transport, absorption and scattering in many relevant societal applications, and requires numerical approximations. However, classical numerical algorithms scale unfavorably with respect to the dimensionality of such radiative transfer problems, where solutions depend on physical as well as angular variables. In this paper we address this dimensionality issue by developing a low-rank tensor product framework for the RTE in plane-parallel geometry. We exploit the tensor product nature of the phase space to recover an operator equation where the operator is given by a short sum of Kronecker products. This equation is solved by a preconditioned and rank-controlled Richardson iteration in Hilbert spaces. Using exponential sums approximations we construct a preconditioner that is compatible with the low-rank tensor product framework. The use of suitable preconditioning techniques yields a transformation of the operator equation in Hilbert space into a sequence space with Euclidean inner product, enabling rigorous error and rank control in the Euclidean metric.","sentences":["The radiative transfer equation (RTE) has been established as a fundamental tool for the description of energy transport, absorption and scattering in many relevant societal applications, and requires numerical approximations.","However, classical numerical algorithms scale unfavorably with respect to the dimensionality of such radiative transfer problems, where solutions depend on physical as well as angular variables.","In this paper we address this dimensionality issue by developing a low-rank tensor product framework for the RTE in plane-parallel geometry.","We exploit the tensor product nature of the phase space to recover an operator equation where the operator is given by a short sum of Kronecker products.","This equation is solved by a preconditioned and rank-controlled Richardson iteration in Hilbert spaces.","Using exponential sums approximations we construct a preconditioner that is compatible with the low-rank tensor product framework.","The use of suitable preconditioning techniques yields a transformation of the operator equation in Hilbert space into a sequence space with Euclidean inner product, enabling rigorous error and rank control in the Euclidean metric."],"url":"http://arxiv.org/abs/2403.14229v1","category":"math.NA"}
{"created":"2024-03-21 08:30:44","title":"Stitching for Neuroevolution: Recombining Deep Neural Networks without Breaking Them","abstract":"Traditional approaches to neuroevolution often start from scratch. This becomes prohibitively expensive in terms of computational and data requirements when targeting modern, deep neural networks. Using a warm start could be highly advantageous, e.g., using previously trained networks, potentially from different sources. This moreover enables leveraging the benefits of transfer learning (in particular vastly reduced training effort). However, recombining trained networks is non-trivial because architectures and feature representations typically differ. Consequently, a straightforward exchange of layers tends to lead to a performance breakdown. We overcome this by matching the layers of parent networks based on their connectivity, identifying potential crossover points. To correct for differing feature representations between these layers we employ stitching, which merges the networks by introducing new layers at crossover points. To train the merged network, only stitching layers need to be considered. New networks can then be created by selecting a subnetwork by choosing which stitching layers to (not) use. Assessing their performance is efficient as only their evaluation on data is required. We experimentally show that our approach enables finding networks that represent novel trade-offs between performance and computational cost, with some even dominating the original networks.","sentences":["Traditional approaches to neuroevolution often start from scratch.","This becomes prohibitively expensive in terms of computational and data requirements when targeting modern, deep neural networks.","Using a warm start could be highly advantageous, e.g., using previously trained networks, potentially from different sources.","This moreover enables leveraging the benefits of transfer learning (in particular vastly reduced training effort).","However, recombining trained networks is non-trivial because architectures and feature representations typically differ.","Consequently, a straightforward exchange of layers tends to lead to a performance breakdown.","We overcome this by matching the layers of parent networks based on their connectivity, identifying potential crossover points.","To correct for differing feature representations between these layers we employ stitching, which merges the networks by introducing new layers at crossover points.","To train the merged network, only stitching layers need to be considered.","New networks can then be created by selecting a subnetwork by choosing which stitching layers to (not) use.","Assessing their performance is efficient as only their evaluation on data is required.","We experimentally show that our approach enables finding networks that represent novel trade-offs between performance and computational cost, with some even dominating the original networks."],"url":"http://arxiv.org/abs/2403.14224v1","category":"cs.NE"}
{"created":"2024-03-21 06:33:10","title":"Motion of spinning particles around a polymer black hole in loop quantum gravity","abstract":"In the curved spacetime background, the trajectory of a spinning test particle will deviate from the geodesic. Using the effective potential method, we study the motion of a spinning test particle on the equatorial plane of a polymer black hole in loop quantum gravity described by the Mathisson-Papapetrou-Dixon equations with minimal spin-gravity interaction. We find that for the bounded orbits in the radial direction, the particle's motion is timelike when its spin is small. The radial range of the orbit and its eccentricity decrease with the loop quantum gravity parameter. However, when the particle takes a large enough spin, we observe an interesting phenomenon that the timelike and spacelike motions alternately appear while are separated by a critical radius. Outside the critical radius, the motion is timelike, however inside it is spacelike, and on the radius $r_c$ it is null. To explore more observable effects of the loop quantum gravity parameter on the motion of the spinning particle, we focus our attention on the circular orbits, particularly the innermost stable circular orbits, near the black hole. The result shows that for the same spin, there are two different innermost stable circular orbits, one with a larger radius and the other with a smaller radius. Both the radii decrease as the loop quantum gravity parameter increases. More significantly, with the increase of the spin of the particle, the small innermost stable circular orbit transition from timelike to spacelike, while the one with large radius does not. Instead, it terminates at a certain value of spin. All the results present the significant influences of the loop quantum gravity parameter on the motion of the spinning particles.","sentences":["In the curved spacetime background, the trajectory of a spinning test particle will deviate from the geodesic.","Using the effective potential method, we study the motion of a spinning test particle on the equatorial plane of a polymer black hole in loop quantum gravity described by the Mathisson-Papapetrou-Dixon equations with minimal spin-gravity interaction.","We find that for the bounded orbits in the radial direction, the particle's motion is timelike when its spin is small.","The radial range of the orbit and its eccentricity decrease with the loop quantum gravity parameter.","However, when the particle takes a large enough spin, we observe an interesting phenomenon that the timelike and spacelike motions alternately appear while are separated by a critical radius.","Outside the critical radius, the motion is timelike, however inside it is spacelike, and on the radius $r_c$ it is null.","To explore more observable effects of the loop quantum gravity parameter on the motion of the spinning particle, we focus our attention on the circular orbits, particularly the innermost stable circular orbits, near the black hole.","The result shows that for the same spin, there are two different innermost stable circular orbits, one with a larger radius and the other with a smaller radius.","Both the radii decrease as the loop quantum gravity parameter increases.","More significantly, with the increase of the spin of the particle, the small innermost stable circular orbit transition from timelike to spacelike, while the one with large radius does not.","Instead, it terminates at a certain value of spin.","All the results present the significant influences of the loop quantum gravity parameter on the motion of the spinning particles."],"url":"http://arxiv.org/abs/2403.14164v1","category":"gr-qc"}
{"created":"2024-03-21 05:33:49","title":"Learning Decomposable and Debiased Representations via Attribute-Centric Information Bottlenecks","abstract":"Biased attributes, spuriously correlated with target labels in a dataset, can problematically lead to neural networks that learn improper shortcuts for classifications and limit their capabilities for out-of-distribution (OOD) generalization. Although many debiasing approaches have been proposed to ensure correct predictions from biased datasets, few studies have considered learning latent embedding consisting of intrinsic and biased attributes that contribute to improved performance and explain how the model pays attention to attributes. In this paper, we propose a novel debiasing framework, Debiasing Global Workspace, introducing attention-based information bottlenecks for learning compositional representations of attributes without defining specific bias types. Based on our observation that learning shape-centric representation helps robust performance on OOD datasets, we adopt those abilities to learn robust and generalizable representations of decomposable latent embeddings corresponding to intrinsic and biasing attributes. We conduct comprehensive evaluations on biased datasets, along with both quantitative and qualitative analyses, to showcase our approach's efficacy in attribute-centric representation learning and its ability to differentiate between intrinsic and bias-related features.","sentences":["Biased attributes, spuriously correlated with target labels in a dataset, can problematically lead to neural networks that learn improper shortcuts for classifications and limit their capabilities for out-of-distribution (OOD) generalization.","Although many debiasing approaches have been proposed to ensure correct predictions from biased datasets, few studies have considered learning latent embedding consisting of intrinsic and biased attributes that contribute to improved performance and explain how the model pays attention to attributes.","In this paper, we propose a novel debiasing framework, Debiasing Global Workspace, introducing attention-based information bottlenecks for learning compositional representations of attributes without defining specific bias types.","Based on our observation that learning shape-centric representation helps robust performance on OOD datasets, we adopt those abilities to learn robust and generalizable representations of decomposable latent embeddings corresponding to intrinsic and biasing attributes.","We conduct comprehensive evaluations on biased datasets, along with both quantitative and qualitative analyses, to showcase our approach's efficacy in attribute-centric representation learning and its ability to differentiate between intrinsic and bias-related features."],"url":"http://arxiv.org/abs/2403.14140v1","category":"cs.CV"}
{"created":"2024-03-21 04:31:59","title":"AI and Memory Wall","abstract":"The availability of unprecedented unsupervised training data, along with neural scaling laws, has resulted in an unprecedented surge in model size and compute requirements for serving/training LLMs. However, the main performance bottleneck is increasingly shifting to memory bandwidth. Over the past 20 years, peak server hardware FLOPS has been scaling at 3.0x/2yrs, outpacing the growth of DRAM and interconnect bandwidth, which have only scaled at 1.6 and 1.4 times every 2 years, respectively. This disparity has made memory, rather than compute, the primary bottleneck in AI applications, particularly in serving. Here, we analyze encoder and decoder Transformer models and show how memory bandwidth can become the dominant bottleneck for decoder models. We argue for a redesign in model architecture, training, and deployment strategies to overcome this memory limitation.","sentences":["The availability of unprecedented unsupervised training data, along with neural scaling laws, has resulted in an unprecedented surge in model size and compute requirements for serving/training LLMs.","However, the main performance bottleneck is increasingly shifting to memory bandwidth.","Over the past 20 years, peak server hardware FLOPS has been scaling at 3.0x/2yrs, outpacing the growth of DRAM and interconnect bandwidth, which have only scaled at 1.6 and 1.4 times every 2 years, respectively.","This disparity has made memory, rather than compute, the primary bottleneck in AI applications, particularly in serving.","Here, we analyze encoder and decoder Transformer models and show how memory bandwidth can become the dominant bottleneck for decoder models.","We argue for a redesign in model architecture, training, and deployment strategies to overcome this memory limitation."],"url":"http://arxiv.org/abs/2403.14123v1","category":"cs.LG"}
{"created":"2024-03-21 03:36:22","title":"Bulk properties of PSR J0030+0451 inferred with the compactness measurement of NICER","abstract":"In 2019, Neutron star Interior Composition ExploreR (NICER) mission released its findings on the mass and radius of the isolated neutron star (INS) PSR J0030+0451, revealing a mass of approximately 1.4 solar masses ($M_{\\odot}$) and a radius near 13 kilometers. However, the recent re-analysis by the NICER collaboration \\citep{vinciguerra2024updated} suggests that the available data primarily yields a precise inference of the compactness for this source while the resulting mass and radius are strongly model-dependent and diverse (the 68.3\\% confidence counters just overlap slightly for the ST+PDT and PDT-U models). By integrating this compactness data with the equation of state (EoS) refined by our latest investigations, we have deduced the mass and radius for PSR J0030+0451, delivering estimates of $M=1.48^{+0.09}_{-0.10}~M_\\odot$ and $R=12.39_{-0.70}^{+0.50}~{\\rm km}$ for the compactness found in ST+PDT model, alongside $M=1.47^{+0.14}_{-0.20}~M_\\odot$ and $R=12.37_{-0.70}^{+0.50}~{\\rm km}$ for the compactness in PDT-U model. These two groups of results are well consistent with each other and the direct X-ray data inference within the ST+PDT model seems to be favored. Additionally, we have calculated the tidal deformability, moment of inertia, and gravitational binding energy for this NS. Furthermore, employing these refined EoS models, we have updated mass-radius estimates for three INSs with established gravitational redshifts.","sentences":["In 2019, Neutron star Interior Composition ExploreR (NICER) mission released its findings on the mass and radius of the isolated neutron star (INS) PSR J0030+0451, revealing a mass of approximately 1.4 solar masses ($M_{\\odot}$) and a radius near 13 kilometers.","However, the recent re-analysis by the NICER collaboration \\citep{vinciguerra2024updated} suggests that the available data primarily yields a precise inference of the compactness for this source while the resulting mass and radius are strongly model-dependent and diverse (the 68.3\\% confidence counters just overlap slightly for the ST+PDT and PDT-U models).","By integrating this compactness data with the equation of state (EoS) refined by our latest investigations, we have deduced the mass and radius for PSR J0030+0451, delivering estimates of $M=1.48^{+0.09}_{-0.10}~M_\\odot$ and $R=12.39_{-0.70}^{+0.50}~{\\rm km}$ for the compactness found in ST+PDT model, alongside $M=1.47^{+0.14}_{-0.20}~M_\\odot$ and $R=12.37_{-0.70}^{+0.50}~{\\rm km}$ for the compactness in PDT-U model.","These two groups of results are well consistent with each other and the direct X-ray data inference within the ST+PDT model seems to be favored.","Additionally, we have calculated the tidal deformability, moment of inertia, and gravitational binding energy for this NS.","Furthermore, employing these refined EoS models, we have updated mass-radius estimates for three INSs with established gravitational redshifts."],"url":"http://arxiv.org/abs/2403.14105v1","category":"astro-ph.HE"}
{"created":"2024-03-21 03:14:36","title":"Transverse Ricci solitons on a compact foliated manifold","abstract":"We investigate transverse Ricci solitons, the self-similar solutions of the transverse Ricci flow, on a compact foliated manifold. In particular, we show the relations between a taut Riemannian foliation and a transverse Ricci soliton. Moreover, we find some examples of transverse Ricci solitons.","sentences":["We investigate transverse Ricci solitons, the self-similar solutions of the transverse Ricci flow, on a compact foliated manifold.","In particular, we show the relations between a taut Riemannian foliation and a transverse Ricci soliton.","Moreover, we find some examples of transverse Ricci solitons."],"url":"http://arxiv.org/abs/2403.14099v1","category":"math.DG"}
{"created":"2024-03-21 02:47:47","title":"Dynamic motion trajectory control with nanoradian accuracy for multi-element X-ray optical systems via laser interferometry","abstract":"The past decades have witnessed the development of new X-ray beam sources with brightness growing at a rate surpassing Moore's law. Current and upcoming diffraction limited and fully coherent X-ray beam sources, including multi-bend achromat based synchrotron sources and high repetition rate X-ray free electron lasers, puts increasingly stringent requirements on stability and accuracy of X-ray optics systems. Parasitic motion errors at sub-micro radian scale in beam transport and beam conditioning optics can lead to significant loss of coherence and brightness delivered from source to experiment. To address this challenge, we incorporated optical metrology based on interferometry and differential wavefront sensing as part of the X-ray optics motion control system. A prototype X-ray optics system was constructed following the optical layout of a tunable X-ray cavity. On-line interferometric metrology enabled dynamical feedback to a motion control system to track and compensate for motion errors. The system achieved sub-microradian scale performance, as multiple optical elements are synchronously and continuously adjusted. This first proof of principle measurement demonstrated both the potential and necessity of incorporating optical metrology as part of the motion control architecture for large scale X-ray optical systems such as monochromators, delay lines, and in particular, X-ray cavity systems to enable the next generation cavity-based X-ray free electron lasers.","sentences":["The past decades have witnessed the development of new X-ray beam sources with brightness growing at a rate surpassing Moore's law.","Current and upcoming diffraction limited and fully coherent X-ray beam sources, including multi-bend achromat based synchrotron sources and high repetition rate X-ray free electron lasers, puts increasingly stringent requirements on stability and accuracy of X-ray optics systems.","Parasitic motion errors at sub-micro radian scale in beam transport and beam conditioning optics can lead to significant loss of coherence and brightness delivered from source to experiment.","To address this challenge, we incorporated optical metrology based on interferometry and differential wavefront sensing as part of the X-ray optics motion control system.","A prototype X-ray optics system was constructed following the optical layout of a tunable X-ray cavity.","On-line interferometric metrology enabled dynamical feedback to a motion control system to track and compensate for motion errors.","The system achieved sub-microradian scale performance, as multiple optical elements are synchronously and continuously adjusted.","This first proof of principle measurement demonstrated both the potential and necessity of incorporating optical metrology as part of the motion control architecture for large scale X-ray optical systems such as monochromators, delay lines, and in particular, X-ray cavity systems to enable the next generation cavity-based X-ray free electron lasers."],"url":"http://arxiv.org/abs/2403.14090v1","category":"physics.optics"}
{"created":"2024-03-21 02:45:16","title":"Unsupervised Intrinsic Image Decomposition with LiDAR Intensity Enhanced Training","abstract":"Unsupervised intrinsic image decomposition (IID) is the process of separating a natural image into albedo and shade without these ground truths. A recent model employing light detection and ranging (LiDAR) intensity demonstrated impressive performance, though the necessity of LiDAR intensity during inference restricts its practicality. Thus, IID models employing only a single image during inference while keeping as high IID quality as the one with an image plus LiDAR intensity are highly desired. To address this challenge, we propose a novel approach that utilizes only an image during inference while utilizing an image and LiDAR intensity during training. Specifically, we introduce a partially-shared model that accepts an image and LiDAR intensity individually using a different specific encoder but processes them together in specific components to learn shared representations. In addition, to enhance IID quality, we propose albedo-alignment loss and image-LiDAR conversion (ILC) paths. Albedo-alignment loss aligns the gray-scale albedo from an image to that inferred from LiDAR intensity, thereby reducing cast shadows in albedo from an image due to the absence of cast shadows in LiDAR intensity. Furthermore, to translate the input image into albedo and shade style while keeping the image contents, the input image is separated into style code and content code by encoders. The ILC path mutually translates the image and LiDAR intensity, which share content but differ in style, contributing to the distinct differentiation of style from content. Consequently, LIET achieves comparable IID quality to the existing model with LiDAR intensity, while utilizing only an image without LiDAR intensity during inference.","sentences":["Unsupervised intrinsic image decomposition (IID) is the process of separating a natural image into albedo and shade without these ground truths.","A recent model employing light detection and ranging (LiDAR) intensity demonstrated impressive performance, though the necessity of LiDAR intensity during inference restricts its practicality.","Thus, IID models employing only a single image during inference while keeping as high IID quality as the one with an image plus LiDAR intensity are highly desired.","To address this challenge, we propose a novel approach that utilizes only an image during inference while utilizing an image and LiDAR intensity during training.","Specifically, we introduce a partially-shared model that accepts an image and LiDAR intensity individually using a different specific encoder but processes them together in specific components to learn shared representations.","In addition, to enhance IID quality, we propose albedo-alignment loss and image-LiDAR conversion (ILC) paths.","Albedo-alignment loss aligns the gray-scale albedo from an image to that inferred from LiDAR intensity, thereby reducing cast shadows in albedo from an image due to the absence of cast shadows in LiDAR intensity.","Furthermore, to translate the input image into albedo and shade style while keeping the image contents, the input image is separated into style code and content code by encoders.","The ILC path mutually translates the image and LiDAR intensity, which share content but differ in style, contributing to the distinct differentiation of style from content.","Consequently, LIET achieves comparable IID quality to the existing model with LiDAR intensity, while utilizing only an image without LiDAR intensity during inference."],"url":"http://arxiv.org/abs/2403.14089v1","category":"cs.CV"}
{"created":"2024-03-21 02:30:56","title":"Learning-based Multi-continuum Model for Multiscale Flow Problems","abstract":"Multiscale problems can usually be approximated through numerical homogenization by an equation with some effective parameters that can capture the macroscopic behavior of the original system on the coarse grid to speed up the simulation. However, this approach usually assumes scale separation and that the heterogeneity of the solution can be approximated by the solution average in each coarse block. For complex multiscale problems, the computed single effective properties/continuum might be inadequate. In this paper, we propose a novel learning-based multi-continuum model to enrich the homogenized equation and improve the accuracy of the single continuum model for multiscale problems with some given data. Without loss of generalization, we consider a two-continuum case. The first flow equation keeps the information of the original homogenized equation with an additional interaction term. The second continuum is newly introduced, and the effective permeability in the second flow equation is determined by a neural network. The interaction term between the two continua aligns with that used in the Dual-porosity model but with a learnable coefficient determined by another neural network. The new model with neural network terms is then optimized using trusted data. We discuss both direct back-propagation and the adjoint method for the PDE-constraint optimization problem. Our proposed learning-based multi-continuum model can resolve multiple interacted media within each coarse grid block and describe the mass transfer among them, and it has been demonstrated to significantly improve the simulation results through numerical experiments involving both linear and nonlinear flow equations.","sentences":["Multiscale problems can usually be approximated through numerical homogenization by an equation with some effective parameters that can capture the macroscopic behavior of the original system on the coarse grid to speed up the simulation.","However, this approach usually assumes scale separation and that the heterogeneity of the solution can be approximated by the solution average in each coarse block.","For complex multiscale problems, the computed single effective properties/continuum might be inadequate.","In this paper, we propose a novel learning-based multi-continuum model to enrich the homogenized equation and improve the accuracy of the single continuum model for multiscale problems with some given data.","Without loss of generalization, we consider a two-continuum case.","The first flow equation keeps the information of the original homogenized equation with an additional interaction term.","The second continuum is newly introduced, and the effective permeability in the second flow equation is determined by a neural network.","The interaction term between the two continua aligns with that used in the Dual-porosity model but with a learnable coefficient determined by another neural network.","The new model with neural network terms is then optimized using trusted data.","We discuss both direct back-propagation and the adjoint method for the PDE-constraint optimization problem.","Our proposed learning-based multi-continuum model can resolve multiple interacted media within each coarse grid block and describe the mass transfer among them, and it has been demonstrated to significantly improve the simulation results through numerical experiments involving both linear and nonlinear flow equations."],"url":"http://arxiv.org/abs/2403.14084v1","category":"math.NA"}
{"created":"2024-03-21 02:26:30","title":"emoDARTS: Joint Optimisation of CNN & Sequential Neural Network Architectures for Superior Speech Emotion Recognition","abstract":"Speech Emotion Recognition (SER) is crucial for enabling computers to understand the emotions conveyed in human communication. With recent advancements in Deep Learning (DL), the performance of SER models has significantly improved. However, designing an optimal DL architecture requires specialised knowledge and experimental assessments. Fortunately, Neural Architecture Search (NAS) provides a potential solution for automatically determining the best DL model. The Differentiable Architecture Search (DARTS) is a particularly efficient method for discovering optimal models. This study presents emoDARTS, a DARTS-optimised joint CNN and Sequential Neural Network (SeqNN: LSTM, RNN) architecture that enhances SER performance. The literature supports the selection of CNN and LSTM coupling to improve performance.   While DARTS has previously been used to choose CNN and LSTM operations independently, our technique adds a novel mechanism for selecting CNN and SeqNN operations in conjunction using DARTS. Unlike earlier work, we do not impose limits on the layer order of the CNN. Instead, we let DARTS choose the best layer order inside the DARTS cell. We demonstrate that emoDARTS outperforms conventionally designed CNN-LSTM models and surpasses the best-reported SER results achieved through DARTS on CNN-LSTM by evaluating our approach on the IEMOCAP, MSP-IMPROV, and MSP-Podcast datasets.","sentences":["Speech Emotion Recognition (SER) is crucial for enabling computers to understand the emotions conveyed in human communication.","With recent advancements in Deep Learning (DL), the performance of SER models has significantly improved.","However, designing an optimal DL architecture requires specialised knowledge and experimental assessments.","Fortunately, Neural Architecture Search (NAS) provides a potential solution for automatically determining the best DL model.","The Differentiable Architecture Search (DARTS) is a particularly efficient method for discovering optimal models.","This study presents emoDARTS, a DARTS-optimised joint CNN and Sequential Neural Network (SeqNN: LSTM, RNN) architecture that enhances SER performance.","The literature supports the selection of CNN and LSTM coupling to improve performance.   ","While DARTS has previously been used to choose CNN and LSTM operations independently, our technique adds a novel mechanism for selecting CNN and SeqNN operations in conjunction using DARTS.","Unlike earlier work, we do not impose limits on the layer order of the CNN.","Instead, we let DARTS choose the best layer order inside the DARTS cell.","We demonstrate that emoDARTS outperforms conventionally designed CNN-LSTM models and surpasses the best-reported SER results achieved through DARTS on CNN-LSTM by evaluating our approach on the IEMOCAP, MSP-IMPROV, and MSP-Podcast datasets."],"url":"http://arxiv.org/abs/2403.14083v1","category":"cs.SD"}
{"created":"2024-03-21 02:07:24","title":"Derivation of Yudovich solutions of Incompressible Euler from the Vlasov-Poisson system","abstract":"We derive the two dimensional incompressible Euler equation as a quasineutral limit of the Vlasov-Poisson equation using a modulated energy approach. We propose a strategy which enables to treat solutions where the gradient of the velocity is merely $\\mathrm{BMO}$, in accordance to the celebrated Yudovich theorem.","sentences":["We derive the two dimensional incompressible Euler equation as a quasineutral limit of the Vlasov-Poisson equation using a modulated energy approach.","We propose a strategy which enables to treat solutions where the gradient of the velocity is merely $\\mathrm{BMO}$, in accordance to the celebrated Yudovich theorem."],"url":"http://arxiv.org/abs/2403.14080v1","category":"math.AP"}
{"created":"2024-03-21 02:04:54","title":"Addressing complex boundary conditions of miscible flow and transport in two and three dimensions with application to optimal control","abstract":"We investigate complex boundary conditions of the miscible displacement system in two and three space dimensions with the commonly-used Bear-Scheidegger diffusion-dispersion tensor, which describes, e.g., the porous medium flow processes in petroleum reservoir simulation or groundwater contaminant transport. Specifically, we incorporate the no-flux boundary condition for the Darcy velocity to prove that the general no-flux boundary condition for the transport equation is equivalent to the normal derivative boundary condition of the concentration, based on which we further prove several complex boundary conditions by the Bear-Scheidegger tensor and its derivative. The derived boundary conditions not only provide new insights and distinct properties of the Bear-Scheidegger diffusion-dispersion tensor, but accommodate the coupling and the nonlinearity of the miscible displacement system and the Bear-Scheidegger tensor in deriving the first-order optimality condition of the corresponding optimal control problem for practical application.","sentences":["We investigate complex boundary conditions of the miscible displacement system in two and three space dimensions with the commonly-used Bear-Scheidegger diffusion-dispersion tensor, which describes, e.g., the porous medium flow processes in petroleum reservoir simulation or groundwater contaminant transport.","Specifically, we incorporate the no-flux boundary condition for the Darcy velocity to prove that the general no-flux boundary condition for the transport equation is equivalent to the normal derivative boundary condition of the concentration, based on which we further prove several complex boundary conditions by the Bear-Scheidegger tensor and its derivative.","The derived boundary conditions not only provide new insights and distinct properties of the Bear-Scheidegger diffusion-dispersion tensor, but accommodate the coupling and the nonlinearity of the miscible displacement system and the Bear-Scheidegger tensor in deriving the first-order optimality condition of the corresponding optimal control problem for practical application."],"url":"http://arxiv.org/abs/2403.14079v1","category":"math.OC"}
{"created":"2024-03-21 00:35:31","title":"Leveraging Thermal Modality to Enhance Reconstruction in Low-Light Conditions","abstract":"Neural Radiance Fields (NeRF) accomplishes photo-realistic novel view synthesis by learning the implicit volumetric representation of a scene from multi-view images, which faithfully convey the colorimetric information. However, sensor noises will contaminate low-value pixel signals, and the lossy camera image signal processor will further remove near-zero intensities in extremely dark situations, deteriorating the synthesis performance. Existing approaches reconstruct low-light scenes from raw images but struggle to recover texture and boundary details in dark regions. Additionally, they are unsuitable for high-speed models relying on explicit representations. To address these issues, we present Thermal-NeRF, which takes thermal and visible raw images as inputs, considering the thermal camera is robust to the illumination variation and raw images preserve any possible clues in the dark, to accomplish visible and thermal view synthesis simultaneously. Also, the first multi-view thermal and visible dataset (MVTV) is established to support the research on multimodal NeRF. Thermal-NeRF achieves the best trade-off between detail preservation and noise smoothing and provides better synthesis performance than previous work. Finally, we demonstrate that both modalities are beneficial to each other in 3D reconstruction.","sentences":["Neural Radiance Fields (NeRF) accomplishes photo-realistic novel view synthesis by learning the implicit volumetric representation of a scene from multi-view images, which faithfully convey the colorimetric information.","However, sensor noises will contaminate low-value pixel signals, and the lossy camera image signal processor will further remove near-zero intensities in extremely dark situations, deteriorating the synthesis performance.","Existing approaches reconstruct low-light scenes from raw images but struggle to recover texture and boundary details in dark regions.","Additionally, they are unsuitable for high-speed models relying on explicit representations.","To address these issues, we present Thermal-NeRF, which takes thermal and visible raw images as inputs, considering the thermal camera is robust to the illumination variation and raw images preserve any possible clues in the dark, to accomplish visible and thermal view synthesis simultaneously.","Also, the first multi-view thermal and visible dataset (MVTV) is established to support the research on multimodal NeRF.","Thermal-NeRF achieves the best trade-off between detail preservation and noise smoothing and provides better synthesis performance than previous work.","Finally, we demonstrate that both modalities are beneficial to each other in 3D reconstruction."],"url":"http://arxiv.org/abs/2403.14053v1","category":"cs.CV"}
{"created":"2024-03-21 00:35:02","title":"Exact solutions and bifurcation curves of nonlocal elliptic equations with convolutional Kirchhoff functions","abstract":"We study the one-dimensional nonlocal elliptic equation of Kirchhoff type with convolutional Kirchhoff functions. We establish the exact solutions $u_\\lambda$ and bifurcation curves $\\lambda(\\alpha)$, where $\\alpha:= \\Vert u_\\lambda\\Vert_\\infty$.","sentences":["We study the one-dimensional nonlocal elliptic equation of Kirchhoff type with convolutional Kirchhoff functions.","We establish the exact solutions $u_\\lambda$ and bifurcation curves $\\lambda(\\alpha)$, where $\\alpha:= \\Vert u_\\lambda\\Vert_\\infty$."],"url":"http://arxiv.org/abs/2403.14052v1","category":"math.AP"}
{"created":"2024-03-20 22:09:50","title":"Algebraic structures on parallelizable manifolds","abstract":"In this paper we explore algebraic and geometric structures that arise on parallelizable manifolds. Given a parallelizable manifold $\\mathbb{L}$, there exists a global trivialization of the tangent bundle, which defines a map $\\rho_p:\\mathfrak{l} \\longrightarrow T_p \\mathbb{L}$ for each point $p \\in \\mathbb{L}$, where $\\mathfrak{l}$ is some vector space. This allows us to define a particular class of vector fields, known as fundamental vector fields, that correspond to each element of $\\mathfrak{l}$. Furthermore, flows of these vector fields give rise to a product between elements of $% \\mathfrak{l}$ and $\\mathbb{L}$, which in turn induces a local loop structure (i.e. a non-associative analog of a group). Furthermore, we also define a generalization of a Lie algebra structure on $\\mathfrak{l}$. We will describe the properties and examples of these constructions.","sentences":["In this paper we explore algebraic and geometric structures that arise on parallelizable manifolds.","Given a parallelizable manifold $\\mathbb{L}$, there exists a global trivialization of the tangent bundle, which defines a map $\\rho_p:\\mathfrak{l} \\longrightarrow T_p \\mathbb{L}$ for each point $p \\in \\mathbb{L}$, where $\\mathfrak{l}$ is some vector space.","This allows us to define a particular class of vector fields, known as fundamental vector fields, that correspond to each element of $\\mathfrak{l}$. Furthermore, flows of these vector fields give rise to a product between elements of $% \\mathfrak{l}$ and $\\mathbb{L}$, which in turn induces a local loop structure (i.e. a non-associative analog of a group).","Furthermore, we also define a generalization of a Lie algebra structure on $\\mathfrak{l}$. We will describe the properties and examples of these constructions."],"url":"http://arxiv.org/abs/2403.14005v1","category":"math.RA"}
{"created":"2024-03-20 21:57:14","title":"Visual Imitation Learning of Task-Oriented Object Grasping and Rearrangement","abstract":"Task-oriented object grasping and rearrangement are critical skills for robots to accomplish different real-world manipulation tasks. However, they remain challenging due to partial observations of the objects and shape variations in categorical objects. In this paper, we propose the Multi-feature Implicit Model (MIMO), a novel object representation that encodes multiple spatial features between a point and an object in an implicit neural field. Training such a model on multiple features ensures that it embeds the object shapes consistently in different aspects, thus improving its performance in object shape reconstruction from partial observation, shape similarity measure, and modeling spatial relations between objects. Based on MIMO, we propose a framework to learn task-oriented object grasping and rearrangement from single or multiple human demonstration videos. The evaluations in simulation show that our approach outperforms the state-of-the-art methods for multi- and single-view observations. Real-world experiments demonstrate the efficacy of our approach in one- and few-shot imitation learning of manipulation tasks.","sentences":["Task-oriented object grasping and rearrangement are critical skills for robots to accomplish different real-world manipulation tasks.","However, they remain challenging due to partial observations of the objects and shape variations in categorical objects.","In this paper, we propose the Multi-feature Implicit Model (MIMO), a novel object representation that encodes multiple spatial features between a point and an object in an implicit neural field.","Training such a model on multiple features ensures that it embeds the object shapes consistently in different aspects, thus improving its performance in object shape reconstruction from partial observation, shape similarity measure, and modeling spatial relations between objects.","Based on MIMO, we propose a framework to learn task-oriented object grasping and rearrangement from single or multiple human demonstration videos.","The evaluations in simulation show that our approach outperforms the state-of-the-art methods for multi- and single-view observations.","Real-world experiments demonstrate the efficacy of our approach in one- and few-shot imitation learning of manipulation tasks."],"url":"http://arxiv.org/abs/2403.14000v1","category":"cs.RO"}
{"created":"2024-03-20 21:56:36","title":"On the Z_2-valued index of elliptic odd symmetric operators on non-compact manifolds","abstract":"We investigate elliptic operators with a symmetry that forces their index to vanish. We study the secondary index, defined modulo 2. We examine Callias-type operators with this symmetry on non-compact manifolds and establish mod 2 versions of the Gromov-Lawson relative index theorem, the Callias index theorem, and the Boutet de Monvel's index theorem for Toeplitz operators.","sentences":["We investigate elliptic operators with a symmetry that forces their index to vanish.","We study the secondary index, defined modulo 2.","We examine Callias-type operators with this symmetry on non-compact manifolds and establish mod 2 versions of the Gromov-Lawson relative index theorem, the Callias index theorem, and the Boutet de Monvel's index theorem for Toeplitz operators."],"url":"http://arxiv.org/abs/2403.13999v1","category":"math.DG"}
{"created":"2024-03-20 21:55:57","title":"A Geometric flow towards hamiltonian stationary submanifolds","abstract":"In this paper, we introduce a geometric flow for Lagrangian submanifolds in a K\\\"ahler manifold that stays in its initial Hamiltonian isotopy class and is a gradient flow for volume. The stationary solutions are the Hamiltonian stationary Lagrangian submanifolds. The flow is not strictly parabolic but it corresponds to a fourth order strictly parabolic scalar equation in the cotangent bundle of the submanifold via Weinstein's Lagrangian neighborhood theorem. For any compact initial Lagrangian immersion, we establish short-time existence, uniqueness, and higher order estimates when the second fundamental forms are uniformly bounded up to time $T$.","sentences":["In this paper, we introduce a geometric flow for Lagrangian submanifolds in a K\\\"ahler manifold that stays in its initial Hamiltonian isotopy class and is a gradient flow for volume.","The stationary solutions are the Hamiltonian stationary Lagrangian submanifolds.","The flow is not strictly parabolic but it corresponds to a fourth order strictly parabolic scalar equation in the cotangent bundle of the submanifold via Weinstein's Lagrangian neighborhood theorem.","For any compact initial Lagrangian immersion, we establish short-time existence, uniqueness, and higher order estimates when the second fundamental forms are uniformly bounded up to time $T$."],"url":"http://arxiv.org/abs/2403.13997v1","category":"math.DG"}
{"created":"2024-03-20 21:36:44","title":"Singular Solutions for the Conformal Dirac-Einstein Problem on the Sphere","abstract":"In this paper we investigate the existence of singular solutions to the conformal Dirac-Einstein system. Because of its conformal invariance, there are many similarities with the classical construction of singular solutions for the Yamabe problem. We construct here a family of singular solutions, on the three-dimensional sphere, having exactly two singularities.","sentences":["In this paper we investigate the existence of singular solutions to the conformal Dirac-Einstein system.","Because of its conformal invariance, there are many similarities with the classical construction of singular solutions for the Yamabe problem.","We construct here a family of singular solutions, on the three-dimensional sphere, having exactly two singularities."],"url":"http://arxiv.org/abs/2403.13984v1","category":"math.DG"}
{"created":"2024-03-20 21:07:31","title":"Spectral Analysis of Lattice Schr\u00f6dinger-Type Operators Associated with the Nonstationary Anderson Model and Intermittency","abstract":"The research explores a high irregularity, commonly referred to as intermittency, of the solution to the non-stationary parabolic Anderson problem: \\begin{equation*}   \\frac{\\partial u}{\\partial t} = \\varkappa \\mathcal{L}u(t,x) + \\xi_{t}(x)u(t,x) \\end{equation*} with the initial condition \\(u(0,x) \\equiv 1\\), where \\((t,x) \\in [0,\\infty)\\times \\mathbb{Z}^d\\). Here, \\(\\varkappa \\mathcal{L}\\) denotes a non-local Laplacian, and \\(\\xi_{t}(x)\\) is a correlated white noise potential. The observed irregularity is intricately linked to the upper part of the spectrum of the multiparticle Schr\\\"{o}dinger equations for the moment functions \\(m_p(t,x_1,x_2,\\cdots,x_p) = \\langle u(t,x_1)u(t,x_2)\\cdots u(t,x_p)\\rangle\\).   In the first half of the paper, a weak form of intermittency is expressed through moment functions of order $p\\geq 3$ and established for a wide class of operators $\\varkappa \\mathcal{L}$ with a positive-definite correlator $B=B(x))$ of the white noise. In the second half of the paper, the strong intermittency is studied. It relates to the existence of a positive eigenvalue for the lattice Schr\\\"odinger type operator with the potential $B$. This operator is associated with the second moment $m_2$. Now $B$ is not necessarily positive-definite, but $\\sum B(x)\\geq 0$.","sentences":["The research explores a high irregularity, commonly referred to as intermittency, of the solution to the non-stationary parabolic Anderson problem: \\begin{equation*}   \\frac{\\partial u}{\\partial t} = \\varkappa \\mathcal{L}u(t,x) + \\xi_{t}(x)u(t,x) \\end{equation*} with the initial condition \\(u(0,x) \\equiv 1\\), where \\((t,x) \\in","[0,\\infty)\\times \\mathbb{Z}^d\\).","Here, \\(\\varkappa \\mathcal{L}\\) denotes a non-local Laplacian, and \\(\\xi_{t}(x)\\) is a correlated white noise potential.","The observed irregularity is intricately linked to the upper part of the spectrum of the multiparticle Schr\\\"{o}dinger equations for the moment functions \\(m_p(t,x_1,x_2,\\cdots,x_p) = \\langle u(t,x_1)u(t,x_2)\\cdots u(t,x_p)\\rangle\\).   ","In the first half of the paper, a weak form of intermittency is expressed through moment functions of order $p\\geq 3$ and established for a wide class of operators $\\varkappa \\mathcal{L}$ with a positive-definite correlator $B=B(x))$ of the white noise.","In the second half of the paper, the strong intermittency is studied.","It relates to the existence of a positive eigenvalue for the lattice Schr\\\"odinger type operator with the potential $B$. This operator is associated with the second moment $m_2$. Now $B$ is not necessarily positive-definite, but $\\sum B(x)\\geq 0$."],"url":"http://arxiv.org/abs/2403.13977v1","category":"math-ph"}
{"created":"2024-03-20 20:14:57","title":"A Gap in the Densities of Small Planets Orbiting M Dwarfs: Rigorous Statistical Confirmation Using the Open-source Code RhoPop","abstract":"Using mass-radius-composition models, small planets ($\\mathrm{R}\\lesssim 2 \\mathrm{R_\\oplus}$) are typically classified into three types: iron-rich, nominally Earth-like, and those with solid/liquid water and/or atmosphere. These classes are generally expected to be variations within a compositional continuum. Recently, however, Luque & Pall\\'e observed that potentially Earth-like planets around M dwarfs are separated from a lower-density population by a density gap. Meanwhile, the results of Adibekyan et al. hint that iron-rich planets around FGK stars are also a distinct population. It therefore remains unclear whether small planets represent a continuum or multiple distinct populations. Differentiating the nature of these populations will help constrain potential formation mechanisms. We present the RhoPop software for identifying small-planet populations. RhoPop employs mixture models in a hierarchical framework and a nested sampler for parameter and evidence estimates. Using RhoPop, we confirm the two populations of Luque & Pall\\'e with $>4\\sigma$ significance. The intrinsic scatter in the Earth-like subpopulation is roughly half that expected based on stellar abundance variations in local FGK stars, perhaps implying M dwarfs have a smaller spread in the major rock-building elements (Fe, Mg, Si) than FGK stars. We apply RhoPop to the Adibekyan et al. sample and find no evidence of more than one population. We estimate the sample size required to resolve a population of planets with Mercury-like compositions from those with Earth-like compositions for various mass-radius precisions. Only 16 planets are needed when $\\sigma_{M_p} = 5\\%$ and $\\sigma_{R_p} = 1\\%$. At $\\sigma_{M_p} = 10\\%$ and $\\sigma_{R_p} = 2.5\\%$, however, over 154 planets are needed, an order of magnitude increase.","sentences":["Using mass-radius-composition models, small planets ($\\mathrm{R}\\lesssim 2 \\mathrm{R_\\oplus}$) are typically classified into three types: iron-rich, nominally Earth-like, and those with solid/liquid water and/or atmosphere.","These classes are generally expected to be variations within a compositional continuum.","Recently, however, Luque & Pall\\'e observed that potentially Earth-like planets around M dwarfs are separated from a lower-density population by a density gap.","Meanwhile, the results of Adibekyan et al. hint that iron-rich planets around FGK stars are also a distinct population.","It therefore remains unclear whether small planets represent a continuum or multiple distinct populations.","Differentiating the nature of these populations will help constrain potential formation mechanisms.","We present the RhoPop software for identifying small-planet populations.","RhoPop employs mixture models in a hierarchical framework and a nested sampler for parameter and evidence estimates.","Using RhoPop, we confirm the two populations of Luque & Pall\\'e with $>4\\sigma$ significance.","The intrinsic scatter in the Earth-like subpopulation is roughly half that expected based on stellar abundance variations in local FGK stars, perhaps implying M dwarfs have a smaller spread in the major rock-building elements (Fe, Mg, Si) than FGK stars.","We apply RhoPop to the Adibekyan et al. sample and find no evidence of more than one population.","We estimate the sample size required to resolve a population of planets with Mercury-like compositions from those with Earth-like compositions for various mass-radius precisions.","Only 16 planets are needed when $\\sigma_{M_p} = 5\\%$ and $\\sigma_{R_p} = 1\\%$. At $\\sigma_{M_p} = 10\\%$ and $\\sigma_{R_p} = 2.5\\%$, however, over 154 planets are needed, an order of magnitude increase."],"url":"http://arxiv.org/abs/2403.13961v1","category":"astro-ph.EP"}
{"created":"2024-03-20 18:41:56","title":"$L^p$-bounds in Safarov pseudo-differential calculus on manifolds with bounded geometry","abstract":"Given a smooth complete Riemannian manifold with bounded geometry $(M,g)$ and a linear connection $\\nabla$ on it (not necessarily a metric one), we prove the $L^p$-boundedness of operators belonging to the global pseudo-differential classes $\\Psi_{\\rho, \\delta}^m\\left(\\Omega^\\kappa, \\nabla, \\tau\\right)$ constructed by Safarov. Our result recovers classical Fefferman's theorem, and extends it to the following two situations: $\\rho>1/3$ and $\\nabla$ symmetric; and $\\nabla$ flat with any values of $\\rho$ and $\\delta$. Moreover, as a consequence of our main result, we obtain boundedness on Sobolev and Besov spaces and some $L^p-L^q$ boundedness. Different examples and applications are presented.","sentences":["Given a smooth complete Riemannian manifold with bounded geometry $(M,g)$ and a linear connection $\\nabla$ on it (not necessarily a metric one), we prove the $L^p$-boundedness of operators belonging to the global pseudo-differential classes $\\Psi_{\\rho, \\delta}^m\\left(\\Omega^\\kappa, \\nabla, \\tau\\right)$ constructed by Safarov.","Our result recovers classical Fefferman's theorem, and extends it to the following two situations: $\\rho>1/3$ and $\\nabla$ symmetric; and $\\nabla$ flat with any values of $\\rho$ and $\\delta$.","Moreover, as a consequence of our main result, we obtain boundedness on Sobolev and Besov spaces and some $L^p-L^q$ boundedness.","Different examples and applications are presented."],"url":"http://arxiv.org/abs/2403.13920v1","category":"math.AP"}
{"created":"2024-03-20 18:40:53","title":"General vacuum solution of modified $F(G)$-gravity with Gauss-Bonnet term","abstract":"In this paper we present a general vacuum solution of the modified Gauss-Bonnet gravity equations for the Friedmann-Lema\\^itre-Robertson-Walker metric. We use an ansatz to reduce the gravitational equations to an ordinary differential equations for function $F=F(G)$. The solution obtained depends on an arbitrary function and is new. As an example we take an arbitrary function in the form of a power one and analyze the solutions for both $ F(G) $ and the main cosmological physical quantities as scale factor, Hubble rate, the Gauss-Bonnet term and the scalar curvature.","sentences":["In this paper we present a general vacuum solution of the modified Gauss-Bonnet gravity equations for the Friedmann-Lema\\^itre-Robertson-Walker metric.","We use an ansatz to reduce the gravitational equations to an ordinary differential equations for function $F=F(G)$.","The solution obtained depends on an arbitrary function and is new.","As an example we take an arbitrary function in the form of a power one and analyze the solutions for both $ F(G) $ and the main cosmological physical quantities as scale factor, Hubble rate, the Gauss-Bonnet term and the scalar curvature."],"url":"http://arxiv.org/abs/2403.13919v1","category":"gr-qc"}
{"created":"2024-03-20 18:31:49","title":"Entanglement, Soft Modes, and Celestial CFT","abstract":"We revisit the calculation of vacuum entanglement entropy in free Maxwell theory in four-dimensional Minkowski spacetime. Weyl invariance allows for this theory to be embedded as a patch inside the Einstein static universe. We use conformal inversions to extend the conformal primary solutions of the equations of motion labelled by a boost-weight $\\Delta = 1 + i\\lambda$ to an inverted Minkowski patch centered at spacelike infinity of the original patch. For $\\lambda \\neq 0$ these solutions admit an expansion in terms of wavefunctions supported in the (future) Milne wedges of the original and inverted Minkowski patches, that diagonalize the respective Milne times. The Minkowski vacuum can then be described as a thermofield double state on these two Milne wedges. We characterize the soft sectors of $\\lambda = 0$ modes supported in the two Milne wedges. Upon reinterpreting the non-pure gauge $\\lambda = 0$ wavefunctions as sourced by image charges in the inverse Minkowski patch, we construct an appropriate entangling constraint in the Minkowski theory, thereby characterizing the physical state space. We show that the edge mode contribution to the vacuum entanglement entropy is due to correlations between the soft charges of the two Milne patches, or equivalently non-trivial conformally soft mode configurations at the entangling surface.","sentences":["We revisit the calculation of vacuum entanglement entropy in free Maxwell theory in four-dimensional Minkowski spacetime.","Weyl invariance allows for this theory to be embedded as a patch inside the Einstein static universe.","We use conformal inversions to extend the conformal primary solutions of the equations of motion labelled by a boost-weight $\\Delta = 1 + i\\lambda$ to an inverted Minkowski patch centered at spacelike infinity of the original patch.","For $\\lambda \\neq 0$ these solutions admit an expansion in terms of wavefunctions supported in the (future)","Milne wedges of the original and inverted Minkowski patches, that diagonalize the respective Milne times.","The Minkowski vacuum can then be described as a thermofield double state on these two Milne wedges.","We characterize the soft sectors of $\\lambda = 0$ modes supported in the two Milne wedges.","Upon reinterpreting the non-pure gauge $\\lambda = 0$ wavefunctions as sourced by image charges in the inverse Minkowski patch, we construct an appropriate entangling constraint in the Minkowski theory, thereby characterizing the physical state space.","We show that the edge mode contribution to the vacuum entanglement entropy is due to correlations between the soft charges of the two Milne patches, or equivalently non-trivial conformally soft mode configurations at the entangling surface."],"url":"http://arxiv.org/abs/2403.13913v1","category":"hep-th"}
{"created":"2024-03-20 18:18:48","title":"Leveraging Linguistically Enhanced Embeddings for Open Information Extraction","abstract":"Open Information Extraction (OIE) is a structured prediction (SP) task in Natural Language Processing (NLP) that aims to extract structured $n$-ary tuples - usually subject-relation-object triples - from free text. The word embeddings in the input text can be enhanced with linguistic features, usually Part-of-Speech (PoS) and Syntactic Dependency Parse (SynDP) labels. However, past enhancement techniques cannot leverage the power of pretrained language models (PLMs), which themselves have been hardly used for OIE. To bridge this gap, we are the first to leverage linguistic features with a Seq2Seq PLM for OIE. We do so by introducing two methods - Weighted Addition and Linearized Concatenation. Our work can give any neural OIE architecture the key performance boost from both PLMs and linguistic features in one go. In our settings, this shows wide improvements of up to 24.9%, 27.3% and 14.9% on Precision, Recall and F1 scores respectively over the baseline. Beyond this, we address other important challenges in the field: to reduce compute overheads with the features, we are the first ones to exploit Semantic Dependency Parse (SemDP) tags; to address flaws in current datasets, we create a clean synthetic dataset; finally, we contribute the first known study of OIE behaviour in SP models.","sentences":["Open Information Extraction (OIE) is a structured prediction (SP) task in Natural Language Processing (NLP) that aims to extract structured $n$-ary tuples - usually subject-relation-object triples - from free text.","The word embeddings in the input text can be enhanced with linguistic features, usually Part-of-Speech (PoS) and Syntactic Dependency Parse (SynDP) labels.","However, past enhancement techniques cannot leverage the power of pretrained language models (PLMs), which themselves have been hardly used for OIE.","To bridge this gap, we are the first to leverage linguistic features with a Seq2Seq PLM for OIE.","We do so by introducing two methods - Weighted Addition and Linearized Concatenation.","Our work can give any neural OIE architecture the key performance boost from both PLMs and linguistic features in one go.","In our settings, this shows wide improvements of up to 24.9%, 27.3% and 14.9% on Precision, Recall and F1 scores respectively over the baseline.","Beyond this, we address other important challenges in the field: to reduce compute overheads with the features, we are the first ones to exploit Semantic Dependency Parse (SemDP) tags; to address flaws in current datasets, we create a clean synthetic dataset; finally, we contribute the first known study of OIE behaviour in SP models."],"url":"http://arxiv.org/abs/2403.13903v1","category":"cs.CL"}
{"created":"2024-03-20 18:03:14","title":"A scattering theory construction of dynamical solitons in 3d","abstract":"We study the energy critical wave equation in 3 dimensions around a single soliton. We obtain energy boundedness (modulo unstable modes) for the linearised problem. We use this to construct scattering solutions in a neighbourhood of timelike infinity ($i_+$), provided the data on null infinity ($\\scri$) decay polynomially. Moreover, the solutions we construct are conormal on a blow-up of Minkowski space. The methods of proof also extend to some energy supercritical modifications of the equation.","sentences":["We study the energy critical wave equation in 3 dimensions around a single soliton.","We obtain energy boundedness (modulo unstable modes) for the linearised problem.","We use this to construct scattering solutions in a neighbourhood of timelike infinity ($i_+$), provided the data on null infinity ($\\scri$) decay polynomially.","Moreover, the solutions we construct are conormal on a blow-up of Minkowski space.","The methods of proof also extend to some energy supercritical modifications of the equation."],"url":"http://arxiv.org/abs/2403.13891v1","category":"math.AP"}
{"created":"2024-03-20 18:00:03","title":"The DEHVILS in the Details: Type Ia Supernova Hubble Residual Comparisons and Mass Step Analysis in the Near-Infrared","abstract":"Measurements of Type Ia Supernovae (SNe Ia) in the near-infrared (NIR) have been used both as an alternate path to cosmology compared to optical measurements and as a method of constraining key systematics for the larger optical studies. With the DEHVILS sample, the largest published NIR sample with consistent NIR coverage of maximum light across three NIR bands ($Y$, $J$, and $H$), we check three key systematics: (i) the reduction in Hubble residual scatter as compared to the optical, (ii) the measurement of a \"mass step\" or lack thereof and its implications, and (iii) the ability to distinguish between various dust models by analyzing correlations between Hubble residuals in the NIR and optical. We produce accurate simulations of the DEHVILS sample and find, contrary to assumptions in the literature, it is $\\textit{harder}$ to differentiate between various dust models than previously understood. Additionally, we find that fitting with the current SALT3 model does not yield accurate wavelength-dependent stretch-luminosity correlations, and we propose a limited solution for this problem. From the data, we see that (i) the standard deviation of Hubble residual values from NIR bands treated as standard candles are 0.007-0.042 mag smaller than those in the optical, (ii) the NIR mass step is not constrainable with the current sample size from DEHVILS, and (iii) Hubble residuals in the NIR and optical are correlated in both the simulations and the data. We test a few variations on the number and combinations of filters and data samples, and we observe that none of our findings or conclusions are significantly impacted by these modifications.","sentences":["Measurements of Type Ia Supernovae (SNe Ia) in the near-infrared (NIR) have been used both as an alternate path to cosmology compared to optical measurements and as a method of constraining key systematics for the larger optical studies.","With the DEHVILS sample, the largest published NIR sample with consistent NIR coverage of maximum light across three NIR bands ($Y$, $J$, and $H$), we check three key systematics: (i) the reduction in Hubble residual scatter as compared to the optical, (ii) the measurement of a \"mass step\" or lack thereof and its implications, and (iii) the ability to distinguish between various dust models by analyzing correlations between Hubble residuals in the NIR and optical.","We produce accurate simulations of the DEHVILS sample and find, contrary to assumptions in the literature, it is $\\textit{harder}$ to differentiate between various dust models than previously understood.","Additionally, we find that fitting with the current SALT3 model does not yield accurate wavelength-dependent stretch-luminosity correlations, and we propose a limited solution for this problem.","From the data, we see that (i) the standard deviation of Hubble residual values from NIR bands treated as standard candles are 0.007-0.042 mag smaller than those in the optical, (ii) the NIR mass step is not constrainable with the current sample size from DEHVILS, and (iii) Hubble residuals in the NIR and optical are correlated in both the simulations and the data.","We test a few variations on the number and combinations of filters and data samples, and we observe that none of our findings or conclusions are significantly impacted by these modifications."],"url":"http://arxiv.org/abs/2403.13885v1","category":"astro-ph.CO"}
{"created":"2024-03-20 15:27:17","title":"Spatial-Temporal Graph Representation Learning for Tactical Networks Future State Prediction","abstract":"Resource allocation in tactical ad-hoc networks presents unique challenges due to their dynamic and multi-hop nature. Accurate prediction of future network connectivity is essential for effective resource allocation in such environments. In this paper, we introduce the Spatial-Temporal Graph Encoder-Decoder (STGED) framework for Tactical Communication Networks that leverages both spatial and temporal features of network states to learn latent tactical behaviors effectively. STGED hierarchically utilizes graph-based attention mechanism to spatially encode a series of communication network states, leverages a recurrent neural network to temporally encode the evolution of states, and a fully-connected feed-forward network to decode the connectivity in the future state. Through extensive experiments, we demonstrate that STGED consistently outperforms baseline models by large margins across different time-steps input, achieving an accuracy of up to 99.2\\% for the future state prediction task of tactical communication networks.","sentences":["Resource allocation in tactical ad-hoc networks presents unique challenges due to their dynamic and multi-hop nature.","Accurate prediction of future network connectivity is essential for effective resource allocation in such environments.","In this paper, we introduce the Spatial-Temporal Graph Encoder-Decoder (STGED) framework for Tactical Communication Networks that leverages both spatial and temporal features of network states to learn latent tactical behaviors effectively.","STGED hierarchically utilizes graph-based attention mechanism to spatially encode a series of communication network states, leverages a recurrent neural network to temporally encode the evolution of states, and a fully-connected feed-forward network to decode the connectivity in the future state.","Through extensive experiments, we demonstrate that STGED consistently outperforms baseline models by large margins across different time-steps input, achieving an accuracy of up to 99.2\\% for the future state prediction task of tactical communication networks."],"url":"http://arxiv.org/abs/2403.13872v1","category":"cs.LG"}
{"created":"2024-03-20 13:39:19","title":"Analysing heavy-tail properties of Stochastic Gradient Descent by means of Stochastic Recurrence Equations","abstract":"In recent works on the theory of machine learning, it has been observed that heavy tail properties of Stochastic Gradient Descent (SGD) can be studied in the probabilistic framework of stochastic recursions. In particular, G\\\"{u}rb\\\"{u}zbalaban et al. (arXiv:2006.04740) considered a setup corresponding to linear regression for which iterations of SGD can be modelled by a multivariate affine stochastic recursion $X_k=A_k X_{k-1}+B_k$, for independent and identically distributed pairs $(A_k, B_k)$, where $A_k$ is a random symmetric matrix and $B_k$ is a random vector. In this work, we will answer several open questions of the quoted paper and extend their results by applying the theory of irreducible-proximal (i-p) matrices.","sentences":["In recent works on the theory of machine learning, it has been observed that heavy tail properties of Stochastic Gradient Descent (SGD) can be studied in the probabilistic framework of stochastic recursions.","In particular, G\\\"{u}rb\\\"{u}zbalaban et al. (arXiv:2006.04740) considered a setup corresponding to linear regression for which iterations of SGD can be modelled by a multivariate affine stochastic recursion $X_k=A_k X_{k-1}+B_k$, for independent and identically distributed pairs $(A_k, B_k)$, where $A_k$ is a random symmetric matrix and $B_k$ is a random vector.","In this work, we will answer several open questions of the quoted paper and extend their results by applying the theory of irreducible-proximal (i-p) matrices."],"url":"http://arxiv.org/abs/2403.13868v1","category":"stat.ML"}
{"created":"2024-03-20 12:17:49","title":"Capsule Neural Networks as Noise Stabilizer for Time Series Data","abstract":"Capsule Neural Networks utilize capsules, which bind neurons into a single vector and learn position equivariant features, which makes them more robust than original Convolutional Neural Networks. CapsNets employ an affine transformation matrix and dynamic routing with coupling coefficients to learn robustly. In this paper, we investigate the effectiveness of CapsNets in analyzing highly sensitive and noisy time series sensor data. To demonstrate CapsNets robustness, we compare their performance with original CNNs on electrocardiogram data, a medical time series sensor data with complex patterns and noise. Our study provides empirical evidence that CapsNets function as noise stabilizers, as investigated by manual and adversarial attack experiments using the fast gradient sign method and three manual attacks, including offset shifting, gradual drift, and temporal lagging. In summary, CapsNets outperform CNNs in both manual and adversarial attacked data. Our findings suggest that CapsNets can be effectively applied to various sensor systems to improve their resilience to noise attacks. These results have significant implications for designing and implementing robust machine learning models in real world applications. Additionally, this study contributes to the effectiveness of CapsNet models in handling noisy data and highlights their potential for addressing the challenges of noise data in time series analysis.","sentences":["Capsule Neural Networks utilize capsules, which bind neurons into a single vector and learn position equivariant features, which makes them more robust than original Convolutional Neural Networks.","CapsNets employ an affine transformation matrix and dynamic routing with coupling coefficients to learn robustly.","In this paper, we investigate the effectiveness of CapsNets in analyzing highly sensitive and noisy time series sensor data.","To demonstrate CapsNets robustness, we compare their performance with original CNNs on electrocardiogram data, a medical time series sensor data with complex patterns and noise.","Our study provides empirical evidence that CapsNets function as noise stabilizers, as investigated by manual and adversarial attack experiments using the fast gradient sign method and three manual attacks, including offset shifting, gradual drift, and temporal lagging.","In summary, CapsNets outperform CNNs in both manual and adversarial attacked data.","Our findings suggest that CapsNets can be effectively applied to various sensor systems to improve their resilience to noise attacks.","These results have significant implications for designing and implementing robust machine learning models in real world applications.","Additionally, this study contributes to the effectiveness of CapsNet models in handling noisy data and highlights their potential for addressing the challenges of noise data in time series analysis."],"url":"http://arxiv.org/abs/2403.13867v1","category":"cs.LG"}
{"created":"2024-03-21 17:20:23","title":"A Transfer Learning Causal Approach to Evaluate Racial/Ethnic and Geographic Variation in Outcomes Following Congenital Heart Surgery","abstract":"Congenital heart defects (CHD) are the most prevalent birth defects in the United States and surgical outcomes vary considerably across the country. The outcomes of treatment for CHD differ for specific patient subgroups, with non-Hispanic Black and Hispanic populations experiencing higher rates of mortality and morbidity. A valid comparison of outcomes within racial/ethnic subgroups is difficult given large differences in case-mix and small subgroup sizes. We propose a causal inference framework for outcome assessment and leverage advances in transfer learning to incorporate data from both target and source populations to help estimate causal effects while accounting for different sources of risk factor and outcome differences across populations. Using the Society of Thoracic Surgeons' Congenital Heart Surgery Database (STS-CHSD), we focus on a national cohort of patients undergoing the Norwood operation from 2016-2022 to assess operative mortality and morbidity outcomes across U.S. geographic regions by race/ethnicity. We find racial and ethnic outcome differences after controlling for potential confounding factors. While geography does not have a causal effect on outcomes for non-Hispanic Caucasian patients, non-Hispanic Black patients experience wide variability in outcomes with estimated 30-day mortality ranging from 5.9% (standard error 2.2%) to 21.6% (4.4%) across U.S. regions.","sentences":["Congenital heart defects (CHD) are the most prevalent birth defects in the United States and surgical outcomes vary considerably across the country.","The outcomes of treatment for CHD differ for specific patient subgroups, with non-Hispanic Black and Hispanic populations experiencing higher rates of mortality and morbidity.","A valid comparison of outcomes within racial/ethnic subgroups is difficult given large differences in case-mix and small subgroup sizes.","We propose a causal inference framework for outcome assessment and leverage advances in transfer learning to incorporate data from both target and source populations to help estimate causal effects while accounting for different sources of risk factor and outcome differences across populations.","Using the Society of Thoracic Surgeons' Congenital Heart Surgery Database (STS-CHSD), we focus on a national cohort of patients undergoing the Norwood operation from 2016-2022 to assess operative mortality and morbidity outcomes across U.S. geographic regions by race/ethnicity.","We find racial and ethnic outcome differences after controlling for potential confounding factors.","While geography does not have a causal effect on outcomes for non-Hispanic Caucasian patients, non-Hispanic Black patients experience wide variability in outcomes with estimated 30-day mortality ranging from 5.9% (standard error 2.2%) to 21.6% (4.4%) across U.S. regions."],"url":"http://arxiv.org/abs/2403.14573v1","category":"stat.ME"}
{"created":"2024-03-21 17:05:38","title":"Looking Together $\\neq$ Seeing the Same Thing: Understanding Surgeons' Visual Needs During Intra-operative Coordination and Instruction","abstract":"Shared gaze visualizations have been found to enhance collaboration and communication outcomes in diverse HCI scenarios including computer supported collaborative work and learning contexts. Given the importance of gaze in surgery operations, especially when a surgeon trainer and trainee need to coordinate their actions, research on the use of gaze to facilitate intra-operative coordination and instruction has been limited and shows mixed implications. We performed a field observation of 8 surgeries and an interview study with 14 surgeons to understand their visual needs during operations, informing ways to leverage and augment gaze to enhance intra-operative coordination and instruction. We found that trainees have varying needs in receiving visual guidance which are often unfulfilled by the trainers' instructions. It is critical for surgeons to control the timing of the gaze-based visualizations and effectively interpret gaze data. We suggest overlay technologies, e.g., gaze-based summaries and depth sensing, to augment raw gaze in support of surgical coordination and instruction.","sentences":["Shared gaze visualizations have been found to enhance collaboration and communication outcomes in diverse HCI scenarios including computer supported collaborative work and learning contexts.","Given the importance of gaze in surgery operations, especially when a surgeon trainer and trainee need to coordinate their actions, research on the use of gaze to facilitate intra-operative coordination and instruction has been limited and shows mixed implications.","We performed a field observation of 8 surgeries and an interview study with 14 surgeons to understand their visual needs during operations, informing ways to leverage and augment gaze to enhance intra-operative coordination and instruction.","We found that trainees have varying needs in receiving visual guidance which are often unfulfilled by the trainers' instructions.","It is critical for surgeons to control the timing of the gaze-based visualizations and effectively interpret gaze data.","We suggest overlay technologies, e.g., gaze-based summaries and depth sensing, to augment raw gaze in support of surgical coordination and instruction."],"url":"http://arxiv.org/abs/2403.14561v1","category":"cs.HC"}
{"created":"2024-03-21 16:48:45","title":"Estimating Physical Information Consistency of Channel Data Augmentation for Remote Sensing Images","abstract":"The application of data augmentation for deep learning (DL) methods plays an important role in achieving state-of-the-art results in supervised, semi-supervised, and self-supervised image classification. In particular, channel transformations (e.g., solarize, grayscale, brightness adjustments) are integrated into data augmentation pipelines for remote sensing (RS) image classification tasks. However, contradicting beliefs exist about their proper applications to RS images. A common point of critique is that the application of channel augmentation techniques may lead to physically inconsistent spectral data (i.e., pixel signatures). To shed light on the open debate, we propose an approach to estimate whether a channel augmentation technique affects the physical information of RS images. To this end, the proposed approach estimates a score that measures the alignment of a pixel signature within a time series that can be naturally subject to deviations caused by factors such as acquisition conditions or phenological states of vegetation. We compare the scores associated with original and augmented pixel signatures to evaluate the physical consistency. Experimental results on a multi-label image classification task show that channel augmentations yielding a score that exceeds the expected deviation of original pixel signatures can not improve the performance of a baseline model trained without augmentation.","sentences":["The application of data augmentation for deep learning (DL) methods plays an important role in achieving state-of-the-art results in supervised, semi-supervised, and self-supervised image classification.","In particular, channel transformations (e.g., solarize, grayscale, brightness adjustments) are integrated into data augmentation pipelines for remote sensing (RS) image classification tasks.","However, contradicting beliefs exist about their proper applications to RS images.","A common point of critique is that the application of channel augmentation techniques may lead to physically inconsistent spectral data (i.e., pixel signatures).","To shed light on the open debate, we propose an approach to estimate whether a channel augmentation technique affects the physical information of RS images.","To this end, the proposed approach estimates a score that measures the alignment of a pixel signature within a time series that can be naturally subject to deviations caused by factors such as acquisition conditions or phenological states of vegetation.","We compare the scores associated with original and augmented pixel signatures to evaluate the physical consistency.","Experimental results on a multi-label image classification task show that channel augmentations yielding a score that exceeds the expected deviation of original pixel signatures can not improve the performance of a baseline model trained without augmentation."],"url":"http://arxiv.org/abs/2403.14547v1","category":"cs.CV"}
{"created":"2024-03-21 14:51:51","title":"More than Just Statistical Recurrence: Human and Machine Unsupervised Learning of M\u0101ori Word Segmentation across Morphological Processes","abstract":"Non-M\\=aori-speaking New Zealanders (NMS)are able to segment M\\=aori words in a highlysimilar way to fluent speakers (Panther et al.,2024). This ability is assumed to derive through the identification and extraction of statistically recurrent forms. We examine this assumption by asking how NMS segmentations compare to those produced by Morfessor, an unsupervised machine learning model that operates based on statistical recurrence, across words formed by a variety of morphological processes. Both NMS and Morfessor succeed in segmenting words formed by concatenative processes (compounding and affixation without allomorphy), but NMS also succeed for words that invoke templates (reduplication and allomorphy) and other cues to morphological structure, implying that their learning process is sensitive to more than just statistical recurrence.","sentences":["Non-M\\=aori-speaking New Zealanders (NMS)are able to segment M\\=aori words in a highlysimilar way to fluent speakers (Panther et al.,2024).","This ability is assumed to derive through the identification and extraction of statistically recurrent forms.","We examine this assumption by asking how NMS segmentations compare to those produced by Morfessor, an unsupervised machine learning model that operates based on statistical recurrence, across words formed by a variety of morphological processes.","Both NMS and Morfessor succeed in segmenting words formed by concatenative processes (compounding and affixation without allomorphy), but NMS also succeed for words that invoke templates (reduplication and allomorphy) and other cues to morphological structure, implying that their learning process is sensitive to more than just statistical recurrence."],"url":"http://arxiv.org/abs/2403.14444v1","category":"cs.CL"}
{"created":"2024-03-21 13:30:36","title":"Application of deep learning and inline holography to estimate the droplet size distribution","abstract":"We examine five machine learning-based architectures to estimate the droplet size distributions obtained using digital inline holography. The architectures, namely, U-Net, R2 U-Net, Attention U-Net, V-Net, and Residual U-Net are trained using synthetic holographic images. Our assessment focuses on evaluating the training, validation, and prediction performance of these architectures. We found that U-Net and R2 U-Net to be the most proficient, displaying consistent performance trends and achieving the highest Intersection Over Union (IOU) scores compared to the other three architectures. We employ additional training using experimental holographic images for the two top-performing architectures to validate their efficacy further. Subsequently, they are employed to segment an experimental dataset illustrating the bag breakup phenomenon, facilitating the extraction of size distribution. The extracted size distribution from U-Net and R2 U-Net segmentation is then compared with the analytical model proposed by \\cite{jackiw2022prediction} by employing the gamma and log-normal distributions. Our findings indicate that the gamma distribution provides a more accurate prediction of the multi-modal size distribution than the log-normal distribution owing to its long exponential tail. The present study offers valuable insights into the effectiveness of machine learning architectures in estimating particle/droplet sizes, highlighting their practical application in real-world experimental scenarios.","sentences":["We examine five machine learning-based architectures to estimate the droplet size distributions obtained using digital inline holography.","The architectures, namely, U-Net, R2 U-Net, Attention U-Net, V-Net, and Residual U-Net are trained using synthetic holographic images.","Our assessment focuses on evaluating the training, validation, and prediction performance of these architectures.","We found that U-Net and R2 U-Net to be the most proficient, displaying consistent performance trends and achieving the highest Intersection Over Union (IOU) scores compared to the other three architectures.","We employ additional training using experimental holographic images for the two top-performing architectures to validate their efficacy further.","Subsequently, they are employed to segment an experimental dataset illustrating the bag breakup phenomenon, facilitating the extraction of size distribution.","The extracted size distribution from U-Net and R2 U-Net segmentation is then compared with the analytical model proposed by \\cite{jackiw2022prediction} by employing the gamma and log-normal distributions.","Our findings indicate that the gamma distribution provides a more accurate prediction of the multi-modal size distribution than the log-normal distribution owing to its long exponential tail.","The present study offers valuable insights into the effectiveness of machine learning architectures in estimating particle/droplet sizes, highlighting their practical application in real-world experimental scenarios."],"url":"http://arxiv.org/abs/2403.14391v1","category":"physics.flu-dyn"}
{"created":"2024-03-21 12:40:41","title":"Varroa destructor detection on honey bees using hyperspectral imagery","abstract":"Hyperspectral (HS) imagery in agriculture is becoming increasingly common. These images have the advantage of higher spectral resolution. Advanced spectral processing techniques are required to unlock the information potential in these HS images. The present paper introduces a method rooted in multivariate statistics designed to detect parasitic Varroa destructor mites on the body of western honey bee Apis mellifera, enabling easier and continuous monitoring of the bee hives. The methodology explores unsupervised (K-means++) and recently developed supervised (Kernel Flows - Partial Least-Squares, KF-PLS) methods for parasitic identification. Additionally, in light of the emergence of custom-band multispectral cameras, the present research outlines a strategy for identifying the specific wavelengths necessary for effective bee-mite separation, suitable for implementation in a custom-band camera. Illustrated with a real-case dataset, our findings demonstrate that as few as four spectral bands are sufficient for accurate parasite identification.","sentences":["Hyperspectral (HS) imagery in agriculture is becoming increasingly common.","These images have the advantage of higher spectral resolution.","Advanced spectral processing techniques are required to unlock the information potential in these HS images.","The present paper introduces a method rooted in multivariate statistics designed to detect parasitic Varroa destructor mites on the body of western honey bee Apis mellifera, enabling easier and continuous monitoring of the bee hives.","The methodology explores unsupervised (K-means++) and recently developed supervised (Kernel Flows - Partial Least-Squares, KF-PLS) methods for parasitic identification.","Additionally, in light of the emergence of custom-band multispectral cameras, the present research outlines a strategy for identifying the specific wavelengths necessary for effective bee-mite separation, suitable for implementation in a custom-band camera.","Illustrated with a real-case dataset, our findings demonstrate that as few as four spectral bands are sufficient for accurate parasite identification."],"url":"http://arxiv.org/abs/2403.14359v1","category":"cs.CV"}
{"created":"2024-03-21 12:24:53","title":"On the Concept Trustworthiness in Concept Bottleneck Models","abstract":"Concept Bottleneck Models (CBMs), which break down the reasoning process into the input-to-concept mapping and the concept-to-label prediction, have garnered significant attention due to their remarkable interpretability achieved by the interpretable concept bottleneck. However, despite the transparency of the concept-to-label prediction, the mapping from the input to the intermediate concept remains a black box, giving rise to concerns about the trustworthiness of the learned concepts (i.e., these concepts may be predicted based on spurious cues). The issue of concept untrustworthiness greatly hampers the interpretability of CBMs, thereby hindering their further advancement. To conduct a comprehensive analysis on this issue, in this study we establish a benchmark to assess the trustworthiness of concepts in CBMs. A pioneering metric, referred to as concept trustworthiness score, is proposed to gauge whether the concepts are derived from relevant regions. Additionally, an enhanced CBM is introduced, enabling concept predictions to be made specifically from distinct parts of the feature map, thereby facilitating the exploration of their related regions. Besides, we introduce three modules, namely the cross-layer alignment (CLA) module, the cross-image alignment (CIA) module, and the prediction alignment (PA) module, to further enhance the concept trustworthiness within the elaborated CBM. The experiments on five datasets across ten architectures demonstrate that without using any concept localization annotations during training, our model improves the concept trustworthiness by a large margin, meanwhile achieving superior accuracy to the state-of-the-arts. Our code is available at https://github.com/hqhQAQ/ProtoCBM.","sentences":["Concept Bottleneck Models (CBMs), which break down the reasoning process into the input-to-concept mapping and the concept-to-label prediction, have garnered significant attention due to their remarkable interpretability achieved by the interpretable concept bottleneck.","However, despite the transparency of the concept-to-label prediction, the mapping from the input to the intermediate concept remains a black box, giving rise to concerns about the trustworthiness of the learned concepts (i.e., these concepts may be predicted based on spurious cues).","The issue of concept untrustworthiness greatly hampers the interpretability of CBMs, thereby hindering their further advancement.","To conduct a comprehensive analysis on this issue, in this study we establish a benchmark to assess the trustworthiness of concepts in CBMs.","A pioneering metric, referred to as concept trustworthiness score, is proposed to gauge whether the concepts are derived from relevant regions.","Additionally, an enhanced CBM is introduced, enabling concept predictions to be made specifically from distinct parts of the feature map, thereby facilitating the exploration of their related regions.","Besides, we introduce three modules, namely the cross-layer alignment (CLA) module, the cross-image alignment (CIA) module, and the prediction alignment (PA) module, to further enhance the concept trustworthiness within the elaborated CBM.","The experiments on five datasets across ten architectures demonstrate that without using any concept localization annotations during training, our model improves the concept trustworthiness by a large margin, meanwhile achieving superior accuracy to the state-of-the-arts.","Our code is available at https://github.com/hqhQAQ/ProtoCBM."],"url":"http://arxiv.org/abs/2403.14349v1","category":"cs.CV"}
{"created":"2024-03-21 12:23:29","title":"Towards Efficient Information Fusion: Concentric Dual Fusion Attention Based Multiple Instance Learning for Whole Slide Images","abstract":"In the realm of digital pathology, multi-magnification Multiple Instance Learning (multi-mag MIL) has proven effective in leveraging the hierarchical structure of Whole Slide Images (WSIs) to reduce information loss and redundant data. However, current methods fall short in bridging the domain gap between pretrained models and medical imaging, and often fail to account for spatial relationships across different magnifications. Addressing these challenges, we introduce the Concentric Dual Fusion Attention-MIL (CDFA-MIL) framework,which innovatively combines point-to-area feature-colum attention and point-to-point concentric-row attention using concentric patch. This approach is designed to effectively fuse correlated information, enhancing feature representation and providing stronger correlation guidance for WSI analysis. CDFA-MIL distinguishes itself by offering a robust fusion strategy that leads to superior WSI recognition. Its application has demonstrated exceptional performance, significantly surpassing existing MIL methods in accuracy and F1 scores on prominent datasets like Camelyon16 and TCGA-NSCLC. Specifically, CDFA-MIL achieved an average accuracy and F1-score of 93.7\\% and 94.1\\% respectively on these datasets, marking a notable advancement over traditional MIL approaches.","sentences":["In the realm of digital pathology, multi-magnification Multiple Instance Learning (multi-mag MIL) has proven effective in leveraging the hierarchical structure of Whole Slide Images (WSIs) to reduce information loss and redundant data.","However, current methods fall short in bridging the domain gap between pretrained models and medical imaging, and often fail to account for spatial relationships across different magnifications.","Addressing these challenges, we introduce the Concentric Dual Fusion Attention-MIL (CDFA-MIL) framework,which innovatively combines point-to-area feature-colum attention and point-to-point concentric-row attention using concentric patch.","This approach is designed to effectively fuse correlated information, enhancing feature representation and providing stronger correlation guidance for WSI analysis.","CDFA-MIL distinguishes itself by offering a robust fusion strategy that leads to superior WSI recognition.","Its application has demonstrated exceptional performance, significantly surpassing existing MIL methods in accuracy and F1 scores on prominent datasets like Camelyon16 and TCGA-NSCLC.","Specifically, CDFA-MIL achieved an average accuracy and F1-score of 93.7\\% and 94.1\\% respectively on these datasets, marking a notable advancement over traditional MIL approaches."],"url":"http://arxiv.org/abs/2403.14346v1","category":"cs.CV"}
{"created":"2024-03-21 09:28:38","title":"ERD: A Framework for Improving LLM Reasoning for Cognitive Distortion Classification","abstract":"Improving the accessibility of psychotherapy with the aid of Large Language Models (LLMs) is garnering a significant attention in recent years. Recognizing cognitive distortions from the interviewee's utterances can be an essential part of psychotherapy, especially for cognitive behavioral therapy. In this paper, we propose ERD, which improves LLM-based cognitive distortion classification performance with the aid of additional modules of (1) extracting the parts related to cognitive distortion, and (2) debating the reasoning steps by multiple agents. Our experimental results on a public dataset show that ERD improves the multi-class F1 score as well as binary specificity score. Regarding the latter score, it turns out that our method is effective in debiasing the baseline method which has high false positive rate, especially when the summary of multi-agent debate is provided to LLMs.","sentences":["Improving the accessibility of psychotherapy with the aid of Large Language Models (LLMs) is garnering a significant attention in recent years.","Recognizing cognitive distortions from the interviewee's utterances can be an essential part of psychotherapy, especially for cognitive behavioral therapy.","In this paper, we propose ERD, which improves LLM-based cognitive distortion classification performance with the aid of additional modules of (1) extracting the parts related to cognitive distortion, and (2) debating the reasoning steps by multiple agents.","Our experimental results on a public dataset show that ERD improves the multi-class F1 score as well as binary specificity score.","Regarding the latter score, it turns out that our method is effective in debiasing the baseline method which has high false positive rate, especially when the summary of multi-agent debate is provided to LLMs."],"url":"http://arxiv.org/abs/2403.14255v1","category":"cs.CL"}
{"created":"2024-03-21 08:52:39","title":"RG-CAT: Detection Pipeline and Catalogue of Radio Galaxies in the EMU Pilot Survey","abstract":"We present source detection and catalogue construction pipelines to build the first catalogue of radio galaxies from the 270 $\\rm deg^2$ pilot survey of the Evolutionary Map of the Universe (EMU-PS) conducted with the Australian Square Kilometre Array Pathfinder (ASKAP) telescope. The detection pipeline uses Gal-DINO computer-vision networks (Gupta et al., 2024) to predict the categories of radio morphology and bounding boxes for radio sources, as well as their potential infrared host positions. The Gal-DINO network is trained and evaluated on approximately 5,000 visually inspected radio galaxies and their infrared hosts, encompassing both compact and extended radio morphologies. We find that the Intersection over Union (IoU) for the predicted and ground truth bounding boxes is larger than 0.5 for 99% of the radio sources, and 98% of predicted host positions are within $3^{\\prime \\prime}$ of the ground truth infrared host in the evaluation set. The catalogue construction pipeline uses the predictions of the trained network on the radio and infrared image cutouts based on the catalogue of radio components identified using the Selavy source finder algorithm. Confidence scores of the predictions are then used to prioritize Selavy components with higher scores and incorporate them first into the catalogue. This results in identifications for a total of 211,625 radio sources, with 201,211 classified as compact and unresolved. The remaining 10,414 are categorized as extended radio morphologies, including 582 FR-I, 5,602 FR-II, 1,494 FR-x (uncertain whether FR-I or FR-II), 2,375 R (single-peak resolved) radio galaxies, and 361 with peculiar and other rare morphologies. We cross-match the radio sources in the catalogue with the infrared and optical catalogues, finding infrared cross-matches for 73% and photometric redshifts for 36% of the radio galaxies.","sentences":["We present source detection and catalogue construction pipelines to build the first catalogue of radio galaxies from the 270 $\\rm deg^2$ pilot survey of the Evolutionary Map of the Universe (EMU-PS) conducted with the Australian Square Kilometre Array Pathfinder (ASKAP) telescope.","The detection pipeline uses Gal-DINO computer-vision networks (Gupta et al., 2024) to predict the categories of radio morphology and bounding boxes for radio sources, as well as their potential infrared host positions.","The Gal-DINO network is trained and evaluated on approximately 5,000 visually inspected radio galaxies and their infrared hosts, encompassing both compact and extended radio morphologies.","We find that the Intersection over Union (IoU) for the predicted and ground truth bounding boxes is larger than 0.5 for 99% of the radio sources, and 98% of predicted host positions are within $3^{\\prime \\prime}$ of the ground truth infrared host in the evaluation set.","The catalogue construction pipeline uses the predictions of the trained network on the radio and infrared image cutouts based on the catalogue of radio components identified using the Selavy source finder algorithm.","Confidence scores of the predictions are then used to prioritize Selavy components with higher scores and incorporate them first into the catalogue.","This results in identifications for a total of 211,625 radio sources, with 201,211 classified as compact and unresolved.","The remaining 10,414 are categorized as extended radio morphologies, including 582 FR-I, 5,602 FR-II, 1,494 FR-x (uncertain whether FR-I or FR-II), 2,375 R (single-peak resolved) radio galaxies, and 361 with peculiar and other rare morphologies.","We cross-match the radio sources in the catalogue with the infrared and optical catalogues, finding infrared cross-matches for 73% and photometric redshifts for 36% of the radio galaxies."],"url":"http://arxiv.org/abs/2403.14235v1","category":"astro-ph.GA"}
{"created":"2024-03-21 08:39:13","title":"Recovering Latent Confounders from High-dimensional Proxy Variables","abstract":"Detecting latent confounders from proxy variables is an essential problem in causal effect estimation. Previous approaches are limited to low-dimensional proxies, sorted proxies, and binary treatments. We remove these assumptions and present a novel Proxy Confounder Factorization (PCF) framework for continuous treatment effect estimation when latent confounders manifest through high-dimensional, mixed proxy variables. For specific sample sizes, our two-step PCF implementation, using Independent Component Analysis (ICA-PCF), and the end-to-end implementation, using Gradient Descent (GD-PCF), achieve high correlation with the latent confounder and low absolute error in causal effect estimation with synthetic datasets in the high sample size regime. Even when faced with climate data, ICA-PCF recovers four components that explain $75.9\\%$ of the variance in the North Atlantic Oscillation, a known confounder of precipitation patterns in Europe. Code for our PCF implementations and experiments can be found here: https://github.com/IPL-UV/confound_it. The proposed methodology constitutes a stepping stone towards discovering latent confounders and can be applied to many problems in disciplines dealing with high-dimensional observed proxies, e.g., spatiotemporal fields.","sentences":["Detecting latent confounders from proxy variables is an essential problem in causal effect estimation.","Previous approaches are limited to low-dimensional proxies, sorted proxies, and binary treatments.","We remove these assumptions and present a novel Proxy Confounder Factorization (PCF) framework for continuous treatment effect estimation when latent confounders manifest through high-dimensional, mixed proxy variables.","For specific sample sizes, our two-step PCF implementation, using Independent Component Analysis (ICA-PCF), and the end-to-end implementation, using Gradient Descent (GD-PCF), achieve high correlation with the latent confounder and low absolute error in causal effect estimation with synthetic datasets in the high sample size regime.","Even when faced with climate data, ICA-PCF recovers four components that explain $75.9\\%$ of the variance in the North Atlantic Oscillation, a known confounder of precipitation patterns in Europe.","Code for our PCF implementations and experiments can be found here: https://github.com/IPL-UV/confound_it.","The proposed methodology constitutes a stepping stone towards discovering latent confounders and can be applied to many problems in disciplines dealing with high-dimensional observed proxies, e.g., spatiotemporal fields."],"url":"http://arxiv.org/abs/2403.14228v1","category":"stat.ML"}
{"created":"2024-03-21 08:22:44","title":"Large-Scale Label Interpretation Learning for Few-Shot Named Entity Recognition","abstract":"Few-shot named entity recognition (NER) detects named entities within text using only a few annotated examples. One promising line of research is to leverage natural language descriptions of each entity type: the common label PER might, for example, be verbalized as ''person entity.'' In an initial label interpretation learning phase, the model learns to interpret such verbalized descriptions of entity types. In a subsequent few-shot tagset extension phase, this model is then given a description of a previously unseen entity type (such as ''music album'') and optionally a few training examples to perform few-shot NER for this type. In this paper, we systematically explore the impact of a strong semantic prior to interpret verbalizations of new entity types by massively scaling up the number and granularity of entity types used for label interpretation learning. To this end, we leverage an entity linking benchmark to create a dataset with orders of magnitude of more distinct entity types and descriptions as currently used datasets. We find that this increased signal yields strong results in zero- and few-shot NER in in-domain, cross-domain, and even cross-lingual settings. Our findings indicate significant potential for improving few-shot NER through heuristical data-based optimization.","sentences":["Few-shot named entity recognition (NER) detects named entities within text using only a few annotated examples.","One promising line of research is to leverage natural language descriptions of each entity type: the common label PER might, for example, be verbalized as ''person entity.''","In an initial label interpretation learning phase, the model learns to interpret such verbalized descriptions of entity types.","In a subsequent few-shot tagset extension phase, this model is then given a description of a previously unseen entity type (such as ''music album'') and optionally a few training examples to perform few-shot NER for this type.","In this paper, we systematically explore the impact of a strong semantic prior to interpret verbalizations of new entity types by massively scaling up the number and granularity of entity types used for label interpretation learning.","To this end, we leverage an entity linking benchmark to create a dataset with orders of magnitude of more distinct entity types and descriptions as currently used datasets.","We find that this increased signal yields strong results in zero- and few-shot NER in in-domain, cross-domain, and even cross-lingual settings.","Our findings indicate significant potential for improving few-shot NER through heuristical data-based optimization."],"url":"http://arxiv.org/abs/2403.14222v1","category":"cs.CL"}
{"created":"2024-03-21 05:48:48","title":"Efficient Video Diffusion Models via Content-Frame Motion-Latent Decomposition","abstract":"Video diffusion models have recently made great progress in generation quality, but are still limited by the high memory and computational requirements. This is because current video diffusion models often attempt to process high-dimensional videos directly. To tackle this issue, we propose content-motion latent diffusion model (CMD), a novel efficient extension of pretrained image diffusion models for video generation. Specifically, we propose an autoencoder that succinctly encodes a video as a combination of a content frame (like an image) and a low-dimensional motion latent representation. The former represents the common content, and the latter represents the underlying motion in the video, respectively. We generate the content frame by fine-tuning a pretrained image diffusion model, and we generate the motion latent representation by training a new lightweight diffusion model. A key innovation here is the design of a compact latent space that can directly utilizes a pretrained image diffusion model, which has not been done in previous latent video diffusion models. This leads to considerably better quality generation and reduced computational costs. For instance, CMD can sample a video 7.7$\\times$ faster than prior approaches by generating a video of 512$\\times$1024 resolution and length 16 in 3.1 seconds. Moreover, CMD achieves an FVD score of 212.7 on WebVid-10M, 27.3% better than the previous state-of-the-art of 292.4.","sentences":["Video diffusion models have recently made great progress in generation quality, but are still limited by the high memory and computational requirements.","This is because current video diffusion models often attempt to process high-dimensional videos directly.","To tackle this issue, we propose content-motion latent diffusion model (CMD), a novel efficient extension of pretrained image diffusion models for video generation.","Specifically, we propose an autoencoder that succinctly encodes a video as a combination of a content frame (like an image) and a low-dimensional motion latent representation.","The former represents the common content, and the latter represents the underlying motion in the video, respectively.","We generate the content frame by fine-tuning a pretrained image diffusion model, and we generate the motion latent representation by training a new lightweight diffusion model.","A key innovation here is the design of a compact latent space that can directly utilizes a pretrained image diffusion model, which has not been done in previous latent video diffusion models.","This leads to considerably better quality generation and reduced computational costs.","For instance, CMD can sample a video 7.7$\\times$ faster than prior approaches by generating a video of 512$\\times$1024 resolution and length 16 in 3.1 seconds.","Moreover, CMD achieves an FVD score of 212.7 on WebVid-10M, 27.3% better than the previous state-of-the-art of 292.4."],"url":"http://arxiv.org/abs/2403.14148v1","category":"cs.CV"}
{"created":"2024-03-21 05:13:12","title":"Improving Image Classification Accuracy through Complementary Intra-Class and Inter-Class Mixup","abstract":"MixUp and its variants, such as Manifold MixUp, have two key limitations in image classification tasks. First, they often neglect mixing within the same class (intra-class mixup), leading to an underutilization of the relationships among samples within the same class. Second, although these methods effectively enhance inter-class separability by mixing between different classes (inter-class mixup), they fall short in improving intra-class cohesion through their mixing operations, limiting their classification performance. To tackle these issues, we propose a novel mixup method and a comprehensive integrated solution.Our mixup approach specifically targets intra-class mixup, an aspect commonly overlooked, to strengthen intra-class cohesion-a feature not provided by current mixup techniques.For each mini-batch, our method utilizes feature representations of unaugmented original images from each class within the mini-batch to generate a single synthesized feature representation through random linear interpolation. All synthesized representations for this mini-batch are then fed into the classification and loss layers to calculate an average classification loss that can markedly enhance intra-class cohesion. Moreover, our integrated solution seamlessly combines our intra-class mixup method with an existing mixup approach such as MixUp or Manifold MixUp. This comprehensive solution incorporates inter- and intra-class mixup in a balanced manner while concurrently improving intra-class cohesion and inter-class separability. Experimental results on six public datasets demonstrate that our integrated solution achieves a 0.1% to 3.43% higher accuracy than the best of either MixUp or our intra-class mixup method, averaging a 1.16% gain. It also outperforms the better performer of either Manifold MixUp or our intra-class mixup method by 0.12% to 5.16%, with an average gain of 1.11%.","sentences":["MixUp and its variants, such as Manifold MixUp, have two key limitations in image classification tasks.","First, they often neglect mixing within the same class (intra-class mixup), leading to an underutilization of the relationships among samples within the same class.","Second, although these methods effectively enhance inter-class separability by mixing between different classes (inter-class mixup), they fall short in improving intra-class cohesion through their mixing operations, limiting their classification performance.","To tackle these issues, we propose a novel mixup method and a comprehensive integrated solution.","Our mixup approach specifically targets intra-class mixup, an aspect commonly overlooked, to strengthen intra-class cohesion-a feature not provided by current mixup techniques.","For each mini-batch, our method utilizes feature representations of unaugmented original images from each class within the mini-batch to generate a single synthesized feature representation through random linear interpolation.","All synthesized representations for this mini-batch are then fed into the classification and loss layers to calculate an average classification loss that can markedly enhance intra-class cohesion.","Moreover, our integrated solution seamlessly combines our intra-class mixup method with an existing mixup approach such as MixUp or Manifold MixUp.","This comprehensive solution incorporates inter- and intra-class mixup in a balanced manner while concurrently improving intra-class cohesion and inter-class separability.","Experimental results on six public datasets demonstrate that our integrated solution achieves a 0.1% to 3.43% higher accuracy than the best of either MixUp or our intra-class mixup method, averaging a 1.16% gain.","It also outperforms the better performer of either Manifold MixUp or our intra-class mixup method by 0.12% to 5.16%, with an average gain of 1.11%."],"url":"http://arxiv.org/abs/2403.14137v1","category":"cs.CV"}
{"created":"2024-03-21 04:07:40","title":"From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation","abstract":"Machine Translation Quality Estimation (MTQE) is the task of estimating the quality of machine-translated text in real time without the need for reference translations, which is of great importance for the development of MT. After two decades of evolution, QE has yielded a wealth of results. This article provides a comprehensive overview of QE datasets, annotation methods, shared tasks, methodologies, challenges, and future research directions. It begins with an introduction to the background and significance of QE, followed by an explanation of the concepts and evaluation metrics for word-level QE, sentence-level QE, document-level QE, and explainable QE. The paper categorizes the methods developed throughout the history of QE into those based on handcrafted features, deep learning, and Large Language Models (LLMs), with a further division of deep learning-based methods into classic deep learning and those incorporating pre-trained language models (LMs). Additionally, the article details the advantages and limitations of each method and offers a straightforward comparison of different approaches. Finally, the paper discusses the current challenges in QE research and provides an outlook on future research directions.","sentences":["Machine Translation Quality Estimation (MTQE) is the task of estimating the quality of machine-translated text in real time without the need for reference translations, which is of great importance for the development of MT.","After two decades of evolution, QE has yielded a wealth of results.","This article provides a comprehensive overview of QE datasets, annotation methods, shared tasks, methodologies, challenges, and future research directions.","It begins with an introduction to the background and significance of QE, followed by an explanation of the concepts and evaluation metrics for word-level QE, sentence-level QE, document-level QE, and explainable QE.","The paper categorizes the methods developed throughout the history of QE into those based on handcrafted features, deep learning, and Large Language Models (LLMs), with a further division of deep learning-based methods into classic deep learning and those incorporating pre-trained language models (LMs).","Additionally, the article details the advantages and limitations of each method and offers a straightforward comparison of different approaches.","Finally, the paper discusses the current challenges in QE research and provides an outlook on future research directions."],"url":"http://arxiv.org/abs/2403.14118v1","category":"cs.CL"}
{"created":"2024-03-21 03:56:24","title":"Spatio-Temporal Proximity-Aware Dual-Path Model for Panoramic Activity Recognition","abstract":"Panoramic Activity Recognition (PAR) seeks to identify diverse human activities across different scales, from individual actions to social group and global activities in crowded panoramic scenes. PAR presents two major challenges: 1) recognizing the nuanced interactions among numerous individuals and 2) understanding multi-granular human activities. To address these, we propose Social Proximity-aware Dual-Path Network (SPDP-Net) based on two key design principles. First, while previous works often focus on spatial distance among individuals within an image, we argue to consider the spatio-temporal proximity. It is crucial for individual relation encoding to correctly understand social dynamics. Secondly, deviating from existing hierarchical approaches (individual-to-social-to-global activity), we introduce a dual-path architecture for multi-granular activity recognition. This architecture comprises individual-to-global and individual-to-social paths, mutually reinforcing each other's task with global-local context through multiple layers. Through extensive experiments, we validate the effectiveness of the spatio-temporal proximity among individuals and the dual-path architecture in PAR. Furthermore, SPDP-Net achieves new state-of-the-art performance with 46.5\\% of overall F1 score on JRDB-PAR dataset.","sentences":["Panoramic Activity Recognition (PAR) seeks to identify diverse human activities across different scales, from individual actions to social group and global activities in crowded panoramic scenes.","PAR presents two major challenges: 1) recognizing the nuanced interactions among numerous individuals and 2) understanding multi-granular human activities.","To address these, we propose Social Proximity-aware Dual-Path Network (SPDP-Net) based on two key design principles.","First, while previous works often focus on spatial distance among individuals within an image, we argue to consider the spatio-temporal proximity.","It is crucial for individual relation encoding to correctly understand social dynamics.","Secondly, deviating from existing hierarchical approaches (individual-to-social-to-global activity), we introduce a dual-path architecture for multi-granular activity recognition.","This architecture comprises individual-to-global and individual-to-social paths, mutually reinforcing each other's task with global-local context through multiple layers.","Through extensive experiments, we validate the effectiveness of the spatio-temporal proximity among individuals and the dual-path architecture in PAR.","Furthermore, SPDP-Net achieves new state-of-the-art performance with 46.5\\% of overall F1 score on JRDB-PAR dataset."],"url":"http://arxiv.org/abs/2403.14113v1","category":"cs.CV"}
{"created":"2024-03-21 03:47:26","title":"HETAL: Efficient Privacy-preserving Transfer Learning with Homomorphic Encryption","abstract":"Transfer learning is a de facto standard method for efficiently training machine learning models for data-scarce problems by adding and fine-tuning new classification layers to a model pre-trained on large datasets. Although numerous previous studies proposed to use homomorphic encryption to resolve the data privacy issue in transfer learning in the machine learning as a service setting, most of them only focused on encrypted inference. In this study, we present HETAL, an efficient Homomorphic Encryption based Transfer Learning algorithm, that protects the client's privacy in training tasks by encrypting the client data using the CKKS homomorphic encryption scheme. HETAL is the first practical scheme that strictly provides encrypted training, adopting validation-based early stopping and achieving the accuracy of nonencrypted training. We propose an efficient encrypted matrix multiplication algorithm, which is 1.8 to 323 times faster than prior methods, and a highly precise softmax approximation algorithm with increased coverage. The experimental results for five well-known benchmark datasets show total training times of 567-3442 seconds, which is less than an hour.","sentences":["Transfer learning is a de facto standard method for efficiently training machine learning models for data-scarce problems by adding and fine-tuning new classification layers to a model pre-trained on large datasets.","Although numerous previous studies proposed to use homomorphic encryption to resolve the data privacy issue in transfer learning in the machine learning as a service setting, most of them only focused on encrypted inference.","In this study, we present HETAL, an efficient Homomorphic Encryption based Transfer Learning algorithm, that protects the client's privacy in training tasks by encrypting the client data using the CKKS homomorphic encryption scheme.","HETAL is the first practical scheme that strictly provides encrypted training, adopting validation-based early stopping and achieving the accuracy of nonencrypted training.","We propose an efficient encrypted matrix multiplication algorithm, which is 1.8 to 323 times faster than prior methods, and a highly precise softmax approximation algorithm with increased coverage.","The experimental results for five well-known benchmark datasets show total training times of 567-3442 seconds, which is less than an hour."],"url":"http://arxiv.org/abs/2403.14111v1","category":"cs.CR"}
{"created":"2024-03-21 03:24:01","title":"Text-Enhanced Data-free Approach for Federated Class-Incremental Learning","abstract":"Federated Class-Incremental Learning (FCIL) is an underexplored yet pivotal issue, involving the dynamic addition of new classes in the context of federated learning. In this field, Data-Free Knowledge Transfer (DFKT) plays a crucial role in addressing catastrophic forgetting and data privacy problems. However, prior approaches lack the crucial synergy between DFKT and the model training phases, causing DFKT to encounter difficulties in generating high-quality data from a non-anchored latent space of the old task model. In this paper, we introduce LANDER (Label Text Centered Data-Free Knowledge Transfer) to address this issue by utilizing label text embeddings (LTE) produced by pretrained language models. Specifically, during the model training phase, our approach treats LTE as anchor points and constrains the feature embeddings of corresponding training samples around them, enriching the surrounding area with more meaningful information. In the DFKT phase, by using these LTE anchors, LANDER can synthesize more meaningful samples, thereby effectively addressing the forgetting problem. Additionally, instead of tightly constraining embeddings toward the anchor, the Bounding Loss is introduced to encourage sample embeddings to remain flexible within a defined radius. This approach preserves the natural differences in sample embeddings and mitigates the embedding overlap caused by heterogeneous federated settings. Extensive experiments conducted on CIFAR100, Tiny-ImageNet, and ImageNet demonstrate that LANDER significantly outperforms previous methods and achieves state-of-the-art performance in FCIL. The code is available at https://github.com/tmtuan1307/lander.","sentences":["Federated Class-Incremental Learning (FCIL) is an underexplored yet pivotal issue, involving the dynamic addition of new classes in the context of federated learning.","In this field, Data-Free Knowledge Transfer (DFKT) plays a crucial role in addressing catastrophic forgetting and data privacy problems.","However, prior approaches lack the crucial synergy between DFKT and the model training phases, causing DFKT to encounter difficulties in generating high-quality data from a non-anchored latent space of the old task model.","In this paper, we introduce LANDER (Label Text Centered Data-Free Knowledge Transfer) to address this issue by utilizing label text embeddings (LTE) produced by pretrained language models.","Specifically, during the model training phase, our approach treats LTE as anchor points and constrains the feature embeddings of corresponding training samples around them, enriching the surrounding area with more meaningful information.","In the DFKT phase, by using these LTE anchors, LANDER can synthesize more meaningful samples, thereby effectively addressing the forgetting problem.","Additionally, instead of tightly constraining embeddings toward the anchor, the Bounding Loss is introduced to encourage sample embeddings to remain flexible within a defined radius.","This approach preserves the natural differences in sample embeddings and mitigates the embedding overlap caused by heterogeneous federated settings.","Extensive experiments conducted on CIFAR100, Tiny-ImageNet, and ImageNet demonstrate that LANDER significantly outperforms previous methods and achieves state-of-the-art performance in FCIL.","The code is available at https://github.com/tmtuan1307/lander."],"url":"http://arxiv.org/abs/2403.14101v1","category":"cs.CV"}
{"created":"2024-03-21 02:54:14","title":"Evidence for an Outer Component in the Continuum Reverberation Mapping of Active Galactic Nuclei","abstract":"The continuum reverberation mapping is widely used in studying accretion disk of active galactic nuclei (AGN). While some indirect evidence and simulations indicated that the diffuse continuum, especially the strong Balmer continuum from the broad line region (BLR), may contribute to the continuum in the u/U band. Here, we present direct evidence for this contribution. In this work, we apply the ICCF-Cut method to continuum reverberation mapping to extract the possible diffuse continuum light curves of 6 AGNs with high cadence, high quality and multi-band observations. We find the existence of an outer component out of the accretion disk for each of 6 AGNs in the Swift U band. Meanwhile, similar results can be derived by JAVELIN Photometric Reverberation Mapping Model for 4 of them. The lags of the outer components are consistent with the predicted Balmer continuum lags, which are about half of the H$\\beta$ lag values. Our result directly reinforces that an outer component, especially the Balmer continuum in the rest-frame u/U band, can contribute significantly to the continuum reverberation lags of AGNs.","sentences":["The continuum reverberation mapping is widely used in studying accretion disk of active galactic nuclei (AGN).","While some indirect evidence and simulations indicated that the diffuse continuum, especially the strong Balmer continuum from the broad line region (BLR), may contribute to the continuum in the u/U band.","Here, we present direct evidence for this contribution.","In this work, we apply the ICCF-Cut method to continuum reverberation mapping to extract the possible diffuse continuum light curves of 6 AGNs with high cadence, high quality and multi-band observations.","We find the existence of an outer component out of the accretion disk for each of 6 AGNs in the Swift U band.","Meanwhile, similar results can be derived by JAVELIN Photometric Reverberation Mapping Model for 4 of them.","The lags of the outer components are consistent with the predicted Balmer continuum lags, which are about half of the H$\\beta$ lag values.","Our result directly reinforces that an outer component, especially the Balmer continuum in the rest-frame u/U band, can contribute significantly to the continuum reverberation lags of AGNs."],"url":"http://arxiv.org/abs/2403.14091v1","category":"astro-ph.GA"}
{"created":"2024-03-21 02:44:08","title":"Protein Conformation Generation via Force-Guided SE(3) Diffusion Models","abstract":"The conformational landscape of proteins is crucial to understanding their functionality in complex biological processes. Traditional physics-based computational methods, such as molecular dynamics (MD) simulations, suffer from rare event sampling and long equilibration time problems, hindering their applications in general protein systems. Recently, deep generative modeling techniques, especially diffusion models, have been employed to generate novel protein conformations. However, existing score-based diffusion methods cannot properly incorporate important physical prior knowledge to guide the generation process, causing large deviations in the sampled protein conformations from the equilibrium distribution. In this paper, to overcome these limitations, we propose a force-guided SE(3) diffusion model, ConfDiff, for protein conformation generation. By incorporating a force-guided network with a mixture of data-based score models, ConfDiff can can generate protein conformations with rich diversity while preserving high fidelity. Experiments on a variety of protein conformation prediction tasks, including 12 fast-folding proteins and the Bovine Pancreatic Trypsin Inhibitor (BPTI), demonstrate that our method surpasses the state-of-the-art method.","sentences":["The conformational landscape of proteins is crucial to understanding their functionality in complex biological processes.","Traditional physics-based computational methods, such as molecular dynamics (MD) simulations, suffer from rare event sampling and long equilibration time problems, hindering their applications in general protein systems.","Recently, deep generative modeling techniques, especially diffusion models, have been employed to generate novel protein conformations.","However, existing score-based diffusion methods cannot properly incorporate important physical prior knowledge to guide the generation process, causing large deviations in the sampled protein conformations from the equilibrium distribution.","In this paper, to overcome these limitations, we propose a force-guided SE(3) diffusion model, ConfDiff, for protein conformation generation.","By incorporating a force-guided network with a mixture of data-based score models, ConfDiff can can generate protein conformations with rich diversity while preserving high fidelity.","Experiments on a variety of protein conformation prediction tasks, including 12 fast-folding proteins and the Bovine Pancreatic Trypsin Inhibitor (BPTI), demonstrate that our method surpasses the state-of-the-art method."],"url":"http://arxiv.org/abs/2403.14088v1","category":"q-bio.BM"}
{"created":"2024-03-21 01:52:07","title":"M3: A Multi-Task Mixed-Objective Learning Framework for Open-Domain Multi-Hop Dense Sentence Retrieval","abstract":"In recent research, contrastive learning has proven to be a highly effective method for representation learning and is widely used for dense retrieval. However, we identify that relying solely on contrastive learning can lead to suboptimal retrieval performance. On the other hand, despite many retrieval datasets supporting various learning objectives beyond contrastive learning, combining them efficiently in multi-task learning scenarios can be challenging. In this paper, we introduce M3, an advanced recursive Multi-hop dense sentence retrieval system built upon a novel Multi-task Mixed-objective approach for dense text representation learning, addressing the aforementioned challenges. Our approach yields state-of-the-art performance on a large-scale open-domain fact verification benchmark dataset, FEVER. Code and data are available at: https://github.com/TonyBY/M3","sentences":["In recent research, contrastive learning has proven to be a highly effective method for representation learning and is widely used for dense retrieval.","However, we identify that relying solely on contrastive learning can lead to suboptimal retrieval performance.","On the other hand, despite many retrieval datasets supporting various learning objectives beyond contrastive learning, combining them efficiently in multi-task learning scenarios can be challenging.","In this paper, we introduce M3, an advanced recursive Multi-hop dense sentence retrieval system built upon a novel Multi-task Mixed-objective approach for dense text representation learning, addressing the aforementioned challenges.","Our approach yields state-of-the-art performance on a large-scale open-domain fact verification benchmark dataset, FEVER.","Code and data are available at: https://github.com/TonyBY/M3"],"url":"http://arxiv.org/abs/2403.14074v1","category":"cs.IR"}
{"created":"2024-03-21 01:40:17","title":"Empowering Personalized Learning through a Conversation-based Tutoring System with Student Modeling","abstract":"As the recent Large Language Models(LLM's) become increasingly competent in zero-shot and few-shot reasoning across various domains, educators are showing a growing interest in leveraging these LLM's in conversation-based tutoring systems. However, building a conversation-based personalized tutoring system poses considerable challenges in accurately assessing the student and strategically incorporating the assessment into teaching within the conversation. In this paper, we discuss design considerations for a personalized tutoring system that involves the following two key components: (1) a student modeling with diagnostic components, and (2) a conversation-based tutor utilizing LLM with prompt engineering that incorporates student assessment outcomes and various instructional strategies. Based on these design considerations, we created a proof-of-concept tutoring system focused on personalization and tested it with 20 participants. The results substantiate that our system's framework facilitates personalization, with particular emphasis on the elements constituting student modeling. A web demo of our system is available at http://rlearning-its.com.","sentences":["As the recent Large Language Models(LLM's) become increasingly competent in zero-shot and few-shot reasoning across various domains, educators are showing a growing interest in leveraging these LLM's in conversation-based tutoring systems.","However, building a conversation-based personalized tutoring system poses considerable challenges in accurately assessing the student and strategically incorporating the assessment into teaching within the conversation.","In this paper, we discuss design considerations for a personalized tutoring system that involves the following two key components: (1) a student modeling with diagnostic components, and (2) a conversation-based tutor utilizing LLM with prompt engineering that incorporates student assessment outcomes and various instructional strategies.","Based on these design considerations, we created a proof-of-concept tutoring system focused on personalization and tested it with 20 participants.","The results substantiate that our system's framework facilitates personalization, with particular emphasis on the elements constituting student modeling.","A web demo of our system is available at http://rlearning-its.com."],"url":"http://arxiv.org/abs/2403.14071v1","category":"cs.HC"}
{"created":"2024-03-21 01:37:50","title":"QSMDiff: Unsupervised 3D Diffusion Models for Quantitative Susceptibility Mapping","abstract":"Quantitative Susceptibility Mapping (QSM) dipole inversion is an ill-posed inverse problem for quantifying magnetic susceptibility distributions from MRI tissue phases. While supervised deep learning methods have shown success in specific QSM tasks, their generalizability across different acquisition scenarios remains constrained. Recent developments in diffusion models have demonstrated potential for solving 2D medical imaging inverse problems. However, their application to 3D modalities, such as QSM, remains challenging due to high computational demands. In this work, we developed a 3D image patch-based diffusion model, namely QSMDiff, for robust QSM reconstruction across different scan parameters, alongside simultaneous super-resolution and image-denoising tasks. QSMDiff adopts unsupervised 3D image patch training and full-size measurement guidance during inference for controlled image generation. Evaluation on simulated and in-vivo human brains, using gradient-echo and echo-planar imaging sequences across different acquisition parameters, demonstrates superior performance. The method proposed in QSMDiff also holds promise for impacting other 3D medical imaging applications beyond QSM.","sentences":["Quantitative Susceptibility Mapping (QSM) dipole inversion is an ill-posed inverse problem for quantifying magnetic susceptibility distributions from MRI tissue phases.","While supervised deep learning methods have shown success in specific QSM tasks, their generalizability across different acquisition scenarios remains constrained.","Recent developments in diffusion models have demonstrated potential for solving 2D medical imaging inverse problems.","However, their application to 3D modalities, such as QSM, remains challenging due to high computational demands.","In this work, we developed a 3D image patch-based diffusion model, namely QSMDiff, for robust QSM reconstruction across different scan parameters, alongside simultaneous super-resolution and image-denoising tasks.","QSMDiff adopts unsupervised 3D image patch training and full-size measurement guidance during inference for controlled image generation.","Evaluation on simulated and in-vivo human brains, using gradient-echo and echo-planar imaging sequences across different acquisition parameters, demonstrates superior performance.","The method proposed in QSMDiff also holds promise for impacting other 3D medical imaging applications beyond QSM."],"url":"http://arxiv.org/abs/2403.14070v1","category":"eess.IV"}
{"created":"2024-03-21 01:35:03","title":"Sampling Audit Evidence Using a Naive Bayes Classifier","abstract":"Taiwan's auditors have suffered from processing excessive audit data, including drawing audit evidence. This study advances sampling techniques by integrating machine learning with sampling. This machine learning integration helps avoid sampling bias, keep randomness and variability, and target risker samples. We first classify data using a Naive Bayes classifier into some classes. Next, a user-based, item-based, or hybrid approach is employed to draw audit evidence. The representativeness index is the primary metric for measuring its representativeness. The user-based approach samples data symmetric around the median of a class as audit evidence. It may be equivalent to a combination of monetary and variable samplings. The item-based approach represents asymmetric sampling based on posterior probabilities for obtaining risky samples as audit evidence. It may be identical to a combination of non-statistical and monetary samplings. Auditors can hybridize those user-based and item-based approaches to balance representativeness and riskiness in selecting audit evidence. Three experiments show that sampling using machine learning integration has the benefits of drawing unbiased samples, handling complex patterns, correlations, and unstructured data, and improving efficiency in sampling big data. However, the limitations are the classification accuracy output by machine learning algorithms and the range of prior probabilities.","sentences":["Taiwan's auditors have suffered from processing excessive audit data, including drawing audit evidence.","This study advances sampling techniques by integrating machine learning with sampling.","This machine learning integration helps avoid sampling bias, keep randomness and variability, and target risker samples.","We first classify data using a Naive Bayes classifier into some classes.","Next, a user-based, item-based, or hybrid approach is employed to draw audit evidence.","The representativeness index is the primary metric for measuring its representativeness.","The user-based approach samples data symmetric around the median of a class as audit evidence.","It may be equivalent to a combination of monetary and variable samplings.","The item-based approach represents asymmetric sampling based on posterior probabilities for obtaining risky samples as audit evidence.","It may be identical to a combination of non-statistical and monetary samplings.","Auditors can hybridize those user-based and item-based approaches to balance representativeness and riskiness in selecting audit evidence.","Three experiments show that sampling using machine learning integration has the benefits of drawing unbiased samples, handling complex patterns, correlations, and unstructured data, and improving efficiency in sampling big data.","However, the limitations are the classification accuracy output by machine learning algorithms and the range of prior probabilities."],"url":"http://arxiv.org/abs/2403.14069v1","category":"cs.LG"}
{"created":"2024-03-20 23:36:30","title":"\"It's Not a Replacement:\" Enabling Parent-Robot Collaboration to Support In-Home Learning Experiences of Young Children","abstract":"Learning companion robots for young children are increasingly adopted in informal learning environments. Although parents play a pivotal role in their children's learning, very little is known about how parents prefer to incorporate robots into their children's learning activities. We developed prototype capabilities for a learning companion robot to deliver educational prompts and responses to parent-child pairs during reading sessions and conducted in-home user studies involving 10 families with children aged 3-5. Our data indicates that parents want to work with robots as collaborators to augment parental activities to foster children's learning, introducing the notion of parent-robot collaboration. Our findings offer an empirical understanding of the needs and challenges of parent-child interaction in informal learning scenarios and design opportunities for integrating a companion robot into these interactions. We offer insights into how robots might be designed to facilitate parent-robot collaboration, including parenting policies, collaboration patterns, and interaction paradigms.","sentences":["Learning companion robots for young children are increasingly adopted in informal learning environments.","Although parents play a pivotal role in their children's learning, very little is known about how parents prefer to incorporate robots into their children's learning activities.","We developed prototype capabilities for a learning companion robot to deliver educational prompts and responses to parent-child pairs during reading sessions and conducted in-home user studies involving 10 families with children aged 3-5.","Our data indicates that parents want to work with robots as collaborators to augment parental activities to foster children's learning, introducing the notion of parent-robot collaboration.","Our findings offer an empirical understanding of the needs and challenges of parent-child interaction in informal learning scenarios and design opportunities for integrating a companion robot into these interactions.","We offer insights into how robots might be designed to facilitate parent-robot collaboration, including parenting policies, collaboration patterns, and interaction paradigms."],"url":"http://arxiv.org/abs/2403.14041v1","category":"cs.HC"}
{"created":"2024-03-20 22:24:36","title":"Towards a connection between the capacitated vehicle routing problem and the constrained centroid-based clustering","abstract":"Efficiently solving a vehicle routing problem (VRP) in a practical runtime is a critical challenge for delivery management companies. This paper explores both a theoretical and experimental connection between the Capacitated Vehicle Routing Problem (CVRP) and the Constrained Centroid-Based Clustering (CCBC). Reducing a CVRP to a CCBC is a synonym for a transition from an exponential to a polynomial complexity using commonly known algorithms for clustering, i.e K-means. At the beginning, we conduct an exploratory analysis to highlight the existence of such a relationship between the two problems through illustrative small-size examples and simultaneously deduce some mathematically-related formulations and properties. On a second level, the paper proposes a CCBC based approach endowed with some enhancements. The proposed framework consists of three stages. At the first step, a constrained centroid-based clustering algorithm generates feasible clusters of customers. This methodology incorporates three enhancement tools to achieve near-optimal clusters, namely: a multi-start procedure for initial centroids, a customer assignment metric, and a self-adjustment mechanism for choosing the number of clusters. At the second step, a traveling salesman problem (T SP) solver is used to optimize the order of customers within each cluster. Finally, we introduce a process relying on routes cutting and relinking procedure, which calls upon solving a linear and integer programming model to further improve the obtained routes. This step is inspired by the ruin & recreate algorithm. This approach is an extension of the classical cluster-first, route-second method and provides near-optimal solutions on well-known benchmark instances in terms of solution quality and computational runtime, offering a milestone in solving VRP.","sentences":["Efficiently solving a vehicle routing problem (VRP) in a practical runtime is a critical challenge for delivery management companies.","This paper explores both a theoretical and experimental connection between the Capacitated Vehicle Routing Problem (CVRP) and the Constrained Centroid-Based Clustering (CCBC).","Reducing a CVRP to a CCBC is a synonym for a transition from an exponential to a polynomial complexity using commonly known algorithms for clustering, i.e K-means.","At the beginning, we conduct an exploratory analysis to highlight the existence of such a relationship between the two problems through illustrative small-size examples and simultaneously deduce some mathematically-related formulations and properties.","On a second level, the paper proposes a CCBC based approach endowed with some enhancements.","The proposed framework consists of three stages.","At the first step, a constrained centroid-based clustering algorithm generates feasible clusters of customers.","This methodology incorporates three enhancement tools to achieve near-optimal clusters, namely: a multi-start procedure for initial centroids, a customer assignment metric, and a self-adjustment mechanism for choosing the number of clusters.","At the second step, a traveling salesman problem (T SP) solver is used to optimize the order of customers within each cluster.","Finally, we introduce a process relying on routes cutting and relinking procedure, which calls upon solving a linear and integer programming model to further improve the obtained routes.","This step is inspired by the ruin & recreate algorithm.","This approach is an extension of the classical cluster-first, route-second method and provides near-optimal solutions on well-known benchmark instances in terms of solution quality and computational runtime, offering a milestone in solving VRP."],"url":"http://arxiv.org/abs/2403.14013v1","category":"math.OC"}
{"created":"2024-03-20 22:05:18","title":"Multi-Modal Hallucination Control by Visual Information Grounding","abstract":"Generative Vision-Language Models (VLMs) are prone to generate plausible-sounding textual answers that, however, are not always grounded in the input image. We investigate this phenomenon, usually referred to as \"hallucination\" and show that it stems from an excessive reliance on the language prior. In particular, we show that as more tokens are generated, the reliance on the visual prompt decreases, and this behavior strongly correlates with the emergence of hallucinations. To reduce hallucinations, we introduce Multi-Modal Mutual-Information Decoding (M3ID), a new sampling method for prompt amplification. M3ID amplifies the influence of the reference image over the language prior, hence favoring the generation of tokens with higher mutual information with the visual prompt. M3ID can be applied to any pre-trained autoregressive VLM at inference time without necessitating further training and with minimal computational overhead. If training is an option, we show that M3ID can be paired with Direct Preference Optimization (DPO) to improve the model's reliance on the prompt image without requiring any labels. Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers. Specifically, for the LLaVA 13B model, M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24%.","sentences":["Generative Vision-Language Models (VLMs) are prone to generate plausible-sounding textual answers that, however, are not always grounded in the input image.","We investigate this phenomenon, usually referred to as \"hallucination\" and show that it stems from an excessive reliance on the language prior.","In particular, we show that as more tokens are generated, the reliance on the visual prompt decreases, and this behavior strongly correlates with the emergence of hallucinations.","To reduce hallucinations, we introduce Multi-Modal Mutual-Information Decoding (M3ID), a new sampling method for prompt amplification.","M3ID amplifies the influence of the reference image over the language prior, hence favoring the generation of tokens with higher mutual information with the visual prompt.","M3ID can be applied to any pre-trained autoregressive VLM at inference time without necessitating further training and with minimal computational overhead.","If training is an option, we show that M3ID can be paired with Direct Preference Optimization (DPO) to improve the model's reliance on the prompt image without requiring any labels.","Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.","Specifically, for the LLaVA 13B model, M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24%."],"url":"http://arxiv.org/abs/2403.14003v1","category":"cs.CV"}
{"created":"2024-03-20 20:47:53","title":"SeFFeC: Semantic Facial Feature Control for Fine-grained Face Editing","abstract":"We propose Semantic Facial Feature Control (SeFFeC) - a novel method for fine-grained face shape editing. Our method enables the manipulation of human-understandable, semantic face features, such as nose length or mouth width, which are defined by different groups of facial landmarks. In contrast to existing methods, the use of facial landmarks enables precise measurement of the facial features, which then enables training SeFFeC without any manually annotated labels. SeFFeC consists of a transformer-based encoder network that takes a latent vector of a pre-trained generative model and a facial feature embedding as input, and learns to modify the latent vector to perform the desired face edit operation. To ensure that the desired feature measurement is changed towards the target value without altering uncorrelated features, we introduced a novel semantic face feature loss. Qualitative and quantitative results show that SeFFeC enables precise and fine-grained control of 23 facial features, some of which could not previously be controlled by other methods, without requiring manual annotations. Unlike existing methods, SeFFeC also provides deterministic control over the exact values of the facial features and more localised and disentangled face edits.","sentences":["We propose Semantic Facial Feature Control (SeFFeC) - a novel method for fine-grained face shape editing.","Our method enables the manipulation of human-understandable, semantic face features, such as nose length or mouth width, which are defined by different groups of facial landmarks.","In contrast to existing methods, the use of facial landmarks enables precise measurement of the facial features, which then enables training SeFFeC without any manually annotated labels.","SeFFeC consists of a transformer-based encoder network that takes a latent vector of a pre-trained generative model and a facial feature embedding as input, and learns to modify the latent vector to perform the desired face edit operation.","To ensure that the desired feature measurement is changed towards the target value without altering uncorrelated features, we introduced a novel semantic face feature loss.","Qualitative and quantitative results show that SeFFeC enables precise and fine-grained control of 23 facial features, some of which could not previously be controlled by other methods, without requiring manual annotations.","Unlike existing methods, SeFFeC also provides deterministic control over the exact values of the facial features and more localised and disentangled face edits."],"url":"http://arxiv.org/abs/2403.13972v1","category":"cs.CV"}
{"created":"2024-03-20 20:37:13","title":"ConGeo: Robust Cross-view Geo-localization across Ground View Variations","abstract":"Cross-view geo-localization aims at localizing a ground-level query image by matching it to its corresponding geo-referenced aerial view. In real-world scenarios, the task requires accommodating diverse ground images captured by users with varying orientations and reduced field of views (FoVs). However, existing learning pipelines are orientation-specific or FoV-specific, demanding separate model training for different ground view variations. Such models heavily depend on the North-aligned spatial correspondence and predefined FoVs in the training data, compromising their robustness across different settings. To tackle this challenge, we propose ConGeo, a single- and cross-modal Contrastive method for Geo-localization: it enhances robustness and consistency in feature representations to improve a model's invariance to orientation and its resilience to FoV variations, by enforcing proximity between ground view variations of the same location. As a generic learning objective for cross-view geo-localization, when integrated into state-of-the-art pipelines, ConGeo significantly boosts the performance of three base models on four geo-localization benchmarks for diverse ground view variations and outperforms competing methods that train separate models for each ground view variation.","sentences":["Cross-view geo-localization aims at localizing a ground-level query image by matching it to its corresponding geo-referenced aerial view.","In real-world scenarios, the task requires accommodating diverse ground images captured by users with varying orientations and reduced field of views (FoVs).","However, existing learning pipelines are orientation-specific or FoV-specific, demanding separate model training for different ground view variations.","Such models heavily depend on the North-aligned spatial correspondence and predefined FoVs in the training data, compromising their robustness across different settings.","To tackle this challenge, we propose ConGeo, a single- and cross-modal Contrastive method for Geo-localization: it enhances robustness and consistency in feature representations to improve a model's invariance to orientation and its resilience to FoV variations, by enforcing proximity between ground view variations of the same location.","As a generic learning objective for cross-view geo-localization, when integrated into state-of-the-art pipelines, ConGeo significantly boosts the performance of three base models on four geo-localization benchmarks for diverse ground view variations and outperforms competing methods that train separate models for each ground view variation."],"url":"http://arxiv.org/abs/2403.13965v1","category":"cs.CV"}
{"created":"2024-03-20 18:49:59","title":"Visually Grounded Speech Models have a Mutual Exclusivity Bias","abstract":"When children learn new words, they employ constraints such as the mutual exclusivity (ME) bias: a novel word is mapped to a novel object rather than a familiar one. This bias has been studied computationally, but only in models that use discrete word representations as input, ignoring the high variability of spoken words. We investigate the ME bias in the context of visually grounded speech models that learn from natural images and continuous speech audio. Concretely, we train a model on familiar words and test its ME bias by asking it to select between a novel and a familiar object when queried with a novel word. To simulate prior acoustic and visual knowledge, we experiment with several initialisation strategies using pretrained speech and vision networks. Our findings reveal the ME bias across the different initialisation approaches, with a stronger bias in models with more prior (in particular, visual) knowledge. Additional tests confirm the robustness of our results, even when different loss functions are considered.","sentences":["When children learn new words, they employ constraints such as the mutual exclusivity (ME) bias: a novel word is mapped to a novel object rather than a familiar one.","This bias has been studied computationally, but only in models that use discrete word representations as input, ignoring the high variability of spoken words.","We investigate the ME bias in the context of visually grounded speech models that learn from natural images and continuous speech audio.","Concretely, we train a model on familiar words and test its ME bias by asking it to select between a novel and a familiar object when queried with a novel word.","To simulate prior acoustic and visual knowledge, we experiment with several initialisation strategies using pretrained speech and vision networks.","Our findings reveal the ME bias across the different initialisation approaches, with a stronger bias in models with more prior (in particular, visual) knowledge.","Additional tests confirm the robustness of our results, even when different loss functions are considered."],"url":"http://arxiv.org/abs/2403.13922v1","category":"cs.CL"}
{"created":"2024-03-20 18:36:30","title":"Enhancing Fingerprint Image Synthesis with GANs, Diffusion Models, and Style Transfer Techniques","abstract":"We present novel approaches involving generative adversarial networks and diffusion models in order to synthesize high quality, live and spoof fingerprint images while preserving features such as uniqueness and diversity. We generate live fingerprints from noise with a variety of methods, and we use image translation techniques to translate live fingerprint images to spoof. To generate different types of spoof images based on limited training data we incorporate style transfer techniques through a cycle autoencoder equipped with a Wasserstein metric along with Gradient Penalty (CycleWGAN-GP) in order to avoid mode collapse and instability. We find that when the spoof training data includes distinct spoof characteristics, it leads to improved live-to-spoof translation. We assess the diversity and realism of the generated live fingerprint images mainly through the Fr\\'echet Inception Distance (FID) and the False Acceptance Rate (FAR). Our best diffusion model achieved a FID of 15.78. The comparable WGAN-GP model achieved slightly higher FID while performing better in the uniqueness assessment due to a slightly lower FAR when matched against the training data, indicating better creativity. Moreover, we give example images showing that a DDPM model clearly can generate realistic fingerprint images.","sentences":["We present novel approaches involving generative adversarial networks and diffusion models in order to synthesize high quality, live and spoof fingerprint images while preserving features such as uniqueness and diversity.","We generate live fingerprints from noise with a variety of methods, and we use image translation techniques to translate live fingerprint images to spoof.","To generate different types of spoof images based on limited training data we incorporate style transfer techniques through a cycle autoencoder equipped with a Wasserstein metric along with Gradient Penalty (CycleWGAN-GP) in order to avoid mode collapse and instability.","We find that when the spoof training data includes distinct spoof characteristics, it leads to improved live-to-spoof translation.","We assess the diversity and realism of the generated live fingerprint images mainly through the Fr\\'echet Inception Distance (FID) and the False Acceptance Rate (FAR).","Our best diffusion model achieved a FID of 15.78.","The comparable WGAN-GP model achieved slightly higher FID while performing better in the uniqueness assessment due to a slightly lower FAR when matched against the training data, indicating better creativity.","Moreover, we give example images showing that a DDPM model clearly can generate realistic fingerprint images."],"url":"http://arxiv.org/abs/2403.13916v1","category":"cs.CV"}
{"created":"2024-03-21 17:59:55","title":"ODTFormer: Efficient Obstacle Detection and Tracking with Stereo Cameras Based on Transformer","abstract":"Obstacle detection and tracking represent a critical component in robot autonomous navigation. In this paper, we propose ODTFormer, a Transformer-based model to address both obstacle detection and tracking problems. For the detection task, our approach leverages deformable attention to construct a 3D cost volume, which is decoded progressively in the form of voxel occupancy grids. We further track the obstacles by matching the voxels between consecutive frames. The entire model can be optimized in an end-to-end manner. Through extensive experiments on DrivingStereo and KITTI benchmarks, our model achieves state-of-the-art performance in the obstacle detection task. We also report comparable accuracy to state-of-the-art obstacle tracking models while requiring only a fraction of their computation cost, typically ten-fold to twenty-fold less. The code and model weights will be publicly released.","sentences":["Obstacle detection and tracking represent a critical component in robot autonomous navigation.","In this paper, we propose ODTFormer, a Transformer-based model to address both obstacle detection and tracking problems.","For the detection task, our approach leverages deformable attention to construct a 3D cost volume, which is decoded progressively in the form of voxel occupancy grids.","We further track the obstacles by matching the voxels between consecutive frames.","The entire model can be optimized in an end-to-end manner.","Through extensive experiments on DrivingStereo and KITTI benchmarks, our model achieves state-of-the-art performance in the obstacle detection task.","We also report comparable accuracy to state-of-the-art obstacle tracking models while requiring only a fraction of their computation cost, typically ten-fold to twenty-fold less.","The code and model weights will be publicly released."],"url":"http://arxiv.org/abs/2403.14626v1","category":"cs.RO"}
{"created":"2024-03-21 16:40:26","title":"Quantitative Indicators for Strength of Inequalities with Respect to a Polyhedron, Part II: Applications and Computational Evidence","abstract":"The first paper explored two strength indicators (extreme point ratio (EPR) and centroid distance (CD)), both of which predict that subtours of the spanning tree in hypergraph polytope STHGP having large cardinality are significantly stronger than the corresponding subtours of small cardinality. In this second paper, we exploit this previously unknown property algorithmically within GeoSteiner, presenting strong computational evidence that the EPR and CD indicators are highly predictive of actual computational strength, at least for subtour inequalities of STHGP. The GeoSteiner package for geometric Steiner trees in the plane uses STHGP to solve the Full Steiner Tree (FST) concatenation problem. The separation algorithms in previous versions of GeoSteiner find violated subtours of only relatively small cardinality, and we examine the underlying algorithmic causes. We present improvements in GeoSteiner specifically designed to strengthen violated subtours by augmentation (instead of reduction), yielding violated subtours of large cardinality. Across all distance metrics and instance classes studied, the computational results are remarkable -- culminating with an optimal solution of a 1,000,000 terminal random Euclidean instance. The conclusion is that the EPR and CD strength indicators presented in the first paper have strong predictive power regarding actual computational strength (at least regarding STHGP subtours). The ability to accurately measure the strength of inequalities has numerous applications of great importance, both in theory and practice.","sentences":["The first paper explored two strength indicators (extreme point ratio (EPR) and centroid distance (CD)), both of which predict that subtours of the spanning tree in hypergraph polytope STHGP having large cardinality are significantly stronger than the corresponding subtours of small cardinality.","In this second paper, we exploit this previously unknown property algorithmically within GeoSteiner, presenting strong computational evidence that the EPR and CD indicators are highly predictive of actual computational strength, at least for subtour inequalities of STHGP.","The GeoSteiner package for geometric Steiner trees in the plane uses STHGP to solve the Full Steiner Tree (FST) concatenation problem.","The separation algorithms in previous versions of GeoSteiner find violated subtours of only relatively small cardinality, and we examine the underlying algorithmic causes.","We present improvements in GeoSteiner specifically designed to strengthen violated subtours by augmentation (instead of reduction), yielding violated subtours of large cardinality.","Across all distance metrics and instance classes studied, the computational results are remarkable -- culminating with an optimal solution of a 1,000,000 terminal random Euclidean instance.","The conclusion is that the EPR and CD strength indicators presented in the first paper have strong predictive power regarding actual computational strength (at least regarding STHGP subtours).","The ability to accurately measure the strength of inequalities has numerous applications of great importance, both in theory and practice."],"url":"http://arxiv.org/abs/2403.14540v1","category":"math.OC"}
{"created":"2024-03-21 16:37:34","title":"First-Order Methods for Linear Programming","abstract":"Linear programming is the seminal optimization problem that has spawned and grown into today's rich and diverse optimization modeling and algorithmic landscape. This article provides an overview of the recent development of first-order methods for solving large-scale linear programming.","sentences":["Linear programming is the seminal optimization problem that has spawned and grown into today's rich and diverse optimization modeling and algorithmic landscape.","This article provides an overview of the recent development of first-order methods for solving large-scale linear programming."],"url":"http://arxiv.org/abs/2403.14535v1","category":"math.OC"}
{"created":"2024-03-21 16:21:22","title":"Quantitative Indicators for Strength of Inequalities with Respect to a Polyhedron, Part I: Theory","abstract":"We study strength of inequalities used in mixed-integer programming, and in branch-and-cut algorithms that solve such problems. Strength is an ethereal property lacking good formal definition, but crucial for computational speed. We review several quantitative indicators proposed in the literature we claim provide a measure of the relative strength of inequalities with respect to a given polyhedron. We evaluate two of these indicators (extreme point ratio (EPR) and centroid distance (CD)) on various facet classes for both the traveling salesman polytope TSP, and spanning tree in hypergraph polytope STHGP, obtaining closed-forms for EPR and CD on each facet class. Within each facet class, the two indicators yield strikingly similar strength rankings, with excellent agreement on which facets are strongest and which are weakest. Both indicators corroborate all known computational experience with both polytopes. The indicators also reveal previously unknown properties of STHGP subtours. We also evaluate EPR and CD for the subtour inequalities of the spanning tree in graphs polytope STGP, obtaining surprising and unexpected results that (at least for STGP and STHGP subtours) lead us to believe EPR to be a more accurate estimate of strength than CD. Applications include: comparing the relative strength of different classes of inequalities; design of rapidly-converging separation algorithms; design or justification for constraint strengthening procedures.   The companion paper exploits one of the newly revealed properties of STHGP subtours in GeoSteiner, with detailed computational results. Across all distance metrics and instances studied, these results are remarkable -- culminating with an optimal solution of a 1,000,000 terminal random Euclidean instance. This confirms these indicators to be highly predictive and strongly correlated with actual computational strength.","sentences":["We study strength of inequalities used in mixed-integer programming, and in branch-and-cut algorithms that solve such problems.","Strength is an ethereal property lacking good formal definition, but crucial for computational speed.","We review several quantitative indicators proposed in the literature we claim provide a measure of the relative strength of inequalities with respect to a given polyhedron.","We evaluate two of these indicators (extreme point ratio (EPR) and centroid distance (CD)) on various facet classes for both the traveling salesman polytope TSP, and spanning tree in hypergraph polytope STHGP, obtaining closed-forms for EPR and CD on each facet class.","Within each facet class, the two indicators yield strikingly similar strength rankings, with excellent agreement on which facets are strongest and which are weakest.","Both indicators corroborate all known computational experience with both polytopes.","The indicators also reveal previously unknown properties of STHGP subtours.","We also evaluate EPR and CD for the subtour inequalities of the spanning tree in graphs polytope STGP, obtaining surprising and unexpected results that (at least for STGP and STHGP subtours) lead us to believe EPR to be a more accurate estimate of strength than CD.","Applications include: comparing the relative strength of different classes of inequalities; design of rapidly-converging separation algorithms; design or justification for constraint strengthening procedures.   ","The companion paper exploits one of the newly revealed properties of STHGP subtours in GeoSteiner, with detailed computational results.","Across all distance metrics and instances studied, these results are remarkable -- culminating with an optimal solution of a 1,000,000 terminal random Euclidean instance.","This confirms these indicators to be highly predictive and strongly correlated with actual computational strength."],"url":"http://arxiv.org/abs/2403.14522v1","category":"math.OC"}
{"created":"2024-03-21 15:52:05","title":"Denoising Diffusion Models for 3D Healthy Brain Tissue Inpainting","abstract":"Monitoring diseases that affect the brain's structural integrity requires automated analysis of magnetic resonance (MR) images, e.g., for the evaluation of volumetric changes. However, many of the evaluation tools are optimized for analyzing healthy tissue. To enable the evaluation of scans containing pathological tissue, it is therefore required to restore healthy tissue in the pathological areas. In this work, we explore and extend denoising diffusion models for consistent inpainting of healthy 3D brain tissue. We modify state-of-the-art 2D, pseudo-3D, and 3D methods working in the image space, as well as 3D latent and 3D wavelet diffusion models, and train them to synthesize healthy brain tissue. Our evaluation shows that the pseudo-3D model performs best regarding the structural-similarity index, peak signal-to-noise ratio, and mean squared error. To emphasize the clinical relevance, we fine-tune this model on data containing synthetic MS lesions and evaluate it on a downstream brain tissue segmentation task, whereby it outperforms the established FMRIB Software Library (FSL) lesion-filling method.","sentences":["Monitoring diseases that affect the brain's structural integrity requires automated analysis of magnetic resonance (MR) images, e.g., for the evaluation of volumetric changes.","However, many of the evaluation tools are optimized for analyzing healthy tissue.","To enable the evaluation of scans containing pathological tissue, it is therefore required to restore healthy tissue in the pathological areas.","In this work, we explore and extend denoising diffusion models for consistent inpainting of healthy 3D brain tissue.","We modify state-of-the-art 2D, pseudo-3D, and 3D methods working in the image space, as well as 3D latent and 3D wavelet diffusion models, and train them to synthesize healthy brain tissue.","Our evaluation shows that the pseudo-3D model performs best regarding the structural-similarity index, peak signal-to-noise ratio, and mean squared error.","To emphasize the clinical relevance, we fine-tune this model on data containing synthetic MS lesions and evaluate it on a downstream brain tissue segmentation task, whereby it outperforms the established FMRIB Software Library (FSL) lesion-filling method."],"url":"http://arxiv.org/abs/2403.14499v1","category":"eess.IV"}
{"created":"2024-03-21 12:04:36","title":"An empirical appraisal of methods for the dynamic prediction of survival with numerous longitudinal predictors","abstract":"Recently, the increasing availability of repeated measurements in biomedical studies has motivated the development of several statistical methods for the dynamic prediction of survival in settings where a large (potentially high-dimensional) number of longitudinal covariates is available. These methods differ in both how they model the longitudinal covariates trajectories, and how they specify the relationship between the longitudinal covariates and the survival outcome. Because these methods are still quite new, little is known about their applicability, limitations and performance when applied to real-world data.   To investigate these questions, we present a comparison of the predictive performance of the aforementioned methods and two simpler prediction approaches to three datasets that differ in terms of outcome type, sample size, number of longitudinal covariates and length of follow-up. We discuss how different modelling choices can have an impact on the possibility to accommodate unbalanced study designs and on computing time, and compare the predictive performance of the different approaches using a range of performance measures and landmark times.","sentences":["Recently, the increasing availability of repeated measurements in biomedical studies has motivated the development of several statistical methods for the dynamic prediction of survival in settings where a large (potentially high-dimensional) number of longitudinal covariates is available.","These methods differ in both how they model the longitudinal covariates trajectories, and how they specify the relationship between the longitudinal covariates and the survival outcome.","Because these methods are still quite new, little is known about their applicability, limitations and performance when applied to real-world data.   ","To investigate these questions, we present a comparison of the predictive performance of the aforementioned methods and two simpler prediction approaches to three datasets that differ in terms of outcome type, sample size, number of longitudinal covariates and length of follow-up.","We discuss how different modelling choices can have an impact on the possibility to accommodate unbalanced study designs and on computing time, and compare the predictive performance of the different approaches using a range of performance measures and landmark times."],"url":"http://arxiv.org/abs/2403.14336v1","category":"stat.ME"}
{"created":"2024-03-21 11:00:59","title":"Optimal prevention strategies for chronic diseases in an compartmental disease trajectory model","abstract":"In countries with growing elderly populations, multimorbidity poses a significant healthcare challenge. The trajectories along which diseases accumulate as patients age and how they can be targeted by prevention efforts are still not fully understood. We propose a compartmental model, traditionally used in infectious diseases, describing chronic disease trajectories across 132 distinct multimorbidity patterns (compartments). Leveraging a comprehensive dataset from approximately 45 million hospital stays spanning 17 years in Austria, our compartmental disease trajectory model (CDTM) forecasts changes in the incidence of 131 diagnostic groups and their combinations until 2030, highlighting patterns involving hypertensive diseases with cardiovascular diseases and metabolic disorders. We pinpoint specific diagnoses with the greatest potential for preventive interventions to promote healthy aging. According to our model, a reduction of new onsets by 5% of hypertensive diseases (I10-I15) leads to a reduction in all-cause mortality over a period of 15 years by 0.57 (0.06)% and for malignant neoplasms (C00-C97) mortality is reduced by 0.57 (0.07)%. Furthermore, we use the model to assess the long-term consequences of the Covid-19 pandemic on hospitalizations, revealing earlier and more frequent hospitalizations across multiple diagnoses. Our fully data-driven approach identifies leverage points for proactive preparation by physicians and policymakers to reduce the overall disease burden in the population, emphasizing a shift towards patient-centered care.","sentences":["In countries with growing elderly populations, multimorbidity poses a significant healthcare challenge.","The trajectories along which diseases accumulate as patients age and how they can be targeted by prevention efforts are still not fully understood.","We propose a compartmental model, traditionally used in infectious diseases, describing chronic disease trajectories across 132 distinct multimorbidity patterns (compartments).","Leveraging a comprehensive dataset from approximately 45 million hospital stays spanning 17 years in Austria, our compartmental disease trajectory model (CDTM) forecasts changes in the incidence of 131 diagnostic groups and their combinations until 2030, highlighting patterns involving hypertensive diseases with cardiovascular diseases and metabolic disorders.","We pinpoint specific diagnoses with the greatest potential for preventive interventions to promote healthy aging.","According to our model, a reduction of new onsets by 5% of hypertensive diseases (I10-I15) leads to a reduction in all-cause mortality over a period of 15 years by 0.57 (0.06)% and for malignant neoplasms (C00-C97) mortality is reduced by 0.57 (0.07)%.","Furthermore, we use the model to assess the long-term consequences of the Covid-19 pandemic on hospitalizations, revealing earlier and more frequent hospitalizations across multiple diagnoses.","Our fully data-driven approach identifies leverage points for proactive preparation by physicians and policymakers to reduce the overall disease burden in the population, emphasizing a shift towards patient-centered care."],"url":"http://arxiv.org/abs/2403.14296v1","category":"physics.soc-ph"}
{"created":"2024-03-21 08:11:10","title":"Asteroseismological analysis of the non-Blazhko RRab star EPIC~248846335 in LAMOST -- Kepler$/$ K2 project","abstract":"We conduct an asteroseismological analysis on the non-Blazhko ab-type RR Lyrae star EPIC 248846335 employing the Radial Stellar Pulsations (RSP) module of the Modules for Experiments in Stellar Astrophysics (MESA) based on the set of stellar parameters. The atmospheric parameters as $T_\\mathrm{eff}$ = 6933$\\pm$70 $K$, log $g$ = 3.35$\\pm$ 0.50 and [Fe/H] = -1.18 $\\pm$ 0.14 are estimated from the Low-Resolution Spectra of LAMOST DR9. The luminosity $L$ = 49.70$_{-1.80}^{+2.99}$ $L_\\odot$ and mass M = 0.56 $\\pm$ 0.07 $M_\\odot$ are calculated, respectively, using the distance provided by Gaia and the metallicity estimated from the Low-Resolution Spectra. The Fourier parameters of the light curves observed by $K2$ and RV curves determined from the Medium-Resolution Spectra of LAMOST DR10 are also calculated in this work. The period of the fundamental mode of the star and the residuals $r$ of the Fourier parameters between the models and observations serve to select optimal model, whose stellar parameters are $T_\\mathrm{eff}$ = 6700 $\\pm$ 220 K, log $g$ = 2.70, [Fe/H] = -1.20 $\\pm$ 0.2, M = 0.59 $\\pm$ 0.05 $M_\\odot$, and $L$ = 56.0 $\\pm$ 4.2 $L_\\odot$. The projection factors are constrained as 1.20 $\\pm$ 0.02 and 1.59 $\\pm$ 0.13 by the blue- and red-arm observed velocities with their corresponding RV curves derived from the best-fit model, respectively. The precise determination of stellar parameters in ab-type RR Lyrae stars is crucial for understanding the physical processes that occur during pulsation and for providing a deeper understanding of its Period-Luminosity relationship.","sentences":["We conduct an asteroseismological analysis on the non-Blazhko ab-type RR Lyrae star EPIC 248846335 employing the Radial Stellar Pulsations (RSP) module of the Modules for Experiments in Stellar Astrophysics (MESA) based on the set of stellar parameters.","The atmospheric parameters as $T_\\mathrm{eff}$ = 6933$\\pm$70 $K$, log $g$ = 3.35$\\pm$ 0.50 and [Fe/H] = -1.18 $\\pm$ 0.14 are estimated from the Low-Resolution Spectra of LAMOST DR9.","The luminosity $L$ = 49.70$_{-1.80}^{+2.99}$ $L_\\odot$ and mass M = 0.56 $\\pm$ 0.07 $M_\\odot$ are calculated, respectively, using the distance provided by Gaia and the metallicity estimated from the Low-Resolution Spectra.","The Fourier parameters of the light curves observed by $K2$ and RV curves determined from the Medium-Resolution Spectra of LAMOST DR10 are also calculated in this work.","The period of the fundamental mode of the star and the residuals $r$ of the Fourier parameters between the models and observations serve to select optimal model, whose stellar parameters are $T_\\mathrm{eff}$ = 6700 $\\pm$ 220 K, log $g$ = 2.70, [Fe/H] = -1.20 $\\pm$ 0.2, M = 0.59 $\\pm$ 0.05 $M_\\odot$, and $L$ = 56.0 $\\pm$ 4.2 $L_\\odot$. The projection factors are constrained as 1.20 $\\pm$ 0.02 and 1.59 $\\pm$ 0.13 by the blue- and red-arm observed velocities with their corresponding RV curves derived from the best-fit model, respectively.","The precise determination of stellar parameters in ab-type RR Lyrae stars is crucial for understanding the physical processes that occur during pulsation and for providing a deeper understanding of its Period-Luminosity relationship."],"url":"http://arxiv.org/abs/2403.14214v1","category":"astro-ph.SR"}
{"created":"2024-03-21 08:06:50","title":"CMOS-compatible photonic integrated circuits on thin-film ScAlN","abstract":"Scandium aluminum nitride (ScAlN) has recently emerged as an attractive material for integrated photonics due to its favorable nonlinear optical properties and compatibility with CMOS fabrication. Despite the promising and versatile material properties, it is still an outstanding challenge to realize low-loss photonic circuits on thin-film ScAlN-on-insulator wafers. Here, we present a systematic study on the material quality of sputtered thin-film ScAlN produced in a CMOS-compatible 200 mm line, and an optimized fabrication process to yield 400 nm thick, fully etched waveguides. With surface polishing and annealing, we achieve micro-ring resonators with an intrinsic quality factor as high as $1.47\\times 10^5$, corresponding to a propagation loss of 2.4 dB/cm. These results serve as a critical step towards developing future large-scale, low-loss photonic integrated circuits based on ScAlN.","sentences":["Scandium aluminum nitride (ScAlN) has recently emerged as an attractive material for integrated photonics due to its favorable nonlinear optical properties and compatibility with CMOS fabrication.","Despite the promising and versatile material properties, it is still an outstanding challenge to realize low-loss photonic circuits on thin-film ScAlN-on-insulator wafers.","Here, we present a systematic study on the material quality of sputtered thin-film ScAlN produced in a CMOS-compatible 200 mm line, and an optimized fabrication process to yield 400 nm thick, fully etched waveguides.","With surface polishing and annealing, we achieve micro-ring resonators with an intrinsic quality factor as high as $1.47\\times 10^5$, corresponding to a propagation loss of 2.4 dB/cm.","These results serve as a critical step towards developing future large-scale, low-loss photonic integrated circuits based on ScAlN."],"url":"http://arxiv.org/abs/2403.14212v1","category":"physics.optics"}
{"created":"2024-03-21 06:03:51","title":"Harmonizing Visual and Textual Embeddings for Zero-Shot Text-to-Image Customization","abstract":"In a surge of text-to-image (T2I) models and their customization methods that generate new images of a user-provided subject, current works focus on alleviating the costs incurred by a lengthy per-subject optimization. These zero-shot customization methods encode the image of a specified subject into a visual embedding which is then utilized alongside the textual embedding for diffusion guidance. The visual embedding incorporates intrinsic information about the subject, while the textual embedding provides a new, transient context. However, the existing methods often 1) are significantly affected by the input images, eg., generating images with the same pose, and 2) exhibit deterioration in the subject's identity. We first pin down the problem and show that redundant pose information in the visual embedding interferes with the textual embedding containing the desired pose information. To address this issue, we propose orthogonal visual embedding which effectively harmonizes with the given textual embedding. We also adopt the visual-only embedding and inject the subject's clear features utilizing a self-attention swap. Our results demonstrate the effectiveness and robustness of our method, which offers highly flexible zero-shot generation while effectively maintaining the subject's identity.","sentences":["In a surge of text-to-image (T2I) models and their customization methods that generate new images of a user-provided subject, current works focus on alleviating the costs incurred by a lengthy per-subject optimization.","These zero-shot customization methods encode the image of a specified subject into a visual embedding which is then utilized alongside the textual embedding for diffusion guidance.","The visual embedding incorporates intrinsic information about the subject, while the textual embedding provides a new, transient context.","However, the existing methods often 1) are significantly affected by the input images, eg., generating images with the same pose, and 2) exhibit deterioration in the subject's identity.","We first pin down the problem and show that redundant pose information in the visual embedding interferes with the textual embedding containing the desired pose information.","To address this issue, we propose orthogonal visual embedding which effectively harmonizes with the given textual embedding.","We also adopt the visual-only embedding and inject the subject's clear features utilizing a self-attention swap.","Our results demonstrate the effectiveness and robustness of our method, which offers highly flexible zero-shot generation while effectively maintaining the subject's identity."],"url":"http://arxiv.org/abs/2403.14155v1","category":"cs.CV"}
{"created":"2024-03-21 05:04:52","title":"3D Object Detection from Point Cloud via Voting Step Diffusion","abstract":"3D object detection is a fundamental task in scene understanding. Numerous research efforts have been dedicated to better incorporate Hough voting into the 3D object detection pipeline. However, due to the noisy, cluttered, and partial nature of real 3D scans, existing voting-based methods often receive votes from the partial surfaces of individual objects together with severe noises, leading to sub-optimal detection performance. In this work, we focus on the distributional properties of point clouds and formulate the voting process as generating new points in the high-density region of the distribution of object centers. To achieve this, we propose a new method to move random 3D points toward the high-density region of the distribution by estimating the score function of the distribution with a noise conditioned score network. Specifically, we first generate a set of object center proposals to coarsely identify the high-density region of the object center distribution. To estimate the score function, we perturb the generated object center proposals by adding normalized Gaussian noise, and then jointly estimate the score function of all perturbed distributions. Finally, we generate new votes by moving random 3D points to the high-density region of the object center distribution according to the estimated score function. Extensive experiments on two large scale indoor 3D scene datasets, SUN RGB-D and ScanNet V2, demonstrate the superiority of our proposed method. The code will be released at https://github.com/HHrEtvP/DiffVote.","sentences":["3D object detection is a fundamental task in scene understanding.","Numerous research efforts have been dedicated to better incorporate Hough voting into the 3D object detection pipeline.","However, due to the noisy, cluttered, and partial nature of real 3D scans, existing voting-based methods often receive votes from the partial surfaces of individual objects together with severe noises, leading to sub-optimal detection performance.","In this work, we focus on the distributional properties of point clouds and formulate the voting process as generating new points in the high-density region of the distribution of object centers.","To achieve this, we propose a new method to move random 3D points toward the high-density region of the distribution by estimating the score function of the distribution with a noise conditioned score network.","Specifically, we first generate a set of object center proposals to coarsely identify the high-density region of the object center distribution.","To estimate the score function, we perturb the generated object center proposals by adding normalized Gaussian noise, and then jointly estimate the score function of all perturbed distributions.","Finally, we generate new votes by moving random 3D points to the high-density region of the object center distribution according to the estimated score function.","Extensive experiments on two large scale indoor 3D scene datasets, SUN RGB-D and ScanNet V2, demonstrate the superiority of our proposed method.","The code will be released at https://github.com/HHrEtvP/DiffVote."],"url":"http://arxiv.org/abs/2403.14133v1","category":"cs.CV"}
{"created":"2024-03-21 05:04:01","title":"Enhancing sensitivity of atomic microwave receiver combining laser arrays","abstract":"Rydberg atom,which exhibits a strong response to weak electric(E) fields,is regarded as a promising atomic receiver to surpass sensitivity of conventional receivers. However, its sensitivity is strongly limited by the noise coming from both classical and quantum levels and how to enhance it significantly remains challenging. Here we experimentally prove that the sensitivity of Rydberg atomic receiver can be increased to 23 nV/cm/Hz1/2 by combining laser arrays. Theoretically, we demonstrate that multiple beams illuminating on a PD perform better than multiple PDs for laser arrays.In our experiment,10 dB SNR enhancement is achieved by utilizing 2 * 2 probe beam arrays, compared to the performance of a laser beam,and it can be enhanced further just by adding a resonator. The results could offer an avenue for the design and optimization of ultrahigh-sensitivity Rydberg atomic receivers and promote applications in cosmology, meteorology, communication, and microwave quantum technology.","sentences":["Rydberg atom,which exhibits a strong response to weak electric(E) fields,is regarded as a promising atomic receiver to surpass sensitivity of conventional receivers.","However, its sensitivity is strongly limited by the noise coming from both classical and quantum levels and how to enhance it significantly remains challenging.","Here we experimentally prove that the sensitivity of Rydberg atomic receiver can be increased to 23 nV/cm/Hz1/2 by combining laser arrays.","Theoretically, we demonstrate that multiple beams illuminating on a PD perform better than multiple PDs for laser arrays.","In our experiment,10 dB SNR enhancement is achieved by utilizing 2 * 2 probe beam arrays, compared to the performance of a laser beam,and it can be enhanced further just by adding a resonator.","The results could offer an avenue for the design and optimization of ultrahigh-sensitivity Rydberg atomic receivers and promote applications in cosmology, meteorology, communication, and microwave quantum technology."],"url":"http://arxiv.org/abs/2403.14132v1","category":"physics.atom-ph"}
{"created":"2024-03-21 03:52:01","title":"Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations","abstract":"We introduce CHARM, the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense. We evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5 representative prompt strategies for improving LLMs' reasoning ability, such as Chain-of-Thought. Our findings indicate that the LLM's language orientation and the task's domain influence the effectiveness of the prompt strategy, which enriches previous research findings. We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability, while others show differences in reasoning despite similar memorization performance. We also evaluated the LLMs' memorization-independent reasoning abilities and analyzed the typical errors. Our study precisely identified the LLMs' strengths and weaknesses, providing the clear direction for optimization. It can also serve as a reference for studies in other fields. We will release CHARM at https://github.com/opendatalab/CHARM .","sentences":["We introduce CHARM, the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense.","We evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5 representative prompt strategies for improving LLMs' reasoning ability, such as Chain-of-Thought.","Our findings indicate that the LLM's language orientation and the task's domain influence the effectiveness of the prompt strategy, which enriches previous research findings.","We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability, while others show differences in reasoning despite similar memorization performance.","We also evaluated the LLMs' memorization-independent reasoning abilities and analyzed the typical errors.","Our study precisely identified the LLMs' strengths and weaknesses, providing the clear direction for optimization.","It can also serve as a reference for studies in other fields.","We will release CHARM at https://github.com/opendatalab/CHARM ."],"url":"http://arxiv.org/abs/2403.14112v1","category":"cs.CL"}
{"created":"2024-03-20 23:57:38","title":"Accelerated Objective Gap and Gradient Norm Convergence for Gradient Descent via Long Steps","abstract":"This work considers gradient descent for L-smooth convex optimization with stepsizes larger than the classic regime where descent can be ensured. The stepsize schedules considered are similar to but differ slightly from the concurrently developed silver stepsizes of Altschuler and Parillo. For one of our stepsize sequences, we prove a $O\\left(\\frac{1}{N^{1.2716\\dots}}\\right)$ convergence rate in terms of objective gap decrease and for the other, we show the same rate of decrease for the squared-gradient-norm. This first result improves on the recent result of Altschuler and Parillo by a constant factor, while the second results improve on the exponent of the prior best squared-gradient-norm convergence guarantee of $O(\\frac{1}{N})$.","sentences":["This work considers gradient descent for L-smooth convex optimization with stepsizes larger than the classic regime where descent can be ensured.","The stepsize schedules considered are similar to but differ slightly from the concurrently developed silver stepsizes of Altschuler and Parillo.","For one of our stepsize sequences, we prove a $O\\left(\\frac{1}{N^{1.2716\\dots}}\\right)$ convergence rate in terms of objective gap decrease and for the other, we show the same rate of decrease for the squared-gradient-norm.","This first result improves on the recent result of Altschuler and Parillo by a constant factor, while the second results improve on the exponent of the prior best squared-gradient-norm convergence guarantee of $O(\\frac{1}{N})$."],"url":"http://arxiv.org/abs/2403.14045v1","category":"math.OC"}
{"created":"2024-03-20 23:14:25","title":"Filming runners with drones is hard","abstract":"The use of drones or Unmanned Aerial Vehicles (UAVs) for aerial photography and cinematography is becoming widespread. The following optimization problem has been recently considered. Let us imagine a sporting event where a group of runners are competing and a team of drones with cameras are used to cover the event. The media \\emph{director} selects a set of \\emph{filming scenes} (determined by locations and time intervals) and the goal is to maximize the total \\emph{filming time} (the sum of recordings) achieved by the aerial cinematographers. Recently, it has been showed that this problem can be solved in polynomial time assuming the drones have unlimited battery endurance. In this paper, we prove that the problem is NP-hard for the more realistic case in which the battery endurance of the drones is limited.","sentences":["The use of drones or Unmanned Aerial Vehicles (UAVs) for aerial photography and cinematography is becoming widespread.","The following optimization problem has been recently considered.","Let us imagine a sporting event where a group of runners are competing and a team of drones with cameras are used to cover the event.","The media \\emph{director} selects a set of \\emph{filming scenes} (determined by locations and time intervals) and the goal is to maximize the total \\emph{filming time} (the sum of recordings) achieved by the aerial cinematographers.","Recently, it has been showed that this problem can be solved in polynomial time assuming the drones have unlimited battery endurance.","In this paper, we prove that the problem is NP-hard for the more realistic case in which the battery endurance of the drones is limited."],"url":"http://arxiv.org/abs/2403.14033v1","category":"cs.CG"}
{"created":"2024-03-20 22:56:11","title":"Performance-Guaranteed Solutions for Multi-Agent Optimal Coverage Problems using Submodularity, Curvature, and Greedy Algorithms","abstract":"We consider a class of multi-agent optimal coverage problems where the goal is to determine the optimal placement of a group of agents in a given mission space such that they maximize a joint ``coverage'' objective. This class of problems is extremely challenging due to the non-convex nature of the mission space and of the coverage objective. With this motivation, we propose to use a greedy algorithm as a means of getting feasible coverage solutions efficiently. Even though such greedy solutions are suboptimal, the submodularity (diminishing returns) property of the coverage objective can be exploited to provide performance bound guarantees - not only for the greedy solutions but also for any subsequently improved solutions. Moreover, we show that improved performance bound guarantees (beyond the standard (1-1/e) performance bound) can be established using various curvature measures that further characterize the considered coverage problem. In particular, we provide a brief review of all existing popular curvature measures found in the submodular maximization literature, including a recent curvature measure that we proposed, and discuss in detail their applicability, practicality, and effectiveness in the context of optimal coverage problems. Moreover, we characterize the dependence of the effectiveness of different curvature measures (in providing improved performance bound guarantees) on the agent sensing capabilities. Finally, we provide several numerical results to support our findings and propose several potential future research directions.","sentences":["We consider a class of multi-agent optimal coverage problems where the goal is to determine the optimal placement of a group of agents in a given mission space such that they maximize a joint ``coverage'' objective.","This class of problems is extremely challenging due to the non-convex nature of the mission space and of the coverage objective.","With this motivation, we propose to use a greedy algorithm as a means of getting feasible coverage solutions efficiently.","Even though such greedy solutions are suboptimal, the submodularity (diminishing returns) property of the coverage objective can be exploited to provide performance bound guarantees - not only for the greedy solutions but also for any subsequently improved solutions.","Moreover, we show that improved performance bound guarantees (beyond the standard (1-1/e) performance bound) can be established using various curvature measures that further characterize the considered coverage problem.","In particular, we provide a brief review of all existing popular curvature measures found in the submodular maximization literature, including a recent curvature measure that we proposed, and discuss in detail their applicability, practicality, and effectiveness in the context of optimal coverage problems.","Moreover, we characterize the dependence of the effectiveness of different curvature measures (in providing improved performance bound guarantees) on the agent sensing capabilities.","Finally, we provide several numerical results to support our findings and propose several potential future research directions."],"url":"http://arxiv.org/abs/2403.14028v1","category":"eess.SY"}
{"created":"2024-03-20 22:21:22","title":"A Unified Toll Lane Framework for Autonomous and High-Occupancy Vehicles in Interactive Mixed Autonomy","abstract":"In this study, we introduce a toll lane framework that optimizes the mixed flow of autonomous and high-occupancy vehicles on freeways, where human-driven and autonomous vehicles of varying commuter occupancy share a segment. Autonomous vehicles, with their ability to maintain shorter headways, boost traffic throughput. Our framework designates a toll lane for autonomous vehicles with high occupancy to use free of charge, while others pay a toll. We explore the lane choice equilibria when all vehicles minimize travel costs, and characterize the equilibria by ranking vehicles by their mobility enhancement potential, a concept we term the mobility degree. Through numerical examples, we demonstrate the framework's utility in addressing design challenges such as setting optimal tolls, determining occupancy thresholds, and designing lane policies, showing how it facilitates the integration of high-occupancy and autonomous vehicles. We also propose an algorithm for assigning rational tolls to decrease total commuter delay and examine the effects of toll non-compliance. Our findings suggest that self-interest-driven behavior mitigates moderate non-compliance impacts, highlighting the framework's resilience. This work presents a pioneering comprehensive analysis of a toll lane framework that emphasizes the coexistence of autonomous and high-occupancy vehicles, offering insights for traffic management improvements and the integration of autonomous vehicles into existing transportation infrastructures.","sentences":["In this study, we introduce a toll lane framework that optimizes the mixed flow of autonomous and high-occupancy vehicles on freeways, where human-driven and autonomous vehicles of varying commuter occupancy share a segment.","Autonomous vehicles, with their ability to maintain shorter headways, boost traffic throughput.","Our framework designates a toll lane for autonomous vehicles with high occupancy to use free of charge, while others pay a toll.","We explore the lane choice equilibria when all vehicles minimize travel costs, and characterize the equilibria by ranking vehicles by their mobility enhancement potential, a concept we term the mobility degree.","Through numerical examples, we demonstrate the framework's utility in addressing design challenges such as setting optimal tolls, determining occupancy thresholds, and designing lane policies, showing how it facilitates the integration of high-occupancy and autonomous vehicles.","We also propose an algorithm for assigning rational tolls to decrease total commuter delay and examine the effects of toll non-compliance.","Our findings suggest that self-interest-driven behavior mitigates moderate non-compliance impacts, highlighting the framework's resilience.","This work presents a pioneering comprehensive analysis of a toll lane framework that emphasizes the coexistence of autonomous and high-occupancy vehicles, offering insights for traffic management improvements and the integration of autonomous vehicles into existing transportation infrastructures."],"url":"http://arxiv.org/abs/2403.14011v1","category":"eess.SY"}
{"created":"2024-03-20 22:15:33","title":"When are Lossy Energy Storage Optimization Models Convex?","abstract":"We consider a class of optimization problems involving the optimal operation of a single lossy energy storage system that incurs energy loss when charging or discharging. Such inefficiencies in the energy storage dynamics are known to result in a nonconvex set of feasible charging and discharging power profiles. In this letter, we provide an equivalent reformulation for this class of optimization problems, along with sufficient conditions for the convexity of the proposed reformulation. The conditions provided generalize existing conditions for convexity in the literature.","sentences":["We consider a class of optimization problems involving the optimal operation of a single lossy energy storage system that incurs energy loss when charging or discharging.","Such inefficiencies in the energy storage dynamics are known to result in a nonconvex set of feasible charging and discharging power profiles.","In this letter, we provide an equivalent reformulation for this class of optimization problems, along with sufficient conditions for the convexity of the proposed reformulation.","The conditions provided generalize existing conditions for convexity in the literature."],"url":"http://arxiv.org/abs/2403.14010v1","category":"eess.SY"}
{"created":"2024-03-20 22:08:41","title":"Pricing4SaaS: a suite of software libraries for pricing-driven feature toggling","abstract":"As the digital marketplace evolves, the ability to dynamically adjust or disable features and services in response to market demands and pricing strategies becomes increasingly crucial for maintaining competitive advantage and enhancing user engagement. This paper introduces a novel suite of software libraries named Pricing4SaaS, designed to facilitate the implementation of pricing-driven feature toggles in both the front-end and back-end of SaaS systems, and discuss its architectural design principles. Including Pricing4React for front-end and Pricing4Java for back-end, the suite enables developers a streamlined and efficient approach to integrating feature toggles that can be controlled based on pricing plans, emphasizing centralized toggle management, and secure synchronization of the toggling state between the client and server. We also present a case study based on the popular Spring PetClinic project to illustrate how the suite can be leveraged to optimize developer productivity, avoiding technical debt, and improving operational efficiency.","sentences":["As the digital marketplace evolves, the ability to dynamically adjust or disable features and services in response to market demands and pricing strategies becomes increasingly crucial for maintaining competitive advantage and enhancing user engagement.","This paper introduces a novel suite of software libraries named Pricing4SaaS, designed to facilitate the implementation of pricing-driven feature toggles in both the front-end and back-end of SaaS systems, and discuss its architectural design principles.","Including Pricing4React for front-end and Pricing4Java for back-end, the suite enables developers a streamlined and efficient approach to integrating feature toggles that can be controlled based on pricing plans, emphasizing centralized toggle management, and secure synchronization of the toggling state between the client and server.","We also present a case study based on the popular Spring PetClinic project to illustrate how the suite can be leveraged to optimize developer productivity, avoiding technical debt, and improving operational efficiency."],"url":"http://arxiv.org/abs/2403.14004v1","category":"cs.SE"}
{"created":"2024-03-20 21:45:33","title":"Maximum Likelihood Alternating Summation for Multistatic Angle-based Multitarget Localization","abstract":"Recent advancements in Wi-Fi sensing have sparked interest in exploiting OFDM modulated communication signals for target detection and tracking. In this study, we address the angle-based localization of multiple targets using a multistatic OFDM radar. While the maximum likelihood approach optimally merges data from each radar pair comprised by the system, it entails a complex multi-dimensional search process. Leveraging pre-estimation of the targets' parameters obtained via the MUSIC algorithm, our method decouples this multi-dimensional search into a single two-dimensional estimator per target. The proposed alternating summation method allows the computation of a combined likelihood map aggregating contributions from each radar pair, enabling target detection via peak selection. Besides reducing computational complexity, the method effectively captures target interactions and accommodates varying radar pair localization abilities. Also, it requires transmitting only the estimated channel covariance matrices of each radar pair to the central processor. Numerical simulations demonstrate superior performance over existing approaches.","sentences":["Recent advancements in Wi-Fi sensing have sparked interest in exploiting OFDM modulated communication signals for target detection and tracking.","In this study, we address the angle-based localization of multiple targets using a multistatic OFDM radar.","While the maximum likelihood approach optimally merges data from each radar pair comprised by the system, it entails a complex multi-dimensional search process.","Leveraging pre-estimation of the targets' parameters obtained via the MUSIC algorithm, our method decouples this multi-dimensional search into a single two-dimensional estimator per target.","The proposed alternating summation method allows the computation of a combined likelihood map aggregating contributions from each radar pair, enabling target detection via peak selection.","Besides reducing computational complexity, the method effectively captures target interactions and accommodates varying radar pair localization abilities.","Also, it requires transmitting only the estimated channel covariance matrices of each radar pair to the central processor.","Numerical simulations demonstrate superior performance over existing approaches."],"url":"http://arxiv.org/abs/2403.13992v1","category":"eess.SP"}
{"created":"2024-03-20 19:18:10","title":"Data integration methods for micro-randomized trials","abstract":"Existing statistical methods for the analysis of micro-randomized trials (MRTs) are designed to estimate causal excursion effects using data from a single MRT. In practice, however, researchers can often find previous MRTs that employ similar interventions. In this paper, we develop data integration methods that capitalize on this additional information, leading to statistical efficiency gains. To further increase efficiency, we demonstrate how to combine these approaches according to a generalization of multivariate precision weighting that allows for correlation between estimates, and we show that the resulting meta-estimator possesses an asymptotic optimality property. We illustrate our methods in simulation and in a case study involving two MRTs in the area of smoking cessation.","sentences":["Existing statistical methods for the analysis of micro-randomized trials (MRTs) are designed to estimate causal excursion effects using data from a single MRT.","In practice, however, researchers can often find previous MRTs that employ similar interventions.","In this paper, we develop data integration methods that capitalize on this additional information, leading to statistical efficiency gains.","To further increase efficiency, we demonstrate how to combine these approaches according to a generalization of multivariate precision weighting that allows for correlation between estimates, and we show that the resulting meta-estimator possesses an asymptotic optimality property.","We illustrate our methods in simulation and in a case study involving two MRTs in the area of smoking cessation."],"url":"http://arxiv.org/abs/2403.13934v1","category":"stat.ME"}
{"created":"2024-03-20 19:03:26","title":"Safety-Aware Perception for Autonomous Collision Avoidance in Dynamic Environments","abstract":"Autonomous collision avoidance requires accurate environmental perception; however, flight systems often possess limited sensing capabilities with field-of-view (FOV) restrictions. To navigate this challenge, we present a safety-aware approach for online determination of the optimal sensor-pointing direction $\\psi_\\text{d}$ which utilizes control barrier functions (CBFs). First, we generate a spatial density function $\\Phi$ which leverages CBF constraints to map the collision risk of all local coordinates. Then, we convolve $\\Phi$ with an attitude-dependent sensor FOV quality function to produce the objective function $\\Gamma$ which quantifies the total observed risk for a given pointing direction. Finally, by finding the global optimizer for $\\Gamma$, we identify the value of $\\psi_\\text{d}$ which maximizes the perception of risk within the FOV. We incorporate $\\psi_\\text{d}$ into a safety-critical flight architecture and conduct a numerical analysis using multiple simulated mission profiles. Our algorithm achieves a success rate of $88-96\\%$, constituting a $16-29\\%$ improvement compared to the best heuristic methods. We demonstrate the functionality of our approach via a flight demonstration using the Crazyflie 2.1 micro-quadrotor. Without a priori obstacle knowledge, the quadrotor follows a dynamic flight path while simultaneously calculating and tracking $\\psi_\\text{d}$ to perceive and avoid two static obstacles with an average computation time of 371 $\\mu$s.","sentences":["Autonomous collision avoidance requires accurate environmental perception; however, flight systems often possess limited sensing capabilities with field-of-view (FOV) restrictions.","To navigate this challenge, we present a safety-aware approach for online determination of the optimal sensor-pointing direction $\\psi_\\text{d}$ which utilizes control barrier functions (CBFs).","First, we generate a spatial density function $\\Phi$ which leverages CBF constraints to map the collision risk of all local coordinates.","Then, we convolve $\\Phi$ with an attitude-dependent sensor FOV quality function to produce the objective function $\\Gamma$ which quantifies the total observed risk for a given pointing direction.","Finally, by finding the global optimizer for $\\Gamma$, we identify the value of $\\psi_\\text{d}$ which maximizes the perception of risk within the FOV.","We incorporate $\\psi_\\text{d}$ into a safety-critical flight architecture and conduct a numerical analysis using multiple simulated mission profiles.","Our algorithm achieves a success rate of $88-96\\%$, constituting a $16-29\\%$ improvement compared to the best heuristic methods.","We demonstrate the functionality of our approach via a flight demonstration using the Crazyflie 2.1 micro-quadrotor.","Without a priori obstacle knowledge, the quadrotor follows a dynamic flight path while simultaneously calculating and tracking $\\psi_\\text{d}$ to perceive and avoid two static obstacles with an average computation time of 371 $\\mu$s."],"url":"http://arxiv.org/abs/2403.13929v1","category":"cs.RO"}
{"created":"2024-03-20 18:23:15","title":"Motion Prediction of Multi-agent systems with Multi-view clustering","abstract":"This paper presents a method for future motion prediction of multi-agent systems by including group formation information and future intent. Formation of groups depends on a physics-based clustering method that follows the agglomerative hierarchical clustering algorithm. We identify clusters that incorporate the minimum cost-to-go function of a relevant optimal control problem as a metric for clustering between the groups among agents, where groups with similar associated costs are assumed to be likely to move together. The cost metric accounts for proximity to other agents as well as the intended goal of each agent. An unscented Kalman filter based approach is used to update the established clusters as well as add new clusters when new information is obtained. Our approach is verified through non-trivial numerical simulations implementing the proposed algorithm on different datasets pertaining to a variety of scenarios and agents.","sentences":["This paper presents a method for future motion prediction of multi-agent systems by including group formation information and future intent.","Formation of groups depends on a physics-based clustering method that follows the agglomerative hierarchical clustering algorithm.","We identify clusters that incorporate the minimum cost-to-go function of a relevant optimal control problem as a metric for clustering between the groups among agents, where groups with similar associated costs are assumed to be likely to move together.","The cost metric accounts for proximity to other agents as well as the intended goal of each agent.","An unscented Kalman filter based approach is used to update the established clusters as well as add new clusters when new information is obtained.","Our approach is verified through non-trivial numerical simulations implementing the proposed algorithm on different datasets pertaining to a variety of scenarios and agents."],"url":"http://arxiv.org/abs/2403.13905v1","category":"cs.MA"}
{"created":"2024-03-20 18:05:52","title":"Data Acquisition via Experimental Design for Decentralized Data Markets","abstract":"Acquiring high-quality training data is essential for current machine learning models. Data markets provide a way to increase the supply of data, particularly in data-scarce domains such as healthcare, by incentivizing potential data sellers to join the market. A major challenge for a data buyer in such a market is selecting the most valuable data points from a data seller. Unlike prior work in data valuation, which assumes centralized data access, we propose a federated approach to the data selection problem that is inspired by linear experimental design. Our proposed data selection method achieves lower prediction error without requiring labeled validation data and can be optimized in a fast and federated procedure. The key insight of our work is that a method that directly estimates the benefit of acquiring data for test set prediction is particularly compatible with a decentralized market setting.","sentences":["Acquiring high-quality training data is essential for current machine learning models.","Data markets provide a way to increase the supply of data, particularly in data-scarce domains such as healthcare, by incentivizing potential data sellers to join the market.","A major challenge for a data buyer in such a market is selecting the most valuable data points from a data seller.","Unlike prior work in data valuation, which assumes centralized data access, we propose a federated approach to the data selection problem that is inspired by linear experimental design.","Our proposed data selection method achieves lower prediction error without requiring labeled validation data and can be optimized in a fast and federated procedure.","The key insight of our work is that a method that directly estimates the benefit of acquiring data for test set prediction is particularly compatible with a decentralized market setting."],"url":"http://arxiv.org/abs/2403.13893v1","category":"cs.LG"}
{"created":"2024-03-20 09:23:20","title":"Optimal Transport for Fairness: Archival Data Repair using Small Research Data Sets","abstract":"With the advent of the AI Act and other regulations, there is now an urgent need for algorithms that repair unfairness in training data. In this paper, we define fairness in terms of conditional independence between protected attributes ($S$) and features ($X$), given unprotected attributes ($U$). We address the important setting in which torrents of archival data need to be repaired, using only a small proportion of these data, which are $S|U$-labelled (the research data). We use the latter to design optimal transport (OT)-based repair plans on interpolated supports. This allows {\\em off-sample}, labelled, archival data to be repaired, subject to stationarity assumptions. It also significantly reduces the size of the supports of the OT plans, with correspondingly large savings in the cost of their design and of their {\\em sequential\\/} application to the off-sample data. We provide detailed experimental results with simulated and benchmark real data (the Adult data set). Our performance figures demonstrate effective repair -- in the sense of quenching conditional dependence -- of large quantities of off-sample, labelled (archival) data.","sentences":["With the advent of the AI Act and other regulations, there is now an urgent need for algorithms that repair unfairness in training data.","In this paper, we define fairness in terms of conditional independence between protected attributes ($S$) and features ($X$), given unprotected attributes ($U$).","We address the important setting in which torrents of archival data need to be repaired, using only a small proportion of these data, which are $S|U$-labelled (the research data).","We use the latter to design optimal transport (OT)-based repair plans on interpolated supports.","This allows {\\em off-sample}, labelled, archival data to be repaired, subject to stationarity assumptions.","It also significantly reduces the size of the supports of the OT plans, with correspondingly large savings in the cost of their design and of their {\\em sequential\\/} application to the off-sample data.","We provide detailed experimental results with simulated and benchmark real data (the Adult data set).","Our performance figures demonstrate effective repair -- in the sense of quenching conditional dependence -- of large quantities of off-sample, labelled (archival) data."],"url":"http://arxiv.org/abs/2403.13864v1","category":"cs.LG"}
{"created":"2024-03-21 17:59:32","title":"From Local to Emergent Altermagnetism: Footprints of Free Fermions Band Topology","abstract":"Altermagnets are crystallographic rotational symmetry breaking spin-ordered states, possessing a net zero magnetization despite manifesting Kramers non-degenerate bands. Here, we show that momentum-independent local spin nematic orders in monolayer, Bernal bilayer and rhombohedral trilayer graphene give rise to $p$-wave, $d$-wave and $f$-wave altermagnets, respectively, thereby inheriting the free-fermion topology of linear, bi-quadratic and bi-cubic band touchings that are also described in terms of angular momentum $\\ell=1,\\; 2$ and $3$ spherical harmonics in the reciprocal space. The same conclusions also hold inside a nematic spin-triplet superconductor, featuring Majorana altermagnets. Altogether, these findings highlight the importance of electronic band structure in identifying such exotic magnetic orders in real materials. We depict the effects of in-plane magnetic fields on altermagnets, and showcase a novel spin-disordered alter-valleymagnet phase.","sentences":["Altermagnets are crystallographic rotational symmetry breaking spin-ordered states, possessing a net zero magnetization despite manifesting Kramers non-degenerate bands.","Here, we show that momentum-independent local spin nematic orders in monolayer, Bernal bilayer and rhombohedral trilayer graphene give rise to $p$-wave, $d$-wave and $f$-wave altermagnets, respectively, thereby inheriting the free-fermion topology of linear, bi-quadratic and bi-cubic band touchings that are also described in terms of angular momentum $\\ell=1,\\; 2$ and $3$ spherical harmonics in the reciprocal space.","The same conclusions also hold inside a nematic spin-triplet superconductor, featuring Majorana altermagnets.","Altogether, these findings highlight the importance of electronic band structure in identifying such exotic magnetic orders in real materials.","We depict the effects of in-plane magnetic fields on altermagnets, and showcase a novel spin-disordered alter-valleymagnet phase."],"url":"http://arxiv.org/abs/2403.14620v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-21 17:58:03","title":"Observing the Galactic Underworld: Predicting photometry and astrometry from compact remnant microlensing events","abstract":"Isolated black holes (BHs) and neutron stars (NSs) are largely undetectable across the electromagnetic spectrum. For this reason, our only real prospect of observing these isolated compact remnants is via microlensing; a feat recently performed for the first time. However, characterisation of the microlensing events caused by BHs and NSs is still in its infancy. In this work, we perform N-body simulations to explore the frequency and physical characteristics of microlensing events across the entire sky. Our simulations find that every year we can expect $88_{-6}^{+6}$ BH, $6.8_{-1.6}^{+1.7}$ NS and $20^{+30}_{-20}$ stellar microlensing events which cause an astrometric shift larger than 2~mas. Similarly, we can expect $21_{-3}^{+3}$ BH, $18_{-3}^{+3}$ NS and $7500_{-500}^{+500}$ stellar microlensing events which cause a bump magnitude larger than 1~mag. Leveraging a more comprehensive dynamical model than prior work, we predict the fraction of microlensing events caused by BHs as a function of Einstein time to be smaller than previously thought. Comparison of our microlensing simulations to events from in Gaia finds good agreement. Finally, we predict that in the combination of Gaia and GaiaNIR data there will be $14700_{-900}^{+600}$ BH and $1600_{-200}^{+300}$ NS events creating a centroid shift larger than 1~mas and $330_{-120}^{+100}$ BH and $310_{-100}^{+110}$ NS events causing bump magnitudes $< -1$. Of these, $<10$ BH and $5_{-5}^{+10}$ NS events should be detectable using current analysis techniques. These results inform future astrometric mission design, such as GaiaNIR, as they indicate that, compared to stellar events, there are fewer observable BH events than previously thought.","sentences":["Isolated black holes (BHs) and neutron stars (NSs) are largely undetectable across the electromagnetic spectrum.","For this reason, our only real prospect of observing these isolated compact remnants is via microlensing; a feat recently performed for the first time.","However, characterisation of the microlensing events caused by BHs and NSs is still in its infancy.","In this work, we perform N-body simulations to explore the frequency and physical characteristics of microlensing events across the entire sky.","Our simulations find that every year we can expect $88_{-6}^{+6}$ BH, $6.8_{-1.6}^{+1.7}$ NS and $20^{+30}_{-20}$ stellar microlensing events which cause an astrometric shift larger than 2~mas.","Similarly, we can expect $21_{-3}^{+3}$ BH, $18_{-3}^{+3}$ NS and $7500_{-500}^{+500}$ stellar microlensing events which cause a bump magnitude larger than 1~mag.","Leveraging a more comprehensive dynamical model than prior work, we predict the fraction of microlensing events caused by BHs as a function of Einstein time to be smaller than previously thought.","Comparison of our microlensing simulations to events from in Gaia finds good agreement.","Finally, we predict that in the combination of Gaia and GaiaNIR data there will be $14700_{-900}^{+600}$ BH and $1600_{-200}^{+300}$ NS events creating a centroid shift larger than 1~mas and $330_{-120}^{+100}$ BH and $310_{-100}^{+110}$ NS events causing bump magnitudes $< -1$.","Of these, $<10$ BH and $5_{-5}^{+10}$ NS events should be detectable using current analysis techniques.","These results inform future astrometric mission design, such as GaiaNIR, as they indicate that, compared to stellar events, there are fewer observable BH events than previously thought."],"url":"http://arxiv.org/abs/2403.14612v1","category":"astro-ph.GA"}
{"created":"2024-03-21 17:43:28","title":"Exponential Networks for Linear Partitions","abstract":"Previous work has given proof and evidence that BPS states in local Calabi-Yau 3-folds can be described and counted by exponential networks on the punctured plane, with the help of a suitable non-abelianization map to the mirror curve. This provides an appealing elementary depiction of moduli of special Lagrangian submanifolds, but so far only a handful of examples have been successfully worked out in detail. In this note, we exhibit an explicit correspondence between torus fixed points of the Hilbert scheme of points on $\\mathbb C^2\\subset\\mathbb C^3$ and anomaly free exponential networks attached to the quadratically framed pair of pants. This description realizes an interesting, and seemingly novel, \"age decomposition\" of linear partitions. We also provide further details about the networks' perspective on the full D-brane moduli space.","sentences":["Previous work has given proof and evidence that BPS states in local Calabi-Yau 3-folds can be described and counted by exponential networks on the punctured plane, with the help of a suitable non-abelianization map to the mirror curve.","This provides an appealing elementary depiction of moduli of special Lagrangian submanifolds, but so far only a handful of examples have been successfully worked out in detail.","In this note, we exhibit an explicit correspondence between torus fixed points of the Hilbert scheme of points on $\\mathbb C^2\\subset\\mathbb C^3$ and anomaly free exponential networks attached to the quadratically framed pair of pants.","This description realizes an interesting, and seemingly novel, \"age decomposition\" of linear partitions.","We also provide further details about the networks' perspective on the full D-brane moduli space."],"url":"http://arxiv.org/abs/2403.14588v1","category":"hep-th"}
{"created":"2024-03-21 17:27:41","title":"Fully Evaluated Left-Sequential Logics","abstract":"We consider a family of two-valued \"fully evaluated left-sequential logics\" (FELs), of which Free FEL (defined by Staudt in 2012) is most distinguishing (weakest) and immune to atomic side effects. Next is Memorising FEL, in which evaluations of subexpressions are memorised. The following stronger logic is Conditional FEL (inspired by Guzm\\'an and Squier's Conditional logic, 1990). The strongest FEL is static FEL, a sequential version of propositional logic. We use evaluation trees as a simple, intuitive semantics and provide complete axiomatisations for closed terms (left-sequential propositional expressions).   For each FEL except Static FEL, we also define its three-valued version, with a constant U for \"undefinedness\" and again provide complete, independent aziomatisations, each one containing two additional axioms for U on top of the axiomatisations of the two-valued case. In this setting, the strongest FEL is equivalent to Bochvar's strict logic.","sentences":["We consider a family of two-valued \"fully evaluated left-sequential logics\" (FELs), of which Free FEL (defined by Staudt in 2012) is most distinguishing (weakest) and immune to atomic side effects.","Next is Memorising FEL, in which evaluations of subexpressions are memorised.","The following stronger logic is Conditional FEL (inspired by Guzm\\'an and Squier's Conditional logic, 1990).","The strongest FEL is static FEL, a sequential version of propositional logic.","We use evaluation trees as a simple, intuitive semantics and provide complete axiomatisations for closed terms (left-sequential propositional expressions).   ","For each FEL except Static FEL, we also define its three-valued version, with a constant U for \"undefinedness\" and again provide complete, independent aziomatisations, each one containing two additional axioms for U on top of the axiomatisations of the two-valued case.","In this setting, the strongest FEL is equivalent to Bochvar's strict logic."],"url":"http://arxiv.org/abs/2403.14576v1","category":"cs.LO"}
{"created":"2024-03-21 16:44:16","title":"Conformal Perturbation Theory and Tachyon-Dilaton Eschatology via String Fields","abstract":"We analyze deformations of two-dimensional conformal field theory (CFT) from the perspective of classical bosonic closed string field theory (SFT). The latter can be viewed as a version of Wilsonian renormalization group (RG) improved conformal perturbation theory, where the renormalization scheme is defined through the choice of string vertices in the construction of SFT. Furthermore, the CFT data at the RG fixed point can be recovered from the spectrum and amplitudes of string field fluctuations. As applications, we construct the Horowitz-Polchinski \"string star\" solution in SFT, and a solution of tachyon-dilaton condensation that deforms the noncompact boson to minimal models by creating a pair of \"Runkel-Watts walls\".","sentences":["We analyze deformations of two-dimensional conformal field theory (CFT) from the perspective of classical bosonic closed string field theory (SFT).","The latter can be viewed as a version of Wilsonian renormalization group (RG) improved conformal perturbation theory, where the renormalization scheme is defined through the choice of string vertices in the construction of SFT.","Furthermore, the CFT data at the RG fixed point can be recovered from the spectrum and amplitudes of string field fluctuations.","As applications, we construct the Horowitz-Polchinski \"string star\" solution in SFT, and a solution of tachyon-dilaton condensation that deforms the noncompact boson to minimal models by creating a pair of \"Runkel-Watts walls\"."],"url":"http://arxiv.org/abs/2403.14544v1","category":"hep-th"}
{"created":"2024-03-21 16:39:12","title":"Qu8its for Quantum Simulations of Lattice Quantum Chromodynamics","abstract":"We explore the utility of $d=8$ qudits, qu8its, for quantum simulations of the dynamics of 1+1D SU(3) lattice quantum chromodynamics, including a mapping for arbitrary numbers of flavors and lattice size and a re-organization of the Hamiltonian for efficient time-evolution. Recent advances in parallel gate applications, along with the shorter application times of single-qudit operations compared with two-qudit operations, lead to significant projected advantages in quantum simulation fidelities and circuit depths using qu8its rather than qubits. The number of two-qudit entangling gates required for time evolution using qu8its is found to be more than a factor of five fewer than for qubits. We anticipate that the developments presented in this work will enable improved quantum simulations to be performed using emerging quantum hardware.","sentences":["We explore the utility of $d=8$ qudits, qu8its, for quantum simulations of the dynamics of 1+1D SU(3) lattice quantum chromodynamics, including a mapping for arbitrary numbers of flavors and lattice size and a re-organization of the Hamiltonian for efficient time-evolution.","Recent advances in parallel gate applications, along with the shorter application times of single-qudit operations compared with two-qudit operations, lead to significant projected advantages in quantum simulation fidelities and circuit depths using qu8its rather than qubits.","The number of two-qudit entangling gates required for time evolution using qu8its is found to be more than a factor of five fewer than for qubits.","We anticipate that the developments presented in this work will enable improved quantum simulations to be performed using emerging quantum hardware."],"url":"http://arxiv.org/abs/2403.14537v1","category":"quant-ph"}
{"created":"2024-03-21 16:26:42","title":"Manifest color-kinematics duality for point particles interacting with self-dual fields","abstract":"We find that point particles interacting with a self-dual Yang-Mills field and self-dual gravity manifestly satisfy color-kinematics duality at the level of action. In a similar way color-kinematics duality also holds for a scalar field minimally coupled to a self-dual Yang-Mills field and self-dual gravity. By applying the appropriate limiting procedure to these scalar field theories, we reproduce point particle theories we started from. This allows us to connect worldline color-kinematics duality to amplitude color-kinematics duality in field theory. Considering that point particles act as sources of classical solutions, our results may be regarded as a step towards establishing a precise relation between the amplitude and the classical double copies in the self-dual sector. Finally, we briefly mention that the extension of this discussion to the higher-spin case suggests that scalar point particles cannot interact with chiral higher-spin fields.","sentences":["We find that point particles interacting with a self-dual Yang-Mills field and self-dual gravity manifestly satisfy color-kinematics duality at the level of action.","In a similar way color-kinematics duality also holds for a scalar field minimally coupled to a self-dual Yang-Mills field and self-dual gravity.","By applying the appropriate limiting procedure to these scalar field theories, we reproduce point particle theories we started from.","This allows us to connect worldline color-kinematics duality to amplitude color-kinematics duality in field theory.","Considering that point particles act as sources of classical solutions, our results may be regarded as a step towards establishing a precise relation between the amplitude and the classical double copies in the self-dual sector.","Finally, we briefly mention that the extension of this discussion to the higher-spin case suggests that scalar point particles cannot interact with chiral higher-spin fields."],"url":"http://arxiv.org/abs/2403.14527v1","category":"hep-th"}
{"created":"2024-03-21 16:00:36","title":"CO2 capture using boron, nitrogen, and phosphorus-doped C20 in the present electric field: A DFT study","abstract":"Burning fossil fuels emits a significant amount of CO2, causing climate change concerns. CO2 Capture and Storage (CCS) aims to reduce emissions, with fullerenes showing promise as CO2 adsorbents. Recent research focuses on modifying fullerenes using an electric field. In light of this, we carried out DFT studies on some B, N, and P doped C20 (C20-nXn (n = 0, 1, 2, and 3; X = B, N, and P)) in the absence and presence of an electric field in the range of 0-0.02 a.u.. The cohesive energy was calculated to ensure their thermodynamic stability showing, that despite having lesser cohesive energies than C20, they appear in a favorable range. Moreover, the charge distribution for all structures was depicted using the ESP map. Most importantly, we evaluated the adsorption energy, height, and CO2 angle, demonstrating the B and N-doped fullerenes had the stronger interaction with CO2, which by far exceeded C20's, improving its physisorption to physicochemical adsorption. Although the adsorption energy of P-doped fullerenes was not as satisfactory, in most cases, increasing the electric field led to enhancing CO2 adsorption and incorporating chemical attributes to CO2-fullerene interaction. The HOMO--LUMO plots were obtained by which we discovered that unlike the P-doped C20, the surprising activity of B and N-doped C20s against CO2 originates from a high concentration of the HOMO-LUMO orbitals on B and N atoms. Additionally, the charge distribution for all structures was depicted using the ESP map. In the present article, we attempt to introduce more effective fullerene-based materials for CO2 capture as well as strategies to enhance their efficiency and revealing adsorption nature over B, N, and P-doped fullerenes.","sentences":["Burning fossil fuels emits a significant amount of CO2, causing climate change concerns.","CO2 Capture and Storage (CCS) aims to reduce emissions, with fullerenes showing promise as CO2 adsorbents.","Recent research focuses on modifying fullerenes using an electric field.","In light of this, we carried out DFT studies on some B, N, and P doped C20 (C20-nXn (n = 0, 1, 2, and 3; X = B, N, and P)) in the absence and presence of an electric field in the range of 0-0.02 a.u..","The cohesive energy was calculated to ensure their thermodynamic stability showing, that despite having lesser cohesive energies than C20, they appear in a favorable range.","Moreover, the charge distribution for all structures was depicted using the ESP map.","Most importantly, we evaluated the adsorption energy, height, and CO2 angle, demonstrating the B and N-doped fullerenes had the stronger interaction with CO2, which by far exceeded C20's, improving its physisorption to physicochemical adsorption.","Although the adsorption energy of P-doped fullerenes was not as satisfactory, in most cases, increasing the electric field led to enhancing CO2 adsorption and incorporating chemical attributes to CO2-fullerene interaction.","The HOMO--LUMO plots were obtained by which we discovered that unlike the P-doped C20, the surprising activity of B and N-doped C20s against CO2 originates from a high concentration of the HOMO-LUMO orbitals on B and N atoms.","Additionally, the charge distribution for all structures was depicted using the ESP map.","In the present article, we attempt to introduce more effective fullerene-based materials for CO2 capture as well as strategies to enhance their efficiency and revealing adsorption nature over B, N, and P-doped fullerenes."],"url":"http://arxiv.org/abs/2403.14507v1","category":"physics.chem-ph"}
{"created":"2024-03-21 15:55:23","title":"A new class of axion haloscope resonators: the polygonal coaxial cavity","abstract":"In the search for axionic Dark Matter, the high frequency part of the QCD axion parameter space is favored, as indicated by both cosmological and astrophysical arguments and recent indications from lattice QCD calculations. To extend the probing range of cavity haloscopes, solutions addressing the unfavorable scaling of cavity volume with frequency must be developed. Here, we present a novel type of high-volume thin shell resonator for high frequency haloscope dark matter searches. The cavity is formed by two nested and coaxial right angle polygonal prisms enclosed within two flat endcaps. For the axion-sensitive (pseudo-)TM010 mode, finite element simulations yield form factor of the order of 0.8 and Q factor of the order of 60000 for a copper cavity at 4$\\,$K. High tunability of up to $\\sim 5\\%$ is achieved by reciprocal rotation of the two prisms, without significant changes in haloscope sensitivity. A prototype aluminium hexagonal cavity was built and tested, confirming the main characteristics of the design.","sentences":["In the search for axionic Dark Matter, the high frequency part of the QCD axion parameter space is favored, as indicated by both cosmological and astrophysical arguments and recent indications from lattice QCD calculations.","To extend the probing range of cavity haloscopes, solutions addressing the unfavorable scaling of cavity volume with frequency must be developed.","Here, we present a novel type of high-volume thin shell resonator for high frequency haloscope dark matter searches.","The cavity is formed by two nested and coaxial right angle polygonal prisms enclosed within two flat endcaps.","For the axion-sensitive (pseudo-)TM010 mode, finite element simulations yield form factor of the order of 0.8 and Q factor of the order of 60000 for a copper cavity at 4$\\,$K. High tunability of up to $\\sim 5\\%$ is achieved by reciprocal rotation of the two prisms, without significant changes in haloscope sensitivity.","A prototype aluminium hexagonal cavity was built and tested, confirming the main characteristics of the design."],"url":"http://arxiv.org/abs/2403.14503v1","category":"physics.ins-det"}
{"created":"2024-03-21 15:55:02","title":"An On-Shell Derivation of the Soft Effective Action in Abelian Gauge Theories","abstract":"We derive the soft effective action in $(d+2)$-dimensional abelian gauge theories from the on-shell action obeying Neumann boundary conditions at timelike and null infinity and Dirichlet boundary conditions at spatial infinity. This allows us to identify the on-shell degrees of freedom on the boundary with the soft modes living on the celestial sphere. Following the work of Donnelly and Wall, this suggests that we can interpret soft modes as entanglement edge modes on the celestial sphere and study entanglement properties of soft modes in abelian gauge theories.","sentences":["We derive the soft effective action in $(d+2)$-dimensional abelian gauge theories from the on-shell action obeying Neumann boundary conditions at timelike and null infinity and Dirichlet boundary conditions at spatial infinity.","This allows us to identify the on-shell degrees of freedom on the boundary with the soft modes living on the celestial sphere.","Following the work of Donnelly and Wall, this suggests that we can interpret soft modes as entanglement edge modes on the celestial sphere and study entanglement properties of soft modes in abelian gauge theories."],"url":"http://arxiv.org/abs/2403.14502v1","category":"hep-th"}
{"created":"2024-03-21 15:21:09","title":"Tensor-force effects on nuclear matter in relativistic ab initio theory","abstract":"Within the relativistic Brueckner-Hartree-Fock theory in the full Dirac space, the tensor-force effects on infinite nuclear matter are elucidated by subtracting the matrix elements of tensor forces from the realistic nucleon-nucleon interaction. The tensor-force effects for the binding energy per particle of symmetric nuclear matter (SNM) as well as the symmetry energy are attractive and are more pronounced around the empirical saturation density, while the tensor forces have little impact on the pure neutron matter. By tuning the tensor-force strength, an infinite (negative) scattering length in the spin-triplet channel is found. This locates the dilute SNM with only the $^3S_1$-$^3D_1$ channel interaction at the unitary limit. Its ground-state energy is found proportional to the energy of a free Fermi gas with a scaling factor 0.38, revealing good universal properties. This work paves the way to study the tensor-force effects in neutron stars as well as finite nuclei from realistic nucleon-nucleon interactions and highlights the role of the tensor force on the deviation of the nuclear physics to the unitary limit.","sentences":["Within the relativistic Brueckner-Hartree-Fock theory in the full Dirac space, the tensor-force effects on infinite nuclear matter are elucidated by subtracting the matrix elements of tensor forces from the realistic nucleon-nucleon interaction.","The tensor-force effects for the binding energy per particle of symmetric nuclear matter (SNM) as well as the symmetry energy are attractive and are more pronounced around the empirical saturation density, while the tensor forces have little impact on the pure neutron matter.","By tuning the tensor-force strength, an infinite (negative) scattering length in the spin-triplet channel is found.","This locates the dilute SNM with only the $^3S_1$-$^3D_1$ channel interaction at the unitary limit.","Its ground-state energy is found proportional to the energy of a free Fermi gas with a scaling factor 0.38, revealing good universal properties.","This work paves the way to study the tensor-force effects in neutron stars as well as finite nuclei from realistic nucleon-nucleon interactions and highlights the role of the tensor force on the deviation of the nuclear physics to the unitary limit."],"url":"http://arxiv.org/abs/2403.14474v1","category":"nucl-th"}
{"created":"2024-03-21 15:11:29","title":"Large N instantons, BPS states, and the replica limit","abstract":"We study the relation between large N instantons and conventional instantons, focusing on matrix models and topological strings. We show that the resurgent properties of the perturbative series at fixed but arbitrary N, including the replica limit N = 0, can be obtained from large N instantons. In the case of topological strings, it has been conjectured that the resurgent structure encoded by large N instantons is closely related to the spectrum of BPS states. We give direct evidence for this connection in the case of Seiberg-Witten theory and other topological string models, and we show in detail how the resurgent properties at fixed N follow from the large N theory, and therefore can be used to obtain information on BPS invariants.","sentences":["We study the relation between large N instantons and conventional instantons, focusing on matrix models and topological strings.","We show that the resurgent properties of the perturbative series at fixed but arbitrary N, including the replica limit N = 0, can be obtained from large N instantons.","In the case of topological strings, it has been conjectured that the resurgent structure encoded by large N instantons is closely related to the spectrum of BPS states.","We give direct evidence for this connection in the case of Seiberg-Witten theory and other topological string models, and we show in detail how the resurgent properties at fixed N follow from the large N theory, and therefore can be used to obtain information on BPS invariants."],"url":"http://arxiv.org/abs/2403.14462v1","category":"hep-th"}
{"created":"2024-03-21 13:36:20","title":"Band gap formation in commensurate twisted bilayer graphene/hBN moir\u00e9 lattices","abstract":"We report on the investigation of periodic superstructures in twisted bilayer graphene (tBLG) van-der-Waals heterostructures, where one of the graphene layers is aligned to hexagonal boron nitride (hBN). Our theoretical simulations reveal that if the ratio of the resulting two moir\\'e unit cell areas is a simple fraction, the graphene/hBN moir\\'e lattice acts as a staggered potential, breaking the degeneracy between tBLG AA sites. This leads to additional band gaps at energies where a subset of tBLG AA sites is fully occupied. These gaps manifest as Landau fans in magnetotransport, which we experimentally observe in an aligned tBLG/hBN heterostructure. Our study demonstrates the identification of commensurate tBLG/hBN van-der-Waals heterostructures by magnetotransport, highlights the persistence of moir\\'e effects on length scales of tens of nanometers, and represents an interesting step forward in the ongoing effort to realise designed quantum materials with tailored properties.","sentences":["We report on the investigation of periodic superstructures in twisted bilayer graphene (tBLG) van-der-Waals heterostructures, where one of the graphene layers is aligned to hexagonal boron nitride (hBN).","Our theoretical simulations reveal that if the ratio of the resulting two moir\\'e unit cell areas is a simple fraction, the graphene/hBN moir\\'e lattice acts as a staggered potential, breaking the degeneracy between tBLG AA sites.","This leads to additional band gaps at energies where a subset of tBLG AA sites is fully occupied.","These gaps manifest as Landau fans in magnetotransport, which we experimentally observe in an aligned tBLG/hBN heterostructure.","Our study demonstrates the identification of commensurate tBLG/hBN van-der-Waals heterostructures by magnetotransport, highlights the persistence of moir\\'e effects on length scales of tens of nanometers, and represents an interesting step forward in the ongoing effort to realise designed quantum materials with tailored properties."],"url":"http://arxiv.org/abs/2403.14393v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-21 12:43:42","title":"Hubble Space Telescope images of SN 1987A: Evolution of the ejecta and the equatorial ring from 2009 to 2022","abstract":"Supernova (SN) 1987A offers a unique opportunity to study how a spatially resolved SN evolves into a young supernova remnant (SNR). We present and analyze Hubble Space Telescope (HST) imaging observations of SN 1987A obtained in 2022 and compare them with HST observations from 2009 to 2021. These observations allow us to follow the evolution of the equatorial ring (ER), the rapidly expanding ejecta, and emission from the center over a wide range in wavelength from 2000 to 11 000 AA. The ER has continued to fade since it reached its maximum ~8200 days after the explosion. In contrast, the ejecta brightened until day ~11000 before their emission levelled off; the west side brightened more than the east side, which we attribute to the stronger X-ray emission by the ER on that side. The asymmetric ejecta expand homologously in all filters, which are dominated by various emission lines from hydrogen, calcium, and iron. From this overall similarity, we infer the ejecta are chemically well-mixed on large scales. The exception is the diffuse morphology observed in the UV filters dominated by emission from the Mg II resonance lines that get scattered before escaping. The 2022 observations do not show any sign of the compact object that was inferred from highly-ionized emission near the remnant's center observed with JWST. We determine an upper limit on the flux from a compact central source in the [O III] HST image. The non-detection of this line indicates that the S and Ar lines observed with JWST originate from the O free inner Si - S - Ar rich zone and/or that the observed [O III] flux is strongly affected by dust scattering.","sentences":["Supernova (SN) 1987A offers a unique opportunity to study how a spatially resolved SN evolves into a young supernova remnant (SNR).","We present and analyze Hubble Space Telescope (HST) imaging observations of SN 1987A obtained in 2022 and compare them with HST observations from 2009 to 2021.","These observations allow us to follow the evolution of the equatorial ring (ER), the rapidly expanding ejecta, and emission from the center over a wide range in wavelength from 2000 to 11 000 AA.","The ER has continued to fade since it reached its maximum ~8200 days after the explosion.","In contrast, the ejecta brightened until day ~11000 before their emission levelled off; the west side brightened more than the east side, which we attribute to the stronger X-ray emission by the ER on that side.","The asymmetric ejecta expand homologously in all filters, which are dominated by various emission lines from hydrogen, calcium, and iron.","From this overall similarity, we infer the ejecta are chemically well-mixed on large scales.","The exception is the diffuse morphology observed in the UV filters dominated by emission from the Mg II resonance lines that get scattered before escaping.","The 2022 observations do not show any sign of the compact object that was inferred from highly-ionized emission near the remnant's center observed with JWST.","We determine an upper limit on the flux from a compact central source in the [O III] HST image.","The non-detection of this line indicates that the S and Ar lines observed with JWST originate from the O free inner Si - S - Ar rich zone and/or that the observed [O III] flux is strongly affected by dust scattering."],"url":"http://arxiv.org/abs/2403.14361v1","category":"astro-ph.HE"}
{"created":"2024-03-21 11:00:20","title":"Electron-positron annihilation into $K\\bar{K}\u03c0$ and their contributions to $(g-2)_\u03bc$","abstract":"In this paper, a coherent study of the $e^+e^-$ annihilation into $K^+K^-\\pi^0$, $K^0_SK^0_L\\pi^0$ and $K^0_SK^\\pm\\pi^\\mp$ is carried out within the framework of resonance chiral theory. The amplitudes are fixed by fitting to the experimental cross-section and invariant mass spectrum. With these amplitudes, one can calculate the hadronic vacuum polarization form factors of these processes. The leading order contributions of $\\sigma(e^+e^- \\to K\\bar{K}\\pi)$ to the anomalous magnetic moment of the muon, $(g-2)_\\mu$, is obtained as $a_\\mu^{\\rm HVP,LO}(e^+e^- \\to K\\bar{K}\\pi)=(3.07\\pm0.07)\\times10^{-10}$ up to $E_{\\rm cm}=2.3$ GeV.","sentences":["In this paper, a coherent study of the $e^+e^-$ annihilation into $K^+K^-\\pi^0$, $K^0_SK^0_L\\pi^0$ and $K^0_SK^\\pm\\pi^\\mp$ is carried out within the framework of resonance chiral theory.","The amplitudes are fixed by fitting to the experimental cross-section and invariant mass spectrum.","With these amplitudes, one can calculate the hadronic vacuum polarization form factors of these processes.","The leading order contributions of $\\sigma(e^+e^- \\to K\\bar{K}\\pi)$ to the anomalous magnetic moment of the muon, $(g-2)_\\mu$, is obtained as $a_\\mu^{\\rm HVP,LO}(e^+e^- \\to K\\bar{K}\\pi)=(3.07\\pm0.07)\\times10^{-10}$ up to $E_{\\rm cm}=2.3$ GeV."],"url":"http://arxiv.org/abs/2403.14294v1","category":"hep-ph"}
{"created":"2024-03-21 09:26:04","title":"K-Act2Emo: Korean Commonsense Knowledge Graph for Indirect Emotional Expression","abstract":"In many literary texts, emotions are indirectly conveyed through descriptions of actions, facial expressions, and appearances, necessitating emotion inference for narrative understanding. In this paper, we introduce K-Act2Emo, a Korean commonsense knowledge graph (CSKG) comprising 1,900 indirect emotional expressions and the emotions inferable from them. We categorize reasoning types into inferences in positive situations, inferences in negative situations, and inferences when expressions do not serve as emotional cues. Unlike existing CSKGs, K-Act2Emo specializes in emotional contexts, and experimental results validate its effectiveness for training emotion inference models. Significantly, the BART-based knowledge model fine-tuned with K-Act2Emo outperforms various existing Korean large language models, achieving performance levels comparable to GPT-4 Turbo.","sentences":["In many literary texts, emotions are indirectly conveyed through descriptions of actions, facial expressions, and appearances, necessitating emotion inference for narrative understanding.","In this paper, we introduce K-Act2Emo, a Korean commonsense knowledge graph (CSKG) comprising 1,900 indirect emotional expressions and the emotions inferable from them.","We categorize reasoning types into inferences in positive situations, inferences in negative situations, and inferences when expressions do not serve as emotional cues.","Unlike existing CSKGs, K-Act2Emo specializes in emotional contexts, and experimental results validate its effectiveness for training emotion inference models.","Significantly, the BART-based knowledge model fine-tuned with K-Act2Emo outperforms various existing Korean large language models, achieving performance levels comparable to GPT-4 Turbo."],"url":"http://arxiv.org/abs/2403.14253v1","category":"cs.CL"}
{"created":"2024-03-21 07:58:16","title":"A finite element method for anisotropic crystal growth on surfaces","abstract":"Phase transition problems on curved surfaces can lead to a panopticon of fascinating patterns. In this paper we consider finite element approximations of phase field models with a spatially inhomogeneous and anisotropic surface energy density. The problems are either posed in $\\mathbb R^3$ or on a two-dimensional hypersurface in $\\mathbb R^3$. In the latter case, a fundamental choice regarding the anisotropic energy density has to be made. Our numerical method can be employed both situations, where for the problems on hypersurfaces the algorithm uses parametric finite elements. We prove an unconditional stability result for our schemes and present several numerical experiments, including for the modelling of ice crystal growth on a sphere.","sentences":["Phase transition problems on curved surfaces can lead to a panopticon of fascinating patterns.","In this paper we consider finite element approximations of phase field models with a spatially inhomogeneous and anisotropic surface energy density.","The problems are either posed in $\\mathbb R^3$ or on a two-dimensional hypersurface in $\\mathbb R^3$.","In the latter case, a fundamental choice regarding the anisotropic energy density has to be made.","Our numerical method can be employed both situations, where for the problems on hypersurfaces the algorithm uses parametric finite elements.","We prove an unconditional stability result for our schemes and present several numerical experiments, including for the modelling of ice crystal growth on a sphere."],"url":"http://arxiv.org/abs/2403.14206v1","category":"math.NA"}
{"created":"2024-03-21 07:58:06","title":"Solvent-Free Silsesquioxane Self-Welding for 3D Printing Multi-Refractive Index Glass Objects","abstract":"The growing interest in 3D printing of silica glass has spurred substantial research efforts. Our prior work utilizing a liquid silica resin (LSR) demonstrated high printing accuracy and resolution. However, the resin's sensitivity to moisture posed limitations, restricting the printing environment. On the other hand, polyhedral oligomeric silsesquioxane (POSS)-based materials offer excellent water stability and sinterless features. Yet, they suffer from relatively high shrinkage due to the presence of additional organic monomers. In this study, we present a polymeric silsesquioxane (PSQ) resin with reduced shrinkage, enhanced moisture stability, and the retention of sinterless features, providing a promising solution for achieving high-resolution 3D printing of glass objects. Leveraging the two-photon polymerization (2PP) method, we realized nanostructures with feature sizes below 80 nm. Moreover, we demonstrate the tunability of the refractive index by incorporating zirconium moieties into the resin, facilitating the fabrication of glass micro-optics with varying refractive indices. Importantly, the self-welding capability observed between two individual components provides a flexible approach for producing micro-optics with multiple components, each possessing distinct refractive indices. This research represents a significant advancement in the field of advanced glass manufacturing, paving the way for future applications in micro- and nano-scale glass objects.","sentences":["The growing interest in 3D printing of silica glass has spurred substantial research efforts.","Our prior work utilizing a liquid silica resin (LSR) demonstrated high printing accuracy and resolution.","However, the resin's sensitivity to moisture posed limitations, restricting the printing environment.","On the other hand, polyhedral oligomeric silsesquioxane (POSS)-based materials offer excellent water stability and sinterless features.","Yet, they suffer from relatively high shrinkage due to the presence of additional organic monomers.","In this study, we present a polymeric silsesquioxane (PSQ) resin with reduced shrinkage, enhanced moisture stability, and the retention of sinterless features, providing a promising solution for achieving high-resolution 3D printing of glass objects.","Leveraging the two-photon polymerization (2PP) method, we realized nanostructures with feature sizes below 80 nm.","Moreover, we demonstrate the tunability of the refractive index by incorporating zirconium moieties into the resin, facilitating the fabrication of glass micro-optics with varying refractive indices.","Importantly, the self-welding capability observed between two individual components provides a flexible approach for producing micro-optics with multiple components, each possessing distinct refractive indices.","This research represents a significant advancement in the field of advanced glass manufacturing, paving the way for future applications in micro- and nano-scale glass objects."],"url":"http://arxiv.org/abs/2403.14205v1","category":"physics.optics"}
{"created":"2024-03-21 07:54:11","title":"Two fitness inference schemes compared using allele frequencies from 1,068,391 sequences sampled in the UK during the COVID-19 pandemic","abstract":"Throughout the course of the SARS-CoV-2 pandemic, genetic variation has contributed to the spread and persistence of the virus. For example, various mutations have allowed SARS-CoV-2 to escape antibody neutralization or to bind more strongly to the receptors that it uses to enter human cells. Here, we compared two methods that estimate the fitness effects of viral mutations using the abundant sequence data gathered over the course of the pandemic. Both approaches are grounded in population genetics theory but with different assumptions. One approach, tQLE, features an epistatic fitness landscape and assumes that alleles are nearly in linkage equilibrium. Another approach, MPL, assumes a simple, additive fitness landscape, but allows for any level of correlation between alleles. We characterized differences in the distributions of fitness values inferred by each approach and in the ranks of fitness values that they assign to sequences across time. We find that in a large fraction of weeks the two methods are in good agreement as to their top-ranked sequences, i.e., as to which sequences observed that week are most fit. We also find that agreement between ranking of sequences varies with genetic unimodality in the population in a given week.","sentences":["Throughout the course of the SARS-CoV-2 pandemic, genetic variation has contributed to the spread and persistence of the virus.","For example, various mutations have allowed SARS-CoV-2 to escape antibody neutralization or to bind more strongly to the receptors that it uses to enter human cells.","Here, we compared two methods that estimate the fitness effects of viral mutations using the abundant sequence data gathered over the course of the pandemic.","Both approaches are grounded in population genetics theory but with different assumptions.","One approach, tQLE, features an epistatic fitness landscape and assumes that alleles are nearly in linkage equilibrium.","Another approach, MPL, assumes a simple, additive fitness landscape, but allows for any level of correlation between alleles.","We characterized differences in the distributions of fitness values inferred by each approach and in the ranks of fitness values that they assign to sequences across time.","We find that in a large fraction of weeks the two methods are in good agreement as to their top-ranked sequences, i.e., as to which sequences observed that week are most fit.","We also find that agreement between ranking of sequences varies with genetic unimodality in the population in a given week."],"url":"http://arxiv.org/abs/2403.14202v1","category":"q-bio.PE"}
{"created":"2024-03-21 07:34:31","title":"PECI-Net: Bolus segmentation from video fluoroscopic swallowing study images using preprocessing ensemble and cascaded inference","abstract":"Bolus segmentation is crucial for the automated detection of swallowing disorders in videofluoroscopic swallowing studies (VFSS). However, it is difficult for the model to accurately segment a bolus region in a VFSS image because VFSS images are translucent, have low contrast and unclear region boundaries, and lack color information. To overcome these challenges, we propose PECI-Net, a network architecture for VFSS image analysis that combines two novel techniques: the preprocessing ensemble network (PEN) and the cascaded inference network (CIN). PEN enhances the sharpness and contrast of the VFSS image by combining multiple preprocessing algorithms in a learnable way. CIN reduces ambiguity in bolus segmentation by using context from other regions through cascaded inference. Moreover, CIN prevents undesirable side effects from unreliably segmented regions by referring to the context in an asymmetric way. In experiments, PECI-Net exhibited higher performance than four recently developed baseline models, outperforming TernausNet, the best among the baseline models, by 4.54\\% and the widely used UNet by 10.83\\%. The results of the ablation studies confirm that CIN and PEN are effective in improving bolus segmentation performance.","sentences":["Bolus segmentation is crucial for the automated detection of swallowing disorders in videofluoroscopic swallowing studies (VFSS).","However, it is difficult for the model to accurately segment a bolus region in a VFSS image because VFSS images are translucent, have low contrast and unclear region boundaries, and lack color information.","To overcome these challenges, we propose PECI-Net, a network architecture for VFSS image analysis that combines two novel techniques: the preprocessing ensemble network (PEN) and the cascaded inference network (CIN).","PEN enhances the sharpness and contrast of the VFSS image by combining multiple preprocessing algorithms in a learnable way.","CIN reduces ambiguity in bolus segmentation by using context from other regions through cascaded inference.","Moreover, CIN prevents undesirable side effects from unreliably segmented regions by referring to the context in an asymmetric way.","In experiments, PECI-Net exhibited higher performance than four recently developed baseline models, outperforming TernausNet, the best among the baseline models, by 4.54\\% and the widely used UNet by 10.83\\%.","The results of the ablation studies confirm that CIN and PEN are effective in improving bolus segmentation performance."],"url":"http://arxiv.org/abs/2403.14191v1","category":"cs.CV"}
{"created":"2024-03-21 06:57:28","title":"ReFeree: Radar-based efficient global descriptor using a Feature and Free space for Place Recognition","abstract":"Radar is highlighted for robust sensing capabilities in adverse weather conditions (e.g. dense fog, heavy rain, or snowfall). In addition, Radar can cover wide areas and penetrate small particles. Despite these advantages, Radar-based place recognition remains in the early stages compared to other sensors due to its unique characteristics such as low resolution, and significant noise. In this paper, we propose a Radarbased place recognition utilizing a descriptor called ReFeree using a feature and free space. Unlike traditional methods, we overwhelmingly summarize the Radar image. Despite being lightweight, it contains semi-metric information and is also outstanding from the perspective of place recognition performance. For concrete validation, we test a single session from the MulRan dataset and a multi-session from the Oxford Radar RobotCar and the Boreas dataset.","sentences":["Radar is highlighted for robust sensing capabilities in adverse weather conditions (e.g. dense fog, heavy rain, or snowfall).","In addition, Radar can cover wide areas and penetrate small particles.","Despite these advantages, Radar-based place recognition remains in the early stages compared to other sensors due to its unique characteristics such as low resolution, and significant noise.","In this paper, we propose a Radarbased place recognition utilizing a descriptor called ReFeree using a feature and free space.","Unlike traditional methods, we overwhelmingly summarize the Radar image.","Despite being lightweight, it contains semi-metric information and is also outstanding from the perspective of place recognition performance.","For concrete validation, we test a single session from the MulRan dataset and a multi-session from the Oxford Radar RobotCar and the Boreas dataset."],"url":"http://arxiv.org/abs/2403.14176v1","category":"cs.RO"}
{"created":"2024-03-21 06:46:43","title":"Spectro-polarimetric study to constrain accretion-ejection properties of MCG-5-23-16 using IXPE and NuSTAR observations","abstract":"We conducted a study on the X-ray polarization properties of MGC-5-23-16 by analyzing long-term monitoring data from {\\it NuSTAR} jointly with {\\it IXPE} observations in May and November 2022. The re-analysis of {\\it IXPE} data gives model-dependent polarization degree, PD (\\%) = $1.55\\pm0.99$ and $1.31\\pm0.95$ in the energy band $2-8$ keV, agrees with previous studies within error bars. The model-independent analysis of PD poses an upper limit of $\\leq3.8$ ($1\\sigma$ level) for the same energy band. The observed upper limit of PD, along with broadband spectral analysis ($3-79$ keV), allowed us to derive corona geometry (i.e. length and height) and the accretion disk inclination ($\\sim 33^\\circ$). Additional {\\it NuSTAR} observations were also analyzed to gain insights into the accretion flow properties of the source and to estimate the expected polarization during those epochs with PD $\\sim 4.3\\%$. The radius and height of the corona exhibited to vary between $28.2\\pm3.1 - 39.8\\pm4.6$ r$_s$ and $14.3\\pm1.7-21.4\\pm1.9$ r$_s$, with a mass outflow rate from the corona measuring $0.14\\pm0.03-0.2\\pm0.03$ Eddington rate ($\\dot M_{\\rm Edd}$). The estimated PD values were nearly constant up to a certain radial distance and height of the corona and then decreased for increasing corona geometry. The spectral analysis further provided an estimate for the mass of the central black hole $\\sim 2\\times 10^7$ M$_\\odot$ and the velocity of the outflowing gas $\\sim 0.16-0.19c$. Our modeling of the disk-corona-outflows and polarization connection can be extended and validated with data from recently launched India's first X-ray Polarimeter Satellite, offering potential applications to other sources.","sentences":["We conducted a study on the X-ray polarization properties of MGC-5-23-16 by analyzing long-term monitoring data from {\\it NuSTAR} jointly with {\\it IXPE} observations in May and November 2022.","The re-analysis of {\\it IXPE} data gives model-dependent polarization degree, PD (\\%) = $1.55\\pm0.99$ and $1.31\\pm0.95$ in the energy band $2-8$ keV, agrees with previous studies within error bars.","The model-independent analysis of PD poses an upper limit of $\\leq3.8$ ($1\\sigma$ level) for the same energy band.","The observed upper limit of PD, along with broadband spectral analysis ($3-79$ keV), allowed us to derive corona geometry (i.e. length and height) and the accretion disk inclination ($\\sim 33^\\circ$).","Additional {\\it NuSTAR} observations were also analyzed to gain insights into the accretion flow properties of the source and to estimate the expected polarization during those epochs with PD $\\sim 4.3\\%$. The radius and height of the corona exhibited to vary between $28.2\\pm3.1 - 39.8\\pm4.6$ r$_s$ and $14.3\\pm1.7-21.4\\pm1.9$ r$_s$, with a mass outflow rate from the corona measuring $0.14\\pm0.03-0.2\\pm0.03$ Eddington rate ($\\dot M_{\\rm Edd}$).","The estimated PD values were nearly constant up to a certain radial distance and height of the corona and then decreased for increasing corona geometry.","The spectral analysis further provided an estimate for the mass of the central black hole $\\sim 2\\times 10^7$ M$_\\odot$ and the velocity of the outflowing gas $\\sim 0.16-0.19c$. Our modeling of the disk-corona-outflows and polarization connection can be extended and validated with data from recently launched India's first X-ray Polarimeter Satellite, offering potential applications to other sources."],"url":"http://arxiv.org/abs/2403.14169v1","category":"astro-ph.HE"}
{"created":"2024-03-21 05:12:02","title":"Temperature dependent conductivity, dielectric relaxation, electrical modulus and impedance spectroscopy of Ni substituted Na$_{3+2x}$Zr$_{2-x}$Ni$_{x}$Si$_2$PO$_{\\rm 12}$","abstract":"We investigate the structural, dielectric relaxation, electric modulus and impedance behavior of Ni-doped NASICON ceramic Na$_{3+2x}$Zr$_{2-x}$Ni$_{x}$Si$_2$PO$_{\\rm 12}$ ($x=$ 0.05--0.2) prepared using the solid-state reaction method. The increase in dielectric constant with temperature and decrease with frequency is explained on the basis of space charge polarization using the two-layer model of Maxwell-Wagner relaxation. The dielectric loss peak at lower temperatures follows the Arrhenius-type behavior with frequency having activation energy of 0.27$\\pm$0.01~eV of dipolar relaxation, suggests similar type of defects are responsible for all the doped samples. The real ($\\epsilon$ $^{'}$) and imaginary ($\\epsilon$ $^{''}$) permittivity variation with frequency shows the broad relaxation behavior indicates the non-Debye type of relaxation in the measured temperature range. The permittivity values decrease with the amount of doping due to the increased number of charge carriers upon Ni doping at the Zr site. The grain contributions are observed at higher frequencies, while grain-boundary contributions occur at the lower side of frequencies. The imaginary part of the electric modulus also shows two types of relaxation peaks for all the samples indicating similar activation energy at low temperatures and variable activation energy at higher temperatures. The fitting of the imaginary modulus using KWW function shows the non-Debye type of relaxation. We find that all modulus curves merge with each other at low temperatures showing a similar type of relaxation, while curves at high temperatures show the dispersed behavior above the peak frequency. The {\\it a.c.} conductivity data are fitted using the double power law confirming the grain and grain boundary contributions in total conductivity.","sentences":["We investigate the structural, dielectric relaxation, electric modulus and impedance behavior of Ni-doped NASICON ceramic Na$_{3+2x}$Zr$_{2-x}$Ni$_{x}$Si$_2$PO$_{\\rm 12}$ ($x=$ 0.05--0.2) prepared using the solid-state reaction method.","The increase in dielectric constant with temperature and decrease with frequency is explained on the basis of space charge polarization using the two-layer model of Maxwell-Wagner relaxation.","The dielectric loss peak at lower temperatures follows the Arrhenius-type behavior with frequency having activation energy of 0.27$\\pm$0.01~eV of dipolar relaxation, suggests similar type of defects are responsible for all the doped samples.","The real ($\\epsilon$ $^{'}$) and imaginary ($\\epsilon$ $^{''}$) permittivity variation with frequency shows the broad relaxation behavior indicates the non-Debye type of relaxation in the measured temperature range.","The permittivity values decrease with the amount of doping due to the increased number of charge carriers upon Ni doping at the Zr site.","The grain contributions are observed at higher frequencies, while grain-boundary contributions occur at the lower side of frequencies.","The imaginary part of the electric modulus also shows two types of relaxation peaks for all the samples indicating similar activation energy at low temperatures and variable activation energy at higher temperatures.","The fitting of the imaginary modulus using KWW function shows the non-Debye type of relaxation.","We find that all modulus curves merge with each other at low temperatures showing a similar type of relaxation, while curves at high temperatures show the dispersed behavior above the peak frequency.","The {\\it a.c.} conductivity data are fitted using the double power law confirming the grain and grain boundary contributions in total conductivity."],"url":"http://arxiv.org/abs/2403.14136v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-21 03:39:30","title":"Generalized Free Conformal Field Theories at Infinite Temperature -- In memory of Iwasaki-sensei","abstract":"Can space-time symmetries such as Lorentz, dilatation, or conformal symmetry be recovered at infinite temperature? To address this question, we study correlation functions of generalized free conformal field theories (a.k.a free holographic theories in thermal AdS space-time) at infinite temperature. We show that they are broken at the leading order in bosonic correlation functions, but a non-trivial scaling symmetry may emerge.","sentences":["Can space-time symmetries such as Lorentz, dilatation, or conformal symmetry be recovered at infinite temperature?","To address this question, we study correlation functions of generalized free conformal field theories (a.k.a free holographic theories in thermal AdS space-time) at infinite temperature.","We show that they are broken at the leading order in bosonic correlation functions, but a non-trivial scaling symmetry may emerge."],"url":"http://arxiv.org/abs/2403.14107v1","category":"hep-th"}
{"created":"2024-03-21 03:39:01","title":"The disappearance of the blue and luminous progenitor of the Type IIn SN 2010jl","abstract":"Type IIn supernovae (SNe) exhibit narrow hydrogen lines that arise from the strong interaction between ejecta and circumstellar material. It remains poorly understood, however, what progenitor stars give rise to these explosions. In this work, we perform a detailed analysis of the progenitor and environment of the nearby Type IIn SN 2010jl. With newer images taken by the Hubble Space Telescope, we confirm that the previously reported progenitor candidate is a blend of the progenitor itself and a field star cluster in its close vicinity. SN 2010jl has now become much fainter than the progenitor. The progenitor is very blue and luminous with an effective temperature of log $T_{\\rm eff}/{\\rm K}$=4.26$^{+0.11}_{-0.09}$ and a luminosity of log $L/L_{\\odot}$ =6.52$^{+0.20}_{-0.16}$. It is located in a very young star-forming region, but its luminosity is much higher than that expected from the environmental stellar populations. We suggest that the progenitor was in outburst when observed. Its nature and evolutionary history remain to be investigated.","sentences":["Type IIn supernovae (SNe) exhibit narrow hydrogen lines that arise from the strong interaction between ejecta and circumstellar material.","It remains poorly understood, however, what progenitor stars give rise to these explosions.","In this work, we perform a detailed analysis of the progenitor and environment of the nearby Type IIn SN 2010jl.","With newer images taken by the Hubble Space Telescope, we confirm that the previously reported progenitor candidate is a blend of the progenitor itself and a field star cluster in its close vicinity.","SN 2010jl has now become much fainter than the progenitor.","The progenitor is very blue and luminous with an effective temperature of log $T_{\\rm eff}/{\\rm K}$=4.26$^{+0.11}_{-0.09}$ and a luminosity of log $L/L_{\\odot}$ =6.52$^{+0.20}_{-0.16}$. It is located in a very young star-forming region, but its luminosity is much higher than that expected from the environmental stellar populations.","We suggest that the progenitor was in outburst when observed.","Its nature and evolutionary history remain to be investigated."],"url":"http://arxiv.org/abs/2403.14106v1","category":"astro-ph.HE"}
{"created":"2024-03-21 03:04:04","title":"Magnetization and exchange-stiffness constants of Fe-Al-Si alloys at finite-temperatures: A first-principles study","abstract":"We investigated the magnetic properties of Sendust (Fe-Al-Si) alloys not only at 0 K but also at finite-temperatures by means of the first-principles calculations assuming A2, B2, and DO3 structures. We confirmed that the itinerant characteristics of 3d electrons of Fe are not negligible for A2 and B2 structures and a significantly small exchange stiffness constant exists at zero-temperature in a B2 structure. However, the calculated Curie temperatures are in the same order for all structures; this indicates that the Curie temperature cannot be determined only by the exchange interactions at zero-temperature in itinerant electron systems. Temperature dependence of the exchange interaction, namely spin configuration dependence, also might be important for determining it. In addition, this property might also be related to the unique behavior of the temperature dependence of the exchange stiffness constant for the B2 structure, which does not decrease monotonically as temperatures increase, contrary to the behavior expected from the Heisenberg model. In addition, we investigated composition dependence on the exchange stiffness constant at zero-temperature and confirmed that the substitution of Si with Al could improve the amplitude of the exchange stiffness constant at zero-temperature for all structures.","sentences":["We investigated the magnetic properties of Sendust (Fe-Al-Si) alloys not only at 0 K but also at finite-temperatures by means of the first-principles calculations assuming A2, B2, and DO3 structures.","We confirmed that the itinerant characteristics of 3d electrons of Fe are not negligible for A2 and B2 structures and a significantly small exchange stiffness constant exists at zero-temperature in a B2 structure.","However, the calculated Curie temperatures are in the same order for all structures; this indicates that the Curie temperature cannot be determined only by the exchange interactions at zero-temperature in itinerant electron systems.","Temperature dependence of the exchange interaction, namely spin configuration dependence, also might be important for determining it.","In addition, this property might also be related to the unique behavior of the temperature dependence of the exchange stiffness constant for the B2 structure, which does not decrease monotonically as temperatures increase, contrary to the behavior expected from the Heisenberg model.","In addition, we investigated composition dependence on the exchange stiffness constant at zero-temperature and confirmed that the substitution of Si with Al could improve the amplitude of the exchange stiffness constant at zero-temperature for all structures."],"url":"http://arxiv.org/abs/2403.14096v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-21 02:16:31","title":"Non-Uniform Lattices of Large Systole Containing a Fixed 3-Manifold Group","abstract":"Let $d$ be a square free positive integer and $\\mathbb{Q}(\\sqrt{d})$ a totally real quadratic field over $\\mathbb{Q}$. We show there exists an arithmetic lattice L in $SL(8,\\mathbb{R})$ with entries in the ring of integers of $\\mathbb{Q}(\\sqrt{d})$ and a sequence of lattices $\\Gamma_n $ commensurable to L such that the systole of the locally symmetric finite volume manifold $\\Gamma_n \\diagdown SL(8,\\mathbb{R}) \\diagup SO(8)$ goes to infinity as $n \\rightarrow \\infty$, yet every $\\Gamma_n$ contains the same hyperbolic 3-manifold group $\\Pi$, a finite index subgroup of the arithmetic hyperbolic 3-manifold vol3. Notably, such an example does not exist in rank one, so this is a feature unique to higher rank lattices.","sentences":["Let $d$ be a square free positive integer and $\\mathbb{Q}(\\sqrt{d})$ a totally real quadratic field over $\\mathbb{Q}$. We show there exists an arithmetic lattice L in $SL(8,\\mathbb{R})$ with entries in the ring of integers of $\\mathbb{Q}(\\sqrt{d})$ and a sequence of lattices $\\Gamma_n $ commensurable to L such that the systole of the locally symmetric finite volume manifold $\\Gamma_n","\\diagdown SL(8,\\mathbb{R})","\\diagup SO(8)$ goes to infinity as $n \\rightarrow \\infty$, yet every $\\Gamma_n$ contains the same hyperbolic 3-manifold group $\\Pi$, a finite index subgroup of the arithmetic hyperbolic 3-manifold vol3.","Notably, such an example does not exist in rank one, so this is a feature unique to higher rank lattices."],"url":"http://arxiv.org/abs/2403.14081v1","category":"math.GT"}
{"created":"2024-03-21 02:03:45","title":"Evolution of flat band and role of lattice relaxations in twisted bilayer graphene","abstract":"Magic-angle twisted bilayer graphene (MATBG) exhibits correlated phenomena such as superconductivity and Mott insulating state related to the weakly dispersing flat band near the Fermi energy. Beyond its moir\\'e period, such flat band is expected to be sensitive to lattice relaxations. Thus, clarifying the evolution of the electronic structure with twist angle is critical for understanding the physics of MATBG. Here, we combine nanospot angle-resolved photoemission spectroscopy and atomic force microscopy to resolve the fine electronic structure of the flat band and remote bands, and their evolution with twist angles from 1.07$^\\circ$ to 2.60$^\\circ$. Near the magic angle, dispersion is characterized by a flat band near the Fermi energy with a strongly reduced bandwidth. Moreover, near 1.07$^\\circ$, we observe a spectral weight transfer between remote bands at higher binding energy and extract the modulated interlayer spacing near the magic angle. Our work provides direct spectroscopic information on flat band physics and highlights the role of lattice relaxations.","sentences":["Magic-angle twisted bilayer graphene (MATBG) exhibits correlated phenomena such as superconductivity and Mott insulating state related to the weakly dispersing flat band near the Fermi energy.","Beyond its moir\\'e period, such flat band is expected to be sensitive to lattice relaxations.","Thus, clarifying the evolution of the electronic structure with twist angle is critical for understanding the physics of MATBG.","Here, we combine nanospot angle-resolved photoemission spectroscopy and atomic force microscopy to resolve the fine electronic structure of the flat band and remote bands, and their evolution with twist angles from 1.07$^\\circ$ to 2.60$^\\circ$. Near the magic angle, dispersion is characterized by a flat band near the Fermi energy with a strongly reduced bandwidth.","Moreover, near 1.07$^\\circ$, we observe a spectral weight transfer between remote bands at higher binding energy and extract the modulated interlayer spacing near the magic angle.","Our work provides direct spectroscopic information on flat band physics and highlights the role of lattice relaxations."],"url":"http://arxiv.org/abs/2403.14078v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-21 01:47:29","title":"Examining the Impact of Local Condition Violations on Energy Computations in DFT","abstract":"This work introduces Extent of Violation Indices (EVIs), a novel metric for quantifying how well exchange-correlation functionals adhere to local conditions. Applying EVIs to a diverse set of molecules for GGA functionals reveals widespread violations, particularly for semi-empirical functionals. We leverage EVIs to explore potential connections between these violations and errors in chemical properties. While no correlation is observed for atomization energies, a link emerges between EVIs and total energies. Similarly, the analysis of reaction energies suggests weak positive correlations for specific conditions, but definitive conclusions about error cancellation require advancements in both functional accuracy and our understanding of cancellation mechanisms. Overall, this study highlights EVIs as a powerful tool for analyzing functional behavior and adherence to local conditions, paving the way for future research to fully elucidate the impact of violations on energy errors.","sentences":["This work introduces Extent of Violation Indices (EVIs), a novel metric for quantifying how well exchange-correlation functionals adhere to local conditions.","Applying EVIs to a diverse set of molecules for GGA functionals reveals widespread violations, particularly for semi-empirical functionals.","We leverage EVIs to explore potential connections between these violations and errors in chemical properties.","While no correlation is observed for atomization energies, a link emerges between EVIs and total energies.","Similarly, the analysis of reaction energies suggests weak positive correlations for specific conditions, but definitive conclusions about error cancellation require advancements in both functional accuracy and our understanding of cancellation mechanisms.","Overall, this study highlights EVIs as a powerful tool for analyzing functional behavior and adherence to local conditions, paving the way for future research to fully elucidate the impact of violations on energy errors."],"url":"http://arxiv.org/abs/2403.14073v1","category":"physics.chem-ph"}
{"created":"2024-03-21 01:16:03","title":"A (2+1)-Dimensional Domain Wall at One-Loop","abstract":"We consider the domain wall in the (2+1)-dimensional $\\phi^4$ double well model, created by extending the $\\phi^4$ kink in an additional infinite direction. Classically, the tension is $m^3/3\\lambda$ where $\\lambda$ is the coupling and $m$ is the meson mass. At order $O(\\lambda^0)$ all ultraviolet divergences can be removed by normal ordering, less trivial divergences arrive only at the next order. This allows us to easily quantize the domain wall, working at order $O(\\lambda^0)$. We calculate the leading quantum correction to its tension as a two-dimensional integral over a function which is determined analytically. This integral is performed numerically, resulting in $-0.0866m^2$. We also find, at this order, the excitation spectrum.","sentences":["We consider the domain wall in the (2+1)-dimensional $\\phi^4$ double well model, created by extending the $\\phi^4$ kink in an additional infinite direction.","Classically, the tension is $m^3/3\\lambda$ where $\\lambda$ is the coupling and $m$ is the meson mass.","At order $O(\\lambda^0)$ all ultraviolet divergences can be removed by normal ordering, less trivial divergences arrive only at the next order.","This allows us to easily quantize the domain wall, working at order $O(\\lambda^0)$. We calculate the leading quantum correction to its tension as a two-dimensional integral over a function which is determined analytically.","This integral is performed numerically, resulting in $-0.0866m^2$. We also find, at this order, the excitation spectrum."],"url":"http://arxiv.org/abs/2403.14062v1","category":"hep-th"}
{"created":"2024-03-21 00:09:04","title":"Accelerating ViT Inference on FPGA through Static and Dynamic Pruning","abstract":"Vision Transformers (ViTs) have achieved state-of-the-art accuracy on various computer vision tasks. However, their high computational complexity prevents them from being applied to many real-world applications. Weight and token pruning are two well-known methods for reducing complexity: weight pruning reduces the model size and associated computational demands, while token pruning further dynamically reduces the computation based on the input. Combining these two techniques should significantly reduce computation complexity and model size; however, naively integrating them results in irregular computation patterns, leading to significant accuracy drops and difficulties in hardware acceleration.   Addressing the above challenges, we propose a comprehensive algorithm-hardware codesign for accelerating ViT on FPGA through simultaneous pruning -combining static weight pruning and dynamic token pruning. For algorithm design, we systematically combine a hardware-aware structured block-pruning method for pruning model parameters and a dynamic token pruning method for removing unimportant token vectors. Moreover, we design a novel training algorithm to recover the model's accuracy. For hardware design, we develop a novel hardware accelerator for executing the pruned model. The proposed hardware design employs multi-level parallelism with load balancing strategy to efficiently deal with the irregular computation pattern led by the two pruning approaches. Moreover, we develop an efficient hardware mechanism for efficiently executing the on-the-fly token pruning.","sentences":["Vision Transformers (ViTs) have achieved state-of-the-art accuracy on various computer vision tasks.","However, their high computational complexity prevents them from being applied to many real-world applications.","Weight and token pruning are two well-known methods for reducing complexity: weight pruning reduces the model size and associated computational demands, while token pruning further dynamically reduces the computation based on the input.","Combining these two techniques should significantly reduce computation complexity and model size; however, naively integrating them results in irregular computation patterns, leading to significant accuracy drops and difficulties in hardware acceleration.   ","Addressing the above challenges, we propose a comprehensive algorithm-hardware codesign for accelerating ViT on FPGA through simultaneous pruning -combining static weight pruning and dynamic token pruning.","For algorithm design, we systematically combine a hardware-aware structured block-pruning method for pruning model parameters and a dynamic token pruning method for removing unimportant token vectors.","Moreover, we design a novel training algorithm to recover the model's accuracy.","For hardware design, we develop a novel hardware accelerator for executing the pruned model.","The proposed hardware design employs multi-level parallelism with load balancing strategy to efficiently deal with the irregular computation pattern led by the two pruning approaches.","Moreover, we develop an efficient hardware mechanism for efficiently executing the on-the-fly token pruning."],"url":"http://arxiv.org/abs/2403.14047v1","category":"cs.DC"}
{"created":"2024-03-20 22:57:34","title":"Quadcopter Team Configurable Motion Guided by a Quadruped","abstract":"The paper focuses on modeling and experimental evaluation of a quadcopter team configurable coordination guided by a single quadruped robot. We consider the quadcopter team as particles of a two-dimensional deformable body and propose a two-dimensional affine transformation model for safe and collision-free configurable coordination of this heterogeneous robotic system. The proposed affine transformation is decomposed into translation, that is specified by the quadruped global position, and configurable motion of the quadcopters, which is determined by a nonsingular Jacobian matrix so that the quadcopter team can safely navigate a constrained environment while avoiding collision. We propose two methods to experimentally evaluate the proposed heterogeneous robot coordination model. The first method measures real positions of quadcopters, quadruped, and environmental objects all with respect to the global coordinate system. On the other hand, the second method measures position with respect to the local coordinate system fixed on the dog robot which in turn enables safe planning the Jacobian matrix of the quadcopter team while the world is virtually approached the robotic system.","sentences":["The paper focuses on modeling and experimental evaluation of a quadcopter team configurable coordination guided by a single quadruped robot.","We consider the quadcopter team as particles of a two-dimensional deformable body and propose a two-dimensional affine transformation model for safe and collision-free configurable coordination of this heterogeneous robotic system.","The proposed affine transformation is decomposed into translation, that is specified by the quadruped global position, and configurable motion of the quadcopters, which is determined by a nonsingular Jacobian matrix so that the quadcopter team can safely navigate a constrained environment while avoiding collision.","We propose two methods to experimentally evaluate the proposed heterogeneous robot coordination model.","The first method measures real positions of quadcopters, quadruped, and environmental objects all with respect to the global coordinate system.","On the other hand, the second method measures position with respect to the local coordinate system fixed on the dog robot which in turn enables safe planning the Jacobian matrix of the quadcopter team while the world is virtually approached the robotic system."],"url":"http://arxiv.org/abs/2403.14029v1","category":"cs.RO"}
{"created":"2024-03-20 22:52:04","title":"Modal reduction principles: a parametric shift to graphs","abstract":"Graph-based frames have been introduced as a logical framework which internalizes an inherent boundary to knowability. They also support the interpretation of lattice-based (modal) logics as hyper-constructive logics of evidential reasoning. Conceptually, the present paper proposes graph-based frames as a formal framework suitable for generalizing Pawlak's rough set theory to a setting in which inherent limits to knowability need to be considered. Technically, the present paper establishes systematic connections between the first-order correspondents of Sahlqvist modal reduction principles on Kripke frames, and on the more general relational environments of graph-based and polarity-based frames. This work is part of a research line aiming at: (a) comparing and inter-relating the various (first-order) conditions corresponding to a given (modal) axiom in different relational semantics (b) recognizing when first-order sentences in the frame-correspondence languages of different relational structures encode the same modal content (c) meaningfully transferring relational properties across different semantic contexts. The present paper develops these results for the graph-based semantics, polarity-based semantics, and all Sahlqvist modal reduction principles. As an application, we study well known modal axioms in rough set theory on graph-based frames and show that, although these axioms correspond to different first-order conditions on graph-based frames, their intuitive meaning is retained.This allows us to introduce the notion of hyperconstructivist approximation spaces as the subclass of graph-based frames defined by the first-order conditions corresponding to the same modal axioms defining classical generalized approximation spaces, and to transfer the properties and the intuitive understanding of different approximation spaces to graph-based frames.","sentences":["Graph-based frames have been introduced as a logical framework which internalizes an inherent boundary to knowability.","They also support the interpretation of lattice-based (modal) logics as hyper-constructive logics of evidential reasoning.","Conceptually, the present paper proposes graph-based frames as a formal framework suitable for generalizing Pawlak's rough set theory to a setting in which inherent limits to knowability need to be considered.","Technically, the present paper establishes systematic connections between the first-order correspondents of Sahlqvist modal reduction principles on Kripke frames, and on the more general relational environments of graph-based and polarity-based frames.","This work is part of a research line aiming at: (a) comparing and inter-relating the various (first-order) conditions corresponding to a given (modal) axiom in different relational semantics (b) recognizing when first-order sentences in the frame-correspondence languages of different relational structures encode the same modal content (c) meaningfully transferring relational properties across different semantic contexts.","The present paper develops these results for the graph-based semantics, polarity-based semantics, and all Sahlqvist modal reduction principles.","As an application, we study well known modal axioms in rough set theory on graph-based frames and show that, although these axioms correspond to different first-order conditions on graph-based frames, their intuitive meaning is retained.","This allows us to introduce the notion of hyperconstructivist approximation spaces as the subclass of graph-based frames defined by the first-order conditions corresponding to the same modal axioms defining classical generalized approximation spaces, and to transfer the properties and the intuitive understanding of different approximation spaces to graph-based frames."],"url":"http://arxiv.org/abs/2403.14026v1","category":"math.LO"}
{"created":"2024-03-20 22:33:14","title":"Kilonova parameters estimation with LSST at Vera C. Rubin Observatory","abstract":"The up-coming Vera Rubin Observatory's Legacy Survey of Space and Time (LSST) opens a new opportunity to rapidly survey the southern Sky at optical wavelenghts (\\ie ugrizy bands). In this study, we aim to test the possibility of using LSST observations to constrain the mass and velocity of different KN ejecta components from the observation of a combined set of light curves from GRB afterglows and KN\\ae. We used a sample of simulated light curves from the aforementioned events as they would have been seen during the LSST survey to study how observing strategies' choice impacts the parameters' estimation. We found that the observing strategy design to be the best compromise between light curves coverage, observed filters and fit reliability involves a high number of visits with long gap pairs of about 4 hours every 2 nights in the same or different filters. The features of the observing strategy will allow us to recognize the different stages of the light curve evolution and gather observations in at least 3 filters.","sentences":["The up-coming Vera Rubin Observatory's Legacy Survey of Space and Time (LSST) opens a new opportunity to rapidly survey the southern Sky at optical wavelenghts (\\ie ugrizy bands).","In this study, we aim to test the possibility of using LSST observations to constrain the mass and velocity of different KN ejecta components from the observation of a combined set of light curves from GRB afterglows and KN\\ae.","We used a sample of simulated light curves from the aforementioned events as they would have been seen during the LSST survey to study how observing strategies' choice impacts the parameters' estimation.","We found that the observing strategy design to be the best compromise between light curves coverage, observed filters and fit reliability involves a high number of visits with long gap pairs of about 4 hours every 2 nights in the same or different filters.","The features of the observing strategy will allow us to recognize the different stages of the light curve evolution and gather observations in at least 3 filters."],"url":"http://arxiv.org/abs/2403.14016v1","category":"astro-ph.HE"}
{"created":"2024-03-20 22:23:13","title":"Hybridization induced triplet superconductivity with $S^z=0$","abstract":"The Kitaev superconducting chain is a model of spinless fermions with triplet-like superconductivity. It has raised interest since for some values of its parameters it presents a non-trivial topological phase that host Majorana fermions. The physical realization of a Kitaev chain is complicated by the scarcity of triplet superconductivity in real physical systems. Many proposals have been put forward to overcome this difficulty and fabricate artificial triplet superconducting chains. In this work we study a superconducting chain of spinful fermions forming Cooper pairs, in a triplet $S=1$ state, but with $S^z=0$. The motivation is that such pairing can be induced in chains that couple through an antisymmetric hybridization to an s-wave superconducting substrate. We study the nature of edge states and the topological properties of these chains. In the presence of a magnetic field the chain can sustain gapless superconductivity with pairs of Fermi points. The momentum space topology of these Fermi points is non-trivial, in the sense that they can only disappear by annihilating each other. For small magnetic fields, we find well defined degenerate edge modes with finite Zeemann energy. These modes are not symmetry protected and decay abruptly in the bulk as their energy merges with the continuum of excitations.","sentences":["The Kitaev superconducting chain is a model of spinless fermions with triplet-like superconductivity.","It has raised interest since for some values of its parameters it presents a non-trivial topological phase that host Majorana fermions.","The physical realization of a Kitaev chain is complicated by the scarcity of triplet superconductivity in real physical systems.","Many proposals have been put forward to overcome this difficulty and fabricate artificial triplet superconducting chains.","In this work we study a superconducting chain of spinful fermions forming Cooper pairs, in a triplet $S=1$ state, but with $S^z=0$. The motivation is that such pairing can be induced in chains that couple through an antisymmetric hybridization to an s-wave superconducting substrate.","We study the nature of edge states and the topological properties of these chains.","In the presence of a magnetic field the chain can sustain gapless superconductivity with pairs of Fermi points.","The momentum space topology of these Fermi points is non-trivial, in the sense that they can only disappear by annihilating each other.","For small magnetic fields, we find well defined degenerate edge modes with finite Zeemann energy.","These modes are not symmetry protected and decay abruptly in the bulk as their energy merges with the continuum of excitations."],"url":"http://arxiv.org/abs/2403.14012v1","category":"cond-mat.supr-con"}
{"created":"2024-03-20 22:14:04","title":"The Quantum Theory Of Gravitation, Effective Field Theories, and Strings: Yesterday And Today","abstract":"This paper analyzes the effective field theory perspective on modern physics through the lens of the quantum theory of gravitational interaction. The historical part argues that the search for a theory of quantum gravity stimulated the change in outlook that characterizes the modern approach to the Standard Model of particle physics and General Relativity. We present some landmarks covering a long period, i.e., from the beginning of the 1930s until 1994, when, according to Steven Weinberg, the modern bottom-up approach to General Relativity began. Starting from the first attempt to apply the quantum field theory techniques to perturbatively quantize Einstein's theory, we explore its developments and interaction with the top-down approach encoded by String Theory. In the last part of the paper, we focus on this last approach to describe the relationship between our modern understanding of String Theory and Effective Field Theory in today's panorama. To this end, the non-historical part briefly explains the modern concepts of moduli stabilization and Swampland to understand another change in focus that explains the present framework where some string theorists move.","sentences":["This paper analyzes the effective field theory perspective on modern physics through the lens of the quantum theory of gravitational interaction.","The historical part argues that the search for a theory of quantum gravity stimulated the change in outlook that characterizes the modern approach to the Standard Model of particle physics and General Relativity.","We present some landmarks covering a long period, i.e., from the beginning of the 1930s until 1994, when, according to Steven Weinberg, the modern bottom-up approach to General Relativity began.","Starting from the first attempt to apply the quantum field theory techniques to perturbatively quantize Einstein's theory, we explore its developments and interaction with the top-down approach encoded by String Theory.","In the last part of the paper, we focus on this last approach to describe the relationship between our modern understanding of String Theory and Effective Field Theory in today's panorama.","To this end, the non-historical part briefly explains the modern concepts of moduli stabilization and Swampland to understand another change in focus that explains the present framework where some string theorists move."],"url":"http://arxiv.org/abs/2403.14008v1","category":"physics.hist-ph"}
{"created":"2024-03-20 21:43:05","title":"Smooth $\u03bc$-Hybrid and Non-Minimal Higgs Inflation in $SU(4)_{C}\\times SU(2)_{L}\\times SU(2)_{R}$ With Observable Gravitational Waves","abstract":"We propose to study a smooth variant of the $\\mu$-hybrid inflation model and a non-minimal Higgs model of inflation with quartic non-minimal coupling between the Higgs field and gravity within the context of a realistic GUT gauge group based on supersymmetric $SU(4)_{C}\\times SU(2)_{L}\\times SU(2)_{R}$. These models are incorporated with a realistic scenario of reheating and non-thermal leptogenesis, compatible with the constraints from the baryon asymmetry of the universe. Notably, both models successfully address the MSSM $\\mu$-problem and avoid the issue of primordial magnetic monopoles. Our analysis reveals that both models predict a scalar spectral index $n_s$ that closely aligns with the central observationally favored value of Planck2018 + BICEP2/Keck Array (BK15) data and yield a large tensor-to-scalar ratio ($r > 10^{-3}$), potentially detectable in forthcoming CMB experiments.","sentences":["We propose to study a smooth variant of the $\\mu$-hybrid inflation model and a non-minimal Higgs model of inflation with quartic non-minimal coupling between the Higgs field and gravity within the context of a realistic GUT gauge group based on supersymmetric $SU(4)_{C}\\times SU(2)_{L}\\times SU(2)_{R}$.","These models are incorporated with a realistic scenario of reheating and non-thermal leptogenesis, compatible with the constraints from the baryon asymmetry of the universe.","Notably, both models successfully address the MSSM $\\mu$-problem and avoid the issue of primordial magnetic monopoles.","Our analysis reveals that both models predict a scalar spectral index $n_s$ that closely aligns with the central observationally favored value of Planck2018 + BICEP2/Keck Array (BK15) data and yield a large tensor-to-scalar ratio ($r > 10^{-3}$), potentially detectable in forthcoming CMB experiments."],"url":"http://arxiv.org/abs/2403.13991v1","category":"hep-ph"}
