{"created":"2024-05-28 17:59:47","title":"A Ramsey theorem for the reals","abstract":"We prove that for every colouring of pairs of reals with finitely-many colours, there is a set homeomorphic to the rationals which takes no more than two colours. This was conjectured by Galvin in 1970, and a colouring of Sierpi{\\'n}ski from 1933 witnesses that the number of colours cannot be reduced to one. Previously in 1985 Shelah had shown that a stronger statement is consistent with a forcing construction assuming the existence of large cardinals. Then in 2018 Raghavan and Todor\\v{c}evi\\'c had proved it assuming the existence of large cardinals. We prove it in $ZFC$. In fact Raghavan and Todor\\v{c}evi\\'c proved, assuming more large cardinals, a similar result for a large class of topological spaces. We prove this also, again in $ZFC$.","sentences":["We prove that for every colouring of pairs of reals with finitely-many colours, there is a set homeomorphic to the rationals which takes no more than two colours.","This was conjectured by Galvin in 1970, and a colouring of Sierpi{\\'n}ski from 1933 witnesses that the number of colours cannot be reduced to one.","Previously in 1985 Shelah had shown that a stronger statement is consistent with a forcing construction assuming the existence of large cardinals.","Then in 2018 Raghavan and Todor\\v{c}evi\\'c had proved it assuming the existence of large cardinals.","We prove it in $ZFC$. In fact Raghavan and Todor\\v{c}evi\\'c proved, assuming more large cardinals, a similar result for a large class of topological spaces.","We prove this also, again in $ZFC$."],"url":"http://arxiv.org/abs/2405.18431v1","category":"math.LO"}
{"created":"2024-05-28 17:59:33","title":"DiG: Scalable and Efficient Diffusion Models with Gated Linear Attention","abstract":"Diffusion models with large-scale pre-training have achieved significant success in the field of visual content generation, particularly exemplified by Diffusion Transformers (DiT). However, DiT models have faced challenges with scalability and quadratic complexity efficiency. In this paper, we aim to leverage the long sequence modeling capability of Gated Linear Attention (GLA) Transformers, expanding its applicability to diffusion models. We introduce Diffusion Gated Linear Attention Transformers (DiG), a simple, adoptable solution with minimal parameter overhead, following the DiT design, but offering superior efficiency and effectiveness. In addition to better performance than DiT, DiG-S/2 exhibits $2.5\\times$ higher training speed than DiT-S/2 and saves $75.7\\%$ GPU memory at a resolution of $1792 \\times 1792$. Moreover, we analyze the scalability of DiG across a variety of computational complexity. DiG models, with increased depth/width or augmentation of input tokens, consistently exhibit decreasing FID. We further compare DiG with other subquadratic-time diffusion models. With the same model size, DiG-XL/2 is $4.2\\times$ faster than the recent Mamba-based diffusion model at a $1024$ resolution, and is $1.8\\times$ faster than DiT with CUDA-optimized FlashAttention-2 under the $2048$ resolution. All these results demonstrate its superior efficiency among the latest diffusion models. Code is released at https://github.com/hustvl/DiG.","sentences":["Diffusion models with large-scale pre-training have achieved significant success in the field of visual content generation, particularly exemplified by Diffusion Transformers (DiT).","However, DiT models have faced challenges with scalability and quadratic complexity efficiency.","In this paper, we aim to leverage the long sequence modeling capability of Gated Linear Attention (GLA) Transformers, expanding its applicability to diffusion models.","We introduce Diffusion Gated Linear Attention Transformers (DiG), a simple, adoptable solution with minimal parameter overhead, following the DiT design, but offering superior efficiency and effectiveness.","In addition to better performance than DiT, DiG-S/2 exhibits $2.5\\times$ higher training speed than DiT-S/2 and saves $75.7\\%$ GPU memory at a resolution of $1792 \\times 1792$.","Moreover, we analyze the scalability of DiG across a variety of computational complexity.","DiG models, with increased depth/width or augmentation of input tokens, consistently exhibit decreasing FID.","We further compare DiG with other subquadratic-time diffusion models.","With the same model size, DiG-XL/2 is $4.2\\times$ faster than the recent Mamba-based diffusion model at a $1024$ resolution, and is $1.8\\times$ faster than DiT with CUDA-optimized FlashAttention-2 under the $2048$ resolution.","All these results demonstrate its superior efficiency among the latest diffusion models.","Code is released at https://github.com/hustvl/DiG."],"url":"http://arxiv.org/abs/2405.18428v1","category":"cs.CV"}
{"created":"2024-05-28 17:59:31","title":"Classifying Overlapping Gaussian Mixtures in High Dimensions: From Optimal Classifiers to Neural Nets","abstract":"We derive closed-form expressions for the Bayes optimal decision boundaries in binary classification of high dimensional overlapping Gaussian mixture model (GMM) data, and show how they depend on the eigenstructure of the class covariances, for particularly interesting structured data. We empirically demonstrate, through experiments on synthetic GMMs inspired by real-world data, that deep neural networks trained for classification, learn predictors which approximate the derived optimal classifiers. We further extend our study to networks trained on authentic data, observing that decision thresholds correlate with the covariance eigenvectors rather than the eigenvalues, mirroring our GMM analysis. This provides theoretical insights regarding neural networks' ability to perform probabilistic inference and distill statistical patterns from intricate distributions.","sentences":["We derive closed-form expressions for the Bayes optimal decision boundaries in binary classification of high dimensional overlapping Gaussian mixture model (GMM) data, and show how they depend on the eigenstructure of the class covariances, for particularly interesting structured data.","We empirically demonstrate, through experiments on synthetic GMMs inspired by real-world data, that deep neural networks trained for classification, learn predictors which approximate the derived optimal classifiers.","We further extend our study to networks trained on authentic data, observing that decision thresholds correlate with the covariance eigenvectors rather than the eigenvalues, mirroring our GMM analysis.","This provides theoretical insights regarding neural networks' ability to perform probabilistic inference and distill statistical patterns from intricate distributions."],"url":"http://arxiv.org/abs/2405.18427v1","category":"stat.ML"}
{"created":"2024-05-28 17:59:22","title":"GFlow: Recovering 4D World from Monocular Video","abstract":"Reconstructing 4D scenes from video inputs is a crucial yet challenging task. Conventional methods usually rely on the assumptions of multi-view video inputs, known camera parameters, or static scenes, all of which are typically absent under in-the-wild scenarios. In this paper, we relax all these constraints and tackle a highly ambitious but practical task, which we termed as AnyV4D: we assume only one monocular video is available without any camera parameters as input, and we aim to recover the dynamic 4D world alongside the camera poses. To this end, we introduce GFlow, a new framework that utilizes only 2D priors (depth and optical flow) to lift a video (3D) to a 4D explicit representation, entailing a flow of Gaussian splatting through space and time. GFlow first clusters the scene into still and moving parts, then applies a sequential optimization process that optimizes camera poses and the dynamics of 3D Gaussian points based on 2D priors and scene clustering, ensuring fidelity among neighboring points and smooth movement across frames. Since dynamic scenes always introduce new content, we also propose a new pixel-wise densification strategy for Gaussian points to integrate new visual content. Moreover, GFlow transcends the boundaries of mere 4D reconstruction; it also enables tracking of any points across frames without the need for prior training and segments moving objects from the scene in an unsupervised way. Additionally, the camera poses of each frame can be derived from GFlow, allowing for rendering novel views of a video scene through changing camera pose. By employing the explicit representation, we may readily conduct scene-level or object-level editing as desired, underscoring its versatility and power. Visit our project website at: https://littlepure2333.github.io/GFlow","sentences":["Reconstructing 4D scenes from video inputs is a crucial yet challenging task.","Conventional methods usually rely on the assumptions of multi-view video inputs, known camera parameters, or static scenes, all of which are typically absent under in-the-wild scenarios.","In this paper, we relax all these constraints and tackle a highly ambitious but practical task, which we termed as AnyV4D: we assume only one monocular video is available without any camera parameters as input, and we aim to recover the dynamic 4D world alongside the camera poses.","To this end, we introduce GFlow, a new framework that utilizes only 2D priors (depth and optical flow) to lift a video (3D) to a 4D explicit representation, entailing a flow of Gaussian splatting through space and time.","GFlow first clusters the scene into still and moving parts, then applies a sequential optimization process that optimizes camera poses and the dynamics of 3D Gaussian points based on 2D priors and scene clustering, ensuring fidelity among neighboring points and smooth movement across frames.","Since dynamic scenes always introduce new content, we also propose a new pixel-wise densification strategy for Gaussian points to integrate new visual content.","Moreover, GFlow transcends the boundaries of mere 4D reconstruction; it also enables tracking of any points across frames without the need for prior training and segments moving objects from the scene in an unsupervised way.","Additionally, the camera poses of each frame can be derived from GFlow, allowing for rendering novel views of a video scene through changing camera pose.","By employing the explicit representation, we may readily conduct scene-level or object-level editing as desired, underscoring its versatility and power.","Visit our project website at: https://littlepure2333.github.io/GFlow"],"url":"http://arxiv.org/abs/2405.18426v1","category":"cs.CV"}
{"created":"2024-05-28 17:59:21","title":"ViG: Linear-complexity Visual Sequence Learning with Gated Linear Attention","abstract":"Recently, linear complexity sequence modeling networks have achieved modeling capabilities similar to Vision Transformers on a variety of computer vision tasks, while using fewer FLOPs and less memory. However, their advantage in terms of actual runtime speed is not significant. To address this issue, we introduce Gated Linear Attention (GLA) for vision, leveraging its superior hardware-awareness and efficiency. We propose direction-wise gating to capture 1D global context through bidirectional modeling and a 2D gating locality injection to adaptively inject 2D local details into 1D global context. Our hardware-aware implementation further merges forward and backward scanning into a single kernel, enhancing parallelism and reducing memory cost and latency. The proposed model, \\name{}, offers a favorable trade-off in accuracy, parameters, and FLOPs on ImageNet and downstream tasks, outperforming popular Transformer and CNN-based models. Notably, \\name{}-S matches DeiT-B's accuracy while using only 27\\% of the parameters and 20\\% of the FLOPs, running 2$\\times$ faster on $224\\times224$ images. At $1024\\times1024$ resolution, \\name{}-T uses 5.2$\\times$ fewer FLOPs, saves 90\\% GPU memory, runs 4.8$\\times$ faster, and achieves 20.7\\% higher top-1 accuracy than DeiT-T. These results position \\name{} as an efficient and scalable solution for visual representation learning. Code is available at \\url{https://github.com/hustvl/ViG}.","sentences":["Recently, linear complexity sequence modeling networks have achieved modeling capabilities similar to Vision Transformers on a variety of computer vision tasks, while using fewer FLOPs and less memory.","However, their advantage in terms of actual runtime speed is not significant.","To address this issue, we introduce Gated Linear Attention (GLA) for vision, leveraging its superior hardware-awareness and efficiency.","We propose direction-wise gating to capture 1D global context through bidirectional modeling and a 2D gating locality injection to adaptively inject 2D local details into 1D global context.","Our hardware-aware implementation further merges forward and backward scanning into a single kernel, enhancing parallelism and reducing memory cost and latency.","The proposed model, \\name{}, offers a favorable trade-off in accuracy, parameters, and FLOPs on ImageNet and downstream tasks, outperforming popular Transformer and CNN-based models.","Notably, \\name{}-S matches DeiT-B's accuracy while using only 27\\% of the parameters and 20\\% of the FLOPs, running 2$\\times$ faster on $224\\times224$ images.","At $1024\\times1024$ resolution, \\name{}-T uses 5.2$\\times$ fewer FLOPs, saves 90\\% GPU memory, runs 4.8$\\times$ faster, and achieves 20.7\\% higher top-1 accuracy than DeiT-T. These results position \\name{} as an efficient and scalable solution for visual representation learning.","Code is available at \\url{https://github.com/hustvl/ViG}."],"url":"http://arxiv.org/abs/2405.18425v1","category":"cs.CV"}
{"created":"2024-05-28 17:59:01","title":"3DitScene: Editing Any Scene via Language-guided Disentangled Gaussian Splatting","abstract":"Scene image editing is crucial for entertainment, photography, and advertising design. Existing methods solely focus on either 2D individual object or 3D global scene editing. This results in a lack of a unified approach to effectively control and manipulate scenes at the 3D level with different levels of granularity. In this work, we propose 3DitScene, a novel and unified scene editing framework leveraging language-guided disentangled Gaussian Splatting that enables seamless editing from 2D to 3D, allowing precise control over scene composition and individual objects. We first incorporate 3D Gaussians that are refined through generative priors and optimization techniques. Language features from CLIP then introduce semantics into 3D geometry for object disentanglement. With the disentangled Gaussians, 3DitScene allows for manipulation at both the global and individual levels, revolutionizing creative expression and empowering control over scenes and objects. Experimental results demonstrate the effectiveness and versatility of 3DitScene in scene image editing. Code and online demo can be found at our project homepage: https://zqh0253.github.io/3DitScene/.","sentences":["Scene image editing is crucial for entertainment, photography, and advertising design.","Existing methods solely focus on either 2D individual object or 3D global scene editing.","This results in a lack of a unified approach to effectively control and manipulate scenes at the 3D level with different levels of granularity.","In this work, we propose 3DitScene, a novel and unified scene editing framework leveraging language-guided disentangled Gaussian Splatting that enables seamless editing from 2D to 3D, allowing precise control over scene composition and individual objects.","We first incorporate 3D Gaussians that are refined through generative priors and optimization techniques.","Language features from CLIP then introduce semantics into 3D geometry for object disentanglement.","With the disentangled Gaussians, 3DitScene allows for manipulation at both the global and individual levels, revolutionizing creative expression and empowering control over scenes and objects.","Experimental results demonstrate the effectiveness and versatility of 3DitScene in scene image editing.","Code and online demo can be found at our project homepage: https://zqh0253.github.io/3DitScene/."],"url":"http://arxiv.org/abs/2405.18424v1","category":"cs.CV"}
{"created":"2024-05-28 17:58:46","title":"Bifurcations in Latch-Mediated Spring Actuation (LaMSA) Systems","abstract":"In nature, different species of smaller animals produce ultra-fast movements to aid in their locomotion or protect themselves against predators. These ultra-fast impulsive motions are possible, as often times, there exist a small latch in the organism that could hold the potential energy of the system, and once released, generate an impulsive motion. These types of systems are classified as Latch Mediated Spring Actuated (LaMSA) systems, a multi-dimensional, multi-mode hybrid system that switches between a latched and an unlatched state. The LaMSA mechanism has been studied extensively in the field of biology and is observed in a wide range of animal species, such as the mantis shrimp, grasshoppers, and trap-jaw ants. In recent years, research has been done in mathematically modeling the LaMSA behavior with physical implementations of the mechanism. A significant focus is given to mimicking the physiological behavior of the species and following an end-to-end trajectory of impulsive motion. This paper introduces a foundational analysis of the theoretical dynamics of the contact latch-based LaMSA mechanism. The authors answer the question on what makes these small-scale systems impulsive, with a focus on the intrinsic properties of the system using bifurcations. Necessary and sufficient conditions are derived for the existence of the saddle fixed points. The authors propose a mathematical explanation for mediating the latch when a saddle node exists, and the impulsive behavior after the bifurcation happens.","sentences":["In nature, different species of smaller animals produce ultra-fast movements to aid in their locomotion or protect themselves against predators.","These ultra-fast impulsive motions are possible, as often times, there exist a small latch in the organism that could hold the potential energy of the system, and once released, generate an impulsive motion.","These types of systems are classified as Latch Mediated Spring Actuated (LaMSA) systems, a multi-dimensional, multi-mode hybrid system that switches between a latched and an unlatched state.","The LaMSA mechanism has been studied extensively in the field of biology and is observed in a wide range of animal species, such as the mantis shrimp, grasshoppers, and trap-jaw ants.","In recent years, research has been done in mathematically modeling the LaMSA behavior with physical implementations of the mechanism.","A significant focus is given to mimicking the physiological behavior of the species and following an end-to-end trajectory of impulsive motion.","This paper introduces a foundational analysis of the theoretical dynamics of the contact latch-based LaMSA mechanism.","The authors answer the question on what makes these small-scale systems impulsive, with a focus on the intrinsic properties of the system using bifurcations.","Necessary and sufficient conditions are derived for the existence of the saddle fixed points.","The authors propose a mathematical explanation for mediating the latch when a saddle node exists, and the impulsive behavior after the bifurcation happens."],"url":"http://arxiv.org/abs/2405.18421v1","category":"eess.SY"}
{"created":"2024-05-28 17:58:08","title":"On attractor behavior in braneworld constant-roll inflation","abstract":"We investigate in detail the attractor behavior of some inflationary models based on braneworld dynamics under the constant-roll condition. We describe the dynamics of the models, assuming that the second slow-roll parameter remains constant during inflation. We show that the dynamics of the considered models have the property of a cosmological attractor.","sentences":["We investigate in detail the attractor behavior of some inflationary models based on braneworld dynamics under the constant-roll condition.","We describe the dynamics of the models, assuming that the second slow-roll parameter remains constant during inflation.","We show that the dynamics of the considered models have the property of a cosmological attractor."],"url":"http://arxiv.org/abs/2405.18420v1","category":"gr-qc"}
{"created":"2024-05-28 17:57:23","title":"Hierarchical World Models as Visual Whole-Body Humanoid Controllers","abstract":"Whole-body control for humanoids is challenging due to the high-dimensional nature of the problem, coupled with the inherent instability of a bipedal morphology. Learning from visual observations further exacerbates this difficulty. In this work, we explore highly data-driven approaches to visual whole-body humanoid control based on reinforcement learning, without any simplifying assumptions, reward design, or skill primitives. Specifically, we propose a hierarchical world model in which a high-level agent generates commands based on visual observations for a low-level agent to execute, both of which are trained with rewards. Our approach produces highly performant control policies in 8 tasks with a simulated 56-DoF humanoid, while synthesizing motions that are broadly preferred by humans. Code and videos: https://nicklashansen.com/rlpuppeteer","sentences":["Whole-body control for humanoids is challenging due to the high-dimensional nature of the problem, coupled with the inherent instability of a bipedal morphology.","Learning from visual observations further exacerbates this difficulty.","In this work, we explore highly data-driven approaches to visual whole-body humanoid control based on reinforcement learning, without any simplifying assumptions, reward design, or skill primitives.","Specifically, we propose a hierarchical world model in which a high-level agent generates commands based on visual observations for a low-level agent to execute, both of which are trained with rewards.","Our approach produces highly performant control policies in 8 tasks with a simulated 56-DoF humanoid, while synthesizing motions that are broadly preferred by humans.","Code and videos: https://nicklashansen.com/rlpuppeteer"],"url":"http://arxiv.org/abs/2405.18418v1","category":"cs.LG"}
{"created":"2024-05-28 17:57:06","title":"Why are Visually-Grounded Language Models Bad at Image Classification?","abstract":"Image classification is one of the most fundamental capabilities of machine vision intelligence. In this work, we revisit the image classification task using visually-grounded language models (VLMs) such as GPT-4V and LLaVA. We find that existing proprietary and public VLMs, despite often using CLIP as a vision encoder and having many more parameters, significantly underperform CLIP on standard image classification benchmarks like ImageNet. To understand the reason, we explore several hypotheses concerning the inference algorithms, training objectives, and data processing in VLMs. Our analysis reveals that the primary cause is data-related: critical information for image classification is encoded in the VLM's latent space but can only be effectively decoded with enough training data. Specifically, there is a strong correlation between the frequency of class exposure during VLM training and instruction-tuning and the VLM's performance in those classes; when trained with sufficient data, VLMs can match the accuracy of state-of-the-art classification models. Based on these findings, we enhance a VLM by integrating classification-focused datasets into its training, and demonstrate that the enhanced classification performance of the VLM transfers to its general capabilities, resulting in an improvement of 11.8% on the newly collected ImageWikiQA dataset.","sentences":["Image classification is one of the most fundamental capabilities of machine vision intelligence.","In this work, we revisit the image classification task using visually-grounded language models (VLMs) such as GPT-4V and LLaVA.","We find that existing proprietary and public VLMs, despite often using CLIP as a vision encoder and having many more parameters, significantly underperform CLIP on standard image classification benchmarks like ImageNet.","To understand the reason, we explore several hypotheses concerning the inference algorithms, training objectives, and data processing in VLMs.","Our analysis reveals that the primary cause is data-related: critical information for image classification is encoded in the VLM's latent space but can only be effectively decoded with enough training data.","Specifically, there is a strong correlation between the frequency of class exposure during VLM training and instruction-tuning and the VLM's performance in those classes; when trained with sufficient data, VLMs can match the accuracy of state-of-the-art classification models.","Based on these findings, we enhance a VLM by integrating classification-focused datasets into its training, and demonstrate that the enhanced classification performance of the VLM transfers to its general capabilities, resulting in an improvement of 11.8% on the newly collected ImageWikiQA dataset."],"url":"http://arxiv.org/abs/2405.18415v1","category":"cs.CV"}
{"created":"2024-05-28 17:56:46","title":"Don't Forget to Connect! Improving RAG with Graph-based Reranking","abstract":"Retrieval Augmented Generation (RAG) has greatly improved the performance of Large Language Model (LLM) responses by grounding generation with context from existing documents. These systems work well when documents are clearly relevant to a question context. But what about when a document has partial information, or less obvious connections to the context? And how should we reason about connections between documents? In this work, we seek to answer these two core questions about RAG generation. We introduce G-RAG, a reranker based on graph neural networks (GNNs) between the retriever and reader in RAG. Our method combines both connections between documents and semantic information (via Abstract Meaning Representation graphs) to provide a context-informed ranker for RAG. G-RAG outperforms state-of-the-art approaches while having smaller computational footprint. Additionally, we assess the performance of PaLM 2 as a reranker and find it to significantly underperform G-RAG. This result emphasizes the importance of reranking for RAG even when using Large Language Models.","sentences":["Retrieval Augmented Generation (RAG) has greatly improved the performance of Large Language Model (LLM) responses by grounding generation with context from existing documents.","These systems work well when documents are clearly relevant to a question context.","But what about when a document has partial information, or less obvious connections to the context?","And how should we reason about connections between documents?","In this work, we seek to answer these two core questions about RAG generation.","We introduce G-RAG, a reranker based on graph neural networks (GNNs) between the retriever and reader in RAG.","Our method combines both connections between documents and semantic information (via Abstract Meaning Representation graphs) to provide a context-informed ranker for RAG.","G-RAG outperforms state-of-the-art approaches while having smaller computational footprint.","Additionally, we assess the performance of PaLM 2 as a reranker and find it to significantly underperform G-RAG.","This result emphasizes the importance of reranking for RAG even when using Large Language Models."],"url":"http://arxiv.org/abs/2405.18414v1","category":"cs.CL"}
{"created":"2024-05-28 17:53:47","title":"Towards a Sampling Theory for Implicit Neural Representations","abstract":"Implicit neural representations (INRs) have emerged as a powerful tool for solving inverse problems in computer vision and computational imaging. INRs represent images as continuous domain functions realized by a neural network taking spatial coordinates as inputs. However, unlike traditional pixel representations, little is known about the sample complexity of estimating images using INRs in the context of linear inverse problems. Towards this end, we study the sampling requirements for recovery of a continuous domain image from its low-pass Fourier coefficients by fitting a single hidden-layer INR with ReLU activation and a Fourier features layer using a generalized form of weight decay regularization. Our key insight is to relate minimizers of this non-convex parameter space optimization problem to minimizers of a convex penalty defined over an infinite-dimensional space of measures. We identify a sufficient number of samples for which an image realized by a width-1 INR is exactly recoverable by solving the INR training problem, and give a conjecture for the general width-$W$ case. To validate our theory, we empirically assess the probability of achieving exact recovery of images realized by low-width single hidden-layer INRs, and illustrate the performance of INR on super-resolution recovery of more realistic continuous domain phantom images.","sentences":["Implicit neural representations (INRs) have emerged as a powerful tool for solving inverse problems in computer vision and computational imaging.","INRs represent images as continuous domain functions realized by a neural network taking spatial coordinates as inputs.","However, unlike traditional pixel representations, little is known about the sample complexity of estimating images using INRs in the context of linear inverse problems.","Towards this end, we study the sampling requirements for recovery of a continuous domain image from its low-pass Fourier coefficients by fitting a single hidden-layer INR with ReLU activation and a Fourier features layer using a generalized form of weight decay regularization.","Our key insight is to relate minimizers of this non-convex parameter space optimization problem to minimizers of a convex penalty defined over an infinite-dimensional space of measures.","We identify a sufficient number of samples for which an image realized by a width-1 INR is exactly recoverable by solving the INR training problem, and give a conjecture for the general width-$W$ case.","To validate our theory, we empirically assess the probability of achieving exact recovery of images realized by low-width single hidden-layer INRs, and illustrate the performance of INR on super-resolution recovery of more realistic continuous domain phantom images."],"url":"http://arxiv.org/abs/2405.18410v1","category":"eess.IV"}
{"created":"2024-05-28 17:47:19","title":"Phased Consistency Model","abstract":"The consistency model (CM) has recently made significant progress in accelerating the generation of diffusion models. However, its application to high-resolution, text-conditioned image generation in the latent space (a.k.a., LCM) remains unsatisfactory. In this paper, we identify three key flaws in the current design of LCM. We investigate the reasons behind these limitations and propose the Phased Consistency Model (PCM), which generalizes the design space and addresses all identified limitations. Our evaluations demonstrate that PCM significantly outperforms LCM across 1--16 step generation settings. While PCM is specifically designed for multi-step refinement, it achieves even superior or comparable 1-step generation results to previously state-of-the-art specifically designed 1-step methods. Furthermore, we show that PCM's methodology is versatile and applicable to video generation, enabling us to train the state-of-the-art few-step text-to-video generator. More details are available at https://g-u-n.github.io/projects/pcm/.","sentences":["The consistency model (CM) has recently made significant progress in accelerating the generation of diffusion models.","However, its application to high-resolution, text-conditioned image generation in the latent space (a.k.a., LCM) remains unsatisfactory.","In this paper, we identify three key flaws in the current design of LCM.","We investigate the reasons behind these limitations and propose the Phased Consistency Model (PCM), which generalizes the design space and addresses all identified limitations.","Our evaluations demonstrate that PCM significantly outperforms LCM across 1--16 step generation settings.","While PCM is specifically designed for multi-step refinement, it achieves even superior or comparable 1-step generation results to previously state-of-the-art specifically designed 1-step methods.","Furthermore, we show that PCM's methodology is versatile and applicable to video generation, enabling us to train the state-of-the-art few-step text-to-video generator.","More details are available at https://g-u-n.github.io/projects/pcm/."],"url":"http://arxiv.org/abs/2405.18407v1","category":"cs.LG"}
{"created":"2024-05-28 17:46:36","title":"RACCooN: Remove, Add, and Change Video Content with Auto-Generated Narratives","abstract":"Recent video generative models primarily rely on carefully written text prompts for specific tasks, like inpainting or style editing. They require labor-intensive textual descriptions for input videos, hindering their flexibility to adapt personal/raw videos to user specifications. This paper proposes RACCooN, a versatile and user-friendly video-to-paragraph-to-video generative framework that supports multiple video editing capabilities such as removal, addition, and modification, through a unified pipeline. RACCooN consists of two principal stages: Video-to-Paragraph (V2P) and Paragraph-to-Video (P2V). In the V2P stage, we automatically describe video scenes in well-structured natural language, capturing both the holistic context and focused object details. Subsequently, in the P2V stage, users can optionally refine these descriptions to guide the video diffusion model, enabling various modifications to the input video, such as removing, changing subjects, and/or adding new objects. The proposed approach stands out from other methods through several significant contributions: (1) RACCooN suggests a multi-granular spatiotemporal pooling strategy to generate well-structured video descriptions, capturing both the broad context and object details without requiring complex human annotations, simplifying precise video content editing based on text for users. (2) Our video generative model incorporates auto-generated narratives or instructions to enhance the quality and accuracy of the generated content. It supports the addition of video objects, inpainting, and attribute modification within a unified framework, surpassing existing video editing and inpainting benchmarks. The proposed framework demonstrates impressive versatile capabilities in video-to-paragraph generation, video content editing, and can be incorporated into other SoTA video generative models for further enhancement.","sentences":["Recent video generative models primarily rely on carefully written text prompts for specific tasks, like inpainting or style editing.","They require labor-intensive textual descriptions for input videos, hindering their flexibility to adapt personal/raw videos to user specifications.","This paper proposes RACCooN, a versatile and user-friendly video-to-paragraph-to-video generative framework that supports multiple video editing capabilities such as removal, addition, and modification, through a unified pipeline.","RACCooN consists of two principal stages: Video-to-Paragraph (V2P) and Paragraph-to-Video (P2V).","In the V2P stage, we automatically describe video scenes in well-structured natural language, capturing both the holistic context and focused object details.","Subsequently, in the P2V stage, users can optionally refine these descriptions to guide the video diffusion model, enabling various modifications to the input video, such as removing, changing subjects, and/or adding new objects.","The proposed approach stands out from other methods through several significant contributions: (1) RACCooN suggests a multi-granular spatiotemporal pooling strategy to generate well-structured video descriptions, capturing both the broad context and object details without requiring complex human annotations, simplifying precise video content editing based on text for users.","(2) Our video generative model incorporates auto-generated narratives or instructions to enhance the quality and accuracy of the generated content.","It supports the addition of video objects, inpainting, and attribute modification within a unified framework, surpassing existing video editing and inpainting benchmarks.","The proposed framework demonstrates impressive versatile capabilities in video-to-paragraph generation, video content editing, and can be incorporated into other SoTA video generative models for further enhancement."],"url":"http://arxiv.org/abs/2405.18406v1","category":"cs.CV"}
{"created":"2024-05-28 17:46:27","title":"WIDIn: Wording Image for Domain-Invariant Representation in Single-Source Domain Generalization","abstract":"Language has been useful in extending the vision encoder to data from diverse distributions without empirical discovery in training domains. However, as the image description is mostly at coarse-grained level and ignores visual details, the resulted embeddings are still ineffective in overcoming complexity of domains at inference time. We present a self-supervision framework WIDIn, Wording Images for Domain-Invariant representation, to disentangle discriminative visual representation, by only leveraging data in a single domain and without any test prior. Specifically, for each image, we first estimate the language embedding with fine-grained alignment, which can be consequently used to adaptively identify and then remove domain-specific counterpart from the raw visual embedding. WIDIn can be applied to both pretrained vision-language models like CLIP, and separately trained uni-modal models like MoCo and BERT. Experimental studies on three domain generalization datasets demonstrate the effectiveness of our approach.","sentences":["Language has been useful in extending the vision encoder to data from diverse distributions without empirical discovery in training domains.","However, as the image description is mostly at coarse-grained level and ignores visual details, the resulted embeddings are still ineffective in overcoming complexity of domains at inference time.","We present a self-supervision framework WIDIn, Wording Images for Domain-Invariant representation, to disentangle discriminative visual representation, by only leveraging data in a single domain and without any test prior.","Specifically, for each image, we first estimate the language embedding with fine-grained alignment, which can be consequently used to adaptively identify and then remove domain-specific counterpart from the raw visual embedding.","WIDIn can be applied to both pretrained vision-language models like CLIP, and separately trained uni-modal models like MoCo and BERT.","Experimental studies on three domain generalization datasets demonstrate the effectiveness of our approach."],"url":"http://arxiv.org/abs/2405.18405v1","category":"cs.CV"}
{"created":"2024-05-28 17:45:07","title":"Distributed quantum multiparameter estimation with optimal local measurements","abstract":"We study the multiparameter sensitivity bounds of a sensor made by an array of $d$ spatially-distributed Mach-Zehnder interferometers (MZIs). A generic single non-classical state is mixed with $d-1$ vacuums to create a $d$-modes entangled state, each mode entering one input port of a MZI, while a coherent state enters its second port. We show that local measurements, independently performed on each MZI, are sufficient to provide a sensitivity saturating the quantum Cram\\'er-Rao bound. The sensor can overcome the shot noise limit for the estimation of arbitrary linear combinations of the $d$ phase shifts, provided that the non-classical probe state has an anti-squeezed quadrature variance. We compare the sensitivity bounds of this sensor with that achievable with $d$ independent MZIs, each probed with a nonclassical state and a coherent state. We find that the $d$ independent interferometers can achieve the same sensitivity of the entangled protocol but at the cost of using additional $d$ non-classical states rather than a single one. When using in the two protocols the same average number of particles per shot $\\bar{n}_T$, we find analytically a sensitivity scaling $1/\\bar{n}_T^2$ for the entangled case which provides a gain factor $d$ with respect to the separable case where the sensitivity scales as $d/\\bar{n}_T^2$. We have numerical evidences that the gain factor $d$ is also obtained when fixing the total average number of particles, namely when optimizing with respect to the number of repeated measurements.","sentences":["We study the multiparameter sensitivity bounds of a sensor made by an array of $d$ spatially-distributed Mach-Zehnder interferometers (MZIs).","A generic single non-classical state is mixed with $d-1$ vacuums to create a $d$-modes entangled state, each mode entering one input port of a MZI, while a coherent state enters its second port.","We show that local measurements, independently performed on each MZI, are sufficient to provide a sensitivity saturating the quantum Cram\\'er-Rao bound.","The sensor can overcome the shot noise limit for the estimation of arbitrary linear combinations of the $d$ phase shifts, provided that the non-classical probe state has an anti-squeezed quadrature variance.","We compare the sensitivity bounds of this sensor with that achievable with $d$ independent MZIs, each probed with a nonclassical state and a coherent state.","We find that the $d$ independent interferometers can achieve the same sensitivity of the entangled protocol but at the cost of using additional $d$ non-classical states rather than a single one.","When using in the two protocols the same average number of particles per shot $\\bar{n}_T$, we find analytically a sensitivity scaling $1/\\bar{n}_T^2$ for the entangled case which provides a gain factor $d$ with respect to the separable case where the sensitivity scales as $d/\\bar{n}_T^2$. We have numerical evidences that the gain factor $d$ is also obtained when fixing the total average number of particles, namely when optimizing with respect to the number of repeated measurements."],"url":"http://arxiv.org/abs/2405.18404v1","category":"quant-ph"}
{"created":"2024-05-28 17:43:16","title":"Explicit Formulae to Interchangeably use Hyperplanes and Hyperballs using Inversive Geometry","abstract":"Many algorithms require discriminative boundaries, such as separating hyperplanes or hyperballs, or are specifically designed to work on spherical data. By applying inversive geometry, we show that the two discriminative boundaries can be used interchangeably, and that general Euclidean data can be transformed into spherical data, whenever a change in point distances is acceptable. We provide explicit formulae to embed general Euclidean data into spherical data and to unembed it back. We further show a duality between hyperspherical caps, i.e., the volume created by a separating hyperplane on spherical data, and hyperballs and provide explicit formulae to map between the two. We further provide equations to translate inner products and Euclidean distances between the two spaces, to avoid explicit embedding and unembedding. We also provide a method to enforce projections of the general Euclidean space onto hemi-hyperspheres and propose an intrinsic dimensionality based method to obtain \"all-purpose\" parameters. To show the usefulness of the cap-ball-duality, we discuss example applications in machine learning and vector similarity search.","sentences":["Many algorithms require discriminative boundaries, such as separating hyperplanes or hyperballs, or are specifically designed to work on spherical data.","By applying inversive geometry, we show that the two discriminative boundaries can be used interchangeably, and that general Euclidean data can be transformed into spherical data, whenever a change in point distances is acceptable.","We provide explicit formulae to embed general Euclidean data into spherical data and to unembed it back.","We further show a duality between hyperspherical caps, i.e., the volume created by a separating hyperplane on spherical data, and hyperballs and provide explicit formulae to map between the two.","We further provide equations to translate inner products and Euclidean distances between the two spaces, to avoid explicit embedding and unembedding.","We also provide a method to enforce projections of the general Euclidean space onto hemi-hyperspheres and propose an intrinsic dimensionality based method to obtain \"all-purpose\" parameters.","To show the usefulness of the cap-ball-duality, we discuss example applications in machine learning and vector similarity search."],"url":"http://arxiv.org/abs/2405.18401v1","category":"cs.LG"}
{"created":"2024-05-28 17:40:48","title":"Superposed Decoding: Multiple Generations from a Single Autoregressive Inference Pass","abstract":"Many applications today provide users with multiple auto-complete drafts as they type, including GitHub's code completion, Gmail's smart compose, and Apple's messaging auto-suggestions. Under the hood, language models support this by running an autoregressive inference pass to provide a draft. Consequently, providing $k$ drafts to the user requires running an expensive language model $k$ times. To alleviate the computation cost of running $k$ inference passes, we propose Superposed Decoding, a new decoding algorithm that generates $k$ drafts at the computation cost of one autoregressive inference pass. We achieve this by feeding a superposition of the $k$ most recent token embeddings from the drafts as input to the next decoding step of the language model. At every inference step we combine the $k$ drafts with the top-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options, using an n-gram interpolation with minimal compute overhead to filter out incoherent generations. Our experiments show that $k$ drafts from Superposed Decoding are at least as coherent and factual as Nucleus Sampling and Greedy Decoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In a compute-normalized setting, user evaluations demonstrably favor text generated by Superposed Decoding over Nucleus Sampling. Code and more examples open-sourced at https://github.com/RAIVNLab/SuperposedDecoding.","sentences":["Many applications today provide users with multiple auto-complete drafts as they type, including GitHub's code completion, Gmail's smart compose, and Apple's messaging auto-suggestions.","Under the hood, language models support this by running an autoregressive inference pass to provide a draft.","Consequently, providing $k$ drafts to the user requires running an expensive language model $k$ times.","To alleviate the computation cost of running $k$ inference passes, we propose Superposed Decoding, a new decoding algorithm that generates $k$ drafts at the computation cost of one autoregressive inference pass.","We achieve this by feeding a superposition of the $k$ most recent token embeddings from the drafts as input to the next decoding step of the language model.","At every inference step we combine the $k$ drafts with the top-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options, using an n-gram interpolation with minimal compute overhead to filter out incoherent generations.","Our experiments show that $k$ drafts from Superposed Decoding are at least as coherent and factual as Nucleus Sampling and Greedy Decoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In a compute-normalized setting, user evaluations demonstrably favor text generated by Superposed Decoding over Nucleus Sampling.","Code and more examples open-sourced at https://github.com/RAIVNLab/SuperposedDecoding."],"url":"http://arxiv.org/abs/2405.18400v1","category":"cs.CL"}
{"created":"2024-05-28 17:36:22","title":"Curvature fluctuations in a baby quantum gravity model","abstract":"Understanding the microscopic behavior of spacetime is critical for developing a theory of quantum gravity and perhaps solving the cosmological constant problem. In this context, it has been proposed that the quantity of interest is the quantum uncertainty in the Ricci scalar and here we investigate this for a discrete baby quantum gravity model based a single square cell. We find that the averaged Ricci scalar vanishes to leading order but has UV-divergent fluctuations. While this behavior is stable under renormalization, it appears not to be under the introduction of a small cosmological constant.","sentences":["Understanding the microscopic behavior of spacetime is critical for developing a theory of quantum gravity and perhaps solving the cosmological constant problem.","In this context, it has been proposed that the quantity of interest is the quantum uncertainty in the Ricci scalar and here we investigate this for a discrete baby quantum gravity model based a single square cell.","We find that the averaged Ricci scalar vanishes to leading order but has UV-divergent fluctuations.","While this behavior is stable under renormalization, it appears not to be under the introduction of a small cosmological constant."],"url":"http://arxiv.org/abs/2405.18397v1","category":"gr-qc"}
{"created":"2024-05-28 17:35:05","title":"MC-GTA: Metric-Constrained Model-Based Clustering using Goodness-of-fit Tests with Autocorrelations","abstract":"A wide range of (multivariate) temporal (1D) and spatial (2D) data analysis tasks, such as grouping vehicle sensor trajectories, can be formulated as clustering with given metric constraints. Existing metric-constrained clustering algorithms overlook the rich correlation between feature similarity and metric distance, i.e., metric autocorrelation. The model-based variations of these clustering algorithms (e.g. TICC and STICC) achieve SOTA performance, yet suffer from computational instability and complexity by using a metric-constrained Expectation-Maximization procedure. In order to address these two problems, we propose a novel clustering algorithm, MC-GTA (Model-based Clustering via Goodness-of-fit Tests with Autocorrelations). Its objective is only composed of pairwise weighted sums of feature similarity terms (square Wasserstein-2 distance) and metric autocorrelation terms (a novel multivariate generalization of classic semivariogram). We show that MC-GTA is effectively minimizing the total hinge loss for intra-cluster observation pairs not passing goodness-of-fit tests, i.e., statistically not originating from the same distribution. Experiments on 1D/2D synthetic and real-world datasets demonstrate that MC-GTA successfully incorporates metric autocorrelation. It outperforms strong baselines by large margins (up to 14.3% in ARI and 32.1% in NMI) with faster and stabler optimization (>10x speedup).","sentences":["A wide range of (multivariate) temporal (1D) and spatial (2D) data analysis tasks, such as grouping vehicle sensor trajectories, can be formulated as clustering with given metric constraints.","Existing metric-constrained clustering algorithms overlook the rich correlation between feature similarity and metric distance, i.e., metric autocorrelation.","The model-based variations of these clustering algorithms (e.g. TICC and STICC) achieve SOTA performance, yet suffer from computational instability and complexity by using a metric-constrained Expectation-Maximization procedure.","In order to address these two problems, we propose a novel clustering algorithm, MC-GTA (Model-based Clustering via Goodness-of-fit Tests with Autocorrelations).","Its objective is only composed of pairwise weighted sums of feature similarity terms (square Wasserstein-2 distance) and metric autocorrelation terms (a novel multivariate generalization of classic semivariogram).","We show that MC-GTA is effectively minimizing the total hinge loss for intra-cluster observation pairs not passing goodness-of-fit tests, i.e., statistically not originating from the same distribution.","Experiments on 1D/2D synthetic and real-world datasets demonstrate that MC-GTA successfully incorporates metric autocorrelation.","It outperforms strong baselines by large margins (up to 14.3% in ARI and 32.1% in NMI) with faster and stabler optimization (>10x speedup)."],"url":"http://arxiv.org/abs/2405.18395v1","category":"cs.LG"}
{"created":"2024-05-28 17:34:21","title":"Ensemble Generalization of the Perdew-Zunger Self-Interaction Correction: a Way Out of Multiple Minima and Symmetry Breaking","abstract":"The Perdew-Zunger (PZ) self-interaction correction (SIC) is an established tool to correct unphysical behavior in density functional approximations. Yet, PZ-SIC is well-known to sometimes break molecular symmetries. An example of this is the benzene molecule, for which PZ-SIC predicts a symmetry-broken electron density and molecular geometry, since the method does not describe the two possible Kekul\\'e structures on an even footing, leading to local minima [Lehtola et al, J. Chem. Theory Comput. 2016, 12, 3195]. PZ-SIC is often implemented with Fermi-L\\\"owdin orbitals (FLOs), yielding the FLO-SIC method, which likewise has issues with symmetry breaking and local minima [Trepte et al, J. Chem. Phys. 2021, 155, 224109].   In this work, we propose a generalization of PZ-SIC - the ensemble PZ-SIC (E-PZ-SIC) method - which shares the asymptotic computational scaling of PZ-SIC (albeit with an additional prefactor). E-PZ-SIC is straightforwardly applicable to various molecules, merely requiring one to average the self-interaction correction over all possible Kekul\\'e structures, in line with chemical intuition. We showcase the implementation of E-PZ-SIC with FLOs, as the resulting E-FLO-SIC method is easy to realize on top of an existing implementation of FLO-SIC. We show that E-FLO-SIC indeed eliminates symmetry breaking, reproducing a symmetric electron density and molecular geometry for benzene. The ensemble approach suggested herein could also be employed within locally scaled variants of PZ-SIC and their FLO-SIC versions.","sentences":["The Perdew-Zunger (PZ) self-interaction correction (SIC) is an established tool to correct unphysical behavior in density functional approximations.","Yet, PZ-SIC is well-known to sometimes break molecular symmetries.","An example of this is the benzene molecule, for which PZ-SIC predicts a symmetry-broken electron density and molecular geometry, since the method does not describe the two possible Kekul\\'e structures on an even footing, leading to local minima","[Lehtola et al, J. Chem.","Theory Comput.","2016, 12, 3195].","PZ-SIC is often implemented with Fermi-L\\\"owdin orbitals (FLOs), yielding the FLO-SIC method, which likewise has issues with symmetry breaking and local minima","[Trepte et al, J. Chem.","Phys. 2021, 155, 224109].   ","In this work, we propose a generalization of PZ-SIC - the ensemble PZ-SIC (E-PZ-SIC) method - which shares the asymptotic computational scaling of PZ-SIC (albeit with an additional prefactor).","E-PZ-SIC is straightforwardly applicable to various molecules, merely requiring one to average the self-interaction correction over all possible Kekul\\'e structures, in line with chemical intuition.","We showcase the implementation of E-PZ-SIC with FLOs, as the resulting E-FLO-SIC method is easy to realize on top of an existing implementation of FLO-SIC.","We show that E-FLO-SIC indeed eliminates symmetry breaking, reproducing a symmetric electron density and molecular geometry for benzene.","The ensemble approach suggested herein could also be employed within locally scaled variants of PZ-SIC and their FLO-SIC versions."],"url":"http://arxiv.org/abs/2405.18394v1","category":"physics.chem-ph"}
{"created":"2024-05-28 17:33:54","title":"Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations","abstract":"Scale has become a main ingredient in obtaining strong machine learning models. As a result, understanding a model's scaling properties is key to effectively designing both the right training setup as well as future generations of architectures. In this work, we argue that scale and training research has been needlessly complex due to reliance on the cosine schedule, which prevents training across different lengths for the same model size. We investigate the training behavior of a direct alternative - constant learning rate and cooldowns - and find that it scales predictably and reliably similar to cosine. Additionally, we show that stochastic weight averaging yields improved performance along the training trajectory, without additional training costs, across different scales. Importantly, with these findings we demonstrate that scaling experiments can be performed with significantly reduced compute and GPU hours by utilizing fewer but reusable training runs.","sentences":["Scale has become a main ingredient in obtaining strong machine learning models.","As a result, understanding a model's scaling properties is key to effectively designing both the right training setup as well as future generations of architectures.","In this work, we argue that scale and training research has been needlessly complex due to reliance on the cosine schedule, which prevents training across different lengths for the same model size.","We investigate the training behavior of a direct alternative - constant learning rate and cooldowns - and find that it scales predictably and reliably similar to cosine.","Additionally, we show that stochastic weight averaging yields improved performance along the training trajectory, without additional training costs, across different scales.","Importantly, with these findings we demonstrate that scaling experiments can be performed with significantly reduced compute and GPU hours by utilizing fewer but reusable training runs."],"url":"http://arxiv.org/abs/2405.18392v1","category":"cs.LG"}
{"created":"2024-05-28 17:29:51","title":"Global solutions to the Euler-Coriolis system","abstract":"We prove the global well-posedness and scattering for the 3D incompressible Euler-Coriolis system with sufficiently small, regular and suitably localized initial data. Equivalently, we obtain the asymptotic stability for \"rigid body\" rotational solutions to the pure Euler equations. This extends the recent work of Guo, Pausader and Widmayer to the general non-axisymmetric setting.","sentences":["We prove the global well-posedness and scattering for the 3D incompressible Euler-Coriolis system with sufficiently small, regular and suitably localized initial data.","Equivalently, we obtain the asymptotic stability for \"rigid body\" rotational solutions to the pure Euler equations.","This extends the recent work of Guo, Pausader and Widmayer to the general non-axisymmetric setting."],"url":"http://arxiv.org/abs/2405.18390v1","category":"math.AP"}
{"created":"2024-05-28 17:29:37","title":"Stability of the Parabolic Picard Sheaf","abstract":"Let $X$ be a smooth irreducible complex projective curve of genus $g\\,\\geq\\, 2$, and let $D\\,=\\,x_1+\\dots+x_r$ be a reduced effective divisor on $X$. Denote by $U_{\\alpha}(L)$ the moduli space of stable parabolic bundles on $X$ of rank $n$, determinant $L$ of degree $d$ with flag type $\\{k^i_j\\}$ and generic parabolic weights $\\{\\alpha^i_j\\}$ at each $x_i$, $1\\, \\leq\\, i\\,\\leq \\, r$. There is a universal family on $X\\times U_{\\alpha}(L)$; its direct image to $U_{\\alpha}(L)$ is called the parabolic Picard sheaf. We prove that the parabolic Picard sheaf is stable.","sentences":["Let $X$ be a smooth irreducible complex projective curve of genus $g\\,\\geq\\, 2$, and let $D\\,=\\,x_1+\\dots+x_r$ be a reduced effective divisor on $X$. Denote by $U_{\\alpha}(L)$ the moduli space of stable parabolic bundles on $X$ of rank $n$, determinant $L$ of degree $d$ with flag type $\\{k^i_j\\}$ and generic parabolic weights $\\{\\alpha^i_j\\}$ at each $x_i$, $1\\, \\leq\\, i\\,\\leq \\, r$. There is a universal family on $X\\times U_{\\alpha}(L)$; its direct image to $U_{\\alpha}(L)$ is called the parabolic Picard sheaf.","We prove that the parabolic Picard sheaf is stable."],"url":"http://arxiv.org/abs/2405.18389v1","category":"math.AG"}
{"created":"2024-05-28 17:28:20","title":"Natural numbers from integers","abstract":"In homotopy type theory, a natural number type is freely generated by an element and an endomorphism. Similarly, an integer type is freely generated by an element and an automorphism. Using only dependent sums, identity types, extensional dependent products, and a type of two elements with large elimination, we construct a natural number type from an integer type. As a corollary, homotopy type theory with only $\\Sigma$, $\\mathsf{Id}$, $\\Pi$, and finite colimits with descent (and no universes) admits a natural number type. This improves and simplifies a result by Rose.","sentences":["In homotopy type theory, a natural number type is freely generated by an element and an endomorphism.","Similarly, an integer type is freely generated by an element and an automorphism.","Using only dependent sums, identity types, extensional dependent products, and a type of two elements with large elimination, we construct a natural number type from an integer type.","As a corollary, homotopy type theory with only $\\Sigma$, $\\mathsf{Id}$, $\\Pi$, and finite colimits with descent (and no universes) admits a natural number type.","This improves and simplifies a result by Rose."],"url":"http://arxiv.org/abs/2405.18388v1","category":"cs.LO"}
{"created":"2024-05-28 17:27:24","title":"A Review and Implementation of Object Detection Models and Optimizations for Real-time Medical Mask Detection during the COVID-19 Pandemic","abstract":"Convolutional Neural Networks (CNN) are commonly used for the problem of object detection thanks to their increased accuracy. Nevertheless, the performance of CNN-based detection models is ambiguous when detection speed is considered. To the best of our knowledge, there has not been sufficient evaluation of the available methods in terms of the speed/accuracy trade-off in related literature. This work assesses the most fundamental object detection models on the Common Objects in Context (COCO) dataset with respect to this trade-off, their memory consumption, and computational and storage cost. Next, we select a highly efficient model called YOLOv5 to train on the topical and unexplored dataset of human faces with medical masks, the Properly-Wearing Masked Faces Dataset (PWMFD), and analyze the benefits of specific optimization techniques for real-time medical mask detection: transfer learning, data augmentations, and a Squeeze-and-Excitation attention mechanism. Using our findings in the context of the COVID-19 pandemic, we propose an optimized model based on YOLOv5s using transfer learning for the detection of correctly and incorrectly worn medical masks that surpassed more than two times in speed (69 frames per second) the state-of-the-art model SE-YOLOv3 on the PWMFD dataset while maintaining the same level of mean Average Precision (67%).","sentences":["Convolutional Neural Networks (CNN) are commonly used for the problem of object detection thanks to their increased accuracy.","Nevertheless, the performance of CNN-based detection models is ambiguous when detection speed is considered.","To the best of our knowledge, there has not been sufficient evaluation of the available methods in terms of the speed/accuracy trade-off in related literature.","This work assesses the most fundamental object detection models on the Common Objects in Context (COCO) dataset with respect to this trade-off, their memory consumption, and computational and storage cost.","Next, we select a highly efficient model called YOLOv5 to train on the topical and unexplored dataset of human faces with medical masks, the Properly-Wearing Masked Faces Dataset (PWMFD), and analyze the benefits of specific optimization techniques for real-time medical mask detection: transfer learning, data augmentations, and a Squeeze-and-Excitation attention mechanism.","Using our findings in the context of the COVID-19 pandemic, we propose an optimized model based on YOLOv5s using transfer learning for the detection of correctly and incorrectly worn medical masks that surpassed more than two times in speed (69 frames per second) the state-of-the-art model SE-YOLOv3 on the PWMFD dataset while maintaining the same level of mean Average Precision (67%)."],"url":"http://arxiv.org/abs/2405.18387v1","category":"cs.CV"}
{"created":"2024-05-28 17:27:20","title":"Instruct-MusicGen: Unlocking Text-to-Music Editing for Music Language Models via Instruction Tuning","abstract":"Recent advances in text-to-music editing, which employ text queries to modify music (e.g.\\ by changing its style or adjusting instrumental components), present unique challenges and opportunities for AI-assisted music creation. Previous approaches in this domain have been constrained by the necessity to train specific editing models from scratch, which is both resource-intensive and inefficient; other research uses large language models to predict edited music, resulting in imprecise audio reconstruction. To Combine the strengths and address these limitations, we introduce Instruct-MusicGen, a novel approach that finetunes a pretrained MusicGen model to efficiently follow editing instructions such as adding, removing, or separating stems. Our approach involves a modification of the original MusicGen architecture by incorporating a text fusion module and an audio fusion module, which allow the model to process instruction texts and audio inputs concurrently and yield the desired edited music. Remarkably, Instruct-MusicGen only introduces 8% new parameters to the original MusicGen model and only trains for 5K steps, yet it achieves superior performance across all tasks compared to existing baselines, and demonstrates performance comparable to the models trained for specific tasks. This advancement not only enhances the efficiency of text-to-music editing but also broadens the applicability of music language models in dynamic music production environments.","sentences":["Recent advances in text-to-music editing, which employ text queries to modify music (e.g.\\ by changing its style or adjusting instrumental components), present unique challenges and opportunities for AI-assisted music creation.","Previous approaches in this domain have been constrained by the necessity to train specific editing models from scratch, which is both resource-intensive and inefficient; other research uses large language models to predict edited music, resulting in imprecise audio reconstruction.","To Combine the strengths and address these limitations, we introduce Instruct-MusicGen, a novel approach that finetunes a pretrained MusicGen model to efficiently follow editing instructions such as adding, removing, or separating stems.","Our approach involves a modification of the original MusicGen architecture by incorporating a text fusion module and an audio fusion module, which allow the model to process instruction texts and audio inputs concurrently and yield the desired edited music.","Remarkably, Instruct-MusicGen only introduces 8% new parameters to the original MusicGen model and only trains for 5K steps, yet it achieves superior performance across all tasks compared to existing baselines, and demonstrates performance comparable to the models trained for specific tasks.","This advancement not only enhances the efficiency of text-to-music editing but also broadens the applicability of music language models in dynamic music production environments."],"url":"http://arxiv.org/abs/2405.18386v1","category":"cs.SD"}
{"created":"2024-05-28 17:26:57","title":"Blocking Tracking JavaScript at the Function Granularity","abstract":"Modern websites extensively rely on JavaScript to implement both functionality and tracking. Existing privacy enhancing content blocking tools struggle against mixed scripts, which simultaneously implement both functionality and tracking, because blocking the script would break functionality and not blocking it would allow tracking. We propose Not.js, a fine grained JavaScript blocking tool that operates at the function level granularity. Not.js's strengths lie in analyzing the dynamic execution context, including the call stack and calling context of each JavaScript function, and then encoding this context to build a rich graph representation. Not.js trains a supervised machine learning classifier on a webpage's graph representation to first detect tracking at the JavaScript function level and then automatically generate surrogate scripts that preserve functionality while removing tracking. Our evaluation of Not.js on the top 10K websites demonstrates that it achieves high precision (94%) and recall (98%) in detecting tracking JavaScript functions, outperforming the state of the art while being robust against off the shelf JavaScript obfuscation. Fine grained detection of tracking functions allows Not.js to automatically generate surrogate scripts that remove tracking JavaScript functions without causing major breakage. Our deployment of Not.js shows that mixed scripts are present on 62.3% of the top 10K websites, with 70.6% of the mixed scripts being third party that engage in tracking activities such as cookie ghostwriting. We share a sample of the tracking functions detected by Not.js within mixed scripts not currently on filter lists with filter list authors, who confirm that these scripts are not blocked due to potential functionality breakage, despite being known to implement tracking.","sentences":["Modern websites extensively rely on JavaScript to implement both functionality and tracking.","Existing privacy enhancing content blocking tools struggle against mixed scripts, which simultaneously implement both functionality and tracking, because blocking the script would break functionality and not blocking it would allow tracking.","We propose Not.js, a fine grained JavaScript blocking tool that operates at the function level granularity.","Not.js's strengths lie in analyzing the dynamic execution context, including the call stack and calling context of each JavaScript function, and then encoding this context to build a rich graph representation.","Not.js trains a supervised machine learning classifier on a webpage's graph representation to first detect tracking at the JavaScript function level and then automatically generate surrogate scripts that preserve functionality while removing tracking.","Our evaluation of Not.js on the top 10K websites demonstrates that it achieves high precision (94%) and recall (98%) in detecting tracking JavaScript functions, outperforming the state of the art while being robust against off the shelf JavaScript obfuscation.","Fine grained detection of tracking functions allows Not.js to automatically generate surrogate scripts that remove tracking JavaScript functions without causing major breakage.","Our deployment of Not.js shows that mixed scripts are present on 62.3% of the top 10K websites, with 70.6% of the mixed scripts being third party that engage in tracking activities such as cookie ghostwriting.","We share a sample of the tracking functions detected by Not.js within mixed scripts not currently on filter lists with filter list authors, who confirm that these scripts are not blocked due to potential functionality breakage, despite being known to implement tracking."],"url":"http://arxiv.org/abs/2405.18385v1","category":"cs.CR"}
{"created":"2024-05-28 17:25:43","title":"Brain Tumor Segmentation (BraTS) Challenge 2024: Meningioma Radiotherapy Planning Automated Segmentation","abstract":"The 2024 Brain Tumor Segmentation Meningioma Radiotherapy (BraTS-MEN-RT) challenge aims to advance automated segmentation algorithms using the largest known multi-institutional dataset of radiotherapy planning brain MRIs with expert-annotated target labels for patients with intact or post-operative meningioma that underwent either conventional external beam radiotherapy or stereotactic radiosurgery. Each case includes a defaced 3D post-contrast T1-weighted radiotherapy planning MRI in its native acquisition space, accompanied by a single-label \"target volume\" representing the gross tumor volume (GTV) and any at-risk post-operative site. Target volume annotations adhere to established radiotherapy planning protocols, ensuring consistency across cases and institutions. For pre-operative meningiomas, the target volume encompasses the entire GTV and associated nodular dural tail, while for post-operative cases, it includes at-risk resection cavity margins as determined by the treating institution. Case annotations were reviewed and approved by expert neuroradiologists and radiation oncologists. Participating teams will develop, containerize, and evaluate automated segmentation models using this comprehensive dataset. Model performance will be assessed using the lesion-wise Dice Similarity Coefficient and the 95% Hausdorff distance. The top-performing teams will be recognized at the Medical Image Computing and Computer Assisted Intervention Conference in October 2024. BraTS-MEN-RT is expected to significantly advance automated radiotherapy planning by enabling precise tumor segmentation and facilitating tailored treatment, ultimately improving patient outcomes.","sentences":["The 2024 Brain Tumor Segmentation Meningioma Radiotherapy (BraTS-MEN-RT) challenge aims to advance automated segmentation algorithms using the largest known multi-institutional dataset of radiotherapy planning brain MRIs with expert-annotated target labels for patients with intact or post-operative meningioma that underwent either conventional external beam radiotherapy or stereotactic radiosurgery.","Each case includes a defaced 3D post-contrast T1-weighted radiotherapy planning MRI in its native acquisition space, accompanied by a single-label \"target volume\" representing the gross tumor volume (GTV) and any at-risk post-operative site.","Target volume annotations adhere to established radiotherapy planning protocols, ensuring consistency across cases and institutions.","For pre-operative meningiomas, the target volume encompasses the entire GTV and associated nodular dural tail, while for post-operative cases, it includes at-risk resection cavity margins as determined by the treating institution.","Case annotations were reviewed and approved by expert neuroradiologists and radiation oncologists.","Participating teams will develop, containerize, and evaluate automated segmentation models using this comprehensive dataset.","Model performance will be assessed using the lesion-wise Dice Similarity Coefficient and the 95% Hausdorff distance.","The top-performing teams will be recognized at the Medical Image Computing and Computer Assisted Intervention Conference in October 2024.","BraTS-MEN-RT is expected to significantly advance automated radiotherapy planning by enabling precise tumor segmentation and facilitating tailored treatment, ultimately improving patient outcomes."],"url":"http://arxiv.org/abs/2405.18383v1","category":"cs.CV"}
{"created":"2024-05-28 17:22:22","title":"OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection for Memory-Efficient LLM Fine-tuning","abstract":"The rapid advancements in Large Language Models (LLMs) have revolutionized various natural language processing tasks. However, the substantial size of LLMs presents significant challenges in training or fine-tuning. While parameter-efficient approaches such as low-rank adaptation (LoRA) have gained popularity, they often compromise performance compared to full-rank fine-tuning. In this paper, we propose Outlier-weighed Layerwise Sampled Low-Rank Projection (OwLore), a new memory-efficient fine-tuning approach, inspired by the layerwise outlier distribution of LLMs, which dynamically samples pre-trained layers to fine-tune instead of adding additional adaptors. We first interpret the outlier phenomenon through the lens of Heavy-Tailed Self-Regularization theory (HT-SR), discovering that layers with more outliers tend to be more heavy-tailed and consequently better trained. Inspired by this finding, OwLore strategically assigns higher sampling probabilities to layers with more outliers to better leverage the knowledge stored in pre-trained LLMs. To further mitigate the memory demands of fine-tuning, we integrate gradient low-rank projection into our approach, which facilitates each layer to be efficiently trained in a low-rank manner. By incorporating the efficient characteristics of low-rank and optimal layerwise sampling, OwLore significantly improves the memory-performance trade-off in LLM pruning. Our extensive experiments across various architectures, including LLaMa2, LLaMa3, and Mistral, demonstrate that OwLore consistently outperforms baseline approaches, including full fine-tuning. Specifically, it achieves up to a 1.1% average accuracy gain on the Commonsense Reasoning benchmark, a 3.0% improvement on MMLU, and a notable 10% boost on MT-Bench, while being more memory efficient. OwLore allows us to fine-tune LLaMa2-7B with only 21GB of memory.","sentences":["The rapid advancements in Large Language Models (LLMs) have revolutionized various natural language processing tasks.","However, the substantial size of LLMs presents significant challenges in training or fine-tuning.","While parameter-efficient approaches such as low-rank adaptation (LoRA) have gained popularity, they often compromise performance compared to full-rank fine-tuning.","In this paper, we propose Outlier-weighed Layerwise Sampled Low-Rank Projection (OwLore), a new memory-efficient fine-tuning approach, inspired by the layerwise outlier distribution of LLMs, which dynamically samples pre-trained layers to fine-tune instead of adding additional adaptors.","We first interpret the outlier phenomenon through the lens of Heavy-Tailed Self-Regularization theory (HT-SR), discovering that layers with more outliers tend to be more heavy-tailed and consequently better trained.","Inspired by this finding, OwLore strategically assigns higher sampling probabilities to layers with more outliers to better leverage the knowledge stored in pre-trained LLMs.","To further mitigate the memory demands of fine-tuning, we integrate gradient low-rank projection into our approach, which facilitates each layer to be efficiently trained in a low-rank manner.","By incorporating the efficient characteristics of low-rank and optimal layerwise sampling, OwLore significantly improves the memory-performance trade-off in LLM pruning.","Our extensive experiments across various architectures, including LLaMa2, LLaMa3, and Mistral, demonstrate that OwLore consistently outperforms baseline approaches, including full fine-tuning.","Specifically, it achieves up to a 1.1% average accuracy gain on the Commonsense Reasoning benchmark, a 3.0% improvement on MMLU, and a notable 10% boost on MT-Bench, while being more memory efficient.","OwLore allows us to fine-tune LLaMa2-7B with only 21GB of memory."],"url":"http://arxiv.org/abs/2405.18380v1","category":"cs.LG"}
{"created":"2024-05-28 17:20:44","title":"LLaMA-NAS: Efficient Neural Architecture Search for Large Language Models","abstract":"The abilities of modern large language models (LLMs) in solving natural language processing, complex reasoning, sentiment analysis and other tasks have been extraordinary which has prompted their extensive adoption. Unfortunately, these abilities come with very high memory and computational costs which precludes the use of LLMs on most hardware platforms. To mitigate this, we propose an effective method of finding Pareto-optimal network architectures based on LLaMA2-7B using one-shot NAS. In particular, we fine-tune LLaMA2-7B only once and then apply genetic algorithm-based search to find smaller, less computationally complex network architectures. We show that, for certain standard benchmark tasks, the pre-trained LLaMA2-7B network is unnecessarily large and complex. More specifically, we demonstrate a 1.5x reduction in model size and 1.3x speedup in throughput for certain tasks with negligible drop in accuracy. In addition to finding smaller, higher-performing network architectures, our method does so more effectively and efficiently than certain pruning or sparsification techniques. Finally, we demonstrate how quantization is complementary to our method and that the size and complexity of the networks we find can be further decreased using quantization. We believe that our work provides a way to automatically create LLMs which can be used on less expensive and more readily available hardware platforms.","sentences":["The abilities of modern large language models (LLMs) in solving natural language processing, complex reasoning, sentiment analysis and other tasks have been extraordinary which has prompted their extensive adoption.","Unfortunately, these abilities come with very high memory and computational costs which precludes the use of LLMs on most hardware platforms.","To mitigate this, we propose an effective method of finding Pareto-optimal network architectures based on LLaMA2-7B using one-shot NAS.","In particular, we fine-tune LLaMA2-7B only once and then apply genetic algorithm-based search to find smaller, less computationally complex network architectures.","We show that, for certain standard benchmark tasks, the pre-trained LLaMA2-7B network is unnecessarily large and complex.","More specifically, we demonstrate a 1.5x reduction in model size and 1.3x speedup in throughput for certain tasks with negligible drop in accuracy.","In addition to finding smaller, higher-performing network architectures, our method does so more effectively and efficiently than certain pruning or sparsification techniques.","Finally, we demonstrate how quantization is complementary to our method and that the size and complexity of the networks we find can be further decreased using quantization.","We believe that our work provides a way to automatically create LLMs which can be used on less expensive and more readily available hardware platforms."],"url":"http://arxiv.org/abs/2405.18377v1","category":"cs.AI"}
{"created":"2024-05-28 17:10:20","title":"ML-QLS: Multilevel Quantum Layout Synthesis","abstract":"Quantum Layout Synthesis (QLS) plays a crucial role in optimizing quantum circuit execution on physical quantum devices. As we enter the era where quantum computers have hundreds of qubits, we are faced with scalability issues using optimal approaches and degrading heuristic methods' performance due to the lack of global optimization. To this end, we introduce a hybrid design that obtains the much improved solution for the heuristic method utilizing the multilevel framework, which is an effective methodology to solve large-scale problems in VLSI design. In this paper, we present ML-QLS, the first multilevel quantum layout tool with a scalable refinement operation integrated with novel cost functions and clustering strategies. Our clustering provides valuable insights into generating a proper problem approximation for quantum circuits and devices. Our experimental results demonstrate that ML-QLS can scale up to problems involving hundreds of qubits and achieve a remarkable 52% performance improvement over leading heuristic QLS tools for large circuits, which underscores the effectiveness of multilevel frameworks in quantum applications.","sentences":["Quantum Layout Synthesis (QLS) plays a crucial role in optimizing quantum circuit execution on physical quantum devices.","As we enter the era where quantum computers have hundreds of qubits, we are faced with scalability issues using optimal approaches and degrading heuristic methods' performance due to the lack of global optimization.","To this end, we introduce a hybrid design that obtains the much improved solution for the heuristic method utilizing the multilevel framework, which is an effective methodology to solve large-scale problems in VLSI design.","In this paper, we present ML-QLS, the first multilevel quantum layout tool with a scalable refinement operation integrated with novel cost functions and clustering strategies.","Our clustering provides valuable insights into generating a proper problem approximation for quantum circuits and devices.","Our experimental results demonstrate that ML-QLS can scale up to problems involving hundreds of qubits and achieve a remarkable 52% performance improvement over leading heuristic QLS tools for large circuits, which underscores the effectiveness of multilevel frameworks in quantum applications."],"url":"http://arxiv.org/abs/2405.18371v1","category":"quant-ph"}
{"created":"2024-05-28 17:09:46","title":"Cyclotomic Structures in Symplectic Topology","abstract":"We extend the Cohen-Jones-Segal construction of stable homotopy types associated to flow categories of Morse-Smale functions $f$ to the setting where $f$ is equivariant under a finite group action and is Morse but no longer Morse-Smale. This setting occurs universally, as equivariant Morse functions can rarely be perturbed to nearby equivariant Morse-Smale functions. The method is very general, and allows one to do equivariant Floer theory while avoiding all the complications typically caused by issues of equivariant transversality. The construction assigns a (genuine) equivariant orthogonal spectrum to an equivariant framed virtually smooth flow category. Using this method, we construct, for a compact symplectic manifold $M$, which is symplectically atoroidal with contact boundary, and is equipped with an equivariant trivialization of its polarization class, a cyclotomic structure on the spectral lift of the symplectic cohomology $SH^*(M)$. This generalizes a variant of the map which sends loops to their $p$-fold covers on free loop spaces to the setting of general Liouville domains, and suggests a systematic connection between Floer homology and $p$-adic Hodge theory.","sentences":["We extend the Cohen-Jones-Segal construction of stable homotopy types associated to flow categories of Morse-Smale functions $f$ to the setting where $f$ is equivariant under a finite group action and is Morse but no longer Morse-Smale.","This setting occurs universally, as equivariant Morse functions can rarely be perturbed to nearby equivariant Morse-Smale functions.","The method is very general, and allows one to do equivariant Floer theory while avoiding all the complications typically caused by issues of equivariant transversality.","The construction assigns a (genuine) equivariant orthogonal spectrum to an equivariant framed virtually smooth flow category.","Using this method, we construct, for a compact symplectic manifold $M$, which is symplectically atoroidal with contact boundary, and is equipped with an equivariant trivialization of its polarization class, a cyclotomic structure on the spectral lift of the symplectic cohomology $SH^*(M)$. This generalizes a variant of the map which sends loops to their $p$-fold covers on free loop spaces to the setting of general Liouville domains, and suggests a systematic connection between Floer homology and $p$-adic Hodge theory."],"url":"http://arxiv.org/abs/2405.18370v1","category":"math.SG"}
{"created":"2024-05-28 17:08:31","title":"PromptWizard: Task-Aware Agent-driven Prompt Optimization Framework","abstract":"Large language models (LLMs) have revolutionized AI across diverse domains, showcasing remarkable capabilities. Central to their success is the concept of prompting, which guides model output generation. However, manual prompt engineering is labor-intensive and domain-specific, necessitating automated solutions. This paper introduces PromptWizard, a novel framework leveraging LLMs to iteratively synthesize and refine prompts tailored to specific tasks. Unlike existing approaches, PromptWizard optimizes both prompt instructions and in-context examples, maximizing model performance. The framework iteratively refines prompts by mutating instructions and incorporating negative examples to deepen understanding and ensure diversity. It further enhances both instructions and examples with the aid of a critic, synthesizing new instructions and examples enriched with detailed reasoning steps for optimal performance. PromptWizard offers several key features and capabilities, including computational efficiency compared to state-of-the-art approaches, adaptability to scenarios with varying amounts of training data, and effectiveness with smaller LLMs. Rigorous evaluation across 35 tasks on 8 datasets demonstrates PromptWizard's superiority over existing prompt strategies, showcasing its efficacy and scalability in prompt optimization.","sentences":["Large language models (LLMs) have revolutionized AI across diverse domains, showcasing remarkable capabilities.","Central to their success is the concept of prompting, which guides model output generation.","However, manual prompt engineering is labor-intensive and domain-specific, necessitating automated solutions.","This paper introduces PromptWizard, a novel framework leveraging LLMs to iteratively synthesize and refine prompts tailored to specific tasks.","Unlike existing approaches, PromptWizard optimizes both prompt instructions and in-context examples, maximizing model performance.","The framework iteratively refines prompts by mutating instructions and incorporating negative examples to deepen understanding and ensure diversity.","It further enhances both instructions and examples with the aid of a critic, synthesizing new instructions and examples enriched with detailed reasoning steps for optimal performance.","PromptWizard offers several key features and capabilities, including computational efficiency compared to state-of-the-art approaches, adaptability to scenarios with varying amounts of training data, and effectiveness with smaller LLMs.","Rigorous evaluation across 35 tasks on 8 datasets demonstrates PromptWizard's superiority over existing prompt strategies, showcasing its efficacy and scalability in prompt optimization."],"url":"http://arxiv.org/abs/2405.18369v1","category":"cs.CL"}
{"created":"2024-05-28 17:05:29","title":"Black Hole Search in Dynamic Graphs","abstract":"A black hole in a graph is a dangerous site that disposes any incoming agent into that node without leaving any trace of its existence. In the Black Hole Search (BHS) problem, the goal is for at least one agent to survive, locate the position of the black hole, and then terminate. This problem has been extensively studied for static graphs, where the edges do not disappear with time. In dynamic graphs, where the edges may disappear and reappear with time, the problem has only been studied for specific graphs such as rings and cactuses. In this work, we investigate the problem of BHS for general graphs with a much weaker model with respect to the one used for the cases of rings and cactus graphs\\cite{bhattacharya_2023, Paola_2024}. We consider two cases: (a) where the adversary can remove at most one edge in each round, and (b) where the adversary can remove at most $f$ edges in each round. In both scenarios, we consider rooted configuration.   In the case when the adversary can remove at most one edge from the graph, we provide an algorithm that uses 9 agents to solve the BHS problem in $O(m^2)$ time given that each node $v$ is equipped with $O(\\log \\delta_v)$ storage in the form of a whiteboard, where $m$ is the number of edges in $G$ and $\\delta_v$ is the degree of node $v$. We also prove that it is impossible for $2\\delta_{BH}$ many agents with $O(\\log n)$ memory to locate the black hole where $\\delta_{BH}$ is the degree of the black hole even if the nodes are equipped with whiteboards of $O(\\log \\delta_v)$ storage.   In a scenario where the adversary can remove at most $f$ edges and the initial configuration is rooted, we present an algorithm that uses $6f$ agents to solve the BHS problem. We also prove that solving BHS using $2f+1$ agents starting from a rooted configuration on a general graph is impossible, even with unlimited node storage and infinite agent memory.","sentences":["A black hole in a graph is a dangerous site that disposes any incoming agent into that node without leaving any trace of its existence.","In the Black Hole Search (BHS) problem, the goal is for at least one agent to survive, locate the position of the black hole, and then terminate.","This problem has been extensively studied for static graphs, where the edges do not disappear with time.","In dynamic graphs, where the edges may disappear and reappear with time, the problem has only been studied for specific graphs such as rings and cactuses.","In this work, we investigate the problem of BHS for general graphs with a much weaker model with respect to the one used for the cases of rings and cactus graphs\\cite{bhattacharya_2023, Paola_2024}.","We consider two cases: (a) where the adversary can remove at most one edge in each round, and (b) where the adversary can remove at most $f$ edges in each round.","In both scenarios, we consider rooted configuration.   ","In the case when the adversary can remove at most one edge from the graph, we provide an algorithm that uses 9 agents to solve the BHS problem in $O(m^2)$ time given that each node $v$ is equipped with $O(\\log \\delta_v)$ storage in the form of a whiteboard, where $m$ is the number of edges in $G$ and $\\delta_v$ is the degree of node $v$. We also prove that it is impossible for $2\\delta_{BH}$ many agents with $O(\\log n)$ memory to locate the black hole where $\\delta_{BH}$ is the degree of the black hole even if the nodes are equipped with whiteboards of $O(\\log \\delta_v)$ storage.   ","In a scenario where the adversary can remove at most $f$ edges and the initial configuration is rooted, we present an algorithm that uses $6f$ agents to solve the BHS problem.","We also prove that solving BHS using $2f+1$ agents starting from a rooted configuration on a general graph is impossible, even with unlimited node storage and infinite agent memory."],"url":"http://arxiv.org/abs/2405.18367v1","category":"cs.DC"}
{"created":"2024-05-28 17:00:37","title":"Pixel domain implementation of the Minimally Informed CMB MAp foreground Cleaning (MICMAC) method","abstract":"High fidelity separation of astrophysical foreground contributions from the cosmic microwave background (CMB) signal has been recognized as one of the main challenges of modern CMB data analysis, and one which needs to be addressed in a robust way to ensure that the next generation of CMB polarization experiments lives up to its promise. In this work we consider the non-parametric maximum likelihood CMB cleaning approach recently proposed by some of the authors which has been shown to match the performance of standard parametric techniques for simple foreground models, while superseding it in cases where the foregrounds do not exhibit a simple frequency dependence. We present a new implementation of the method in pixel space, extending its functionalities to account for spatial variability of the properties of the foregrounds. We describe the algorithmic details of our approach and its validation against the original code as well as the parametric method for various experimental set-ups and different models of the foreground components. We argue that the method provides a compelling alternative to other state-of-the-art techniques.","sentences":["High fidelity separation of astrophysical foreground contributions from the cosmic microwave background (CMB) signal has been recognized as one of the main challenges of modern CMB data analysis, and one which needs to be addressed in a robust way to ensure that the next generation of CMB polarization experiments lives up to its promise.","In this work we consider the non-parametric maximum likelihood CMB cleaning approach recently proposed by some of the authors which has been shown to match the performance of standard parametric techniques for simple foreground models, while superseding it in cases where the foregrounds do not exhibit a simple frequency dependence.","We present a new implementation of the method in pixel space, extending its functionalities to account for spatial variability of the properties of the foregrounds.","We describe the algorithmic details of our approach and its validation against the original code as well as the parametric method for various experimental set-ups and different models of the foreground components.","We argue that the method provides a compelling alternative to other state-of-the-art techniques."],"url":"http://arxiv.org/abs/2405.18365v1","category":"astro-ph.CO"}
{"created":"2024-05-28 16:56:52","title":"The finite Friedman-Stanley jumps: generic dichotomies for Borel homomorphisms","abstract":"Fix $n=1,2,3,\\dots$ or $n=\\omega$. We prove a dichotomy for Borel homomorphisms from the $n$-th Friedman-Stanley jump $=^{+n}$ to an equivalence relation $E$ which is classifiable by countable structures: if there is no reduction from $=^{+n}$ to $E$, then in fact all Borel homomorphisms are very far from a reduction. For this we use a different presentation of $=^{+n}$, equivalent up to Borel bi-reducibility, which is susceptible to Baire-category techniques.   This dichotomy is seen as a method for proving positive Borel reducibility results from $=^{+n}$. As corollaries we prove: (1) for $n\\leq\\omega$, $=^{+n}$ is in the spectrum of the meager ideal. This extends a result of Kanovei, Sabok, and Zapletal for $n=1$; (2) $=^{+\\omega}$ is a regular equivalence relation. This answers positively a question of Clemens; (3) for $n<\\omega$, the equivalence relations, classifiable by countable structures, which do not Borel reduce $=^{+n}$ are closed under countable products. This extends a result of Kanovei, Sabok, and Zapletal for $n=1$.","sentences":["Fix $n=1,2,3,\\dots$ or $n=\\omega$. We prove a dichotomy for Borel homomorphisms from the $n$-th Friedman-Stanley jump $=^{+n}$ to an equivalence relation $E$ which is classifiable by countable structures: if there is no reduction from $=^{+n}$ to $E$, then in fact all Borel homomorphisms are very far from a reduction.","For this we use a different presentation of $=^{+n}$, equivalent up to Borel bi-reducibility, which is susceptible to Baire-category techniques.   ","This dichotomy is seen as a method for proving positive Borel reducibility results from $=^{+n}$. As corollaries we prove: (1) for $n\\leq\\omega$, $=^{+n}$ is in the spectrum of the meager ideal.","This extends a result of Kanovei, Sabok, and Zapletal for $n=1$; (2) $=^{+\\omega}$ is a regular equivalence relation.","This answers positively a question of Clemens; (3) for $n<\\omega$, the equivalence relations, classifiable by countable structures, which do not Borel reduce $=^{+n}$ are closed under countable products.","This extends a result of Kanovei, Sabok, and Zapletal for $n=1$."],"url":"http://arxiv.org/abs/2405.18360v1","category":"math.LO"}
{"created":"2024-05-28 16:56:42","title":"Bridging the Gap: Dynamic Learning Strategies for Improving Multilingual Performance in LLMs","abstract":"Large language models (LLMs) are at the forefront of transforming numerous domains globally. However, their inclusivity and effectiveness remain limited for non-Latin scripts and low-resource languages. This paper tackles the imperative challenge of enhancing the multilingual performance of LLMs without extensive training or fine-tuning. Through systematic investigation and evaluation of diverse languages using popular question-answering (QA) datasets, we present novel techniques that unlock the true potential of LLMs in a polyglot landscape. Our approach encompasses three key strategies that yield significant improvements in multilingual proficiency. First, by meticulously optimizing prompts tailored for polyglot LLMs, we unlock their latent capabilities, resulting in substantial performance boosts across languages. Second, we introduce a new hybrid approach that synergizes LLM Retrieval Augmented Generation (RAG) with multilingual embeddings and achieves improved multilingual task performance. Finally, we introduce a novel learning approach that dynamically selects the optimal prompt strategy, LLM model, and embedding model per query at run-time. This dynamic adaptation maximizes the efficacy of LLMs across languages, outperforming best static and random strategies. Additionally, our approach adapts configurations in both offline and online settings, and can seamlessly adapt to new languages and datasets, leading to substantial advancements in multilingual understanding and generation across diverse languages.","sentences":["Large language models (LLMs) are at the forefront of transforming numerous domains globally.","However, their inclusivity and effectiveness remain limited for non-Latin scripts and low-resource languages.","This paper tackles the imperative challenge of enhancing the multilingual performance of LLMs without extensive training or fine-tuning.","Through systematic investigation and evaluation of diverse languages using popular question-answering (QA) datasets, we present novel techniques that unlock the true potential of LLMs in a polyglot landscape.","Our approach encompasses three key strategies that yield significant improvements in multilingual proficiency.","First, by meticulously optimizing prompts tailored for polyglot LLMs, we unlock their latent capabilities, resulting in substantial performance boosts across languages.","Second, we introduce a new hybrid approach that synergizes LLM Retrieval Augmented Generation (RAG) with multilingual embeddings and achieves improved multilingual task performance.","Finally, we introduce a novel learning approach that dynamically selects the optimal prompt strategy, LLM model, and embedding model per query at run-time.","This dynamic adaptation maximizes the efficacy of LLMs across languages, outperforming best static and random strategies.","Additionally, our approach adapts configurations in both offline and online settings, and can seamlessly adapt to new languages and datasets, leading to substantial advancements in multilingual understanding and generation across diverse languages."],"url":"http://arxiv.org/abs/2405.18359v1","category":"cs.CL"}
{"created":"2024-05-28 16:55:41","title":"MMCTAgent: Multi-modal Critical Thinking Agent Framework for Complex Visual Reasoning","abstract":"Recent advancements in Multi-modal Large Language Models (MLLMs) have significantly improved their performance in tasks combining vision and language. However, challenges persist in detailed multi-modal understanding, comprehension of complex tasks, and reasoning over multi-modal information. This paper introduces MMCTAgent, a novel multi-modal critical thinking agent framework designed to address the inherent limitations of current MLLMs in complex visual reasoning tasks. Inspired by human cognitive processes and critical thinking, MMCTAgent iteratively analyzes multi-modal information, decomposes queries, plans strategies, and dynamically evolves its reasoning. Additionally, MMCTAgent incorporates critical thinking elements such as verification of final answers and self-reflection through a novel approach that defines a vision-based critic and identifies task-specific evaluation criteria, thereby enhancing its decision-making abilities. Through rigorous evaluations across various image and video understanding benchmarks, we demonstrate that MMCTAgent (with and without the critic) outperforms both foundational MLLMs and other tool-augmented pipelines.","sentences":["Recent advancements in Multi-modal Large Language Models (MLLMs) have significantly improved their performance in tasks combining vision and language.","However, challenges persist in detailed multi-modal understanding, comprehension of complex tasks, and reasoning over multi-modal information.","This paper introduces MMCTAgent, a novel multi-modal critical thinking agent framework designed to address the inherent limitations of current MLLMs in complex visual reasoning tasks.","Inspired by human cognitive processes and critical thinking, MMCTAgent iteratively analyzes multi-modal information, decomposes queries, plans strategies, and dynamically evolves its reasoning.","Additionally, MMCTAgent incorporates critical thinking elements such as verification of final answers and self-reflection through a novel approach that defines a vision-based critic and identifies task-specific evaluation criteria, thereby enhancing its decision-making abilities.","Through rigorous evaluations across various image and video understanding benchmarks, we demonstrate that MMCTAgent (with and without the critic) outperforms both foundational MLLMs and other tool-augmented pipelines."],"url":"http://arxiv.org/abs/2405.18358v1","category":"cs.CL"}
{"created":"2024-05-28 16:55:15","title":"Universal and Extensible Language-Vision Models for Organ Segmentation and Tumor Detection from Abdominal Computed Tomography","abstract":"The advancement of artificial intelligence (AI) for organ segmentation and tumor detection is propelled by the growing availability of computed tomography (CT) datasets with detailed, per-voxel annotations. However, these AI models often struggle with flexibility for partially annotated datasets and extensibility for new classes due to limitations in the one-hot encoding, architectural design, and learning scheme. To overcome these limitations, we propose a universal, extensible framework enabling a single model, termed Universal Model, to deal with multiple public datasets and adapt to new classes (e.g., organs/tumors). Firstly, we introduce a novel language-driven parameter generator that leverages language embeddings from large language models, enriching semantic encoding compared with one-hot encoding. Secondly, the conventional output layers are replaced with lightweight, class-specific heads, allowing Universal Model to simultaneously segment 25 organs and six types of tumors and ease the addition of new classes. We train our Universal Model on 3,410 CT volumes assembled from 14 publicly available datasets and then test it on 6,173 CT volumes from four external datasets. Universal Model achieves first place on six CT tasks in the Medical Segmentation Decathlon (MSD) public leaderboard and leading performance on the Beyond The Cranial Vault (BTCV) dataset. In summary, Universal Model exhibits remarkable computational efficiency (6x faster than other dataset-specific models), demonstrates strong generalization across different hospitals, transfers well to numerous downstream tasks, and more importantly, facilitates the extensibility to new classes while alleviating the catastrophic forgetting of previously learned classes. Codes, models, and datasets are available at https://github.com/ljwztc/CLIP-Driven-Universal-Model","sentences":["The advancement of artificial intelligence (AI) for organ segmentation and tumor detection is propelled by the growing availability of computed tomography (CT) datasets with detailed, per-voxel annotations.","However, these AI models often struggle with flexibility for partially annotated datasets and extensibility for new classes due to limitations in the one-hot encoding, architectural design, and learning scheme.","To overcome these limitations, we propose a universal, extensible framework enabling a single model, termed Universal Model, to deal with multiple public datasets and adapt to new classes (e.g., organs/tumors).","Firstly, we introduce a novel language-driven parameter generator that leverages language embeddings from large language models, enriching semantic encoding compared with one-hot encoding.","Secondly, the conventional output layers are replaced with lightweight, class-specific heads, allowing Universal Model to simultaneously segment 25 organs and six types of tumors and ease the addition of new classes.","We train our Universal Model on 3,410 CT volumes assembled from 14 publicly available datasets and then test it on 6,173 CT volumes from four external datasets.","Universal Model achieves first place on six CT tasks in the Medical Segmentation Decathlon (MSD) public leaderboard and leading performance on the Beyond The Cranial Vault (BTCV) dataset.","In summary, Universal Model exhibits remarkable computational efficiency (6x faster than other dataset-specific models), demonstrates strong generalization across different hospitals, transfers well to numerous downstream tasks, and more importantly, facilitates the extensibility to new classes while alleviating the catastrophic forgetting of previously learned classes.","Codes, models, and datasets are available at https://github.com/ljwztc/CLIP-Driven-Universal-Model"],"url":"http://arxiv.org/abs/2405.18356v1","category":"eess.IV"}
{"created":"2024-05-28 16:53:45","title":"Finite presentability of twisted Brin-Thompson groups","abstract":"Given a group $G$ acting faithfully on a set $S$, we characterize precisely when the twisted Brin-Thompson group $SV_G$ is finitely presented. The answer is that $SV_G$ is finitely presented if and only if we have the following: $G$ is finitely presented, the action of $G$ on $S$ has finitely many orbits of two-element subsets of $S$, and the stabilizer in $G$ of any element of $S$ is finitely generated. Since twisted Brin-Thompson groups are simple, a consequence is that any subgroup of a group admitting an action as above satisfies the Boone-Higman conjecture. In the course of proving this, we also establish a sufficient condition for a group acting cocompactly on a simply connected complex to be finitely presented, even if certain edge stabilizers are not finitely generated, which may be of independent interest.","sentences":["Given a group $G$ acting faithfully on a set $S$, we characterize precisely when the twisted Brin-Thompson group $SV_G$ is finitely presented.","The answer is that $SV_G$ is finitely presented if and only if we have the following: $G$ is finitely presented, the action of $G$ on $S$ has finitely many orbits of two-element subsets of $S$, and the stabilizer in $G$ of any element of $S$ is finitely generated.","Since twisted Brin-Thompson groups are simple, a consequence is that any subgroup of a group admitting an action as above satisfies the Boone-Higman conjecture.","In the course of proving this, we also establish a sufficient condition for a group acting cocompactly on a simply connected complex to be finitely presented, even if certain edge stabilizers are not finitely generated, which may be of independent interest."],"url":"http://arxiv.org/abs/2405.18354v1","category":"math.GR"}
{"created":"2024-05-28 16:49:28","title":"Evaluating Bayesian deep learning for radio galaxy classification","abstract":"The radio astronomy community is rapidly adopting deep learning techniques to deal with the huge data volumes expected from the next generation of radio observatories. Bayesian neural networks (BNNs) provide a principled way to model uncertainty in the predictions made by such deep learning models and will play an important role in extracting well-calibrated uncertainty estimates on their outputs. In this work, we evaluate the performance of different BNNs against the following criteria: predictive performance, uncertainty calibration and distribution-shift detection for the radio galaxy classification problem.","sentences":["The radio astronomy community is rapidly adopting deep learning techniques to deal with the huge data volumes expected from the next generation of radio observatories.","Bayesian neural networks (BNNs) provide a principled way to model uncertainty in the predictions made by such deep learning models and will play an important role in extracting well-calibrated uncertainty estimates on their outputs.","In this work, we evaluate the performance of different BNNs against the following criteria: predictive performance, uncertainty calibration and distribution-shift detection for the radio galaxy classification problem."],"url":"http://arxiv.org/abs/2405.18351v1","category":"cs.LG"}
{"created":"2024-05-28 16:48:05","title":"A System for Automatic English Text Expansion","abstract":"We present an automatic text expansion system to generate English sentences, which performs automatic Natural Language Generation (NLG) by combining linguistic rules with statistical approaches. Here, \"automatic\" means that the system can generate coherent and correct sentences from a minimum set of words. From its inception, the design is modular and adaptable to other languages. This adaptability is one of its greatest advantages. For English, we have created the highly precise aLexiE lexicon with wide coverage, which represents a contribution on its own. We have evaluated the resulting NLG library in an Augmentative and Alternative Communication (AAC) proof of concept, both directly (by regenerating corpus sentences) and manually (from annotations) using a popular corpus in the NLG field. We performed a second analysis by comparing the quality of text expansion in English to Spanish, using an ad-hoc Spanish-English parallel corpus. The system might also be applied to other domains such as report and news generation.","sentences":["We present an automatic text expansion system to generate English sentences, which performs automatic Natural Language Generation (NLG) by combining linguistic rules with statistical approaches.","Here, \"automatic\" means that the system can generate coherent and correct sentences from a minimum set of words.","From its inception, the design is modular and adaptable to other languages.","This adaptability is one of its greatest advantages.","For English, we have created the highly precise aLexiE lexicon with wide coverage, which represents a contribution on its own.","We have evaluated the resulting NLG library in an Augmentative and Alternative Communication (AAC) proof of concept, both directly (by regenerating corpus sentences) and manually (from annotations) using a popular corpus in the NLG field.","We performed a second analysis by comparing the quality of text expansion in English to Spanish, using an ad-hoc Spanish-English parallel corpus.","The system might also be applied to other domains such as report and news generation."],"url":"http://arxiv.org/abs/2405.18350v1","category":"cs.CL"}
{"created":"2024-05-28 16:46:00","title":"Cosmological phase transitions at three loops: the final verdict on perturbation theory","abstract":"We complete the perturbative program for equilibrium thermodynamics of cosmological first-order phase transitions by determining the finite-temperature effective potential of gauge-Higgs theories at next-to-next-to-next-to-next-to-leading order (N$^4$LO). The computation of the three-loop effective potential required to reach this order is extended to generic models in dimensionally reduced effective theories in a companion article. Our N$^4$LO result is the last perturbative order before confinement renders electroweak gauge-Higgs theories non-perturbative at four loops. By contrasting our analysis with non-perturbative lattice results, we find a remarkable agreement. As a direct application for predictions of gravitational waves produced by a first-order transition, our computation provides the final fully perturbative results for the phase transition strength and speed of sound.","sentences":["We complete the perturbative program for equilibrium thermodynamics of cosmological first-order phase transitions by determining the finite-temperature effective potential of gauge-Higgs theories at next-to-next-to-next-to-next-to-leading order (N$^4$LO).","The computation of the three-loop effective potential required to reach this order is extended to generic models in dimensionally reduced effective theories in a companion article.","Our N$^4$LO result is the last perturbative order before confinement renders electroweak gauge-Higgs theories non-perturbative at four loops.","By contrasting our analysis with non-perturbative lattice results, we find a remarkable agreement.","As a direct application for predictions of gravitational waves produced by a first-order transition, our computation provides the final fully perturbative results for the phase transition strength and speed of sound."],"url":"http://arxiv.org/abs/2405.18349v1","category":"hep-ph"}
{"created":"2024-05-28 16:43:41","title":"Intelligent Clinical Documentation: Harnessing Generative AI for Patient-Centric Clinical Note Generation","abstract":"Comprehensive clinical documentation is crucial for effective healthcare delivery, yet it poses a significant burden on healthcare professionals, leading to burnout, increased medical errors, and compromised patient safety. This paper explores the potential of generative AI (Artificial Intelligence) to streamline the clinical documentation process, specifically focusing on generating SOAP (Subjective, Objective, Assessment, Plan) and BIRP (Behavior, Intervention, Response, Plan) notes. We present a case study demonstrating the application of natural language processing (NLP) and automatic speech recognition (ASR) technologies to transcribe patient-clinician interactions, coupled with advanced prompting techniques to generate draft clinical notes using large language models (LLMs). The study highlights the benefits of this approach, including time savings, improved documentation quality, and enhanced patient-centered care. Additionally, we discuss ethical considerations, such as maintaining patient confidentiality and addressing model biases, underscoring the need for responsible deployment of generative AI in healthcare settings. The findings suggest that generative AI has the potential to revolutionize clinical documentation practices, alleviating administrative burdens and enabling healthcare professionals to focus more on direct patient care.","sentences":["Comprehensive clinical documentation is crucial for effective healthcare delivery, yet it poses a significant burden on healthcare professionals, leading to burnout, increased medical errors, and compromised patient safety.","This paper explores the potential of generative AI (Artificial Intelligence) to streamline the clinical documentation process, specifically focusing on generating SOAP (Subjective, Objective, Assessment, Plan) and BIRP (Behavior, Intervention, Response, Plan) notes.","We present a case study demonstrating the application of natural language processing (NLP) and automatic speech recognition (ASR) technologies to transcribe patient-clinician interactions, coupled with advanced prompting techniques to generate draft clinical notes using large language models (LLMs).","The study highlights the benefits of this approach, including time savings, improved documentation quality, and enhanced patient-centered care.","Additionally, we discuss ethical considerations, such as maintaining patient confidentiality and addressing model biases, underscoring the need for responsible deployment of generative AI in healthcare settings.","The findings suggest that generative AI has the potential to revolutionize clinical documentation practices, alleviating administrative burdens and enabling healthcare professionals to focus more on direct patient care."],"url":"http://arxiv.org/abs/2405.18346v1","category":"cs.AI"}
{"created":"2024-05-28 16:43:01","title":"Fine Tuning of the Rotational Rate of Light-Driven, Second Generation Molecular Motors by Fluorine Substitutions","abstract":"The relaxation time of several second generation molecular motors is analysed by calculating the minimum energy path between the metastable and stable states and evaluating the transition rate within harmonic transition state theory based on energetics obtained from density functional theory. Comparison with published experimental data shows remarkably good agreement and demonstrates the predictive capability of the theoretical approach. While previous measurements by Feringa and coworkers [Chem.\\,Eur.\\,J.\\,(2017) 23, 6643] have shown that a replacement of the stereogenic hydrogen by a fluorine atom increases the relaxation time because of destabilization of the transition state for the thermal helix inversion, we find that a replacement of CH$_3$ by a CF$_3$ group at the same site shortens the relaxation time because of elevated energy of the metastable state without a significant shift in the transition state energy. Since these two fluorine substitutions have an opposite effect on the relaxation time, the two combined can provide a way to fine tune the rotational speed of a molecular motor.","sentences":["The relaxation time of several second generation molecular motors is analysed by calculating the minimum energy path between the metastable and stable states and evaluating the transition rate within harmonic transition state theory based on energetics obtained from density functional theory.","Comparison with published experimental data shows remarkably good agreement and demonstrates the predictive capability of the theoretical approach.","While previous measurements by Feringa and coworkers [Chem.\\,Eur.\\,J.\\,(2017) 23, 6643] have shown that a replacement of the stereogenic hydrogen by a fluorine atom increases the relaxation time because of destabilization of the transition state for the thermal helix inversion, we find that a replacement of CH$_3$ by a CF$_3$ group at the same site shortens the relaxation time because of elevated energy of the metastable state without a significant shift in the transition state energy.","Since these two fluorine substitutions have an opposite effect on the relaxation time, the two combined can provide a way to fine tune the rotational speed of a molecular motor."],"url":"http://arxiv.org/abs/2405.18345v1","category":"physics.chem-ph"}
{"created":"2024-05-28 16:42:43","title":"The Battle of LLMs: A Comparative Study in Conversational QA Tasks","abstract":"Large language models have gained considerable interest for their impressive performance on various tasks. Within this domain, ChatGPT and GPT-4, developed by OpenAI, and the Gemini, developed by Google, have emerged as particularly popular among early adopters. Additionally, Mixtral by Mistral AI and Claude by Anthropic are newly released, further expanding the landscape of advanced language models. These models are viewed as disruptive technologies with applications spanning customer service, education, healthcare, and finance. More recently, Mistral has entered the scene, captivating users with its unique ability to generate creative content. Understanding the perspectives of these users is crucial, as they can offer valuable insights into the potential strengths, weaknesses, and overall success or failure of these technologies in various domains. This research delves into the responses generated by ChatGPT, GPT-4, Gemini, Mixtral and Claude across different Conversational QA corpora. Evaluation scores were meticulously computed and subsequently compared to ascertain the overall performance of these models. Our study pinpointed instances where these models provided inaccurate answers to questions, offering insights into potential areas where they might be susceptible to errors. In essence, this research provides a comprehensive comparison and evaluation of these state of-the-art language models, shedding light on their capabilities while also highlighting potential areas for improvement","sentences":["Large language models have gained considerable interest for their impressive performance on various tasks.","Within this domain, ChatGPT and GPT-4, developed by OpenAI, and the Gemini, developed by Google, have emerged as particularly popular among early adopters.","Additionally, Mixtral by Mistral AI and Claude by Anthropic are newly released, further expanding the landscape of advanced language models.","These models are viewed as disruptive technologies with applications spanning customer service, education, healthcare, and finance.","More recently, Mistral has entered the scene, captivating users with its unique ability to generate creative content.","Understanding the perspectives of these users is crucial, as they can offer valuable insights into the potential strengths, weaknesses, and overall success or failure of these technologies in various domains.","This research delves into the responses generated by ChatGPT, GPT-4, Gemini, Mixtral and Claude across different Conversational QA corpora.","Evaluation scores were meticulously computed and subsequently compared to ascertain the overall performance of these models.","Our study pinpointed instances where these models provided inaccurate answers to questions, offering insights into potential areas where they might be susceptible to errors.","In essence, this research provides a comprehensive comparison and evaluation of these state of-the-art language models, shedding light on their capabilities while also highlighting potential areas for improvement"],"url":"http://arxiv.org/abs/2405.18344v1","category":"cs.CL"}
{"created":"2024-05-28 16:37:27","title":"On One Generalization of the Multipoint Nonlocal Contact Problem for Elliptic Equation in Rectangular Area","abstract":"A nonlocal contact problem for two-dimensional linear elliptic equations is stated and investigated. The method of separation of variables is used to find the solution of a stated problem in case of Poisson's equation. Then the more general problem with nonlocal multipoint contact conditions for elliptic equation with variable coefficients is considered and the iterative method to solve the problem numerically is constructed and investigated. The uniqueness and existence of the regular solution is proved. The iterative method allows to reduce the solution of a nonlocal contact problem to the solution of a sequence of classical boundary value problems.","sentences":["A nonlocal contact problem for two-dimensional linear elliptic equations is stated and investigated.","The method of separation of variables is used to find the solution of a stated problem in case of Poisson's equation.","Then the more general problem with nonlocal multipoint contact conditions for elliptic equation with variable coefficients is considered and the iterative method to solve the problem numerically is constructed and investigated.","The uniqueness and existence of the regular solution is proved.","The iterative method allows to reduce the solution of a nonlocal contact problem to the solution of a sequence of classical boundary value problems."],"url":"http://arxiv.org/abs/2405.18342v1","category":"math.AP"}
{"created":"2024-05-28 16:37:06","title":"The Ross-Darboux-Stieltjes Integral","abstract":"Motivated by the limitations of the traditional definitions of the Riemann-Stieltjes and Darboux-Stieltjes integrals, we introduce a generalized Darboux-Stieltjes integral that is equivalent to an earlier generalization by Ross \\cite{Ross}. Our definition builds upon an approach to the Darboux-Stieltjes integral recently introduced by the first author and Convertito \\cite{TSI}. We show that our definition agrees with all previous definitions, but that the class of integrable functions is much larger. We develop all the analogs of the classic results for the Riemann integral, and rectify the problems inherent in the definition of the Darboux-Stieltjes integral in \\cite{TSI}. In particular, we show the Bounded Convergence Theorem holds for our definition and that it agrees with the Lebesgue-Stieltjes integral.","sentences":["Motivated by the limitations of the traditional definitions of the Riemann-Stieltjes and Darboux-Stieltjes integrals, we introduce a generalized Darboux-Stieltjes integral that is equivalent to an earlier generalization by Ross \\cite{Ross}.","Our definition builds upon an approach to the Darboux-Stieltjes integral recently introduced by the first author and Convertito \\cite{TSI}.","We show that our definition agrees with all previous definitions, but that the class of integrable functions is much larger.","We develop all the analogs of the classic results for the Riemann integral, and rectify the problems inherent in the definition of the Darboux-Stieltjes integral in \\cite{TSI}.","In particular, we show the Bounded Convergence Theorem holds for our definition and that it agrees with the Lebesgue-Stieltjes integral."],"url":"http://arxiv.org/abs/2405.18341v1","category":"math.CA"}
{"created":"2024-05-28 16:36:22","title":"RealTimeTransport: An open-source C++ library for quantum transport simulations in the strong coupling regime","abstract":"The description of quantum transport in the strong system-reservoir coupling regime poses a significant theoretical and computational challenge that demands specialized tools for accurate analysis. RealTimeTransport is a new open-source C++ library that enables the computation of both stationary and transient transport observables for generic quantum systems connected to metallic reservoirs. It computes the Nakajima-Zwanzig memory kernels for both dynamics and transport in real-time going beyond traditional expansions in the bare system-reservoir couplings. Currently, several methods are available: (i) A renormalized perturbation theory in leading and next-to-leading order which avoids the low-temperature breakdown that limits the traditional theory. (ii) Starting from this well-behaved reference solution a 2- and 3-loop self-consistent renormalization-group transformation of the memory kernels is implemented. This allows refined quantitative predictions even in the presence of many body resonances, such as the Kondo enhancement of cotunneling. This paper provides an overview of the theory, the architecture of RealTimeTransport and practical demonstrations of the currently implemented methods. In particular, we analyze the stationary transport through a serial double quantum dot and showcase for the $T=0$ interacting Anderson model the complete time-development of single-electron tunneling (SET), cotunneling-assisted SET (CO-SET) and inelastic cotunneling resonances throughout the entire gate-bias stability diagram. We discuss the range of applicability of the implemented methods and benchmark them against other advanced approaches.","sentences":["The description of quantum transport in the strong system-reservoir coupling regime poses a significant theoretical and computational challenge that demands specialized tools for accurate analysis.","RealTimeTransport is a new open-source C++ library that enables the computation of both stationary and transient transport observables for generic quantum systems connected to metallic reservoirs.","It computes the Nakajima-Zwanzig memory kernels for both dynamics and transport in real-time going beyond traditional expansions in the bare system-reservoir couplings.","Currently, several methods are available: (i) A renormalized perturbation theory in leading and next-to-leading order which avoids the low-temperature breakdown that limits the traditional theory.","(ii) Starting from this well-behaved reference solution a 2- and 3-loop self-consistent renormalization-group transformation of the memory kernels is implemented.","This allows refined quantitative predictions even in the presence of many body resonances, such as the Kondo enhancement of cotunneling.","This paper provides an overview of the theory, the architecture of RealTimeTransport and practical demonstrations of the currently implemented methods.","In particular, we analyze the stationary transport through a serial double quantum dot and showcase for the $T=0$ interacting Anderson model the complete time-development of single-electron tunneling (SET), cotunneling-assisted SET (CO-SET) and inelastic cotunneling resonances throughout the entire gate-bias stability diagram.","We discuss the range of applicability of the implemented methods and benchmark them against other advanced approaches."],"url":"http://arxiv.org/abs/2405.18340v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-28 16:28:58","title":"Interpretable classification of wiki-review streams","abstract":"Wiki articles are created and maintained by a crowd of editors, producing a continuous stream of reviews. Reviews can take the form of additions, reverts, or both. This crowdsourcing model is exposed to manipulation since neither reviews nor editors are automatically screened and purged. To protect articles against vandalism or damage, the stream of reviews can be mined to classify reviews and profile editors in real-time. The goal of this work is to anticipate and explain which reviews to revert. This way, editors are informed why their edits will be reverted. The proposed method employs stream-based processing, updating the profiling and classification models on each incoming event. The profiling uses side and content-based features employing Natural Language Processing, and editor profiles are incrementally updated based on their reviews. Since the proposed method relies on self-explainable classification algorithms, it is possible to understand why a review has been classified as a revert or a non-revert. In addition, this work contributes an algorithm for generating synthetic data for class balancing, making the final classification fairer. The proposed online method was tested with a real data set from Wikivoyage, which was balanced through the aforementioned synthetic data generation. The results attained near-90 % values for all evaluation metrics (accuracy, precision, recall, and F-measure).","sentences":["Wiki articles are created and maintained by a crowd of editors, producing a continuous stream of reviews.","Reviews can take the form of additions, reverts, or both.","This crowdsourcing model is exposed to manipulation since neither reviews nor editors are automatically screened and purged.","To protect articles against vandalism or damage, the stream of reviews can be mined to classify reviews and profile editors in real-time.","The goal of this work is to anticipate and explain which reviews to revert.","This way, editors are informed why their edits will be reverted.","The proposed method employs stream-based processing, updating the profiling and classification models on each incoming event.","The profiling uses side and content-based features employing Natural Language Processing, and editor profiles are incrementally updated based on their reviews.","Since the proposed method relies on self-explainable classification algorithms, it is possible to understand why a review has been classified as a revert or a non-revert.","In addition, this work contributes an algorithm for generating synthetic data for class balancing, making the final classification fairer.","The proposed online method was tested with a real data set from Wikivoyage, which was balanced through the aforementioned synthetic data generation.","The results attained near-90 % values for all evaluation metrics (accuracy, precision, recall, and F-measure)."],"url":"http://arxiv.org/abs/2405.18335v1","category":"cs.CL"}
{"created":"2024-05-28 16:28:28","title":"On the analysis of a higher-order Lotka-Volterra model: an application of S-tensors and the polynomial complementarity problem","abstract":"It is known that the effect of species' density on species' growth is non-additive in real ecological systems. This challenges the conventional Lotka-Volterra model, where the interactions are always pairwise and their effects are additive. To address this challenge, we introduce HOIs (Higher-Order Interactions) which are able to capture, for example, the indirect effect of one species on a second one correlating to a third species. Towards this end, we propose a general higher-order Lotka-Volterra model. We provide an existence result of a positive equilibrium for a non-homogeneous polynomial equation system with the help of S-tensors. Afterward, by utilizing the latter result, as well as the theory of monotone systems and results from the polynomial complementarity problem, we provide comprehensive results regarding the existence, uniqueness, and stability of the corresponding equilibrium. These results can be regarded as natural extensions of many analogous ones for the classical Lotka-Volterra model, especially in the case of full cooperation, competition among two factions, and pure competition. Finally, illustrative numerical examples are provided to highlight our contributions.","sentences":["It is known that the effect of species' density on species' growth is non-additive in real ecological systems.","This challenges the conventional Lotka-Volterra model, where the interactions are always pairwise and their effects are additive.","To address this challenge, we introduce HOIs (Higher-Order Interactions) which are able to capture, for example, the indirect effect of one species on a second one correlating to a third species.","Towards this end, we propose a general higher-order Lotka-Volterra model.","We provide an existence result of a positive equilibrium for a non-homogeneous polynomial equation system with the help of S-tensors.","Afterward, by utilizing the latter result, as well as the theory of monotone systems and results from the polynomial complementarity problem, we provide comprehensive results regarding the existence, uniqueness, and stability of the corresponding equilibrium.","These results can be regarded as natural extensions of many analogous ones for the classical Lotka-Volterra model, especially in the case of full cooperation, competition among two factions, and pure competition.","Finally, illustrative numerical examples are provided to highlight our contributions."],"url":"http://arxiv.org/abs/2405.18333v1","category":"eess.SY"}
{"created":"2024-05-28 16:24:47","title":"Frustratingly Easy Test-Time Adaptation of Vision-Language Models","abstract":"Vision-Language Models seamlessly discriminate among arbitrary semantic categories, yet they still suffer from poor generalization when presented with challenging examples. For this reason, Episodic Test-Time Adaptation (TTA) strategies have recently emerged as powerful techniques to adapt VLMs in the presence of a single unlabeled image. The recent literature on TTA is dominated by the paradigm of prompt tuning by Marginal Entropy Minimization, which, relying on online backpropagation, inevitably slows down inference while increasing memory. In this work, we theoretically investigate the properties of this approach and unveil that a surprisingly strong TTA method lies dormant and hidden within it. We term this approach ZERO (TTA with \"zero\" temperature), whose design is both incredibly effective and frustratingly simple: augment N times, predict, retain the most confident predictions, and marginalize after setting the Softmax temperature to zero. Remarkably, ZERO requires a single batched forward pass through the vision encoder only and no backward passes. We thoroughly evaluate our approach following the experimental protocol established in the literature and show that ZERO largely surpasses or compares favorably w.r.t. the state-of-the-art while being almost 10x faster and 13x more memory-friendly than standard Test-Time Prompt Tuning. Thanks to its simplicity and comparatively negligible computation, ZERO can serve as a strong baseline for future work in this field. The code is available at https://github.com/FarinaMatteo/zero.","sentences":["Vision-Language Models seamlessly discriminate among arbitrary semantic categories, yet they still suffer from poor generalization when presented with challenging examples.","For this reason, Episodic Test-Time Adaptation (TTA) strategies have recently emerged as powerful techniques to adapt VLMs in the presence of a single unlabeled image.","The recent literature on TTA is dominated by the paradigm of prompt tuning by Marginal Entropy Minimization, which, relying on online backpropagation, inevitably slows down inference while increasing memory.","In this work, we theoretically investigate the properties of this approach and unveil that a surprisingly strong TTA method lies dormant and hidden within it.","We term this approach ZERO (TTA with \"zero\" temperature), whose design is both incredibly effective and frustratingly simple: augment N times, predict, retain the most confident predictions, and marginalize after setting the Softmax temperature to zero.","Remarkably, ZERO requires a single batched forward pass through the vision encoder only and no backward passes.","We thoroughly evaluate our approach following the experimental protocol established in the literature and show that ZERO largely surpasses or compares favorably w.r.t.","the state-of-the-art while being almost 10x faster and 13x more memory-friendly than standard Test-Time Prompt Tuning.","Thanks to its simplicity and comparatively negligible computation, ZERO can serve as a strong baseline for future work in this field.","The code is available at https://github.com/FarinaMatteo/zero."],"url":"http://arxiv.org/abs/2405.18330v1","category":"cs.CV"}
{"created":"2024-05-28 16:21:20","title":"Histopathology Based AI Model Predicts Anti-Angiogenic Therapy Response in Renal Cancer Clinical Trial","abstract":"Predictive biomarkers of treatment response are lacking for metastatic clear cell renal cell carcinoma (ccRCC), a tumor type that is treated with angiogenesis inhibitors, immune checkpoint inhibitors, mTOR inhibitors and a HIF2 inhibitor. The Angioscore, an RNA-based quantification of angiogenesis, is arguably the best candidate to predict anti-angiogenic (AA) response. However, the clinical adoption of transcriptomic assays faces several challenges including standardization, time delay, and high cost. Further, ccRCC tumors are highly heterogenous, and sampling multiple areas for sequencing is impractical. Here we present a novel deep learning (DL) approach to predict the Angioscore from ubiquitous histopathology slides. To overcome the lack of interpretability, one of the biggest limitations of typical DL models, our model produces a visual vascular network which is the basis of the model's prediction. To test its reliability, we applied this model to multiple cohorts including a clinical trial dataset. Our model accurately predicts the RNA-based Angioscore on multiple independent cohorts (spearman correlations of 0.77 and 0.73). Further, the predictions help unravel meaningful biology such as association of angiogenesis with grade, stage, and driver mutation status. Finally, we find our model can predict response to AA therapy, in both a real-world cohort and the IMmotion150 clinical trial. The predictive power of our model vastly exceeds that of CD31, a marker of vasculature, and nearly rivals the performance (c-index 0.66 vs 0.67) of the ground truth RNA-based Angioscore at a fraction of the cost. By providing a robust yet interpretable prediction of the Angioscore from histopathology slides alone, our approach offers insights into angiogenesis biology and AA treatment response.","sentences":["Predictive biomarkers of treatment response are lacking for metastatic clear cell renal cell carcinoma (ccRCC), a tumor type that is treated with angiogenesis inhibitors, immune checkpoint inhibitors, mTOR inhibitors and a HIF2 inhibitor.","The Angioscore, an RNA-based quantification of angiogenesis, is arguably the best candidate to predict anti-angiogenic (AA) response.","However, the clinical adoption of transcriptomic assays faces several challenges including standardization, time delay, and high cost.","Further, ccRCC tumors are highly heterogenous, and sampling multiple areas for sequencing is impractical.","Here we present a novel deep learning (DL) approach to predict the Angioscore from ubiquitous histopathology slides.","To overcome the lack of interpretability, one of the biggest limitations of typical DL models, our model produces a visual vascular network which is the basis of the model's prediction.","To test its reliability, we applied this model to multiple cohorts including a clinical trial dataset.","Our model accurately predicts the RNA-based Angioscore on multiple independent cohorts (spearman correlations of 0.77 and 0.73).","Further, the predictions help unravel meaningful biology such as association of angiogenesis with grade, stage, and driver mutation status.","Finally, we find our model can predict response to AA therapy, in both a real-world cohort and the IMmotion150 clinical trial.","The predictive power of our model vastly exceeds that of CD31, a marker of vasculature, and nearly rivals the performance (c-index 0.66 vs 0.67) of the ground truth RNA-based Angioscore at a fraction of the cost.","By providing a robust yet interpretable prediction of the Angioscore from histopathology slides alone, our approach offers insights into angiogenesis biology and AA treatment response."],"url":"http://arxiv.org/abs/2405.18327v1","category":"q-bio.QM"}
{"created":"2024-05-28 16:21:03","title":"VITON-DiT: Learning In-the-Wild Video Try-On from Human Dance Videos via Diffusion Transformers","abstract":"Video try-on stands as a promising area for its tremendous real-world potential. Prior works are limited to transferring product clothing images onto person videos with simple poses and backgrounds, while underperforming on casually captured videos. Recently, Sora revealed the scalability of Diffusion Transformer (DiT) in generating lifelike videos featuring real-world scenarios. Inspired by this, we explore and propose the first DiT-based video try-on framework for practical in-the-wild applications, named VITON-DiT. Specifically, VITON-DiT consists of a garment extractor, a Spatial-Temporal denoising DiT, and an identity preservation ControlNet. To faithfully recover the clothing details, the extracted garment features are fused with the self-attention outputs of the denoising DiT and the ControlNet. We also introduce novel random selection strategies during training and an Interpolated Auto-Regressive (IAR) technique at inference to facilitate long video generation. Unlike existing attempts that require the laborious and restrictive construction of a paired training dataset, severely limiting their scalability, VITON-DiT alleviates this by relying solely on unpaired human dance videos and a carefully designed multi-stage training strategy. Furthermore, we curate a challenging benchmark dataset to evaluate the performance of casual video try-on. Extensive experiments demonstrate the superiority of VITON-DiT in generating spatio-temporal consistent try-on results for in-the-wild videos with complicated human poses.","sentences":["Video try-on stands as a promising area for its tremendous real-world potential.","Prior works are limited to transferring product clothing images onto person videos with simple poses and backgrounds, while underperforming on casually captured videos.","Recently, Sora revealed the scalability of Diffusion Transformer (DiT) in generating lifelike videos featuring real-world scenarios.","Inspired by this, we explore and propose the first DiT-based video try-on framework for practical in-the-wild applications, named VITON-DiT. Specifically, VITON-DiT consists of a garment extractor, a Spatial-Temporal denoising DiT, and an identity preservation ControlNet.","To faithfully recover the clothing details, the extracted garment features are fused with the self-attention outputs of the denoising DiT and the ControlNet.","We also introduce novel random selection strategies during training and an Interpolated Auto-Regressive (IAR) technique at inference to facilitate long video generation.","Unlike existing attempts that require the laborious and restrictive construction of a paired training dataset, severely limiting their scalability, VITON-DiT alleviates this by relying solely on unpaired human dance videos and a carefully designed multi-stage training strategy.","Furthermore, we curate a challenging benchmark dataset to evaluate the performance of casual video try-on.","Extensive experiments demonstrate the superiority of VITON-DiT in generating spatio-temporal consistent try-on results for in-the-wild videos with complicated human poses."],"url":"http://arxiv.org/abs/2405.18326v1","category":"cs.CV"}
{"created":"2024-05-28 16:20:46","title":"Metamagnetic multiband Hall effect in Ising antiferromagnet ErGa$_2$","abstract":"Frustrated rare-earth-based intermetallics provide a promising platform for emergent magnetotransport properties through exchange coupling between conduction electrons and localized rare-earth magnetic moments. Metamagnetism, the abrupt change of magnetization under an external magnetic field, is a signature of first-order magnetic phase transitions; recently, metamagnetic transitions in frustrated rare earths intermetallics have attracted interest for their accompanying nontrivial spin structures (e.g. skyrmions) and associated non-linear and topological Hall effects. Here, we present metamagnetism-induced Hall anomalies in single-crystalline ErGa$_2$, which recalls features arising from the topological Hall effect but wherein the strong Ising type anisotropy of Er moments prohibit noncoplanar spin structures. We show that the observed anomalies are neither due to anomalous Hall effect nor topological Hall effect, instead, can be accounted for via 4f-5d interactions which produce a band-dependent mobility modulation. This leads to a pronounced multiband Hall response across the magnetization process -- a metamagnetic multiband Hall effect that resembles a topological-Hall-like response but without nontrivial origins. The present findings may be of general relevance in itinerant metamagnetic systems regardless of coplanar/non-coplanar nature of spins and are important for the accurate identification of Hall signals due to emergent magnetic fields.","sentences":["Frustrated rare-earth-based intermetallics provide a promising platform for emergent magnetotransport properties through exchange coupling between conduction electrons and localized rare-earth magnetic moments.","Metamagnetism, the abrupt change of magnetization under an external magnetic field, is a signature of first-order magnetic phase transitions; recently, metamagnetic transitions in frustrated rare earths intermetallics have attracted interest for their accompanying nontrivial spin structures (e.g. skyrmions) and associated non-linear and topological Hall effects.","Here, we present metamagnetism-induced Hall anomalies in single-crystalline ErGa$_2$, which recalls features arising from the topological Hall effect but wherein the strong Ising type anisotropy of Er moments prohibit noncoplanar spin structures.","We show that the observed anomalies are neither due to anomalous Hall effect nor topological Hall effect, instead, can be accounted for via 4f-5d interactions which produce a band-dependent mobility modulation.","This leads to a pronounced multiband Hall response across the magnetization process -- a metamagnetic multiband Hall effect that resembles a topological-Hall-like response but without nontrivial origins.","The present findings may be of general relevance in itinerant metamagnetic systems regardless of coplanar/non-coplanar nature of spins and are important for the accurate identification of Hall signals due to emergent magnetic fields."],"url":"http://arxiv.org/abs/2405.18325v1","category":"cond-mat.str-el"}
{"created":"2024-05-28 16:14:10","title":"SCE-MAE: Selective Correspondence Enhancement with Masked Autoencoder for Self-Supervised Landmark Estimation","abstract":"Self-supervised landmark estimation is a challenging task that demands the formation of locally distinct feature representations to identify sparse facial landmarks in the absence of annotated data. To tackle this task, existing state-of-the-art (SOTA) methods (1) extract coarse features from backbones that are trained with instance-level self-supervised learning (SSL) paradigms, which neglect the dense prediction nature of the task, (2) aggregate them into memory-intensive hypercolumn formations, and (3) supervise lightweight projector networks to naively establish full local correspondences among all pairs of spatial features. In this paper, we introduce SCE-MAE, a framework that (1) leverages the MAE, a region-level SSL method that naturally better suits the landmark prediction task, (2) operates on the vanilla feature map instead of on expensive hypercolumns, and (3) employs a Correspondence Approximation and Refinement Block (CARB) that utilizes a simple density peak clustering algorithm and our proposed Locality-Constrained Repellence Loss to directly hone only select local correspondences. We demonstrate through extensive experiments that SCE-MAE is highly effective and robust, outperforming existing SOTA methods by large margins of approximately 20%-44% on the landmark matching and approximately 9%-15% on the landmark detection tasks.","sentences":["Self-supervised landmark estimation is a challenging task that demands the formation of locally distinct feature representations to identify sparse facial landmarks in the absence of annotated data.","To tackle this task, existing state-of-the-art (SOTA) methods (1) extract coarse features from backbones that are trained with instance-level self-supervised learning (SSL) paradigms, which neglect the dense prediction nature of the task, (2) aggregate them into memory-intensive hypercolumn formations, and (3) supervise lightweight projector networks to naively establish full local correspondences among all pairs of spatial features.","In this paper, we introduce SCE-MAE, a framework that (1) leverages the MAE, a region-level SSL method that naturally better suits the landmark prediction task, (2) operates on the vanilla feature map instead of on expensive hypercolumns, and (3) employs a Correspondence Approximation and Refinement Block (CARB) that utilizes a simple density peak clustering algorithm and our proposed Locality-Constrained Repellence Loss to directly hone only select local correspondences.","We demonstrate through extensive experiments that SCE-MAE is highly effective and robust, outperforming existing SOTA methods by large margins of approximately 20%-44% on the landmark matching and approximately 9%-15% on the landmark detection tasks."],"url":"http://arxiv.org/abs/2405.18322v1","category":"cs.CV"}
{"created":"2024-05-28 16:11:11","title":"Self-Supervised Learning Based Handwriting Verification","abstract":"We present SSL-HV: Self-Supervised Learning approaches applied to the task of Handwriting Verification. This task involves determining whether a given pair of handwritten images originate from the same or different writer distribution. We have compared the performance of multiple generative, contrastive SSL approaches against handcrafted feature extractors and supervised learning on CEDAR AND dataset. We show that ResNet based Variational Auto-Encoder (VAE) outperforms other generative approaches achieving 76.3% accuracy, while ResNet-18 fine-tuned using Variance-Invariance-Covariance Regularization (VICReg) outperforms other contrastive approaches achieving 78% accuracy. Using a pre-trained VAE and VICReg for the downstream task of writer verification we observed a relative improvement in accuracy of 6.7% and 9% over ResNet-18 supervised baseline with 10% writer labels.","sentences":["We present SSL-HV: Self-Supervised Learning approaches applied to the task of Handwriting Verification.","This task involves determining whether a given pair of handwritten images originate from the same or different writer distribution.","We have compared the performance of multiple generative, contrastive SSL approaches against handcrafted feature extractors and supervised learning on CEDAR AND dataset.","We show that ResNet based Variational Auto-Encoder (VAE) outperforms other generative approaches achieving 76.3% accuracy, while ResNet-18 fine-tuned using Variance-Invariance-Covariance Regularization (VICReg) outperforms other contrastive approaches achieving 78% accuracy.","Using a pre-trained VAE and VICReg for the downstream task of writer verification we observed a relative improvement in accuracy of 6.7% and 9% over ResNet-18 supervised baseline with 10% writer labels."],"url":"http://arxiv.org/abs/2405.18320v1","category":"cs.CV"}
{"created":"2024-05-28 16:10:09","title":"Unraveling the where and when of coarse-grained entropy production: General theory meets single-molecule experiments","abstract":"The laws of thermodynamics apply to biophysical systems on the nanoscale as described by the framework of stochastic thermodynamics. This theory provides universal, exact relations for quantities like work, which have been verified in experiments where a fully resolved description allows direct access to such quantities. Complementary studies consider partially hidden, coarse-grained descriptions, in which the mean entropy production typically is not directly accessible but can be bounded in terms of observable quantities. Going beyond the mean, we introduce a fluctuating entropy production that applies to individual trajectories in a coarse-grained description under time-dependent driving. Thus, this concept is applicable to the broad and experimentally significant class of driven systems in which not all relevant states can be resolved. We provide a paradigmatic example by studying an experimentally verified protein unfolding process. As a consequence, the entire distribution of the coarse-grained entropy production rather than merely its mean retains spatial and temporal information about the microscopic process. In particular, we obtain a bound on the distribution of the physical entropy production of individual unfolding events.","sentences":["The laws of thermodynamics apply to biophysical systems on the nanoscale as described by the framework of stochastic thermodynamics.","This theory provides universal, exact relations for quantities like work, which have been verified in experiments where a fully resolved description allows direct access to such quantities.","Complementary studies consider partially hidden, coarse-grained descriptions, in which the mean entropy production typically is not directly accessible but can be bounded in terms of observable quantities.","Going beyond the mean, we introduce a fluctuating entropy production that applies to individual trajectories in a coarse-grained description under time-dependent driving.","Thus, this concept is applicable to the broad and experimentally significant class of driven systems in which not all relevant states can be resolved.","We provide a paradigmatic example by studying an experimentally verified protein unfolding process.","As a consequence, the entire distribution of the coarse-grained entropy production rather than merely its mean retains spatial and temporal information about the microscopic process.","In particular, we obtain a bound on the distribution of the physical entropy production of individual unfolding events."],"url":"http://arxiv.org/abs/2405.18316v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-28 16:07:45","title":"DSDL: Data Set Description Language for Bridging Modalities and Tasks in AI Data","abstract":"In the era of artificial intelligence, the diversity of data modalities and annotation formats often renders data unusable directly, requiring understanding and format conversion before it can be used by researchers or developers with different needs. To tackle this problem, this article introduces a framework called Dataset Description Language (DSDL) that aims to simplify dataset processing by providing a unified standard for AI datasets. DSDL adheres to the three basic practical principles of generic, portable, and extensible, using a unified standard to express data of different modalities and structures, facilitating the dissemination of AI data, and easily extending to new modalities and tasks. The standardized specifications of DSDL reduce the workload for users in data dissemination, processing, and usage. To further improve user convenience, we provide predefined DSDL templates for various tasks, convert mainstream datasets to comply with DSDL specifications, and provide comprehensive documentation and DSDL tools. These efforts aim to simplify the use of AI data, thereby improving the efficiency of AI development.","sentences":["In the era of artificial intelligence, the diversity of data modalities and annotation formats often renders data unusable directly, requiring understanding and format conversion before it can be used by researchers or developers with different needs.","To tackle this problem, this article introduces a framework called Dataset Description Language (DSDL) that aims to simplify dataset processing by providing a unified standard for AI datasets.","DSDL adheres to the three basic practical principles of generic, portable, and extensible, using a unified standard to express data of different modalities and structures, facilitating the dissemination of AI data, and easily extending to new modalities and tasks.","The standardized specifications of DSDL reduce the workload for users in data dissemination, processing, and usage.","To further improve user convenience, we provide predefined DSDL templates for various tasks, convert mainstream datasets to comply with DSDL specifications, and provide comprehensive documentation and DSDL tools.","These efforts aim to simplify the use of AI data, thereby improving the efficiency of AI development."],"url":"http://arxiv.org/abs/2405.18315v1","category":"cs.AI"}
{"created":"2024-05-28 16:06:43","title":"Automorphisms and deformations of regular semisimple Hessenberg varieties","abstract":"We show that regular semisimple Hessenberg varieties can have moduli. To be precise, suppose $X$ is a regular semisimple Hessenberg variety, which is a divisor in the flag variety $G/B$, where $G$ is a simple algebraic group of rank $r$ over $\\mathbb{C}$. Then we show that the space $\\mathrm{H}^1(X,TX)$ of first order deformations of $X$ has dimension $r-1$ except in type $A_2$. (In type $A_2$, the Hessenberg varieties in question are all isomorphic to the permutahedral toric surface. So $\\dim\\mathrm{H}^1(X,TX) = 0$.) We also show that the connected component of the automorphism group of $X$ is the maximal torus of $G$, and we show that $\\mathrm{H}^i(X,TX) = 0$ for $i \\geq 2$.   In type $A$, we can give an even more precise statement determining when two regular semisimple Hessenberg varieties, which are divisors in $G/B$, are isomorphic. We also compute the automorphism groups explicitly in type~$A_{n-1}$ in the terms of stabilizer subgroups of the action of the symmetric group $S_{n}$ on the moduli space $M_{0,n+1}$ of smooth genus $0$ curves with $n + 1$ marked points. Using this, we describe the moduli stack of the regular semisimple Hessenberg varieties $X$ explicitly as a quotient stack of $M_{0,n+1}$.   We prove several analogous results for Hessenberg varieties in generalized flag varieties. In type $A$, these are used in the proofs of the results for $G/B$, but they are also independently interesting because the associated moduli stacks are related directly to the action of $S_n$ on $M_{0,n}$.","sentences":["We show that regular semisimple Hessenberg varieties can have moduli.","To be precise, suppose $X$ is a regular semisimple Hessenberg variety, which is a divisor in the flag variety $G/B$, where $G$ is a simple algebraic group of rank","$r$ over $\\mathbb{C}$. Then we show that the space $\\mathrm{H}^1(X,TX)$ of first order deformations of $X$ has dimension $r-1$ except in type $A_2$. (In type $A_2$, the Hessenberg varieties in question are all isomorphic to the permutahedral toric surface.","So $\\dim\\mathrm{H}^1(X,TX) = 0$.)","We also show that the connected component of the automorphism group of $X$ is the maximal torus of $G$, and we show that $\\mathrm{H}^i(X,TX) = 0$ for $i \\geq 2$.   In type $A$, we can give an even more precise statement determining when two regular semisimple Hessenberg varieties, which are divisors in $G/B$, are isomorphic.","We also compute the automorphism groups explicitly in type~$A_{n-1}$ in the terms of stabilizer subgroups of the action of the symmetric group $S_{n}$ on the moduli space $M_{0,n+1}$ of smooth genus $0$ curves with $n + 1$ marked points.","Using this, we describe the moduli stack of the regular semisimple Hessenberg varieties $X$ explicitly as a quotient stack of $M_{0,n+1}$.   We prove several analogous results for Hessenberg varieties in generalized flag varieties.","In type $A$, these are used in the proofs of the results for $G/B$, but they are also independently interesting because the associated moduli stacks are related directly to the action of $S_n$ on $M_{0,n}$."],"url":"http://arxiv.org/abs/2405.18313v1","category":"math.AG"}
{"created":"2024-05-28 16:02:06","title":"Actions of Drinfeld doubles of finite groups on Artin-Schelter regular algebras","abstract":"For a finite group $G$ and $\\Bbbk$ an algebraically closed field of characteristic zero we consider Artin-Schelter regular algebras $A$ on which the Drinfeld double $D(G)$ acts inner faithfully, and its associated algebras of invariants $A^{D(G)}$. Explicit computations for the cases when $G$ is the (generalized) quaternion group of order 8 and 16 are given.","sentences":["For a finite group $G$ and $\\Bbbk$ an algebraically closed field of characteristic zero we consider Artin-Schelter regular algebras $A$ on which the Drinfeld double $D(G)$ acts inner faithfully, and its associated algebras of invariants $A^{D(G)}$. Explicit computations for the cases when $G$ is the (generalized) quaternion group of order 8 and 16 are given."],"url":"http://arxiv.org/abs/2405.18310v1","category":"math.RA"}
{"created":"2024-05-28 15:59:28","title":"Volt-PF Control Mode for Distribution Feeder Voltage Management Under High Penetration of Distributed Energy Resources","abstract":"Volt-VAr control is a popular method for mitigating overvoltage violations caused by high penetration of distributed energy resources (DERs) in distribution feeders. An inherent limitation of volt-VAr control is that the reactive power (Q) absorbed/injected by the DER is determined based only on the terminal voltage, without considering the active power (P) generated by the DER. This leads to an inequitable burden of Q support, in the sense that those DERs generating lower P, and hence contributing less to overvoltage issues, may be required to provide more than their share of $Q$ support. The resulting PF of these DERs is required to vary over a wide range, which many current DERs do not support. A new control scheme, namely volt-PF control, is proposed here where the Q support is inherently a function of both the voltage and $P$ from DERs, which alleviates the above concerns while limiting the PF variation within a narrow range of 0.9 to 1. The proposed scheme is validated through extensive static and dynamic simulations on a real, large (8000+ nodes) feeder with very high penetration (>200%) of DERs.The implementation of the new scheme in new and existing commercial hardware inverters is described.","sentences":["Volt-VAr control is a popular method for mitigating overvoltage violations caused by high penetration of distributed energy resources (DERs) in distribution feeders.","An inherent limitation of volt-VAr control is that the reactive power (Q) absorbed/injected by the DER is determined based only on the terminal voltage, without considering the active power (P) generated by the DER.","This leads to an inequitable burden of Q support, in the sense that those DERs generating lower P, and hence contributing less to overvoltage issues, may be required to provide more than their share of $Q$ support.","The resulting PF of these DERs is required to vary over a wide range, which many current DERs do not support.","A new control scheme, namely volt-PF control, is proposed here where the Q support is inherently a function of both the voltage and $P$ from DERs, which alleviates the above concerns while limiting the PF variation within a narrow range of 0.9 to 1.","The proposed scheme is validated through extensive static and dynamic simulations on a real, large (8000+ nodes) feeder with very high penetration (>200%) of DERs.","The implementation of the new scheme in new and existing commercial hardware inverters is described."],"url":"http://arxiv.org/abs/2405.18305v1","category":"eess.SY"}
{"created":"2024-05-28 15:58:31","title":"Multi-modal Generation via Cross-Modal In-Context Learning","abstract":"In this work, we study the problem of generating novel images from complex multimodal prompt sequences. While existing methods achieve promising results for text-to-image generation, they often struggle to capture fine-grained details from lengthy prompts and maintain contextual coherence within prompt sequences. Moreover, they often result in misaligned image generation for prompt sequences featuring multiple objects. To address this, we propose a Multi-modal Generation via Cross-Modal In-Context Learning (MGCC) method that generates novel images from complex multimodal prompt sequences by leveraging the combined capabilities of large language models (LLMs) and diffusion models. Our MGCC comprises a novel Cross-Modal Refinement module to explicitly learn cross-modal dependencies between the text and image in the LLM embedding space, and a contextual object grounding module to generate object bounding boxes specifically targeting scenes with multiple objects. Our MGCC demonstrates a diverse range of multimodal capabilities, like novel image generation, the facilitation of multimodal dialogue, and generation of texts. Experimental evaluations on two benchmark datasets, demonstrate the effectiveness of our method. On Visual Story Generation (VIST) dataset with multimodal inputs, our MGCC achieves a CLIP Similarity score of $0.652$ compared to SOTA GILL $0.641$. Similarly, on Visual Dialogue Context (VisDial) having lengthy dialogue sequences, our MGCC achieves an impressive CLIP score of $0.660$, largely outperforming existing SOTA method scoring $0.645$. Code: https://github.com/VIROBO-15/MGCC","sentences":["In this work, we study the problem of generating novel images from complex multimodal prompt sequences.","While existing methods achieve promising results for text-to-image generation, they often struggle to capture fine-grained details from lengthy prompts and maintain contextual coherence within prompt sequences.","Moreover, they often result in misaligned image generation for prompt sequences featuring multiple objects.","To address this, we propose a Multi-modal Generation via Cross-Modal In-Context Learning (MGCC) method that generates novel images from complex multimodal prompt sequences by leveraging the combined capabilities of large language models (LLMs) and diffusion models.","Our MGCC comprises a novel Cross-Modal Refinement module to explicitly learn cross-modal dependencies between the text and image in the LLM embedding space, and a contextual object grounding module to generate object bounding boxes specifically targeting scenes with multiple objects.","Our MGCC demonstrates a diverse range of multimodal capabilities, like novel image generation, the facilitation of multimodal dialogue, and generation of texts.","Experimental evaluations on two benchmark datasets, demonstrate the effectiveness of our method.","On Visual Story Generation (VIST) dataset with multimodal inputs, our MGCC achieves a CLIP Similarity score of $0.652$ compared to SOTA GILL $0.641$. Similarly, on Visual Dialogue Context (VisDial) having lengthy dialogue sequences, our MGCC achieves an impressive CLIP score of $0.660$, largely outperforming existing SOTA method scoring $0.645$. Code: https://github.com/VIROBO-15/MGCC"],"url":"http://arxiv.org/abs/2405.18304v1","category":"cs.CV"}
{"created":"2024-05-28 15:58:19","title":"Nonlinear effect of absorption on the ringdown of a spinning black hole","abstract":"The ringdown gravitational wave signal arising e.g., in the final stage of a black hole binary merger, contains important information about the properties of the remnant, and can potentially be used to perform clean tests of general relativity. However, interpreting the ringdown signal, in particular when it is the loudest, requires understanding the role of nonlinearities and their potential impact on modelling this phase using quasinormal modes. Here, we focus on a particular nonlinear effect arising from the change in the black hole's mass and spin due to the partial absorption of a quasinormal perturbation. We isolate and systematically study this third-order, secular effect by evolving the equations governing linear metric perturbations on the background of a spinning black hole, but allowing the properties of the background to evolve in a prescribed way. We find that this leads to the excitation of quasinormal modes with higher polar angular number, retrograde modes (counter-rotating with respect to the black hole), and overtones, as well as giving rise to a component of the signal at early times that cannot be fully described using quasinormal modes. Quantifying these effects, we find that they may be relevant in analyzing the ringdown in black hole mergers.","sentences":["The ringdown gravitational wave signal arising e.g., in the final stage of a black hole binary merger, contains important information about the properties of the remnant, and can potentially be used to perform clean tests of general relativity.","However, interpreting the ringdown signal, in particular when it is the loudest, requires understanding the role of nonlinearities and their potential impact on modelling this phase using quasinormal modes.","Here, we focus on a particular nonlinear effect arising from the change in the black hole's mass and spin due to the partial absorption of a quasinormal perturbation.","We isolate and systematically study this third-order, secular effect by evolving the equations governing linear metric perturbations on the background of a spinning black hole, but allowing the properties of the background to evolve in a prescribed way.","We find that this leads to the excitation of quasinormal modes with higher polar angular number, retrograde modes (counter-rotating with respect to the black hole), and overtones, as well as giving rise to a component of the signal at early times that cannot be fully described using quasinormal modes.","Quantifying these effects, we find that they may be relevant in analyzing the ringdown in black hole mergers."],"url":"http://arxiv.org/abs/2405.18303v1","category":"gr-qc"}
{"created":"2024-05-28 15:53:02","title":"CompetEvo: Towards Morphological Evolution from Competition","abstract":"Training an agent to adapt to specific tasks through co-optimization of morphology and control has widely attracted attention. However, whether there exists an optimal configuration and tactics for agents in a multiagent competition scenario is still an issue that is challenging to definitively conclude. In this context, we propose competitive evolution (CompetEvo), which co-evolves agents' designs and tactics in confrontation. We build arenas consisting of three animals and their evolved derivatives, placing agents with different morphologies in direct competition with each other. The results reveal that our method enables agents to evolve a more suitable design and strategy for fighting compared to fixed-morph agents, allowing them to obtain advantages in combat scenarios. Moreover, we demonstrate the amazing and impressive behaviors that emerge when confrontations are conducted under asymmetrical morphs.","sentences":["Training an agent to adapt to specific tasks through co-optimization of morphology and control has widely attracted attention.","However, whether there exists an optimal configuration and tactics for agents in a multiagent competition scenario is still an issue that is challenging to definitively conclude.","In this context, we propose competitive evolution (CompetEvo), which co-evolves agents' designs and tactics in confrontation.","We build arenas consisting of three animals and their evolved derivatives, placing agents with different morphologies in direct competition with each other.","The results reveal that our method enables agents to evolve a more suitable design and strategy for fighting compared to fixed-morph agents, allowing them to obtain advantages in combat scenarios.","Moreover, we demonstrate the amazing and impressive behaviors that emerge when confrontations are conducted under asymmetrical morphs."],"url":"http://arxiv.org/abs/2405.18300v1","category":"cs.AI"}
{"created":"2024-05-28 15:51:18","title":"Deep Learning Innovations for Underwater Waste Detection: An In-Depth Analysis","abstract":"Addressing the issue of submerged underwater trash is crucial for safeguarding aquatic ecosystems and preserving marine life. While identifying debris present on the surface of water bodies is straightforward, assessing the underwater submerged waste is a challenge due to the image distortions caused by factors such as light refraction, absorption, suspended particles, color shifts, and occlusion. This paper conducts a comprehensive review of state-of-the-art architectures and on the existing datasets to establish a baseline for submerged waste and trash detection. The primary goal remains to establish the benchmark of the object localization techniques to be leveraged by advanced underwater sensors and autonomous underwater vehicles. The ultimate objective is to explore the underwater environment, to identify, and remove underwater debris. The absence of benchmarks (dataset or algorithm) in many researches emphasizes the need for a more robust algorithmic solution. Through this research, we aim to give performance comparative analysis of various underwater trash detection algorithms.","sentences":["Addressing the issue of submerged underwater trash is crucial for safeguarding aquatic ecosystems and preserving marine life.","While identifying debris present on the surface of water bodies is straightforward, assessing the underwater submerged waste is a challenge due to the image distortions caused by factors such as light refraction, absorption, suspended particles, color shifts, and occlusion.","This paper conducts a comprehensive review of state-of-the-art architectures and on the existing datasets to establish a baseline for submerged waste and trash detection.","The primary goal remains to establish the benchmark of the object localization techniques to be leveraged by advanced underwater sensors and autonomous underwater vehicles.","The ultimate objective is to explore the underwater environment, to identify, and remove underwater debris.","The absence of benchmarks (dataset or algorithm) in many researches emphasizes the need for a more robust algorithmic solution.","Through this research, we aim to give performance comparative analysis of various underwater trash detection algorithms."],"url":"http://arxiv.org/abs/2405.18299v1","category":"cs.CV"}
{"created":"2024-05-28 15:50:50","title":"Context-Specific Refinements of Bayesian Network Classifiers","abstract":"Supervised classification is one of the most ubiquitous tasks in machine learning. Generative classifiers based on Bayesian networks are often used because of their interpretability and competitive accuracy. The widely used naive and TAN classifiers are specific instances of Bayesian network classifiers with a constrained underlying graph. This paper introduces novel classes of generative classifiers extending TAN and other famous types of Bayesian network classifiers. Our approach is based on staged tree models, which extend Bayesian networks by allowing for complex, context-specific patterns of dependence. We formally study the relationship between our novel classes of classifiers and Bayesian networks. We introduce and implement data-driven learning routines for our models and investigate their accuracy in an extensive computational study. The study demonstrates that models embedding asymmetric information can enhance classification accuracy.","sentences":["Supervised classification is one of the most ubiquitous tasks in machine learning.","Generative classifiers based on Bayesian networks are often used because of their interpretability and competitive accuracy.","The widely used naive and TAN classifiers are specific instances of Bayesian network classifiers with a constrained underlying graph.","This paper introduces novel classes of generative classifiers extending TAN and other famous types of Bayesian network classifiers.","Our approach is based on staged tree models, which extend Bayesian networks by allowing for complex, context-specific patterns of dependence.","We formally study the relationship between our novel classes of classifiers and Bayesian networks.","We introduce and implement data-driven learning routines for our models and investigate their accuracy in an extensive computational study.","The study demonstrates that models embedding asymmetric information can enhance classification accuracy."],"url":"http://arxiv.org/abs/2405.18298v1","category":"stat.ML"}
{"created":"2024-05-28 15:50:29","title":"Artificial Intelligence Satellite Telecommunication Testbed using Commercial Off-The-Shelf Chipsets","abstract":"The Artificial Intelligence Satellite Telecommunications Testbed (AISTT), part of the ESA project SPAICE, is focused on the transformation of the satellite payload by using artificial intelligence (AI) and machine learning (ML) methodologies over available commercial off-the-shelf (COTS) AI chips for on-board processing. The objectives include validating artificial intelligence-driven SATCOM scenarios such as interference detection, spectrum sharing, radio resource management, decoding, and beamforming. The study highlights hardware selection and payload architecture. Preliminary results show that ML models significantly improve signal quality, spectral efficiency, and throughput compared to conventional payload. Moreover, the testbed aims to evaluate the performance and application of AI-capable COTS chips in onboard SATCOM contexts.","sentences":["The Artificial Intelligence Satellite Telecommunications Testbed (AISTT), part of the ESA project SPAICE, is focused on the transformation of the satellite payload by using artificial intelligence (AI) and machine learning (ML) methodologies over available commercial off-the-shelf (COTS)","AI chips for on-board processing.","The objectives include validating artificial intelligence-driven SATCOM scenarios such as interference detection, spectrum sharing, radio resource management, decoding, and beamforming.","The study highlights hardware selection and payload architecture.","Preliminary results show that ML models significantly improve signal quality, spectral efficiency, and throughput compared to conventional payload.","Moreover, the testbed aims to evaluate the performance and application of AI-capable COTS chips in onboard SATCOM contexts."],"url":"http://arxiv.org/abs/2405.18297v1","category":"eess.SP"}
{"created":"2024-05-28 15:50:10","title":"Bias in Motion: Theoretical Insights into the Dynamics of Bias in SGD Training","abstract":"Machine learning systems often acquire biases by leveraging undesired features in the data, impacting accuracy variably across different sub-populations. Current understanding of bias formation mostly focuses on the initial and final stages of learning, leaving a gap in knowledge regarding the transient dynamics. To address this gap, this paper explores the evolution of bias in a teacher-student setup modeling different data sub-populations with a Gaussian-mixture model. We provide an analytical description of the stochastic gradient descent dynamics of a linear classifier in this setting, which we prove to be exact in high dimension. Notably, our analysis reveals how different properties of sub-populations influence bias at different timescales, showing a shifting preference of the classifier during training. Applying our findings to fairness and robustness, we delineate how and when heterogeneous data and spurious features can generate and amplify bias. We empirically validate our results in more complex scenarios by training deeper networks on synthetic and real datasets, including CIFAR10, MNIST, and CelebA.","sentences":["Machine learning systems often acquire biases by leveraging undesired features in the data, impacting accuracy variably across different sub-populations.","Current understanding of bias formation mostly focuses on the initial and final stages of learning, leaving a gap in knowledge regarding the transient dynamics.","To address this gap, this paper explores the evolution of bias in a teacher-student setup modeling different data sub-populations with a Gaussian-mixture model.","We provide an analytical description of the stochastic gradient descent dynamics of a linear classifier in this setting, which we prove to be exact in high dimension.","Notably, our analysis reveals how different properties of sub-populations influence bias at different timescales, showing a shifting preference of the classifier during training.","Applying our findings to fairness and robustness, we delineate how and when heterogeneous data and spurious features can generate and amplify bias.","We empirically validate our results in more complex scenarios by training deeper networks on synthetic and real datasets, including CIFAR10, MNIST, and CelebA."],"url":"http://arxiv.org/abs/2405.18296v1","category":"cs.LG"}
{"created":"2024-05-28 15:43:29","title":"FedSAC: Dynamic Submodel Allocation for Collaborative Fairness in Federated Learning","abstract":"Collaborative fairness stands as an essential element in federated learning to encourage client participation by equitably distributing rewards based on individual contributions. Existing methods primarily focus on adjusting gradient allocations among clients to achieve collaborative fairness. However, they frequently overlook crucial factors such as maintaining consistency across local models and catering to the diverse requirements of high-contributing clients. This oversight inevitably decreases both fairness and model accuracy in practice. To address these issues, we propose FedSAC, a novel Federated learning framework with dynamic Submodel Allocation for Collaborative fairness, backed by a theoretical convergence guarantee. First, we present the concept of \"bounded collaborative fairness (BCF)\", which ensures fairness by tailoring rewards to individual clients based on their contributions. Second, to implement the BCF, we design a submodel allocation module with a theoretical guarantee of fairness. This module incentivizes high-contributing clients with high-performance submodels containing a diverse range of crucial neurons, thereby preserving consistency across local models. Third, we further develop a dynamic aggregation module to adaptively aggregate submodels, ensuring the equitable treatment of low-frequency neurons and consequently enhancing overall model accuracy. Extensive experiments conducted on three public benchmarks demonstrate that FedSAC outperforms all baseline methods in both fairness and model accuracy. We see this work as a significant step towards incentivizing broader client participation in federated learning. The source code is available at https://github.com/wangzihuixmu/FedSAC.","sentences":["Collaborative fairness stands as an essential element in federated learning to encourage client participation by equitably distributing rewards based on individual contributions.","Existing methods primarily focus on adjusting gradient allocations among clients to achieve collaborative fairness.","However, they frequently overlook crucial factors such as maintaining consistency across local models and catering to the diverse requirements of high-contributing clients.","This oversight inevitably decreases both fairness and model accuracy in practice.","To address these issues, we propose FedSAC, a novel Federated learning framework with dynamic Submodel Allocation for Collaborative fairness, backed by a theoretical convergence guarantee.","First, we present the concept of \"bounded collaborative fairness (BCF)\", which ensures fairness by tailoring rewards to individual clients based on their contributions.","Second, to implement the BCF, we design a submodel allocation module with a theoretical guarantee of fairness.","This module incentivizes high-contributing clients with high-performance submodels containing a diverse range of crucial neurons, thereby preserving consistency across local models.","Third, we further develop a dynamic aggregation module to adaptively aggregate submodels, ensuring the equitable treatment of low-frequency neurons and consequently enhancing overall model accuracy.","Extensive experiments conducted on three public benchmarks demonstrate that FedSAC outperforms all baseline methods in both fairness and model accuracy.","We see this work as a significant step towards incentivizing broader client participation in federated learning.","The source code is available at https://github.com/wangzihuixmu/FedSAC."],"url":"http://arxiv.org/abs/2405.18291v1","category":"cs.LG"}
{"created":"2024-05-28 15:43:05","title":"Probabilistic and progressive deblended far-infrared and sub-millimetre point source catalogues I. Methodology and first application in the COSMOS field","abstract":"Single-dish far-infrared (far-IR) and sub-millimetre (sub-mm) point source catalogues and their connections with catalogues at other wavelengths are of paramount importance. However, due to the large mismatch in spatial resolution, cross-matching galaxies at different wavelengths is challenging. This work aims to develop the next-generation deblended far-IR and sub-mm catalogues and present the first application in the COSMOS field. Our progressive deblending used the Bayesian probabilistic framework known as XID+. The deblending started from the Spitzer/MIPS 24 micron data, using an initial prior list composed of sources selected from the COSMOS2020 catalogue and radio catalogues from the VLA and the MeerKAT surveys, based on spectral energy distribution modelling which predicts fluxes of the known sources at the deblending wavelength. To speed up flux prediction, we made use of a neural network-based emulator. After deblending the 24 micron data, we proceeded to the Herschel PACS (100 & 160 micron) and SPIRE wavebands (250, 350 & 500 micron). Each time we constructed a tailor-made prior list based on the predicted fluxes of the known sources. Using simulated far-IR and sub-mm sky, we detailed the performance of our deblending pipeline. After validation with simulations, we then deblended the real observations from 24 to 500 micron and compared with blindly extracted catalogues and previous versions of deblended catalogues. As an additional test, we deblended the SCUBA-2 850 micron map and compared our deblended fluxes with ALMA measurements, which demonstrates a higher level of flux accuracy compared to previous results.We publicly release our XID+ deblended point source catalogues. These deblended long-wavelength data are crucial for studies such as deriving the fraction of dust-obscured star formation and better separation of quiescent galaxies from dusty star-forming galaxies.","sentences":["Single-dish far-infrared (far-IR) and sub-millimetre (sub-mm) point source catalogues and their connections with catalogues at other wavelengths are of paramount importance.","However, due to the large mismatch in spatial resolution, cross-matching galaxies at different wavelengths is challenging.","This work aims to develop the next-generation deblended far-IR and sub-mm catalogues and present the first application in the COSMOS field.","Our progressive deblending used the Bayesian probabilistic framework known as XID+.","The deblending started from the Spitzer/MIPS 24 micron data, using an initial prior list composed of sources selected from the COSMOS2020 catalogue and radio catalogues from the VLA and the MeerKAT surveys, based on spectral energy distribution modelling which predicts fluxes of the known sources at the deblending wavelength.","To speed up flux prediction, we made use of a neural network-based emulator.","After deblending the 24 micron data, we proceeded to the Herschel PACS (100 & 160 micron) and SPIRE wavebands (250, 350 & 500 micron).","Each time we constructed a tailor-made prior list based on the predicted fluxes of the known sources.","Using simulated far-IR and sub-mm sky, we detailed the performance of our deblending pipeline.","After validation with simulations, we then deblended the real observations from 24 to 500 micron and compared with blindly extracted catalogues and previous versions of deblended catalogues.","As an additional test, we deblended the SCUBA-2 850 micron map and compared our deblended fluxes with ALMA measurements, which demonstrates a higher level of flux accuracy compared to previous results.","We publicly release our XID+ deblended point source catalogues.","These deblended long-wavelength data are crucial for studies such as deriving the fraction of dust-obscured star formation and better separation of quiescent galaxies from dusty star-forming galaxies."],"url":"http://arxiv.org/abs/2405.18290v1","category":"astro-ph.GA"}
{"created":"2024-05-28 15:42:45","title":"Highway Reinforcement Learning","abstract":"Learning from multi-step off-policy data collected by a set of policies is a core problem of reinforcement learning (RL). Approaches based on importance sampling (IS) often suffer from large variances due to products of IS ratios. Typical IS-free methods, such as $n$-step Q-learning, look ahead for $n$ time steps along the trajectory of actions (where $n$ is called the lookahead depth) and utilize off-policy data directly without any additional adjustment. They work well for proper choices of $n$. We show, however, that such IS-free methods underestimate the optimal value function (VF), especially for large $n$, restricting their capacity to efficiently utilize information from distant future time steps. To overcome this problem, we introduce a novel, IS-free, multi-step off-policy method that avoids the underestimation issue and converges to the optimal VF. At its core lies a simple but non-trivial \\emph{highway gate}, which controls the information flow from the distant future by comparing it to a threshold. The highway gate guarantees convergence to the optimal VF for arbitrary $n$ and arbitrary behavioral policies. It gives rise to a novel family of off-policy RL algorithms that safely learn even when $n$ is very large, facilitating rapid credit assignment from the far future to the past. On tasks with greatly delayed rewards, including video games where the reward is given only at the end of the game, our new methods outperform many existing multi-step off-policy algorithms.","sentences":["Learning from multi-step off-policy data collected by a set of policies is a core problem of reinforcement learning (RL).","Approaches based on importance sampling (IS) often suffer from large variances due to products of IS ratios.","Typical IS-free methods, such as $n$-step Q-learning, look ahead for $n$ time steps along the trajectory of actions (where $n$ is called the lookahead depth) and utilize off-policy data directly without any additional adjustment.","They work well for proper choices of $n$. We show, however, that such IS-free methods underestimate the optimal value function (VF), especially for large $n$, restricting their capacity to efficiently utilize information from distant future time steps.","To overcome this problem, we introduce a novel, IS-free, multi-step off-policy method that avoids the underestimation issue and converges to the optimal VF.","At its core lies a simple but non-trivial \\emph{highway gate}, which controls the information flow from the distant future by comparing it to a threshold.","The highway gate guarantees convergence to the optimal VF for arbitrary $n$ and arbitrary behavioral policies.","It gives rise to a novel family of off-policy RL algorithms that safely learn even when $n$ is very large, facilitating rapid credit assignment from the far future to the past.","On tasks with greatly delayed rewards, including video games where the reward is given only at the end of the game, our new methods outperform many existing multi-step off-policy algorithms."],"url":"http://arxiv.org/abs/2405.18289v1","category":"cs.LG"}
{"created":"2024-05-28 15:36:48","title":"Adaptive debiased SGD in high-dimensional GLMs with steaming data","abstract":"Online statistical inference facilitates real-time analysis of sequentially collected data, making it different from traditional methods that rely on static datasets. This paper introduces a novel approach to online inference in high-dimensional generalized linear models, where we update regression coefficient estimates and their standard errors upon each new data arrival. In contrast to existing methods that either require full dataset access or large-dimensional summary statistics storage, our method operates in a single-pass mode, significantly reducing both time and space complexity. The core of our methodological innovation lies in an adaptive stochastic gradient descent algorithm tailored for dynamic objective functions, coupled with a novel online debiasing procedure. This allows us to maintain low-dimensional summary statistics while effectively controlling optimization errors introduced by the dynamically changing loss functions. We demonstrate that our method, termed the Approximated Debiased Lasso (ADL), not only mitigates the need for the bounded individual probability condition but also significantly improves numerical performance. Numerical experiments demonstrate that the proposed ADL method consistently exhibits robust performance across various covariance matrix structures.","sentences":["Online statistical inference facilitates real-time analysis of sequentially collected data, making it different from traditional methods that rely on static datasets.","This paper introduces a novel approach to online inference in high-dimensional generalized linear models, where we update regression coefficient estimates and their standard errors upon each new data arrival.","In contrast to existing methods that either require full dataset access or large-dimensional summary statistics storage, our method operates in a single-pass mode, significantly reducing both time and space complexity.","The core of our methodological innovation lies in an adaptive stochastic gradient descent algorithm tailored for dynamic objective functions, coupled with a novel online debiasing procedure.","This allows us to maintain low-dimensional summary statistics while effectively controlling optimization errors introduced by the dynamically changing loss functions.","We demonstrate that our method, termed the Approximated Debiased Lasso (ADL), not only mitigates the need for the bounded individual probability condition but also significantly improves numerical performance.","Numerical experiments demonstrate that the proposed ADL method consistently exhibits robust performance across various covariance matrix structures."],"url":"http://arxiv.org/abs/2405.18284v1","category":"stat.ML"}
{"created":"2024-05-28 15:35:02","title":"A Topological Approach to Simple Descriptions of Convex Hulls of Sets Defined by Three Quadrics","abstract":"We study the convex hull of a set $S\\subset \\mathbb{R}^n$ defined by three quadratic inequalities. A simple way of generating inequalities valid on $S$ is to take nonnegative linear combinations of the defining inequalities of $S$. We call such inequalities aggregations. We introduce a new technique relating aggregations to properties of the spectral curve, i.e. the curve defined by the vanishing of the determinant polynomial, and utilizing known spectral sequences (Agrachev and Lerario, 2012). We find new families beyond those identified in (Dey, Mu\\~noz, and Serrano, 2022; Blekherman, Dey, and Sun, 2024), where the convex hull is defined by aggregations. We also prove a characterization of the emptiness of the projective variety defined by $3$ homogeneous quadratics in terms of the spectral curve generalizing results of (Agrachev, 1988).","sentences":["We study the convex hull of a set $S\\subset \\mathbb{R}^n$ defined by three quadratic inequalities.","A simple way of generating inequalities valid on $S$ is to take nonnegative linear combinations of the defining inequalities of $S$. We call such inequalities aggregations.","We introduce a new technique relating aggregations to properties of the spectral curve, i.e. the curve defined by the vanishing of the determinant polynomial, and utilizing known spectral sequences (Agrachev and Lerario, 2012).","We find new families beyond those identified in (Dey, Mu\\~noz, and Serrano, 2022; Blekherman, Dey, and Sun, 2024), where the convex hull is defined by aggregations.","We also prove a characterization of the emptiness of the projective variety defined by $3$ homogeneous quadratics in terms of the spectral curve generalizing results of (Agrachev, 1988)."],"url":"http://arxiv.org/abs/2405.18282v1","category":"math.AG"}
{"created":"2024-05-28 15:34:33","title":"MODL: Multilearner Online Deep Learning","abstract":"Online deep learning solves the problem of learning from streams of data, reconciling two opposing objectives: learn fast and learn deep. Existing work focuses almost exclusively on exploring pure deep learning solutions, which are much better suited to handle the \"deep\" than the \"fast\" part of the online learning equation. In our work, we propose a different paradigm, based on a hybrid multilearner approach. First, we develop a fast online logistic regression learner. This learner does not rely on backpropagation. Instead, it uses closed form recursive updates of model parameters, handling the fast learning part of the online learning problem. We then analyze the existing online deep learning theory and show that the widespread ODL approach, currently operating at complexity $O(L^2)$ in terms of the number of layers $L$, can be equivalently implemented in $O(L)$ complexity. This further leads us to the cascaded multilearner design, in which multiple shallow and deep learners are co-trained to solve the online learning problem in a cooperative, synergistic fashion. We show that this approach achieves state-of-the-art results on common online learning datasets, while also being able to handle missing features gracefully. Our code is publicly available at https://github.com/AntonValk/MODL.","sentences":["Online deep learning solves the problem of learning from streams of data, reconciling two opposing objectives: learn fast and learn deep.","Existing work focuses almost exclusively on exploring pure deep learning solutions, which are much better suited to handle the \"deep\" than the \"fast\" part of the online learning equation.","In our work, we propose a different paradigm, based on a hybrid multilearner approach.","First, we develop a fast online logistic regression learner.","This learner does not rely on backpropagation.","Instead, it uses closed form recursive updates of model parameters, handling the fast learning part of the online learning problem.","We then analyze the existing online deep learning theory and show that the widespread ODL approach, currently operating at complexity $O(L^2)$ in terms of the number of layers $L$, can be equivalently implemented in $O(L)$ complexity.","This further leads us to the cascaded multilearner design, in which multiple shallow and deep learners are co-trained to solve the online learning problem in a cooperative, synergistic fashion.","We show that this approach achieves state-of-the-art results on common online learning datasets, while also being able to handle missing features gracefully.","Our code is publicly available at https://github.com/AntonValk/MODL."],"url":"http://arxiv.org/abs/2405.18281v1","category":"cs.LG"}
{"created":"2024-05-28 15:26:38","title":"Signatures of gravitational wave memory in the radiative process of entangled quantum probes","abstract":"In this article, we examine entangled quantum probes in geodesic trajectories in a background with gravitational wave (GW) burst. In particular, these quantum probes are prepared initially either in the symmetric or anti-symmetric Bell's states and we study the radiative process as the GW burst passes. Our considered GW burst backgrounds have the profiles of -- Heaviside-theta, $tanh$, Gaussian, and $sech$-squared functions respectively. The first two burst profiles have an asymmetric nature and thus result in non-zero gravitational wave memory. Whereas, for the last two symmetric profiles there is no asymptotic memory. For eternal switching, our observations suggest that the collective transition rate for the entangled probes due to symmetric GW bursts remains the same as obtained in the flat space. Whereas, for asymmetric bursts with memory, there is a finite change, indicating a direct possibility to distinguish between the two above-mentioned scenarios. We also consider finite switching in terms of Gaussian functions and observe characteristic differences in the radiative process between the GW backgrounds with and without memory. Notably, if the Gaussian switching is peaked much later compared to the passing of GW, only memory profiles contribute to the radiative process. We further discuss the physical implications of our findings.","sentences":["In this article, we examine entangled quantum probes in geodesic trajectories in a background with gravitational wave (GW) burst.","In particular, these quantum probes are prepared initially either in the symmetric or anti-symmetric Bell's states and we study the radiative process as the GW burst passes.","Our considered GW burst backgrounds have the profiles of -- Heaviside-theta, $tanh$, Gaussian, and $sech$-squared functions respectively.","The first two burst profiles have an asymmetric nature and thus result in non-zero gravitational wave memory.","Whereas, for the last two symmetric profiles there is no asymptotic memory.","For eternal switching, our observations suggest that the collective transition rate for the entangled probes due to symmetric GW bursts remains the same as obtained in the flat space.","Whereas, for asymmetric bursts with memory, there is a finite change, indicating a direct possibility to distinguish between the two above-mentioned scenarios.","We also consider finite switching in terms of Gaussian functions and observe characteristic differences in the radiative process between the GW backgrounds with and without memory.","Notably, if the Gaussian switching is peaked much later compared to the passing of GW, only memory profiles contribute to the radiative process.","We further discuss the physical implications of our findings."],"url":"http://arxiv.org/abs/2405.18277v1","category":"gr-qc"}
{"created":"2024-05-28 15:23:46","title":"Metaheuristics and Large Language Models Join Forces: Towards an Integrated Optimization Approach","abstract":"Since the rise of Large Language Models (LLMs) a couple of years ago, researchers in metaheuristics (MHs) have wondered how to use their power in a beneficial way within their algorithms. This paper introduces a novel approach that leverages LLMs as pattern recognition tools to improve MHs. The resulting hybrid method, tested in the context of a social network-based combinatorial optimization problem, outperforms existing state-of-the-art approaches that combine machine learning with MHs regarding the obtained solution quality. By carefully designing prompts, we demonstrate that the output obtained from LLMs can be used as problem knowledge, leading to improved results. Lastly, we acknowledge LLMs' potential drawbacks and limitations and consider it essential to examine them to advance this type of research further.","sentences":["Since the rise of Large Language Models (LLMs) a couple of years ago, researchers in metaheuristics (MHs) have wondered how to use their power in a beneficial way within their algorithms.","This paper introduces a novel approach that leverages LLMs as pattern recognition tools to improve MHs.","The resulting hybrid method, tested in the context of a social network-based combinatorial optimization problem, outperforms existing state-of-the-art approaches that combine machine learning with MHs regarding the obtained solution quality.","By carefully designing prompts, we demonstrate that the output obtained from LLMs can be used as problem knowledge, leading to improved results.","Lastly, we acknowledge LLMs' potential drawbacks and limitations and consider it essential to examine them to advance this type of research further."],"url":"http://arxiv.org/abs/2405.18272v1","category":"cs.AI"}
{"created":"2024-05-28 15:18:27","title":"Motivic realization of rigid G-local systems on curves and tamely ramified geometric Langlands","abstract":"For a reductive group $G$, we prove that complex irreducible rigid $G$-local systems with quasi-unipotent monodromies and finite order abelianization on a smooth curve are motivic, generalizing a theorem of Katz for $GL_n$. We do so by showing that the Hecke eigensheaf corresponding to such a local system is itself motivic. Unlike other works in the subject, we work entirely over the complex numbers. In the setting of de Rham geometric Langlands, we prove the existence of Hecke eigensheaves associated to any irreducible $G$-local system with regular singularities. We also provide a spectral decomposition of a naturally defined automorphic category over the stack of regular singular local systems with prescribed eigenvalues of the local monodromies at infinity. Finally, we establish a relationship between rigidity for complex local systems and automorphic rigidity, answering a conjecture of Yun in the tame complex setting.","sentences":["For a reductive group $G$, we prove that complex irreducible rigid $G$-local systems with quasi-unipotent monodromies and finite order abelianization on a smooth curve are motivic, generalizing a theorem of Katz for $GL_n$. We do so by showing that the Hecke eigensheaf corresponding to such a local system is itself motivic.","Unlike other works in the subject, we work entirely over the complex numbers.","In the setting of de Rham geometric Langlands, we prove the existence of Hecke eigensheaves associated to any irreducible $G$-local system with regular singularities.","We also provide a spectral decomposition of a naturally defined automorphic category over the stack of regular singular local systems with prescribed eigenvalues of the local monodromies at infinity.","Finally, we establish a relationship between rigidity for complex local systems and automorphic rigidity, answering a conjecture of Yun in the tame complex setting."],"url":"http://arxiv.org/abs/2405.18268v1","category":"math.AG"}
{"created":"2024-05-28 15:13:29","title":"A Vlogger-augmented Graph Neural Network Model for Micro-video Recommendation","abstract":"Existing micro-video recommendation models exploit the interactions between users and micro-videos and/or multi-modal information of micro-videos to predict the next micro-video a user will watch, ignoring the information related to vloggers, i.e., the producers of micro-videos. However, in micro-video scenarios, vloggers play a significant role in user-video interactions, since vloggers generally focus on specific topics and users tend to follow the vloggers they are interested in. Therefore, in the paper, we propose a vlogger-augmented graph neural network model VA-GNN, which takes the effect of vloggers into consideration. Specifically, we construct a tripartite graph with users, micro-videos, and vloggers as nodes, capturing user preferences from different views, i.e., the video-view and the vlogger-view. Moreover, we conduct cross-view contrastive learning to keep the consistency between node embeddings from the two different views. Besides, when predicting the next user-video interaction, we adaptively combine the user preferences for a video itself and its vlogger. We conduct extensive experiments on two real-world datasets. The experimental results show that VA-GNN outperforms multiple existing GNN-based recommendation models.","sentences":["Existing micro-video recommendation models exploit the interactions between users and micro-videos and/or multi-modal information of micro-videos to predict the next micro-video a user will watch, ignoring the information related to vloggers, i.e., the producers of micro-videos.","However, in micro-video scenarios, vloggers play a significant role in user-video interactions, since vloggers generally focus on specific topics and users tend to follow the vloggers they are interested in.","Therefore, in the paper, we propose a vlogger-augmented graph neural network model VA-GNN, which takes the effect of vloggers into consideration.","Specifically, we construct a tripartite graph with users, micro-videos, and vloggers as nodes, capturing user preferences from different views, i.e., the video-view and the vlogger-view.","Moreover, we conduct cross-view contrastive learning to keep the consistency between node embeddings from the two different views.","Besides, when predicting the next user-video interaction, we adaptively combine the user preferences for a video itself and its vlogger.","We conduct extensive experiments on two real-world datasets.","The experimental results show that VA-GNN outperforms multiple existing GNN-based recommendation models."],"url":"http://arxiv.org/abs/2405.18260v1","category":"cs.IR"}
{"created":"2024-05-28 15:13:19","title":"Ranking with Ties based on Noisy Performance Data","abstract":"We consider the problem of ranking a set of objects based on their performance when the measurement of said performance is subject to noise. In this scenario, the performance is measured repeatedly, resulting in a range of measurements for each object. If the ranges of two objects do not overlap, then we consider one object as 'better' than the other, and we expect it to receive a higher rank; if, however, the ranges overlap, then the objects are incomparable, and we wish them to be assigned the same rank. Unfortunately, the incomparability relation of ranges is in general not transitive; as a consequence, in general the two requirements cannot be satisfied simultaneously, i.e., it is not possible to guarantee both distinct ranks for objects with separated ranges, and same rank for objects with overlapping ranges. This conflict leads to more than one reasonable way to rank a set of objects. In this paper, we explore the ambiguities that arise when ranking with ties, and define a set of reasonable rankings, which we call partial rankings. We develop and analyse three different methodologies to compute a partial ranking. Finally, we show how performance differences among objects can be investigated with the help of partial ranking.","sentences":["We consider the problem of ranking a set of objects based on their performance when the measurement of said performance is subject to noise.","In this scenario, the performance is measured repeatedly, resulting in a range of measurements for each object.","If the ranges of two objects do not overlap, then we consider one object as 'better' than the other, and we expect it to receive a higher rank; if, however, the ranges overlap, then the objects are incomparable, and we wish them to be assigned the same rank.","Unfortunately, the incomparability relation of ranges is in general not transitive; as a consequence, in general the two requirements cannot be satisfied simultaneously, i.e., it is not possible to guarantee both distinct ranks for objects with separated ranges, and same rank for objects with overlapping ranges.","This conflict leads to more than one reasonable way to rank a set of objects.","In this paper, we explore the ambiguities that arise when ranking with ties, and define a set of reasonable rankings, which we call partial rankings.","We develop and analyse three different methodologies to compute a partial ranking.","Finally, we show how performance differences among objects can be investigated with the help of partial ranking."],"url":"http://arxiv.org/abs/2405.18259v1","category":"cs.PF"}
{"created":"2024-05-28 15:11:17","title":"Text-only Synthesis for Image Captioning","abstract":"From paired image-text training to text-only training for image captioning, the pursuit of relaxing the requirements for high-cost and large-scale annotation of good quality data remains consistent. In this paper, we propose Text-only Synthesis for Image Captioning (ToCa), which further advances this relaxation with fewer human labor and less computing time. Specifically, we deconstruct caption text into structures and lexical words, which serve as the fundamental components of the caption. By combining different structures and lexical words as inputs to the large language model, massive captions that contain various patterns of lexical words are generated. This method not only approaches the target domain but also surpasses it by generating new captions, thereby enhancing the zero-shot generalization ability of the model. Considering the different levels of data access in the real world, we define three synthesis scenarios: cross-domain synthesis, in-domain synthesis, and data-efficient synthesis. Experiments in these scenarios demonstrate the generalizability, transferability and practicability of ToCa with a nearly 5 CIDEr improvement for zero-shot cross-domain captioning and a maximum increase of over 20 CIDEr for data-efficient captioning.","sentences":["From paired image-text training to text-only training for image captioning, the pursuit of relaxing the requirements for high-cost and large-scale annotation of good quality data remains consistent.","In this paper, we propose Text-only Synthesis for Image Captioning (ToCa), which further advances this relaxation with fewer human labor and less computing time.","Specifically, we deconstruct caption text into structures and lexical words, which serve as the fundamental components of the caption.","By combining different structures and lexical words as inputs to the large language model, massive captions that contain various patterns of lexical words are generated.","This method not only approaches the target domain but also surpasses it by generating new captions, thereby enhancing the zero-shot generalization ability of the model.","Considering the different levels of data access in the real world, we define three synthesis scenarios: cross-domain synthesis, in-domain synthesis, and data-efficient synthesis.","Experiments in these scenarios demonstrate the generalizability, transferability and practicability of ToCa with a nearly 5 CIDEr improvement for zero-shot cross-domain captioning and a maximum increase of over 20 CIDEr for data-efficient captioning."],"url":"http://arxiv.org/abs/2405.18258v1","category":"cs.CV"}
{"created":"2024-05-28 15:09:20","title":"SubDLe: identification of substructures in cosmological simulations with deep learning","abstract":"The identification of substructures within halos in cosmological hydrodynamical simulations is a fundamental step to identify the simulated counterparts of real objects, namely galaxies. For this reason, substructure finders play a crucial role in extracting relevant information from the simulation outputs. They are based on physically-motivated definitions of substructures, performing multiple steps of particle-by-particle operations, thus computationally expensive. The purpose of this work is to develop a fast algorithm to identify substructures in simulations. The final aim, besides a faster production of subhalo catalogues, is to provide an algorithm fast enough to be applied with a fine time-cadence during the evolution of the simulations. We chose to apply the architecture of a well known Fully Convolutional Network, U-Net, to the identification of substructures within the mass density field of the simulation. We have developed SubDLe (Substructure identification with Deep Learning), an algorithm which combines a 3D generalization of U-Net and a Friends-of-Friends algorithm, and trained it to reproduce the identification of substructures performed by the SubFind algorithm in a set of zoom-in cosmological hydrodynamical simulations of galaxy clusters. For the feasibility study presented in this work, we have trained and tested SubDLe on galaxy clusters at $z=0$, using a NVIDIA P100 GPU. We focused our tests on the version of the algorithm working on the identification of purely stellar substructures, stellar SubDLe. Our stellar SubDLe is capable of identifying the majority of galaxies in the challenging high-density environment of galaxy clusters in short computing times. This result has interesting implications in view of the possibility of integrating fast subhalo finders within simulation codes, that can take advantage of accelerators available on state-of-art computing nodes.","sentences":["The identification of substructures within halos in cosmological hydrodynamical simulations is a fundamental step to identify the simulated counterparts of real objects, namely galaxies.","For this reason, substructure finders play a crucial role in extracting relevant information from the simulation outputs.","They are based on physically-motivated definitions of substructures, performing multiple steps of particle-by-particle operations, thus computationally expensive.","The purpose of this work is to develop a fast algorithm to identify substructures in simulations.","The final aim, besides a faster production of subhalo catalogues, is to provide an algorithm fast enough to be applied with a fine time-cadence during the evolution of the simulations.","We chose to apply the architecture of a well known Fully Convolutional Network, U-Net, to the identification of substructures within the mass density field of the simulation.","We have developed SubDLe (Substructure identification with Deep Learning), an algorithm which combines a 3D generalization of U-Net and a Friends-of-Friends algorithm, and trained it to reproduce the identification of substructures performed by the SubFind algorithm in a set of zoom-in cosmological hydrodynamical simulations of galaxy clusters.","For the feasibility study presented in this work, we have trained and tested SubDLe on galaxy clusters at $z=0$, using a NVIDIA P100 GPU.","We focused our tests on the version of the algorithm working on the identification of purely stellar substructures, stellar SubDLe.","Our stellar SubDLe is capable of identifying the majority of galaxies in the challenging high-density environment of galaxy clusters in short computing times.","This result has interesting implications in view of the possibility of integrating fast subhalo finders within simulation codes, that can take advantage of accelerators available on state-of-art computing nodes."],"url":"http://arxiv.org/abs/2405.18257v1","category":"astro-ph.CO"}
{"created":"2024-05-28 15:03:09","title":"On the Analysis of Quantum Repeater Chains with Sequential Swaps","abstract":"We evaluate the performance of two-way quantum repeater chains with sequential entanglement swapping. Within the analysis we consider memory decoherence, gate imperfections, and imperfect link-level entanglement generation. Our main results include closed-form expressions for the average entanglement fidelity of the generated end-to-end entangled states. We generalize previous findings for the one-shot fidelity analysis and study the case where repeater chains serve end-to-end requests continuously. We provide solutions to the continuous request scenario by combining results from quantum information theory and queuing theory. Finally, we apply the formulas obtained to analyze the impacts of hardware parameters, i.e., coherence times and gate fidelity, and distance on the entanglement fidelity and secret key rate of homogeneous quantum repeater chains.","sentences":["We evaluate the performance of two-way quantum repeater chains with sequential entanglement swapping.","Within the analysis we consider memory decoherence, gate imperfections, and imperfect link-level entanglement generation.","Our main results include closed-form expressions for the average entanglement fidelity of the generated end-to-end entangled states.","We generalize previous findings for the one-shot fidelity analysis and study the case where repeater chains serve end-to-end requests continuously.","We provide solutions to the continuous request scenario by combining results from quantum information theory and queuing theory.","Finally, we apply the formulas obtained to analyze the impacts of hardware parameters, i.e., coherence times and gate fidelity, and distance on the entanglement fidelity and secret key rate of homogeneous quantum repeater chains."],"url":"http://arxiv.org/abs/2405.18252v1","category":"quant-ph"}
{"created":"2024-05-28 14:58:43","title":"Extreme Value Monte Carlo Tree Search","abstract":"Despite being successful in board games and reinforcement learning (RL), UCT, a Monte-Carlo Tree Search (MCTS) combined with UCB1 Multi-Armed Bandit (MAB), has had limited success in domain-independent planning until recently. Previous work showed that UCB1, designed for $[0,1]$-bounded rewards, is not appropriate for estimating the distance-to-go which are potentially unbounded in $\\mathbb{R}$, such as heuristic functions used in classical planning, then proposed combining MCTS with MABs designed for Gaussian reward distributions and successfully improved the performance. In this paper, we further sharpen our understanding of ideal bandits for planning tasks. Existing work has two issues: First, while Gaussian MABs no longer over-specify the distances as $h\\in [0,1]$, they under-specify them as $h\\in [-\\infty,\\infty]$ while they are non-negative and can be further bounded in some cases. Second, there is no theoretical justifications for Full-Bellman backup (Schulte & Keller, 2014) that backpropagates minimum/maximum of samples. We identified \\emph{extreme value} statistics as a theoretical framework that resolves both issues at once and propose two bandits, UCB1-Uniform/Power, and apply them to MCTS for classical planning. We formally prove their regret bounds and empirically demonstrate their performance in classical planning.","sentences":["Despite being successful in board games and reinforcement learning (RL), UCT, a Monte-Carlo Tree Search (MCTS) combined with UCB1 Multi-Armed Bandit (MAB), has had limited success in domain-independent planning until recently.","Previous work showed that UCB1, designed for $[0,1]$-bounded rewards, is not appropriate for estimating the distance-to-go which are potentially unbounded in $\\mathbb{R}$, such as heuristic functions used in classical planning, then proposed combining MCTS with MABs designed for Gaussian reward distributions and successfully improved the performance.","In this paper, we further sharpen our understanding of ideal bandits for planning tasks.","Existing work has two issues: First, while Gaussian MABs no longer over-specify the distances as $h\\in [0,1]$, they under-specify them as $h\\in [-\\infty,\\infty]$ while they are non-negative and can be further bounded in some cases.","Second, there is no theoretical justifications for Full-Bellman backup (Schulte & Keller, 2014) that backpropagates minimum/maximum of samples.","We identified \\emph{extreme value} statistics as a theoretical framework that resolves both issues at once and propose two bandits, UCB1-Uniform/Power, and apply them to MCTS for classical planning.","We formally prove their regret bounds and empirically demonstrate their performance in classical planning."],"url":"http://arxiv.org/abs/2405.18248v1","category":"cs.AI"}
{"created":"2024-05-28 14:58:07","title":"Utilitarian Algorithm Configuration for Infinite Parameter Spaces","abstract":"Utilitarian algorithm configuration is a general-purpose technique for automatically searching the parameter space of a given algorithm to optimize its performance, as measured by a given utility function, on a given set of inputs. Recently introduced utilitarian configuration procedures offer optimality guarantees about the returned parameterization while provably adapting to the hardness of the underlying problem. However, the applicability of these approaches is severely limited by the fact that they only search a finite, relatively small set of parameters. They cannot effectively search the configuration space of algorithms with continuous or uncountable parameters. In this paper we introduce a new procedure, which we dub COUP (Continuous, Optimistic Utilitarian Procrastination). COUP is designed to search infinite parameter spaces efficiently to find good configurations quickly. Furthermore, COUP maintains the theoretical benefits of previous utilitarian configuration procedures when applied to finite parameter spaces but is significantly faster, both provably and experimentally.","sentences":["Utilitarian algorithm configuration is a general-purpose technique for automatically searching the parameter space of a given algorithm to optimize its performance, as measured by a given utility function, on a given set of inputs.","Recently introduced utilitarian configuration procedures offer optimality guarantees about the returned parameterization while provably adapting to the hardness of the underlying problem.","However, the applicability of these approaches is severely limited by the fact that they only search a finite, relatively small set of parameters.","They cannot effectively search the configuration space of algorithms with continuous or uncountable parameters.","In this paper we introduce a new procedure, which we dub COUP (Continuous, Optimistic Utilitarian Procrastination).","COUP is designed to search infinite parameter spaces efficiently to find good configurations quickly.","Furthermore, COUP maintains the theoretical benefits of previous utilitarian configuration procedures when applied to finite parameter spaces but is significantly faster, both provably and experimentally."],"url":"http://arxiv.org/abs/2405.18246v1","category":"cs.AI"}
{"created":"2024-05-28 14:50:22","title":"Active Use of Latent Constituency Representation in both Humans and Large Language Models","abstract":"Understanding how sentences are internally represented in the human brain, as well as in large language models (LLMs) such as ChatGPT, is a major challenge for cognitive science. Classic linguistic theories propose that the brain represents a sentence by parsing it into hierarchically organized constituents. In contrast, LLMs do not explicitly parse linguistic constituents and their latent representations remains poorly explained. Here, we demonstrate that humans and LLMs construct similar latent representations of hierarchical linguistic constituents by analyzing their behaviors during a novel one-shot learning task, in which they infer which words should be deleted from a sentence. Both humans and LLMs tend to delete a constituent, instead of a nonconstituent word string. In contrast, a naive sequence processing model that has access to word properties and ordinal positions does not show this property. Based on the word deletion behaviors, we can reconstruct the latent constituency tree representation of a sentence for both humans and LLMs. These results demonstrate that a latent tree-structured constituency representation can emerge in both the human brain and LLMs.","sentences":["Understanding how sentences are internally represented in the human brain, as well as in large language models (LLMs) such as ChatGPT, is a major challenge for cognitive science.","Classic linguistic theories propose that the brain represents a sentence by parsing it into hierarchically organized constituents.","In contrast, LLMs do not explicitly parse linguistic constituents and their latent representations remains poorly explained.","Here, we demonstrate that humans and LLMs construct similar latent representations of hierarchical linguistic constituents by analyzing their behaviors during a novel one-shot learning task, in which they infer which words should be deleted from a sentence.","Both humans and LLMs tend to delete a constituent, instead of a nonconstituent word string.","In contrast, a naive sequence processing model that has access to word properties and ordinal positions does not show this property.","Based on the word deletion behaviors, we can reconstruct the latent constituency tree representation of a sentence for both humans and LLMs.","These results demonstrate that a latent tree-structured constituency representation can emerge in both the human brain and LLMs."],"url":"http://arxiv.org/abs/2405.18241v1","category":"cs.CL"}
{"created":"2024-05-28 14:48:19","title":"The CoExplorer Technology Probe: A Generative AI-Powered Adaptive Interface to Support Intentionality in Planning and Running Video Meetings","abstract":"Effective meetings are effortful, but traditional videoconferencing systems offer little support for reducing this effort across the meeting lifecycle. Generative AI (GenAI) has the potential to radically redefine meetings by augmenting intentional meeting behaviors. CoExplorer, our novel adaptive meeting prototype, preemptively generates likely phases that meetings would undergo, tools that allow capturing attendees' thoughts before the meeting, and for each phase, window layouts, and appropriate applications and files. Using CoExplorer as a technology probe in a guided walkthrough, we studied its potential in a sample of participants from a global technology company. Our findings suggest that GenAI has the potential to help meetings stay on track and reduce workload, although concerns were raised about users' agency, trust, and possible disruption to traditional meeting norms. We discuss these concerns and their design implications for the development of GenAI meeting technology.","sentences":["Effective meetings are effortful, but traditional videoconferencing systems offer little support for reducing this effort across the meeting lifecycle.","Generative AI (GenAI) has the potential to radically redefine meetings by augmenting intentional meeting behaviors.","CoExplorer, our novel adaptive meeting prototype, preemptively generates likely phases that meetings would undergo, tools that allow capturing attendees' thoughts before the meeting, and for each phase, window layouts, and appropriate applications and files.","Using CoExplorer as a technology probe in a guided walkthrough, we studied its potential in a sample of participants from a global technology company.","Our findings suggest that GenAI has the potential to help meetings stay on track and reduce workload, although concerns were raised about users' agency, trust, and possible disruption to traditional meeting norms.","We discuss these concerns and their design implications for the development of GenAI meeting technology."],"url":"http://arxiv.org/abs/2405.18239v1","category":"cs.HC"}
{"created":"2024-05-28 14:47:40","title":"Nucleon Helicity Parton Distribution Function in the Continuum Limit with Self-Renormalization","abstract":"We present the first lattice calculation of the nucleon isovector helicity parton distribution function (PDF) in the framework of large-momentum effective theory (LaMET) that uses the hybrid scheme with self-renormalization. We use ensembles generated by the MILC collaboration at lattice spacings $a=\\{0.1207,0.0888,0.0582\\}$ fm, with $N_f=2+1+1$ flavors of highly improved staggered quarks at sea pion mass of $M_{\\pi}\\approx 315$ MeV. We use clover-improved action for our valence quarks with nucleon boost momentum $P_z\\approx 1.75$ GeV and high-statistics measurements for the LaMET matrix elements. We perform an extrapolation to the continuum limit and improve the handling of systematic errors using renormalization-group resummation (RGR) and leading-renormalon resummation (LRR). Our final nucleon helicity PDF is renormalized in the $\\overline{\\text{MS}}$ scheme at energy scale $\\mu=2.0$ GeV. We compare our results with and without the two systematic improvements of RGR and LRR at each lattice spacing as well as the continuum limit, and we see that the application of RGR and LRR greatly reduces the systematic errors across the whole $x$ range. Our continuum results with both RGR and LRR show a small positive antiquark region for the nucleon helicity PDF as well as a change of as much as a factor of two in the central values compared to results with neither RGR or LRR. By contrast, the application of RGR and LRR only changes the central values by about 5\\% in the quark region. We compare our lattice results with the global fits by the JAM, NNPDF and DSSV collaborations, and we observe some tension between our results.","sentences":["We present the first lattice calculation of the nucleon isovector helicity parton distribution function (PDF) in the framework of large-momentum effective theory (LaMET) that uses the hybrid scheme with self-renormalization.","We use ensembles generated by the MILC collaboration at lattice spacings $a=\\{0.1207,0.0888,0.0582\\}$ fm, with $N_f=2+1+1$ flavors of highly improved staggered quarks at sea pion mass of $M_{\\pi}\\approx 315$ MeV. We use clover-improved action for our valence quarks with nucleon boost momentum $P_z\\approx 1.75$ GeV and high-statistics measurements for the LaMET matrix elements.","We perform an extrapolation to the continuum limit and improve the handling of systematic errors using renormalization-group resummation (RGR) and leading-renormalon resummation (LRR).","Our final nucleon helicity PDF is renormalized in the $\\overline{\\text{MS}}$ scheme at energy scale $\\mu=2.0$ GeV.","We compare our results with and without the two systematic improvements of RGR and LRR at each lattice spacing as well as the continuum limit, and we see that the application of RGR and LRR greatly reduces the systematic errors across the whole $x$ range.","Our continuum results with both RGR and LRR show a small positive antiquark region for the nucleon helicity PDF as well as a change of as much as a factor of two in the central values compared to results with neither RGR or LRR.","By contrast, the application of RGR and LRR only changes the central values by about 5\\% in the quark region.","We compare our lattice results with the global fits by the JAM, NNPDF and DSSV collaborations, and we observe some tension between our results."],"url":"http://arxiv.org/abs/2405.18238v1","category":"hep-lat"}
{"created":"2024-05-28 14:45:54","title":"Selector form of Weaver's conjecture, Feichtinger's conjecture, and frame sparsification","abstract":"We show an extension of a probabilistic result of Marcus, Spielman, and Srivastava, which resolved the Kadison-Singer problem, for block diagonal positive semidefinite random matrices. We use this result to show several selector results, which generalize their partition counterparts. This includes a selector form of Weaver's KS$_r$ conjecture for block diagonal trace class operators, which extends a selector result for Bessel sequences, or equivalently rank one matrices, due to Londner and the author. We also show a selector variant of Feichtinger's conjecture for a (possibly infinite) collection of Bessel sequences, extending earlier results for a single Bessel sequence. We prove a generalization of the $R_\\epsilon$ conjecture of Casazza, Tremain, and Vershynin for infinite collection of equal norm Bessel sequences. In particular, our selector result yields a conjectured asymptotically optimal bound for a single Bessel sequence in terms of Riesz sequence tightness parameter.   We establish an iterated selector form of Weaver's KS$_2$ conjecture and show its applications. This includes a solution of an open problem on nearly unit norm Parseval frames of exponentials, which was posed by Londner and the author. We generalize a discretization result for continuous frames by Freeman and Speegle in two ways. First, we extend their result from the setting of rank one operators to positive trace operator valued measures. Second, we establish a nearly tight discretization of bounded continuous Parseval frames. In particular, our selector result yields an improvement of the result of Nitzan, Olevskii, and Ulanovskii and implies the existence of nearly tight exponential frames for unbounded sets with an explicit control on their frame redundancy.","sentences":["We show an extension of a probabilistic result of Marcus, Spielman, and Srivastava, which resolved the Kadison-Singer problem, for block diagonal positive semidefinite random matrices.","We use this result to show several selector results, which generalize their partition counterparts.","This includes a selector form of Weaver's KS$_r$ conjecture for block diagonal trace class operators, which extends a selector result for Bessel sequences, or equivalently rank one matrices, due to Londner and the author.","We also show a selector variant of Feichtinger's conjecture for a (possibly infinite) collection of Bessel sequences, extending earlier results for a single Bessel sequence.","We prove a generalization of the $R_\\epsilon$ conjecture of Casazza, Tremain, and Vershynin for infinite collection of equal norm Bessel sequences.","In particular, our selector result yields a conjectured asymptotically optimal bound for a single Bessel sequence in terms of Riesz sequence tightness parameter.   ","We establish an iterated selector form of Weaver's KS$_2$ conjecture and show its applications.","This includes a solution of an open problem on nearly unit norm Parseval frames of exponentials, which was posed by Londner and the author.","We generalize a discretization result for continuous frames by Freeman and Speegle in two ways.","First, we extend their result from the setting of rank one operators to positive trace operator valued measures.","Second, we establish a nearly tight discretization of bounded continuous Parseval frames.","In particular, our selector result yields an improvement of the result of Nitzan, Olevskii, and Ulanovskii and implies the existence of nearly tight exponential frames for unbounded sets with an explicit control on their frame redundancy."],"url":"http://arxiv.org/abs/2405.18235v1","category":"math.CA"}
{"created":"2024-05-28 14:40:10","title":"Relative Langlands Duality of Toric Periods","abstract":"The relative Langlands program introduced by Ben-Zvi--Sakellaridis--Venkatesh posits a duality structure exchanging automorphic periods and L-functions, which can be encoded by pairs of dual Hamiltonian actions. In work of the author and Venkatesh, an extension of the definitions to certain singular spaces was made with the objective of restoring duality in some well-known automorphic integrals. In this companion article we apply these definitions to establish duality in the context of affine toric varieties, and study finer structures regarding regularization and stabilizers that are instructive for the general case.","sentences":["The relative Langlands program introduced by Ben-Zvi--Sakellaridis--Venkatesh posits a duality structure exchanging automorphic periods and L-functions, which can be encoded by pairs of dual Hamiltonian actions.","In work of the author and Venkatesh, an extension of the definitions to certain singular spaces was made with the objective of restoring duality in some well-known automorphic integrals.","In this companion article we apply these definitions to establish duality in the context of affine toric varieties, and study finer structures regarding regularization and stabilizers that are instructive for the general case."],"url":"http://arxiv.org/abs/2405.18231v1","category":"math.NT"}
{"created":"2024-05-28 14:39:54","title":"Quantum Active Learning","abstract":"Quantum machine learning, as an extension of classical machine learning that harnesses quantum mechanics, facilitates effiient learning from data encoded in quantum states. Training a quantum neural network typically demands a substantial labeled training set for supervised learning. Human annotators, often experts, provide labels for samples through additional experiments, adding to the training cost. To mitigate this expense, there is a quest for methods that maintain model performance over fully labeled datasets while requiring fewer labeled samples in practice, thereby extending few-shot learning to the quantum realm. Quantum active learning estimates the uncertainty of quantum data to select the most informative samples from a pool for labeling. Consequently, a QML model is supposed to accumulate maximal knowledge as the training set comprises labeled samples selected via sampling strategies. Notably, the QML models trained within the QAL framework are not restricted to specific types, enabling performance enhancement from the model architecture's perspective towards few-shot learning. Recognizing symmetry as a fundamental concept in physics ubiquitous across various domains, we leverage the symmetry inherent in quantum states induced by the embedding of classical data for model design. We employ an equivariant QNN capable of generalizing from fewer data with geometric priors. We benchmark the performance of QAL on two classification problems, observing both positive and negative results. QAL effectively trains the model, achieving performance comparable to that on fully labeled datasets by labeling less than 7\\% of the samples in the pool with unbiased sampling behavior. Furthermore, we elucidate the negative result of QAL being overtaken by random sampling baseline through miscellaneous numerical experiments. (character count limit, see the main text)","sentences":["Quantum machine learning, as an extension of classical machine learning that harnesses quantum mechanics, facilitates effiient learning from data encoded in quantum states.","Training a quantum neural network typically demands a substantial labeled training set for supervised learning.","Human annotators, often experts, provide labels for samples through additional experiments, adding to the training cost.","To mitigate this expense, there is a quest for methods that maintain model performance over fully labeled datasets while requiring fewer labeled samples in practice, thereby extending few-shot learning to the quantum realm.","Quantum active learning estimates the uncertainty of quantum data to select the most informative samples from a pool for labeling.","Consequently, a QML model is supposed to accumulate maximal knowledge as the training set comprises labeled samples selected via sampling strategies.","Notably, the QML models trained within the QAL framework are not restricted to specific types, enabling performance enhancement from the model architecture's perspective towards few-shot learning.","Recognizing symmetry as a fundamental concept in physics ubiquitous across various domains, we leverage the symmetry inherent in quantum states induced by the embedding of classical data for model design.","We employ an equivariant QNN capable of generalizing from fewer data with geometric priors.","We benchmark the performance of QAL on two classification problems, observing both positive and negative results.","QAL effectively trains the model, achieving performance comparable to that on fully labeled datasets by labeling less than 7\\% of the samples in the pool with unbiased sampling behavior.","Furthermore, we elucidate the negative result of QAL being overtaken by random sampling baseline through miscellaneous numerical experiments.","(character count limit, see the main text)"],"url":"http://arxiv.org/abs/2405.18230v1","category":"quant-ph"}
{"created":"2024-05-28 14:37:24","title":"Concurrent Particle Acceleration and Pitch-Angle Anisotropy Driven by Magnetic Reconnection: Ion-Electron Plasmas","abstract":"Particle acceleration and pitch-angle anisotropy resulting from magnetic reconnection are investigated in highly magnetized ion-electron plasmas. By means of fully kinetic particle-in-cell simulations, we demonstrate that magnetic reconnection generates anisotropic particle distributions $f_s \\left( {|\\cos \\alpha|,\\varepsilon} \\right)$, characterized by broken power laws in the particle energy spectrum $f_s (\\varepsilon) \\propto \\varepsilon^{-p}$ and pitch angle $\\langle \\sin^2 \\alpha \\rangle \\propto \\varepsilon^m$. Their characteristics are determined by the ratio of the guide field to the reconnecting field ($B_g/B_0$) and the plasma magnetization ($\\sigma_0$). Below the break energy $\\varepsilon_0$, ion and electron energy spectra are extremely hard ($p_<\\lesssim 1$) for any $B_g/B_0$ and $\\sigma_0 \\gtrsim 1$, while above $\\varepsilon_0$, the spectral index steepens ($p_> \\gtrsim 2$), displaying high sensitivity to both $B_g/B_0$ and $\\sigma_0$. The pitch angle displays power-law ranges with negative slopes ($m_<$) below and positive slopes ($m_>$) above $\\varepsilon_{\\min \\alpha}$, steepening with increasing $B_g/B_0$ and $\\sigma_0$. The ratio $B_g/B_0$ regulates the redistribution of magnetic energy between ions ($\\Delta E_i$) and electrons ($\\Delta E_e$), with $\\Delta E_i \\gg \\Delta E_e$ for $B_g/B_0 \\ll 1$, $\\Delta E_i \\sim \\Delta E_e$ for $B_g/B_0 \\sim 1$, and $\\Delta E_i \\ll \\Delta E_e$ for $B_g/B_0 \\gg 1$, with $\\Delta E_i/\\Delta E_e$ approaching unity when $\\sigma_0 \\gg 1$. The anisotropic distribution of accelerated particles results in an optically thin synchrotron power spectrum $F_\\nu(\\nu) \\propto\\nu^{(2-2p+m)/(4+m)}$ and a linear polarization degree $\\Pi_{\\rm lin} = (p+1)/(p+7/3+m/3)$. Pitch-angle anisotropy also induces temperature anisotropy and eases synchrotron cooling, along with producing beamed radiation, potentially responsible for frequency-dependent variability.","sentences":["Particle acceleration and pitch-angle anisotropy resulting from magnetic reconnection are investigated in highly magnetized ion-electron plasmas.","By means of fully kinetic particle-in-cell simulations, we demonstrate that magnetic reconnection generates anisotropic particle distributions $f_s","\\left( {|\\cos \\alpha|,\\varepsilon} \\right)$, characterized by broken power laws in the particle energy spectrum $f_s","(\\varepsilon) \\propto \\varepsilon^{-p}$ and pitch angle $\\langle \\sin^2 \\alpha \\rangle \\propto \\varepsilon^m$. Their characteristics are determined by the ratio of the guide field to the reconnecting field ($B_g/B_0$) and the plasma magnetization ($\\sigma_0$).","Below the break energy $\\varepsilon_0$, ion and electron energy spectra are extremely hard ($p_<\\lesssim 1$) for any $B_g/B_0$ and $\\sigma_0 \\gtrsim 1$, while above $\\varepsilon_0$, the spectral index steepens ($p_>","\\gtrsim 2$), displaying high sensitivity to both $B_g/B_0$ and $\\sigma_0$. The pitch angle displays power-law ranges with negative slopes ($m_<$) below and positive slopes ($m_>$) above $\\varepsilon_{\\min \\alpha}$, steepening with increasing $B_g/B_0$ and $\\sigma_0$. The ratio $B_g/B_0$ regulates the redistribution of magnetic energy between ions ($\\Delta E_i$) and electrons ($\\Delta E_e$), with $\\Delta E_i \\gg \\Delta E_e$ for $B_g/B_0 \\ll 1$, $\\Delta E_i \\sim \\Delta E_e$ for $B_g/B_0 \\sim 1$, and $\\Delta E_i \\ll \\Delta E_e$ for $B_g/B_0 \\gg 1$, with $\\Delta E_i/\\Delta E_e$ approaching unity when $\\sigma_0 \\gg 1$.","The anisotropic distribution of accelerated particles results in an optically thin synchrotron power spectrum $F_\\nu(\\nu) \\propto\\nu^{(2-2p+m)/(4+m)}$ and a linear polarization degree $\\Pi_{\\rm lin} = (p+1)/(p+7/3+m/3)$. Pitch-angle anisotropy also induces temperature anisotropy and eases synchrotron cooling, along with producing beamed radiation, potentially responsible for frequency-dependent variability."],"url":"http://arxiv.org/abs/2405.18227v1","category":"astro-ph.HE"}
{"created":"2024-05-28 14:30:07","title":"From Learning to Optimize to Learning Optimization Algorithms","abstract":"Towards designing learned optimization algorithms that are usable beyond their training setting, we identify key principles that classical algorithms obey, but have up to now, not been used for Learning to Optimize (L2O). Following these principles, we provide a general design pipeline, taking into account data, architecture and learning strategy, and thereby enabling a synergy between classical optimization and L2O, resulting in a philosophy of Learning Optimization Algorithms. As a consequence our learned algorithms perform well far beyond problems from the training distribution. We demonstrate the success of these novel principles by designing a new learning-enhanced BFGS algorithm and provide numerical experiments evidencing its adaptation to many settings at test time.","sentences":["Towards designing learned optimization algorithms that are usable beyond their training setting, we identify key principles that classical algorithms obey, but have up to now, not been used for Learning to Optimize (L2O).","Following these principles, we provide a general design pipeline, taking into account data, architecture and learning strategy, and thereby enabling a synergy between classical optimization and L2O, resulting in a philosophy of Learning Optimization Algorithms.","As a consequence our learned algorithms perform well far beyond problems from the training distribution.","We demonstrate the success of these novel principles by designing a new learning-enhanced BFGS algorithm and provide numerical experiments evidencing its adaptation to many settings at test time."],"url":"http://arxiv.org/abs/2405.18222v1","category":"cs.LG"}
{"created":"2024-05-28 14:28:28","title":"Non-negative Tensor Mixture Learning for Discrete Density Estimation","abstract":"We present an expectation-maximization (EM) based unified framework for non-negative tensor decomposition that optimizes the Kullback-Leibler divergence. To avoid iterations in each M-step and learning rate tuning, we establish a general relationship between low-rank decomposition and many-body approximation. Using this connection, we exploit that the closed-form solution of the many-body approximation can be used to update all parameters simultaneously in the M-step. Our framework not only offers a unified methodology for a variety of low-rank structures, including CP, Tucker, and Train decompositions, but also their combinations forming mixtures of tensors as well as robust adaptive noise modeling. Empirically, we demonstrate that our framework provides superior generalization for discrete density estimation compared to conventional tensor-based approaches.","sentences":["We present an expectation-maximization (EM) based unified framework for non-negative tensor decomposition that optimizes the Kullback-Leibler divergence.","To avoid iterations in each M-step and learning rate tuning, we establish a general relationship between low-rank decomposition and many-body approximation.","Using this connection, we exploit that the closed-form solution of the many-body approximation can be used to update all parameters simultaneously in the M-step.","Our framework not only offers a unified methodology for a variety of low-rank structures, including CP, Tucker, and Train decompositions, but also their combinations forming mixtures of tensors as well as robust adaptive noise modeling.","Empirically, we demonstrate that our framework provides superior generalization for discrete density estimation compared to conventional tensor-based approaches."],"url":"http://arxiv.org/abs/2405.18220v1","category":"stat.ML"}
{"created":"2024-05-28 14:23:11","title":"Conceptual Design of a Doppler Spectrometer for 10$^2$ m/s Cross-Field Flows in Tokamak Divertors","abstract":"It has been theoretically predicted that the \\ExB drift caused by the spontaneously generated potential in scrape-off-layers (SOLs) and divertors in tokamaks is of a similar size to the poloidal component of the parallel flow and turbulent flow, thereby it significantly impacts on the plasma transport there. Many experiments indeed have implied the role of the electric potential, however, its direct observation through its \\ExB flow measurement has never been realized because the drift velocity ($10^2$--$10^3$ m/s) is significantly below the detection limit of existing diagnostics. To realize a cross-field ion flow measurement, variety of systematic uncertainties of the system must be narrowed down. Here, we develop a conceptual design of the Doppler spectrometry that enables to measure the impurity flows with $10^2$-m/s accuracy, based on an in-situ wavelength-calibration techniques developed in astrophysics field, the iodine-cell method. We discuss its properties and applicability. In particular, the scaling relation of the wavelength accuracy and various spectroscopic parameters is newly presented, which suggests the high importance of the wavelength resolution of the system. Based on transport simulations for the JT-60SA divertor, the feasibility of the system is assessed.","sentences":["It has been theoretically predicted that the \\ExB drift caused by the spontaneously generated potential in scrape-off-layers (SOLs) and divertors in tokamaks is of a similar size to the poloidal component of the parallel flow and turbulent flow, thereby it significantly impacts on the plasma transport there.","Many experiments indeed have implied the role of the electric potential, however, its direct observation through its \\ExB flow measurement has never been realized because the drift velocity ($10^2$--$10^3$ m/s) is significantly below the detection limit of existing diagnostics.","To realize a cross-field ion flow measurement, variety of systematic uncertainties of the system must be narrowed down.","Here, we develop a conceptual design of the Doppler spectrometry that enables to measure the impurity flows with $10^2$-m/s accuracy, based on an in-situ wavelength-calibration techniques developed in astrophysics field, the iodine-cell method.","We discuss its properties and applicability.","In particular, the scaling relation of the wavelength accuracy and various spectroscopic parameters is newly presented, which suggests the high importance of the wavelength resolution of the system.","Based on transport simulations for the JT-60SA divertor, the feasibility of the system is assessed."],"url":"http://arxiv.org/abs/2405.18219v1","category":"physics.plasm-ph"}
{"created":"2024-05-28 14:17:41","title":"NeRAF: 3D Scene Infused Neural Radiance and Acoustic Fields","abstract":"Sound plays a major role in human perception, providing essential scene information alongside vision for understanding our environment. Despite progress in neural implicit representations, learning acoustics that match a visual scene is still challenging. We propose NeRAF, a method that jointly learns acoustic and radiance fields. NeRAF is designed as a Nerfstudio module for convenient access to realistic audio-visual generation. It synthesizes both novel views and spatialized audio at new positions, leveraging radiance field capabilities to condition the acoustic field with 3D scene information. At inference, each modality can be rendered independently and at spatially separated positions, providing greater versatility. We demonstrate the advantages of our method on the SoundSpaces dataset. NeRAF achieves substantial performance improvements over previous works while being more data-efficient. Furthermore, NeRAF enhances novel view synthesis of complex scenes trained with sparse data through cross-modal learning.","sentences":["Sound plays a major role in human perception, providing essential scene information alongside vision for understanding our environment.","Despite progress in neural implicit representations, learning acoustics that match a visual scene is still challenging.","We propose NeRAF, a method that jointly learns acoustic and radiance fields.","NeRAF is designed as a Nerfstudio module for convenient access to realistic audio-visual generation.","It synthesizes both novel views and spatialized audio at new positions, leveraging radiance field capabilities to condition the acoustic field with 3D scene information.","At inference, each modality can be rendered independently and at spatially separated positions, providing greater versatility.","We demonstrate the advantages of our method on the SoundSpaces dataset.","NeRAF achieves substantial performance improvements over previous works while being more data-efficient.","Furthermore, NeRAF enhances novel view synthesis of complex scenes trained with sparse data through cross-modal learning."],"url":"http://arxiv.org/abs/2405.18213v1","category":"cs.SD"}
{"created":"2024-05-28 14:13:32","title":"A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models","abstract":"Recent studies have highlighted their proficiency in some simple tasks like writing and coding through various reasoning strategies. However, LLM agents still struggle with tasks that require comprehensive planning, a process that challenges current models and remains a critical research issue. In this study, we concentrate on travel planning, a Multi-Phases planning problem, that involves multiple interconnected stages, such as outlining, information gathering, and planning, often characterized by the need to manage various constraints and uncertainties. Existing reasoning approaches have struggled to effectively address this complex task. Our research aims to address this challenge by developing a human-like planning framework for LLM agents, i.e., guiding the LLM agent to simulate various steps that humans take when solving Multi-Phases problems. Specifically, we implement several strategies to enable LLM agents to generate a coherent outline for each travel query, mirroring human planning patterns. Additionally, we integrate Strategy Block and Knowledge Block into our framework: Strategy Block facilitates information collection, while Knowledge Block provides essential information for detailed planning. Through our extensive experiments, we demonstrate that our framework significantly improves the planning capabilities of LLM agents, enabling them to tackle the travel planning task with improved efficiency and effectiveness. Our experimental results showcase the exceptional performance of the proposed framework; when combined with GPT-4-Turbo, it attains $10\\times$ the performance gains in comparison to the baseline framework deployed on GPT-4-Turbo.","sentences":["Recent studies have highlighted their proficiency in some simple tasks like writing and coding through various reasoning strategies.","However, LLM agents still struggle with tasks that require comprehensive planning, a process that challenges current models and remains a critical research issue.","In this study, we concentrate on travel planning, a Multi-Phases planning problem, that involves multiple interconnected stages, such as outlining, information gathering, and planning, often characterized by the need to manage various constraints and uncertainties.","Existing reasoning approaches have struggled to effectively address this complex task.","Our research aims to address this challenge by developing a human-like planning framework for LLM agents, i.e., guiding the LLM agent to simulate various steps that humans take when solving Multi-Phases problems.","Specifically, we implement several strategies to enable LLM agents to generate a coherent outline for each travel query, mirroring human planning patterns.","Additionally, we integrate Strategy Block and Knowledge Block into our framework: Strategy Block facilitates information collection, while Knowledge Block provides essential information for detailed planning.","Through our extensive experiments, we demonstrate that our framework significantly improves the planning capabilities of LLM agents, enabling them to tackle the travel planning task with improved efficiency and effectiveness.","Our experimental results showcase the exceptional performance of the proposed framework; when combined with GPT-4-Turbo, it attains $10\\times$ the performance gains in comparison to the baseline framework deployed on GPT-4-Turbo."],"url":"http://arxiv.org/abs/2405.18208v1","category":"cs.AI"}
{"created":"2024-05-28 14:12:25","title":"Multi-CATE: Multi-Accurate Conditional Average Treatment Effect Estimation Robust to Unknown Covariate Shifts","abstract":"Estimating heterogeneous treatment effects is important to tailor treatments to those individuals who would most likely benefit. However, conditional average treatment effect predictors may often be trained on one population but possibly deployed on different, possibly unknown populations. We use methodology for learning multi-accurate predictors to post-process CATE T-learners (differenced regressions) to become robust to unknown covariate shifts at the time of deployment. The method works in general for pseudo-outcome regression, such as the DR-learner. We show how this approach can combine (large) confounded observational and (smaller) randomized datasets by learning a confounded predictor from the observational dataset, and auditing for multi-accuracy on the randomized controlled trial. We show improvements in bias and mean squared error in simulations with increasingly larger covariate shift, and on a semi-synthetic case study of a parallel large observational study and smaller randomized controlled experiment. Overall, we establish a connection between methods developed for multi-distribution learning and achieve appealing desiderata (e.g. external validity) in causal inference and machine learning.","sentences":["Estimating heterogeneous treatment effects is important to tailor treatments to those individuals who would most likely benefit.","However, conditional average treatment effect predictors may often be trained on one population but possibly deployed on different, possibly unknown populations.","We use methodology for learning multi-accurate predictors to post-process CATE T-learners (differenced regressions) to become robust to unknown covariate shifts at the time of deployment.","The method works in general for pseudo-outcome regression, such as the DR-learner.","We show how this approach can combine (large) confounded observational and (smaller) randomized datasets by learning a confounded predictor from the observational dataset, and auditing for multi-accuracy on the randomized controlled trial.","We show improvements in bias and mean squared error in simulations with increasingly larger covariate shift, and on a semi-synthetic case study of a parallel large observational study and smaller randomized controlled experiment.","Overall, we establish a connection between methods developed for multi-distribution learning and achieve appealing desiderata (e.g. external validity) in causal inference and machine learning."],"url":"http://arxiv.org/abs/2405.18206v1","category":"cs.LG"}
{"created":"2024-05-28 14:11:42","title":"Joint Radar Sensing, Location, and Communication Resources Optimization in 6G Network","abstract":"The possibility of jointly optimizing location sensing and communication resources, facilitated by the existence of communication and sensing spectrum sharing, is what promotes the system performance to a higher level. However, the rapid mobility of user equipment (UE) can result in inaccurate location estimation, which can severely degrade system performance. Therefore, the precise UE location sensing and resource allocation issues are investigated in a spectrum sharing sixth generation network. An approach is proposed for joint subcarrier and power optimization based on UE location sensing, aiming to minimize system energy consumption. The joint allocation process is separated into two key phases of operation. In the radar location sensing phase, the multipath interference and Doppler effects are considered simultaneously, and the issues of UE's location and channel state estimation are transformed into a convex optimization problem, which is then solved through gradient descent. In the communication phase, a subcarrier allocation method based on subcarrier weights is proposed. To further minimize system energy consumption, a joint subcarrier and power allocation method is introduced, resolved via the Lagrange multiplier method for the non-convex resource allocation problem. Simulation analysis results indicate that the location sensing algorithm exhibits a prominent improvement in accuracy compared to benchmark algorithms. Simultaneously, the proposed resource allocation scheme also demonstrates a substantial enhancement in performance relative to baseline schemes.","sentences":["The possibility of jointly optimizing location sensing and communication resources, facilitated by the existence of communication and sensing spectrum sharing, is what promotes the system performance to a higher level.","However, the rapid mobility of user equipment (UE) can result in inaccurate location estimation, which can severely degrade system performance.","Therefore, the precise UE location sensing and resource allocation issues are investigated in a spectrum sharing sixth generation network.","An approach is proposed for joint subcarrier and power optimization based on UE location sensing, aiming to minimize system energy consumption.","The joint allocation process is separated into two key phases of operation.","In the radar location sensing phase, the multipath interference and Doppler effects are considered simultaneously, and the issues of UE's location and channel state estimation are transformed into a convex optimization problem, which is then solved through gradient descent.","In the communication phase, a subcarrier allocation method based on subcarrier weights is proposed.","To further minimize system energy consumption, a joint subcarrier and power allocation method is introduced, resolved via the Lagrange multiplier method for the non-convex resource allocation problem.","Simulation analysis results indicate that the location sensing algorithm exhibits a prominent improvement in accuracy compared to benchmark algorithms.","Simultaneously, the proposed resource allocation scheme also demonstrates a substantial enhancement in performance relative to baseline schemes."],"url":"http://arxiv.org/abs/2405.18205v1","category":"eess.SY"}
{"created":"2024-05-28 14:11:22","title":"Non-equilibrium dynamics of long-range interacting Fermions","abstract":"A fundamental problem of out-of-equilibrium physics is the speed at which the order parameter grows upon crossing a phase transition. Here, we investigate the dynamics of ordering in a Fermi gas undergoing a density-wave phase transition induced by quenching of long-range, cavity-mediated interactions. We observe in real-time the exponential rise of the order parameter and track its growth over several orders of magnitude. Remarkably, the growth rate is insensitive to the contact interaction strength from the ideal gas up to the unitary limit and can exceed the Fermi energy by an order of magnitude, in quantitative agreement with a linearized instability analysis. We then generalize our results to linear interaction ramps, where deviations from the adiabatic behaviour are captured by a simple dynamical ansatz. Our study offers a paradigmatic example of the interplay between non-locality and non-equilibrium dynamics, where universal scaling behaviour emerges despite strong interactions at the microscopic level.","sentences":["A fundamental problem of out-of-equilibrium physics is the speed at which the order parameter grows upon crossing a phase transition.","Here, we investigate the dynamics of ordering in a Fermi gas undergoing a density-wave phase transition induced by quenching of long-range, cavity-mediated interactions.","We observe in real-time the exponential rise of the order parameter and track its growth over several orders of magnitude.","Remarkably, the growth rate is insensitive to the contact interaction strength from the ideal gas up to the unitary limit and can exceed the Fermi energy by an order of magnitude, in quantitative agreement with a linearized instability analysis.","We then generalize our results to linear interaction ramps, where deviations from the adiabatic behaviour are captured by a simple dynamical ansatz.","Our study offers a paradigmatic example of the interplay between non-locality and non-equilibrium dynamics, where universal scaling behaviour emerges despite strong interactions at the microscopic level."],"url":"http://arxiv.org/abs/2405.18204v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-28 14:11:01","title":"IAPT: Instruction-Aware Prompt Tuning for Large Language Models","abstract":"Soft prompt tuning is a widely studied parameter-efficient fine-tuning method. However, it has a clear drawback: many soft tokens must be inserted into the input sequences to guarantee downstream performance. As a result, soft prompt tuning is less considered than Low-rank adaptation (LoRA) in the large language modeling (LLM) era. In this work, we propose a novel prompt tuning method, Instruction-Aware Prompt Tuning (IAPT), that requires only four soft tokens. First, we install a parameter-efficient soft prompt generator at each Transformer layer to generate idiosyncratic soft prompts for each input instruction. The generated soft prompts can be seen as a semantic summary of the input instructions and can effectively guide the output generation. Second, the soft prompt generators are modules with a bottleneck architecture consisting of a self-attention pooling operation, two linear projections, and an activation function. Pilot experiments show that prompt generators at different Transformer layers require different activation functions. Thus, we propose to learn the idiosyncratic activation functions for prompt generators automatically with the help of rational functions. We have conducted experiments on various tasks, and the experimental results demonstrate that (a) our IAPT method can outperform the recent baselines with comparable tunable parameters. (b) Our IAPT method is more efficient than LoRA under the single-backbone multi-tenant setting.","sentences":["Soft prompt tuning is a widely studied parameter-efficient fine-tuning method.","However, it has a clear drawback: many soft tokens must be inserted into the input sequences to guarantee downstream performance.","As a result, soft prompt tuning is less considered than Low-rank adaptation (LoRA) in the large language modeling (LLM) era.","In this work, we propose a novel prompt tuning method, Instruction-Aware Prompt Tuning (IAPT), that requires only four soft tokens.","First, we install a parameter-efficient soft prompt generator at each Transformer layer to generate idiosyncratic soft prompts for each input instruction.","The generated soft prompts can be seen as a semantic summary of the input instructions and can effectively guide the output generation.","Second, the soft prompt generators are modules with a bottleneck architecture consisting of a self-attention pooling operation, two linear projections, and an activation function.","Pilot experiments show that prompt generators at different Transformer layers require different activation functions.","Thus, we propose to learn the idiosyncratic activation functions for prompt generators automatically with the help of rational functions.","We have conducted experiments on various tasks, and the experimental results demonstrate that (a) our IAPT method can outperform the recent baselines with comparable tunable parameters.","(b) Our IAPT method is more efficient than LoRA under the single-backbone multi-tenant setting."],"url":"http://arxiv.org/abs/2405.18203v1","category":"cs.CL"}
{"created":"2024-05-28 14:10:51","title":"IM-Context: In-Context Learning for Imbalanced Regression Tasks","abstract":"Regression models often fail to generalize effectively in regions characterized by highly imbalanced label distributions. Previous methods for deep imbalanced regression rely on gradient-based weight updates, which tend to overfit in underrepresented regions. This paper proposes a paradigm shift towards in-context learning as an effective alternative to conventional in-weight learning methods, particularly for addressing imbalanced regression. In-context learning refers to the ability of a model to condition itself, given a prompt sequence composed of in-context samples (input-label pairs) alongside a new query input to generate predictions, without requiring any parameter updates. In this paper, we study the impact of the prompt sequence on the model performance from both theoretical and empirical perspectives. We emphasize the importance of localized context in reducing bias within regions of high imbalance. Empirical evaluations across a variety of real-world datasets demonstrate that in-context learning substantially outperforms existing in-weight learning methods in scenarios with high levels of imbalance.","sentences":["Regression models often fail to generalize effectively in regions characterized by highly imbalanced label distributions.","Previous methods for deep imbalanced regression rely on gradient-based weight updates, which tend to overfit in underrepresented regions.","This paper proposes a paradigm shift towards in-context learning as an effective alternative to conventional in-weight learning methods, particularly for addressing imbalanced regression.","In-context learning refers to the ability of a model to condition itself, given a prompt sequence composed of in-context samples (input-label pairs) alongside a new query input to generate predictions, without requiring any parameter updates.","In this paper, we study the impact of the prompt sequence on the model performance from both theoretical and empirical perspectives.","We emphasize the importance of localized context in reducing bias within regions of high imbalance.","Empirical evaluations across a variety of real-world datasets demonstrate that in-context learning substantially outperforms existing in-weight learning methods in scenarios with high levels of imbalance."],"url":"http://arxiv.org/abs/2405.18202v1","category":"cs.LG"}
{"created":"2024-05-28 14:07:15","title":"OREO: O-RAN intElligence Orchestration of xApp-based network services","abstract":"The Open Radio Access Network (O-RAN) architecture aims to support a plethora of network services, such as beam management and network slicing, through the use of third-party applications called xApps. To efficiently provide network services at the radio interface, it is thus essential that the deployment of the xApps is carefully orchestrated. In this paper, we introduce OREO, an O-RAN xApp orchestrator, designed to maximize the offered services. OREO's key idea is that services can share xApps whenever they correspond to semantically equivalent functions, and the xApp output is of sufficient quality to fulfill the service requirements. By leveraging a multi-layer graph model that captures all the system components, from services to xApps, OREO implements an algorithmic solution that selects the best service configuration, maximizes the number of shared xApps, and efficiently and dynamically allocates resources to them. Numerical results as well as experimental tests performed using our proof-of-concept implementation, demonstrate that OREO closely matches the optimum, obtained by solving an NP-hard problem. Further, it outperforms the state of the art, deploying up to 35% more services with an average of 30% fewer xApps and a similar reduction in the resource consumption.","sentences":["The Open Radio Access Network (O-RAN) architecture aims to support a plethora of network services, such as beam management and network slicing, through the use of third-party applications called xApps.","To efficiently provide network services at the radio interface, it is thus essential that the deployment of the xApps is carefully orchestrated.","In this paper, we introduce OREO, an O-RAN xApp orchestrator, designed to maximize the offered services.","OREO's key idea is that services can share xApps whenever they correspond to semantically equivalent functions, and the xApp output is of sufficient quality to fulfill the service requirements.","By leveraging a multi-layer graph model that captures all the system components, from services to xApps, OREO implements an algorithmic solution that selects the best service configuration, maximizes the number of shared xApps, and efficiently and dynamically allocates resources to them.","Numerical results as well as experimental tests performed using our proof-of-concept implementation, demonstrate that OREO closely matches the optimum, obtained by solving an NP-hard problem.","Further, it outperforms the state of the art, deploying up to 35% more services with an average of 30% fewer xApps and a similar reduction in the resource consumption."],"url":"http://arxiv.org/abs/2405.18198v1","category":"cs.NI"}
{"created":"2024-05-28 14:06:10","title":"Render and Diffuse: Aligning Image and Action Spaces for Diffusion-based Behaviour Cloning","abstract":"In the field of Robot Learning, the complex mapping between high-dimensional observations such as RGB images and low-level robotic actions, two inherently very different spaces, constitutes a complex learning problem, especially with limited amounts of data. In this work, we introduce Render and Diffuse (R&D) a method that unifies low-level robot actions and RGB observations within the image space using virtual renders of the 3D model of the robot. Using this joint observation-action representation it computes low-level robot actions using a learnt diffusion process that iteratively updates the virtual renders of the robot. This space unification simplifies the learning problem and introduces inductive biases that are crucial for sample efficiency and spatial generalisation. We thoroughly evaluate several variants of R&D in simulation and showcase their applicability on six everyday tasks in the real world. Our results show that R&D exhibits strong spatial generalisation capabilities and is more sample efficient than more common image-to-action methods.","sentences":["In the field of Robot Learning, the complex mapping between high-dimensional observations such as RGB images and low-level robotic actions, two inherently very different spaces, constitutes a complex learning problem, especially with limited amounts of data.","In this work, we introduce Render and Diffuse (R&D) a method that unifies low-level robot actions and RGB observations within the image space using virtual renders of the 3D model of the robot.","Using this joint observation-action representation it computes low-level robot actions using a learnt diffusion process that iteratively updates the virtual renders of the robot.","This space unification simplifies the learning problem and introduces inductive biases that are crucial for sample efficiency and spatial generalisation.","We thoroughly evaluate several variants of R&D in simulation and showcase their applicability on six everyday tasks in the real world.","Our results show that R&D exhibits strong spatial generalisation capabilities and is more sample efficient than more common image-to-action methods."],"url":"http://arxiv.org/abs/2405.18196v1","category":"cs.RO"}
{"created":"2024-05-28 14:03:52","title":"In-Context Symmetries: Self-Supervised Learning through Contextual World Models","abstract":"At the core of self-supervised learning for vision is the idea of learning invariant or equivariant representations with respect to a set of data transformations. This approach, however, introduces strong inductive biases, which can render the representations fragile in downstream tasks that do not conform to these symmetries. In this work, drawing insights from world models, we propose to instead learn a general representation that can adapt to be invariant or equivariant to different transformations by paying attention to context -- a memory module that tracks task-specific states, actions, and future states. Here, the action is the transformation, while the current and future states respectively represent the input's representation before and after the transformation. Our proposed algorithm, Contextual Self-Supervised Learning (ContextSSL), learns equivariance to all transformations (as opposed to invariance). In this way, the model can learn to encode all relevant features as general representations while having the versatility to tail down to task-wise symmetries when given a few examples as the context. Empirically, we demonstrate significant performance gains over existing methods on equivariance-related tasks, supported by both qualitative and quantitative evaluations.","sentences":["At the core of self-supervised learning for vision is the idea of learning invariant or equivariant representations with respect to a set of data transformations.","This approach, however, introduces strong inductive biases, which can render the representations fragile in downstream tasks that do not conform to these symmetries.","In this work, drawing insights from world models, we propose to instead learn a general representation that can adapt to be invariant or equivariant to different transformations by paying attention to context -- a memory module that tracks task-specific states, actions, and future states.","Here, the action is the transformation, while the current and future states respectively represent the input's representation before and after the transformation.","Our proposed algorithm, Contextual Self-Supervised Learning (ContextSSL), learns equivariance to all transformations (as opposed to invariance).","In this way, the model can learn to encode all relevant features as general representations while having the versatility to tail down to task-wise symmetries when given a few examples as the context.","Empirically, we demonstrate significant performance gains over existing methods on equivariance-related tasks, supported by both qualitative and quantitative evaluations."],"url":"http://arxiv.org/abs/2405.18193v1","category":"cs.LG"}
{"created":"2024-05-28 14:02:44","title":"Mutation-Bias Learning in Games","abstract":"We present two variants of a multi-agent reinforcement learning algorithm based on evolutionary game theoretic considerations. The intentional simplicity of one variant enables us to prove results on its relationship to a system of ordinary differential equations of replicator-mutator dynamics type, allowing us to present proofs on the algorithm's convergence conditions in various settings via its ODE counterpart. The more complicated variant enables comparisons to Q-learning based algorithms. We compare both variants experimentally to WoLF-PHC and frequency-adjusted Q-learning on a range of settings, illustrating cases of increasing dimensionality where our variants preserve convergence in contrast to more complicated algorithms. The availability of analytic results provides a degree of transferability of results as compared to purely empirical case studies, illustrating the general utility of a dynamical systems perspective on multi-agent reinforcement learning when addressing questions of convergence and reliable generalisation.","sentences":["We present two variants of a multi-agent reinforcement learning algorithm based on evolutionary game theoretic considerations.","The intentional simplicity of one variant enables us to prove results on its relationship to a system of ordinary differential equations of replicator-mutator dynamics type, allowing us to present proofs on the algorithm's convergence conditions in various settings via its ODE counterpart.","The more complicated variant enables comparisons to Q-learning based algorithms.","We compare both variants experimentally to WoLF-PHC and frequency-adjusted Q-learning on a range of settings, illustrating cases of increasing dimensionality where our variants preserve convergence in contrast to more complicated algorithms.","The availability of analytic results provides a degree of transferability of results as compared to purely empirical case studies, illustrating the general utility of a dynamical systems perspective on multi-agent reinforcement learning when addressing questions of convergence and reliable generalisation."],"url":"http://arxiv.org/abs/2405.18190v1","category":"cs.LG"}
{"created":"2024-05-28 14:02:08","title":"On walk-regular graphs and optimal duals of frames generated by graphs","abstract":"Erasures are a common problem that arises while signals or data are being transmitted. A profound challenge in frame theory is to find the optimal dual frames ($OD$-frames) to minimize the reconstruction error if erasures occur. In this paper, we study the optimal duals of frames generated by graphs. First, we characterize walk-regular graphs. Then, it is shown that the diagonal entries of the Moore-Penrose inverse of the Laplacian matrix (or adjacency matrix) of a walk-regular graph are equal. Besides, we prove that connected graphs generate full spark frames. Using these results, we establish that the canonical dual frames are the unique $OD$-frames of a frame generated by a walk-regular graph. A sufficient condition under which the canonical dual frame is the unique $OD$-frame is known. Here, we establish that the condition is also necessary if the frame is generated by a connected graph.","sentences":["Erasures are a common problem that arises while signals or data are being transmitted.","A profound challenge in frame theory is to find the optimal dual frames ($OD$-frames) to minimize the reconstruction error if erasures occur.","In this paper, we study the optimal duals of frames generated by graphs.","First, we characterize walk-regular graphs.","Then, it is shown that the diagonal entries of the Moore-Penrose inverse of the Laplacian matrix (or adjacency matrix) of a walk-regular graph are equal.","Besides, we prove that connected graphs generate full spark frames.","Using these results, we establish that the canonical dual frames are the unique $OD$-frames of a frame generated by a walk-regular graph.","A sufficient condition under which the canonical dual frame is the unique $OD$-frame is known.","Here, we establish that the condition is also necessary if the frame is generated by a connected graph."],"url":"http://arxiv.org/abs/2405.18189v1","category":"math.CO"}
{"created":"2024-05-28 13:54:48","title":"A blended physics-based and black-box identification approach for spacecraft inertia estimation -- EXTENDED VERSION","abstract":"In this paper, the problem of identifying inertial characteristics of a generic space vehicle relying on the physical and structural insights of the dynamical system is presented. To this aim, we exploit a recently introduced framework for the identification of physical parameters directly feeding the measurements into a backpropagation-like learning algorithm. In particular, this paper extends this approach by introducing a recursive algorithm that combines physics-based and black-box techniques to enhance accuracy and reliability in estimating spacecraft inertia. We demonstrate through numerical results that, relying on the derived algorithm to identify the inertia tensor of a nanosatellite, we can achieve improved estimation accuracy and robustness, by integrating physical constraints and leveraging partial knowledge of the system dynamics. In particular, we show how it is possible to enhance the convergence of the physics-based algorithm to the true values by either overparametrization or introducing a black-box term that captures the unmodelled dynamics related to the off-diagonal components.","sentences":["In this paper, the problem of identifying inertial characteristics of a generic space vehicle relying on the physical and structural insights of the dynamical system is presented.","To this aim, we exploit a recently introduced framework for the identification of physical parameters directly feeding the measurements into a backpropagation-like learning algorithm.","In particular, this paper extends this approach by introducing a recursive algorithm that combines physics-based and black-box techniques to enhance accuracy and reliability in estimating spacecraft inertia.","We demonstrate through numerical results that, relying on the derived algorithm to identify the inertia tensor of a nanosatellite, we can achieve improved estimation accuracy and robustness, by integrating physical constraints and leveraging partial knowledge of the system dynamics.","In particular, we show how it is possible to enhance the convergence of the physics-based algorithm to the true values by either overparametrization or introducing a black-box term that captures the unmodelled dynamics related to the off-diagonal components."],"url":"http://arxiv.org/abs/2405.18186v1","category":"eess.SY"}
{"created":"2024-05-28 13:50:20","title":"Three-body Forces in Oscillator Bases Expansion","abstract":"The oscillator bases expansion stands as an efficient approximation method for the time-independent Schr\\\"odinger equation. The method, originally formulated with one non-linear variational parameter, can be extended to incorporate two such parameters. It handles both non- and semi-relativistic kinematics with generic two-body interactions. In the current work, focusing on systems of three identical bodies, the method is generalised to include the management of a given class of three-body forces. The computational cost of this generalisation proves to not exceed the one for two-body interactions. The accuracy of the generalisation is assessed by comparing with results from Lagrange mesh method and hyperspherical harmonic expansions. Extensions for systems of $N$ identical bodies and for systems of two identical particles and one distinct are also discussed.","sentences":["The oscillator bases expansion stands as an efficient approximation method for the time-independent Schr\\\"odinger equation.","The method, originally formulated with one non-linear variational parameter, can be extended to incorporate two such parameters.","It handles both non- and semi-relativistic kinematics with generic two-body interactions.","In the current work, focusing on systems of three identical bodies, the method is generalised to include the management of a given class of three-body forces.","The computational cost of this generalisation proves to not exceed the one for two-body interactions.","The accuracy of the generalisation is assessed by comparing with results from Lagrange mesh method and hyperspherical harmonic expansions.","Extensions for systems of $N$ identical bodies and for systems of two identical particles and one distinct are also discussed."],"url":"http://arxiv.org/abs/2405.18184v1","category":"quant-ph"}
{"created":"2024-05-28 13:47:21","title":"Safe Reinforcement Learning in Black-Box Environments via Adaptive Shielding","abstract":"Empowering safe exploration of reinforcement learning (RL) agents during training is a critical impediment towards deploying RL agents in many real-world scenarios. Training RL agents in unknown, black-box environments poses an even greater safety risk when prior knowledge of the domain/task is unavailable. We introduce ADVICE (Adaptive Shielding with a Contrastive Autoencoder), a novel post-shielding technique that distinguishes safe and unsafe features of state-action pairs during training, thus protecting the RL agent from executing actions that yield potentially hazardous outcomes. Our comprehensive experimental evaluation against state-of-the-art safe RL exploration techniques demonstrates how ADVICE can significantly reduce safety violations during training while maintaining a competitive outcome reward.","sentences":["Empowering safe exploration of reinforcement learning (RL) agents during training is a critical impediment towards deploying RL agents in many real-world scenarios.","Training RL agents in unknown, black-box environments poses an even greater safety risk when prior knowledge of the domain/task is unavailable.","We introduce ADVICE (Adaptive Shielding with a Contrastive Autoencoder), a novel post-shielding technique that distinguishes safe and unsafe features of state-action pairs during training, thus protecting the RL agent from executing actions that yield potentially hazardous outcomes.","Our comprehensive experimental evaluation against state-of-the-art safe RL exploration techniques demonstrates how ADVICE can significantly reduce safety violations during training while maintaining a competitive outcome reward."],"url":"http://arxiv.org/abs/2405.18180v1","category":"cs.AI"}
{"created":"2024-05-28 13:46:22","title":"Rethinking the A in STEAM: Insights from and for AI Literacy Education","abstract":"This article rethinks the role of arts in STEAM education, emphasizing its importance in AI literacy within K-12 contexts. Arguing against the marginalization of arts, the paper is structured around four key domains: language studies, philosophy, social studies, and visual arts. Each section addresses critical AI-related phenomena and provides pedagogical strate-gies for effective integration into STEAM education. Language studies focus on media representations and the probabilistic nature of AI language models. The philosophy section examines anthropomorphism, ethics, and the misconstrued human-like capabilities of AI. Social studies discuss AI's societal impacts, biases, and ethical considerations in data prac-tices. Visual arts explore the implications of generative AI on artistic processes and intellec-tual property. The article concludes by advocating for a robust inclusion of arts in STEAM to foster a holistic, equitable, and sustainable understanding of AI, ultimately inspiring technologies that promote fairness and creativity.","sentences":["This article rethinks the role of arts in STEAM education, emphasizing its importance in AI literacy within K-12 contexts.","Arguing against the marginalization of arts, the paper is structured around four key domains: language studies, philosophy, social studies, and visual arts.","Each section addresses critical AI-related phenomena and provides pedagogical strate-gies for effective integration into STEAM education.","Language studies focus on media representations and the probabilistic nature of AI language models.","The philosophy section examines anthropomorphism, ethics, and the misconstrued human-like capabilities of AI.","Social studies discuss AI's societal impacts, biases, and ethical considerations in data prac-tices.","Visual arts explore the implications of generative AI on artistic processes and intellec-tual property.","The article concludes by advocating for a robust inclusion of arts in STEAM to foster a holistic, equitable, and sustainable understanding of AI, ultimately inspiring technologies that promote fairness and creativity."],"url":"http://arxiv.org/abs/2405.18179v1","category":"cs.CY"}
{"created":"2024-05-28 13:43:34","title":"SEMF: Supervised Expectation-Maximization Framework for Predicting Intervals","abstract":"This work introduces the Supervised Expectation-Maximization Framework (SEMF), a versatile and model-agnostic framework that generates prediction intervals for datasets with complete or missing data. SEMF extends the Expectation-Maximization (EM) algorithm, traditionally used in unsupervised learning, to a supervised context, enabling it to extract latent representations for uncertainty estimation. The framework demonstrates robustness through extensive empirical evaluation across 11 tabular datasets, achieving$\\unicode{x2013}$in some cases$\\unicode{x2013}$narrower normalized prediction intervals and higher coverage than traditional quantile regression methods. Furthermore, SEMF integrates seamlessly with existing machine learning algorithms, such as gradient-boosted trees and neural networks, exemplifying its usefulness for real-world applications. The experimental results highlight SEMF's potential to advance state-of-the-art techniques in uncertainty quantification.","sentences":["This work introduces the Supervised Expectation-Maximization Framework (SEMF), a versatile and model-agnostic framework that generates prediction intervals for datasets with complete or missing data.","SEMF extends the Expectation-Maximization (EM) algorithm, traditionally used in unsupervised learning, to a supervised context, enabling it to extract latent representations for uncertainty estimation.","The framework demonstrates robustness through extensive empirical evaluation across 11 tabular datasets, achieving$\\unicode{x2013}$in some cases$\\unicode{x2013}$narrower normalized prediction intervals and higher coverage than traditional quantile regression methods.","Furthermore, SEMF integrates seamlessly with existing machine learning algorithms, such as gradient-boosted trees and neural networks, exemplifying its usefulness for real-world applications.","The experimental results highlight SEMF's potential to advance state-of-the-art techniques in uncertainty quantification."],"url":"http://arxiv.org/abs/2405.18176v1","category":"stat.ML"}
{"created":"2024-05-28 13:36:31","title":"Crash Report Accumulation During Continuous Fuzzing","abstract":"Crash report accumulation is a necessary step during continuous fuzzing. Dynamic software analysis techniques like fuzzing and dynamic symbolic execution generate a large number of crashes for analysis. However, the time and resource constraints often lead to the postponement of fixing some less critical issues, potentially introducing new errors in future releases. Thus, there is a need to distinguish new errors from old ones. We propose a crash accumulation method and implemented it as part of the CASR toolset. We evaluated our approach on crash reports collected from fuzzing results.","sentences":["Crash report accumulation is a necessary step during continuous fuzzing.","Dynamic software analysis techniques like fuzzing and dynamic symbolic execution generate a large number of crashes for analysis.","However, the time and resource constraints often lead to the postponement of fixing some less critical issues, potentially introducing new errors in future releases.","Thus, there is a need to distinguish new errors from old ones.","We propose a crash accumulation method and implemented it as part of the CASR toolset.","We evaluated our approach on crash reports collected from fuzzing results."],"url":"http://arxiv.org/abs/2405.18174v1","category":"cs.CR"}
{"created":"2024-05-28 13:33:08","title":"AnyFit: Controllable Virtual Try-on for Any Combination of Attire Across Any Scenario","abstract":"While image-based virtual try-on has made significant strides, emerging approaches still fall short of delivering high-fidelity and robust fitting images across various scenarios, as their models suffer from issues of ill-fitted garment styles and quality degrading during the training process, not to mention the lack of support for various combinations of attire. Therefore, we first propose a lightweight, scalable, operator known as Hydra Block for attire combinations. This is achieved through a parallel attention mechanism that facilitates the feature injection of multiple garments from conditionally encoded branches into the main network. Secondly, to significantly enhance the model's robustness and expressiveness in real-world scenarios, we evolve its potential across diverse settings by synthesizing the residuals of multiple models, as well as implementing a mask region boost strategy to overcome the instability caused by information leakage in existing models. Equipped with the above design, AnyFit surpasses all baselines on high-resolution benchmarks and real-world data by a large gap, excelling in producing well-fitting garments replete with photorealistic and rich details. Furthermore, AnyFit's impressive performance on high-fidelity virtual try-ons in any scenario from any image, paves a new path for future research within the fashion community.","sentences":["While image-based virtual try-on has made significant strides, emerging approaches still fall short of delivering high-fidelity and robust fitting images across various scenarios, as their models suffer from issues of ill-fitted garment styles and quality degrading during the training process, not to mention the lack of support for various combinations of attire.","Therefore, we first propose a lightweight, scalable, operator known as Hydra Block for attire combinations.","This is achieved through a parallel attention mechanism that facilitates the feature injection of multiple garments from conditionally encoded branches into the main network.","Secondly, to significantly enhance the model's robustness and expressiveness in real-world scenarios, we evolve its potential across diverse settings by synthesizing the residuals of multiple models, as well as implementing a mask region boost strategy to overcome the instability caused by information leakage in existing models.","Equipped with the above design, AnyFit surpasses all baselines on high-resolution benchmarks and real-world data by a large gap, excelling in producing well-fitting garments replete with photorealistic and rich details.","Furthermore, AnyFit's impressive performance on high-fidelity virtual try-ons in any scenario from any image, paves a new path for future research within the fashion community."],"url":"http://arxiv.org/abs/2405.18172v1","category":"cs.CV"}
{"created":"2024-05-28 13:30:06","title":"An extension of the cogrowth formula to arbitrary subsets of the tree","abstract":"What is the probability that a random walk in the free group ends in a proper power? Or in a primitive element? We present a formula that computes the exponential decay rate of the probability that a random walk on a regular tree ends in a given subset, in terms of the exponential decay rate of the analogous probability of the non-backtracking random walk. This generalizes the well-known cogrowth formula of Grigorchuk, Cohen and Northshield.","sentences":["What is the probability that a random walk in the free group ends in a proper power?","Or in a primitive element?","We present a formula that computes the exponential decay rate of the probability that a random walk on a regular tree ends in a given subset, in terms of the exponential decay rate of the analogous probability of the non-backtracking random walk.","This generalizes the well-known cogrowth formula of Grigorchuk, Cohen and Northshield."],"url":"http://arxiv.org/abs/2405.18169v1","category":"math.PR"}
{"created":"2024-05-28 13:27:30","title":"Confidence-aware multi-modality learning for eye disease screening","abstract":"Multi-modal ophthalmic image classification plays a key role in diagnosing eye diseases, as it integrates information from different sources to complement their respective performances. However, recent improvements have mainly focused on accuracy, often neglecting the importance of confidence and robustness in predictions for diverse modalities. In this study, we propose a novel multi-modality evidential fusion pipeline for eye disease screening. It provides a measure of confidence for each modality and elegantly integrates the multi-modality information using a multi-distribution fusion perspective. Specifically, our method first utilizes normal inverse gamma prior distributions over pre-trained models to learn both aleatoric and epistemic uncertainty for uni-modality. Then, the normal inverse gamma distribution is analyzed as the Student's t distribution. Furthermore, within a confidence-aware fusion framework, we propose a mixture of Student's t distributions to effectively integrate different modalities, imparting the model with heavy-tailed properties and enhancing its robustness and reliability. More importantly, the confidence-aware multi-modality ranking regularization term induces the model to more reasonably rank the noisy single-modal and fused-modal confidence, leading to improved reliability and accuracy. Experimental results on both public and internal datasets demonstrate that our model excels in robustness, particularly in challenging scenarios involving Gaussian noise and modality missing conditions. Moreover, our model exhibits strong generalization capabilities to out-of-distribution data, underscoring its potential as a promising solution for multimodal eye disease screening.","sentences":["Multi-modal ophthalmic image classification plays a key role in diagnosing eye diseases, as it integrates information from different sources to complement their respective performances.","However, recent improvements have mainly focused on accuracy, often neglecting the importance of confidence and robustness in predictions for diverse modalities.","In this study, we propose a novel multi-modality evidential fusion pipeline for eye disease screening.","It provides a measure of confidence for each modality and elegantly integrates the multi-modality information using a multi-distribution fusion perspective.","Specifically, our method first utilizes normal inverse gamma prior distributions over pre-trained models to learn both aleatoric and epistemic uncertainty for uni-modality.","Then, the normal inverse gamma distribution is analyzed as the Student's t distribution.","Furthermore, within a confidence-aware fusion framework, we propose a mixture of Student's t distributions to effectively integrate different modalities, imparting the model with heavy-tailed properties and enhancing its robustness and reliability.","More importantly, the confidence-aware multi-modality ranking regularization term induces the model to more reasonably rank the noisy single-modal and fused-modal confidence, leading to improved reliability and accuracy.","Experimental results on both public and internal datasets demonstrate that our model excels in robustness, particularly in challenging scenarios involving Gaussian noise and modality missing conditions.","Moreover, our model exhibits strong generalization capabilities to out-of-distribution data, underscoring its potential as a promising solution for multimodal eye disease screening."],"url":"http://arxiv.org/abs/2405.18167v1","category":"eess.IV"}
{"created":"2024-05-28 13:26:12","title":"Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing","abstract":"Large language models (LLMs) are increasingly being adopted in a wide range of real-world applications. Despite their impressive performance, recent studies have shown that LLMs are vulnerable to deliberately crafted adversarial prompts even when aligned via Reinforcement Learning from Human Feedback or supervised fine-tuning. While existing defense methods focus on either detecting harmful prompts or reducing the likelihood of harmful responses through various means, defending LLMs against jailbreak attacks based on the inner mechanisms of LLMs remains largely unexplored. In this work, we investigate how LLMs response to harmful prompts and propose a novel defense method termed \\textbf{L}ayer-specific \\textbf{Ed}iting (LED) to enhance the resilience of LLMs against jailbreak attacks. Through LED, we reveal that several critical \\textit{safety layers} exist among the early layers of LLMs. We then show that realigning these safety layers (and some selected additional layers) with the decoded safe response from selected target layers can significantly improve the alignment of LLMs against jailbreak attacks. Extensive experiments across various LLMs (e.g., Llama2, Mistral) show the effectiveness of LED, which effectively defends against jailbreak attacks while maintaining performance on benign prompts. Our code is available at \\url{https://github.com/ledllm/ledllm}.","sentences":["Large language models (LLMs) are increasingly being adopted in a wide range of real-world applications.","Despite their impressive performance, recent studies have shown that LLMs are vulnerable to deliberately crafted adversarial prompts even when aligned via Reinforcement Learning from Human Feedback or supervised fine-tuning.","While existing defense methods focus on either detecting harmful prompts or reducing the likelihood of harmful responses through various means, defending LLMs against jailbreak attacks based on the inner mechanisms of LLMs remains largely unexplored.","In this work, we investigate how LLMs response to harmful prompts and propose a novel defense method termed \\textbf{L}ayer-specific \\textbf{Ed}iting (LED) to enhance the resilience of LLMs against jailbreak attacks.","Through LED, we reveal that several critical \\textit{safety layers} exist among the early layers of LLMs.","We then show that realigning these safety layers (and some selected additional layers) with the decoded safe response from selected target layers can significantly improve the alignment of LLMs against jailbreak attacks.","Extensive experiments across various LLMs (e.g., Llama2, Mistral) show the effectiveness of LED, which effectively defends against jailbreak attacks while maintaining performance on benign prompts.","Our code is available at \\url{https://github.com/ledllm/ledllm}."],"url":"http://arxiv.org/abs/2405.18166v1","category":"cs.AI"}
{"created":"2024-05-28 13:25:31","title":"Time Series Representation Models","abstract":"Time series analysis remains a major challenge due to its sparse characteristics, high dimensionality, and inconsistent data quality. Recent advancements in transformer-based techniques have enhanced capabilities in forecasting and imputation; however, these methods are still resource-heavy, lack adaptability, and face difficulties in integrating both local and global attributes of time series. To tackle these challenges, we propose a new architectural concept for time series analysis based on introspection. Central to this concept is the self-supervised pretraining of Time Series Representation Models (TSRMs), which once learned can be easily tailored and fine-tuned for specific tasks, such as forecasting and imputation, in an automated and resource-efficient manner. Our architecture is equipped with a flexible and hierarchical representation learning process, which is robust against missing data and outliers. It can capture and learn both local and global features of the structure, semantics, and crucial patterns of a given time series category, such as heart rate data. Our learned time series representation models can be efficiently adapted to a specific task, such as forecasting or imputation, without manual intervention. Furthermore, our architecture's design supports explainability by highlighting the significance of each input value for the task at hand. Our empirical study using four benchmark datasets shows that, compared to investigated state-of-the-art baseline methods, our architecture improves imputation and forecasting errors by up to 90.34% and 71.54%, respectively, while reducing the required trainable parameters by up to 92.43%. The source code is available at https://github.com/RobertLeppich/TSRM.","sentences":["Time series analysis remains a major challenge due to its sparse characteristics, high dimensionality, and inconsistent data quality.","Recent advancements in transformer-based techniques have enhanced capabilities in forecasting and imputation; however, these methods are still resource-heavy, lack adaptability, and face difficulties in integrating both local and global attributes of time series.","To tackle these challenges, we propose a new architectural concept for time series analysis based on introspection.","Central to this concept is the self-supervised pretraining of Time Series Representation Models (TSRMs), which once learned can be easily tailored and fine-tuned for specific tasks, such as forecasting and imputation, in an automated and resource-efficient manner.","Our architecture is equipped with a flexible and hierarchical representation learning process, which is robust against missing data and outliers.","It can capture and learn both local and global features of the structure, semantics, and crucial patterns of a given time series category, such as heart rate data.","Our learned time series representation models can be efficiently adapted to a specific task, such as forecasting or imputation, without manual intervention.","Furthermore, our architecture's design supports explainability by highlighting the significance of each input value for the task at hand.","Our empirical study using four benchmark datasets shows that, compared to investigated state-of-the-art baseline methods, our architecture improves imputation and forecasting errors by up to 90.34% and 71.54%, respectively, while reducing the required trainable parameters by up to 92.43%.","The source code is available at https://github.com/RobertLeppich/TSRM."],"url":"http://arxiv.org/abs/2405.18165v1","category":"cs.LG"}
{"created":"2024-05-28 13:23:04","title":"Back to the Drawing Board for Fair Representation Learning","abstract":"The goal of Fair Representation Learning (FRL) is to mitigate biases in machine learning models by learning data representations that enable high accuracy on downstream tasks while minimizing discrimination based on sensitive attributes. The evaluation of FRL methods in many recent works primarily focuses on the tradeoff between downstream fairness and accuracy with respect to a single task that was used to approximate the utility of representations during training (proxy task). This incentivizes retaining only features relevant to the proxy task while discarding all other information. In extreme cases, this can cause the learned representations to collapse to a trivial, binary value, rendering them unusable in transfer settings. In this work, we argue that this approach is fundamentally mismatched with the original motivation of FRL, which arises from settings with many downstream tasks unknown at training time (transfer tasks). To remedy this, we propose to refocus the evaluation protocol of FRL methods primarily around the performance on transfer tasks. A key challenge when conducting such an evaluation is the lack of adequate benchmarks. We address this by formulating four criteria that a suitable evaluation procedure should fulfill. Based on these, we propose TransFair, a benchmark that satisfies these criteria, consisting of novel variations of popular FRL datasets with carefully calibrated transfer tasks. In this setting, we reevaluate state-of-the-art FRL methods, observing that they often overfit to the proxy task, which causes them to underperform on certain transfer tasks. We further highlight the importance of task-agnostic learning signals for FRL methods, as they can lead to more transferrable representations.","sentences":["The goal of Fair Representation Learning (FRL) is to mitigate biases in machine learning models by learning data representations that enable high accuracy on downstream tasks while minimizing discrimination based on sensitive attributes.","The evaluation of FRL methods in many recent works primarily focuses on the tradeoff between downstream fairness and accuracy with respect to a single task that was used to approximate the utility of representations during training (proxy task).","This incentivizes retaining only features relevant to the proxy task while discarding all other information.","In extreme cases, this can cause the learned representations to collapse to a trivial, binary value, rendering them unusable in transfer settings.","In this work, we argue that this approach is fundamentally mismatched with the original motivation of FRL, which arises from settings with many downstream tasks unknown at training time (transfer tasks).","To remedy this, we propose to refocus the evaluation protocol of FRL methods primarily around the performance on transfer tasks.","A key challenge when conducting such an evaluation is the lack of adequate benchmarks.","We address this by formulating four criteria that a suitable evaluation procedure should fulfill.","Based on these, we propose TransFair, a benchmark that satisfies these criteria, consisting of novel variations of popular FRL datasets with carefully calibrated transfer tasks.","In this setting, we reevaluate state-of-the-art FRL methods, observing that they often overfit to the proxy task, which causes them to underperform on certain transfer tasks.","We further highlight the importance of task-agnostic learning signals for FRL methods, as they can lead to more transferrable representations."],"url":"http://arxiv.org/abs/2405.18161v1","category":"cs.LG"}
{"created":"2024-05-28 13:21:30","title":"The impact of gravitational waveform model systematics on the measurement of the Hubble Constant","abstract":"Matching gravitational-wave observations of binary neutron stars with theoretical model predictions reveals important information about the sources, such as the masses and the distance to the stars. The latter can be used to determine the Hubble Constant, the rate at which the Universe expands. One general problem of all astrophysical measurements is that theoretical models only approximate the real underlying physics, which can lead to systematic uncertainties introducing biases. However, the extent of this bias for the distance measurement due to uncertainties of gravitational waveform models is unknown. In this study, we analyze a synthetic population of 38 binary neutron star sources measured with Advanced LIGO and Advanced Virgo at design sensitivity. We employ a set of four different waveform models and estimate model-dependent systematic biases on the extraction of the Hubble Constant using the bright siren method. Our results indicate that systematic biases are below statistical uncertainties for the current generation of gravitational wave detectors.","sentences":["Matching gravitational-wave observations of binary neutron stars with theoretical model predictions reveals important information about the sources, such as the masses and the distance to the stars.","The latter can be used to determine the Hubble Constant, the rate at which the Universe expands.","One general problem of all astrophysical measurements is that theoretical models only approximate the real underlying physics, which can lead to systematic uncertainties introducing biases.","However, the extent of this bias for the distance measurement due to uncertainties of gravitational waveform models is unknown.","In this study, we analyze a synthetic population of 38 binary neutron star sources measured with Advanced LIGO and Advanced Virgo at design sensitivity.","We employ a set of four different waveform models and estimate model-dependent systematic biases on the extraction of the Hubble Constant using the bright siren method.","Our results indicate that systematic biases are below statistical uncertainties for the current generation of gravitational wave detectors."],"url":"http://arxiv.org/abs/2405.18158v1","category":"gr-qc"}
{"created":"2024-05-28 13:19:00","title":"Some ergodic theorems over squarefree numbers","abstract":"In 2022, Bergelson and Richter gave a new dynamical generalization of the prime number theorem by establishing an ergodic theorem along the number of prime factors of integers. They also showed that this generalization holds as well if the integers are restricted to be squarefree. In this paper, we present the concept of invariant averages under multiplications for arithmetic functions. Utilizing the properties of these invariant averages, we derive several ergodic theorems over squarefree numbers. These theorems have significant connections to the Erd\\H{o}s-Kac Theorem, the Bergelson-Richter Theorem, and the Loyd Theorem.","sentences":["In 2022, Bergelson and Richter gave a new dynamical generalization of the prime number theorem by establishing an ergodic theorem along the number of prime factors of integers.","They also showed that this generalization holds as well if the integers are restricted to be squarefree.","In this paper, we present the concept of invariant averages under multiplications for arithmetic functions.","Utilizing the properties of these invariant averages, we derive several ergodic theorems over squarefree numbers.","These theorems have significant connections to the Erd\\H{o}s-Kac Theorem, the Bergelson-Richter Theorem, and the Loyd Theorem."],"url":"http://arxiv.org/abs/2405.18157v1","category":"math.NT"}
{"created":"2024-05-28 13:18:32","title":"VividPose: Advancing Stable Video Diffusion for Realistic Human Image Animation","abstract":"Human image animation involves generating a video from a static image by following a specified pose sequence. Current approaches typically adopt a multi-stage pipeline that separately learns appearance and motion, which often leads to appearance degradation and temporal inconsistencies. To address these issues, we propose VividPose, an innovative end-to-end pipeline based on Stable Video Diffusion (SVD) that ensures superior temporal stability. To enhance the retention of human identity, we propose an identity-aware appearance controller that integrates additional facial information without compromising other appearance details such as clothing texture and background. This approach ensures that the generated videos maintain high fidelity to the identity of human subject, preserving key facial features across various poses. To accommodate diverse human body shapes and hand movements, we introduce a geometry-aware pose controller that utilizes both dense rendering maps from SMPL-X and sparse skeleton maps. This enables accurate alignment of pose and shape in the generated videos, providing a robust framework capable of handling a wide range of body shapes and dynamic hand movements. Extensive qualitative and quantitative experiments on the UBCFashion and TikTok benchmarks demonstrate that our method achieves state-of-the-art performance. Furthermore, VividPose exhibits superior generalization capabilities on our proposed in-the-wild dataset. Codes and models will be available.","sentences":["Human image animation involves generating a video from a static image by following a specified pose sequence.","Current approaches typically adopt a multi-stage pipeline that separately learns appearance and motion, which often leads to appearance degradation and temporal inconsistencies.","To address these issues, we propose VividPose, an innovative end-to-end pipeline based on Stable Video Diffusion (SVD) that ensures superior temporal stability.","To enhance the retention of human identity, we propose an identity-aware appearance controller that integrates additional facial information without compromising other appearance details such as clothing texture and background.","This approach ensures that the generated videos maintain high fidelity to the identity of human subject, preserving key facial features across various poses.","To accommodate diverse human body shapes and hand movements, we introduce a geometry-aware pose controller that utilizes both dense rendering maps from SMPL-X and sparse skeleton maps.","This enables accurate alignment of pose and shape in the generated videos, providing a robust framework capable of handling a wide range of body shapes and dynamic hand movements.","Extensive qualitative and quantitative experiments on the UBCFashion and TikTok benchmarks demonstrate that our method achieves state-of-the-art performance.","Furthermore, VividPose exhibits superior generalization capabilities on our proposed in-the-wild dataset.","Codes and models will be available."],"url":"http://arxiv.org/abs/2405.18156v1","category":"cs.CV"}
{"created":"2024-05-28 13:16:10","title":"On Bounded Advice Classes","abstract":"Advice classes in computational complexity have frequently been used to model real-world scenarios encountered in cryptography, quantum computing and machine learning, where some computational task may be broken down into a preprocessing and deployment phase, each associated with a different complexity. However, in these scenarios, the advice given by the preprocessing phase must still be generated by some (albeit more powerful) bounded machine, which is not the case in conventional advice classes. To better model these cases we develop `bounded advice classes', where a more powerful Turing machine generates advice for another, less powerful, Turing machine. We then focus on the question of when various classes generate useful advice, to answer this we connect bounded advice to unary languages. This connection allows us to state various conditional and unconditional results on the utility of advice generated by $\\mathsf{EXP}$, $\\mathsf{NP}$, $\\mathsf{BQP}$, $\\mathsf{PSPACE}$, and more. We study the relations between bounded advice classes, quantum bounded advice classes, and randomised bounded advice. We also examine how each of these concepts interact with recently introduced classes, like $\\mathsf{BPP/samp}$. Our results also improve the state of the art in existing research on the complexity of advice functions.","sentences":["Advice classes in computational complexity have frequently been used to model real-world scenarios encountered in cryptography, quantum computing and machine learning, where some computational task may be broken down into a preprocessing and deployment phase, each associated with a different complexity.","However, in these scenarios, the advice given by the preprocessing phase must still be generated by some (albeit more powerful) bounded machine, which is not the case in conventional advice classes.","To better model these cases we develop `bounded advice classes', where a more powerful Turing machine generates advice for another, less powerful, Turing machine.","We then focus on the question of when various classes generate useful advice, to answer this we connect bounded advice to unary languages.","This connection allows us to state various conditional and unconditional results on the utility of advice generated by $\\mathsf{EXP}$, $\\mathsf{NP}$, $\\mathsf{BQP}$, $\\mathsf{PSPACE}$, and more.","We study the relations between bounded advice classes, quantum bounded advice classes, and randomised bounded advice.","We also examine how each of these concepts interact with recently introduced classes, like $\\mathsf{BPP/samp}$. Our results also improve the state of the art in existing research on the complexity of advice functions."],"url":"http://arxiv.org/abs/2405.18155v1","category":"cs.CC"}
{"created":"2024-05-28 13:14:26","title":"Practical aspects for the creation of an audio dataset from field recordings with optimized labeling budget with AI-assisted strategy","abstract":"Machine Listening focuses on developing technologies to extract relevant information from audio signals. A critical aspect of these projects is the acquisition and labeling of contextualized data, which is inherently complex and requires specific resources and strategies. Despite the availability of some audio datasets, many are unsuitable for commercial applications. The paper emphasizes the importance of Active Learning (AL) using expert labelers over crowdsourcing, which often lacks detailed insights into dataset structures. AL is an iterative process combining human labelers and AI models to optimize the labeling budget by intelligently selecting samples for human review. This approach addresses the challenge of handling large, constantly growing datasets that exceed available computational resources and memory. The paper presents a comprehensive data-centric framework for Machine Listening projects, detailing the configuration of recording nodes, database structure, and labeling budget optimization in resource-constrained scenarios. Applied to an industrial port in Valencia, Spain, the framework successfully labeled 6540 ten-second audio samples over five months with a small team, demonstrating its effectiveness and adaptability to various resource availability situations.","sentences":["Machine Listening focuses on developing technologies to extract relevant information from audio signals.","A critical aspect of these projects is the acquisition and labeling of contextualized data, which is inherently complex and requires specific resources and strategies.","Despite the availability of some audio datasets, many are unsuitable for commercial applications.","The paper emphasizes the importance of Active Learning (AL) using expert labelers over crowdsourcing, which often lacks detailed insights into dataset structures.","AL is an iterative process combining human labelers and AI models to optimize the labeling budget by intelligently selecting samples for human review.","This approach addresses the challenge of handling large, constantly growing datasets that exceed available computational resources and memory.","The paper presents a comprehensive data-centric framework for Machine Listening projects, detailing the configuration of recording nodes, database structure, and labeling budget optimization in resource-constrained scenarios.","Applied to an industrial port in Valencia, Spain, the framework successfully labeled 6540 ten-second audio samples over five months with a small team, demonstrating its effectiveness and adaptability to various resource availability situations."],"url":"http://arxiv.org/abs/2405.18153v1","category":"cs.SD"}
{"created":"2024-05-28 13:13:02","title":"A geometric approach to functional equations for general multiple Dirichlet series over function fields","abstract":"Sawin recently gave an axiomatic characterization of multiple Dirichlet series over the function field $\\mathbb{F}_{q}(T)$ and proved their existence by exhibiting the coefficients as trace functions of specific perverse sheaves. However, he did not prove that these series actually converge anywhere, instead treating them as formal power series.   In this paper, we prove that these series do converge in a certain region, and moreover that the functions obtained by analytically continuing them satisfy functional equations.   For convergence, it suffices to obtain bounds on the coefficients, for which we use the decomposition theorem for perverse sheaves, in combination with the Kontsevich moduli space of stable maps to construct a suitable compactification.   For the functional equations, the key identity is a multi-variable generalization of the relationship between a Dirichlet character and its Fourier transform; in the multiple Dirichlet series setting, this uses a density trick for simple perverse sheaves and an explicit formula for intermediate extensions from the complement of a normal crossings divisor.","sentences":["Sawin recently gave an axiomatic characterization of multiple Dirichlet series over the function field $\\mathbb{F}_{q}(T)$ and proved their existence by exhibiting the coefficients as trace functions of specific perverse sheaves.","However, he did not prove that these series actually converge anywhere, instead treating them as formal power series.   ","In this paper, we prove that these series do converge in a certain region, and moreover that the functions obtained by analytically continuing them satisfy functional equations.   ","For convergence, it suffices to obtain bounds on the coefficients, for which we use the decomposition theorem for perverse sheaves, in combination with the Kontsevich moduli space of stable maps to construct a suitable compactification.   ","For the functional equations, the key identity is a multi-variable generalization of the relationship between a Dirichlet character and its Fourier transform; in the multiple Dirichlet series setting, this uses a density trick for simple perverse sheaves and an explicit formula for intermediate extensions from the complement of a normal crossings divisor."],"url":"http://arxiv.org/abs/2405.18152v1","category":"math.NT"}
{"created":"2024-05-28 13:07:35","title":"Learning to Detour: Shortcut Mitigating Augmentation for Weakly Supervised Semantic Segmentation","abstract":"Weakly supervised semantic segmentation (WSSS) employing weak forms of labels has been actively studied to alleviate the annotation cost of acquiring pixel-level labels. However, classifiers trained on biased datasets tend to exploit shortcut features and make predictions based on spurious correlations between certain backgrounds and objects, leading to a poor generalization performance. In this paper, we propose shortcut mitigating augmentation (SMA) for WSSS, which generates synthetic representations of object-background combinations not seen in the training data to reduce the use of shortcut features. Our approach disentangles the object-relevant and background features. We then shuffle and combine the disentangled representations to create synthetic features of diverse object-background combinations. SMA-trained classifier depends less on contexts and focuses more on the target object when making predictions. In addition, we analyzed the behavior of the classifier on shortcut usage after applying our augmentation using an attribution method-based metric. The proposed method achieved the improved performance of semantic segmentation result on PASCAL VOC 2012 and MS COCO 2014 datasets.","sentences":["Weakly supervised semantic segmentation (WSSS) employing weak forms of labels has been actively studied to alleviate the annotation cost of acquiring pixel-level labels.","However, classifiers trained on biased datasets tend to exploit shortcut features and make predictions based on spurious correlations between certain backgrounds and objects, leading to a poor generalization performance.","In this paper, we propose shortcut mitigating augmentation (SMA) for WSSS, which generates synthetic representations of object-background combinations not seen in the training data to reduce the use of shortcut features.","Our approach disentangles the object-relevant and background features.","We then shuffle and combine the disentangled representations to create synthetic features of diverse object-background combinations.","SMA-trained classifier depends less on contexts and focuses more on the target object when making predictions.","In addition, we analyzed the behavior of the classifier on shortcut usage after applying our augmentation using an attribution method-based metric.","The proposed method achieved the improved performance of semantic segmentation result on PASCAL VOC 2012 and MS COCO 2014 datasets."],"url":"http://arxiv.org/abs/2405.18148v1","category":"cs.CV"}
{"created":"2024-05-28 13:02:56","title":"4-bit Shampoo for Memory-Efficient Network Training","abstract":"Second-order optimizers, maintaining a matrix termed a preconditioner, are superior to first-order optimizers in both theory and practice. The states forming the preconditioner and its inverse root restrict the maximum size of models trained by second-order optimizers. To address this, compressing 32-bit optimizer states to lower bitwidths has shown promise in reducing memory usage. However, current approaches only pertain to first-order optimizers. In this paper, we propose the first 4-bit second-order optimizers, exemplified by 4-bit Shampoo, maintaining performance similar to that of 32-bit ones. We show that quantizing the eigenvector matrix of the preconditioner in 4-bit Shampoo is remarkably better than quantizing the preconditioner itself both theoretically and experimentally. By rectifying the orthogonality of the quantized eigenvector matrix, we enhance the approximation of the preconditioner's eigenvector matrix, which also benefits the computation of its inverse 4-th root. Besides, we find that linear square quantization slightly outperforms dynamic tree quantization when quantizing second-order optimizer states. Evaluation on various networks for image classification demonstrates that our 4-bit Shampoo achieves comparable test accuracy to its 32-bit counterpart while being more memory-efficient. The source code will be made available.","sentences":["Second-order optimizers, maintaining a matrix termed a preconditioner, are superior to first-order optimizers in both theory and practice.","The states forming the preconditioner and its inverse root restrict the maximum size of models trained by second-order optimizers.","To address this, compressing 32-bit optimizer states to lower bitwidths has shown promise in reducing memory usage.","However, current approaches only pertain to first-order optimizers.","In this paper, we propose the first 4-bit second-order optimizers, exemplified by 4-bit Shampoo, maintaining performance similar to that of 32-bit ones.","We show that quantizing the eigenvector matrix of the preconditioner in 4-bit Shampoo is remarkably better than quantizing the preconditioner itself both theoretically and experimentally.","By rectifying the orthogonality of the quantized eigenvector matrix, we enhance the approximation of the preconditioner's eigenvector matrix, which also benefits the computation of its inverse 4-th root.","Besides, we find that linear square quantization slightly outperforms dynamic tree quantization when quantizing second-order optimizer states.","Evaluation on various networks for image classification demonstrates that our 4-bit Shampoo achieves comparable test accuracy to its 32-bit counterpart while being more memory-efficient.","The source code will be made available."],"url":"http://arxiv.org/abs/2405.18144v1","category":"cs.LG"}
{"created":"2024-05-28 12:58:32","title":"Lagrange's planetary equations with time-dependent secular perturbations","abstract":"The long-term evolution of astrophysical systems is driven by a Hamiltonian that is independent of the fast angle. As this Hamiltonian may contain explicitly time-dependent parameters, the conservation of mechanical energy is not guaranteed in such systems. We derive how the semi-major axis evolves in these cases. We analyze two astrophysically interesting examples, those of the harmonic and quadrupole perturbations.","sentences":["The long-term evolution of astrophysical systems is driven by a Hamiltonian that is independent of the fast angle.","As this Hamiltonian may contain explicitly time-dependent parameters, the conservation of mechanical energy is not guaranteed in such systems.","We derive how the semi-major axis evolves in these cases.","We analyze two astrophysically interesting examples, those of the harmonic and quadrupole perturbations."],"url":"http://arxiv.org/abs/2405.18140v1","category":"astro-ph.EP"}
{"created":"2024-05-28 12:56:57","title":"Unlocking Futures: A Natural Language Driven Career Prediction System for Computer Science and Software Engineering Students","abstract":"A career is a crucial aspect for any person to fulfill their desires through hard work. During their studies, students cannot find the best career suggestions unless they receive meaningful guidance tailored to their skills. Therefore, we developed an AI-assisted model for early prediction to provide better career suggestions. Although the task is difficult, proper guidance can make it easier. Effective career guidance requires understanding a student's academic skills, interests, and skill-related activities. In this research, we collected essential information from Computer Science (CS) and Software Engineering (SWE) students to train a machine learning (ML) model that predicts career paths based on students' career-related information. To adequately train the models, we applied Natural Language Processing (NLP) techniques and completed dataset pre-processing. For comparative analysis, we utilized multiple classification ML algorithms and deep learning (DL) algorithms. This study contributes valuable insights to educational advising by providing specific career suggestions based on the unique features of CS and SWE students. Additionally, the research helps individual CS and SWE students find suitable jobs that match their skills, interests, and skill-related activities.","sentences":["A career is a crucial aspect for any person to fulfill their desires through hard work.","During their studies, students cannot find the best career suggestions unless they receive meaningful guidance tailored to their skills.","Therefore, we developed an AI-assisted model for early prediction to provide better career suggestions.","Although the task is difficult, proper guidance can make it easier.","Effective career guidance requires understanding a student's academic skills, interests, and skill-related activities.","In this research, we collected essential information from Computer Science (CS) and Software Engineering (SWE) students to train a machine learning (ML) model that predicts career paths based on students' career-related information.","To adequately train the models, we applied Natural Language Processing (NLP) techniques and completed dataset pre-processing.","For comparative analysis, we utilized multiple classification ML algorithms and deep learning (DL) algorithms.","This study contributes valuable insights to educational advising by providing specific career suggestions based on the unique features of CS and SWE students.","Additionally, the research helps individual CS and SWE students find suitable jobs that match their skills, interests, and skill-related activities."],"url":"http://arxiv.org/abs/2405.18139v1","category":"cs.AI"}
{"created":"2024-05-28 12:51:22","title":"The sharp quantitative barycentric isoperimetric inequality for bounded sets","abstract":"We prove the sharp quantitative isoperimetric inequality in the case of the barycentric asymmetry, for bounded sets. This generalizes the $2$-D case recently proved in~\\cite{BCH}.","sentences":["We prove the sharp quantitative isoperimetric inequality in the case of the barycentric asymmetry, for bounded sets.","This generalizes the $2$-D case recently proved in~\\cite{BCH}."],"url":"http://arxiv.org/abs/2405.18138v1","category":"math.FA"}
{"created":"2024-05-28 12:51:01","title":"Exploiting LLM Quantization","abstract":"Quantization leverages lower-precision weights to reduce the memory usage of large language models (LLMs) and is a key technique for enabling their deployment on commodity hardware. While LLM quantization's impact on utility has been extensively explored, this work for the first time studies its adverse effects from a security perspective. We reveal that widely used quantization methods can be exploited to produce a harmful quantized LLM, even though the full-precision counterpart appears benign, potentially tricking users into deploying the malicious quantized model. We demonstrate this threat using a three-staged attack framework: (i) first, we obtain a malicious LLM through fine-tuning on an adversarial task; (ii) next, we quantize the malicious model and calculate constraints that characterize all full-precision models that map to the same quantized model; (iii) finally, using projected gradient descent, we tune out the poisoned behavior from the full-precision model while ensuring that its weights satisfy the constraints computed in step (ii). This procedure results in an LLM that exhibits benign behavior in full precision but when quantized, it follows the adversarial behavior injected in step (i). We experimentally demonstrate the feasibility and severity of such an attack across three diverse scenarios: vulnerable code generation, content injection, and over-refusal attack. In practice, the adversary could host the resulting full-precision model on an LLM community hub such as Hugging Face, exposing millions of users to the threat of deploying its malicious quantized version on their devices.","sentences":["Quantization leverages lower-precision weights to reduce the memory usage of large language models (LLMs) and is a key technique for enabling their deployment on commodity hardware.","While LLM quantization's impact on utility has been extensively explored, this work for the first time studies its adverse effects from a security perspective.","We reveal that widely used quantization methods can be exploited to produce a harmful quantized LLM, even though the full-precision counterpart appears benign, potentially tricking users into deploying the malicious quantized model.","We demonstrate this threat using a three-staged attack framework: (i) first, we obtain a malicious LLM through fine-tuning on an adversarial task; (ii) next, we quantize the malicious model and calculate constraints that characterize all full-precision models that map to the same quantized model; (iii) finally, using projected gradient descent, we tune out the poisoned behavior from the full-precision model while ensuring that its weights satisfy the constraints computed in step (ii).","This procedure results in an LLM that exhibits benign behavior in full precision but when quantized, it follows the adversarial behavior injected in step (i).","We experimentally demonstrate the feasibility and severity of such an attack across three diverse scenarios: vulnerable code generation, content injection, and over-refusal attack.","In practice, the adversary could host the resulting full-precision model on an LLM community hub such as Hugging Face, exposing millions of users to the threat of deploying its malicious quantized version on their devices."],"url":"http://arxiv.org/abs/2405.18137v1","category":"cs.LG"}
{"created":"2024-05-28 12:48:26","title":"Metric and Geometric Spanners that are Resilient to Degree-Bounded Edge Faults","abstract":"Let $H$ be an edge-weighted graph, and let $G$ be a subgraph of $H$. We say that $G$ is an $f$-fault-tolerant $t$-spanner for $H$, if the following is true for any subset $F$ of at most $f$ edges of $G$: For any two vertices $p$ and $q$, the shortest-path distance between $p$ and $q$ in the graph $G \\setminus F$ is at most $t$ times the shortest-path distance between $p$ and $q$ in the graph $H \\setminus F$.   Recently, Bodwin, Haeupler, and Parter generalized this notion to the case when $F$ can be any set of edges in $G$, as long as the maximum degree of $F$ is at most $f$. They gave constructions for general graphs $H$.   We first consider the case when $H$ is a complete graph whose vertex set is an arbitrary metric space. We show that if this metric space contains a $t$-spanner with $m$ edges, then it also contains a graph $G$ with $O(fm)$ edges, that is resilient to edge faults of maximum degree $f$ and has stretch factor $O(ft)$.   Next, we consider the case when $H$ is a complete graph whose vertex set is a metric space that admits a well-separated pair decomposition. We show that, if the metric space has such a decomposition of size $m$, then it contains a graph with at most $(2f+1)^2 m$ edges, that is resilient to edge faults of maximum degree $f$ and has stretch factor at most $1+\\varepsilon$, for any given $\\varepsilon > 0$. For example, if the vertex set is a set of $n$ points in $\\mathbb{R}^d$ ($d$ being a constant) or a set of $n$ points in a metric space of bounded doubling dimension, then the spanner has $O(f^2 n)$ edges.   Finally, for the case when $H$ is a complete graph on $n$ points in $\\mathbb{R}^d$, we show how natural variants of the Yao- and $\\Theta$-graphs lead to graphs with $O(fn)$ edges, that are resilient to edge faults of maximum degree $f$ and have stretch factor at most $1+\\varepsilon$, for any given $\\varepsilon > 0$.","sentences":["Let $H$ be an edge-weighted graph, and let $G$ be a subgraph of $H$. We say that $G$ is an $f$-fault-tolerant $t$-spanner for $H$, if the following is true for any subset $F$ of at most $f$ edges of $G$: For any two vertices $p$ and $q$, the shortest-path distance between $p$ and $q$ in the graph $G \\setminus F$ is at most $t$ times the shortest-path distance between $p$ and $q$ in the graph $H \\setminus F$.   Recently, Bodwin, Haeupler, and Parter generalized this notion to the case when $F$ can be any set of edges in $G$, as long as the maximum degree of $F$ is at most $f$. They gave constructions for general graphs $H$.   We first consider the case when $H$ is a complete graph whose vertex set is an arbitrary metric space.","We show that if this metric space contains a $t$-spanner with $m$ edges, then it also contains a graph $G$ with $O(fm)$ edges, that is resilient to edge faults of maximum degree $f$ and has stretch factor $O(ft)$.   Next, we consider the case when $H$ is a complete graph whose vertex set is a metric space that admits a well-separated pair decomposition.","We show that, if the metric space has such a decomposition of size $m$, then it contains a graph with at most $(2f+1)^2 m$ edges, that is resilient to edge faults of maximum degree $f$ and has stretch factor at most $1+\\varepsilon$, for any given $\\varepsilon > 0$.","For example, if the vertex set is a set of $n$ points in $\\mathbb{R}^d$ ($d$ being a constant) or a set of $n$ points in a metric space of bounded doubling dimension, then the spanner has $O(f^2 n)$ edges.   ","Finally, for the case when $H$ is a complete graph on $n$ points in $\\mathbb{R}^d$, we show how natural variants of the Yao- and $\\Theta$-graphs lead to graphs with $O(fn)$ edges, that are resilient to edge faults of maximum degree $f$ and have stretch factor at most $1+\\varepsilon$, for any given $\\varepsilon > 0$."],"url":"http://arxiv.org/abs/2405.18134v1","category":"cs.CG"}
{"created":"2024-05-28 12:47:22","title":"EG4D: Explicit Generation of 4D Object without Score Distillation","abstract":"In recent years, the increasing demand for dynamic 3D assets in design and gaming applications has given rise to powerful generative pipelines capable of synthesizing high-quality 4D objects. Previous methods generally rely on score distillation sampling (SDS) algorithm to infer the unseen views and motion of 4D objects, thus leading to unsatisfactory results with defects like over-saturation and Janus problem. Therefore, inspired by recent progress of video diffusion models, we propose to optimize a 4D representation by explicitly generating multi-view videos from one input image. However, it is far from trivial to handle practical challenges faced by such a pipeline, including dramatic temporal inconsistency, inter-frame geometry and texture diversity, and semantic defects brought by video generation results. To address these issues, we propose DG4D, a novel multi-stage framework that generates high-quality and consistent 4D assets without score distillation. Specifically, collaborative techniques and solutions are developed, including an attention injection strategy to synthesize temporal-consistent multi-view videos, a robust and efficient dynamic reconstruction method based on Gaussian Splatting, and a refinement stage with diffusion prior for semantic restoration. The qualitative results and user preference study demonstrate that our framework outperforms the baselines in generation quality by a considerable margin. Code will be released at \\url{https://github.com/jasongzy/EG4D}.","sentences":["In recent years, the increasing demand for dynamic 3D assets in design and gaming applications has given rise to powerful generative pipelines capable of synthesizing high-quality 4D objects.","Previous methods generally rely on score distillation sampling (SDS) algorithm to infer the unseen views and motion of 4D objects, thus leading to unsatisfactory results with defects like over-saturation and Janus problem.","Therefore, inspired by recent progress of video diffusion models, we propose to optimize a 4D representation by explicitly generating multi-view videos from one input image.","However, it is far from trivial to handle practical challenges faced by such a pipeline, including dramatic temporal inconsistency, inter-frame geometry and texture diversity, and semantic defects brought by video generation results.","To address these issues, we propose DG4D, a novel multi-stage framework that generates high-quality and consistent 4D assets without score distillation.","Specifically, collaborative techniques and solutions are developed, including an attention injection strategy to synthesize temporal-consistent multi-view videos, a robust and efficient dynamic reconstruction method based on Gaussian Splatting, and a refinement stage with diffusion prior for semantic restoration.","The qualitative results and user preference study demonstrate that our framework outperforms the baselines in generation quality by a considerable margin.","Code will be released at \\url{https://github.com/jasongzy/EG4D}."],"url":"http://arxiv.org/abs/2405.18132v1","category":"cs.CV"}
{"created":"2024-05-28 12:44:28","title":"Self-Supervised Dual Contouring","abstract":"Learning-based isosurface extraction methods have recently emerged as a robust and efficient alternative to axiomatic techniques. However, the vast majority of such approaches rely on supervised training with axiomatically computed ground truths, thus potentially inheriting biases and data artifacts of the corresponding axiomatic methods. Steering away from such dependencies, we propose a self-supervised training scheme for the Neural Dual Contouring meshing framework, resulting in our method: Self-Supervised Dual Contouring (SDC). Instead of optimizing predicted mesh vertices with supervised training, we use two novel self-supervised loss functions that encourage the consistency between distances to the generated mesh up to the first order. Meshes reconstructed by SDC surpass existing data-driven methods in capturing intricate details while being more robust to possible irregularities in the input. Furthermore, we use the same self-supervised training objective linking inferred mesh and input SDF, to regularize the training process of Deep Implicit Networks (DINs). We demonstrate that the resulting DINs produce higher-quality implicit functions, ultimately leading to more accurate and detail-preserving surfaces compared to prior baselines for different input modalities. Finally, we demonstrate that our self-supervised losses improve meshing performance in the single-view reconstruction task by enabling joint training of predicted SDF and resulting output mesh. We open-source our code at https://github.com/Sentient07/SDC","sentences":["Learning-based isosurface extraction methods have recently emerged as a robust and efficient alternative to axiomatic techniques.","However, the vast majority of such approaches rely on supervised training with axiomatically computed ground truths, thus potentially inheriting biases and data artifacts of the corresponding axiomatic methods.","Steering away from such dependencies, we propose a self-supervised training scheme for the Neural Dual Contouring meshing framework, resulting in our method: Self-Supervised Dual Contouring (SDC).","Instead of optimizing predicted mesh vertices with supervised training, we use two novel self-supervised loss functions that encourage the consistency between distances to the generated mesh up to the first order.","Meshes reconstructed by SDC surpass existing data-driven methods in capturing intricate details while being more robust to possible irregularities in the input.","Furthermore, we use the same self-supervised training objective linking inferred mesh and input SDF, to regularize the training process of Deep Implicit Networks (DINs).","We demonstrate that the resulting DINs produce higher-quality implicit functions, ultimately leading to more accurate and detail-preserving surfaces compared to prior baselines for different input modalities.","Finally, we demonstrate that our self-supervised losses improve meshing performance in the single-view reconstruction task by enabling joint training of predicted SDF and resulting output mesh.","We open-source our code at https://github.com/Sentient07/SDC"],"url":"http://arxiv.org/abs/2405.18131v1","category":"cs.CV"}
{"created":"2024-05-28 12:41:01","title":"The Separate Universe approach for multifield inflation models","abstract":"Primordial black holes could constitute part or all of dark matter but they require large inhomogeneities to form in the early universe. These inhomogeneities can strongly backreact on the large scale dynamics of the universe. Stochastic inflation provides a way of studying this backreaction and getting an estimation of the abundance of primordial black holes. Because stochastic inflation focuses on large scale dynamics, it rests on the separate universe approach. However, the validity of this approach has only been checked in single field models, but not in multifield models in which we expect strong boosts in the power spectrum, leading to the formation of primordial black holes. We will check the validity of a separate universe approach in multifield models by matching it with a complete cosmological perturbation theory approach at large scales. In particular, we wish to compare these two paradigms and their differences in the adiabatic and entropic directions of the phase space. This will give us a range of validity and conditions one needs to verify in order to apply the separate universe approach and stochastic inflation in multifield models.","sentences":["Primordial black holes could constitute part or all of dark matter but they require large inhomogeneities to form in the early universe.","These inhomogeneities can strongly backreact on the large scale dynamics of the universe.","Stochastic inflation provides a way of studying this backreaction and getting an estimation of the abundance of primordial black holes.","Because stochastic inflation focuses on large scale dynamics, it rests on the separate universe approach.","However, the validity of this approach has only been checked in single field models, but not in multifield models in which we expect strong boosts in the power spectrum, leading to the formation of primordial black holes.","We will check the validity of a separate universe approach in multifield models by matching it with a complete cosmological perturbation theory approach at large scales.","In particular, we wish to compare these two paradigms and their differences in the adiabatic and entropic directions of the phase space.","This will give us a range of validity and conditions one needs to verify in order to apply the separate universe approach and stochastic inflation in multifield models."],"url":"http://arxiv.org/abs/2405.18129v1","category":"astro-ph.CO"}
{"created":"2024-05-28 12:38:31","title":"Euclid preparation. Observational expectations for redshift z<7 active galactic nuclei in the Euclid Wide and Deep surveys","abstract":"We forecast the expected population of active galactic nuclei (AGN) observable in the Euclid Wide Survey (EWS) and Euclid Deep Survey (EDS). Starting from an X-ray luminosity function (XLF) we generate volume-limited samples of the AGN expected in the survey footprints. Each AGN is assigned an SED appropriate for its X-ray luminosity and redshift, with perturbations sampled from empirical distributions. The photometric detectability of each AGN is assessed via mock observation of the assigned SED. We estimate 40 million AGN will be detectable in at least one band in the EWS and 0.24 million in the EDS, corresponding to surface densities of 2.8$\\times$10$^{3}$ deg$^{-2}$ and 4.7$\\times$10$^{3}$ deg$^{-2}$. Employing colour selection criteria on our simulated data we select a sample of 4.8$\\times$10$^{6}$ (331 deg$^{-2}$) AGN in the EWS and 1.7$\\times$10$^{4}$ (346 deg$^{-2}$) in the EDS, amounting to 10% and 8% of the AGN detectable in the EWS and EDS. Including ancillary Rubin/LSST bands improves the completeness and purity of AGN selection. These data roughly double the total number of selected AGN to comprise 21% and 15% of the detectable AGN in the EWS and EDS. The total expected sample of colour-selected AGN contains 6.0$\\times$10$^{6}$ (74%) unobscured AGN and 2.1$\\times$10$^{6}$ (26%) obscured AGN, covering $0.02 \\leq z \\lesssim 5.2$ and $43 \\leq \\log_{10} (L_{bol} / erg s^{-1}) \\leq 47$. With this simple colour selection, expected surface densities are already comparable to the yield of modern X-ray and mid-infrared surveys of similar area. The relative uncertainty on our expectation for detectable AGN is 6.7% for the EWS and 12.5% for the EDS, driven by the uncertainty of the XLF.","sentences":["We forecast the expected population of active galactic nuclei (AGN) observable in the Euclid Wide Survey (EWS) and Euclid Deep Survey (EDS).","Starting from an X-ray luminosity function (XLF) we generate volume-limited samples of the AGN expected in the survey footprints.","Each AGN is assigned an SED appropriate for its X-ray luminosity and redshift, with perturbations sampled from empirical distributions.","The photometric detectability of each AGN is assessed via mock observation of the assigned SED.","We estimate 40 million AGN will be detectable in at least one band in the EWS and 0.24 million in the EDS, corresponding to surface densities of 2.8$\\times$10$^{3}$ deg$^{-2}$ and 4.7$\\times$10$^{3}$ deg$^{-2}$. Employing colour selection criteria on our simulated data we select a sample of 4.8$\\times$10$^{6}$ (331 deg$^{-2}$) AGN in the EWS and 1.7$\\times$10$^{4}$ (346 deg$^{-2}$) in the EDS, amounting to 10% and 8% of the AGN detectable in the EWS and EDS.","Including ancillary Rubin/LSST bands improves the completeness and purity of AGN selection.","These data roughly double the total number of selected AGN to comprise 21% and 15% of the detectable AGN in the EWS and EDS.","The total expected sample of colour-selected AGN contains 6.0$\\times$10$^{6}$ (74%) unobscured AGN and 2.1$\\times$10$^{6}$ (26%) obscured AGN, covering $0.02 \\leq z \\lesssim 5.2$ and $43 \\leq \\log_{10} (L_{bol} / erg s^{-1}) \\leq 47$.","With this simple colour selection, expected surface densities are already comparable to the yield of modern X-ray and mid-infrared surveys of similar area.","The relative uncertainty on our expectation for detectable AGN is 6.7% for the EWS and 12.5% for the EDS, driven by the uncertainty of the XLF."],"url":"http://arxiv.org/abs/2405.18126v1","category":"astro-ph.GA"}
{"created":"2024-05-28 12:31:42","title":"On the Structure of Multivariate Gabor Frames and a Higher-Dimensional Lyubarskii-Seip-Wallst\u00e9n Result","abstract":"We introduce an equivalence relation on the set of lattices in $\\mathbb{R}^{2d}$ such that equivalent lattices share identical structures of Gabor frames, up to unitary equivalence, a notion we define. These equivalence classes are parameterized by symplectic forms on $\\mathbb{R}^{2d}$ and they consist of lattices related by symplectic transformations. This implies that $2d^2 - d$ parameters suffice to describe the possible structures of Gabor systems over lattices in $\\mathbb{R}^{2d}$, as opposed to the $4d^2$ degrees of freedom in the choice of lattice. We also prove that (under a mild additional assumption) symplectic transformations are the only linear transformations of the time-frequency plane which implement equivalences of this kind, thereby characterizing symplectic transformations as the structure-preserving transformations of the time-frequency plane in the context of Gabor analysis.   We also investigate the equivalence classes that have separable lattices as representatives and find that the parameter space in this case is $d^2$-dimensional. We provide an explicit example showing that non-separable and irrational lattices can behave exactly like separable and rational ones.   Finally, this approach allows us to prove a higher-dimensional variant of the Lyubarskii-Seip-Wallst\\'en Theorem for Gaussian Gabor frames. This gives us, for a large class of lattices in $\\mathbb{R}^{2d}$ (including all symplectic ones), necessary and sufficient conditions for $d$-parameter families of Gaussians to generate Gabor frames.","sentences":["We introduce an equivalence relation on the set of lattices in $\\mathbb{R}^{2d}$ such that equivalent lattices share identical structures of Gabor frames, up to unitary equivalence, a notion we define.","These equivalence classes are parameterized by symplectic forms on $\\mathbb{R}^{2d}$ and they consist of lattices related by symplectic transformations.","This implies that $2d^2 - d$ parameters suffice to describe the possible structures of Gabor systems over lattices in $\\mathbb{R}^{2d}$, as opposed to the $4d^2$ degrees of freedom in the choice of lattice.","We also prove that (under a mild additional assumption) symplectic transformations are the only linear transformations of the time-frequency plane which implement equivalences of this kind, thereby characterizing symplectic transformations as the structure-preserving transformations of the time-frequency plane in the context of Gabor analysis.   ","We also investigate the equivalence classes that have separable lattices as representatives and find that the parameter space in this case is $d^2$-dimensional.","We provide an explicit example showing that non-separable and irrational lattices can behave exactly like separable and rational ones.   ","Finally, this approach allows us to prove a higher-dimensional variant of the Lyubarskii-Seip-Wallst\\'en Theorem for Gaussian Gabor frames.","This gives us, for a large class of lattices in $\\mathbb{R}^{2d}$ (including all symplectic ones), necessary and sufficient conditions for $d$-parameter families of Gaussians to generate Gabor frames."],"url":"http://arxiv.org/abs/2405.18125v1","category":"math.FA"}
{"created":"2024-05-28 12:30:28","title":"PyTAG: Tabletop Games for Multi-Agent Reinforcement Learning","abstract":"Modern Tabletop Games present various interesting challenges for Multi-agent Reinforcement Learning. In this paper, we introduce PyTAG, a new framework that supports interacting with a large collection of games implemented in the Tabletop Games framework. In this work we highlight the challenges tabletop games provide, from a game-playing agent perspective, along with the opportunities they provide for future research. Additionally, we highlight the technical challenges that involve training Reinforcement Learning agents on these games. To explore the Multi-agent setting provided by PyTAG we train the popular Proximal Policy Optimisation Reinforcement Learning algorithm using self-play on a subset of games and evaluate the trained policies against some simple agents and Monte-Carlo Tree Search implemented in the Tabletop Games framework.","sentences":["Modern Tabletop Games present various interesting challenges for Multi-agent Reinforcement Learning.","In this paper, we introduce PyTAG, a new framework that supports interacting with a large collection of games implemented in the Tabletop Games framework.","In this work we highlight the challenges tabletop games provide, from a game-playing agent perspective, along with the opportunities they provide for future research.","Additionally, we highlight the technical challenges that involve training Reinforcement Learning agents on these games.","To explore the Multi-agent setting provided by PyTAG we train the popular Proximal Policy Optimisation Reinforcement Learning algorithm using self-play on a subset of games and evaluate the trained policies against some simple agents and Monte-Carlo Tree Search implemented in the Tabletop Games framework."],"url":"http://arxiv.org/abs/2405.18123v1","category":"cs.AI"}
{"created":"2024-05-28 12:28:12","title":"Low-Resource Crop Classification from Multi-Spectral Time Series Using Lossless Compressors","abstract":"Deep learning has significantly improved the accuracy of crop classification using multispectral temporal data. However, these models have complex structures with numerous parameters, requiring large amounts of data and costly training. In low-resource situations with fewer labeled samples, deep learning models perform poorly due to insufficient data. Conversely, compressors are data-type agnostic, and non-parametric methods do not bring underlying assumptions. Inspired by this insight, we propose a non-training alternative to deep learning models, aiming to address these situations. Specifically, the Symbolic Representation Module is proposed to convert the reflectivity into symbolic representations. The symbolic representations are then cross-transformed in both the channel and time dimensions to generate symbolic embeddings. Next, the Multi-scale Normalised Compression Distance (MNCD) is designed to measure the correlation between any two symbolic embeddings. Finally, based on the MNCDs, high quality crop classification can be achieved using only a k-nearest-neighbor classifier kNN. The entire framework is ready-to-use and lightweight. Without any training, it outperformed, on average, 7 advanced deep learning models trained at scale on three benchmark datasets. It also outperforms more than half of these models in the few-shot setting with sparse crop labels. Therefore, the high performance and robustness of our non-training framework makes it truly applicable to real-world crop mapping. Codes are available at: https://github.com/qinfengsama/Compressor-Based-Crop-Mapping.","sentences":["Deep learning has significantly improved the accuracy of crop classification using multispectral temporal data.","However, these models have complex structures with numerous parameters, requiring large amounts of data and costly training.","In low-resource situations with fewer labeled samples, deep learning models perform poorly due to insufficient data.","Conversely, compressors are data-type agnostic, and non-parametric methods do not bring underlying assumptions.","Inspired by this insight, we propose a non-training alternative to deep learning models, aiming to address these situations.","Specifically, the Symbolic Representation Module is proposed to convert the reflectivity into symbolic representations.","The symbolic representations are then cross-transformed in both the channel and time dimensions to generate symbolic embeddings.","Next, the Multi-scale Normalised Compression Distance (MNCD) is designed to measure the correlation between any two symbolic embeddings.","Finally, based on the MNCDs, high quality crop classification can be achieved using only a k-nearest-neighbor classifier kNN.","The entire framework is ready-to-use and lightweight.","Without any training, it outperformed, on average, 7 advanced deep learning models trained at scale on three benchmark datasets.","It also outperforms more than half of these models in the few-shot setting with sparse crop labels.","Therefore, the high performance and robustness of our non-training framework makes it truly applicable to real-world crop mapping.","Codes are available at: https://github.com/qinfengsama/Compressor-Based-Crop-Mapping."],"url":"http://arxiv.org/abs/2405.18119v1","category":"cs.CV"}
{"created":"2024-05-28 12:27:36","title":"An approach to improve agent learning via guaranteeing goal reaching in all episodes","abstract":"Reinforcement learning is commonly concerned with problems of maximizing accumulated rewards in Markov decision processes. Oftentimes, a certain goal state or a subset of the state space attain maximal reward. In such a case, the environment may be considered solved when the goal is reached. Whereas numerous techniques, learning or non-learning based, exist for solving environments, doing so optimally is the biggest challenge. Say, one may choose a reward rate which penalizes the action effort. Reinforcement learning is currently among the most actively developed frameworks for solving environments optimally by virtue of maximizing accumulated reward, in other words, returns. Yet, tuning agents is a notoriously hard task as reported in a series of works. Our aim here is to help the agent learn a near-optimal policy efficiently while ensuring a goal reaching property of some basis policy that merely solves the environment. We suggest an algorithm, which is fairly flexible, and can be used to augment practically any agent as long as it comprises of a critic. A formal proof of a goal reaching property is provided. Simulation experiments on six problems under five agents, including the benchmarked one, provided an empirical evidence that the learning can indeed be boosted while ensuring goal reaching property.","sentences":["Reinforcement learning is commonly concerned with problems of maximizing accumulated rewards in Markov decision processes.","Oftentimes, a certain goal state or a subset of the state space attain maximal reward.","In such a case, the environment may be considered solved when the goal is reached.","Whereas numerous techniques, learning or non-learning based, exist for solving environments, doing so optimally is the biggest challenge.","Say, one may choose a reward rate which penalizes the action effort.","Reinforcement learning is currently among the most actively developed frameworks for solving environments optimally by virtue of maximizing accumulated reward, in other words, returns.","Yet, tuning agents is a notoriously hard task as reported in a series of works.","Our aim here is to help the agent learn a near-optimal policy efficiently while ensuring a goal reaching property of some basis policy that merely solves the environment.","We suggest an algorithm, which is fairly flexible, and can be used to augment practically any agent as long as it comprises of a critic.","A formal proof of a goal reaching property is provided.","Simulation experiments on six problems under five agents, including the benchmarked one, provided an empirical evidence that the learning can indeed be boosted while ensuring goal reaching property."],"url":"http://arxiv.org/abs/2405.18118v1","category":"cs.AI"}
{"created":"2024-05-28 12:23:24","title":"Fatigue and mental underload further pronounced in L3 conditionally automated driving: Results from an EEG experiment on a test track","abstract":"Drivers' role changes with increasing automation from the primary driver to a system supervisor. This study investigates how supervising an SAE L2 and L3 automated vehicle (AV) affects drivers' mental workload and sleepiness compared to manual driving. Using an AV prototype on a test track, the oscillatory brain activity of 23 adult participants was recorded during L2, L3, and manual driving. Results showed decreased mental workload and increased sleepiness in L3 drives compared to L2 and manual drives, indicated by self-report scales and changes in the frontal alpha and theta power spectral density. These findings suggest that fatigue and mental underload are significant issues in L3 driving and should be considered when designing future AV interfaces.","sentences":["Drivers' role changes with increasing automation from the primary driver to a system supervisor.","This study investigates how supervising an SAE L2 and L3 automated vehicle (AV) affects drivers' mental workload and sleepiness compared to manual driving.","Using an AV prototype on a test track, the oscillatory brain activity of 23 adult participants was recorded during L2, L3, and manual driving.","Results showed decreased mental workload and increased sleepiness in L3 drives compared to L2 and manual drives, indicated by self-report scales and changes in the frontal alpha and theta power spectral density.","These findings suggest that fatigue and mental underload are significant issues in L3 driving and should be considered when designing future AV interfaces."],"url":"http://arxiv.org/abs/2405.18114v1","category":"cs.HC"}
{"created":"2024-05-28 12:23:16","title":"Facilitating Multi-Role and Multi-Behavior Collaboration of Large Language Models for Online Job Seeking and Recruiting","abstract":"The emergence of online recruitment services has revolutionized the traditional landscape of job seeking and recruitment, necessitating the development of high-quality industrial applications to improve person-job fitting. Existing methods generally rely on modeling the latent semantics of resumes and job descriptions and learning a matching function between them. Inspired by the powerful role-playing capabilities of Large Language Models (LLMs), we propose to introduce a mock interview process between LLM-played interviewers and candidates. The mock interview conversations can provide additional evidence for candidate evaluation, thereby augmenting traditional person-job fitting based solely on resumes and job descriptions. However, characterizing these two roles in online recruitment still presents several challenges, such as developing the skills to raise interview questions, formulating appropriate answers, and evaluating two-sided fitness. To this end, we propose MockLLM, a novel applicable framework that divides the person-job matching process into two modules: mock interview generation and two-sided evaluation in handshake protocol, jointly enhancing their performance through collaborative behaviors between interviewers and candidates. We design a role-playing framework as a multi-role and multi-behavior paradigm to enable a single LLM agent to effectively behave with multiple functions for both parties. Moreover, we propose reflection memory generation and dynamic prompt modification techniques to refine the behaviors of both sides, enabling continuous optimization of the augmented additional evidence. Extensive experimental results show that MockLLM can achieve the best performance on person-job matching accompanied by high mock interview quality, envisioning its emerging application in real online recruitment in the future.","sentences":["The emergence of online recruitment services has revolutionized the traditional landscape of job seeking and recruitment, necessitating the development of high-quality industrial applications to improve person-job fitting.","Existing methods generally rely on modeling the latent semantics of resumes and job descriptions and learning a matching function between them.","Inspired by the powerful role-playing capabilities of Large Language Models (LLMs), we propose to introduce a mock interview process between LLM-played interviewers and candidates.","The mock interview conversations can provide additional evidence for candidate evaluation, thereby augmenting traditional person-job fitting based solely on resumes and job descriptions.","However, characterizing these two roles in online recruitment still presents several challenges, such as developing the skills to raise interview questions, formulating appropriate answers, and evaluating two-sided fitness.","To this end, we propose MockLLM, a novel applicable framework that divides the person-job matching process into two modules: mock interview generation and two-sided evaluation in handshake protocol, jointly enhancing their performance through collaborative behaviors between interviewers and candidates.","We design a role-playing framework as a multi-role and multi-behavior paradigm to enable a single LLM agent to effectively behave with multiple functions for both parties.","Moreover, we propose reflection memory generation and dynamic prompt modification techniques to refine the behaviors of both sides, enabling continuous optimization of the augmented additional evidence.","Extensive experimental results show that MockLLM can achieve the best performance on person-job matching accompanied by high mock interview quality, envisioning its emerging application in real online recruitment in the future."],"url":"http://arxiv.org/abs/2405.18113v1","category":"cs.CL"}
{"created":"2024-05-28 12:18:50","title":"ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented Generator","abstract":"Large language model (LLM) has proven to benefit a lot from retrieval augmentation in alleviating hallucinations confronted with knowledge-intensive questions. Retrieval-augmented generation (RAG) adopts IR-based techniques utilizing semantic-relevant documents as the generator's input context and realizes external knowledge injection. However, on today's Internet which is flooded with content generated by LLMs, there are too many \"related yet useless\" documents or even fake knowledge fabricated by LLMs, which will introduce extra noise to the generator and distract it from giving correct results. To this end, we regard the training of the RAG generator model as a multi-agent adversarial-defensive system, guiding the generator to have a better taste of whether a specific document helps answer the question through the Adversarial Tuning in a Multi-agent (ATM) system to strengthen the generator's robustness in an RAG pipeline. After rounds of multi-agent iterative tuning, we find that the ATM Generator can eventually discriminate useful documents amongst LLM fabrications and achieve better performance than strong baselines.","sentences":["Large language model (LLM) has proven to benefit a lot from retrieval augmentation in alleviating hallucinations confronted with knowledge-intensive questions.","Retrieval-augmented generation (RAG) adopts IR-based techniques utilizing semantic-relevant documents as the generator's input context and realizes external knowledge injection.","However, on today's Internet which is flooded with content generated by LLMs, there are too many \"related yet useless\" documents or even fake knowledge fabricated by LLMs, which will introduce extra noise to the generator and distract it from giving correct results.","To this end, we regard the training of the RAG generator model as a multi-agent adversarial-defensive system, guiding the generator to have a better taste of whether a specific document helps answer the question through the Adversarial Tuning in a Multi-agent (ATM) system to strengthen the generator's robustness in an RAG pipeline.","After rounds of multi-agent iterative tuning, we find that the ATM Generator can eventually discriminate useful documents amongst LLM fabrications and achieve better performance than strong baselines."],"url":"http://arxiv.org/abs/2405.18111v1","category":"cs.CL"}
{"created":"2024-05-28 12:18:19","title":"Individual Contributions as Intrinsic Exploration Scaffolds for Multi-agent Reinforcement Learning","abstract":"In multi-agent reinforcement learning (MARL), effective exploration is critical, especially in sparse reward environments. Although introducing global intrinsic rewards can foster exploration in such settings, it often complicates credit assignment among agents. To address this difficulty, we propose Individual Contributions as intrinsic Exploration Scaffolds (ICES), a novel approach to motivate exploration by assessing each agent's contribution from a global view. In particular, ICES constructs exploration scaffolds with Bayesian surprise, leveraging global transition information during centralized training. These scaffolds, used only in training, help to guide individual agents towards actions that significantly impact the global latent state transitions. Additionally, ICES separates exploration policies from exploitation policies, enabling the former to utilize privileged global information during training. Extensive experiments on cooperative benchmark tasks with sparse rewards, including Google Research Football (GRF) and StarCraft Multi-agent Challenge (SMAC), demonstrate that ICES exhibits superior exploration capabilities compared with baselines. The code is publicly available at https://github.com/LXXXXR/ICES.","sentences":["In multi-agent reinforcement learning (MARL), effective exploration is critical, especially in sparse reward environments.","Although introducing global intrinsic rewards can foster exploration in such settings, it often complicates credit assignment among agents.","To address this difficulty, we propose Individual Contributions as intrinsic Exploration Scaffolds (ICES), a novel approach to motivate exploration by assessing each agent's contribution from a global view.","In particular, ICES constructs exploration scaffolds with Bayesian surprise, leveraging global transition information during centralized training.","These scaffolds, used only in training, help to guide individual agents towards actions that significantly impact the global latent state transitions.","Additionally, ICES separates exploration policies from exploitation policies, enabling the former to utilize privileged global information during training.","Extensive experiments on cooperative benchmark tasks with sparse rewards, including Google Research Football (GRF) and StarCraft Multi-agent Challenge (SMAC), demonstrate that ICES exhibits superior exploration capabilities compared with baselines.","The code is publicly available at https://github.com/LXXXXR/ICES."],"url":"http://arxiv.org/abs/2405.18110v1","category":"cs.LG"}
{"created":"2024-05-28 12:13:07","title":"A Unified Temporal Knowledge Graph Reasoning Model Towards Interpolation and Extrapolation","abstract":"Temporal knowledge graph (TKG) reasoning has two settings: interpolation reasoning and extrapolation reasoning. Both of them draw plenty of research interest and have great significance. Methods of the former de-emphasize the temporal correlations among facts sequences, while methods of the latter require strict chronological order of knowledge and ignore inferring clues provided by missing facts of the past. These limit the practicability of TKG applications as almost all of the existing TKG reasoning methods are designed specifically to address either one setting. To this end, this paper proposes an original Temporal PAth-based Reasoning (TPAR) model for both the interpolation and extrapolation reasoning. TPAR performs a neural-driven symbolic reasoning fashion that is robust to ambiguous and noisy temporal data and with fine interpretability as well. Comprehensive experiments show that TPAR outperforms SOTA methods on the link prediction task for both the interpolation and the extrapolation settings. A novel pipeline experimental setting is designed to evaluate the performances of SOTA combinations and the proposed TPAR towards interpolation and extrapolation reasoning. More diverse experiments are conducted to show the robustness and interpretability of TPAR.","sentences":["Temporal knowledge graph (TKG) reasoning has two settings: interpolation reasoning and extrapolation reasoning.","Both of them draw plenty of research interest and have great significance.","Methods of the former de-emphasize the temporal correlations among facts sequences, while methods of the latter require strict chronological order of knowledge and ignore inferring clues provided by missing facts of the past.","These limit the practicability of TKG applications as almost all of the existing TKG reasoning methods are designed specifically to address either one setting.","To this end, this paper proposes an original Temporal PAth-based Reasoning (TPAR) model for both the interpolation and extrapolation reasoning.","TPAR performs a neural-driven symbolic reasoning fashion that is robust to ambiguous and noisy temporal data and with fine interpretability as well.","Comprehensive experiments show that TPAR outperforms SOTA methods on the link prediction task for both the interpolation and the extrapolation settings.","A novel pipeline experimental setting is designed to evaluate the performances of SOTA combinations and the proposed TPAR towards interpolation and extrapolation reasoning.","More diverse experiments are conducted to show the robustness and interpretability of TPAR."],"url":"http://arxiv.org/abs/2405.18106v1","category":"cs.AI"}
{"created":"2024-05-28 12:11:24","title":"Quantum-Classical Autoencoder Architectures for End-to-End Radio Communication","abstract":"This paper presents a comprehensive study on the possible hybrid quantum-classical autoencoder architectures for end-to-end radio communication against noisy channel conditions using standard encoded radio signals. The hybrid scenarios include single-sided, i.e., quantum encoder (transmitter) or quantum decoder (receiver), as well as fully quantum channel autoencoder (transmitter-receiver) systems. We provide detailed formulas for each scenario and validate our model through an extensive set of simulations. Our results demonstrate model robustness and adaptability. Supporting experiments are conducted utilizing 4-QAM and 16-QAM schemes and we expect that the model is adaptable to more general encoding schemes. We explore model performance against both additive white Gaussian noise and Rayleigh fading models. Our findings highlight the importance of designing efficient quantum neural network architectures for meeting application performance constraints -- including data re-uploading methods, encoding schemes, and core layer structures. By offering a general framework, this work paves the way for further exploration and development of quantum machine learning applications in radio communication.","sentences":["This paper presents a comprehensive study on the possible hybrid quantum-classical autoencoder architectures for end-to-end radio communication against noisy channel conditions using standard encoded radio signals.","The hybrid scenarios include single-sided, i.e., quantum encoder (transmitter) or quantum decoder (receiver), as well as fully quantum channel autoencoder (transmitter-receiver) systems.","We provide detailed formulas for each scenario and validate our model through an extensive set of simulations.","Our results demonstrate model robustness and adaptability.","Supporting experiments are conducted utilizing 4-QAM and 16-QAM schemes and we expect that the model is adaptable to more general encoding schemes.","We explore model performance against both additive white Gaussian noise and Rayleigh fading models.","Our findings highlight the importance of designing efficient quantum neural network architectures for meeting application performance constraints -- including data re-uploading methods, encoding schemes, and core layer structures.","By offering a general framework, this work paves the way for further exploration and development of quantum machine learning applications in radio communication."],"url":"http://arxiv.org/abs/2405.18105v1","category":"quant-ph"}
{"created":"2024-05-28 12:08:54","title":"Apportionment with Weighted Seats","abstract":"Apportionment is the task of assigning resources to entities with different entitlements in a fair manner, and specifically a manner that is as proportional as possible. The best-known application concerns the assignment of parliamentary seats to political parties based on their share in the popular vote. Here we enrich the standard model of apportionment by associating each seat with a weight that reflects the value of that seat, for example because seats come with different roles, such as chair or treasurer, that have different (objective) values. We define several apportionment methods and natural fairness requirements for this new setting, and study the extent to which our methods satisfy our requirements. Our findings show that full fairness is harder to achieve than in the standard apportionment setting. At the same time, for relaxations of those requirements we can achieve stronger results than in the more general model of weighted fair division, where the values of objects are subjective.","sentences":["Apportionment is the task of assigning resources to entities with different entitlements in a fair manner, and specifically a manner that is as proportional as possible.","The best-known application concerns the assignment of parliamentary seats to political parties based on their share in the popular vote.","Here we enrich the standard model of apportionment by associating each seat with a weight that reflects the value of that seat, for example because seats come with different roles, such as chair or treasurer, that have different (objective) values.","We define several apportionment methods and natural fairness requirements for this new setting, and study the extent to which our methods satisfy our requirements.","Our findings show that full fairness is harder to achieve than in the standard apportionment setting.","At the same time, for relaxations of those requirements we can achieve stronger results than in the more general model of weighted fair division, where the values of objects are subjective."],"url":"http://arxiv.org/abs/2405.18102v1","category":"cs.GT"}
{"created":"2024-05-28 12:02:56","title":"Coupling Analysis of the Asymptotic Behaviour of a Primal-Dual Langevin Algorithm","abstract":"In this paper, we analyze a recently proposed algorithm for the problem of sampling from probability distributions $\\mu^\\ast$ in $\\mathbb{R}^d$ with a Lebesgue density and potential of the form $f(Kx)+g(x)$, where $K$ is a linear operator and $f$, $g$ are convex and non-smooth. The algorithm is a generalization of the primal-dual hybrid gradient (PDHG) convex optimization algorithm to a sampling scheme. We analyze the method's continuous time limit, an SDE in the joint primal-dual variable. We give mild conditions under which the corresponding Fokker-Planck equation converges to a unique stationary state, which however does not concentrate in the dual variable and consequently does not have $\\mu^\\ast$ as its primal marginal. Under a smoothness assumption on $f$, we show that the scheme converges to the purely primal overdamped Langevin diffusion in the limit of small primal and large dual step sizes. We further prove that the target can never be the primal marginal of the invariant solution for any modification of the SDE with space-homogeneous diffusion coefficient. A correction with inhomogeneous diffusion coefficient and the correct invariant solution is proposed, but the scheme requires the same smoothness assumptions on $f$ and is numerically inferior to overdamped Langevin diffusion. We demonstrate our findings on both small-scale examples in which we can exactly verify the theoretical results, as well as on typical examples of larger scale in the context of Bayesian imaging inverse problems.","sentences":["In this paper, we analyze a recently proposed algorithm for the problem of sampling from probability distributions $\\mu^\\ast$ in $\\mathbb{R}^d$ with a Lebesgue density and potential of the form $f(Kx)+g(x)$, where $K$ is a linear operator and $f$, $g$ are convex and non-smooth.","The algorithm is a generalization of the primal-dual hybrid gradient (PDHG) convex optimization algorithm to a sampling scheme.","We analyze the method's continuous time limit, an SDE in the joint primal-dual variable.","We give mild conditions under which the corresponding Fokker-Planck equation converges to a unique stationary state, which however does not concentrate in the dual variable and consequently does not have $\\mu^\\ast$ as its primal marginal.","Under a smoothness assumption on $f$, we show that the scheme converges to the purely primal overdamped Langevin diffusion in the limit of small primal and large dual step sizes.","We further prove that the target can never be the primal marginal of the invariant solution for any modification of the SDE with space-homogeneous diffusion coefficient.","A correction with inhomogeneous diffusion coefficient and the correct invariant solution is proposed, but the scheme requires the same smoothness assumptions on $f$ and is numerically inferior to overdamped Langevin diffusion.","We demonstrate our findings on both small-scale examples in which we can exactly verify the theoretical results, as well as on typical examples of larger scale in the context of Bayesian imaging inverse problems."],"url":"http://arxiv.org/abs/2405.18098v1","category":"math.OC"}
{"created":"2024-05-28 12:02:47","title":"Entanglement in Lifshitz Fermion Theories","abstract":"We study the static entanglement structure in (1+1)-dimensional free Dirac-fermion theory with Lifshitz symmetry and arbitrary integer dynamical critical exponent. This model is different from the one introduced in [Hartmann et al., SciPost Phys. 11, no.2, 031 (2021)] due to a proper treatment of the square Laplace operator. Dirac fermion Lifshitz theory is local as opposed to its scalar counterpart which strongly affects its entanglement structure. We show that there is quantum entanglement across arbitrary subregions in various pure (including the vacuum) and mixed states of this theory for arbitrary integer values of the dynamical critical exponent. Our numerical investigations show that quantum entanglement in this theory is tightly bounded from above. Such a bound and other physical properties of quantum entanglement are carefully explained from the correlation structure in these theories. A generalization to (2+1)-dimensions where the entanglement structure is seriously different is addressed.","sentences":["We study the static entanglement structure in (1+1)-dimensional free Dirac-fermion theory with Lifshitz symmetry and arbitrary integer dynamical critical exponent.","This model is different from the one introduced in [Hartmann et al., SciPost Phys. 11, no.2, 031 (2021)] due to a proper treatment of the square Laplace operator.","Dirac fermion Lifshitz theory is local as opposed to its scalar counterpart which strongly affects its entanglement structure.","We show that there is quantum entanglement across arbitrary subregions in various pure (including the vacuum) and mixed states of this theory for arbitrary integer values of the dynamical critical exponent.","Our numerical investigations show that quantum entanglement in this theory is tightly bounded from above.","Such a bound and other physical properties of quantum entanglement are carefully explained from the correlation structure in these theories.","A generalization to (2+1)-dimensions where the entanglement structure is seriously different is addressed."],"url":"http://arxiv.org/abs/2405.18097v1","category":"hep-th"}
{"created":"2024-05-28 11:59:40","title":"LLM experiments with simulation: Large Language Model Multi-Agent System for Process Simulation Parametrization in Digital Twins","abstract":"This paper presents a novel design of a multi-agent system framework that applies a large language model (LLM) to automate the parametrization of process simulations in digital twins. We propose a multi-agent framework that includes four types of agents: observation, reasoning, decision and summarization. By enabling dynamic interaction between LLM agents and simulation model, the developed system can automatically explore the parametrization of the simulation and use heuristic reasoning to determine a set of parameters to control the simulation to achieve an objective. The proposed approach enhances the simulation model by infusing it with heuristics from LLM and enables autonomous search for feasible parametrization to solve a user task. Furthermore, the system has the potential to increase user-friendliness and reduce the cognitive load on human users by assisting in complex decision-making processes. The effectiveness and functionality of the system are demonstrated through a case study, and the visualized demos are available at a GitHub Repository: https://github.com/YuchenXia/LLMDrivenSimulation","sentences":["This paper presents a novel design of a multi-agent system framework that applies a large language model (LLM) to automate the parametrization of process simulations in digital twins.","We propose a multi-agent framework that includes four types of agents: observation, reasoning, decision and summarization.","By enabling dynamic interaction between LLM agents and simulation model, the developed system can automatically explore the parametrization of the simulation and use heuristic reasoning to determine a set of parameters to control the simulation to achieve an objective.","The proposed approach enhances the simulation model by infusing it with heuristics from LLM and enables autonomous search for feasible parametrization to solve a user task.","Furthermore, the system has the potential to increase user-friendliness and reduce the cognitive load on human users by assisting in complex decision-making processes.","The effectiveness and functionality of the system are demonstrated through a case study, and the visualized demos are available at a GitHub Repository: https://github.com/YuchenXia/LLMDrivenSimulation"],"url":"http://arxiv.org/abs/2405.18092v1","category":"cs.AI"}
{"created":"2024-05-28 11:53:22","title":"A correspondence between Maxwell--Einstein theory and superfluidity","abstract":"A planar superfluid is considered and interpreted in terms of electromagnetism and gravity. It has previously been suggested that the superfluid flow can be regarded as analogous to an electromagnetic field and that a non-vanishing density gradient give rise to a gravity-like force. The present work seeks to reconcile these hitherto distinct pictures into a unified exposition of vortex electrodynamics in a curved analogue space-time. By constructing a theory in which the planar Maxwell's equations are coupled to a (d+1)-dimensional conformally curved space-time, we expose a correspondence between the resulting equations of motion of the embedded fields and the dynamics of a quantum fluid. Finally, an effective vortex metric whose connection components are torsional is studied and its effect on the superfluid Maxwell's equations is elucidated upon.","sentences":["A planar superfluid is considered and interpreted in terms of electromagnetism and gravity.","It has previously been suggested that the superfluid flow can be regarded as analogous to an electromagnetic field and that a non-vanishing density gradient give rise to a gravity-like force.","The present work seeks to reconcile these hitherto distinct pictures into a unified exposition of vortex electrodynamics in a curved analogue space-time.","By constructing a theory in which the planar Maxwell's equations are coupled to a (d+1)-dimensional conformally curved space-time, we expose a correspondence between the resulting equations of motion of the embedded fields and the dynamics of a quantum fluid.","Finally, an effective vortex metric whose connection components are torsional is studied and its effect on the superfluid Maxwell's equations is elucidated upon."],"url":"http://arxiv.org/abs/2405.18090v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-28 11:50:46","title":"Semi-nonparametric models of multidimensional matching: an optimal transport approach","abstract":"This paper proposes empirically tractable multidimensional matching models, focusing on worker-job matching. We generalize the parametric model proposed by Lindenlaub (2017), which relies on the assumption of joint normality of observed characteristics of workers and jobs. In our paper, we allow unrestricted distributions of characteristics and show identification of the production technology, and equilibrium wage and matching functions using tools from optimal transport theory. Given identification, we propose efficient, consistent, asymptotically normal sieve estimators. We revisit Lindenlaub's empirical application and show that, between 1990 and 2010, the U.S. economy experienced much larger technological progress favoring cognitive abilities than the original findings suggest. Furthermore, our flexible model specifications provide a significantly better fit for patterns in the evolution of wage inequality.","sentences":["This paper proposes empirically tractable multidimensional matching models, focusing on worker-job matching.","We generalize the parametric model proposed by Lindenlaub (2017), which relies on the assumption of joint normality of observed characteristics of workers and jobs.","In our paper, we allow unrestricted distributions of characteristics and show identification of the production technology, and equilibrium wage and matching functions using tools from optimal transport theory.","Given identification, we propose efficient, consistent, asymptotically normal sieve estimators.","We revisit Lindenlaub's empirical application and show that, between 1990 and 2010, the U.S. economy experienced much larger technological progress favoring cognitive abilities than the original findings suggest.","Furthermore, our flexible model specifications provide a significantly better fit for patterns in the evolution of wage inequality."],"url":"http://arxiv.org/abs/2405.18089v1","category":"econ.EM"}
{"created":"2024-05-28 11:47:12","title":"FlowSDF: Flow Matching for Medical Image Segmentation Using Distance Transforms","abstract":"Medical image segmentation is a crucial task that relies on the ability to accurately identify and isolate regions of interest in medical images. Thereby, generative approaches allow to capture the statistical properties of segmentation masks that are dependent on the respective structures. In this work we propose FlowSDF, an image-guided conditional flow matching framework to represent the signed distance function (SDF) leading to an implicit distribution of segmentation masks. The advantage of leveraging the SDF is a more natural distortion when compared to that of binary masks. By learning a vector field that is directly related to the probability path of a conditional distribution of SDFs, we can accurately sample from the distribution of segmentation masks, allowing for the evaluation of statistical quantities. Thus, this probabilistic representation allows for the generation of uncertainty maps represented by the variance, which can aid in further analysis and enhance the predictive robustness. We qualitatively and quantitatively illustrate competitive performance of the proposed method on a public nuclei and gland segmentation data set, highlighting its utility in medical image segmentation applications.","sentences":["Medical image segmentation is a crucial task that relies on the ability to accurately identify and isolate regions of interest in medical images.","Thereby, generative approaches allow to capture the statistical properties of segmentation masks that are dependent on the respective structures.","In this work we propose FlowSDF, an image-guided conditional flow matching framework to represent the signed distance function (SDF) leading to an implicit distribution of segmentation masks.","The advantage of leveraging the SDF is a more natural distortion when compared to that of binary masks.","By learning a vector field that is directly related to the probability path of a conditional distribution of SDFs, we can accurately sample from the distribution of segmentation masks, allowing for the evaluation of statistical quantities.","Thus, this probabilistic representation allows for the generation of uncertainty maps represented by the variance, which can aid in further analysis and enhance the predictive robustness.","We qualitatively and quantitatively illustrate competitive performance of the proposed method on a public nuclei and gland segmentation data set, highlighting its utility in medical image segmentation applications."],"url":"http://arxiv.org/abs/2405.18087v1","category":"cs.CV"}
{"created":"2024-05-28 11:43:38","title":"On ergodic optimization for unimodal maps","abstract":"In this article, we show that for a typical non-uniformly expanding unimodal map, the unique maximizing measure of a generic Lipschitz function is supported on a periodic orbit.","sentences":["In this article, we show that for a typical non-uniformly expanding unimodal map, the unique maximizing measure of a generic Lipschitz function is supported on a periodic orbit."],"url":"http://arxiv.org/abs/2405.18083v1","category":"math.DS"}
{"created":"2024-05-28 11:42:51","title":"Optimality of Approximate Message Passing Algorithms for Spiked Matrix Models with Rotationally Invariant Noise","abstract":"We study the problem of estimating a rank one signal matrix from an observed matrix generated by corrupting the signal with additive rotationally invariant noise. We develop a new class of approximate message-passing algorithms for this problem and provide a simple and concise characterization of their dynamics in the high-dimensional limit. At each iteration, these algorithms exploit prior knowledge about the noise structure by applying a non-linear matrix denoiser to the eigenvalues of the observed matrix and prior information regarding the signal structure by applying a non-linear iterate denoiser to the previous iterates generated by the algorithm. We exploit our result on the dynamics of these algorithms to derive the optimal choices for the matrix and iterate denoisers. We show that the resulting algorithm achieves the smallest possible asymptotic estimation error among a broad class of iterative algorithms under a fixed iteration budget.","sentences":["We study the problem of estimating a rank one signal matrix from an observed matrix generated by corrupting the signal with additive rotationally invariant noise.","We develop a new class of approximate message-passing algorithms for this problem and provide a simple and concise characterization of their dynamics in the high-dimensional limit.","At each iteration, these algorithms exploit prior knowledge about the noise structure by applying a non-linear matrix denoiser to the eigenvalues of the observed matrix and prior information regarding the signal structure by applying a non-linear iterate denoiser to the previous iterates generated by the algorithm.","We exploit our result on the dynamics of these algorithms to derive the optimal choices for the matrix and iterate denoisers.","We show that the resulting algorithm achieves the smallest possible asymptotic estimation error among a broad class of iterative algorithms under a fixed iteration budget."],"url":"http://arxiv.org/abs/2405.18081v1","category":"math.ST"}
{"created":"2024-05-28 11:39:41","title":"On the (growing) gap between Dirichlet and Neumann eigenvalues","abstract":"We provide an answer to a question raised by Levine and Weinberger in their $1986$ paper concerning the difference between Dirichlet and Neumann eigenvalues of the Laplacian on bounded domains in $\\R^{n}$. More precisely, we show that for a certain class of domains there exists a sequence $p(k)$ such that $\\lambda_{k}\\geq \\mu_{k+ p(k)}$ for sufficiently large $k$. This sequence, which is given explicitly, grows with $k^{1-1/n}$ as $k$ goes to infinity, which we conjecture to be optimal, and may be chosen independently of the domain. We also prove the existence of a sequence, now not given explicitly and only of order $k^{1-3/n}$ but valid for bounded Lipschitz domains in $\\R^{n} (n\\geq4)$, for which a similar inequality holds for all $k$. From these results and the analysis of some particular examples we formulate a conjecture for general Euclidean domains.","sentences":["We provide an answer to a question raised by Levine and Weinberger in their $1986$ paper concerning the difference between Dirichlet and Neumann eigenvalues of the Laplacian on bounded domains in $\\R^{n}$. More precisely, we show that for a certain class of domains there exists a sequence $p(k)$ such that $\\lambda_{k}\\geq \\mu_{k+ p(k)}$ for sufficiently large $k$. This sequence, which is given explicitly, grows with $k^{1-1/n}$ as $k$ goes to infinity, which we conjecture to be optimal, and may be chosen independently of the domain.","We also prove the existence of a sequence, now not given explicitly and only of order $k^{1-3/n}$ but valid for bounded Lipschitz domains in $\\R^{n} (n\\geq4)$, for which a similar inequality holds for all $k$. From these results and the analysis of some particular examples we formulate a conjecture for general Euclidean domains."],"url":"http://arxiv.org/abs/2405.18079v1","category":"math.SP"}
{"created":"2024-05-28 11:37:59","title":"Design Principles for Falsifiable, Replicable and Reproducible Empirical ML Research","abstract":"Empirical research plays a fundamental role in the machine learning domain. At the heart of impactful empirical research lies the development of clear research hypotheses, which then shape the design of experiments. The execution of experiments must be carried out with precision to ensure reliable results, followed by statistical analysis to interpret these outcomes. This process is key to either supporting or refuting initial hypotheses. Despite its importance, there is a high variability in research practices across the machine learning community and no uniform understanding of quality criteria for empirical research. To address this gap, we propose a model for the empirical research process, accompanied by guidelines to uphold the validity of empirical research. By embracing these recommendations, greater consistency, enhanced reliability and increased impact can be achieved.","sentences":["Empirical research plays a fundamental role in the machine learning domain.","At the heart of impactful empirical research lies the development of clear research hypotheses, which then shape the design of experiments.","The execution of experiments must be carried out with precision to ensure reliable results, followed by statistical analysis to interpret these outcomes.","This process is key to either supporting or refuting initial hypotheses.","Despite its importance, there is a high variability in research practices across the machine learning community and no uniform understanding of quality criteria for empirical research.","To address this gap, we propose a model for the empirical research process, accompanied by guidelines to uphold the validity of empirical research.","By embracing these recommendations, greater consistency, enhanced reliability and increased impact can be achieved."],"url":"http://arxiv.org/abs/2405.18077v1","category":"cs.LG"}
{"created":"2024-05-28 11:30:19","title":"Implicitly Guided Design with PropEn: Match your Data to Follow the Gradient","abstract":"Across scientific domains, generating new models or optimizing existing ones while meeting specific criteria is crucial. Traditional machine learning frameworks for guided design use a generative model and a surrogate model (discriminator), requiring large datasets. However, real-world scientific applications often have limited data and complex landscapes, making data-hungry models inefficient or impractical. We propose a new framework, PropEn, inspired by ``matching'', which enables implicit guidance without training a discriminator. By matching each sample with a similar one that has a better property value, we create a larger training dataset that inherently indicates the direction of improvement. Matching, combined with an encoder-decoder architecture, forms a domain-agnostic generative framework for property enhancement. We show that training with a matched dataset approximates the gradient of the property of interest while remaining within the data distribution, allowing efficient design optimization. Extensive evaluations in toy problems and scientific applications, such as therapeutic protein design and airfoil optimization, demonstrate PropEn's advantages over common baselines. Notably, the protein design results are validated with wet lab experiments, confirming the competitiveness and effectiveness of our approach.","sentences":["Across scientific domains, generating new models or optimizing existing ones while meeting specific criteria is crucial.","Traditional machine learning frameworks for guided design use a generative model and a surrogate model (discriminator), requiring large datasets.","However, real-world scientific applications often have limited data and complex landscapes, making data-hungry models inefficient or impractical.","We propose a new framework, PropEn, inspired by ``matching'', which enables implicit guidance without training a discriminator.","By matching each sample with a similar one that has a better property value, we create a larger training dataset that inherently indicates the direction of improvement.","Matching, combined with an encoder-decoder architecture, forms a domain-agnostic generative framework for property enhancement.","We show that training with a matched dataset approximates the gradient of the property of interest while remaining within the data distribution, allowing efficient design optimization.","Extensive evaluations in toy problems and scientific applications, such as therapeutic protein design and airfoil optimization, demonstrate PropEn's advantages over common baselines.","Notably, the protein design results are validated with wet lab experiments, confirming the competitiveness and effectiveness of our approach."],"url":"http://arxiv.org/abs/2405.18075v1","category":"cs.LG"}
{"created":"2024-05-28 11:29:59","title":"A Regularization for Time-Fractional Backward Heat Conduction Problem with Inhomogeneous Source Function","abstract":"Recently, Nair and Danumjaya (2023) introduced a new regularization method for the homogeneous time-fractional backward heat conduction problem (TFBHCP) in a one-dimensional space variable, for determining the initial value function. In this paper, the authors extend the analysis done in the above referred paper to a more general setting of an inhomogeneous time-fractional heat equation involving the higher dimensional state variables and a general elliptic operator. We carry out the analysis for the newly introduced regularization method for the TFBHCP providing optimal order error estimates under a source condition by choosing the regularization parameter appropriately, and also carry out numerical experiments illustrating the theoretical results.","sentences":["Recently, Nair and Danumjaya (2023) introduced a new regularization method for the homogeneous time-fractional backward heat conduction problem (TFBHCP) in a one-dimensional space variable, for determining the initial value function.","In this paper, the authors extend the analysis done in the above referred paper to a more general setting of an inhomogeneous time-fractional heat equation involving the higher dimensional state variables and a general elliptic operator.","We carry out the analysis for the newly introduced regularization method for the TFBHCP providing optimal order error estimates under a source condition by choosing the regularization parameter appropriately, and also carry out numerical experiments illustrating the theoretical results."],"url":"http://arxiv.org/abs/2405.18074v1","category":"math.AP"}
{"created":"2024-05-28 11:29:57","title":"Towards Dialogues for Joint Human-AI Reasoning and Value Alignment","abstract":"We argue that enabling human-AI dialogue, purposed to support joint reasoning (i.e., 'inquiry'), is important for ensuring that AI decision making is aligned with human values and preferences. In particular, we point to logic-based models of argumentation and dialogue, and suggest that the traditional focus on persuasion dialogues be replaced by a focus on inquiry dialogues, and the distinct challenges that joint inquiry raises. Given recent dramatic advances in the performance of large language models (LLMs), and the anticipated increase in their use for decision making, we provide a roadmap for research into inquiry dialogues for supporting joint human-LLM reasoning tasks that are ethically salient, and that thereby require that decisions are value aligned.","sentences":["We argue that enabling human-AI dialogue, purposed to support joint reasoning (i.e., 'inquiry'), is important for ensuring that AI decision making is aligned with human values and preferences.","In particular, we point to logic-based models of argumentation and dialogue, and suggest that the traditional focus on persuasion dialogues be replaced by a focus on inquiry dialogues, and the distinct challenges that joint inquiry raises.","Given recent dramatic advances in the performance of large language models (LLMs), and the anticipated increase in their use for decision making, we provide a roadmap for research into inquiry dialogues for supporting joint human-LLM reasoning tasks that are ethically salient, and that thereby require that decisions are value aligned."],"url":"http://arxiv.org/abs/2405.18073v1","category":"cs.AI"}
{"created":"2024-05-28 11:29:32","title":"Asynchronous BFT Asset Transfer: Quasi-Anonymous, Light, and Consensus-Free","abstract":"This article introduces a new asynchronous Byzantine-tolerant asset transfer system (cryptocurrency) with three noteworthy properties: quasi-anonymity, lightness, and consensus-freedom. Quasi-anonymity means no information is leaked regarding the receivers and amounts of the asset transfers. Lightness means that the underlying cryptographic schemes are \\textit{succinct}, and each process only stores data polylogarithmic in the number of its own transfers.Consensus-freedom means the system does not rely on a total order of asset transfers. The proposed algorithm is the first asset transfer system that simultaneously fulfills all these properties in the presence of asynchrony and Byzantine processes. To obtain them, the paper adopts a modular approach combining a new distributed object called agreement proofs and well-known techniques such as vector commitments, universal accumulators, and zero-knowledge proofs. The paper also presents a new non-trivial universal accumulator implementation that does not need knowledge of the underlying accumulated set to generate (non-)membership proofs, which could benefit other crypto-based applications.","sentences":["This article introduces a new asynchronous Byzantine-tolerant asset transfer system (cryptocurrency) with three noteworthy properties: quasi-anonymity, lightness, and consensus-freedom.","Quasi-anonymity means no information is leaked regarding the receivers and amounts of the asset transfers.","Lightness means that the underlying cryptographic schemes are \\textit{succinct}, and each process only stores data polylogarithmic in the number of its own transfers.","Consensus-freedom means the system does not rely on a total order of asset transfers.","The proposed algorithm is the first asset transfer system that simultaneously fulfills all these properties in the presence of asynchrony and Byzantine processes.","To obtain them, the paper adopts a modular approach combining a new distributed object called agreement proofs and well-known techniques such as vector commitments, universal accumulators, and zero-knowledge proofs.","The paper also presents a new non-trivial universal accumulator implementation that does not need knowledge of the underlying accumulated set to generate (non-)membership proofs, which could benefit other crypto-based applications."],"url":"http://arxiv.org/abs/2405.18072v1","category":"cs.DC"}
{"created":"2024-05-28 11:29:30","title":"Text Modality Oriented Image Feature Extraction for Detecting Diffusion-based DeepFake","abstract":"The widespread use of diffusion methods enables the creation of highly realistic images on demand, thereby posing significant risks to the integrity and safety of online information and highlighting the necessity of DeepFake detection. Our analysis of features extracted by traditional image encoders reveals that both low-level and high-level features offer distinct advantages in identifying DeepFake images produced by various diffusion methods. Inspired by this finding, we aim to develop an effective representation that captures both low-level and high-level features to detect diffusion-based DeepFakes. To address the problem, we propose a text modality-oriented feature extraction method, termed TOFE. Specifically, for a given target image, the representation we discovered is a corresponding text embedding that can guide the generation of the target image with a specific text-to-image model. Experiments conducted across ten diffusion types demonstrate the efficacy of our proposed method.","sentences":["The widespread use of diffusion methods enables the creation of highly realistic images on demand, thereby posing significant risks to the integrity and safety of online information and highlighting the necessity of DeepFake detection.","Our analysis of features extracted by traditional image encoders reveals that both low-level and high-level features offer distinct advantages in identifying DeepFake images produced by various diffusion methods.","Inspired by this finding, we aim to develop an effective representation that captures both low-level and high-level features to detect diffusion-based DeepFakes.","To address the problem, we propose a text modality-oriented feature extraction method, termed TOFE.","Specifically, for a given target image, the representation we discovered is a corresponding text embedding that can guide the generation of the target image with a specific text-to-image model.","Experiments conducted across ten diffusion types demonstrate the efficacy of our proposed method."],"url":"http://arxiv.org/abs/2405.18071v1","category":"cs.CV"}
{"created":"2024-05-28 11:28:59","title":"A Survey of Latent Factor Models in Recommender Systems","abstract":"Recommender systems are essential tools in the digital era, providing personalized content to users in areas like e-commerce, entertainment, and social media. Among the many approaches developed to create these systems, latent factor models have proven particularly effective. This survey systematically reviews latent factor models in recommender systems, focusing on their core principles, methodologies, and recent advancements. The literature is examined through a structured framework covering learning data, model architecture, learning strategies, and optimization techniques. The analysis includes a taxonomy of contributions and detailed discussions on the types of learning data used, such as implicit feedback, trust, and content data, various models such as probabilistic, nonlinear, and neural models, and an exploration of diverse learning strategies like online learning, transfer learning, and active learning. Furthermore, the survey addresses the optimization strategies used to train latent factor models, improving their performance and scalability. By identifying trends, gaps, and potential research directions, this survey aims to provide valuable insights for researchers and practitioners looking to advance the field of recommender systems.","sentences":["Recommender systems are essential tools in the digital era, providing personalized content to users in areas like e-commerce, entertainment, and social media.","Among the many approaches developed to create these systems, latent factor models have proven particularly effective.","This survey systematically reviews latent factor models in recommender systems, focusing on their core principles, methodologies, and recent advancements.","The literature is examined through a structured framework covering learning data, model architecture, learning strategies, and optimization techniques.","The analysis includes a taxonomy of contributions and detailed discussions on the types of learning data used, such as implicit feedback, trust, and content data, various models such as probabilistic, nonlinear, and neural models, and an exploration of diverse learning strategies like online learning, transfer learning, and active learning.","Furthermore, the survey addresses the optimization strategies used to train latent factor models, improving their performance and scalability.","By identifying trends, gaps, and potential research directions, this survey aims to provide valuable insights for researchers and practitioners looking to advance the field of recommender systems."],"url":"http://arxiv.org/abs/2405.18068v1","category":"cs.IR"}
{"created":"2024-05-28 11:25:32","title":"An on-demand resource allocation algorithm for a quantum network hub and its performance analysis","abstract":"To effectively support the execution of quantum network applications for multiple sets of user-controlled quantum nodes, a quantum network must efficiently allocate shared resources. We study traffic models for a type of quantum network hub called an Entanglement Generation Switch (EGS), a device that allocates resources to enable entanglement generation between nodes in response to user-generated demand. We propose an on-demand resource allocation algorithm, where a demand is either blocked if no resources are available or else results in immediate resource allocation. We model the EGS as an Erlang loss system, with demands corresponding to sessions whose arrival is modelled as a Poisson process. To reflect the operation of a practical quantum switch, our model captures scenarios where a resource is allocated for batches of entanglement generation attempts, possibly interleaved with calibration periods for the quantum network nodes. Calibration periods are necessary to correct against drifts or jumps in the physical parameters of a quantum node that occur on a timescale that is long compared to the duration of an attempt. We then derive a formula for the demand blocking probability under three different traffic scenarios using analytical methods from applied probability and queueing theory. We prove an insensitivity theorem which guarantees that the probability a demand is blocked only depends upon the mean duration of each entanglement generation attempt and calibration period, and is not sensitive to the underlying distributions of attempt and calibration period duration. We provide numerical results to support our analysis. Our work is the first analysis of traffic characteristics at an EGS system and provides a valuable analytic tool for devising performance driven resource allocation algorithms.","sentences":["To effectively support the execution of quantum network applications for multiple sets of user-controlled quantum nodes, a quantum network must efficiently allocate shared resources.","We study traffic models for a type of quantum network hub called an Entanglement Generation Switch (EGS), a device that allocates resources to enable entanglement generation between nodes in response to user-generated demand.","We propose an on-demand resource allocation algorithm, where a demand is either blocked if no resources are available or else results in immediate resource allocation.","We model the EGS as an Erlang loss system, with demands corresponding to sessions whose arrival is modelled as a Poisson process.","To reflect the operation of a practical quantum switch, our model captures scenarios where a resource is allocated for batches of entanglement generation attempts, possibly interleaved with calibration periods for the quantum network nodes.","Calibration periods are necessary to correct against drifts or jumps in the physical parameters of a quantum node that occur on a timescale that is long compared to the duration of an attempt.","We then derive a formula for the demand blocking probability under three different traffic scenarios using analytical methods from applied probability and queueing theory.","We prove an insensitivity theorem which guarantees that the probability a demand is blocked only depends upon the mean duration of each entanglement generation attempt and calibration period, and is not sensitive to the underlying distributions of attempt and calibration period duration.","We provide numerical results to support our analysis.","Our work is the first analysis of traffic characteristics at an EGS system and provides a valuable analytic tool for devising performance driven resource allocation algorithms."],"url":"http://arxiv.org/abs/2405.18066v1","category":"quant-ph"}
{"created":"2024-05-28 11:24:41","title":"EffoVPR: Effective Foundation Model Utilization for Visual Place Recognition","abstract":"The task of Visual Place Recognition (VPR) is to predict the location of a query image from a database of geo-tagged images. Recent studies in VPR have highlighted the significant advantage of employing pre-trained foundation models like DINOv2 for the VPR task. However, these models are often deemed inadequate for VPR without further fine-tuning on task-specific data. In this paper, we propose a simple yet powerful approach to better exploit the potential of a foundation model for VPR. We first demonstrate that features extracted from self-attention layers can serve as a powerful re-ranker for VPR. Utilizing these features in a zero-shot manner, our method surpasses previous zero-shot methods and achieves competitive results compared to supervised methods across multiple datasets. Subsequently, we demonstrate that a single-stage method leveraging internal ViT layers for pooling can generate global features that achieve state-of-the-art results, even when reduced to a dimensionality as low as 128D. Nevertheless, incorporating our local foundation features for re-ranking, expands this gap. Our approach further demonstrates remarkable robustness and generalization, achieving state-of-the-art results, with a significant gap, in challenging scenarios, involving occlusion, day-night variations, and seasonal changes.","sentences":["The task of Visual Place Recognition (VPR) is to predict the location of a query image from a database of geo-tagged images.","Recent studies in VPR have highlighted the significant advantage of employing pre-trained foundation models like DINOv2 for the VPR task.","However, these models are often deemed inadequate for VPR without further fine-tuning on task-specific data.","In this paper, we propose a simple yet powerful approach to better exploit the potential of a foundation model for VPR.","We first demonstrate that features extracted from self-attention layers can serve as a powerful re-ranker for VPR.","Utilizing these features in a zero-shot manner, our method surpasses previous zero-shot methods and achieves competitive results compared to supervised methods across multiple datasets.","Subsequently, we demonstrate that a single-stage method leveraging internal ViT layers for pooling can generate global features that achieve state-of-the-art results, even when reduced to a dimensionality as low as 128D. Nevertheless, incorporating our local foundation features for re-ranking, expands this gap.","Our approach further demonstrates remarkable robustness and generalization, achieving state-of-the-art results, with a significant gap, in challenging scenarios, involving occlusion, day-night variations, and seasonal changes."],"url":"http://arxiv.org/abs/2405.18065v1","category":"cs.CV"}
{"created":"2024-05-28 11:24:20","title":"Automated Real-World Sustainability Data Generation from Images of Buildings","abstract":"When data on building features is unavailable, the task of determining how to improve that building in terms of carbon emissions becomes infeasible. We show that from only a set of images, a Large Language Model with appropriate prompt engineering and domain knowledge can successfully estimate a range of building features relevant for sustainability calculations. We compare our novel image-to-data method with a ground truth comprising real building data for 47 apartments and achieve accuracy better than a human performing the same task. We also demonstrate that the method can generate tailored recommendations to the owner on how best to improve their properties and discuss methods to scale the approach.","sentences":["When data on building features is unavailable, the task of determining how to improve that building in terms of carbon emissions becomes infeasible.","We show that from only a set of images, a Large Language Model with appropriate prompt engineering and domain knowledge can successfully estimate a range of building features relevant for sustainability calculations.","We compare our novel image-to-data method with a ground truth comprising real building data for 47 apartments and achieve accuracy better than a human performing the same task.","We also demonstrate that the method can generate tailored recommendations to the owner on how best to improve their properties and discuss methods to scale the approach."],"url":"http://arxiv.org/abs/2405.18064v1","category":"cs.AI"}
{"created":"2024-05-28 11:21:45","title":"Towards Integrating Emerging AI Applications in SE Education","abstract":"Artificial Intelligence (AI) approaches have been incorporated into modern learning environments and software engineering (SE) courses and curricula for several years. However, with the significant rise in popularity of large language models (LLMs) in general, and OpenAI's LLM-powered chatbot ChatGPT in particular in the last year, educators are faced with rapidly changing classroom environments and disrupted teaching principles. Examples range from programming assignment solutions that are fully generated via ChatGPT, to various forms of cheating during exams. However, despite these negative aspects and emerging challenges, AI tools in general, and LLM applications in particular, can also provide significant opportunities in a wide variety of SE courses, supporting both students and educators in meaningful ways. In this early research paper, we present preliminary results of a systematic analysis of current trends in the area of AI, and how they can be integrated into university-level SE curricula, guidelines, and approaches to support both instructors and learners. We collected both teaching and research papers and analyzed their potential usage in SE education, using the ACM Computer Science Curriculum Guidelines CS2023. As an initial outcome, we discuss a series of opportunities for AI applications and further research areas.","sentences":["Artificial Intelligence (AI) approaches have been incorporated into modern learning environments and software engineering (SE) courses and curricula for several years.","However, with the significant rise in popularity of large language models (LLMs) in general, and OpenAI's LLM-powered chatbot ChatGPT in particular in the last year, educators are faced with rapidly changing classroom environments and disrupted teaching principles.","Examples range from programming assignment solutions that are fully generated via ChatGPT, to various forms of cheating during exams.","However, despite these negative aspects and emerging challenges, AI tools in general, and LLM applications in particular, can also provide significant opportunities in a wide variety of SE courses, supporting both students and educators in meaningful ways.","In this early research paper, we present preliminary results of a systematic analysis of current trends in the area of AI, and how they can be integrated into university-level SE curricula, guidelines, and approaches to support both instructors and learners.","We collected both teaching and research papers and analyzed their potential usage in SE education, using the ACM Computer Science Curriculum Guidelines CS2023.","As an initial outcome, we discuss a series of opportunities for AI applications and further research areas."],"url":"http://arxiv.org/abs/2405.18062v1","category":"cs.SE"}
{"created":"2024-05-28 11:13:50","title":"Non-local quasilinear singular SPDEs","abstract":"We study in this short note a counterpart to the quasilinear generalized parabolic Anderson model (gPAM) on the 2-dimensional torus where the coefficients are nonlocal functionals of the solution. Under a positivity assumption on the diffusion coefficient we give a local in time solution theory within the framework of paracontrolled calculus.","sentences":["We study in this short note a counterpart to the quasilinear generalized parabolic Anderson model (gPAM) on the 2-dimensional torus where the coefficients are nonlocal functionals of the solution.","Under a positivity assumption on the diffusion coefficient we give a local in time solution theory within the framework of paracontrolled calculus."],"url":"http://arxiv.org/abs/2405.18057v1","category":"math.AP"}
{"created":"2024-05-28 11:06:15","title":"Algebraic Geometry Codes for Cross-Subspace Alignment in Private Information Retrieval","abstract":"A new framework for interference alignment in secure and private information retrieval (PIR) from colluding servers is proposed, generalizing the original cross-subspace alignment (CSA) codes proposed by Jia, Sun, and Jafar. The general scheme is built on algebraic geometry codes and explicit constructions with replicated storage are given over curves of genus zero and one. It is shown that the proposed scheme offers interesting tradeoffs between the field size, file size, number of colluding servers, and the total number of servers. When the field size is fixed, this translates in some cases to higher retrieval rates than those of the original scheme. In addition, the new schemes exist also in cases where the original ones do not.","sentences":["A new framework for interference alignment in secure and private information retrieval (PIR) from colluding servers is proposed, generalizing the original cross-subspace alignment (CSA) codes proposed by Jia, Sun, and Jafar.","The general scheme is built on algebraic geometry codes and explicit constructions with replicated storage are given over curves of genus zero and one.","It is shown that the proposed scheme offers interesting tradeoffs between the field size, file size, number of colluding servers, and the total number of servers.","When the field size is fixed, this translates in some cases to higher retrieval rates than those of the original scheme.","In addition, the new schemes exist also in cases where the original ones do not."],"url":"http://arxiv.org/abs/2405.18052v1","category":"cs.IT"}
{"created":"2024-05-28 11:06:02","title":"Predicting Progression Events in Multiple Myeloma from Routine Blood Work","abstract":"The ability to accurately predict disease progression is paramount for optimizing multiple myeloma patient care. This study introduces a hybrid neural network architecture, combining Long Short-Term Memory networks with a Conditional Restricted Boltzmann Machine, to predict future blood work of affected patients from a series of historical laboratory results. We demonstrate that our model can replicate the statistical moments of the time series ($0.95~\\pm~0.01~\\geq~R^2~\\geq~0.83~\\pm~0.03$) and forecast future blood work features with high correlation to actual patient data ($0.92\\pm0.02~\\geq~r~\\geq~0.52~\\pm~0.09$). Subsequently, a second Long Short-Term Memory network is employed to detect and annotate disease progression events within the forecasted blood work time series. We show that these annotations enable the prediction of progression events with significant reliability (AUROC$~=~0.88~\\pm~0.01$), up to 12 months in advance (AUROC($t+12~$mos)$~=0.65~\\pm~0.01$). Our system is designed in a modular fashion, featuring separate entities for forecasting and progression event annotation. This structure not only enhances interpretability but also facilitates the integration of additional modules to perform subsequent operations on the generated outputs. Our approach utilizes a minimal set of routine blood work measurements, which avoids the need for expensive or resource-intensive tests and ensures accessibility of the system in clinical routine. This capability allows for individualized risk assessment and making informed treatment decisions tailored to a patient's unique disease kinetics. The represented approach contributes to the development of a scalable and cost-effective virtual human twin system for optimized healthcare resource utilization and improved patient outcomes in multiple myeloma care.","sentences":["The ability to accurately predict disease progression is paramount for optimizing multiple myeloma patient care.","This study introduces a hybrid neural network architecture, combining Long Short-Term Memory networks with a Conditional Restricted Boltzmann Machine, to predict future blood work of affected patients from a series of historical laboratory results.","We demonstrate that our model can replicate the statistical moments of the time series ($0.95~\\pm~0.01~\\geq~R^2~\\geq~0.83~\\pm~0.03$) and forecast future blood work features with high correlation to actual patient data ($0.92\\pm0.02~\\geq~r~\\geq~0.52~\\pm~0.09$).","Subsequently, a second Long Short-Term Memory network is employed to detect and annotate disease progression events within the forecasted blood work time series.","We show that these annotations enable the prediction of progression events with significant reliability (AUROC$~=~0.88~\\pm~0.01$), up to 12 months in advance (AUROC($t+12~$mos)$~=0.65~\\pm~0.01$).","Our system is designed in a modular fashion, featuring separate entities for forecasting and progression event annotation.","This structure not only enhances interpretability but also facilitates the integration of additional modules to perform subsequent operations on the generated outputs.","Our approach utilizes a minimal set of routine blood work measurements, which avoids the need for expensive or resource-intensive tests and ensures accessibility of the system in clinical routine.","This capability allows for individualized risk assessment and making informed treatment decisions tailored to a patient's unique disease kinetics.","The represented approach contributes to the development of a scalable and cost-effective virtual human twin system for optimized healthcare resource utilization and improved patient outcomes in multiple myeloma care."],"url":"http://arxiv.org/abs/2405.18051v1","category":"stat.AP"}
{"created":"2024-05-28 11:05:41","title":"Learning-Based Link Anomaly Detection in Continuous-Time Dynamic Graphs","abstract":"Anomaly detection in continuous-time dynamic graphs is an emerging field yet under-explored in the context of learning-based approaches. In this paper, we pioneer structured analyses of link-level anomalies and graph representation learning for identifying anomalous links in these graphs. First, we introduce a fine-grain taxonomy for edge-level anomalies leveraging structural, temporal, and contextual graph properties. We present a method for generating and injecting such typed anomalies into graphs. Next, we introduce a novel method to generate continuous-time dynamic graphs with consistent patterns across time, structure, and context. To allow temporal graph methods to learn the link anomaly detection task, we extend the generic link prediction setting by: (1) conditioning link existence on contextual edge attributes; and (2) refining the training regime to accommodate diverse perturbations in the negative edge sampler. Building on this, we benchmark methods for anomaly detection. Comprehensive experiments on synthetic and real-world datasets -- featuring synthetic and labeled organic anomalies and employing six state-of-the-art learning methods -- validate our taxonomy and generation processes for anomalies and benign graphs, as well as our approach to adapting link prediction methods for anomaly detection. Our results further reveal that different learning methods excel in capturing different aspects of graph normality and detecting different types of anomalies. We conclude with a comprehensive list of findings highlighting opportunities for future research.","sentences":["Anomaly detection in continuous-time dynamic graphs is an emerging field yet under-explored in the context of learning-based approaches.","In this paper, we pioneer structured analyses of link-level anomalies and graph representation learning for identifying anomalous links in these graphs.","First, we introduce a fine-grain taxonomy for edge-level anomalies leveraging structural, temporal, and contextual graph properties.","We present a method for generating and injecting such typed anomalies into graphs.","Next, we introduce a novel method to generate continuous-time dynamic graphs with consistent patterns across time, structure, and context.","To allow temporal graph methods to learn the link anomaly detection task, we extend the generic link prediction setting by: (1) conditioning link existence on contextual edge attributes; and (2) refining the training regime to accommodate diverse perturbations in the negative edge sampler.","Building on this, we benchmark methods for anomaly detection.","Comprehensive experiments on synthetic and real-world datasets -- featuring synthetic and labeled organic anomalies and employing six state-of-the-art learning methods -- validate our taxonomy and generation processes for anomalies and benign graphs, as well as our approach to adapting link prediction methods for anomaly detection.","Our results further reveal that different learning methods excel in capturing different aspects of graph normality and detecting different types of anomalies.","We conclude with a comprehensive list of findings highlighting opportunities for future research."],"url":"http://arxiv.org/abs/2405.18050v1","category":"cs.LG"}
{"created":"2024-05-28 11:02:01","title":"2BP: 2-Stage Backpropagation","abstract":"As Deep Neural Networks (DNNs) grow in size and complexity, they often exceed the memory capacity of a single accelerator, necessitating the sharding of model parameters across multiple accelerators. Pipeline parallelism is a commonly used sharding strategy for training large DNNs. However, current implementations of pipeline parallelism are being unintentionally bottlenecked by the automatic differentiation tools provided by ML frameworks. This paper introduces 2-stage backpropagation (2BP). By splitting the backward propagation step into two separate stages, we can reduce idle compute time. We tested 2BP on various model architectures and pipelining schedules, achieving increases in throughput in all cases. Using 2BP, we were able to achieve a 1.70x increase in throughput compared to traditional methods when training a LLaMa-like transformer with 7 billion parameters across 4 GPUs.","sentences":["As Deep Neural Networks (DNNs) grow in size and complexity, they often exceed the memory capacity of a single accelerator, necessitating the sharding of model parameters across multiple accelerators.","Pipeline parallelism is a commonly used sharding strategy for training large DNNs.","However, current implementations of pipeline parallelism are being unintentionally bottlenecked by the automatic differentiation tools provided by ML frameworks.","This paper introduces 2-stage backpropagation (2BP).","By splitting the backward propagation step into two separate stages, we can reduce idle compute time.","We tested 2BP on various model architectures and pipelining schedules, achieving increases in throughput in all cases.","Using 2BP, we were able to achieve a 1.70x increase in throughput compared to traditional methods when training a LLaMa-like transformer with 7 billion parameters across 4 GPUs."],"url":"http://arxiv.org/abs/2405.18047v1","category":"cs.LG"}
{"created":"2024-05-28 10:59:33","title":"Cognitive Insights and Stable Coalition Matching for Fostering Multi-Agent Cooperation","abstract":"Cognitive abilities, such as Theory of Mind (ToM), play a vital role in facilitating cooperation in human social interactions. However, our study reveals that agents with higher ToM abilities may not necessarily exhibit better cooperative behavior compared to those with lower ToM abilities. To address this challenge, we propose a novel matching coalition mechanism that leverages the strengths of agents with different ToM levels by explicitly considering belief alignment and specialized abilities when forming coalitions. Our proposed matching algorithm seeks to find stable coalitions that maximize the potential for cooperative behavior and ensure long-term viability. By incorporating cognitive insights into the design of multi-agent systems, our work demonstrates the potential of leveraging ToM to create more sophisticated and human-like coordination strategies that foster cooperation and improve overall system performance.","sentences":["Cognitive abilities, such as Theory of Mind (ToM), play a vital role in facilitating cooperation in human social interactions.","However, our study reveals that agents with higher ToM abilities may not necessarily exhibit better cooperative behavior compared to those with lower ToM abilities.","To address this challenge, we propose a novel matching coalition mechanism that leverages the strengths of agents with different ToM levels by explicitly considering belief alignment and specialized abilities when forming coalitions.","Our proposed matching algorithm seeks to find stable coalitions that maximize the potential for cooperative behavior and ensure long-term viability.","By incorporating cognitive insights into the design of multi-agent systems, our work demonstrates the potential of leveraging ToM to create more sophisticated and human-like coordination strategies that foster cooperation and improve overall system performance."],"url":"http://arxiv.org/abs/2405.18044v1","category":"cs.MA"}
{"created":"2024-05-28 10:56:01","title":"Improving mid-infrared thermal background subtraction with Principal Component Analysis","abstract":"Ground-based large-aperture telescopes, interferometers, and future Extremely Large Telescopes equipped with adaptive-optics systems provide angular resolution and high-contrast performance that are superior to space-based telescopes at thermal-infrared wavelengths. Their sensitivity, however, is critically limited by the high thermal background inherent to ground-based observations in this wavelength regime. We aim to improve the subtraction quality of the thermal-infrared background from ground-based observations, using Principal Component Analysis (PCA). We use data obtained with the Nulling-Optimized Mid-Infrared Camera on the Large Binocular Telescope Interferometer as a proxy for general high-sensitivity, AO-assisted ground-based data. We apply both a classical background subtraction -- using the mean of dedicated background observations -- and a new background subtraction based on a PCA of the background observations. We compare the performances of these two methods in both high-contrast imaging and aperture photometry. Compared to the classical background subtraction approach, PCA background subtraction delivers up to two times better contrasts down to the diffraction limit of the LBT's primary aperture (i.e., 350 mas in N band), that is, in the case of high-contrast imaging. Improvement factor between two and three are obtained over the mean background retrieval within the diffraction limit in the case of aperture photometry. PCA background subtraction significantly improves the sensitivity of ground-based thermal-infrared imaging observations. When applied to LBTI's nulling interferometry data, we expect the method to improve the sensitivity by a similar factor 2-3. This study paves the way to maximising the potential of future infrared ground-based instruments and facilities, such as the future 30m-class telescopes.","sentences":["Ground-based large-aperture telescopes, interferometers, and future Extremely Large Telescopes equipped with adaptive-optics systems provide angular resolution and high-contrast performance that are superior to space-based telescopes at thermal-infrared wavelengths.","Their sensitivity, however, is critically limited by the high thermal background inherent to ground-based observations in this wavelength regime.","We aim to improve the subtraction quality of the thermal-infrared background from ground-based observations, using Principal Component Analysis (PCA).","We use data obtained with the Nulling-Optimized Mid-Infrared Camera on the Large Binocular Telescope Interferometer as a proxy for general high-sensitivity, AO-assisted ground-based data.","We apply both a classical background subtraction -- using the mean of dedicated background observations -- and a new background subtraction based on a PCA of the background observations.","We compare the performances of these two methods in both high-contrast imaging and aperture photometry.","Compared to the classical background subtraction approach, PCA background subtraction delivers up to two times better contrasts down to the diffraction limit of the LBT's primary aperture (i.e., 350 mas in N band), that is, in the case of high-contrast imaging.","Improvement factor between two and three are obtained over the mean background retrieval within the diffraction limit in the case of aperture photometry.","PCA background subtraction significantly improves the sensitivity of ground-based thermal-infrared imaging observations.","When applied to LBTI's nulling interferometry data, we expect the method to improve the sensitivity by a similar factor 2-3.","This study paves the way to maximising the potential of future infrared ground-based instruments and facilities, such as the future 30m-class telescopes."],"url":"http://arxiv.org/abs/2405.18043v1","category":"astro-ph.IM"}
{"created":"2024-05-28 10:54:26","title":"Visualizing the loss landscape of Self-supervised Vision Transformer","abstract":"The Masked autoencoder (MAE) has drawn attention as a representative self-supervised approach for masked image modeling with vision transformers. However, even though MAE shows better generalization capability than fully supervised training from scratch, the reason why has not been explored. In another line of work, the Reconstruction Consistent Masked Auto Encoder (RC-MAE), has been proposed which adopts a self-distillation scheme in the form of an exponential moving average (EMA) teacher into MAE, and it has been shown that the EMA-teacher performs a conditional gradient correction during optimization. To further investigate the reason for better generalization of the self-supervised ViT when trained by MAE (MAE-ViT) and the effect of the gradient correction of RC-MAE from the perspective of optimization, we visualize the loss landscapes of the self-supervised vision transformer by both MAE and RC-MAE and compare them with the supervised ViT (Sup-ViT). Unlike previous loss landscape visualizations of neural networks based on classification task loss, we visualize the loss landscape of ViT by computing pre-training task loss. Through the lens of loss landscapes, we find two interesting observations: (1) MAE-ViT has a smoother and wider overall loss curvature than Sup-ViT. (2) The EMA-teacher allows MAE to widen the region of convexity in both pretraining and linear probing, leading to quicker convergence. To the best of our knowledge, this work is the first to investigate the self-supervised ViT through the lens of the loss landscape.","sentences":["The Masked autoencoder (MAE) has drawn attention as a representative self-supervised approach for masked image modeling with vision transformers.","However, even though MAE shows better generalization capability than fully supervised training from scratch, the reason why has not been explored.","In another line of work, the Reconstruction Consistent Masked Auto Encoder (RC-MAE), has been proposed which adopts a self-distillation scheme in the form of an exponential moving average (EMA) teacher into MAE, and it has been shown that the EMA-teacher performs a conditional gradient correction during optimization.","To further investigate the reason for better generalization of the self-supervised ViT when trained by MAE (MAE-ViT) and the effect of the gradient correction of RC-MAE from the perspective of optimization, we visualize the loss landscapes of the self-supervised vision transformer by both MAE and RC-MAE and compare them with the supervised ViT (Sup-ViT).","Unlike previous loss landscape visualizations of neural networks based on classification task loss, we visualize the loss landscape of ViT by computing pre-training task loss.","Through the lens of loss landscapes, we find two interesting observations: (1) MAE-ViT has a smoother and wider overall loss curvature than Sup-ViT. (2) The EMA-teacher allows MAE to widen the region of convexity in both pretraining and linear probing, leading to quicker convergence.","To the best of our knowledge, this work is the first to investigate the self-supervised ViT through the lens of the loss landscape."],"url":"http://arxiv.org/abs/2405.18042v1","category":"cs.CV"}
{"created":"2024-05-28 10:51:38","title":"Fast-FedUL: A Training-Free Federated Unlearning with Provable Skew Resilience","abstract":"Federated learning (FL) has recently emerged as a compelling machine learning paradigm, prioritizing the protection of privacy for training data. The increasing demand to address issues such as ``the right to be forgotten'' and combat data poisoning attacks highlights the importance of techniques, known as \\textit{unlearning}, which facilitate the removal of specific training data from trained FL models. Despite numerous unlearning methods proposed for centralized learning, they often prove inapplicable to FL due to fundamental differences in the operation of the two learning paradigms. Consequently, unlearning in FL remains in its early stages, presenting several challenges. Many existing unlearning solutions in FL require a costly retraining process, which can be burdensome for clients. Moreover, these methods are primarily validated through experiments, lacking theoretical assurances. In this study, we introduce Fast-FedUL, a tailored unlearning method for FL, which eliminates the need for retraining entirely. Through meticulous analysis of the target client's influence on the global model in each round, we develop an algorithm to systematically remove the impact of the target client from the trained model. In addition to presenting empirical findings, we offer a theoretical analysis delineating the upper bound of our unlearned model and the exact retrained model (the one obtained through retraining using untargeted clients). Experimental results with backdoor attack scenarios indicate that Fast-FedUL effectively removes almost all traces of the target client, while retaining the knowledge of untargeted clients (obtaining a high accuracy of up to 98\\% on the main task). Significantly, Fast-FedUL attains the lowest time complexity, providing a speed that is 1000 times faster than retraining. Our source code is publicly available at \\url{https://github.com/thanhtrunghuynh93/fastFedUL}.","sentences":["Federated learning (FL) has recently emerged as a compelling machine learning paradigm, prioritizing the protection of privacy for training data.","The increasing demand to address issues such as ``the right to be forgotten'' and combat data poisoning attacks highlights the importance of techniques, known as \\textit{unlearning}, which facilitate the removal of specific training data from trained FL models.","Despite numerous unlearning methods proposed for centralized learning, they often prove inapplicable to FL due to fundamental differences in the operation of the two learning paradigms.","Consequently, unlearning in FL remains in its early stages, presenting several challenges.","Many existing unlearning solutions in FL require a costly retraining process, which can be burdensome for clients.","Moreover, these methods are primarily validated through experiments, lacking theoretical assurances.","In this study, we introduce Fast-FedUL, a tailored unlearning method for FL, which eliminates the need for retraining entirely.","Through meticulous analysis of the target client's influence on the global model in each round, we develop an algorithm to systematically remove the impact of the target client from the trained model.","In addition to presenting empirical findings, we offer a theoretical analysis delineating the upper bound of our unlearned model and the exact retrained model (the one obtained through retraining using untargeted clients).","Experimental results with backdoor attack scenarios indicate that Fast-FedUL effectively removes almost all traces of the target client, while retaining the knowledge of untargeted clients (obtaining a high accuracy of up to 98\\% on the main task).","Significantly, Fast-FedUL attains the lowest time complexity, providing a speed that is 1000 times faster than retraining.","Our source code is publicly available at \\url{https://github.com/thanhtrunghuynh93/fastFedUL}."],"url":"http://arxiv.org/abs/2405.18040v1","category":"cs.LG"}
{"created":"2024-05-28 10:50:35","title":"Large Language Model-Driven Curriculum Design for Mobile Networks","abstract":"This paper proposes a novel framework that leverages large language models (LLMs) to automate curriculum design, thereby enhancing the application of reinforcement learning (RL) in mobile networks. As mobile networks evolve towards the 6G era, managing their increasing complexity and dynamic nature poses significant challenges. Conventional RL approaches often suffer from slow convergence and poor generalization due to conflicting objectives and the large state and action spaces associated with mobile networks. To address these shortcomings, we introduce curriculum learning, a method that systematically exposes the RL agent to progressively challenging tasks, improving convergence and generalization. However, curriculum design typically requires extensive domain knowledge and manual human effort. Our framework mitigates this by utilizing the generative capabilities of LLMs to automate the curriculum design process, significantly reducing human effort while improving the RL agent's convergence and performance. We deploy our approach within a simulated mobile network environment and demonstrate improved RL convergence rates, generalization to unseen scenarios, and overall performance enhancements. As a case study, we consider autonomous coordination and user association in mobile networks. Our obtained results highlight the potential of combining LLM-based curriculum generation with RL for managing next-generation wireless networks, marking a significant step towards fully autonomous network operations.","sentences":["This paper proposes a novel framework that leverages large language models (LLMs) to automate curriculum design, thereby enhancing the application of reinforcement learning (RL) in mobile networks.","As mobile networks evolve towards the 6G era, managing their increasing complexity and dynamic nature poses significant challenges.","Conventional RL approaches often suffer from slow convergence and poor generalization due to conflicting objectives and the large state and action spaces associated with mobile networks.","To address these shortcomings, we introduce curriculum learning, a method that systematically exposes the RL agent to progressively challenging tasks, improving convergence and generalization.","However, curriculum design typically requires extensive domain knowledge and manual human effort.","Our framework mitigates this by utilizing the generative capabilities of LLMs to automate the curriculum design process, significantly reducing human effort while improving the RL agent's convergence and performance.","We deploy our approach within a simulated mobile network environment and demonstrate improved RL convergence rates, generalization to unseen scenarios, and overall performance enhancements.","As a case study, we consider autonomous coordination and user association in mobile networks.","Our obtained results highlight the potential of combining LLM-based curriculum generation with RL for managing next-generation wireless networks, marking a significant step towards fully autonomous network operations."],"url":"http://arxiv.org/abs/2405.18039v1","category":"cs.LG"}
{"created":"2024-05-28 10:50:00","title":"Modular functors from non-semisimple 3d TFTs","abstract":"Given a not necessarily semisimple modular tensor category C, we use the corresponding 3d TFT defined in [arXiv:1912.02063] to explicitly describe a modular functor as a symmetric monoidal 2-functor from a 2-category of oriented bordisms to a 2-category of finite linear categories. This recovers a result by Lyubashenko [arXiv:hep-th/9405168] obtained via generators and relations. Pulling back the modular functor for C to a 2-category of bordisms with orientation reversing involution cancels the gluing anomaly, and further pulling back to the original bordism category along a doubling functor leads to the modular functor for the Drinfeld centre Z(C).","sentences":["Given a not necessarily semisimple modular tensor category C, we use the corresponding 3d TFT defined in [arXiv:1912.02063] to explicitly describe a modular functor as a symmetric monoidal 2-functor from a 2-category of oriented bordisms to a 2-category of finite linear categories.","This recovers a result by Lyubashenko [arXiv:hep-th/9405168] obtained via generators and relations.","Pulling back the modular functor for C to a 2-category of bordisms with orientation reversing involution cancels the gluing anomaly, and further pulling back to the original bordism category along a doubling functor leads to the modular functor for the Drinfeld centre Z(C)."],"url":"http://arxiv.org/abs/2405.18038v1","category":"math.QA"}
{"created":"2024-05-28 10:49:17","title":"One-dimensional half-harmonic maps into the circle and their degree","abstract":"Given a half-harmonic map $u\\in \\dot H^{\\frac{1}{2},2}(\\mathbb{R},\\mathbb{S}^1)$ minimizing the fractional Dirichlet energy under Dirichlet boundary conditions in $\\mathbb{R}\\setminus I$, we show the existence of a second half-harmonic map, minimizing the Dirichlet energy in a different homotopy class. This is a first step towards addressing a problem raised by Brezis and is based on the study of the degree of fractional Sobolev maps and a sharp estimate \\`a la Brezis-Coron. We give examples showing that it is in general not possible to minimize in every homotopy class and show a contrast with the 2-dimensional case.","sentences":["Given a half-harmonic map $u\\in \\dot H^{\\frac{1}{2},2}(\\mathbb{R},\\mathbb{S}^1)$ minimizing the fractional Dirichlet energy under Dirichlet boundary conditions in $\\mathbb{R}\\setminus I$, we show the existence of a second half-harmonic map, minimizing the Dirichlet energy in a different homotopy class.","This is a first step towards addressing a problem raised by Brezis and is based on the study of the degree of fractional Sobolev maps and a sharp estimate \\`a la Brezis-Coron.","We give examples showing that it is in general not possible to minimize in every homotopy class and show a contrast with the 2-dimensional case."],"url":"http://arxiv.org/abs/2405.18037v1","category":"math.AP"}
{"created":"2024-05-28 10:40:20","title":"ForecastGrapher: Redefining Multivariate Time Series Forecasting with Graph Neural Networks","abstract":"The challenge of effectively learning inter-series correlations for multivariate time series forecasting remains a substantial and unresolved problem. Traditional deep learning models, which are largely dependent on the Transformer paradigm for modeling long sequences, often fail to integrate information from multiple time series into a coherent and universally applicable model. To bridge this gap, our paper presents ForecastGrapher, a framework reconceptualizes multivariate time series forecasting as a node regression task, providing a unique avenue for capturing the intricate temporal dynamics and inter-series correlations. Our approach is underpinned by three pivotal steps: firstly, generating custom node embeddings to reflect the temporal variations within each series; secondly, constructing an adaptive adjacency matrix to encode the inter-series correlations; and thirdly, augmenting the GNNs' expressive power by diversifying the node feature distribution. To enhance this expressive power, we introduce the Group Feature Convolution GNN (GFC-GNN). This model employs a learnable scaler to segment node features into multiple groups and applies one-dimensional convolutions with different kernel lengths to each group prior to the aggregation phase. Consequently, the GFC-GNN method enriches the diversity of node feature distribution in a fully end-to-end fashion. Through extensive experiments and ablation studies, we show that ForecastGrapher surpasses strong baselines and leading published techniques in the domain of multivariate time series forecasting.","sentences":["The challenge of effectively learning inter-series correlations for multivariate time series forecasting remains a substantial and unresolved problem.","Traditional deep learning models, which are largely dependent on the Transformer paradigm for modeling long sequences, often fail to integrate information from multiple time series into a coherent and universally applicable model.","To bridge this gap, our paper presents ForecastGrapher, a framework reconceptualizes multivariate time series forecasting as a node regression task, providing a unique avenue for capturing the intricate temporal dynamics and inter-series correlations.","Our approach is underpinned by three pivotal steps: firstly, generating custom node embeddings to reflect the temporal variations within each series; secondly, constructing an adaptive adjacency matrix to encode the inter-series correlations; and thirdly, augmenting the GNNs' expressive power by diversifying the node feature distribution.","To enhance this expressive power, we introduce the Group Feature Convolution GNN (GFC-GNN).","This model employs a learnable scaler to segment node features into multiple groups and applies one-dimensional convolutions with different kernel lengths to each group prior to the aggregation phase.","Consequently, the GFC-GNN method enriches the diversity of node feature distribution in a fully end-to-end fashion.","Through extensive experiments and ablation studies, we show that ForecastGrapher surpasses strong baselines and leading published techniques in the domain of multivariate time series forecasting."],"url":"http://arxiv.org/abs/2405.18036v1","category":"cs.LG"}
{"created":"2024-05-28 10:39:10","title":"Instruction Tuning with Retrieval-based Examples Ranking for Aspect-based Sentiment Analysis","abstract":"Aspect-based sentiment analysis (ABSA) identifies sentiment information related to specific aspects and provides deeper market insights to businesses and organizations. With the emergence of large language models (LMs), recent studies have proposed using fixed examples for instruction tuning to reformulate ABSA as a generation task. However, the performance is sensitive to the selection of in-context examples; several retrieval methods are based on surface similarity and are independent of the LM generative objective. This study proposes an instruction learning method with retrieval-based example ranking for ABSA tasks. For each target sample, an LM was applied as a scorer to estimate the likelihood of the output given the input and a candidate example as the prompt, and training examples were labeled as positive or negative by ranking the scores. An alternating training schema is proposed to train both the retriever and LM. Instructional prompts can be constructed using high-quality examples. The LM is used for both scoring and inference, improving the generation efficiency without incurring additional computational costs or training difficulties. Extensive experiments on three ABSA subtasks verified the effectiveness of the proposed method, demonstrating its superiority over various strong baseline models. Code and data are released at https://anonymous.4open.science/r/IT-RER-ABSA-181F.","sentences":["Aspect-based sentiment analysis (ABSA) identifies sentiment information related to specific aspects and provides deeper market insights to businesses and organizations.","With the emergence of large language models (LMs), recent studies have proposed using fixed examples for instruction tuning to reformulate ABSA as a generation task.","However, the performance is sensitive to the selection of in-context examples; several retrieval methods are based on surface similarity and are independent of the LM generative objective.","This study proposes an instruction learning method with retrieval-based example ranking for ABSA tasks.","For each target sample, an LM was applied as a scorer to estimate the likelihood of the output given the input and a candidate example as the prompt, and training examples were labeled as positive or negative by ranking the scores.","An alternating training schema is proposed to train both the retriever and LM.","Instructional prompts can be constructed using high-quality examples.","The LM is used for both scoring and inference, improving the generation efficiency without incurring additional computational costs or training difficulties.","Extensive experiments on three ABSA subtasks verified the effectiveness of the proposed method, demonstrating its superiority over various strong baseline models.","Code and data are released at https://anonymous.4open.science/r/IT-RER-ABSA-181F."],"url":"http://arxiv.org/abs/2405.18035v1","category":"cs.CL"}
{"created":"2024-05-28 10:34:28","title":"RT-GS2: Real-Time Generalizable Semantic Segmentation for 3D Gaussian Representations of Radiance Fields","abstract":"Gaussian Splatting has revolutionized the world of novel view synthesis by achieving high rendering performance in real-time. Recently, studies have focused on enriching these 3D representations with semantic information for downstream tasks. In this paper, we introduce RT-GS2, the first generalizable semantic segmentation method employing Gaussian Splatting. While existing Gaussian Splatting-based approaches rely on scene-specific training, RT-GS2 demonstrates the ability to generalize to unseen scenes. Our method adopts a new approach by first extracting view-independent 3D Gaussian features in a self-supervised manner, followed by a novel View-Dependent / View-Independent (VDVI) feature fusion to enhance semantic consistency over different views. Extensive experimentation on three different datasets showcases RT-GS2's superiority over the state-of-the-art methods in semantic segmentation quality, exemplified by a 8.01% increase in mIoU on the Replica dataset. Moreover, our method achieves real-time performance of 27.03 FPS, marking an astonishing 901 times speedup compared to existing approaches. This work represents a significant advancement in the field by introducing, to the best of our knowledge, the first real-time generalizable semantic segmentation method for 3D Gaussian representations of radiance fields.","sentences":["Gaussian Splatting has revolutionized the world of novel view synthesis by achieving high rendering performance in real-time.","Recently, studies have focused on enriching these 3D representations with semantic information for downstream tasks.","In this paper, we introduce RT-GS2, the first generalizable semantic segmentation method employing Gaussian Splatting.","While existing Gaussian Splatting-based approaches rely on scene-specific training, RT-GS2 demonstrates the ability to generalize to unseen scenes.","Our method adopts a new approach by first extracting view-independent 3D Gaussian features in a self-supervised manner, followed by a novel View-Dependent / View-Independent (VDVI) feature fusion to enhance semantic consistency over different views.","Extensive experimentation on three different datasets showcases RT-GS2's superiority over the state-of-the-art methods in semantic segmentation quality, exemplified by a 8.01% increase in mIoU on the Replica dataset.","Moreover, our method achieves real-time performance of 27.03 FPS, marking an astonishing 901 times speedup compared to existing approaches.","This work represents a significant advancement in the field by introducing, to the best of our knowledge, the first real-time generalizable semantic segmentation method for 3D Gaussian representations of radiance fields."],"url":"http://arxiv.org/abs/2405.18033v1","category":"cs.CV"}
{"created":"2024-05-28 10:31:15","title":"Automatic Abelian Complexities of Parikh-Collinear Fixed Points","abstract":"Parikh-collinear morphisms have the property that all the Parikh vectors of the images of letters are collinear, i.e., the associated adjacency matrix has rank 1. In the conference DLT-WORDS 2023 we showed that fixed points of Parikh-collinear morphisms are automatic. We also showed that the abelian complexity function of a binary fixed point of such a morphism is automatic under some assumptions. In this note, we fully generalize the latter result. Namely, we show that the abelian complexity function of a fixed point of an arbitrary, possibly erasing, Parikh-collinear morphism is automatic. Furthermore, a deterministic finite automaton with output generating this abelian complexity function is provided by an effective procedure. To that end, we discuss the constant of recognizability of a morphism and the related cutting set.","sentences":["Parikh-collinear morphisms have the property that all the Parikh vectors of the images of letters are collinear, i.e., the associated adjacency matrix has rank 1.","In the conference DLT-WORDS 2023 we showed that fixed points of Parikh-collinear morphisms are automatic.","We also showed that the abelian complexity function of a binary fixed point of such a morphism is automatic under some assumptions.","In this note, we fully generalize the latter result.","Namely, we show that the abelian complexity function of a fixed point of an arbitrary, possibly erasing, Parikh-collinear morphism is automatic.","Furthermore, a deterministic finite automaton with output generating this abelian complexity function is provided by an effective procedure.","To that end, we discuss the constant of recognizability of a morphism and the related cutting set."],"url":"http://arxiv.org/abs/2405.18032v1","category":"cs.DM"}
{"created":"2024-05-28 10:25:06","title":"Are Image Distributions Indistinguishable to Humans Indistinguishable to Classifiers?","abstract":"The ultimate goal of generative models is to characterize the data distribution perfectly. For image generation, common metrics of visual quality (e.g., FID), and the truthlikeness of generated images to the human eyes seem to suggest that we are close to achieving it. However, through distribution classification tasks, we find that, in the eyes of classifiers parameterized by neural networks, the strongest diffusion models are still far from this goal. Specifically, classifiers consistently and effortlessly distinguish between real and generated images in various settings. Further, we observe an intriguing discrepancy: classifiers can identify differences between diffusion models with similar performance (e.g., U-ViT-H vs. DiT-XL), but struggle to differentiate between the smallest and largest models in the same family (e.g., EDM2-XS vs. EDM2-XXL), whereas humans exhibit the opposite tendency. As an explanation, our comprehensive empirical study suggests that, unlike humans, classifiers tend to classify images through edge and high-frequency components. We believe that our methodology can serve as a probe to understand how generative models work and inspire further thought on how existing models can be improved and how the abuse of such models can be prevented.","sentences":["The ultimate goal of generative models is to characterize the data distribution perfectly.","For image generation, common metrics of visual quality (e.g., FID), and the truthlikeness of generated images to the human eyes seem to suggest that we are close to achieving it.","However, through distribution classification tasks, we find that, in the eyes of classifiers parameterized by neural networks, the strongest diffusion models are still far from this goal.","Specifically, classifiers consistently and effortlessly distinguish between real and generated images in various settings.","Further, we observe an intriguing discrepancy: classifiers can identify differences between diffusion models with similar performance (e.g., U-ViT-H vs. DiT-XL), but struggle to differentiate between the smallest and largest models in the same family (e.g., EDM2-XS vs. EDM2-XXL), whereas humans exhibit the opposite tendency.","As an explanation, our comprehensive empirical study suggests that, unlike humans, classifiers tend to classify images through edge and high-frequency components.","We believe that our methodology can serve as a probe to understand how generative models work and inspire further thought on how existing models can be improved and how the abuse of such models can be prevented."],"url":"http://arxiv.org/abs/2405.18029v1","category":"cs.CV"}
{"created":"2024-05-28 10:20:29","title":"Edinburgh Clinical NLP at MEDIQA-CORR 2024: Guiding Large Language Models with Hints","abstract":"The MEDIQA-CORR 2024 shared task aims to assess the ability of Large Language Models (LLMs) to identify and correct medical errors in clinical notes. In this study, we evaluate the capability of general LLMs, specifically GPT-3.5 and GPT-4, to identify and correct medical errors with multiple prompting strategies. Recognising the limitation of LLMs in generating accurate corrections only via prompting strategies, we propose incorporating error-span predictions from a smaller, fine-tuned model in two ways: 1) by presenting it as a hint in the prompt and 2) by framing it as multiple-choice questions from which the LLM can choose the best correction. We found that our proposed prompting strategies significantly improve the LLM's ability to generate corrections. Our best-performing solution with 8-shot + CoT + hints ranked sixth in the shared task leaderboard. Additionally, our comprehensive analyses show the impact of the location of the error sentence, the prompted role, and the position of the multiple-choice option on the accuracy of the LLM. This prompts further questions about the readiness of LLM to be implemented in real-world clinical settings.","sentences":["The MEDIQA-CORR 2024 shared task aims to assess the ability of Large Language Models (LLMs) to identify and correct medical errors in clinical notes.","In this study, we evaluate the capability of general LLMs, specifically GPT-3.5 and GPT-4, to identify and correct medical errors with multiple prompting strategies.","Recognising the limitation of LLMs in generating accurate corrections only via prompting strategies, we propose incorporating error-span predictions from a smaller, fine-tuned model in two ways: 1) by presenting it as a hint in the prompt and 2) by framing it as multiple-choice questions from which the LLM can choose the best correction.","We found that our proposed prompting strategies significantly improve the LLM's ability to generate corrections.","Our best-performing solution with 8-shot + CoT + hints ranked sixth in the shared task leaderboard.","Additionally, our comprehensive analyses show the impact of the location of the error sentence, the prompted role, and the position of the multiple-choice option on the accuracy of the LLM.","This prompts further questions about the readiness of LLM to be implemented in real-world clinical settings."],"url":"http://arxiv.org/abs/2405.18028v1","category":"cs.CL"}
{"created":"2024-05-28 10:19:18","title":"TimeChara: Evaluating Point-in-Time Character Hallucination of Role-Playing Large Language Models","abstract":"While Large Language Models (LLMs) can serve as agents to simulate human behaviors (i.e., role-playing agents), we emphasize the importance of point-in-time role-playing. This situates characters at specific moments in the narrative progression for three main reasons: (i) enhancing users' narrative immersion, (ii) avoiding spoilers, and (iii) fostering engagement in fandom role-playing. To accurately represent characters at specific time points, agents must avoid character hallucination, where they display knowledge that contradicts their characters' identities and historical timelines. We introduce TimeChara, a new benchmark designed to evaluate point-in-time character hallucination in role-playing LLMs. Comprising 10,895 instances generated through an automated pipeline, this benchmark reveals significant hallucination issues in current state-of-the-art LLMs (e.g., GPT-4o). To counter this challenge, we propose Narrative-Experts, a method that decomposes the reasoning steps and utilizes narrative experts to reduce point-in-time character hallucinations effectively. Still, our findings with TimeChara highlight the ongoing challenges of point-in-time character hallucination, calling for further study.","sentences":["While Large Language Models (LLMs) can serve as agents to simulate human behaviors (i.e., role-playing agents), we emphasize the importance of point-in-time role-playing.","This situates characters at specific moments in the narrative progression for three main reasons: (i) enhancing users' narrative immersion, (ii) avoiding spoilers, and (iii) fostering engagement in fandom role-playing.","To accurately represent characters at specific time points, agents must avoid character hallucination, where they display knowledge that contradicts their characters' identities and historical timelines.","We introduce TimeChara, a new benchmark designed to evaluate point-in-time character hallucination in role-playing LLMs.","Comprising 10,895 instances generated through an automated pipeline, this benchmark reveals significant hallucination issues in current state-of-the-art LLMs (e.g., GPT-4o).","To counter this challenge, we propose Narrative-Experts, a method that decomposes the reasoning steps and utilizes narrative experts to reduce point-in-time character hallucinations effectively.","Still, our findings with TimeChara highlight the ongoing challenges of point-in-time character hallucination, calling for further study."],"url":"http://arxiv.org/abs/2405.18027v1","category":"cs.CL"}
{"created":"2024-05-28 10:13:18","title":"Unveiling the Power of Diffusion Features For Personalized Segmentation and Retrieval","abstract":"Personalized retrieval and segmentation aim to locate specific instances within a dataset based on an input image and a short description of the reference instance. While supervised methods are effective, they require extensive labeled data for training. Recently, self-supervised foundation models have been introduced to these tasks showing comparable results to supervised methods. However, a significant flaw in these models is evident: they struggle to locate a desired instance when other instances within the same class are presented. In this paper, we explore text-to-image diffusion models for these tasks. Specifically, we propose a novel approach called PDM for Personalized Features Diffusion Matching, that leverages intermediate features of pre-trained text-to-image models for personalization tasks without any additional training. PDM demonstrates superior performance on popular retrieval and segmentation benchmarks, outperforming even supervised methods. We also highlight notable shortcomings in current instance and segmentation datasets and propose new benchmarks for these tasks.","sentences":["Personalized retrieval and segmentation aim to locate specific instances within a dataset based on an input image and a short description of the reference instance.","While supervised methods are effective, they require extensive labeled data for training.","Recently, self-supervised foundation models have been introduced to these tasks showing comparable results to supervised methods.","However, a significant flaw in these models is evident: they struggle to locate a desired instance when other instances within the same class are presented.","In this paper, we explore text-to-image diffusion models for these tasks.","Specifically, we propose a novel approach called PDM for Personalized Features Diffusion Matching, that leverages intermediate features of pre-trained text-to-image models for personalization tasks without any additional training.","PDM demonstrates superior performance on popular retrieval and segmentation benchmarks, outperforming even supervised methods.","We also highlight notable shortcomings in current instance and segmentation datasets and propose new benchmarks for these tasks."],"url":"http://arxiv.org/abs/2405.18025v1","category":"cs.CV"}
{"created":"2024-05-28 10:11:38","title":"Generator polynomials of cyclic expurgated or extended Goppa codes","abstract":"Classical Goppa codes are a well-known class of codes with applications in code-based cryptography, which are a special case of alternant codes. Many papers are devoted to the search for Goppa codes with a cyclic extension or with a cyclic parity-check subcode. Let $\\Bbb F_q$ be a finite field with $q=2^l$ elements, where $l$ is a positive integer. In this paper, we determine all the generator polynomials of cyclic expurgated or extended Goppa codes under some prescribed permutations induced by the projective general linear automorphism $A \\in PGL_2(\\Bbb F_q)$. Moreover, we provide some examples to support our findings.","sentences":["Classical Goppa codes are a well-known class of codes with applications in code-based cryptography, which are a special case of alternant codes.","Many papers are devoted to the search for Goppa codes with a cyclic extension or with a cyclic parity-check subcode.","Let $\\Bbb F_q$ be a finite field with $q=2^l$ elements, where $l$ is a positive integer.","In this paper, we determine all the generator polynomials of cyclic expurgated or extended Goppa codes under some prescribed permutations induced by the projective general linear automorphism $A \\in PGL_2(\\Bbb F_q)$.","Moreover, we provide some examples to support our findings."],"url":"http://arxiv.org/abs/2405.18023v1","category":"cs.IT"}
{"created":"2024-05-28 10:07:17","title":"Mutual Information Analysis of Neuromorphic Coding for Distributed Wireless Spiking Neural Networks","abstract":"Wireless Spiking neural networks (WSNNs) allow energy-efficient device-to-device (D2D) or vehicle-to-everything (V2X) communications, especially while considering edge intelligence and learning for beyond 5G and 6G systems. Recent research work has revealed that distributed wireless SNNs (DWSNNs) show good performance in terms of inference accuracy and low energy consumption of edge devices, under the constraints of limited bandwidth and spike loss probability. In this work, we focus on neuromorphic, AI-native transmission techniques for DWSNNs, quantitatively evaluating the features of different coding algorithms that can be viewed as impulse radio modulations. Specifically, the main contribution of this work is the evaluation of information-theoretic measures that may help in quantifying performance trade-offs among existing neuromorphic coding techniques.","sentences":["Wireless Spiking neural networks (WSNNs) allow energy-efficient device-to-device (D2D) or vehicle-to-everything (V2X) communications, especially while considering edge intelligence and learning for beyond 5G and 6G systems.","Recent research work has revealed that distributed wireless SNNs (DWSNNs) show good performance in terms of inference accuracy and low energy consumption of edge devices, under the constraints of limited bandwidth and spike loss probability.","In this work, we focus on neuromorphic, AI-native transmission techniques for DWSNNs, quantitatively evaluating the features of different coding algorithms that can be viewed as impulse radio modulations.","Specifically, the main contribution of this work is the evaluation of information-theoretic measures that may help in quantifying performance trade-offs among existing neuromorphic coding techniques."],"url":"http://arxiv.org/abs/2405.18019v1","category":"eess.SP"}
{"created":"2024-05-28 10:05:10","title":"A Calibration Tool for Refractive Underwater Vision","abstract":"Many underwater robotic applications relying on vision sensors require proper camera calibration, i.e. knowing the incoming light ray for each pixel in the image. While for the ideal pinhole camera model all viewing rays intersect in a single 3D point, underwater cameras suffer from - possibly multiple - refractions of light rays at the interfaces of water, glass and air. These changes of direction depend on the position and orientation of the camera inside the water-proof housing, as well as on the shape and properties of the optical window, the port, itself. In recent years explicit models for underwater vision behind common ports such as flat or dome port have been proposed, but the underwater community is still lacking a calibration tool which can determine port parameters through refractive calibration. With this work we provide the first open source implementation of an underwater refractive camera calibration toolbox. It allows end-to-end calibration of underwater vision systems, including camera, stereo and housing calibration for systems with dome or flat ports. The implementation is verified using rendered datasets and real-world experiments.","sentences":["Many underwater robotic applications relying on vision sensors require proper camera calibration, i.e. knowing the incoming light ray for each pixel in the image.","While for the ideal pinhole camera model all viewing rays intersect in a single 3D point, underwater cameras suffer from - possibly multiple - refractions of light rays at the interfaces of water, glass and air.","These changes of direction depend on the position and orientation of the camera inside the water-proof housing, as well as on the shape and properties of the optical window, the port, itself.","In recent years explicit models for underwater vision behind common ports such as flat or dome port have been proposed, but the underwater community is still lacking a calibration tool which can determine port parameters through refractive calibration.","With this work we provide the first open source implementation of an underwater refractive camera calibration toolbox.","It allows end-to-end calibration of underwater vision systems, including camera, stereo and housing calibration for systems with dome or flat ports.","The implementation is verified using rendered datasets and real-world experiments."],"url":"http://arxiv.org/abs/2405.18018v1","category":"cs.CV"}
{"created":"2024-05-28 09:59:08","title":"Threshold Effects on the Massless Neutrino in the Canonical Seesaw Mechanism","abstract":"In this work, we revisit the one-loop renormalization group equations (RGEs) among non-degenerate seesaw scales, i.e., threshold effects in the canonical seesaw mechanism, which have been obtained for more than two decades. We find some contributions from the Weinberg operator to its Wilson coefficient, the neutrino Yukawa coupling matrix, and the Higgs quartic coupling absent in the previous calculations. Based on the updated one-loop RGEs, we derive the RGE of the effective neutrino mass matrix's determinant without any approximation. Then, for the first time, we provide a strict proof that the one-loop RG running effects among non-degenerate seesaw scales can not generate a non-zero mass for the initial massless neutrino in the minimal type-I seesaw mechanism or in the canonical one with a rank-degenerate neutrino Yukawa coupling matrix. One has to include two- or higher-loop corrections to achieve a non-zero mass for the massless neutrino.","sentences":["In this work, we revisit the one-loop renormalization group equations (RGEs) among non-degenerate seesaw scales, i.e., threshold effects in the canonical seesaw mechanism, which have been obtained for more than two decades.","We find some contributions from the Weinberg operator to its Wilson coefficient, the neutrino Yukawa coupling matrix, and the Higgs quartic coupling absent in the previous calculations.","Based on the updated one-loop RGEs, we derive the RGE of the effective neutrino mass matrix's determinant without any approximation.","Then, for the first time, we provide a strict proof that the one-loop RG running effects among non-degenerate seesaw scales can not generate a non-zero mass for the initial massless neutrino in the minimal type-I seesaw mechanism or in the canonical one with a rank-degenerate neutrino Yukawa coupling matrix.","One has to include two- or higher-loop corrections to achieve a non-zero mass for the massless neutrino."],"url":"http://arxiv.org/abs/2405.18017v1","category":"hep-ph"}
{"created":"2024-05-28 09:57:37","title":"On Creativity and Open-Endedness","abstract":"Artificial Life (ALife) as an interdisciplinary field draws inspiration and influence from a variety of perspectives. Scientific progress crucially depends, then, on concerted efforts to invite cross-disciplinary dialogue. The goal of this paper is to revitalize discussions of potential connections between the fields of Computational Creativity (CC) and ALife, focusing specifically on the concept of Open-Endedness (OE); the primary goal of CC is to endow artificial systems with creativity, and ALife has dedicated much research effort into studying and synthesizing OE and artificial innovation. However, despite the close proximity of these concepts, their use so far remains confined to their respective communities, and their relationship is largely unclear. We provide historical context for research in both domains, and review the limited work connecting research on creativity and OE explicitly. We then highlight specific questions to be considered, with the eventual goals of (i) decreasing conceptual ambiguity by highlighting similarities and differences between the concepts of OE, (ii) identifying synergy effects of a research agenda that encompasses both OE and creativity, and (iii) establishing a dialogue between ALife and CC research.","sentences":["Artificial Life (ALife) as an interdisciplinary field draws inspiration and influence from a variety of perspectives.","Scientific progress crucially depends, then, on concerted efforts to invite cross-disciplinary dialogue.","The goal of this paper is to revitalize discussions of potential connections between the fields of Computational Creativity (CC) and ALife, focusing specifically on the concept of Open-Endedness (OE); the primary goal of CC is to endow artificial systems with creativity, and ALife has dedicated much research effort into studying and synthesizing OE and artificial innovation.","However, despite the close proximity of these concepts, their use so far remains confined to their respective communities, and their relationship is largely unclear.","We provide historical context for research in both domains, and review the limited work connecting research on creativity and OE explicitly.","We then highlight specific questions to be considered, with the eventual goals of (i) decreasing conceptual ambiguity by highlighting similarities and differences between the concepts of OE, (ii) identifying synergy effects of a research agenda that encompasses both OE and creativity, and (iii) establishing a dialogue between ALife and CC research."],"url":"http://arxiv.org/abs/2405.18016v1","category":"cs.AI"}
{"created":"2024-05-28 09:57:03","title":"Coupled Mamba: Enhanced Multi-modal Fusion with Coupled State Space Model","abstract":"The essence of multi-modal fusion lies in exploiting the complementary information inherent in diverse modalities. However, prevalent fusion methods rely on traditional neural architectures and are inadequately equipped to capture the dynamics of interactions across modalities, particularly in presence of complex intra- and inter-modality correlations. Recent advancements in State Space Models (SSMs), notably exemplified by the Mamba model, have emerged as promising contenders. Particularly, its state evolving process implies stronger modality fusion paradigm, making multi-modal fusion on SSMs an appealing direction. However, fusing multiple modalities is challenging for SSMs due to its hardware-aware parallelism designs. To this end, this paper proposes the Coupled SSM model, for coupling state chains of multiple modalities while maintaining independence of intra-modality state processes. Specifically, in our coupled scheme, we devise an inter-modal hidden states transition scheme, in which the current state is dependent on the states of its own chain and that of the neighbouring chains at the previous time-step. To fully comply with the hardware-aware parallelism, we devise an expedite coupled state transition scheme and derive its corresponding global convolution kernel for parallelism. Extensive experiments on CMU-MOSEI, CH-SIMS, CH-SIMSV2 through multi-domain input verify the effectiveness of our model compared to current state-of-the-art methods, improved F1-Score by 0.4\\%, 0.9\\%, and 2.3\\% on the three datasets respectively, 49\\% faster inference and 83.7\\% GPU memory save. The results demonstrate that Coupled Mamba model is capable of enhanced multi-modal fusion.","sentences":["The essence of multi-modal fusion lies in exploiting the complementary information inherent in diverse modalities.","However, prevalent fusion methods rely on traditional neural architectures and are inadequately equipped to capture the dynamics of interactions across modalities, particularly in presence of complex intra- and inter-modality correlations.","Recent advancements in State Space Models (SSMs), notably exemplified by the Mamba model, have emerged as promising contenders.","Particularly, its state evolving process implies stronger modality fusion paradigm, making multi-modal fusion on SSMs an appealing direction.","However, fusing multiple modalities is challenging for SSMs due to its hardware-aware parallelism designs.","To this end, this paper proposes the Coupled SSM model, for coupling state chains of multiple modalities while maintaining independence of intra-modality state processes.","Specifically, in our coupled scheme, we devise an inter-modal hidden states transition scheme, in which the current state is dependent on the states of its own chain and that of the neighbouring chains at the previous time-step.","To fully comply with the hardware-aware parallelism, we devise an expedite coupled state transition scheme and derive its corresponding global convolution kernel for parallelism.","Extensive experiments on CMU-MOSEI, CH-SIMS, CH-SIMSV2 through multi-domain input verify the effectiveness of our model compared to current state-of-the-art methods, improved F1-Score by 0.4\\%, 0.9\\%, and 2.3\\% on the three datasets respectively, 49\\% faster inference and 83.7\\% GPU memory save.","The results demonstrate that Coupled Mamba model is capable of enhanced multi-modal fusion."],"url":"http://arxiv.org/abs/2405.18014v1","category":"cs.AI"}
{"created":"2024-05-28 09:49:58","title":"Quantum circuits for block encoding of structured matrices in ocean acoustics","abstract":"Block encoding is a data input model commonly used in a quantum computer. It is a technique that embeds a matrix $A$ satisfying $\\left\\|A\\right\\| \\leq 1$ into a larger unitary matrix $U_{A}$. We consider special structured matrices arising from generalized eigenvalue equations in ocean acoustics. We develop their block encoding scheme and further improve it which results lower subnormalisations. And we discuss how to construct quantum circuits of block encoding for the structured matrices. Two numerical examples are used to illustrate the feasibility of our block encoding schemes. The corresponding codes of the quantum circuits in \\verb|MATLAB| are also presented.","sentences":["Block encoding is a data input model commonly used in a quantum computer.","It is a technique that embeds a matrix $A$ satisfying $\\left\\|A\\right\\| \\leq 1$ into a larger unitary matrix $U_{A}$. We consider special structured matrices arising from generalized eigenvalue equations in ocean acoustics.","We develop their block encoding scheme and further improve it which results lower subnormalisations.","And we discuss how to construct quantum circuits of block encoding for the structured matrices.","Two numerical examples are used to illustrate the feasibility of our block encoding schemes.","The corresponding codes of the quantum circuits in \\verb|MATLAB| are also presented."],"url":"http://arxiv.org/abs/2405.18007v1","category":"quant-ph"}
{"created":"2024-05-28 09:48:23","title":"SkinCAP: A Multi-modal Dermatology Dataset Annotated with Rich Medical Captions","abstract":"With the widespread application of artificial intelligence (AI), particularly deep learning (DL) and vision-based large language models (VLLMs), in skin disease diagnosis, the need for interpretability becomes crucial. However, existing dermatology datasets are limited in their inclusion of concept-level meta-labels, and none offer rich medical descriptions in natural language. This deficiency impedes the advancement of LLM-based methods in dermatological diagnosis. To address this gap and provide a meticulously annotated dermatology dataset with comprehensive natural language descriptions, we introduce SkinCAP: a multi-modal dermatology dataset annotated with rich medical captions. SkinCAP comprises 4,000 images sourced from the Fitzpatrick 17k skin disease dataset and the Diverse Dermatology Images dataset, annotated by board-certified dermatologists to provide extensive medical descriptions and captions. Notably, SkinCAP represents the world's first such dataset and is publicly available at https://huggingface.co/datasets/joshuachou/SkinCAP.","sentences":["With the widespread application of artificial intelligence (AI), particularly deep learning (DL) and vision-based large language models (VLLMs), in skin disease diagnosis, the need for interpretability becomes crucial.","However, existing dermatology datasets are limited in their inclusion of concept-level meta-labels, and none offer rich medical descriptions in natural language.","This deficiency impedes the advancement of LLM-based methods in dermatological diagnosis.","To address this gap and provide a meticulously annotated dermatology dataset with comprehensive natural language descriptions, we introduce SkinCAP: a multi-modal dermatology dataset annotated with rich medical captions.","SkinCAP comprises 4,000 images sourced from the Fitzpatrick 17k skin disease dataset and the Diverse Dermatology Images dataset, annotated by board-certified dermatologists to provide extensive medical descriptions and captions.","Notably, SkinCAP represents the world's first such dataset and is publicly available at https://huggingface.co/datasets/joshuachou/SkinCAP."],"url":"http://arxiv.org/abs/2405.18004v1","category":"cs.CV"}
{"created":"2024-05-28 09:46:09","title":"MAVIN: Multi-Action Video Generation with Diffusion Models via Transition Video Infilling","abstract":"Diffusion-based video generation has achieved significant progress, yet generating multiple actions that occur sequentially remains a formidable task. Directly generating a video with sequential actions can be extremely challenging due to the scarcity of fine-grained action annotations and the difficulty in establishing temporal semantic correspondences and maintaining long-term consistency. To tackle this, we propose an intuitive and straightforward solution: splicing multiple single-action video segments sequentially. The core challenge lies in generating smooth and natural transitions between these segments given the inherent complexity and variability of action transitions. We introduce MAVIN (Multi-Action Video INfilling model), designed to generate transition videos that seamlessly connect two given videos, forming a cohesive integrated sequence. MAVIN incorporates several innovative techniques to address challenges in the transition video infilling task. Firstly, a consecutive noising strategy coupled with variable-length sampling is employed to handle large infilling gaps and varied generation lengths. Secondly, boundary frame guidance (BFG) is proposed to address the lack of semantic guidance during transition generation. Lastly, a Gaussian filter mixer (GFM) dynamically manages noise initialization during inference, mitigating train-test discrepancy while preserving generation flexibility. Additionally, we introduce a new metric, CLIP-RS (CLIP Relative Smoothness), to evaluate temporal coherence and smoothness, complementing traditional quality-based metrics. Experimental results on horse and tiger scenarios demonstrate MAVIN's superior performance in generating smooth and coherent video transitions compared to existing methods.","sentences":["Diffusion-based video generation has achieved significant progress, yet generating multiple actions that occur sequentially remains a formidable task.","Directly generating a video with sequential actions can be extremely challenging due to the scarcity of fine-grained action annotations and the difficulty in establishing temporal semantic correspondences and maintaining long-term consistency.","To tackle this, we propose an intuitive and straightforward solution: splicing multiple single-action video segments sequentially.","The core challenge lies in generating smooth and natural transitions between these segments given the inherent complexity and variability of action transitions.","We introduce MAVIN (Multi-Action Video INfilling model), designed to generate transition videos that seamlessly connect two given videos, forming a cohesive integrated sequence.","MAVIN incorporates several innovative techniques to address challenges in the transition video infilling task.","Firstly, a consecutive noising strategy coupled with variable-length sampling is employed to handle large infilling gaps and varied generation lengths.","Secondly, boundary frame guidance (BFG) is proposed to address the lack of semantic guidance during transition generation.","Lastly, a Gaussian filter mixer (GFM) dynamically manages noise initialization during inference, mitigating train-test discrepancy while preserving generation flexibility.","Additionally, we introduce a new metric, CLIP-RS (CLIP Relative Smoothness), to evaluate temporal coherence and smoothness, complementing traditional quality-based metrics.","Experimental results on horse and tiger scenarios demonstrate MAVIN's superior performance in generating smooth and coherent video transitions compared to existing methods."],"url":"http://arxiv.org/abs/2405.18003v1","category":"cs.CV"}
{"created":"2024-05-28 09:43:07","title":"Generic decompositions of Deligne--Lusztig representations","abstract":"Let $G_0$ be a reductive group over $\\mathbb{F}_p$ with simply connected derived subgroup, (geometrically) connected center and Coxeter number $h+1$. We extend Jantzen's generic decomposition pattern from $(2h-1)$-generic to $h$-generic Deligne--Lusztig representations, which is optimal. We also prove several results on the ``obvious'' Jordan--H\\\"older factors of general Deligne--Lusztig representations. As an application we improve the weight elimination result of arXiv:1610.04819 [math.NT]","sentences":["Let $G_0$ be a reductive group over $\\mathbb{F}_p$ with simply connected derived subgroup, (geometrically) connected center and Coxeter number $h+1$.","We extend Jantzen's generic decomposition pattern from $(2h-1)$-generic to $h$-generic Deligne--Lusztig representations, which is optimal.","We also prove several results on the ``obvious'' Jordan--H\\\"older factors of general Deligne--Lusztig representations.","As an application we improve the weight elimination result of arXiv:1610.04819 [math.","NT]"],"url":"http://arxiv.org/abs/2405.18002v1","category":"math.RT"}
{"created":"2024-05-28 09:34:50","title":"Source Echo Chamber: Exploring the Escalation of Source Bias in User, Data, and Recommender System Feedback Loop","abstract":"Recently, researchers have uncovered that neural retrieval models prefer AI-generated content (AIGC), called source bias. Compared to active search behavior, recommendation represents another important means of information acquisition, where users are more prone to source bias. Furthermore, delving into the recommendation scenario, as AIGC becomes integrated within the feedback loop involving users, data, and the recommender system, it progressively contaminates the candidate items, the user interaction history, and ultimately, the data used to train the recommendation models. How and to what extent the source bias affects the neural recommendation models within feedback loop remains unknown. In this study, we extend the investigation of source bias into the realm of recommender systems, specifically examining its impact across different phases of the feedback loop. We conceptualize the progression of AIGC integration into the recommendation content ecosystem in three distinct phases-HGC dominate, HGC-AIGC coexist, and AIGC dominance-each representing past, present, and future states, respectively. Through extensive experiments across three datasets from diverse domains, we demonstrate the prevalence of source bias and reveal a potential digital echo chamber with source bias amplification throughout the feedback loop. This trend risks creating a recommender ecosystem with limited information source, such as AIGC, being disproportionately recommended. To counteract this bias and prevent its escalation in the feedback loop, we introduce a black-box debiasing method that maintains model impartiality towards both HGC and AIGC. Our experimental results validate the effectiveness of the proposed debiasing method, confirming its potential to disrupt the feedback loop.","sentences":["Recently, researchers have uncovered that neural retrieval models prefer AI-generated content (AIGC), called source bias.","Compared to active search behavior, recommendation represents another important means of information acquisition, where users are more prone to source bias.","Furthermore, delving into the recommendation scenario, as AIGC becomes integrated within the feedback loop involving users, data, and the recommender system, it progressively contaminates the candidate items, the user interaction history, and ultimately, the data used to train the recommendation models.","How and to what extent the source bias affects the neural recommendation models within feedback loop remains unknown.","In this study, we extend the investigation of source bias into the realm of recommender systems, specifically examining its impact across different phases of the feedback loop.","We conceptualize the progression of AIGC integration into the recommendation content ecosystem in three distinct phases-HGC dominate, HGC-AIGC coexist, and AIGC dominance-each representing past, present, and future states, respectively.","Through extensive experiments across three datasets from diverse domains, we demonstrate the prevalence of source bias and reveal a potential digital echo chamber with source bias amplification throughout the feedback loop.","This trend risks creating a recommender ecosystem with limited information source, such as AIGC, being disproportionately recommended.","To counteract this bias and prevent its escalation in the feedback loop, we introduce a black-box debiasing method that maintains model impartiality towards both HGC and AIGC.","Our experimental results validate the effectiveness of the proposed debiasing method, confirming its potential to disrupt the feedback loop."],"url":"http://arxiv.org/abs/2405.17998v1","category":"cs.IR"}
{"created":"2024-05-28 09:30:04","title":"Free scalar field theory on a Sobolev space over a bounded interval","abstract":"This paper discusses several functional analytic issues relevant for field theories in the context of the Hamiltonian formulation for a free, massless, scalar field defined on a closed interval of the real line. The fields that we use belong to a Sobolev space with a scalar product. As we show this choice is useful because it leads to an explicit representation of the points in the fibers of the phase space (the cotangent bundle of the configuration space). The dynamical role of the boundary of the spatial manifold where the fields are defined is analyzed.","sentences":["This paper discusses several functional analytic issues relevant for field theories in the context of the Hamiltonian formulation for a free, massless, scalar field defined on a closed interval of the real line.","The fields that we use belong to a Sobolev space with a scalar product.","As we show this choice is useful because it leads to an explicit representation of the points in the fibers of the phase space (the cotangent bundle of the configuration space).","The dynamical role of the boundary of the spatial manifold where the fields are defined is analyzed."],"url":"http://arxiv.org/abs/2405.17996v1","category":"hep-th"}
{"created":"2024-05-28 17:57:40","title":"Exploring the Evolution of Altruistic Punishment with a PDE Model of Cultural Multilevel Selection","abstract":"Two mechanisms that have been used to study the evolution of cooperative behavior are altruistic punishment, in which cooperative individuals pay additional costs to punish defection, and multilevel selection, in which competition between groups can help to counteract individual-level incentives to cheat. Boyd, Gintis, Bowles, and Richerson have used simulation models of cultural evolution to suggest that altruistic punishment and pairwise group-level competition can work in concert to promote cooperation, even when neither mechanism can do so on its own. In this paper, we formulate a PDE model for multilevel selection motivated by the approach of Boyd and coauthors, modeling individual-level birth-death competition with a replicator equation based on individual payoffs and describing group-level competition with pairwise conflicts based on differences in the average payoffs of the competing groups. Building off of existing PDE models for multilevel selection with frequency-independent group-level competition, we use analytical and numerical techniques to understand how the forms of individual and average payoffs can impact the long-time ability to sustain altruistic punishment in group-structured populations. We find several interesting differences between the behavior of our new PDE model with pairwise group-level competition and existing multilevel PDE models, including the observation that our new model can feature a non-monotonic dependence of the long-time collective payoff on the strength of altruistic punishment. Going forward, our PDE framework can serve as a way to connect and compare disparate approaches for understanding multilevel selection across the literature in evolutionary biology and anthropology.","sentences":["Two mechanisms that have been used to study the evolution of cooperative behavior are altruistic punishment, in which cooperative individuals pay additional costs to punish defection, and multilevel selection, in which competition between groups can help to counteract individual-level incentives to cheat.","Boyd, Gintis, Bowles, and Richerson have used simulation models of cultural evolution to suggest that altruistic punishment and pairwise group-level competition can work in concert to promote cooperation, even when neither mechanism can do so on its own.","In this paper, we formulate a PDE model for multilevel selection motivated by the approach of Boyd and coauthors, modeling individual-level birth-death competition with a replicator equation based on individual payoffs and describing group-level competition with pairwise conflicts based on differences in the average payoffs of the competing groups.","Building off of existing PDE models for multilevel selection with frequency-independent group-level competition, we use analytical and numerical techniques to understand how the forms of individual and average payoffs can impact the long-time ability to sustain altruistic punishment in group-structured populations.","We find several interesting differences between the behavior of our new PDE model with pairwise group-level competition and existing multilevel PDE models, including the observation that our new model can feature a non-monotonic dependence of the long-time collective payoff on the strength of altruistic punishment.","Going forward, our PDE framework can serve as a way to connect and compare disparate approaches for understanding multilevel selection across the literature in evolutionary biology and anthropology."],"url":"http://arxiv.org/abs/2405.18419v1","category":"q-bio.PE"}
{"created":"2024-05-28 17:14:02","title":"Thai Winograd Schemas: A Benchmark for Thai Commonsense Reasoning","abstract":"Commonsense reasoning is one of the important aspect of natural language understanding, with several benchmarks developed to evaluate it. However, only a few of these benchmarks are available in languages other than English. Developing parallel benchmarks facilitates cross-lingual evaluation, enabling a better understanding of different languages. This research introduces a collection of Winograd Schemas in Thai, a novel dataset designed to evaluate commonsense reasoning capabilities in the context of the Thai language.   Through a methodology involving native speakers, professional translators, and thorough validation, the schemas aim to closely reflect Thai language nuances, idioms, and cultural references while maintaining ambiguity and commonsense challenges. We evaluate the performance of popular large language models on this benchmark, revealing their strengths, limitations, and providing insights into the current state-of-the-art. Results indicate that while models like GPT-4 and Claude-3-Opus achieve high accuracy in English, their performance significantly drops in Thai, highlighting the need for further advancements in multilingual commonsense reasoning.","sentences":["Commonsense reasoning is one of the important aspect of natural language understanding, with several benchmarks developed to evaluate it.","However, only a few of these benchmarks are available in languages other than English.","Developing parallel benchmarks facilitates cross-lingual evaluation, enabling a better understanding of different languages.","This research introduces a collection of Winograd Schemas in Thai, a novel dataset designed to evaluate commonsense reasoning capabilities in the context of the Thai language.   ","Through a methodology involving native speakers, professional translators, and thorough validation, the schemas aim to closely reflect Thai language nuances, idioms, and cultural references while maintaining ambiguity and commonsense challenges.","We evaluate the performance of popular large language models on this benchmark, revealing their strengths, limitations, and providing insights into the current state-of-the-art.","Results indicate that while models like GPT-4 and Claude-3-Opus achieve high accuracy in English, their performance significantly drops in Thai, highlighting the need for further advancements in multilingual commonsense reasoning."],"url":"http://arxiv.org/abs/2405.18375v1","category":"cs.CL"}
{"created":"2024-05-28 16:43:57","title":"Dataset Growth","abstract":"Deep learning benefits from the growing abundance of available data. Meanwhile, efficiently dealing with the growing data scale has become a challenge. Data publicly available are from different sources with various qualities, and it is impractical to do manual cleaning against noise and redundancy given today's data scale. There are existing techniques for cleaning/selecting the collected data. However, these methods are mainly proposed for offline settings that target one of the cleanness and redundancy problems. In practice, data are growing exponentially with both problems. This leads to repeated data curation with sub-optimal efficiency. To tackle this challenge, we propose InfoGrowth, an efficient online algorithm for data cleaning and selection, resulting in a growing dataset that keeps up to date with awareness of cleanliness and diversity. InfoGrowth can improve data quality/efficiency on both single-modal and multi-modal tasks, with an efficient and scalable design. Its framework makes it practical for real-world data engines.","sentences":["Deep learning benefits from the growing abundance of available data.","Meanwhile, efficiently dealing with the growing data scale has become a challenge.","Data publicly available are from different sources with various qualities, and it is impractical to do manual cleaning against noise and redundancy given today's data scale.","There are existing techniques for cleaning/selecting the collected data.","However, these methods are mainly proposed for offline settings that target one of the cleanness and redundancy problems.","In practice, data are growing exponentially with both problems.","This leads to repeated data curation with sub-optimal efficiency.","To tackle this challenge, we propose InfoGrowth, an efficient online algorithm for data cleaning and selection, resulting in a growing dataset that keeps up to date with awareness of cleanliness and diversity.","InfoGrowth can improve data quality/efficiency on both single-modal and multi-modal tasks, with an efficient and scalable design.","Its framework makes it practical for real-world data engines."],"url":"http://arxiv.org/abs/2405.18347v1","category":"cs.LG"}
{"created":"2024-05-28 16:03:27","title":"Construction of continuous collective energy landscapes for large amplitude nuclear many-body problems","abstract":"Several protocols are proposed to build continuous energy surfaces of many-body quantum systems, regarding both energy and states. The standard variational principle is augmented with constraints on state overlap, ensuring arbitrary precision on continuity. As an illustration, the lowest energy and excited state paths relevant for the $^{240}$Pu asymmetric fission are studied. The scission is clearly signed, with a neutron excess in the neck, the ultimate glue before its rupture. Our approach can potentially connect any couple of Hilbert space states, which opens up new horizons for various applications.","sentences":["Several protocols are proposed to build continuous energy surfaces of many-body quantum systems, regarding both energy and states.","The standard variational principle is augmented with constraints on state overlap, ensuring arbitrary precision on continuity.","As an illustration, the lowest energy and excited state paths relevant for the $^{240}$Pu asymmetric fission are studied.","The scission is clearly signed, with a neutron excess in the neck, the ultimate glue before its rupture.","Our approach can potentially connect any couple of Hilbert space states, which opens up new horizons for various applications."],"url":"http://arxiv.org/abs/2405.18312v1","category":"nucl-th"}
{"created":"2024-05-28 15:25:04","title":"Can We Trust Recommender System Fairness Evaluation? The Role of Fairness and Relevance","abstract":"Relevance and fairness are two major objectives of recommender systems (RSs). Recent work proposes measures of RS fairness that are either independent from relevance (fairness-only) or conditioned on relevance (joint measures). While fairness-only measures have been studied extensively, we look into whether joint measures can be trusted. We collect all joint evaluation measures of RS relevance and fairness, and ask: How much do they agree with each other? To what extent do they agree with relevance/fairness measures? How sensitive are they to changes in rank position, or to increasingly fair and relevant recommendations? We empirically study for the first time the behaviour of these measures across 4 real-world datasets and 4 recommenders. We find that most of these measures: i) correlate weakly with one another and even contradict each other at times; ii) are less sensitive to rank position changes than relevance- and fairness-only measures, meaning that they are less granular than traditional RS measures; and iii) tend to compress scores at the low end of their range, meaning that they are not very expressive. We counter the above limitations with a set of guidelines on the appropriate usage of such measures, i.e., they should be used with caution due to their tendency to contradict each other and of having a very small empirical range.","sentences":["Relevance and fairness are two major objectives of recommender systems (RSs).","Recent work proposes measures of RS fairness that are either independent from relevance (fairness-only) or conditioned on relevance (joint measures).","While fairness-only measures have been studied extensively, we look into whether joint measures can be trusted.","We collect all joint evaluation measures of RS relevance and fairness, and ask: How much do they agree with each other?","To what extent do they agree with relevance/fairness measures?","How sensitive are they to changes in rank position, or to increasingly fair and relevant recommendations?","We empirically study for the first time the behaviour of these measures across 4 real-world datasets and 4 recommenders.","We find that most of these measures: i) correlate weakly with one another and even contradict each other at times; ii) are less sensitive to rank position changes than relevance- and fairness-only measures, meaning that they are less granular than traditional RS measures; and iii) tend to compress scores at the low end of their range, meaning that they are not very expressive.","We counter the above limitations with a set of guidelines on the appropriate usage of such measures, i.e., they should be used with caution due to their tendency to contradict each other and of having a very small empirical range."],"url":"http://arxiv.org/abs/2405.18276v1","category":"cs.IR"}
{"created":"2024-05-28 15:16:49","title":"The influence of solvent on surface adsorption and desorption","abstract":"The adsorption and desorption of reactants and products from a solid surface is essential for achieving sustained surface chemical reactions. At a liquid-solid interface, these processes can involve the collective reorganization of interfacial solvent molecules in order to accommodate the adsorbing or desorbing species. Identifying the role of solvent in adsorption and desorption is important for advancing our understanding of surface chemical rates and mechanisms and for enabling the rational design and optimization of surface chemical systems. In this manuscript we use all-atom molecular dynamics simulation and transition path sampling to identify water's role in the desorption of CO from a Pt(100) surface in contact with liquid water. We demonstrate that the solvation of CO, as quantified by the water coordination number, is an essential component of the desorption reaction coordinate. We use meta dynamics to compute the desorption free energy surface and conclude based on its features that desorption proceeds via a two-step mechanism whereby the final detachment of CO from the surface is preceded by the formation of a nascent solvation shell.","sentences":["The adsorption and desorption of reactants and products from a solid surface is essential for achieving sustained surface chemical reactions.","At a liquid-solid interface, these processes can involve the collective reorganization of interfacial solvent molecules in order to accommodate the adsorbing or desorbing species.","Identifying the role of solvent in adsorption and desorption is important for advancing our understanding of surface chemical rates and mechanisms and for enabling the rational design and optimization of surface chemical systems.","In this manuscript we use all-atom molecular dynamics simulation and transition path sampling to identify water's role in the desorption of CO from a Pt(100) surface in contact with liquid water.","We demonstrate that the solvation of CO, as quantified by the water coordination number, is an essential component of the desorption reaction coordinate.","We use meta dynamics to compute the desorption free energy surface and conclude based on its features that desorption proceeds via a two-step mechanism whereby the final detachment of CO from the surface is preceded by the formation of a nascent solvation shell."],"url":"http://arxiv.org/abs/2405.18263v1","category":"physics.chem-ph"}
{"created":"2024-05-28 14:37:47","title":"Triple-top-gate technique for studying the strongly-interacting 2D electron systems in heterostructures","abstract":"We develop a technique that dramatically reduces the contact resistances and depletes a shunting channel between the contacts in the ungated area outside the Hall bar in ultra-high mobility SiGe/Si/SiGe heterostructures. It involves the creation of three overlapping independent gates deposited on top of the structure and allows transport measurements to be performed at millikelvin temperatures in the strongly interacting limit at low electron densities, where the energy of the electron-electron interactions dominates all other energy scales. This design allows one to observe the two-threshold voltage-current characteristics that are a signature for the collective depinning and sliding of the electron solid.","sentences":["We develop a technique that dramatically reduces the contact resistances and depletes a shunting channel between the contacts in the ungated area outside the Hall bar in ultra-high mobility SiGe/Si/SiGe heterostructures.","It involves the creation of three overlapping independent gates deposited on top of the structure and allows transport measurements to be performed at millikelvin temperatures in the strongly interacting limit at low electron densities, where the energy of the electron-electron interactions dominates all other energy scales.","This design allows one to observe the two-threshold voltage-current characteristics that are a signature for the collective depinning and sliding of the electron solid."],"url":"http://arxiv.org/abs/2405.18229v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-28 14:20:38","title":"A Survey on Modern Code Review: Progresses, Challenges and Opportunities","abstract":"Over the past decade, modern code review (MCR) has been deemed as a crucial practice of software quality assurance, which is applied to improve software quality and transfer development knowledge within a software team. Despite its importance, MCR is often a complicated and time-consuming activity for practitioners. In recent years, many studies that are dedicated to the comprehension and the improvement of MCR have been explored so that the MCR activity can be carried out more conveniently and efficiently. To provide researchers and practitioners a clear understanding of the current research status on MCR, this paper conducts a systematic literature review of the past years. Given the collected 231 surveyed papers, this paper makes the following five contributions: First, we analyze the research trends of related MCR studies. Second, we provide a taxonomy for the current MCR, encompassing both Improvement Techniques and Understanding Studies. Third, we present the concrete research progress of each novel MCR methodology and prototype tool. Fourth, we exploit the main empirical insights from empirical study and user study that are helpful to improve MCR. Finally, we sum up unsolved challenges and outline several possible research opportunities in the future.","sentences":["Over the past decade, modern code review (MCR) has been deemed as a crucial practice of software quality assurance, which is applied to improve software quality and transfer development knowledge within a software team.","Despite its importance, MCR is often a complicated and time-consuming activity for practitioners.","In recent years, many studies that are dedicated to the comprehension and the improvement of MCR have been explored so that the MCR activity can be carried out more conveniently and efficiently.","To provide researchers and practitioners a clear understanding of the current research status on MCR, this paper conducts a systematic literature review of the past years.","Given the collected 231 surveyed papers, this paper makes the following five contributions: First, we analyze the research trends of related MCR studies.","Second, we provide a taxonomy for the current MCR, encompassing both Improvement Techniques and Understanding Studies.","Third, we present the concrete research progress of each novel MCR methodology and prototype tool.","Fourth, we exploit the main empirical insights from empirical study and user study that are helpful to improve MCR.","Finally, we sum up unsolved challenges and outline several possible research opportunities in the future."],"url":"http://arxiv.org/abs/2405.18216v1","category":"cs.SE"}
{"created":"2024-05-28 13:22:46","title":"Sampling metastable systems using collective variables and Jarzynski-Crooks paths","abstract":"We consider the problem of sampling a high dimensional multimodal target probability measure. We assume that a good proposal kernel to move only a subset of the degrees of freedoms (also known as collective variables) is known a priori. This proposal kernel can for example be built using normalizing flows. We show how to extend the move from the collective variable space to the full space and how to implement an accept-reject step in order to get a reversible chain with respect to a target probability measure. The accept-reject step does not require to know the marginal of the original measure in the collective variable (namely to know the free energy). The obtained algorithm admits several variants, some of them being very close to methods which have been proposed previously in the literature. We show how the obtained acceptance ratio can be expressed in terms of the work which appears in the Jarzynski-Crooks equality, at least for some variants. Numerical illustrations demonstrate the efficiency of the approach on various simple test cases, and allow us to compare the variants of the algorithm.","sentences":["We consider the problem of sampling a high dimensional multimodal target probability measure.","We assume that a good proposal kernel to move only a subset of the degrees of freedoms (also known as collective variables) is known a priori.","This proposal kernel can for example be built using normalizing flows.","We show how to extend the move from the collective variable space to the full space and how to implement an accept-reject step in order to get a reversible chain with respect to a target probability measure.","The accept-reject step does not require to know the marginal of the original measure in the collective variable (namely to know the free energy).","The obtained algorithm admits several variants, some of them being very close to methods which have been proposed previously in the literature.","We show how the obtained acceptance ratio can be expressed in terms of the work which appears in the Jarzynski-Crooks equality, at least for some variants.","Numerical illustrations demonstrate the efficiency of the approach on various simple test cases, and allow us to compare the variants of the algorithm."],"url":"http://arxiv.org/abs/2405.18160v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-28 13:07:38","title":"Search for a standard model-like Higgs boson in the mass range between 70 and 110 GeV in the diphoton final state in proton-proton collisions at $\\sqrt{s}$ = 13 TeV","abstract":"The results of a search for a standard model-like Higgs boson decaying into two photons in the mass range between 70 and 110 GeV are presented. The analysis uses the data set collected by the CMS experiment in proton-proton collisions at $\\sqrt{s}$ = 13 TeV corresponding to integrated luminosities of 36.3 fb$^{-1}$, 41.5 fb$^{-1}$ and 54.4 fb$^{-1}$ during the 2016, 2017, and 2018 LHC running periods, respectively. No significant excess over the background expectation is observed and 95% confidence level upper limits are set on the product of the cross section and branching fraction for decays of an additional Higgs boson into two photons. The maximum deviation with respect to the background is seen for a mass hypothesis of 95.4 GeV with a local (global) significance of 2.9 (1.3) standard deviations. The observed upper limit ranges from 15 to 73 fb.","sentences":["The results of a search for a standard model-like Higgs boson decaying into two photons in the mass range between 70 and 110 GeV are presented.","The analysis uses the data set collected by the CMS experiment in proton-proton collisions at $\\sqrt{s}$ = 13 TeV corresponding to integrated luminosities of 36.3 fb$^{-1}$, 41.5 fb$^{-1}$","and 54.4 fb$^{-1}$","during the 2016, 2017, and","2018 LHC running periods, respectively.","No significant excess over the background expectation is observed and 95% confidence level upper limits are set on the product of the cross section and branching fraction for decays of an additional Higgs boson into two photons.","The maximum deviation with respect to the background is seen for a mass hypothesis of 95.4 GeV with a local (global) significance of 2.9 (1.3) standard deviations.","The observed upper limit ranges from 15 to 73 fb."],"url":"http://arxiv.org/abs/2405.18149v1","category":"hep-ex"}
{"created":"2024-05-28 13:01:53","title":"Data-Driven Distributionally Robust System Level Synthesis","abstract":"We present a novel approach for the control of uncertain, linear time-invariant systems, which are perturbed by potentially unbounded, additive disturbances. We propose a \\emph{doubly robust} data-driven state-feedback controller to ensure reliable performance against both model mismatch and disturbance distribution uncertainty. Our controller, which leverages the System Level Synthesis parameterization, is designed as the solution to a distributionally robust finite-horizon optimal control problem. The goal is to minimize a cost function while satisfying constraints against the worst-case realization of the uncertainty, which is quantified using distributional ambiguity sets. The latter are defined as balls in the Wasserstein metric centered on the predictive empirical distribution computed from a set of collected trajectory data. By harnessing techniques from robust control and distributionally robust optimization, we characterize the distributional shift between the predictive and the actual closed-loop distributions, and highlight its dependency on the model mismatch and the uncertainty about the disturbance distribution. We also provide bounds on the number of samples required to achieve a desired confidence level and propose a tractable approximate formulation for the doubly robust data-driven controller. To demonstrate the effectiveness of our approach, we present a numerical example showcasing the performance of the proposed algorithm.","sentences":["We present a novel approach for the control of uncertain, linear time-invariant systems, which are perturbed by potentially unbounded, additive disturbances.","We propose a \\emph{doubly robust} data-driven state-feedback controller to ensure reliable performance against both model mismatch and disturbance distribution uncertainty.","Our controller, which leverages the System Level Synthesis parameterization, is designed as the solution to a distributionally robust finite-horizon optimal control problem.","The goal is to minimize a cost function while satisfying constraints against the worst-case realization of the uncertainty, which is quantified using distributional ambiguity sets.","The latter are defined as balls in the Wasserstein metric centered on the predictive empirical distribution computed from a set of collected trajectory data.","By harnessing techniques from robust control and distributionally robust optimization, we characterize the distributional shift between the predictive and the actual closed-loop distributions, and highlight its dependency on the model mismatch and the uncertainty about the disturbance distribution.","We also provide bounds on the number of samples required to achieve a desired confidence level and propose a tractable approximate formulation for the doubly robust data-driven controller.","To demonstrate the effectiveness of our approach, we present a numerical example showcasing the performance of the proposed algorithm."],"url":"http://arxiv.org/abs/2405.18142v1","category":"math.OC"}
{"created":"2024-05-28 12:19:15","title":"Charge Collection Performance of 4H-SiC LGAD","abstract":"The 4H-SiC material exhibits good detection performance, but there are still many problems like signal distortion and poor signal quality. The 4H-SiC low gain avalanche detector (LGAD) has been fabricated for the first time to solve these problems, which named SICAR (SIlicon CARbide). The results of electrical characteristics and charge collection performance of the 4H-SiC LGAD are reported. The influence of different metal thicknesses on the leakage current of the device is studied.~By optimizing the fabrication process, the leakage current of the detector is reduced by four orders of magnitude. The experimental results confirm this 4H-SiC LGAD has an obvious gain structure, the gain factor of the SICAR is reported to be about 2 at 150 V. The charge collection efficiency (CCE) of the device was analyzed using $\\alpha$ particle incidence of 5.54 MeV, and the CCE is 90\\% @100~V. This study provides a novel 4H-SiC LGAD radiation detector for application in the field of high energy particle physics.","sentences":["The 4H-SiC material exhibits good detection performance, but there are still many problems like signal distortion and poor signal quality.","The 4H-SiC low gain avalanche detector (LGAD) has been fabricated for the first time to solve these problems, which named SICAR (SIlicon CARbide).","The results of electrical characteristics and charge collection performance of the 4H-SiC LGAD are reported.","The influence of different metal thicknesses on the leakage current of the device is studied.~By optimizing the fabrication process, the leakage current of the detector is reduced by four orders of magnitude.","The experimental results confirm this 4H-SiC LGAD has an obvious gain structure, the gain factor of the SICAR is reported to be about 2 at 150 V.","The charge collection efficiency (CCE) of the device was analyzed using $\\alpha$ particle incidence of 5.54 MeV, and the CCE is 90\\% @100~","V. This study provides a novel 4H-SiC LGAD radiation detector for application in the field of high energy particle physics."],"url":"http://arxiv.org/abs/2405.18112v1","category":"physics.ins-det"}
{"created":"2024-05-28 11:19:13","title":"PRFashion24: A Dataset for Sentiment Analysis of Fashion Products Reviews in Persian","abstract":"The PRFashion24 dataset is a comprehensive Persian dataset collected from various online fashion stores, spanning from April 2020 to March 2024. With 767,272 reviews, it is the first dataset in its kind that encompasses diverse categories within the fashion industry in the Persian language. The goal of this study is to harness deep learning techniques, specifically Long Short-Term Memory (LSTM) networks and a combination of Bidirectional LSTM and Convolutional Neural Network (BiLSTM-CNN), to analyze and reveal sentiments towards online fashion shopping. The LSTM model yielded an accuracy of 81.23%, while the BiLSTM-CNN model reached 82.89%. This research aims not only to introduce a diverse dataset in the field of fashion but also to enhance the public's understanding of opinions on online fashion shopping, which predominantly reflect a positive sentiment. Upon publication, both the optimized models and the PRFashion24 dataset will be available on GitHub.","sentences":["The PRFashion24 dataset is a comprehensive Persian dataset collected from various online fashion stores, spanning from April 2020 to March 2024.","With 767,272 reviews, it is the first dataset in its kind that encompasses diverse categories within the fashion industry in the Persian language.","The goal of this study is to harness deep learning techniques, specifically Long Short-Term Memory (LSTM) networks and a combination of Bidirectional LSTM and Convolutional Neural Network (BiLSTM-CNN), to analyze and reveal sentiments towards online fashion shopping.","The LSTM model yielded an accuracy of 81.23%, while the BiLSTM-CNN model reached 82.89%.","This research aims not only to introduce a diverse dataset in the field of fashion but also to enhance the public's understanding of opinions on online fashion shopping, which predominantly reflect a positive sentiment.","Upon publication, both the optimized models and the PRFashion24 dataset will be available on GitHub."],"url":"http://arxiv.org/abs/2405.18060v1","category":"cs.CL"}
{"created":"2024-05-28 11:15:54","title":"Rank-Refining Seed Selection Methods for Budget Constrained Influence Maximisation in Multilayer Networks under Linear Threshold Model","abstract":"The problem of selecting an optimal seed set to maximise influence in networks has been a subject of intense research in recent years. However, despite numerous works addressing this area, it remains a topic that requires further elaboration. Most often, it is considered within the scope of classically defined graphs with a spreading model in the form of Independent Cascades. In this work, we focus on the problem of budget-constrained influence maximisation in multilayer networks using a Linear Threshold Model. Both the graph model and the spreading process we employ are less prevalent in the literature, even though their application allows for a more precise representation of the opinion dynamics in social networks. This paper aims to answer which of the sixteen evaluated seed selection methods is the most effective and how similar they are. Additionally, we focus our analysis on the impact of spreading model parameters, network characteristics, a budget, and the seed selection methods on the diffusion effectiveness in multilayer networks. Our contribution also includes extending several centrality measures and heuristics to the case of such graphs. The results indicate that all the factors mentioned above collectively contribute to the effectiveness of influence maximisation. Moreover, there is no seed selection method which always provides the best results. However, the seeds chosen with VoteRank-based methods (especially with the $v-rnk-m$ variant we propose) usually provide the most extensive diffusion.","sentences":["The problem of selecting an optimal seed set to maximise influence in networks has been a subject of intense research in recent years.","However, despite numerous works addressing this area, it remains a topic that requires further elaboration.","Most often, it is considered within the scope of classically defined graphs with a spreading model in the form of Independent Cascades.","In this work, we focus on the problem of budget-constrained influence maximisation in multilayer networks using a Linear Threshold Model.","Both the graph model and the spreading process we employ are less prevalent in the literature, even though their application allows for a more precise representation of the opinion dynamics in social networks.","This paper aims to answer which of the sixteen evaluated seed selection methods is the most effective and how similar they are.","Additionally, we focus our analysis on the impact of spreading model parameters, network characteristics, a budget, and the seed selection methods on the diffusion effectiveness in multilayer networks.","Our contribution also includes extending several centrality measures and heuristics to the case of such graphs.","The results indicate that all the factors mentioned above collectively contribute to the effectiveness of influence maximisation.","Moreover, there is no seed selection method which always provides the best results.","However, the seeds chosen with VoteRank-based methods (especially with the $v-rnk-m$ variant we propose) usually provide the most extensive diffusion."],"url":"http://arxiv.org/abs/2405.18059v1","category":"cs.SI"}
{"created":"2024-05-28 09:28:52","title":"DMT-JEPA: Discriminative Masked Targets for Joint-Embedding Predictive Architecture","abstract":"The joint-embedding predictive architecture (JEPA) recently has shown impressive results in extracting visual representations from unlabeled imagery under a masking strategy. However, we reveal its disadvantages, notably its insufficient understanding of local semantics. This deficiency originates from masked modeling in the embedding space, resulting in a reduction of discriminative power and can even lead to the neglect of critical local semantics. To bridge this gap, we introduce DMT-JEPA, a novel masked modeling objective rooted in JEPA, specifically designed to generate discriminative latent targets from neighboring information. Our key idea is simple: we consider a set of semantically similar neighboring patches as a target of a masked patch. To be specific, the proposed DMT-JEPA (a) computes feature similarities between each masked patch and its corresponding neighboring patches to select patches having semantically meaningful relations, and (b) employs lightweight cross-attention heads to aggregate features of neighboring patches as the masked targets. Consequently, DMT-JEPA demonstrates strong discriminative power, offering benefits across a diverse spectrum of downstream tasks. Through extensive experiments, we demonstrate our effectiveness across various visual benchmarks, including ImageNet-1K image classification, ADE20K semantic segmentation, and COCO object detection tasks. Code is available at: \\url{https://github.com/DMTJEPA/DMTJEPA}.","sentences":["The joint-embedding predictive architecture (JEPA) recently has shown impressive results in extracting visual representations from unlabeled imagery under a masking strategy.","However, we reveal its disadvantages, notably its insufficient understanding of local semantics.","This deficiency originates from masked modeling in the embedding space, resulting in a reduction of discriminative power and can even lead to the neglect of critical local semantics.","To bridge this gap, we introduce DMT-JEPA, a novel masked modeling objective rooted in JEPA, specifically designed to generate discriminative latent targets from neighboring information.","Our key idea is simple: we consider a set of semantically similar neighboring patches as a target of a masked patch.","To be specific, the proposed DMT-JEPA (a) computes feature similarities between each masked patch and its corresponding neighboring patches to select patches having semantically meaningful relations, and (b) employs lightweight cross-attention heads to aggregate features of neighboring patches as the masked targets.","Consequently, DMT-JEPA demonstrates strong discriminative power, offering benefits across a diverse spectrum of downstream tasks.","Through extensive experiments, we demonstrate our effectiveness across various visual benchmarks, including ImageNet-1K image classification, ADE20K semantic segmentation, and COCO object detection tasks.","Code is available at: \\url{https://github.com/DMTJEPA/DMTJEPA}."],"url":"http://arxiv.org/abs/2405.17995v1","category":"cs.CV"}
{"created":"2024-05-28 09:25:34","title":"Influence of mid-infrared Galactic bubble on surroundings: A case study on IRAS 16489-4431","abstract":"We studied the influence of a massive star on a mid-infrared bubble and its surrounding gas in the IRAS\\,16489-4431 star-forming region using multi-wavelength data. The {\\it Spitzer} mid-infrared band images revealed the shocked nature of the bubble. Analyses showed that the bubble is developed by a massive star owing to its strong radiation pressure. Evidence of collected material along the edge of the bubble was noted by the cold gas tracer line observed using Atacama Millimeter/submillimeter Array (ALMA). The presence of dense dust cores with bi-polar outflows and young stellar objects toward the collected material is suggestive of active star formation possibly influenced by the expansion of the radiation driven bubble.","sentences":["We studied the influence of a massive star on a mid-infrared bubble and its surrounding gas in the IRAS\\,16489-4431 star-forming region using multi-wavelength data.","The {\\it Spitzer} mid-infrared band images revealed the shocked nature of the bubble.","Analyses showed that the bubble is developed by a massive star owing to its strong radiation pressure.","Evidence of collected material along the edge of the bubble was noted by the cold gas tracer line observed using Atacama Millimeter/submillimeter Array (ALMA).","The presence of dense dust cores with bi-polar outflows and young stellar objects toward the collected material is suggestive of active star formation possibly influenced by the expansion of the radiation driven bubble."],"url":"http://arxiv.org/abs/2405.17993v1","category":"astro-ph.GA"}
{"created":"2024-05-28 09:24:52","title":"fMRI predictors based on language models of increasing complexity recover brain left lateralization","abstract":"Over the past decade, studies of naturalistic language processing where participants are scanned while listening to continuous text have flourished. Using word embeddings at first, then large language models, researchers have created encoding models to analyze the brain signals. Presenting these models with the same text as the participants allows to identify brain areas where there is a significant correlation between the functional magnetic resonance imaging (fMRI) time series and the ones predicted by the models' artificial neurons. One intriguing finding from these studies is that they have revealed highly symmetric bilateral activation patterns, somewhat at odds with the well-known left lateralization of language processing. Here, we report analyses of an fMRI dataset where we manipulate the complexity of large language models, testing 28 pretrained models from 8 different families, ranging from 124M to 14.2B parameters. First, we observe that the performance of models in predicting brain responses follows a scaling law, where the fit with brain activity increases linearly with the logarithm of the number of parameters of the model (and its performance on natural language processing tasks). Second, we show that a left-right asymmetry gradually appears as model size increases, and that the difference in left-right brain correlations also follows a scaling law. Whereas the smallest models show no asymmetry, larger models fit better and better left hemispheric activations than right hemispheric ones. This finding reconciles computational analyses of brain activity using large language models with the classic observation from aphasic patients showing left hemisphere dominance for language.","sentences":["Over the past decade, studies of naturalistic language processing where participants are scanned while listening to continuous text have flourished.","Using word embeddings at first, then large language models, researchers have created encoding models to analyze the brain signals.","Presenting these models with the same text as the participants allows to identify brain areas where there is a significant correlation between the functional magnetic resonance imaging (fMRI) time series and the ones predicted by the models' artificial neurons.","One intriguing finding from these studies is that they have revealed highly symmetric bilateral activation patterns, somewhat at odds with the well-known left lateralization of language processing.","Here, we report analyses of an fMRI dataset where we manipulate the complexity of large language models, testing 28 pretrained models from 8 different families, ranging from 124M to 14.2B parameters.","First, we observe that the performance of models in predicting brain responses follows a scaling law, where the fit with brain activity increases linearly with the logarithm of the number of parameters of the model (and its performance on natural language processing tasks).","Second, we show that a left-right asymmetry gradually appears as model size increases, and that the difference in left-right brain correlations also follows a scaling law.","Whereas the smallest models show no asymmetry, larger models fit better and better left hemispheric activations than right hemispheric ones.","This finding reconciles computational analyses of brain activity using large language models with the classic observation from aphasic patients showing left hemisphere dominance for language."],"url":"http://arxiv.org/abs/2405.17992v1","category":"cs.CL"}
{"created":"2024-05-28 09:23:14","title":"VeLoRA: Memory Efficient Training using Rank-1 Sub-Token Projections","abstract":"Large language models (LLMs) have recently emerged as powerful tools for tackling many language-processing tasks. Despite their success, training and fine-tuning these models is still far too computationally and memory intensive. In this paper, we identify and characterise the important components needed for effective model convergence using gradient descent. In doing so we find that the intermediate activations used to implement backpropagation can be excessively compressed without incurring any degradation in performance. This result leads us to a cheap and memory-efficient algorithm for both fine-tuning and pre-training LLMs. The proposed algorithm simply divides the tokens up into smaller sub-tokens before projecting them onto a fixed 1-dimensional subspace during the forward pass. These features are then coarsely reconstructed during the backward pass to implement the update rules. We confirm the effectiveness of our algorithm as being complimentary to many state-of-the-art PEFT methods on the VTAB-1k fine-tuning benchmark. Furthermore, we outperform QLoRA for fine-tuning LLaMA and show competitive performance against other memory-efficient pre-training methods on the large-scale C4 dataset.","sentences":["Large language models (LLMs) have recently emerged as powerful tools for tackling many language-processing tasks.","Despite their success, training and fine-tuning these models is still far too computationally and memory intensive.","In this paper, we identify and characterise the important components needed for effective model convergence using gradient descent.","In doing so we find that the intermediate activations used to implement backpropagation can be excessively compressed without incurring any degradation in performance.","This result leads us to a cheap and memory-efficient algorithm for both fine-tuning and pre-training LLMs.","The proposed algorithm simply divides the tokens up into smaller sub-tokens before projecting them onto a fixed 1-dimensional subspace during the forward pass.","These features are then coarsely reconstructed during the backward pass to implement the update rules.","We confirm the effectiveness of our algorithm as being complimentary to many state-of-the-art PEFT methods on the VTAB-1k fine-tuning benchmark.","Furthermore, we outperform QLoRA for fine-tuning LLaMA and show competitive performance against other memory-efficient pre-training methods on the large-scale C4 dataset."],"url":"http://arxiv.org/abs/2405.17991v1","category":"cs.CV"}
{"created":"2024-05-28 09:12:44","title":"Peering into the Mind of Language Models: An Approach for Attribution in Contextual Question Answering","abstract":"With the enhancement in the field of generative artificial intelligence (AI), contextual question answering has become extremely relevant. Attributing model generations to the input source document is essential to ensure trustworthiness and reliability. We observe that when large language models (LLMs) are used for contextual question answering, the output answer often consists of text copied verbatim from the input prompt which is linked together with \"glue text\" generated by the LLM. Motivated by this, we propose that LLMs have an inherent awareness from where the text was copied, likely captured in the hidden states of the LLM. We introduce a novel method for attribution in contextual question answering, leveraging the hidden state representations of LLMs. Our approach bypasses the need for extensive model retraining and retrieval model overhead, offering granular attributions and preserving the quality of generated answers. Our experimental results demonstrate that our method performs on par or better than GPT-4 at identifying verbatim copied segments in LLM generations and in attributing these segments to their source. Importantly, our method shows robust performance across various LLM architectures, highlighting its broad applicability. Additionally, we present Verifiability-granular, an attribution dataset which has token level annotations for LLM generations in the contextual question answering setup.","sentences":["With the enhancement in the field of generative artificial intelligence (AI), contextual question answering has become extremely relevant.","Attributing model generations to the input source document is essential to ensure trustworthiness and reliability.","We observe that when large language models (LLMs) are used for contextual question answering, the output answer often consists of text copied verbatim from the input prompt which is linked together with \"glue text\" generated by the LLM.","Motivated by this, we propose that LLMs have an inherent awareness from where the text was copied, likely captured in the hidden states of the LLM.","We introduce a novel method for attribution in contextual question answering, leveraging the hidden state representations of LLMs.","Our approach bypasses the need for extensive model retraining and retrieval model overhead, offering granular attributions and preserving the quality of generated answers.","Our experimental results demonstrate that our method performs on par or better than GPT-4 at identifying verbatim copied segments in LLM generations and in attributing these segments to their source.","Importantly, our method shows robust performance across various LLM architectures, highlighting its broad applicability.","Additionally, we present Verifiability-granular, an attribution dataset which has token level annotations for LLM generations in the contextual question answering setup."],"url":"http://arxiv.org/abs/2405.17980v1","category":"cs.CL"}
{"created":"2024-05-28 09:06:38","title":"FASTopic: A Fast, Adaptive, Stable, and Transferable Topic Modeling Paradigm","abstract":"Topic models have been evolving rapidly over the years, from conventional to recent neural models. However, existing topic models generally struggle with either effectiveness, efficiency, or stability, highly impeding their practical applications. In this paper, we propose FASTopic, a fast, adaptive, stable, and transferable topic model. FASTopic follows a new paradigm: Dual Semantic-relation Reconstruction (DSR). Instead of previous conventional, neural VAE-based or clustering-based methods, DSR discovers latent topics by reconstruction through modeling the semantic relations among document, topic, and word embeddings. This brings about a neat and efficient topic modeling framework. We further propose a novel Embedding Transport Plan (ETP) method. Rather than early straightforward approaches, ETP explicitly regularizes the semantic relations as optimal transport plans. This addresses the relation bias issue and thus leads to effective topic modeling. Extensive experiments on benchmark datasets demonstrate that our FASTopic shows superior effectiveness, efficiency, adaptivity, stability, and transferability, compared to state-of-the-art baselines across various scenarios. Our code is available at https://github.com/bobxwu/FASTopic .","sentences":["Topic models have been evolving rapidly over the years, from conventional to recent neural models.","However, existing topic models generally struggle with either effectiveness, efficiency, or stability, highly impeding their practical applications.","In this paper, we propose FASTopic, a fast, adaptive, stable, and transferable topic model.","FASTopic follows a new paradigm: Dual Semantic-relation Reconstruction (DSR).","Instead of previous conventional, neural VAE-based or clustering-based methods, DSR discovers latent topics by reconstruction through modeling the semantic relations among document, topic, and word embeddings.","This brings about a neat and efficient topic modeling framework.","We further propose a novel Embedding Transport Plan (ETP) method.","Rather than early straightforward approaches, ETP explicitly regularizes the semantic relations as optimal transport plans.","This addresses the relation bias issue and thus leads to effective topic modeling.","Extensive experiments on benchmark datasets demonstrate that our FASTopic shows superior effectiveness, efficiency, adaptivity, stability, and transferability, compared to state-of-the-art baselines across various scenarios.","Our code is available at https://github.com/bobxwu/FASTopic ."],"url":"http://arxiv.org/abs/2405.17978v1","category":"cs.CL"}
{"created":"2024-05-28 09:06:18","title":"Aligning to Thousands of Preferences via System Message Generalization","abstract":"Although humans inherently have diverse values, current large language model (LLM) alignment methods often assume that aligning LLMs with the general public's preferences is optimal. A major challenge in adopting a more individualized approach to LLM alignment is its lack of scalability, as it involves repeatedly acquiring preference data and training new reward models and LLMs for each individual's preferences. To address these challenges, we propose a new paradigm where users specify what they value most within the system message, steering the LLM's generation behavior to better align with the user's intentions. However, a naive application of such an approach is non-trivial since LLMs are typically trained on a uniform system message (e.g., \"You are a helpful assistant\") which limits their ability to generalize to diverse, unseen system messages. To improve this generalization, we create the Multifaceted Collection, a preference dataset with 192k combinations of values beyond generic helpfulness and harmlessness, spanning 65k user instructions. Using this dataset, we train a 7B LLM called Janus and test it on 921 prompts from 5 benchmarks (AlpacaEval 2.0, FLASK, Koala, MT-Bench, and Self-Instruct) by adding various unseen system messages that reflect user preferences. Janus achieves tie+win rate of 75.2%, 72.4%, and 66.4% against Mistral 7B Instruct v0.2, GPT-3.5 Turbo, and GPT-4, respectively. Unexpectedly, on three benchmarks focused on response helpfulness (AlpacaEval 2.0, MT-Bench, Arena Hard Auto v0.1), Janus also outperforms LLaMA 3 8B Instruct by a +4.0%, +0.1%, +3.0% margin, underscoring that training with a vast array of system messages could also enhance alignment to the general public's preference as well. Our code, dataset, benchmark, and models are available at https://github.com/kaistAI/Janus.","sentences":["Although humans inherently have diverse values, current large language model (LLM) alignment methods often assume that aligning LLMs with the general public's preferences is optimal.","A major challenge in adopting a more individualized approach to LLM alignment is its lack of scalability, as it involves repeatedly acquiring preference data and training new reward models and LLMs for each individual's preferences.","To address these challenges, we propose a new paradigm where users specify what they value most within the system message, steering the LLM's generation behavior to better align with the user's intentions.","However, a naive application of such an approach is non-trivial since LLMs are typically trained on a uniform system message (e.g., \"You are a helpful assistant\") which limits their ability to generalize to diverse, unseen system messages.","To improve this generalization, we create the Multifaceted Collection, a preference dataset with 192k combinations of values beyond generic helpfulness and harmlessness, spanning 65k user instructions.","Using this dataset, we train a 7B LLM called Janus and test it on 921 prompts from 5 benchmarks (AlpacaEval 2.0, FLASK, Koala, MT-Bench, and Self-Instruct) by adding various unseen system messages that reflect user preferences.","Janus achieves tie+win rate of 75.2%, 72.4%, and 66.4% against Mistral 7B Instruct v0.2, GPT-3.5 Turbo, and GPT-4, respectively.","Unexpectedly, on three benchmarks focused on response helpfulness (AlpacaEval 2.0, MT-Bench, Arena Hard Auto v0.1), Janus also outperforms LLaMA 3 8B Instruct by a +4.0%, +0.1%, +3.0% margin, underscoring that training with a vast array of system messages could also enhance alignment to the general public's preference as well.","Our code, dataset, benchmark, and models are available at https://github.com/kaistAI/Janus."],"url":"http://arxiv.org/abs/2405.17977v1","category":"cs.CL"}
{"created":"2024-05-28 09:05:08","title":"Yuan 2.0-M32: Mixture of Experts with Attention Router","abstract":"Yuan 2.0-M32, with a similar base architecture as Yuan-2.0 2B, uses a mixture-of-experts architecture with 32 experts of which 2 experts are active. A new router network, Attention Router, is proposed and adopted for a more efficient selection of experts, which boosts the accuracy of 3.8% compared to the model with classical router network. Yuan 2.0-M32 is trained with 2000B tokens from scratch, and the training computation consumption is only 9.25% of a dense model at the same parameter scale. Yuan 2.0-M32 demonstrates competitive capability on coding, math, and various domains of expertise, with only 3.7B active parameters of 40B in total, and 7.4 GFlops forward computation per token, both of which are only 1/19 of Llama3-70B. Yuan 2.0-M32 surpass Llama3-70B on MATH and ARC-Challenge benchmark, with accuracy of 55.89 and 95.8 respectively. The models and source codes of Yuan 2.0-M32 are released at Github.","sentences":["Yuan 2.0-M32, with a similar base architecture as Yuan-2.0 2B, uses a mixture-of-experts architecture with 32 experts of which 2 experts are active.","A new router network, Attention Router, is proposed and adopted for a more efficient selection of experts, which boosts the accuracy of 3.8% compared to the model with classical router network.","Yuan 2.0-M32 is trained with 2000B tokens from scratch, and the training computation consumption is only 9.25% of a dense model at the same parameter scale.","Yuan 2.0-M32 demonstrates competitive capability on coding, math, and various domains of expertise, with only 3.7B active parameters of 40B in total, and 7.4 GFlops forward computation per token, both of which are only 1/19 of Llama3-70B. Yuan 2.0-M32 surpass Llama3-70B on MATH and ARC-Challenge benchmark, with accuracy of 55.89 and 95.8 respectively.","The models and source codes of Yuan 2.0-M32 are released at Github."],"url":"http://arxiv.org/abs/2405.17976v1","category":"cs.AI"}
{"created":"2024-05-28 09:04:13","title":"Recent Trends in Personalized Dialogue Generation: A Review of Datasets, Methodologies, and Evaluations","abstract":"Enhancing user engagement through personalization in conversational agents has gained significance, especially with the advent of large language models that generate fluent responses. Personalized dialogue generation, however, is multifaceted and varies in its definition -- ranging from instilling a persona in the agent to capturing users' explicit and implicit cues. This paper seeks to systemically survey the recent landscape of personalized dialogue generation, including the datasets employed, methodologies developed, and evaluation metrics applied. Covering 22 datasets, we highlight benchmark datasets and newer ones enriched with additional features. We further analyze 17 seminal works from top conferences between 2021-2023 and identify five distinct types of problems. We also shed light on recent progress by LLMs in personalized dialogue generation. Our evaluation section offers a comprehensive summary of assessment facets and metrics utilized in these works. In conclusion, we discuss prevailing challenges and envision prospect directions for future research in personalized dialogue generation.","sentences":["Enhancing user engagement through personalization in conversational agents has gained significance, especially with the advent of large language models that generate fluent responses.","Personalized dialogue generation, however, is multifaceted and varies in its definition -- ranging from instilling a persona in the agent to capturing users' explicit and implicit cues.","This paper seeks to systemically survey the recent landscape of personalized dialogue generation, including the datasets employed, methodologies developed, and evaluation metrics applied.","Covering 22 datasets, we highlight benchmark datasets and newer ones enriched with additional features.","We further analyze 17 seminal works from top conferences between 2021-2023 and identify five distinct types of problems.","We also shed light on recent progress by LLMs in personalized dialogue generation.","Our evaluation section offers a comprehensive summary of assessment facets and metrics utilized in these works.","In conclusion, we discuss prevailing challenges and envision prospect directions for future research in personalized dialogue generation."],"url":"http://arxiv.org/abs/2405.17974v1","category":"cs.CL"}
{"created":"2024-05-28 08:56:33","title":"Knowledge Circuits in Pretrained Transformers","abstract":"The remarkable capabilities of modern large language models are rooted in their vast repositories of knowledge encoded within their parameters, enabling them to perceive the world and engage in reasoning. The inner workings of how these models store knowledge have long been a subject of intense interest and investigation among researchers. To date, most studies have concentrated on isolated components within these models, such as the Multilayer Perceptrons and attention head. In this paper, we delve into the computation graph of the language model to uncover the knowledge circuits that are instrumental in articulating specific knowledge. The experiments, conducted with GPT2 and TinyLLAMA, has allowed us to observe how certain information heads, relation heads, and Multilayer Perceptrons collaboratively encode knowledge within the model. Moreover, we evaluate the impact of current knowledge editing techniques on these knowledge circuits, providing deeper insights into the functioning and constraints of these editing methodologies. Finally, we utilize knowledge circuits to analyze and interpret language model behaviors such as hallucinations and in-context learning. We believe the knowledge circuit holds potential for advancing our understanding of Transformers and guiding the improved design of knowledge editing. Code and data are available in https://github.com/zjunlp/KnowledgeCircuits.","sentences":["The remarkable capabilities of modern large language models are rooted in their vast repositories of knowledge encoded within their parameters, enabling them to perceive the world and engage in reasoning.","The inner workings of how these models store knowledge have long been a subject of intense interest and investigation among researchers.","To date, most studies have concentrated on isolated components within these models, such as the Multilayer Perceptrons and attention head.","In this paper, we delve into the computation graph of the language model to uncover the knowledge circuits that are instrumental in articulating specific knowledge.","The experiments, conducted with GPT2 and TinyLLAMA, has allowed us to observe how certain information heads, relation heads, and Multilayer Perceptrons collaboratively encode knowledge within the model.","Moreover, we evaluate the impact of current knowledge editing techniques on these knowledge circuits, providing deeper insights into the functioning and constraints of these editing methodologies.","Finally, we utilize knowledge circuits to analyze and interpret language model behaviors such as hallucinations and in-context learning.","We believe the knowledge circuit holds potential for advancing our understanding of Transformers and guiding the improved design of knowledge editing.","Code and data are available in https://github.com/zjunlp/KnowledgeCircuits."],"url":"http://arxiv.org/abs/2405.17969v1","category":"cs.CL"}
{"created":"2024-05-28 08:52:55","title":"An even-parity precession protocol for detecting nonclassicality and entanglement","abstract":"We introduce an even-parity precession protocol that can detect nonclassicality of some quantum states using only measurements of a uniformly-precessing variable at different points in time. Depending on the system under study, the protocol may detect the Wigner negativity of a single quantum harmonic oscillator or of a single spin $j\\geq 2$; the non-Gaussian entanglement of two harmonic oscillators; or genuine multipartite entanglement of a spin ensemble, whose total spin is integer. Unlike other nonclassicality tests, simultaneous or sequential measurements are not required. Our protocol can also detect states that commute with the parity operator, which were missed by similar protocols built from Tsirelson's original precession protocol. This work also closes a long-standing gap by showing the possibility of detecting the Greenberger--Horne--Zeilinger entanglement of an even number of qubits using only collective spin measurements.","sentences":["We introduce an even-parity precession protocol that can detect nonclassicality of some quantum states using only measurements of a uniformly-precessing variable at different points in time.","Depending on the system under study, the protocol may detect the Wigner negativity of a single quantum harmonic oscillator or of a single spin $j\\geq 2$; the non-Gaussian entanglement of two harmonic oscillators; or genuine multipartite entanglement of a spin ensemble, whose total spin is integer.","Unlike other nonclassicality tests, simultaneous or sequential measurements are not required.","Our protocol can also detect states that commute with the parity operator, which were missed by similar protocols built from Tsirelson's original precession protocol.","This work also closes a long-standing gap by showing the possibility of detecting the Greenberger--Horne--Zeilinger entanglement of an even number of qubits using only collective spin measurements."],"url":"http://arxiv.org/abs/2405.17966v1","category":"quant-ph"}
{"created":"2024-05-28 08:41:05","title":"Attention-based sequential recommendation system using multimodal data","abstract":"Sequential recommendation systems that model dynamic preferences based on a use's past behavior are crucial to e-commerce. Recent studies on these systems have considered various types of information such as images and texts. However, multimodal data have not yet been utilized directly to recommend products to users. In this study, we propose an attention-based sequential recommendation method that employs multimodal data of items such as images, texts, and categories. First, we extract image and text features from pre-trained VGG and BERT and convert categories into multi-labeled forms. Subsequently, attention operations are performed independent of the item sequence and multimodal representations. Finally, the individual attention information is integrated through an attention fusion function. In addition, we apply multitask learning loss for each modality to improve the generalization performance. The experimental results obtained from the Amazon datasets show that the proposed method outperforms those of conventional sequential recommendation systems.","sentences":["Sequential recommendation systems that model dynamic preferences based on a use's past behavior are crucial to e-commerce.","Recent studies on these systems have considered various types of information such as images and texts.","However, multimodal data have not yet been utilized directly to recommend products to users.","In this study, we propose an attention-based sequential recommendation method that employs multimodal data of items such as images, texts, and categories.","First, we extract image and text features from pre-trained VGG and BERT and convert categories into multi-labeled forms.","Subsequently, attention operations are performed independent of the item sequence and multimodal representations.","Finally, the individual attention information is integrated through an attention fusion function.","In addition, we apply multitask learning loss for each modality to improve the generalization performance.","The experimental results obtained from the Amazon datasets show that the proposed method outperforms those of conventional sequential recommendation systems."],"url":"http://arxiv.org/abs/2405.17959v1","category":"cs.IR"}
{"created":"2024-05-28 08:39:49","title":"Modeling Dynamic Topics in Chain-Free Fashion by Evolution-Tracking Contrastive Learning and Unassociated Word Exclusion","abstract":"Dynamic topic models track the evolution of topics in sequential documents, which have derived various applications like trend analysis and opinion mining. However, existing models suffer from repetitive topic and unassociated topic issues, failing to reveal the evolution and hindering further applications. To address these issues, we break the tradition of simply chaining topics in existing work and propose a novel neural \\modelfullname. We introduce a new evolution-tracking contrastive learning method that builds the similarity relations among dynamic topics. This not only tracks topic evolution but also maintains topic diversity, mitigating the repetitive topic issue. To avoid unassociated topics, we further present an unassociated word exclusion method that consistently excludes unassociated words from discovered topics. Extensive experiments demonstrate our model significantly outperforms state-of-the-art baselines, tracking topic evolution with high-quality topics, showing better performance on downstream tasks, and remaining robust to the hyperparameter for evolution intensities. Our code is available at https://github.com/bobxwu/CFDTM .","sentences":["Dynamic topic models track the evolution of topics in sequential documents, which have derived various applications like trend analysis and opinion mining.","However, existing models suffer from repetitive topic and unassociated topic issues, failing to reveal the evolution and hindering further applications.","To address these issues, we break the tradition of simply chaining topics in existing work and propose a novel neural \\modelfullname.","We introduce a new evolution-tracking contrastive learning method that builds the similarity relations among dynamic topics.","This not only tracks topic evolution but also maintains topic diversity, mitigating the repetitive topic issue.","To avoid unassociated topics, we further present an unassociated word exclusion method that consistently excludes unassociated words from discovered topics.","Extensive experiments demonstrate our model significantly outperforms state-of-the-art baselines, tracking topic evolution with high-quality topics, showing better performance on downstream tasks, and remaining robust to the hyperparameter for evolution intensities.","Our code is available at https://github.com/bobxwu/CFDTM ."],"url":"http://arxiv.org/abs/2405.17957v1","category":"cs.CL"}
{"created":"2024-05-28 08:35:48","title":"Hybrid Preference Optimization: Augmenting Direct Preference Optimization with Auxiliary Objectives","abstract":"For aligning large language models (LLMs), prior work has leveraged reinforcement learning via human feedback (RLHF) or variations of direct preference optimization (DPO). While DPO offers a simpler framework based on maximum likelihood estimation, it compromises on the ability to tune language models to easily maximize non-differentiable and non-binary objectives according to the LLM designer's preferences (e.g., using simpler language or minimizing specific kinds of harmful content). These may neither align with user preferences nor even be able to be captured tractably by binary preference data. To leverage the simplicity and performance of DPO with the generalizability of RL, we propose a hybrid approach between DPO and RLHF. With a simple augmentation to the implicit reward decomposition of DPO, we allow for tuning LLMs to maximize a set of arbitrary auxiliary rewards using offline RL. The proposed method, Hybrid Preference Optimization (HPO), shows the ability to effectively generalize to both user preferences and auxiliary designer objectives, while preserving alignment performance across a range of challenging benchmarks and model sizes.","sentences":["For aligning large language models (LLMs), prior work has leveraged reinforcement learning via human feedback (RLHF) or variations of direct preference optimization (DPO).","While DPO offers a simpler framework based on maximum likelihood estimation, it compromises on the ability to tune language models to easily maximize non-differentiable and non-binary objectives according to the LLM designer's preferences (e.g., using simpler language or minimizing specific kinds of harmful content).","These may neither align with user preferences nor even be able to be captured tractably by binary preference data.","To leverage the simplicity and performance of DPO with the generalizability of RL, we propose a hybrid approach between DPO and RLHF.","With a simple augmentation to the implicit reward decomposition of DPO, we allow for tuning LLMs to maximize a set of arbitrary auxiliary rewards using offline RL.","The proposed method, Hybrid Preference Optimization (HPO), shows the ability to effectively generalize to both user preferences and auxiliary designer objectives, while preserving alignment performance across a range of challenging benchmarks and model sizes."],"url":"http://arxiv.org/abs/2405.17956v1","category":"cs.AI"}
{"created":"2024-05-28 08:26:54","title":"Self-Guiding Exploration for Combinatorial Problems","abstract":"Large Language Models (LLMs) have become pivotal in addressing reasoning tasks across diverse domains, including arithmetic, commonsense, and symbolic reasoning. They utilize prompting techniques such as Exploration-of-Thought, Decomposition, and Refinement to effectively navigate and solve intricate tasks. Despite these advancements, the application of LLMs to Combinatorial Problems (CPs), known for their NP-hardness and critical roles in logistics and resource management remains underexplored. To address this gap, we introduce a novel prompting strategy: Self-Guiding Exploration (SGE), designed to enhance the performance of solving CPs. SGE operates autonomously, generating multiple thought trajectories for each CP task. It then breaks these trajectories down into actionable subtasks, executes them sequentially, and refines the results to ensure optimal outcomes. We present our research as the first to apply LLMs to a broad range of CPs and demonstrate that SGE outperforms existing prompting strategies by over 27.84% in CP optimization performance. Additionally, SGE achieves a 2.46% higher accuracy over the best existing results in other reasoning tasks (arithmetic, commonsense, and symbolic).","sentences":["Large Language Models (LLMs) have become pivotal in addressing reasoning tasks across diverse domains, including arithmetic, commonsense, and symbolic reasoning.","They utilize prompting techniques such as Exploration-of-Thought, Decomposition, and Refinement to effectively navigate and solve intricate tasks.","Despite these advancements, the application of LLMs to Combinatorial Problems (CPs), known for their NP-hardness and critical roles in logistics and resource management remains underexplored.","To address this gap, we introduce a novel prompting strategy: Self-Guiding Exploration (SGE), designed to enhance the performance of solving CPs.","SGE operates autonomously, generating multiple thought trajectories for each CP task.","It then breaks these trajectories down into actionable subtasks, executes them sequentially, and refines the results to ensure optimal outcomes.","We present our research as the first to apply LLMs to a broad range of CPs and demonstrate that SGE outperforms existing prompting strategies by over 27.84% in CP optimization performance.","Additionally, SGE achieves a 2.46% higher accuracy over the best existing results in other reasoning tasks (arithmetic, commonsense, and symbolic)."],"url":"http://arxiv.org/abs/2405.17950v1","category":"cs.AI"}
{"created":"2024-05-28 08:13:49","title":"Self-supervised Pre-training for Transferable Multi-modal Perception","abstract":"In autonomous driving, multi-modal perception models leveraging inputs from multiple sensors exhibit strong robustness in degraded environments. However, these models face challenges in efficiently and effectively transferring learned representations across different modalities and tasks. This paper presents NeRF-Supervised Masked Auto Encoder (NS-MAE), a self-supervised pre-training paradigm for transferable multi-modal representation learning. NS-MAE is designed to provide pre-trained model initializations for efficient and high-performance fine-tuning. Our approach uses masked multi-modal reconstruction in neural radiance fields (NeRF), training the model to reconstruct missing or corrupted input data across multiple modalities. Specifically, multi-modal embeddings are extracted from corrupted LiDAR point clouds and images, conditioned on specific view directions and locations. These embeddings are then rendered into projected multi-modal feature maps using neural rendering techniques. The original multi-modal signals serve as reconstruction targets for the rendered feature maps, facilitating self-supervised representation learning. Extensive experiments demonstrate the promising transferability of NS-MAE representations across diverse multi-modal and single-modal perception models. This transferability is evaluated on various 3D perception downstream tasks, such as 3D object detection and BEV map segmentation, using different amounts of fine-tuning labeled data. Our code will be released to support the community.","sentences":["In autonomous driving, multi-modal perception models leveraging inputs from multiple sensors exhibit strong robustness in degraded environments.","However, these models face challenges in efficiently and effectively transferring learned representations across different modalities and tasks.","This paper presents NeRF-Supervised Masked Auto Encoder (NS-MAE), a self-supervised pre-training paradigm for transferable multi-modal representation learning.","NS-MAE is designed to provide pre-trained model initializations for efficient and high-performance fine-tuning.","Our approach uses masked multi-modal reconstruction in neural radiance fields (NeRF), training the model to reconstruct missing or corrupted input data across multiple modalities.","Specifically, multi-modal embeddings are extracted from corrupted LiDAR point clouds and images, conditioned on specific view directions and locations.","These embeddings are then rendered into projected multi-modal feature maps using neural rendering techniques.","The original multi-modal signals serve as reconstruction targets for the rendered feature maps, facilitating self-supervised representation learning.","Extensive experiments demonstrate the promising transferability of NS-MAE representations across diverse multi-modal and single-modal perception models.","This transferability is evaluated on various 3D perception downstream tasks, such as 3D object detection and BEV map segmentation, using different amounts of fine-tuning labeled data.","Our code will be released to support the community."],"url":"http://arxiv.org/abs/2405.17942v1","category":"cs.CV"}
{"created":"2024-05-28 08:11:12","title":"World Models for General Surgical Grasping","abstract":"Intelligent vision control systems for surgical robots should adapt to unknown and diverse objects while being robust to system disturbances. Previous methods did not meet these requirements due to mainly relying on pose estimation and feature tracking. We propose a world-model-based deep reinforcement learning framework \"Grasp Anything for Surgery\" (GAS), that learns a pixel-level visuomotor policy for surgical grasping, enhancing both generality and robustness. In particular, a novel method is proposed to estimate the values and uncertainties of depth pixels for a rigid-link object's inaccurate region based on the empirical prior of the object's size; both depth and mask images of task objects are encoded to a single compact 3-channel image (size: 64x64x3) by dynamically zooming in the mask regions, minimizing the information loss. The learned controller's effectiveness is extensively evaluated in simulation and in a real robot. Our learned visuomotor policy handles: i) unseen objects, including 5 types of target grasping objects and a robot gripper, in unstructured real-world surgery environments, and ii) disturbances in perception and control. Note that we are the first work to achieve a unified surgical control system that grasps diverse surgical objects using different robot grippers on real robots in complex surgery scenes (average success rate: 69%). Our system also demonstrates significant robustness across 6 conditions including background variation, target disturbance, camera pose variation, kinematic control error, image noise, and re-grasping after the gripped target object drops from the gripper. Videos and codes can be found on our project page: https://linhongbin.github.io/gas/.","sentences":["Intelligent vision control systems for surgical robots should adapt to unknown and diverse objects while being robust to system disturbances.","Previous methods did not meet these requirements due to mainly relying on pose estimation and feature tracking.","We propose a world-model-based deep reinforcement learning framework \"Grasp Anything for Surgery\" (GAS), that learns a pixel-level visuomotor policy for surgical grasping, enhancing both generality and robustness.","In particular, a novel method is proposed to estimate the values and uncertainties of depth pixels for a rigid-link object's inaccurate region based on the empirical prior of the object's size; both depth and mask images of task objects are encoded to a single compact 3-channel image (size: 64x64x3) by dynamically zooming in the mask regions, minimizing the information loss.","The learned controller's effectiveness is extensively evaluated in simulation and in a real robot.","Our learned visuomotor policy handles: i) unseen objects, including 5 types of target grasping objects and a robot gripper, in unstructured real-world surgery environments, and ii) disturbances in perception and control.","Note that we are the first work to achieve a unified surgical control system that grasps diverse surgical objects using different robot grippers on real robots in complex surgery scenes (average success rate: 69%).","Our system also demonstrates significant robustness across 6 conditions including background variation, target disturbance, camera pose variation, kinematic control error, image noise, and re-grasping after the gripped target object drops from the gripper.","Videos and codes can be found on our project page: https://linhongbin.github.io/gas/."],"url":"http://arxiv.org/abs/2405.17940v1","category":"cs.RO"}
{"created":"2024-05-28 08:01:26","title":"Tool Learning with Large Language Models: A Survey","abstract":"Recently, tool learning with large language models (LLMs) has emerged as a promising paradigm for augmenting the capabilities of LLMs to tackle highly complex problems. Despite growing attention and rapid advancements in this field, the existing literature remains fragmented and lacks systematic organization, posing barriers to entry for newcomers. This gap motivates us to conduct a comprehensive survey of existing works on tool learning with LLMs. In this survey, we focus on reviewing existing literature from the two primary aspects (1) why tool learning is beneficial and (2) how tool learning is implemented, enabling a comprehensive understanding of tool learning with LLMs. We first explore the \"why\" by reviewing both the benefits of tool integration and the inherent benefits of the tool learning paradigm from six specific aspects. In terms of \"how\", we systematically review the literature according to a taxonomy of four key stages in the tool learning workflow: task planning, tool selection, tool calling, and response generation. Additionally, we provide a detailed summary of existing benchmarks and evaluation methods, categorizing them according to their relevance to different stages. Finally, we discuss current challenges and outline potential future directions, aiming to inspire both researchers and industrial developers to further explore this emerging and promising area.","sentences":["Recently, tool learning with large language models (LLMs) has emerged as a promising paradigm for augmenting the capabilities of LLMs to tackle highly complex problems.","Despite growing attention and rapid advancements in this field, the existing literature remains fragmented and lacks systematic organization, posing barriers to entry for newcomers.","This gap motivates us to conduct a comprehensive survey of existing works on tool learning with LLMs.","In this survey, we focus on reviewing existing literature from the two primary aspects (1) why tool learning is beneficial and (2) how tool learning is implemented, enabling a comprehensive understanding of tool learning with LLMs.","We first explore the \"why\" by reviewing both the benefits of tool integration and the inherent benefits of the tool learning paradigm from six specific aspects.","In terms of \"how\", we systematically review the literature according to a taxonomy of four key stages in the tool learning workflow: task planning, tool selection, tool calling, and response generation.","Additionally, we provide a detailed summary of existing benchmarks and evaluation methods, categorizing them according to their relevance to different stages.","Finally, we discuss current challenges and outline potential future directions, aiming to inspire both researchers and industrial developers to further explore this emerging and promising area."],"url":"http://arxiv.org/abs/2405.17935v1","category":"cs.CL"}
{"created":"2024-05-28 08:00:54","title":"Proof of Quality: A Costless Paradigm for Trustless Generative AI Model Inference on Blockchains","abstract":"Generative AI models, such as GPT-4 and Stable Diffusion, have demonstrated powerful and disruptive capabilities in natural language and image tasks. However, deploying these models in decentralized environments remains challenging. Unlike traditional centralized deployment, systematically guaranteeing the integrity of AI model services in fully decentralized environments, particularly on trustless blockchains, is both crucial and difficult. In this paper, we present a new inference paradigm called \\emph{proof of quality} (PoQ) to enable the deployment of arbitrarily large generative models on blockchain architecture. Unlike traditional approaches based on validating inference procedures, such as ZKML or OPML, our PoQ paradigm focuses on the outcome quality of model inference. Using lightweight BERT-based cross-encoders as our underlying quality evaluation model, we design and implement PQML, the first practical protocol for real-world NLP generative model inference on blockchains, tailored for popular open-source models such as Llama 3 and Mixtral. Our analysis demonstrates that our protocol is robust against adversarial but rational participants in ecosystems, where lazy or dishonest behavior results in fewer benefits compared to well-behaving participants. The computational overhead of validating the quality evaluation is minimal, allowing quality validators to complete the quality check within a second, even using only a CPU. Preliminary simulation results show that PoQ consensus is generated in milliseconds, 1,000 times faster than any existing scheme.","sentences":["Generative AI models, such as GPT-4 and Stable Diffusion, have demonstrated powerful and disruptive capabilities in natural language and image tasks.","However, deploying these models in decentralized environments remains challenging.","Unlike traditional centralized deployment, systematically guaranteeing the integrity of AI model services in fully decentralized environments, particularly on trustless blockchains, is both crucial and difficult.","In this paper, we present a new inference paradigm called \\emph{proof of quality} (PoQ) to enable the deployment of arbitrarily large generative models on blockchain architecture.","Unlike traditional approaches based on validating inference procedures, such as ZKML or OPML, our PoQ paradigm focuses on the outcome quality of model inference.","Using lightweight BERT-based cross-encoders as our underlying quality evaluation model, we design and implement PQML, the first practical protocol for real-world NLP generative model inference on blockchains, tailored for popular open-source models such as Llama 3 and Mixtral.","Our analysis demonstrates that our protocol is robust against adversarial but rational participants in ecosystems, where lazy or dishonest behavior results in fewer benefits compared to well-behaving participants.","The computational overhead of validating the quality evaluation is minimal, allowing quality validators to complete the quality check within a second, even using only a CPU.","Preliminary simulation results show that PoQ consensus is generated in milliseconds, 1,000 times faster than any existing scheme."],"url":"http://arxiv.org/abs/2405.17934v1","category":"cs.AI"}
{"created":"2024-05-28 07:49:52","title":"Relational Self-supervised Distillation with Compact Descriptors for Image Copy Detection","abstract":"This paper addresses image copy detection, a task in online sharing platforms for copyright protection. While previous approaches have performed exceptionally well, the large size of their networks and descriptors remains a significant disadvantage, complicating their practical application. In this paper, we propose a novel method that achieves a competitive performance by using a lightweight network and compact descriptors. By utilizing relational self-supervised distillation to transfer knowledge from a large network to a small network, we enable the training of lightweight networks with a small descriptor size. Our approach, which we call Relational selfsupervised Distillation with Compact Descriptors (RDCD), introduces relational self-supervised distillation (RSD) for flexible representation in a smaller feature space and applies contrastive learning with a hard negative (HN) loss to prevent dimensional collapse. We demonstrate the effectiveness of our method using the DISC2021, Copydays, and NDEC benchmark datasets, with which our lightweight network with compact descriptors achieves a competitive performance. For the DISC2021 benchmark, ResNet-50/EfficientNet- B0 are used as a teacher and student respectively, the micro average precision improved by 5.0%/4.9%/5.9% for 64/128/256 descriptor sizes compared to the baseline method.","sentences":["This paper addresses image copy detection, a task in online sharing platforms for copyright protection.","While previous approaches have performed exceptionally well, the large size of their networks and descriptors remains a significant disadvantage, complicating their practical application.","In this paper, we propose a novel method that achieves a competitive performance by using a lightweight network and compact descriptors.","By utilizing relational self-supervised distillation to transfer knowledge from a large network to a small network, we enable the training of lightweight networks with a small descriptor size.","Our approach, which we call Relational selfsupervised Distillation with Compact Descriptors (RDCD), introduces relational self-supervised distillation (RSD) for flexible representation in a smaller feature space and applies contrastive learning with a hard negative (HN) loss to prevent dimensional collapse.","We demonstrate the effectiveness of our method using the DISC2021, Copydays, and NDEC benchmark datasets, with which our lightweight network with compact descriptors achieves a competitive performance.","For the DISC2021 benchmark, ResNet-50/EfficientNet- B0 are used as a teacher and student respectively, the micro average precision improved by 5.0%/4.9%/5.9% for 64/128/256 descriptor sizes compared to the baseline method."],"url":"http://arxiv.org/abs/2405.17928v1","category":"cs.CV"}
{"created":"2024-05-28 07:48:15","title":"The Evolution of Multimodal Model Architectures","abstract":"This work uniquely identifies and characterizes four prevalent multimodal model architectural patterns in the contemporary multimodal landscape. Systematically categorizing models by architecture type facilitates monitoring of developments in the multimodal domain. Distinct from recent survey papers that present general information on multimodal architectures, this research conducts a comprehensive exploration of architectural details and identifies four specific architectural types. The types are distinguished by their respective methodologies for integrating multimodal inputs into the deep neural network model. The first two types (Type A and B) deeply fuses multimodal inputs within the internal layers of the model, whereas the following two types (Type C and D) facilitate early fusion at the input stage. Type-A employs standard cross-attention, whereas Type-B utilizes custom-designed layers for modality fusion within the internal layers. On the other hand, Type-C utilizes modality-specific encoders, while Type-D leverages tokenizers to process the modalities at the model's input stage. The identified architecture types aid the monitoring of any-to-any multimodal model development. Notably, Type-C and Type-D are currently favored in the construction of any-to-any multimodal models. Type-C, distinguished by its non-tokenizing multimodal model architecture, is emerging as a viable alternative to Type-D, which utilizes input-tokenizing techniques. To assist in model selection, this work highlights the advantages and disadvantages of each architecture type based on data and compute requirements, architecture complexity, scalability, simplification of adding modalities, training objectives, and any-to-any multimodal generation capability.","sentences":["This work uniquely identifies and characterizes four prevalent multimodal model architectural patterns in the contemporary multimodal landscape.","Systematically categorizing models by architecture type facilitates monitoring of developments in the multimodal domain.","Distinct from recent survey papers that present general information on multimodal architectures, this research conducts a comprehensive exploration of architectural details and identifies four specific architectural types.","The types are distinguished by their respective methodologies for integrating multimodal inputs into the deep neural network model.","The first two types (Type A and B) deeply fuses multimodal inputs within the internal layers of the model, whereas the following two types (Type C and D) facilitate early fusion at the input stage.","Type-A employs standard cross-attention, whereas Type-B utilizes custom-designed layers for modality fusion within the internal layers.","On the other hand, Type-C utilizes modality-specific encoders, while Type-D leverages tokenizers to process the modalities at the model's input stage.","The identified architecture types aid the monitoring of any-to-any multimodal model development.","Notably, Type-C and Type-D are currently favored in the construction of any-to-any multimodal models.","Type-C, distinguished by its non-tokenizing multimodal model architecture, is emerging as a viable alternative to Type-D, which utilizes input-tokenizing techniques.","To assist in model selection, this work highlights the advantages and disadvantages of each architecture type based on data and compute requirements, architecture complexity, scalability, simplification of adding modalities, training objectives, and any-to-any multimodal generation capability."],"url":"http://arxiv.org/abs/2405.17927v1","category":"cs.AI"}
{"created":"2024-05-28 07:47:03","title":"Generative AI Enhances Team Performance and Reduces Need for Traditional Teams","abstract":"Recent advancements in generative artificial intelligence (AI) have transformed collaborative work processes, yet the impact on team performance remains underexplored. Here we examine the role of generative AI in enhancing or replacing traditional team dynamics using a randomized controlled experiment with 435 participants across 122 teams. We show that teams augmented with generative AI significantly outperformed those relying solely on human collaboration across various performance measures. Interestingly, teams with multiple AIs did not exhibit further gains, indicating diminishing returns with increased AI integration. Our analysis suggests that centralized AI usage by a few team members is more effective than distributed engagement. Additionally, individual-AI pairs matched the performance of conventional teams, suggesting a reduced need for traditional team structures in some contexts. However, despite this capability, individual-AI pairs still fell short of the performance levels achieved by AI-assisted teams. These findings underscore that while generative AI can replace some traditional team functions, more comprehensively integrating AI within team structures provides superior benefits, enhancing overall effectiveness beyond individual efforts.","sentences":["Recent advancements in generative artificial intelligence (AI) have transformed collaborative work processes, yet the impact on team performance remains underexplored.","Here we examine the role of generative AI in enhancing or replacing traditional team dynamics using a randomized controlled experiment with 435 participants across 122 teams.","We show that teams augmented with generative AI significantly outperformed those relying solely on human collaboration across various performance measures.","Interestingly, teams with multiple AIs did not exhibit further gains, indicating diminishing returns with increased AI integration.","Our analysis suggests that centralized AI usage by a few team members is more effective than distributed engagement.","Additionally, individual-AI pairs matched the performance of conventional teams, suggesting a reduced need for traditional team structures in some contexts.","However, despite this capability, individual-AI pairs still fell short of the performance levels achieved by AI-assisted teams.","These findings underscore that while generative AI can replace some traditional team functions, more comprehensively integrating AI within team structures provides superior benefits, enhancing overall effectiveness beyond individual efforts."],"url":"http://arxiv.org/abs/2405.17924v1","category":"cs.HC"}
{"created":"2024-05-28 07:42:55","title":"Towards Clinical AI Fairness: Filling Gaps in the Puzzle","abstract":"The ethical integration of Artificial Intelligence (AI) in healthcare necessitates addressing fairness-a concept that is highly context-specific across medical fields. Extensive studies have been conducted to expand the technical components of AI fairness, while tremendous calls for AI fairness have been raised from healthcare. Despite this, a significant disconnect persists between technical advancements and their practical clinical applications, resulting in a lack of contextualized discussion of AI fairness in clinical settings. Through a detailed evidence gap analysis, our review systematically pinpoints several deficiencies concerning both healthcare data and the provided AI fairness solutions. We highlight the scarcity of research on AI fairness in many medical domains where AI technology is increasingly utilized. Additionally, our analysis highlights a substantial reliance on group fairness, aiming to ensure equality among demographic groups from a macro healthcare system perspective; in contrast, individual fairness, focusing on equity at a more granular level, is frequently overlooked. To bridge these gaps, our review advances actionable strategies for both the healthcare and AI research communities. Beyond applying existing AI fairness methods in healthcare, we further emphasize the importance of involving healthcare professionals to refine AI fairness concepts and methods to ensure contextually relevant and ethically sound AI applications in healthcare.","sentences":["The ethical integration of Artificial Intelligence (AI) in healthcare necessitates addressing fairness-a concept that is highly context-specific across medical fields.","Extensive studies have been conducted to expand the technical components of AI fairness, while tremendous calls for AI fairness have been raised from healthcare.","Despite this, a significant disconnect persists between technical advancements and their practical clinical applications, resulting in a lack of contextualized discussion of AI fairness in clinical settings.","Through a detailed evidence gap analysis, our review systematically pinpoints several deficiencies concerning both healthcare data and the provided AI fairness solutions.","We highlight the scarcity of research on AI fairness in many medical domains where AI technology is increasingly utilized.","Additionally, our analysis highlights a substantial reliance on group fairness, aiming to ensure equality among demographic groups from a macro healthcare system perspective; in contrast, individual fairness, focusing on equity at a more granular level, is frequently overlooked.","To bridge these gaps, our review advances actionable strategies for both the healthcare and AI research communities.","Beyond applying existing AI fairness methods in healthcare, we further emphasize the importance of involving healthcare professionals to refine AI fairness concepts and methods to ensure contextually relevant and ethically sound AI applications in healthcare."],"url":"http://arxiv.org/abs/2405.17921v1","category":"cs.AI"}
{"created":"2024-05-28 07:38:39","title":"Cost-Sensitive Multi-Fidelity Bayesian Optimization with Transfer of Learning Curve Extrapolation","abstract":"In this paper, we address the problem of cost-sensitive multi-fidelity Bayesian Optimization (BO) for efficient hyperparameter optimization (HPO). Specifically, we assume a scenario where users want to early-stop the BO when the performance improvement is not satisfactory with respect to the required computational cost. Motivated by this scenario, we introduce utility, which is a function predefined by each user and describes the trade-off between cost and performance of BO. This utility function, combined with our novel acquisition function and stopping criterion, allows us to dynamically choose for each BO step the best configuration that we expect to maximally improve the utility in future, and also automatically stop the BO around the maximum utility. Further, we improve the sample efficiency of existing learning curve (LC) extrapolation methods with transfer learning, while successfully capturing the correlations between different configurations to develop a sensible surrogate function for multi-fidelity BO. We validate our algorithm on various LC datasets and found it outperform all the previous multi-fidelity BO and transfer-BO baselines we consider, achieving significantly better trade-off between cost and performance of BO.","sentences":["In this paper, we address the problem of cost-sensitive multi-fidelity Bayesian Optimization (BO) for efficient hyperparameter optimization (HPO).","Specifically, we assume a scenario where users want to early-stop the BO when the performance improvement is not satisfactory with respect to the required computational cost.","Motivated by this scenario, we introduce utility, which is a function predefined by each user and describes the trade-off between cost and performance of BO.","This utility function, combined with our novel acquisition function and stopping criterion, allows us to dynamically choose for each BO step the best configuration that we expect to maximally improve the utility in future, and also automatically stop the BO around the maximum utility.","Further, we improve the sample efficiency of existing learning curve (LC) extrapolation methods with transfer learning, while successfully capturing the correlations between different configurations to develop a sensible surrogate function for multi-fidelity BO.","We validate our algorithm on various LC datasets and found it outperform all the previous multi-fidelity BO and transfer-BO baselines we consider, achieving significantly better trade-off between cost and performance of BO."],"url":"http://arxiv.org/abs/2405.17918v1","category":"cs.LG"}
{"created":"2024-05-28 07:38:01","title":"Cascaded Group Testing","abstract":"In this paper, we introduce a variation of the group testing problem where each test is specified by an ordered subset of items, and returns the first defective item in the specified order. We refer to this as \\textit{cascaded group testing} and the goal is to identify a small set of $K$ defective items amongst a collection of size $N$, using as few tests as possible. For the adaptive testing regime, we show that a simple scheme is able to find all defective items in at most $K$ tests, which is optimal. For the non-adaptive setting, we first come up with a necessary and sufficient condition for any collection of tests to be feasible for recovering all the defectives. Using this, we are able to show that any feasible non-adaptive strategy requires at least $\\Omega(K^2)$ tests. In terms of achievability, it is easy to show that a collection of $O(K^2 \\log (N/K))$ randomly constructed tests is feasible. We show via carefully constructed explicit designs that one can do significantly better. We provide two simple schemes for $K = 1, 2$ which only require one and two tests respectively irrespective of the number of items $N$. Note that this is in contrast to standard binary group testing, where at least $\\Omega(\\log N)$ tests are required. The case of $K \\ge 3$ is more challenging and here we come up with an iterative design which requires only $\\text{poly}(\\log \\log N)$ tests.","sentences":["In this paper, we introduce a variation of the group testing problem where each test is specified by an ordered subset of items, and returns the first defective item in the specified order.","We refer to this as \\textit{cascaded group testing} and the goal is to identify a small set of $K$ defective items amongst a collection of size $N$, using as few tests as possible.","For the adaptive testing regime, we show that a simple scheme is able to find all defective items in at most $K$ tests, which is optimal.","For the non-adaptive setting, we first come up with a necessary and sufficient condition for any collection of tests to be feasible for recovering all the defectives.","Using this, we are able to show that any feasible non-adaptive strategy requires at least $\\Omega(K^2)$ tests.","In terms of achievability, it is easy to show that a collection of $O(K^2 \\log (N/K))$ randomly constructed tests is feasible.","We show via carefully constructed explicit designs that one can do significantly better.","We provide two simple schemes for $K = 1, 2$ which only require one and two tests respectively irrespective of the number of items $N$. Note that this is in contrast to standard binary group testing, where at least $\\Omega(\\log N)$ tests are required.","The case of $K \\ge 3$ is more challenging and here we come up with an iterative design which requires only $\\text{poly}(\\log \\log N)$ tests."],"url":"http://arxiv.org/abs/2405.17917v1","category":"cs.IT"}
{"created":"2024-05-28 07:34:12","title":"Trustworthy DNN Partition for Blockchain-enabled Digital Twin in Wireless IIoT Networks","abstract":"Digital twin (DT) has emerged as a promising solution to enhance manufacturing efficiency in industrial Internet of Things (IIoT) networks. To promote the efficiency and trustworthiness of DT for wireless IIoT networks, we propose a blockchain-enabled DT (B-DT) framework that employs deep neural network (DNN) partitioning technique and reputation-based consensus mechanism, wherein the DTs maintained at the gateway side execute DNN inference tasks using the data collected from their associated IIoT devices. First, we employ DNN partitioning technique to offload the top-layer DNN inference tasks to the access point (AP) side, which alleviates the computation burden at the gateway side and thereby improves the efficiency of DNN inference. Second, we propose a reputation-based consensus mechanism that integrates Proof of Work (PoW) and Proof of Stake (PoS). Specifically, the proposed consensus mechanism evaluates the off-chain reputation of each AP according to its computation resource contributions to the DNN inference tasks, and utilizes the off-chain reputation as a stake to adjust the block generation difficulty. Third, we formulate a stochastic optimization problem of communication resource (i.e., partition point) and computation resource allocation (i.e., computation frequency of APs for top-layer DNN inference and block generation) to minimize system latency under the time-varying channel state and long-term constraints of off-chain reputation, and solve the problem using Lyapunov optimization method. Experimental results show that the proposed dynamic DNN partitioning and resource allocation (DPRA) algorithm outperforms the baselines in terms of reducing the overall latency while guaranteeing the trustworthiness of the B-DT system.","sentences":["Digital twin (DT) has emerged as a promising solution to enhance manufacturing efficiency in industrial Internet of Things (IIoT) networks.","To promote the efficiency and trustworthiness of DT for wireless IIoT networks, we propose a blockchain-enabled DT (B-DT) framework that employs deep neural network (DNN) partitioning technique and reputation-based consensus mechanism, wherein the DTs maintained at the gateway side execute DNN inference tasks using the data collected from their associated IIoT devices.","First, we employ DNN partitioning technique to offload the top-layer DNN inference tasks to the access point (AP) side, which alleviates the computation burden at the gateway side and thereby improves the efficiency of DNN inference.","Second, we propose a reputation-based consensus mechanism that integrates Proof of Work (PoW) and Proof of Stake (PoS).","Specifically, the proposed consensus mechanism evaluates the off-chain reputation of each AP according to its computation resource contributions to the DNN inference tasks, and utilizes the off-chain reputation as a stake to adjust the block generation difficulty.","Third, we formulate a stochastic optimization problem of communication resource (i.e., partition point) and computation resource allocation (i.e., computation frequency of APs for top-layer DNN inference and block generation) to minimize system latency under the time-varying channel state and long-term constraints of off-chain reputation, and solve the problem using Lyapunov optimization method.","Experimental results show that the proposed dynamic DNN partitioning and resource allocation (DPRA) algorithm outperforms the baselines in terms of reducing the overall latency while guaranteeing the trustworthiness of the B-DT system."],"url":"http://arxiv.org/abs/2405.17914v1","category":"cs.LG"}
{"created":"2024-05-28 07:33:27","title":"OV-DQUO: Open-Vocabulary DETR with Denoising Text Query Training and Open-World Unknown Objects Supervision","abstract":"Open-Vocabulary Detection (OVD) aims to detect objects from novel categories beyond the base categories on which the detector is trained. However, existing open-vocabulary detectors trained on known category data tend to assign higher confidence to trained categories and confuse novel categories with background. To resolve this, we propose OV-DQUO, an \\textbf{O}pen-\\textbf{V}ocabulary DETR with \\textbf{D}enoising text \\textbf{Q}uery training and open-world \\textbf{U}nknown \\textbf{O}bjects supervision. Specifically, we introduce a wildcard matching method that enables the detector to learn from pairs of unknown objects recognized by the open-world detector and text embeddings with general semantics, mitigating the confidence bias between base and novel categories. Additionally, we propose a denoising text query training strategy that synthesizes additional noisy query-box pairs from open-world unknown objects to trains the detector through contrastive learning, enhancing its ability to distinguish novel objects from the background. We conducted extensive experiments on the challenging OV-COCO and OV-LVIS benchmarks, achieving new state-of-the-art results of 45.6 AP50 and 39.3 mAP on novel categories respectively, without the need for additional training data. Models and code are released at https://github.com/xiaomoguhz/OV-DQUO","sentences":["Open-Vocabulary Detection (OVD) aims to detect objects from novel categories beyond the base categories on which the detector is trained.","However, existing open-vocabulary detectors trained on known category data tend to assign higher confidence to trained categories and confuse novel categories with background.","To resolve this, we propose OV-DQUO, an \\textbf{O}pen-\\textbf{V}ocabulary DETR with \\textbf{D}enoising text \\textbf{Q}uery training and open-world \\textbf{U}nknown \\textbf{O}bjects supervision.","Specifically, we introduce a wildcard matching method that enables the detector to learn from pairs of unknown objects recognized by the open-world detector and text embeddings with general semantics, mitigating the confidence bias between base and novel categories.","Additionally, we propose a denoising text query training strategy that synthesizes additional noisy query-box pairs from open-world unknown objects to trains the detector through contrastive learning, enhancing its ability to distinguish novel objects from the background.","We conducted extensive experiments on the challenging OV-COCO and OV-LVIS benchmarks, achieving new state-of-the-art results of 45.6 AP50 and 39.3 mAP on novel categories respectively, without the need for additional training data.","Models and code are released at https://github.com/xiaomoguhz/OV-DQUO"],"url":"http://arxiv.org/abs/2405.17913v1","category":"cs.CV"}
{"created":"2024-05-28 07:30:28","title":"Human-Cobot collaboration's impact on success, time completion, errors, workload, gestures and acceptability during an assembly task","abstract":"The 5.0 industry promotes collaborative robots (cobots). This research studies the impacts of cobot collaboration using an experimental setup. 120 participants realized a simple and a complex assembly task. 50% collaborated with another human (H/H) and 50% with a cobot (H/C). The workload and the acceptability of the cobotic collaboration were measured. Working with a cobot decreases the effect of the task complexity on the human workload and on the output quality. However, it increases the time completion and the number of gestures (while decreasing their frequency). The H/C couples have a higher chance of success but they take more time and more gestures to realize the task. The results of this research could help developers and stakeholders to understand the impacts of implementing a cobot in production chains.","sentences":["The 5.0 industry promotes collaborative robots (cobots).","This research studies the impacts of cobot collaboration using an experimental setup.","120 participants realized a simple and a complex assembly task.","50% collaborated with another human (H/H) and 50% with a cobot (H/C).","The workload and the acceptability of the cobotic collaboration were measured.","Working with a cobot decreases the effect of the task complexity on the human workload and on the output quality.","However, it increases the time completion and the number of gestures (while decreasing their frequency).","The H/C couples have a higher chance of success but they take more time and more gestures to realize the task.","The results of this research could help developers and stakeholders to understand the impacts of implementing a cobot in production chains."],"url":"http://arxiv.org/abs/2405.17910v1","category":"cs.AI"}
{"created":"2024-05-28 07:27:42","title":"Cycle-YOLO: A Efficient and Robust Framework for Pavement Damage Detection","abstract":"With the development of modern society, traffic volume continues to increase in most countries worldwide, leading to an increase in the rate of pavement damage Therefore, the real-time and highly accurate pavement damage detection and maintenance have become the current need. In this paper, an enhanced pavement damage detection method with CycleGAN and improved YOLOv5 algorithm is presented. We selected 7644 self-collected images of pavement damage samples as the initial dataset and augmented it by CycleGAN. Due to a substantial difference between the images generated by CycleGAN and real road images, we proposed a data enhancement method based on an improved Scharr filter, CycleGAN, and Laplacian pyramid. To improve the target recognition effect on a complex background and solve the problem that the spatial pyramid pooling-fast module in the YOLOv5 network cannot handle multiscale targets, we introduced the convolutional block attention module attention mechanism and proposed the atrous spatial pyramid pooling with squeeze-and-excitation structure. In addition, we optimized the loss function of YOLOv5 by replacing the CIoU with EIoU. The experimental results showed that our algorithm achieved a precision of 0.872, recall of 0.854, and mean average precision@0.5 of 0.882 in detecting three main types of pavement damage: cracks, potholes, and patching. On the GPU, its frames per second reached 68, meeting the requirements for real-time detection. Its overall performance even exceeded the current more advanced YOLOv7 and achieved good results in practical applications, providing a basis for decision-making in pavement damage detection and prevention.","sentences":["With the development of modern society, traffic volume continues to increase in most countries worldwide, leading to an increase in the rate of pavement damage Therefore, the real-time and highly accurate pavement damage detection and maintenance have become the current need.","In this paper, an enhanced pavement damage detection method with CycleGAN and improved YOLOv5 algorithm is presented.","We selected 7644 self-collected images of pavement damage samples as the initial dataset and augmented it by CycleGAN.","Due to a substantial difference between the images generated by CycleGAN and real road images, we proposed a data enhancement method based on an improved Scharr filter, CycleGAN, and Laplacian pyramid.","To improve the target recognition effect on a complex background and solve the problem that the spatial pyramid pooling-fast module in the YOLOv5 network cannot handle multiscale targets, we introduced the convolutional block attention module attention mechanism and proposed the atrous spatial pyramid pooling with squeeze-and-excitation structure.","In addition, we optimized the loss function of YOLOv5 by replacing the CIoU with EIoU.","The experimental results showed that our algorithm achieved a precision of 0.872, recall of 0.854, and mean average precision@0.5 of 0.882 in detecting three main types of pavement damage: cracks, potholes, and patching.","On the GPU, its frames per second reached 68, meeting the requirements for real-time detection.","Its overall performance even exceeded the current more advanced YOLOv7 and achieved good results in practical applications, providing a basis for decision-making in pavement damage detection and prevention."],"url":"http://arxiv.org/abs/2405.17905v1","category":"cs.CV"}
{"created":"2024-05-28 07:24:20","title":"Boosting Protein Language Models with Negative Sample Mining","abstract":"We introduce a pioneering methodology for boosting large language models in the domain of protein representation learning. Our primary contribution lies in the refinement process for correlating the over-reliance on co-evolution knowledge, in a way that networks are trained to distill invaluable insights from negative samples, constituted by protein pairs sourced from disparate categories. By capitalizing on this novel approach, our technique steers the training of transformer-based models within the attention score space. This advanced strategy not only amplifies performance but also reflects the nuanced biological behaviors exhibited by proteins, offering aligned evidence with traditional biological mechanisms such as protein-protein interaction. We experimentally observed improved performance on various tasks over datasets, on top of several well-established large protein models. This innovative paradigm opens up promising horizons for further progress in the realms of protein research and computational biology.","sentences":["We introduce a pioneering methodology for boosting large language models in the domain of protein representation learning.","Our primary contribution lies in the refinement process for correlating the over-reliance on co-evolution knowledge, in a way that networks are trained to distill invaluable insights from negative samples, constituted by protein pairs sourced from disparate categories.","By capitalizing on this novel approach, our technique steers the training of transformer-based models within the attention score space.","This advanced strategy not only amplifies performance but also reflects the nuanced biological behaviors exhibited by proteins, offering aligned evidence with traditional biological mechanisms such as protein-protein interaction.","We experimentally observed improved performance on various tasks over datasets, on top of several well-established large protein models.","This innovative paradigm opens up promising horizons for further progress in the realms of protein research and computational biology."],"url":"http://arxiv.org/abs/2405.17902v1","category":"cs.AI"}
{"created":"2024-05-28 07:24:07","title":"Near-Infrared and Low-Rank Adaptation of Vision Transformers in Remote Sensing","abstract":"Plant health can be monitored dynamically using multispectral sensors that measure Near-Infrared reflectance (NIR). Despite this potential, obtaining and annotating high-resolution NIR images poses a significant challenge for training deep neural networks. Typically, large networks pre-trained on the RGB domain are utilized to fine-tune infrared images. This practice introduces a domain shift issue because of the differing visual traits between RGB and NIR images.As an alternative to fine-tuning, a method called low-rank adaptation (LoRA) enables more efficient training by optimizing rank-decomposition matrices while keeping the original network weights frozen. However, existing parameter-efficient adaptation strategies for remote sensing images focus on RGB images and overlook domain shift issues in the NIR domain. Therefore, this study investigates the potential benefits of using vision transformer (ViT) backbones pre-trained in the RGB domain, with low-rank adaptation for downstream tasks in the NIR domain. Extensive experiments demonstrate that employing LoRA with pre-trained ViT backbones yields the best performance for downstream tasks applied to NIR images.","sentences":["Plant health can be monitored dynamically using multispectral sensors that measure Near-Infrared reflectance (NIR).","Despite this potential, obtaining and annotating high-resolution NIR images poses a significant challenge for training deep neural networks.","Typically, large networks pre-trained on the RGB domain are utilized to fine-tune infrared images.","This practice introduces a domain shift issue because of the differing visual traits between RGB and NIR images.","As an alternative to fine-tuning, a method called low-rank adaptation (LoRA) enables more efficient training by optimizing rank-decomposition matrices while keeping the original network weights frozen.","However, existing parameter-efficient adaptation strategies for remote sensing images focus on RGB images and overlook domain shift issues in the NIR domain.","Therefore, this study investigates the potential benefits of using vision transformer (ViT) backbones pre-trained in the RGB domain, with low-rank adaptation for downstream tasks in the NIR domain.","Extensive experiments demonstrate that employing LoRA with pre-trained ViT backbones yields the best performance for downstream tasks applied to NIR images."],"url":"http://arxiv.org/abs/2405.17901v1","category":"cs.CV"}
{"created":"2024-05-28 07:22:30","title":"Enhancing Emotion Recognition in Conversation through Emotional Cross-Modal Fusion and Inter-class Contrastive Learning","abstract":"The purpose of emotion recognition in conversation (ERC) is to identify the emotion category of an utterance based on contextual information. Previous ERC methods relied on simple connections for cross-modal fusion and ignored the information differences between modalities, resulting in the model being unable to focus on modality-specific emotional information. At the same time, the shared information between modalities was not processed to generate emotions. Information redundancy problem. To overcome these limitations, we propose a cross-modal fusion emotion prediction network based on vector connections. The network mainly includes two stages: the multi-modal feature fusion stage based on connection vectors and the emotion classification stage based on fused features. Furthermore, we design a supervised inter-class contrastive learning module based on emotion labels. Experimental results confirm the effectiveness of the proposed method, demonstrating excellent performance on the IEMOCAP and MELD datasets.","sentences":["The purpose of emotion recognition in conversation (ERC) is to identify the emotion category of an utterance based on contextual information.","Previous ERC methods relied on simple connections for cross-modal fusion and ignored the information differences between modalities, resulting in the model being unable to focus on modality-specific emotional information.","At the same time, the shared information between modalities was not processed to generate emotions.","Information redundancy problem.","To overcome these limitations, we propose a cross-modal fusion emotion prediction network based on vector connections.","The network mainly includes two stages: the multi-modal feature fusion stage based on connection vectors and the emotion classification stage based on fused features.","Furthermore, we design a supervised inter-class contrastive learning module based on emotion labels.","Experimental results confirm the effectiveness of the proposed method, demonstrating excellent performance on the IEMOCAP and MELD datasets."],"url":"http://arxiv.org/abs/2405.17900v1","category":"cs.CL"}
{"created":"2024-05-28 07:18:52","title":"FlashST: A Simple and Universal Prompt-Tuning Framework for Traffic Prediction","abstract":"The objective of traffic prediction is to accurately forecast and analyze the dynamics of transportation patterns, considering both space and time. However, the presence of distribution shift poses a significant challenge in this field, as existing models struggle to generalize well when faced with test data that significantly differs from the training distribution. To tackle this issue, this paper introduces a simple and universal spatio-temporal prompt-tuning framework-FlashST, which adapts pre-trained models to the specific characteristics of diverse downstream datasets, improving generalization in diverse traffic prediction scenarios. Specifically, the FlashST framework employs a lightweight spatio-temporal prompt network for in-context learning, capturing spatio-temporal invariant knowledge and facilitating effective adaptation to diverse scenarios. Additionally, we incorporate a distribution mapping mechanism to align the data distributions of pre-training and downstream data, facilitating effective knowledge transfer in spatio-temporal forecasting. Empirical evaluations demonstrate the effectiveness of our FlashST across different spatio-temporal prediction tasks using diverse urban datasets. Code is available at https://github.com/HKUDS/FlashST.","sentences":["The objective of traffic prediction is to accurately forecast and analyze the dynamics of transportation patterns, considering both space and time.","However, the presence of distribution shift poses a significant challenge in this field, as existing models struggle to generalize well when faced with test data that significantly differs from the training distribution.","To tackle this issue, this paper introduces a simple and universal spatio-temporal prompt-tuning framework-FlashST, which adapts pre-trained models to the specific characteristics of diverse downstream datasets, improving generalization in diverse traffic prediction scenarios.","Specifically, the FlashST framework employs a lightweight spatio-temporal prompt network for in-context learning, capturing spatio-temporal invariant knowledge and facilitating effective adaptation to diverse scenarios.","Additionally, we incorporate a distribution mapping mechanism to align the data distributions of pre-training and downstream data, facilitating effective knowledge transfer in spatio-temporal forecasting.","Empirical evaluations demonstrate the effectiveness of our FlashST across different spatio-temporal prediction tasks using diverse urban datasets.","Code is available at https://github.com/HKUDS/FlashST."],"url":"http://arxiv.org/abs/2405.17898v1","category":"cs.LG"}
{"created":"2024-05-28 07:13:30","title":"White-box Multimodal Jailbreaks Against Large Vision-Language Models","abstract":"Recent advancements in Large Vision-Language Models (VLMs) have underscored their superiority in various multimodal tasks. However, the adversarial robustness of VLMs has not been fully explored. Existing methods mainly assess robustness through unimodal adversarial attacks that perturb images, while assuming inherent resilience against text-based attacks. Different from existing attacks, in this work we propose a more comprehensive strategy that jointly attacks both text and image modalities to exploit a broader spectrum of vulnerability within VLMs. Specifically, we propose a dual optimization objective aimed at guiding the model to generate affirmative responses with high toxicity. Our attack method begins by optimizing an adversarial image prefix from random noise to generate diverse harmful responses in the absence of text input, thus imbuing the image with toxic semantics. Subsequently, an adversarial text suffix is integrated and co-optimized with the adversarial image prefix to maximize the probability of eliciting affirmative responses to various harmful instructions. The discovered adversarial image prefix and text suffix are collectively denoted as a Universal Master Key (UMK). When integrated into various malicious queries, UMK can circumvent the alignment defenses of VLMs and lead to the generation of objectionable content, known as jailbreaks. The experimental results demonstrate that our universal attack strategy can effectively jailbreak MiniGPT-4 with a 96% success rate, highlighting the vulnerability of VLMs and the urgent need for new alignment strategies.","sentences":["Recent advancements in Large Vision-Language Models (VLMs) have underscored their superiority in various multimodal tasks.","However, the adversarial robustness of VLMs has not been fully explored.","Existing methods mainly assess robustness through unimodal adversarial attacks that perturb images, while assuming inherent resilience against text-based attacks.","Different from existing attacks, in this work we propose a more comprehensive strategy that jointly attacks both text and image modalities to exploit a broader spectrum of vulnerability within VLMs.","Specifically, we propose a dual optimization objective aimed at guiding the model to generate affirmative responses with high toxicity.","Our attack method begins by optimizing an adversarial image prefix from random noise to generate diverse harmful responses in the absence of text input, thus imbuing the image with toxic semantics.","Subsequently, an adversarial text suffix is integrated and co-optimized with the adversarial image prefix to maximize the probability of eliciting affirmative responses to various harmful instructions.","The discovered adversarial image prefix and text suffix are collectively denoted as a Universal Master Key (UMK).","When integrated into various malicious queries, UMK can circumvent the alignment defenses of VLMs and lead to the generation of objectionable content, known as jailbreaks.","The experimental results demonstrate that our universal attack strategy can effectively jailbreak MiniGPT-4 with a 96% success rate, highlighting the vulnerability of VLMs and the urgent need for new alignment strategies."],"url":"http://arxiv.org/abs/2405.17894v1","category":"cs.CV"}
{"created":"2024-05-28 07:13:25","title":"Arithmetic Reasoning with LLM: Prolog Generation & Permutation","abstract":"Instructing large language models (LLMs) to solve elementary school math problems has shown great success using Chain of Thought (CoT). However, the CoT approach relies on an LLM to generate a sequence of arithmetic calculations which can be prone to cascaded calculation errors. We hypothesize that an LLM should focus on extracting predicates and generating symbolic formulas from the math problem description so that the underlying calculation can be done via an external code interpreter. We investigate using LLM to generate Prolog programs to solve mathematical questions. Experimental results show that our Prolog-based arithmetic problem-solving outperforms CoT generation in the GSM8K benchmark across three distinct LLMs. In addition, given the insensitive ordering of predicates and symbolic formulas in Prolog, we propose to permute the ground truth predicates for more robust LLM training via data augmentation.","sentences":["Instructing large language models (LLMs) to solve elementary school math problems has shown great success using Chain of Thought (CoT).","However, the CoT approach relies on an LLM to generate a sequence of arithmetic calculations which can be prone to cascaded calculation errors.","We hypothesize that an LLM should focus on extracting predicates and generating symbolic formulas from the math problem description so that the underlying calculation can be done via an external code interpreter.","We investigate using LLM to generate Prolog programs to solve mathematical questions.","Experimental results show that our Prolog-based arithmetic problem-solving outperforms CoT generation in the GSM8K benchmark across three distinct LLMs.","In addition, given the insensitive ordering of predicates and symbolic formulas in Prolog, we propose to permute the ground truth predicates for more robust LLM training via data augmentation."],"url":"http://arxiv.org/abs/2405.17893v1","category":"cs.CL"}
{"created":"2024-05-28 07:11:05","title":"Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment","abstract":"Aligning human preference and value is an important requirement for contemporary foundation models. State-of-the-art techniques such as Reinforcement Learning from Human Feedback (RLHF) often consist of two stages: 1) supervised fine-tuning (SFT), where the model is fine-tuned by learning from human demonstration data; 2) Preference learning, where preference data is used to learn a reward model, which is in turn used by a reinforcement learning (RL) step to fine-tune the model. Such reward model serves as a proxy to human preference, and it is critical to guide the RL step towards improving the model quality. In this work, we argue that the SFT stage significantly benefits from learning a reward model as well. Instead of using the human demonstration data directly via supervised learning, we propose to leverage an Inverse Reinforcement Learning (IRL) technique to (explicitly or implicitly) build an reward model, while learning the policy model. This approach leads to new SFT algorithms that are not only efficient to implement, but also promote the ability to distinguish between the preferred and non-preferred continuations. Moreover, we identify a connection between the proposed IRL based approach, and certain self-play approach proposed recently, and showed that self-play is a special case of modeling a reward-learning agent. Theoretically, we show that the proposed algorithms converge to the stationary solutions of the IRL problem. Empirically, we align 1B and 7B models using proposed methods and evaluate them on a reward benchmark model and the HuggingFace Open LLM Leaderboard. The proposed methods show significant performance improvement over existing SFT approaches. Our results indicate that it is beneficial to explicitly or implicitly leverage reward learning throughout the entire alignment process.","sentences":["Aligning human preference and value is an important requirement for contemporary foundation models.","State-of-the-art techniques such as Reinforcement Learning from Human Feedback (RLHF) often consist of two stages: 1) supervised fine-tuning (SFT), where the model is fine-tuned by learning from human demonstration data; 2) Preference learning, where preference data is used to learn a reward model, which is in turn used by a reinforcement learning (RL) step to fine-tune the model.","Such reward model serves as a proxy to human preference, and it is critical to guide the RL step towards improving the model quality.","In this work, we argue that the SFT stage significantly benefits from learning a reward model as well.","Instead of using the human demonstration data directly via supervised learning, we propose to leverage an Inverse Reinforcement Learning (IRL) technique to (explicitly or implicitly) build an reward model, while learning the policy model.","This approach leads to new SFT algorithms that are not only efficient to implement, but also promote the ability to distinguish between the preferred and non-preferred continuations.","Moreover, we identify a connection between the proposed IRL based approach, and certain self-play approach proposed recently, and showed that self-play is a special case of modeling a reward-learning agent.","Theoretically, we show that the proposed algorithms converge to the stationary solutions of the IRL problem.","Empirically, we align 1B and 7B models using proposed methods and evaluate them on a reward benchmark model and the HuggingFace Open LLM Leaderboard.","The proposed methods show significant performance improvement over existing SFT approaches.","Our results indicate that it is beneficial to explicitly or implicitly leverage reward learning throughout the entire alignment process."],"url":"http://arxiv.org/abs/2405.17888v1","category":"cs.AI"}
{"created":"2024-05-28 06:57:22","title":"Resisting Stochastic Risks in Diffusion Planners with the Trajectory Aggregation Tree","abstract":"Diffusion planners have shown promise in handling long-horizon and sparse-reward tasks due to the non-autoregressive plan generation. However, their inherent stochastic risk of generating infeasible trajectories presents significant challenges to their reliability and stability. We introduce a novel approach, the Trajectory Aggregation Tree (TAT), to address this issue in diffusion planners. Compared to prior methods that rely solely on raw trajectory predictions, TAT aggregates information from both historical and current trajectories, forming a dynamic tree-like structure. Each trajectory is conceptualized as a branch and individual states as nodes. As the structure evolves with the integration of new trajectories, unreliable states are marginalized, and the most impactful nodes are prioritized for decision-making. TAT can be deployed without modifying the original training and sampling pipelines of diffusion planners, making it a training-free, ready-to-deploy solution. We provide both theoretical analysis and empirical evidence to support TAT's effectiveness. Our results highlight its remarkable ability to resist the risk from unreliable trajectories, guarantee the performance boosting of diffusion planners in $100\\%$ of tasks, and exhibit an appreciable tolerance margin for sample quality, thereby enabling planning with a more than $3\\times$ acceleration.","sentences":["Diffusion planners have shown promise in handling long-horizon and sparse-reward tasks due to the non-autoregressive plan generation.","However, their inherent stochastic risk of generating infeasible trajectories presents significant challenges to their reliability and stability.","We introduce a novel approach, the Trajectory Aggregation Tree (TAT), to address this issue in diffusion planners.","Compared to prior methods that rely solely on raw trajectory predictions, TAT aggregates information from both historical and current trajectories, forming a dynamic tree-like structure.","Each trajectory is conceptualized as a branch and individual states as nodes.","As the structure evolves with the integration of new trajectories, unreliable states are marginalized, and the most impactful nodes are prioritized for decision-making.","TAT can be deployed without modifying the original training and sampling pipelines of diffusion planners, making it a training-free, ready-to-deploy solution.","We provide both theoretical analysis and empirical evidence to support TAT's effectiveness.","Our results highlight its remarkable ability to resist the risk from unreliable trajectories, guarantee the performance boosting of diffusion planners in $100\\%$ of tasks, and exhibit an appreciable tolerance margin for sample quality, thereby enabling planning with a more than $3\\times$ acceleration."],"url":"http://arxiv.org/abs/2405.17879v1","category":"cs.LG"}
{"created":"2024-05-28 06:57:01","title":"An Information Theoretic Metric for Evaluating Unlearning Models","abstract":"Machine unlearning (MU) addresses privacy concerns by removing information of `forgetting data' samples from trained models. Typically, evaluating MU methods involves comparing unlearned models to those retrained from scratch without forgetting data, using metrics such as membership inference attacks (MIA) and accuracy measurements. These evaluations implicitly assume that if the output logits of the unlearned and retrained models are similar, the unlearned model has successfully forgotten the data. Here, we challenge if this assumption is valid. In particular, we conduct a simple experiment of training only the last layer of a given original model using a novel masked-distillation technique while keeping the rest fixed. Surprisingly, simply altering the last layer yields favorable outcomes in the existing evaluation metrics, while the model does not successfully unlearn the samples or classes. For better evaluating the MU methods, we propose a metric that quantifies the residual information about forgetting data samples in intermediate features using mutual information, called information difference index or IDI for short. The IDI provides a comprehensive evaluation of MU methods by efficiently analyzing the internal structure of DNNs. Our metric is scalable to large datasets and adaptable to various model architectures. Additionally, we present COLapse-and-Align (COLA), a simple contrastive-based method that effectively unlearns intermediate features.","sentences":["Machine unlearning (MU) addresses privacy concerns by removing information of `forgetting data' samples from trained models.","Typically, evaluating MU methods involves comparing unlearned models to those retrained from scratch without forgetting data, using metrics such as membership inference attacks (MIA) and accuracy measurements.","These evaluations implicitly assume that if the output logits of the unlearned and retrained models are similar, the unlearned model has successfully forgotten the data.","Here, we challenge if this assumption is valid.","In particular, we conduct a simple experiment of training only the last layer of a given original model using a novel masked-distillation technique while keeping the rest fixed.","Surprisingly, simply altering the last layer yields favorable outcomes in the existing evaluation metrics, while the model does not successfully unlearn the samples or classes.","For better evaluating the MU methods, we propose a metric that quantifies the residual information about forgetting data samples in intermediate features using mutual information, called information difference index or IDI for short.","The IDI provides a comprehensive evaluation of MU methods by efficiently analyzing the internal structure of DNNs.","Our metric is scalable to large datasets and adaptable to various model architectures.","Additionally, we present COLapse-and-Align (COLA), a simple contrastive-based method that effectively unlearns intermediate features."],"url":"http://arxiv.org/abs/2405.17878v1","category":"cs.LG"}
{"created":"2024-05-28 06:51:42","title":"NUTS, NARS, and Speech","abstract":"To investigate whether \"Intelligence is the capacity of an information-processing system to adapt to its environment while operating with insufficient knowledge and resources\", we look at utilising the non axiomatic reasoning system (NARS) for speech recognition. This article presents NUTS: raNdom dimensionality redUction non axiomaTic reasoning few Shot learner for perception. NUTS consists of naive dimensionality reduction, some pre-processing, and then non axiomatic reasoning (NARS). With only 2 training examples NUTS performs similarly to the Whisper Tiny model for discrete word identification.","sentences":["To investigate whether \"Intelligence is the capacity of an information-processing system to adapt to its environment while operating with insufficient knowledge and resources\", we look at utilising the non axiomatic reasoning system (NARS) for speech recognition.","This article presents NUTS: raNdom dimensionality redUction non axiomaTic reasoning few Shot learner for perception.","NUTS consists of naive dimensionality reduction, some pre-processing, and then non axiomatic reasoning (NARS).","With only 2 training examples NUTS performs similarly to the Whisper Tiny model for discrete word identification."],"url":"http://arxiv.org/abs/2405.17874v1","category":"cs.LG"}
{"created":"2024-05-28 06:50:58","title":"MixDQ: Memory-Efficient Few-Step Text-to-Image Diffusion Models with Metric-Decoupled Mixed Precision Quantization","abstract":"Diffusion models have achieved significant visual generation quality. However, their significant computational and memory costs pose challenge for their application on resource-constrained mobile devices or even desktop GPUs. Recent few-step diffusion models reduces the inference time by reducing the denoising steps. However, their memory consumptions are still excessive. The Post Training Quantization (PTQ) replaces high bit-width FP representation with low-bit integer values (INT4/8) , which is an effective and efficient technique to reduce the memory cost. However, when applying to few-step diffusion models, existing quantization methods face challenges in preserving both the image quality and text alignment. To address this issue, we propose an mixed-precision quantization framework - MixDQ. Firstly, We design specialized BOS-aware quantization method for highly sensitive text embedding quantization. Then, we conduct metric-decoupled sensitivity analysis to measure the sensitivity of each layer. Finally, we develop an integer-programming-based method to conduct bit-width allocation. While existing quantization methods fall short at W8A8, MixDQ could achieve W8A8 without performance loss, and W4A8 with negligible visual degradation. Compared with FP16, we achieve 3-4x reduction in model size and memory cost, and 1.45x latency speedup.","sentences":["Diffusion models have achieved significant visual generation quality.","However, their significant computational and memory costs pose challenge for their application on resource-constrained mobile devices or even desktop GPUs.","Recent few-step diffusion models reduces the inference time by reducing the denoising steps.","However, their memory consumptions are still excessive.","The Post Training Quantization (PTQ) replaces high bit-width FP representation with low-bit integer values (INT4/8) , which is an effective and efficient technique to reduce the memory cost.","However, when applying to few-step diffusion models, existing quantization methods face challenges in preserving both the image quality and text alignment.","To address this issue, we propose an mixed-precision quantization framework - MixDQ.","Firstly, We design specialized BOS-aware quantization method for highly sensitive text embedding quantization.","Then, we conduct metric-decoupled sensitivity analysis to measure the sensitivity of each layer.","Finally, we develop an integer-programming-based method to conduct bit-width allocation.","While existing quantization methods fall short at W8A8, MixDQ could achieve W8A8 without performance loss, and W4A8 with negligible visual degradation.","Compared with FP16, we achieve 3-4x reduction in model size and memory cost, and 1.45x latency speedup."],"url":"http://arxiv.org/abs/2405.17873v1","category":"cs.CV"}
{"created":"2024-05-28 06:44:13","title":"Seeing the Image: Prioritizing Visual Correlation by Contrastive Alignment","abstract":"Existing image-text modality alignment in Vision Language Models (VLMs) treats each text token equally in an autoregressive manner. Despite being simple and effective, this method results in sub-optimal cross-modal alignment by over-emphasizing the text tokens that are less correlated with or even contradictory with the input images. In this paper, we advocate for assigning distinct contributions for each text token based on its visual correlation. Specifically, we present by contrasting image inputs, the difference in prediction logits on each text token provides strong guidance of visual correlation. We therefore introduce Contrastive ALignment (CAL), a simple yet effective re-weighting strategy that prioritizes training visually correlated tokens. Our experimental results demonstrate that CAL consistently improves different types of VLMs across different resolutions and model sizes on various benchmark datasets. Importantly, our method incurs minimal additional computational overhead, rendering it highly efficient compared to alternative data scaling strategies. Codes are available at https://github.com/foundation-multimodal-models/CAL.","sentences":["Existing image-text modality alignment in Vision Language Models (VLMs) treats each text token equally in an autoregressive manner.","Despite being simple and effective, this method results in sub-optimal cross-modal alignment by over-emphasizing the text tokens that are less correlated with or even contradictory with the input images.","In this paper, we advocate for assigning distinct contributions for each text token based on its visual correlation.","Specifically, we present by contrasting image inputs, the difference in prediction logits on each text token provides strong guidance of visual correlation.","We therefore introduce Contrastive ALignment (CAL), a simple yet effective re-weighting strategy that prioritizes training visually correlated tokens.","Our experimental results demonstrate that CAL consistently improves different types of VLMs across different resolutions and model sizes on various benchmark datasets.","Importantly, our method incurs minimal additional computational overhead, rendering it highly efficient compared to alternative data scaling strategies.","Codes are available at https://github.com/foundation-multimodal-models/CAL."],"url":"http://arxiv.org/abs/2405.17871v1","category":"cs.CV"}
{"created":"2024-05-28 05:56:11","title":"I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models","abstract":"Post-training quantization (PTQ) serves as a potent technique to accelerate the inference of large language models (LLMs). Nonetheless, existing works still necessitate a considerable number of floating-point (FP) operations during inference, including additional quantization and de-quantization, as well as non-linear operators such as RMSNorm and Softmax. This limitation hinders the deployment of LLMs on the edge and cloud devices. In this paper, we identify the primary obstacle to integer-only quantization for LLMs lies in the large fluctuation of activations across channels and tokens in both linear and non-linear operations. To address this issue, we propose I-LLM, a novel integer-only fully-quantized PTQ framework tailored for LLMs. Specifically, (1) we develop Fully-Smooth Block-Reconstruction (FSBR) to aggressively smooth inter-channel variations of all activations and weights. (2) to alleviate degradation caused by inter-token variations, we introduce a novel approach called Dynamic Integer-only MatMul (DI-MatMul). This method enables dynamic quantization in full-integer matrix multiplication by dynamically quantizing the input and outputs with integer-only operations. (3) we design DI-ClippedSoftmax, DI-Exp, and DI-Normalization, which utilize bit shift to execute non-linear operators efficiently while maintaining accuracy. The experiment shows that our I-LLM achieves comparable accuracy to the FP baseline and outperforms non-integer quantization methods. For example, I-LLM can operate at W4A4 with negligible loss of accuracy. To our knowledge, we are the first to bridge the gap between integer-only quantization and LLMs. We've published our code on anonymous.4open.science, aiming to contribute to the advancement of this field.","sentences":["Post-training quantization (PTQ) serves as a potent technique to accelerate the inference of large language models (LLMs).","Nonetheless, existing works still necessitate a considerable number of floating-point (FP) operations during inference, including additional quantization and de-quantization, as well as non-linear operators such as RMSNorm and Softmax.","This limitation hinders the deployment of LLMs on the edge and cloud devices.","In this paper, we identify the primary obstacle to integer-only quantization for LLMs lies in the large fluctuation of activations across channels and tokens in both linear and non-linear operations.","To address this issue, we propose I-LLM, a novel integer-only fully-quantized PTQ framework tailored for LLMs.","Specifically, (1) we develop Fully-Smooth Block-Reconstruction (FSBR) to aggressively smooth inter-channel variations of all activations and weights.","(2) to alleviate degradation caused by inter-token variations, we introduce a novel approach called Dynamic Integer-only MatMul (DI-MatMul).","This method enables dynamic quantization in full-integer matrix multiplication by dynamically quantizing the input and outputs with integer-only operations.","(3) we design DI-ClippedSoftmax, DI-Exp, and DI-Normalization, which utilize bit shift to execute non-linear operators efficiently while maintaining accuracy.","The experiment shows that our I-LLM achieves comparable accuracy to the FP baseline and outperforms non-integer quantization methods.","For example, I-LLM can operate at W4A4 with negligible loss of accuracy.","To our knowledge, we are the first to bridge the gap between integer-only quantization and LLMs.","We've published our code on anonymous.4open.science, aiming to contribute to the advancement of this field."],"url":"http://arxiv.org/abs/2405.17849v1","category":"cs.LG"}
{"created":"2024-05-28 05:50:25","title":"Safety Control of Service Robots with LLMs and Embodied Knowledge Graphs","abstract":"Safety limitations in service robotics across various industries have raised significant concerns about the need for robust mechanisms ensuring that robots adhere to safe practices, thereby preventing actions that might harm humans or cause property damage. Despite advances, including the integration of Knowledge Graphs (KGs) with Large Language Models (LLMs), challenges in ensuring consistent safety in autonomous robot actions persist. In this paper, we propose a novel integration of Large Language Models with Embodied Robotic Control Prompts (ERCPs) and Embodied Knowledge Graphs (EKGs) to enhance the safety framework for service robots. ERCPs are designed as predefined instructions that ensure LLMs generate safe and precise responses. These responses are subsequently validated by EKGs, which provide a comprehensive knowledge base ensuring that the actions of the robot are continuously aligned with safety protocols, thereby promoting safer operational practices in varied contexts. Our experimental setup involved diverse real-world tasks, where robots equipped with our framework demonstrated significantly higher compliance with safety standards compared to traditional methods. This integration fosters secure human-robot interactions and positions our methodology at the forefront of AI-driven safety innovations in service robotics.","sentences":["Safety limitations in service robotics across various industries have raised significant concerns about the need for robust mechanisms ensuring that robots adhere to safe practices, thereby preventing actions that might harm humans or cause property damage.","Despite advances, including the integration of Knowledge Graphs (KGs) with Large Language Models (LLMs), challenges in ensuring consistent safety in autonomous robot actions persist.","In this paper, we propose a novel integration of Large Language Models with Embodied Robotic Control Prompts (ERCPs) and Embodied Knowledge Graphs (EKGs) to enhance the safety framework for service robots.","ERCPs are designed as predefined instructions that ensure LLMs generate safe and precise responses.","These responses are subsequently validated by EKGs, which provide a comprehensive knowledge base ensuring that the actions of the robot are continuously aligned with safety protocols, thereby promoting safer operational practices in varied contexts.","Our experimental setup involved diverse real-world tasks, where robots equipped with our framework demonstrated significantly higher compliance with safety standards compared to traditional methods.","This integration fosters secure human-robot interactions and positions our methodology at the forefront of AI-driven safety innovations in service robotics."],"url":"http://arxiv.org/abs/2405.17846v1","category":"cs.RO"}
{"created":"2024-05-28 05:50:10","title":"A System for Quantifying Data Science Workflows with Fine-Grained Procedural Logging and a Pilot Study","abstract":"It is important for researchers to understand precisely how data scientists turn raw data into insights, including typical programming patterns, workflow, and methodology. This paper contributes a novel system, called DataInquirer, that tracks incremental code executions in Jupyter notebooks (a type of computational notebook). The system allows us to quantitatively measure timing, workflow, and operation frequency in data science tasks without resorting to human annotation or interview. In a series of pilot studies, we collect 97 traces, logging data scientist activities across four studies. While this paper presents a general system and data analysis approach, we focus on a foundational sub-question in our pilot studies: How consistent are different data scientists in analyzing the same data? We taxonomize variation between data scientists on the same dataset according to three categories: semantic, syntactic, and methodological. Our results suggest that there are statistically significant differences in the conclusions reached by different data scientists on the same task and present quantitative evidence for this phenomenon. Furthermore, our results suggest that AI-powered code tools subtly influence these results, allowing student participants to generate workflows that more resemble expert data practitioners.","sentences":["It is important for researchers to understand precisely how data scientists turn raw data into insights, including typical programming patterns, workflow, and methodology.","This paper contributes a novel system, called DataInquirer, that tracks incremental code executions in Jupyter notebooks (a type of computational notebook).","The system allows us to quantitatively measure timing, workflow, and operation frequency in data science tasks without resorting to human annotation or interview.","In a series of pilot studies, we collect 97 traces, logging data scientist activities across four studies.","While this paper presents a general system and data analysis approach, we focus on a foundational sub-question in our pilot studies: How consistent are different data scientists in analyzing the same data?","We taxonomize variation between data scientists on the same dataset according to three categories: semantic, syntactic, and methodological.","Our results suggest that there are statistically significant differences in the conclusions reached by different data scientists on the same task and present quantitative evidence for this phenomenon.","Furthermore, our results suggest that AI-powered code tools subtly influence these results, allowing student participants to generate workflows that more resemble expert data practitioners."],"url":"http://arxiv.org/abs/2405.17845v1","category":"cs.HC"}
{"created":"2024-05-28 05:30:18","title":"PeerFL: A Simulator for Peer-to-Peer Federated Learning at Scale","abstract":"This work integrates peer-to-peer federated learning tools with NS3, a widely used network simulator, to create a novel simulator designed to allow heterogeneous device experiments in federated learning. This cross-platform adaptability addresses a critical gap in existing simulation tools, enhancing the overall utility and user experience. NS3 is leveraged to simulate WiFi dynamics to facilitate federated learning experiments with participants that move around physically during training, leading to dynamic network characteristics. Our experiments showcase the simulator's efficiency in computational resource utilization at scale, with a maximum of 450 heterogeneous devices modelled as participants in federated learning. This positions it as a valuable tool for simulation-based investigations in peer-to-peer federated learning. The framework is open source and available for use and extension to the community.","sentences":["This work integrates peer-to-peer federated learning tools with NS3, a widely used network simulator, to create a novel simulator designed to allow heterogeneous device experiments in federated learning.","This cross-platform adaptability addresses a critical gap in existing simulation tools, enhancing the overall utility and user experience.","NS3 is leveraged to simulate WiFi dynamics to facilitate federated learning experiments with participants that move around physically during training, leading to dynamic network characteristics.","Our experiments showcase the simulator's efficiency in computational resource utilization at scale, with a maximum of 450 heterogeneous devices modelled as participants in federated learning.","This positions it as a valuable tool for simulation-based investigations in peer-to-peer federated learning.","The framework is open source and available for use and extension to the community."],"url":"http://arxiv.org/abs/2405.17839v1","category":"cs.DC"}
{"created":"2024-05-28 05:28:49","title":"Trust and Terror: Hazards in Text Reveal Negatively Biased Credulity and Partisan Negativity Bias","abstract":"Socio-linguistic indicators of text, such as emotion or sentiment, are often extracted using neural networks in order to better understand features of social media. One indicator that is often overlooked, however, is the presence of hazards within text. Recent psychological research suggests that statements about hazards are more believable than statements about benefits (a property known as negatively biased credulity), and that political liberals and conservatives differ in how often they share hazards. Here, we develop a new model to detect information concerning hazards, trained on a new collection of annotated X posts, as well as urban legends annotated in previous work. We show that not only does this model perform well (outperforming, e.g., zero-shot human annotator proxies, such as GPT-4) but that the hazard information it extracts is not strongly correlated with other indicators, namely moral outrage, sentiment, emotions, and threat words. (That said, consonant with expectations, hazard information does correlate positively with such emotions as fear, and negatively with emotions like joy.) We then apply this model to three datasets: X posts about COVID-19, X posts about the 2023 Hamas-Israel war, and a new expanded collection of urban legends. From these data, we uncover words associated with hazards unique to each dataset as well as differences in this language between groups of users, such as conservatives and liberals, which informs what these groups perceive as hazards. We further show that information about hazards peaks in frequency after major hazard events, and therefore acts as an automated indicator of such events. Finally, we find that information about hazards is especially prevalent in urban legends, which is consistent with previous work that finds that reports of hazards are more likely to be both believed and transmitted.","sentences":["Socio-linguistic indicators of text, such as emotion or sentiment, are often extracted using neural networks in order to better understand features of social media.","One indicator that is often overlooked, however, is the presence of hazards within text.","Recent psychological research suggests that statements about hazards are more believable than statements about benefits (a property known as negatively biased credulity), and that political liberals and conservatives differ in how often they share hazards.","Here, we develop a new model to detect information concerning hazards, trained on a new collection of annotated X posts, as well as urban legends annotated in previous work.","We show that not only does this model perform well (outperforming, e.g., zero-shot human annotator proxies, such as GPT-4) but that the hazard information it extracts is not strongly correlated with other indicators, namely moral outrage, sentiment, emotions, and threat words.","(That said, consonant with expectations, hazard information does correlate positively with such emotions as fear, and negatively with emotions like joy.)","We then apply this model to three datasets: X posts about COVID-19, X posts about the 2023 Hamas-Israel war, and a new expanded collection of urban legends.","From these data, we uncover words associated with hazards unique to each dataset as well as differences in this language between groups of users, such as conservatives and liberals, which informs what these groups perceive as hazards.","We further show that information about hazards peaks in frequency after major hazard events, and therefore acts as an automated indicator of such events.","Finally, we find that information about hazards is especially prevalent in urban legends, which is consistent with previous work that finds that reports of hazards are more likely to be both believed and transmitted."],"url":"http://arxiv.org/abs/2405.17838v1","category":"cs.LG"}
{"created":"2024-05-28 05:05:33","title":"Mollification Effects of Policy Gradient Methods","abstract":"Policy gradient methods have enabled deep reinforcement learning (RL) to approach challenging continuous control problems, even when the underlying systems involve highly nonlinear dynamics that generate complex non-smooth optimization landscapes. We develop a rigorous framework for understanding how policy gradient methods mollify non-smooth optimization landscapes to enable effective policy search, as well as the downside of it: while making the objective function smoother and easier to optimize, the stochastic objective deviates further from the original problem. We demonstrate the equivalence between policy gradient methods and solving backward heat equations. Following the ill-posedness of backward heat equations from PDE theory, we present a fundamental challenge to the use of policy gradient under stochasticity. Moreover, we make the connection between this limitation and the uncertainty principle in harmonic analysis to understand the effects of exploration with stochastic policies in RL. We also provide experimental results to illustrate both the positive and negative aspects of mollification effects in practice.","sentences":["Policy gradient methods have enabled deep reinforcement learning (RL) to approach challenging continuous control problems, even when the underlying systems involve highly nonlinear dynamics that generate complex non-smooth optimization landscapes.","We develop a rigorous framework for understanding how policy gradient methods mollify non-smooth optimization landscapes to enable effective policy search, as well as the downside of it: while making the objective function smoother and easier to optimize, the stochastic objective deviates further from the original problem.","We demonstrate the equivalence between policy gradient methods and solving backward heat equations.","Following the ill-posedness of backward heat equations from PDE theory, we present a fundamental challenge to the use of policy gradient under stochasticity.","Moreover, we make the connection between this limitation and the uncertainty principle in harmonic analysis to understand the effects of exploration with stochastic policies in RL.","We also provide experimental results to illustrate both the positive and negative aspects of mollification effects in practice."],"url":"http://arxiv.org/abs/2405.17832v1","category":"cs.LG"}
{"created":"2024-05-28 04:59:13","title":"LDMol: Text-Conditioned Molecule Diffusion Model Leveraging Chemically Informative Latent Space","abstract":"With the emergence of diffusion models as the frontline of generative models, many researchers have proposed molecule generation techniques using conditional diffusion models. However, due to the fundamental nature of a molecule, which carries highly entangled correlations within a small number of atoms and bonds, it becomes difficult for a model to connect raw data with the conditions when the conditions become more complex as natural language. To address this, here we present a novel latent diffusion model dubbed LDMol, which enables a natural text-conditioned molecule generation. Specifically, LDMol is composed of three building blocks: a molecule encoder that produces a chemically informative feature space, a natural language-conditioned latent diffusion model using a Diffusion Transformer (DiT), and an autoregressive decoder for molecule re. In particular, recognizing that multiple SMILES notations can represent the same molecule, we employ a contrastive learning strategy to extract the chemical informative feature space. LDMol not only beats the existing baselines on the text-to-molecule generation benchmark but is also capable of zero-shot inference with unseen scenarios. Furthermore, we show that LDMol can be applied to downstream tasks such as molecule-to-text retrieval and text-driven molecule editing, demonstrating its versatility as a diffusion model.","sentences":["With the emergence of diffusion models as the frontline of generative models, many researchers have proposed molecule generation techniques using conditional diffusion models.","However, due to the fundamental nature of a molecule, which carries highly entangled correlations within a small number of atoms and bonds, it becomes difficult for a model to connect raw data with the conditions when the conditions become more complex as natural language.","To address this, here we present a novel latent diffusion model dubbed LDMol, which enables a natural text-conditioned molecule generation.","Specifically, LDMol is composed of three building blocks: a molecule encoder that produces a chemically informative feature space, a natural language-conditioned latent diffusion model using a Diffusion Transformer (DiT), and an autoregressive decoder for molecule re.","In particular, recognizing that multiple SMILES notations can represent the same molecule, we employ a contrastive learning strategy to extract the chemical informative feature space.","LDMol not only beats the existing baselines on the text-to-molecule generation benchmark but is also capable of zero-shot inference with unseen scenarios.","Furthermore, we show that LDMol can be applied to downstream tasks such as molecule-to-text retrieval and text-driven molecule editing, demonstrating its versatility as a diffusion model."],"url":"http://arxiv.org/abs/2405.17829v1","category":"cs.LG"}
{"created":"2024-05-28 04:47:54","title":"Diffusion Model Patching via Mixture-of-Prompts","abstract":"We present Diffusion Model Patching (DMP), a simple method to boost the performance of pre-trained diffusion models that have already reached convergence, with a negligible increase in parameters. DMP inserts a small, learnable set of prompts into the model's input space while keeping the original model frozen. The effectiveness of DMP is not merely due to the addition of parameters but stems from its dynamic gating mechanism, which selects and combines a subset of learnable prompts at every step of the generative process (e.g., reverse denoising steps). This strategy, which we term \"mixture-of-prompts\", enables the model to draw on the distinct expertise of each prompt, essentially \"patching\" the model's functionality at every step with minimal yet specialized parameters. Uniquely, DMP enhances the model by further training on the same dataset on which it was originally trained, even in a scenario where significant improvements are typically not expected due to model convergence. Experiments show that DMP significantly enhances the converged FID of DiT-L/2 on FFHQ 256x256 by 10.38%, achieved with only a 1.43% parameter increase and 50K additional training iterations.","sentences":["We present Diffusion Model Patching (DMP), a simple method to boost the performance of pre-trained diffusion models that have already reached convergence, with a negligible increase in parameters.","DMP inserts a small, learnable set of prompts into the model's input space while keeping the original model frozen.","The effectiveness of DMP is not merely due to the addition of parameters but stems from its dynamic gating mechanism, which selects and combines a subset of learnable prompts at every step of the generative process (e.g., reverse denoising steps).","This strategy, which we term \"mixture-of-prompts\", enables the model to draw on the distinct expertise of each prompt, essentially \"patching\" the model's functionality at every step with minimal yet specialized parameters.","Uniquely, DMP enhances the model by further training on the same dataset on which it was originally trained, even in a scenario where significant improvements are typically not expected due to model convergence.","Experiments show that DMP significantly enhances the converged FID of DiT-L/2 on FFHQ 256x256 by 10.38%, achieved with only a 1.43% parameter increase and 50K additional training iterations."],"url":"http://arxiv.org/abs/2405.17825v1","category":"cs.CV"}
{"created":"2024-05-28 04:46:52","title":"Conv-CoA: Improving Open-domain Question Answering in Large Language Models via Conversational Chain-of-Action","abstract":"We present a Conversational Chain-of-Action (Conv-CoA) framework for Open-domain Conversational Question Answering (OCQA). Compared with literature, Conv-CoA addresses three major challenges: (i) unfaithful hallucination that is inconsistent with real-time or domain facts, (ii) weak reasoning performance in conversational scenarios, and (iii) unsatisfying performance in conversational information retrieval. Our key contribution is a dynamic reasoning-retrieval mechanism that extracts the intent of the question and decomposes it into a reasoning chain to be solved via systematic prompting, pre-designed actions, updating the Contextual Knowledge Set (CKS), and a novel Hopfield-based retriever. Methodologically, we propose a resource-efficiency Hopfield retriever to enhance the efficiency and accuracy of conversational information retrieval within our actions. Additionally, we propose a conversational-multi-reference faith score (Conv-MRFS) to verify and resolve conflicts between retrieved knowledge and answers in conversations. Empirically, we conduct comparisons between our framework and 23 state-of-the-art methods across five different research directions and two public benchmarks. These comparisons demonstrate that our Conv-CoA outperforms other methods in both the accuracy and efficiency dimensions.","sentences":["We present a Conversational Chain-of-Action (Conv-CoA) framework for Open-domain Conversational Question Answering (OCQA).","Compared with literature, Conv-CoA addresses three major challenges: (i) unfaithful hallucination that is inconsistent with real-time or domain facts, (ii) weak reasoning performance in conversational scenarios, and (iii) unsatisfying performance in conversational information retrieval.","Our key contribution is a dynamic reasoning-retrieval mechanism that extracts the intent of the question and decomposes it into a reasoning chain to be solved via systematic prompting, pre-designed actions, updating the Contextual Knowledge Set (CKS), and a novel Hopfield-based retriever.","Methodologically, we propose a resource-efficiency Hopfield retriever to enhance the efficiency and accuracy of conversational information retrieval within our actions.","Additionally, we propose a conversational-multi-reference faith score (Conv-MRFS) to verify and resolve conflicts between retrieved knowledge and answers in conversations.","Empirically, we conduct comparisons between our framework and 23 state-of-the-art methods across five different research directions and two public benchmarks.","These comparisons demonstrate that our Conv-CoA outperforms other methods in both the accuracy and efficiency dimensions."],"url":"http://arxiv.org/abs/2405.17822v1","category":"cs.CL"}
{"created":"2024-05-28 04:41:02","title":"RITUAL: Random Image Transformations as a Universal Anti-hallucination Lever in LVLMs","abstract":"Recent advancements in Large Vision Language Models (LVLMs) have revolutionized how machines understand and generate textual responses based on visual inputs. Despite their impressive capabilities, they often produce \"hallucinatory\" outputs that do not accurately reflect the visual information, posing challenges in reliability and trustworthiness. Current methods such as contrastive decoding have made strides in addressing these issues by contrasting the original probability distribution of generated tokens with distorted counterparts; yet, generating visually-faithful outputs remains a challenge. In this work, we shift our focus to the opposite: What could serve as a complementary enhancement to the original probability distribution? We propose a simple, training-free method termed RITUAL to enhance robustness against hallucinations in LVLMs. Our approach employs random image transformations as complements to the original probability distribution, aiming to mitigate the likelihood of hallucinatory visual explanations by enriching the model's exposure to varied visual scenarios. Our empirical results show that while the isolated use of transformed images initially degrades performance, strategic implementation of these transformations can indeed serve as effective complements. Notably, our method is compatible with current contrastive decoding methods and does not require external models or costly self-feedback mechanisms, making it a practical addition. In experiments, RITUAL significantly outperforms existing contrastive decoding methods across several object hallucination benchmarks, including POPE, CHAIR, and MME.","sentences":["Recent advancements in Large Vision Language Models (LVLMs) have revolutionized how machines understand and generate textual responses based on visual inputs.","Despite their impressive capabilities, they often produce \"hallucinatory\" outputs that do not accurately reflect the visual information, posing challenges in reliability and trustworthiness.","Current methods such as contrastive decoding have made strides in addressing these issues by contrasting the original probability distribution of generated tokens with distorted counterparts; yet, generating visually-faithful outputs remains a challenge.","In this work, we shift our focus to the opposite: What could serve as a complementary enhancement to the original probability distribution?","We propose a simple, training-free method termed RITUAL to enhance robustness against hallucinations in LVLMs.","Our approach employs random image transformations as complements to the original probability distribution, aiming to mitigate the likelihood of hallucinatory visual explanations by enriching the model's exposure to varied visual scenarios.","Our empirical results show that while the isolated use of transformed images initially degrades performance, strategic implementation of these transformations can indeed serve as effective complements.","Notably, our method is compatible with current contrastive decoding methods and does not require external models or costly self-feedback mechanisms, making it a practical addition.","In experiments, RITUAL significantly outperforms existing contrastive decoding methods across several object hallucination benchmarks, including POPE, CHAIR, and MME."],"url":"http://arxiv.org/abs/2405.17821v1","category":"cs.CV"}
{"created":"2024-05-28 04:40:57","title":"Don't Miss the Forest for the Trees: Attentional Vision Calibration for Large Vision Language Models","abstract":"This study addresses the issue observed in Large Vision Language Models (LVLMs), where excessive attention on a few image tokens, referred to as blind tokens, leads to hallucinatory responses in tasks requiring fine-grained understanding of visual objects. We found that tokens receiving lower attention weights often hold essential information for identifying nuanced object details -- ranging from merely recognizing object existence to identifying their attributes (color, position, etc.) and understanding their relationships. To counteract the over-emphasis on blind tokens and to accurately respond to user queries, we introduce a technique called Attentional Vision Calibration (AVC). During the decoding phase, AVC identifies blind tokens by analyzing the image-related attention distribution. It then dynamically adjusts the logits for the next token prediction by contrasting the logits conditioned on the original visual tokens with those conditioned on the blind tokens. This effectively lowers the dependency on blind tokens and promotes a more balanced consideration of all tokens. We validate AVC on benchmarks such as POPE, MME, and AMBER, where it consistently outperforms existing decoding techniques in mitigating object hallucinations in LVLMs.","sentences":["This study addresses the issue observed in Large Vision Language Models (LVLMs), where excessive attention on a few image tokens, referred to as blind tokens, leads to hallucinatory responses in tasks requiring fine-grained understanding of visual objects.","We found that tokens receiving lower attention weights often hold essential information for identifying nuanced object details -- ranging from merely recognizing object existence to identifying their attributes (color, position, etc.)","and understanding their relationships.","To counteract the over-emphasis on blind tokens and to accurately respond to user queries, we introduce a technique called Attentional Vision Calibration (AVC).","During the decoding phase, AVC identifies blind tokens by analyzing the image-related attention distribution.","It then dynamically adjusts the logits for the next token prediction by contrasting the logits conditioned on the original visual tokens with those conditioned on the blind tokens.","This effectively lowers the dependency on blind tokens and promotes a more balanced consideration of all tokens.","We validate AVC on benchmarks such as POPE, MME, and AMBER, where it consistently outperforms existing decoding techniques in mitigating object hallucinations in LVLMs."],"url":"http://arxiv.org/abs/2405.17820v1","category":"cs.CV"}
{"created":"2024-05-28 04:18:00","title":"FAIntbench: A Holistic and Precise Benchmark for Bias Evaluation in Text-to-Image Models","abstract":"The rapid development and reduced barriers to entry for Text-to-Image (T2I) models have raised concerns about the biases in their outputs, but existing research lacks a holistic definition and evaluation framework of biases, limiting the enhancement of debiasing techniques. To address this issue, we introduce FAIntbench, a holistic and precise benchmark for biases in T2I models. In contrast to existing benchmarks that evaluate bias in limited aspects, FAIntbench evaluate biases from four dimensions: manifestation of bias, visibility of bias, acquired attributes, and protected attributes. We applied FAIntbench to evaluate seven recent large-scale T2I models and conducted human evaluation, whose results demonstrated the effectiveness of FAIntbench in identifying various biases. Our study also revealed new research questions about biases, including the side-effect of distillation. The findings presented here are preliminary, highlighting the potential of FAIntbench to advance future research aimed at mitigating the biases in T2I models. Our benchmark is publicly available to ensure the reproducibility.","sentences":["The rapid development and reduced barriers to entry for Text-to-Image (T2I) models have raised concerns about the biases in their outputs, but existing research lacks a holistic definition and evaluation framework of biases, limiting the enhancement of debiasing techniques.","To address this issue, we introduce FAIntbench, a holistic and precise benchmark for biases in T2I models.","In contrast to existing benchmarks that evaluate bias in limited aspects, FAIntbench evaluate biases from four dimensions: manifestation of bias, visibility of bias, acquired attributes, and protected attributes.","We applied FAIntbench to evaluate seven recent large-scale T2I models and conducted human evaluation, whose results demonstrated the effectiveness of FAIntbench in identifying various biases.","Our study also revealed new research questions about biases, including the side-effect of distillation.","The findings presented here are preliminary, highlighting the potential of FAIntbench to advance future research aimed at mitigating the biases in T2I models.","Our benchmark is publicly available to ensure the reproducibility."],"url":"http://arxiv.org/abs/2405.17814v1","category":"cs.CV"}
{"created":"2024-05-28 04:11:37","title":"TransVIP: Speech to Speech Translation System with Voice and Isochrony Preservation","abstract":"There is a rising interest and trend in research towards directly translating speech from one language to another, known as end-to-end speech-to-speech translation. However, most end-to-end models struggle to outperform cascade models, i.e., a pipeline framework by concatenating speech recognition, machine translation and text-to-speech models. The primary challenges stem from the inherent complexities involved in direct translation tasks and the scarcity of data. In this study, we introduce a novel model framework TransVIP that leverages diverse datasets in a cascade fashion yet facilitates end-to-end inference through joint probability. Furthermore, we propose two separated encoders to preserve the speaker's voice characteristics and isochrony from the source speech during the translation process, making it highly suitable for scenarios such as video dubbing. Our experiments on the French-English language pair demonstrate that our model outperforms the current state-of-the-art speech-to-speech translation model.","sentences":["There is a rising interest and trend in research towards directly translating speech from one language to another, known as end-to-end speech-to-speech translation.","However, most end-to-end models struggle to outperform cascade models, i.e., a pipeline framework by concatenating speech recognition, machine translation and text-to-speech models.","The primary challenges stem from the inherent complexities involved in direct translation tasks and the scarcity of data.","In this study, we introduce a novel model framework TransVIP that leverages diverse datasets in a cascade fashion yet facilitates end-to-end inference through joint probability.","Furthermore, we propose two separated encoders to preserve the speaker's voice characteristics and isochrony from the source speech during the translation process, making it highly suitable for scenarios such as video dubbing.","Our experiments on the French-English language pair demonstrate that our model outperforms the current state-of-the-art speech-to-speech translation model."],"url":"http://arxiv.org/abs/2405.17809v1","category":"cs.CL"}
{"created":"2024-05-28 03:53:26","title":"Multi-level Interaction Modeling for Protein Mutational Effect Prediction","abstract":"Protein-protein interactions are central mediators in many biological processes. Accurately predicting the effects of mutations on interactions is crucial for guiding the modulation of these interactions, thereby playing a significant role in therapeutic development and drug discovery. Mutations generally affect interactions hierarchically across three levels: mutated residues exhibit different sidechain conformations, which lead to changes in the backbone conformation, eventually affecting the binding affinity between proteins. However, existing methods typically focus only on sidechain-level interaction modeling, resulting in suboptimal predictions. In this work, we propose a self-supervised multi-level pre-training framework, ProMIM, to fully capture all three levels of interactions with well-designed pretraining objectives. Experiments show ProMIM outperforms all the baselines on the standard benchmark, especially on mutations where significant changes in backbone conformations may occur. In addition, leading results from zero-shot evaluations for SARS-CoV-2 mutational effect prediction and antibody optimization underscore the potential of ProMIM as a powerful next-generation tool for developing novel therapeutic approaches and new drugs.","sentences":["Protein-protein interactions are central mediators in many biological processes.","Accurately predicting the effects of mutations on interactions is crucial for guiding the modulation of these interactions, thereby playing a significant role in therapeutic development and drug discovery.","Mutations generally affect interactions hierarchically across three levels: mutated residues exhibit different sidechain conformations, which lead to changes in the backbone conformation, eventually affecting the binding affinity between proteins.","However, existing methods typically focus only on sidechain-level interaction modeling, resulting in suboptimal predictions.","In this work, we propose a self-supervised multi-level pre-training framework, ProMIM, to fully capture all three levels of interactions with well-designed pretraining objectives.","Experiments show ProMIM outperforms all the baselines on the standard benchmark, especially on mutations where significant changes in backbone conformations may occur.","In addition, leading results from zero-shot evaluations for SARS-CoV-2 mutational effect prediction and antibody optimization underscore the potential of ProMIM as a powerful next-generation tool for developing novel therapeutic approaches and new drugs."],"url":"http://arxiv.org/abs/2405.17802v1","category":"cs.LG"}
{"created":"2024-05-28 03:35:46","title":"Instruct-ReID++: Towards Universal Purpose Instruction-Guided Person Re-identification","abstract":"Human intelligence can retrieve any person according to both visual and language descriptions. However, the current computer vision community studies specific person re-identification (ReID) tasks in different scenarios separately, which limits the applications in the real world. This paper strives to resolve this problem by proposing a novel instruct-ReID task that requires the model to retrieve images according to the given image or language instructions. Instruct-ReID is the first exploration of a general ReID setting, where existing 6 ReID tasks can be viewed as special cases by assigning different instructions. To facilitate research in this new instruct-ReID task, we propose a large-scale OmniReID++ benchmark equipped with diverse data and comprehensive evaluation methods e.g., task specific and task-free evaluation settings. In the task-specific evaluation setting, gallery sets are categorized according to specific ReID tasks. We propose a novel baseline model, IRM, with an adaptive triplet loss to handle various retrieval tasks within a unified framework. For task-free evaluation setting, where target person images are retrieved from task-agnostic gallery sets, we further propose a new method called IRM++ with novel memory bank-assisted learning. Extensive evaluations of IRM and IRM++ on OmniReID++ benchmark demonstrate the superiority of our proposed methods, achieving state-of-the-art performance on 10 test sets. The datasets, the model, and the code will be available at https://github.com/hwz-zju/Instruct-ReID","sentences":["Human intelligence can retrieve any person according to both visual and language descriptions.","However, the current computer vision community studies specific person re-identification (ReID) tasks in different scenarios separately, which limits the applications in the real world.","This paper strives to resolve this problem by proposing a novel instruct-ReID task that requires the model to retrieve images according to the given image or language instructions.","Instruct-ReID is the first exploration of a general ReID setting, where existing 6 ReID tasks can be viewed as special cases by assigning different instructions.","To facilitate research in this new instruct-ReID task, we propose a large-scale OmniReID++ benchmark equipped with diverse data and comprehensive evaluation methods e.g., task specific and task-free evaluation settings.","In the task-specific evaluation setting, gallery sets are categorized according to specific ReID tasks.","We propose a novel baseline model, IRM, with an adaptive triplet loss to handle various retrieval tasks within a unified framework.","For task-free evaluation setting, where target person images are retrieved from task-agnostic gallery sets, we further propose a new method called IRM++ with novel memory bank-assisted learning.","Extensive evaluations of IRM and IRM++ on OmniReID++ benchmark demonstrate the superiority of our proposed methods, achieving state-of-the-art performance on 10 test sets.","The datasets, the model, and the code will be available at https://github.com/hwz-zju/Instruct-ReID"],"url":"http://arxiv.org/abs/2405.17790v1","category":"cs.CV"}
{"created":"2024-05-28 03:28:00","title":"Adaptive Horizon Actor-Critic for Policy Learning in Contact-Rich Differentiable Simulation","abstract":"Model-Free Reinforcement Learning~(MFRL), leveraging the policy gradient theorem, has demonstrated considerable success in continuous control tasks. However, these approaches are plagued by high gradient variance due to zeroth-order gradient estimation, resulting in suboptimal policies. Conversely, First-Order Model-Based Reinforcement Learning~(FO-MBRL) methods, employing differentiable simulation, provide gradients with reduced variance but are susceptible to sampling error in scenarios involving stiff dynamics, such as physical contact. This paper investigates the source of this error and introduces Adaptive Horizon Actor-Critic (AHAC), an FO-MBRL algorithm that reduces gradient error by adapting the model-based horizon to avoid stiff dynamics. Empirical findings reveal that AHAC outperforms MFRL baselines, attaining 40\\% more reward across a set of locomotion tasks, and efficiently scaling to high-dimensional control environments with improved wall-clock-time efficiency.","sentences":["Model-Free Reinforcement Learning~(MFRL), leveraging the policy gradient theorem, has demonstrated considerable success in continuous control tasks.","However, these approaches are plagued by high gradient variance due to zeroth-order gradient estimation, resulting in suboptimal policies.","Conversely, First-Order Model-Based Reinforcement Learning~(FO-MBRL) methods, employing differentiable simulation, provide gradients with reduced variance but are susceptible to sampling error in scenarios involving stiff dynamics, such as physical contact.","This paper investigates the source of this error and introduces Adaptive Horizon Actor-Critic (AHAC), an FO-MBRL algorithm that reduces gradient error by adapting the model-based horizon to avoid stiff dynamics.","Empirical findings reveal that AHAC outperforms MFRL baselines, attaining 40\\% more reward across a set of locomotion tasks, and efficiently scaling to high-dimensional control environments with improved wall-clock-time efficiency."],"url":"http://arxiv.org/abs/2405.17784v1","category":"cs.LG"}
{"created":"2024-05-28 03:12:54","title":"RREH: Reconstruction Relations Embedded Hashing for Semi-Paired Cross-Modal Retrieval","abstract":"Known for efficient computation and easy storage, hashing has been extensively explored in cross-modal retrieval. The majority of current hashing models are predicated on the premise of a direct one-to-one mapping between data points. However, in real practice, data correspondence across modalities may be partially provided. In this research, we introduce an innovative unsupervised hashing technique designed for semi-paired cross-modal retrieval tasks, named Reconstruction Relations Embedded Hashing (RREH). RREH assumes that multi-modal data share a common subspace. For paired data, RREH explores the latent consistent information of heterogeneous modalities by seeking a shared representation. For unpaired data, to effectively capture the latent discriminative features, the high-order relationships between unpaired data and anchors are embedded into the latent subspace, which are computed by efficient linear reconstruction. The anchors are sampled from paired data, which improves the efficiency of hash learning. The RREH trains the underlying features and the binary encodings in a unified framework with high-order reconstruction relations preserved. With the well devised objective function and discrete optimization algorithm, RREH is designed to be scalable, making it suitable for large-scale datasets and facilitating efficient cross-modal retrieval. In the evaluation process, the proposed is tested with partially paired data to establish its superiority over several existing methods.","sentences":["Known for efficient computation and easy storage, hashing has been extensively explored in cross-modal retrieval.","The majority of current hashing models are predicated on the premise of a direct one-to-one mapping between data points.","However, in real practice, data correspondence across modalities may be partially provided.","In this research, we introduce an innovative unsupervised hashing technique designed for semi-paired cross-modal retrieval tasks, named Reconstruction Relations Embedded Hashing (RREH).","RREH assumes that multi-modal data share a common subspace.","For paired data, RREH explores the latent consistent information of heterogeneous modalities by seeking a shared representation.","For unpaired data, to effectively capture the latent discriminative features, the high-order relationships between unpaired data and anchors are embedded into the latent subspace, which are computed by efficient linear reconstruction.","The anchors are sampled from paired data, which improves the efficiency of hash learning.","The RREH trains the underlying features and the binary encodings in a unified framework with high-order reconstruction relations preserved.","With the well devised objective function and discrete optimization algorithm, RREH is designed to be scalable, making it suitable for large-scale datasets and facilitating efficient cross-modal retrieval.","In the evaluation process, the proposed is tested with partially paired data to establish its superiority over several existing methods."],"url":"http://arxiv.org/abs/2405.17777v1","category":"cs.IR"}
{"created":"2024-05-28 02:43:53","title":"SleepFM: Multi-modal Representation Learning for Sleep Across Brain Activity, ECG and Respiratory Signals","abstract":"Sleep is a complex physiological process evaluated through various modalities recording electrical brain, cardiac, and respiratory activities. We curate a large polysomnography dataset from over 14,000 participants comprising over 100,000 hours of multi-modal sleep recordings. Leveraging this extensive dataset, we developed SleepFM, the first multi-modal foundation model for sleep analysis. We show that a novel leave-one-out approach for contrastive learning significantly improves downstream task performance compared to representations from standard pairwise contrastive learning. A logistic regression model trained on SleepFM's learned embeddings outperforms an end-to-end trained convolutional neural network (CNN) on sleep stage classification (macro AUROC 0.88 vs 0.72 and macro AUPRC 0.72 vs 0.48) and sleep disordered breathing detection (AUROC 0.85 vs 0.69 and AUPRC 0.77 vs 0.61). Notably, the learned embeddings achieve 48% top-1 average accuracy in retrieving the corresponding recording clips of other modalities from 90,000 candidates. This work demonstrates the value of holistic multi-modal sleep modeling to fully capture the richness of sleep recordings. SleepFM is open source and available at https://github.com/rthapa84/sleepfm-codebase.","sentences":["Sleep is a complex physiological process evaluated through various modalities recording electrical brain, cardiac, and respiratory activities.","We curate a large polysomnography dataset from over 14,000 participants comprising over 100,000 hours of multi-modal sleep recordings.","Leveraging this extensive dataset, we developed SleepFM, the first multi-modal foundation model for sleep analysis.","We show that a novel leave-one-out approach for contrastive learning significantly improves downstream task performance compared to representations from standard pairwise contrastive learning.","A logistic regression model trained on SleepFM's learned embeddings outperforms an end-to-end trained convolutional neural network (CNN) on sleep stage classification (macro AUROC 0.88 vs 0.72 and macro AUPRC 0.72 vs 0.48) and sleep disordered breathing detection (AUROC 0.85 vs 0.69 and AUPRC 0.77 vs 0.61).","Notably, the learned embeddings achieve 48% top-1 average accuracy in retrieving the corresponding recording clips of other modalities from 90,000 candidates.","This work demonstrates the value of holistic multi-modal sleep modeling to fully capture the richness of sleep recordings.","SleepFM is open source and available at https://github.com/rthapa84/sleepfm-codebase."],"url":"http://arxiv.org/abs/2405.17766v1","category":"cs.LG"}
{"created":"2024-05-28 02:33:38","title":"On the Sequence Evaluation based on Stochastic Processes","abstract":"Modeling and analyzing long sequences of text is an essential task for Natural Language Processing. Success in capturing long text dynamics using neural language models will facilitate many downstream tasks such as coherence evaluation, text generation, machine translation and so on. This paper presents a novel approach to model sequences through a stochastic process. We introduce a likelihood-based training objective for the text encoder and design a more thorough measurement (score) for long text evaluation compared to the previous approach. The proposed training objective effectively preserves the sequence coherence, while the new score comprehensively captures both temporal and spatial dependencies. Theoretical properties of our new score show its advantages in sequence evaluation. Experimental results show superior performance in various sequence evaluation tasks, including global and local discrimination within and between documents of different lengths. We also demonstrate the encoder achieves competitive results on discriminating human and AI written text.","sentences":["Modeling and analyzing long sequences of text is an essential task for Natural Language Processing.","Success in capturing long text dynamics using neural language models will facilitate many downstream tasks such as coherence evaluation, text generation, machine translation and so on.","This paper presents a novel approach to model sequences through a stochastic process.","We introduce a likelihood-based training objective for the text encoder and design a more thorough measurement (score) for long text evaluation compared to the previous approach.","The proposed training objective effectively preserves the sequence coherence, while the new score comprehensively captures both temporal and spatial dependencies.","Theoretical properties of our new score show its advantages in sequence evaluation.","Experimental results show superior performance in various sequence evaluation tasks, including global and local discrimination within and between documents of different lengths.","We also demonstrate the encoder achieves competitive results on discriminating human and AI written text."],"url":"http://arxiv.org/abs/2405.17764v1","category":"cs.CL"}
{"created":"2024-05-28 02:12:35","title":"XL3M: A Training-free Framework for LLM Length Extension Based on Segment-wise Inference","abstract":"Length generalization failure problem, namely the large language model (LLM) fails to generalize to texts longer than its maximum training length, greatly restricts the application of LLM in the scenarios with streaming long inputs. To address this problem, the existing methods either require substantial costs or introduce precision loss. In this paper, we empirically find that the accuracy of the LLM's prediction is highly correlated to its certainty. Based on this, we propose an efficient training free framework, named XL3M (it means extra-long large language model), which enables the LLMs trained on short sequences to reason extremely long sequence without any further training or fine-tuning. Under the XL3M framework, the input context will be firstly decomposed into multiple short sub-contexts, where each sub-context contains an independent segment and a common ``question'' which is a few tokens from the end of the original context. Then XL3M gives a method to measure the relevance between each segment and the ``question'', and constructs a concise key context by splicing all the relevant segments in chronological order. The key context is further used instead of the original context to complete the inference task. Evaluations on comprehensive benchmarks show the superiority of XL3M. Using our framework, a Llama2-7B model is able to reason 20M long sequences on an 8-card Huawei Ascend 910B NPU machine with 64GB memory per card.","sentences":["Length generalization failure problem, namely the large language model (LLM) fails to generalize to texts longer than its maximum training length, greatly restricts the application of LLM in the scenarios with streaming long inputs.","To address this problem, the existing methods either require substantial costs or introduce precision loss.","In this paper, we empirically find that the accuracy of the LLM's prediction is highly correlated to its certainty.","Based on this, we propose an efficient training free framework, named XL3M (it means extra-long large language model), which enables the LLMs trained on short sequences to reason extremely long sequence without any further training or fine-tuning.","Under the XL3M framework, the input context will be firstly decomposed into multiple short sub-contexts, where each sub-context contains an independent segment and a common ``question'' which is a few tokens from the end of the original context.","Then XL3M gives a method to measure the relevance between each segment and the ``question'', and constructs a concise key context by splicing all the relevant segments in chronological order.","The key context is further used instead of the original context to complete the inference task.","Evaluations on comprehensive benchmarks show the superiority of XL3M. Using our framework, a Llama2-7B model is able to reason 20M long sequences on an 8-card Huawei Ascend 910B NPU machine with 64GB memory per card."],"url":"http://arxiv.org/abs/2405.17755v1","category":"cs.CL"}
{"created":"2024-05-28 02:07:54","title":"Isotope effects and Alfven eigenmode stability in JET H, D, T, DT, and He plasmas","abstract":"While much about Alfven eigenmode (AE) stability has been explored in previous and current tokamaks, open questions remain for future burning plasma experiments, especially regarding exact stability threshold conditions and related isotope effects; the latter, of course, requiring good knowledge of the plasma ion composition. In the JET tokamak, eight in-vessel antennas actively excite stable AEs, from which their frequencies, toroidal mode numbers, and net damping rates are assessed. The effective ion mass can also be inferred using measurements of the plasma density and magnetic geometry. Thousands of AE stability measurements have been collected by the Alfven Eigenmode Active Diagnostic in hundreds of JET plasmas during the recent Hydrogen, Deuterium, Tritium, DT, and Helium-4 campaigns. In this novel AE stability database, spanning all four main ion species, damping is observed to decrease with increasing Hydrogenic mass, but increase for Helium, a trend consistent with radiative damping as the dominant damping mechanism. These data are important for confident predictions of AE stability in both non-nuclear (H/He) and nuclear (D/T) operations in future devices. In particular, if radiative damping plays a significant role in overall stability, some AEs could be more easily destabilized in D/T plasmas than their H/He reference pulses, even before considering fast ion and alpha particle drive. Active MHD spectroscopy is also employed on select HD, HT, and DT plasmas to infer the effective ion mass, thereby closing the loop on isotope analysis and demonstrating a complementary method to typical diagnosis of the isotope ratio.","sentences":["While much about Alfven eigenmode (AE) stability has been explored in previous and current tokamaks, open questions remain for future burning plasma experiments, especially regarding exact stability threshold conditions and related isotope effects; the latter, of course, requiring good knowledge of the plasma ion composition.","In the JET tokamak, eight in-vessel antennas actively excite stable AEs, from which their frequencies, toroidal mode numbers, and net damping rates are assessed.","The effective ion mass can also be inferred using measurements of the plasma density and magnetic geometry.","Thousands of AE stability measurements have been collected by the Alfven Eigenmode Active Diagnostic in hundreds of JET plasmas during the recent Hydrogen, Deuterium, Tritium, DT, and Helium-4 campaigns.","In this novel AE stability database, spanning all four main ion species, damping is observed to decrease with increasing Hydrogenic mass, but increase for Helium, a trend consistent with radiative damping as the dominant damping mechanism.","These data are important for confident predictions of AE stability in both non-nuclear (H/He) and nuclear (D/T) operations in future devices.","In particular, if radiative damping plays a significant role in overall stability, some AEs could be more easily destabilized in D/T plasmas than their H/He reference pulses, even before considering fast ion and alpha particle drive.","Active MHD spectroscopy is also employed on select HD, HT, and DT plasmas to infer the effective ion mass, thereby closing the loop on isotope analysis and demonstrating a complementary method to typical diagnosis of the isotope ratio."],"url":"http://arxiv.org/abs/2405.17752v1","category":"physics.plasm-ph"}
{"created":"2024-05-28 02:05:39","title":"Magnitude-based Neuron Pruning for Backdoor Defens","abstract":"Deep Neural Networks (DNNs) are known to be vulnerable to backdoor attacks, posing concerning threats to their reliable deployment. Recent research reveals that backdoors can be erased from infected DNNs by pruning a specific group of neurons, while how to effectively identify and remove these backdoor-associated neurons remains an open challenge. In this paper, we investigate the correlation between backdoor behavior and neuron magnitude, and find that backdoor neurons deviate from the magnitude-saliency correlation of the model. The deviation inspires us to propose a Magnitude-based Neuron Pruning (MNP) method to detect and prune backdoor neurons. Specifically, MNP uses three magnitude-guided objective functions to manipulate the magnitude-saliency correlation of backdoor neurons, thus achieving the purpose of exposing backdoor behavior, eliminating backdoor neurons and preserving clean neurons, respectively. Experiments show our pruning strategy achieves state-of-the-art backdoor defense performance against a variety of backdoor attacks with a limited amount of clean data, demonstrating the crucial role of magnitude for guiding backdoor defenses.","sentences":["Deep Neural Networks (DNNs) are known to be vulnerable to backdoor attacks, posing concerning threats to their reliable deployment.","Recent research reveals that backdoors can be erased from infected DNNs by pruning a specific group of neurons, while how to effectively identify and remove these backdoor-associated neurons remains an open challenge.","In this paper, we investigate the correlation between backdoor behavior and neuron magnitude, and find that backdoor neurons deviate from the magnitude-saliency correlation of the model.","The deviation inspires us to propose a Magnitude-based Neuron Pruning (MNP) method to detect and prune backdoor neurons.","Specifically, MNP uses three magnitude-guided objective functions to manipulate the magnitude-saliency correlation of backdoor neurons, thus achieving the purpose of exposing backdoor behavior, eliminating backdoor neurons and preserving clean neurons, respectively.","Experiments show our pruning strategy achieves state-of-the-art backdoor defense performance against a variety of backdoor attacks with a limited amount of clean data, demonstrating the crucial role of magnitude for guiding backdoor defenses."],"url":"http://arxiv.org/abs/2405.17750v1","category":"cs.LG"}
{"created":"2024-05-28 01:59:06","title":"Rethinking Pruning for Backdoor Mitigation: An Optimization Perspective","abstract":"Deep Neural Networks (DNNs) are known to be vulnerable to backdoor attacks, posing concerning threats to their reliable deployment. Recent research reveals that backdoors can be erased from infected DNNs by pruning a specific group of neurons, while how to effectively identify and remove these backdoor-associated neurons remains an open challenge. Most of the existing defense methods rely on defined rules and focus on neuron's local properties, ignoring the exploration and optimization of pruning policies. To address this gap, we propose an Optimized Neuron Pruning (ONP) method combined with Graph Neural Network (GNN) and Reinforcement Learning (RL) to repair backdoor models. Specifically, ONP first models the target DNN as graphs based on neuron connectivity, and then uses GNN-based RL agents to learn graph embeddings and find a suitable pruning policy. To the best of our knowledge, this is the first attempt to employ GNN and RL for optimizing pruning policies in the field of backdoor defense. Experiments show, with a small amount of clean data, ONP can effectively prune the backdoor neurons implanted by a set of backdoor attacks at the cost of negligible performance degradation, achieving a new state-of-the-art performance for backdoor mitigation.","sentences":["Deep Neural Networks (DNNs) are known to be vulnerable to backdoor attacks, posing concerning threats to their reliable deployment.","Recent research reveals that backdoors can be erased from infected DNNs by pruning a specific group of neurons, while how to effectively identify and remove these backdoor-associated neurons remains an open challenge.","Most of the existing defense methods rely on defined rules and focus on neuron's local properties, ignoring the exploration and optimization of pruning policies.","To address this gap, we propose an Optimized Neuron Pruning (ONP) method combined with Graph Neural Network (GNN) and Reinforcement Learning (RL) to repair backdoor models.","Specifically, ONP first models the target DNN as graphs based on neuron connectivity, and then uses GNN-based RL agents to learn graph embeddings and find a suitable pruning policy.","To the best of our knowledge, this is the first attempt to employ GNN and RL for optimizing pruning policies in the field of backdoor defense.","Experiments show, with a small amount of clean data, ONP can effectively prune the backdoor neurons implanted by a set of backdoor attacks at the cost of negligible performance degradation, achieving a new state-of-the-art performance for backdoor mitigation."],"url":"http://arxiv.org/abs/2405.17746v1","category":"cs.LG"}
{"created":"2024-05-28 01:55:35","title":"ORLM: Training Large Language Models for Optimization Modeling","abstract":"Large Language Models (LLMs) have emerged as powerful tools for complex Operations Research (OR) in automating optimization modeling. However, current methodologies heavily rely on prompt engineering (e.g., multi-agent cooperation) with proprietary LLMs, raising data privacy concerns that could be prohibitive in industry applications. To tackle this issue, we propose training open-source LLMs for optimization modeling. We identify four critical requirements for the training dataset of OR LLMs, design and implement OR-Instruct, a semi-automated process for creating synthetic data tailored to specific requirements. We also introduce the IndustryOR benchmark, the first industrial benchmark for testing LLMs on solving real-world OR problems. We apply the data from OR-Instruct to various open-source LLMs of 7b size (termed as ORLMs), resulting in a significantly improved capability for optimization modeling. Our best-performing ORLM achieves state-of-the-art performance on the NL4OPT, MAMO, and IndustryOR benchmarks. Our code and data will be available at \\url{https://github.com/Cardinal-Operations/ORLM}.","sentences":["Large Language Models (LLMs) have emerged as powerful tools for complex Operations Research (OR) in automating optimization modeling.","However, current methodologies heavily rely on prompt engineering (e.g., multi-agent cooperation) with proprietary LLMs, raising data privacy concerns that could be prohibitive in industry applications.","To tackle this issue, we propose training open-source LLMs for optimization modeling.","We identify four critical requirements for the training dataset of OR LLMs, design and implement OR-Instruct, a semi-automated process for creating synthetic data tailored to specific requirements.","We also introduce the IndustryOR benchmark, the first industrial benchmark for testing LLMs on solving real-world OR problems.","We apply the data from OR-Instruct to various open-source LLMs of 7b size (termed as ORLMs), resulting in a significantly improved capability for optimization modeling.","Our best-performing ORLM achieves state-of-the-art performance on the NL4OPT, MAMO, and IndustryOR benchmarks.","Our code and data will be available at \\url{https://github.com/Cardinal-Operations/ORLM}."],"url":"http://arxiv.org/abs/2405.17743v1","category":"cs.CL"}
{"created":"2024-05-28 01:53:26","title":"LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design","abstract":"Recent literature has found that an effective method to customize or further improve large language models (LLMs) is to add dynamic adapters, such as low-rank adapters (LoRA) with Mixture-of-Experts (MoE) structures. Though such dynamic adapters incur modest computational complexity, they surprisingly lead to huge inference latency overhead, slowing down the decoding speed by 2.5+ times. In this paper, we analyze the fine-grained costs of the dynamic adapters and find that the fragmented CUDA kernel calls are the root cause. Therefore, we propose LoRA-Switch, a system-algorithm co-designed architecture for efficient dynamic adapters. Unlike most existing dynamic structures that adopt layer-wise or block-wise dynamic routing, LoRA-Switch introduces a token-wise routing mechanism. It switches the LoRA adapters and weights for each token and merges them into the backbone for inference. For efficiency, this switching is implemented with an optimized CUDA kernel, which fuses the merging operations for all LoRA adapters at once. Based on experiments with popular open-source LLMs on common benchmarks, our approach has demonstrated similar accuracy improvement as existing dynamic adapters, while reducing the decoding latency by more than 2.4 times.","sentences":["Recent literature has found that an effective method to customize or further improve large language models (LLMs) is to add dynamic adapters, such as low-rank adapters (LoRA) with Mixture-of-Experts (MoE) structures.","Though such dynamic adapters incur modest computational complexity, they surprisingly lead to huge inference latency overhead, slowing down the decoding speed by 2.5+ times.","In this paper, we analyze the fine-grained costs of the dynamic adapters and find that the fragmented CUDA kernel calls are the root cause.","Therefore, we propose LoRA-Switch, a system-algorithm co-designed architecture for efficient dynamic adapters.","Unlike most existing dynamic structures that adopt layer-wise or block-wise dynamic routing, LoRA-Switch introduces a token-wise routing mechanism.","It switches the LoRA adapters and weights for each token and merges them into the backbone for inference.","For efficiency, this switching is implemented with an optimized CUDA kernel, which fuses the merging operations for all LoRA adapters at once.","Based on experiments with popular open-source LLMs on common benchmarks, our approach has demonstrated similar accuracy improvement as existing dynamic adapters, while reducing the decoding latency by more than 2.4 times."],"url":"http://arxiv.org/abs/2405.17741v1","category":"cs.AI"}
{"created":"2024-05-28 01:48:28","title":"The Widening Gap: The Benefits and Harms of Generative AI for Novice Programmers","abstract":"Novice programmers often struggle through programming problem solving due to a lack of metacognitive awareness and strategies. Previous research has shown that novices can encounter multiple metacognitive difficulties while programming. Novices are typically unaware of how these difficulties are hindering their progress. Meanwhile, many novices are now programming with generative AI (GenAI), which can provide complete solutions to most introductory programming problems, code suggestions, hints for next steps when stuck, and explain cryptic error messages. Its impact on novice metacognition has only started to be explored. Here we replicate a previous study that examined novice programming problem solving behavior and extend it by incorporating GenAI tools. Through 21 lab sessions consisting of participant observation, interview, and eye tracking, we explore how novices are coding with GenAI tools. Although 20 of 21 students completed the assigned programming problem, our findings show an unfortunate divide in the use of GenAI tools between students who accelerated and students who struggled. Students who accelerated were able to use GenAI to create code they already intended to make and were able to ignore unhelpful or incorrect inline code suggestions. But for students who struggled, our findings indicate that previously known metacognitive difficulties persist, and that GenAI unfortunately can compound them and even introduce new metacognitive difficulties. Furthermore, struggling students often expressed cognitive dissonance about their problem solving ability, thought they performed better than they did, and finished with an illusion of competence. Based on our observations from both groups, we propose ways to scaffold the novice GenAI experience and make suggestions for future work.","sentences":["Novice programmers often struggle through programming problem solving due to a lack of metacognitive awareness and strategies.","Previous research has shown that novices can encounter multiple metacognitive difficulties while programming.","Novices are typically unaware of how these difficulties are hindering their progress.","Meanwhile, many novices are now programming with generative AI (GenAI), which can provide complete solutions to most introductory programming problems, code suggestions, hints for next steps when stuck, and explain cryptic error messages.","Its impact on novice metacognition has only started to be explored.","Here we replicate a previous study that examined novice programming problem solving behavior and extend it by incorporating GenAI tools.","Through 21 lab sessions consisting of participant observation, interview, and eye tracking, we explore how novices are coding with GenAI tools.","Although 20 of 21 students completed the assigned programming problem, our findings show an unfortunate divide in the use of GenAI tools between students who accelerated and students who struggled.","Students who accelerated were able to use GenAI to create code they already intended to make and were able to ignore unhelpful or incorrect inline code suggestions.","But for students who struggled, our findings indicate that previously known metacognitive difficulties persist, and that GenAI unfortunately can compound them and even introduce new metacognitive difficulties.","Furthermore, struggling students often expressed cognitive dissonance about their problem solving ability, thought they performed better than they did, and finished with an illusion of competence.","Based on our observations from both groups, we propose ways to scaffold the novice GenAI experience and make suggestions for future work."],"url":"http://arxiv.org/abs/2405.17739v1","category":"cs.AI"}
{"created":"2024-05-28 01:19:18","title":"Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB","abstract":"In the era of big data, conventional RDBMS models have become impractical for handling colossal workloads. Consequently, NoSQL databases have emerged as the preferred storage solutions for executing processing-intensive Online Analytical Processing (OLAP) tasks. Within the realm of NoSQL databases, various classifications exist based on their data storage mechanisms, making it challenging to select the most suitable one for a given OLAP workload. While each NoSQL database boasts distinct advantages, inherent scalability, adaptability to diverse data formats, and high data availability are universally recognized benefits crucial for managing OLAP workloads effectively. Existing research predominantly evaluates individual databases within custom data pipeline setups, lacking a standardized approach for comparative analysis across different databases to identify the optimal data pipeline for OLAP workloads. In this paper, we present our experimental insights into how various NoSQL databases handle OLAP workloads within a standardized data processing pipeline. Our experimental pipeline comprises Apache Spark for large-scale transformations, data cleansing, and schema normalization, diverse NoSQL databases as data stores, and a Business Intelligence tool for data analysis and visualization.","sentences":["In the era of big data, conventional RDBMS models have become impractical for handling colossal workloads.","Consequently, NoSQL databases have emerged as the preferred storage solutions for executing processing-intensive Online Analytical Processing (OLAP) tasks.","Within the realm of NoSQL databases, various classifications exist based on their data storage mechanisms, making it challenging to select the most suitable one for a given OLAP workload.","While each NoSQL database boasts distinct advantages, inherent scalability, adaptability to diverse data formats, and high data availability are universally recognized benefits crucial for managing OLAP workloads effectively.","Existing research predominantly evaluates individual databases within custom data pipeline setups, lacking a standardized approach for comparative analysis across different databases to identify the optimal data pipeline for OLAP workloads.","In this paper, we present our experimental insights into how various NoSQL databases handle OLAP workloads within a standardized data processing pipeline.","Our experimental pipeline comprises Apache Spark for large-scale transformations, data cleansing, and schema normalization, diverse NoSQL databases as data stores, and a Business Intelligence tool for data analysis and visualization."],"url":"http://arxiv.org/abs/2405.17731v1","category":"cs.DB"}
{"created":"2024-05-28 01:19:13","title":"MMPareto: Boosting Multimodal Learning with Innocent Unimodal Assistance","abstract":"Multimodal learning methods with targeted unimodal learning objectives have exhibited their superior efficacy in alleviating the imbalanced multimodal learning problem. However, in this paper, we identify the previously ignored gradient conflict between multimodal and unimodal learning objectives, potentially misleading the unimodal encoder optimization. To well diminish these conflicts, we observe the discrepancy between multimodal loss and unimodal loss, where both gradient magnitude and covariance of the easier-to-learn multimodal loss are smaller than the unimodal one. With this property, we analyze Pareto integration under our multimodal scenario and propose MMPareto algorithm, which could ensure a final gradient with direction that is common to all learning objectives and enhanced magnitude to improve generalization, providing innocent unimodal assistance. Finally, experiments across multiple types of modalities and frameworks with dense cross-modal interaction indicate our superior and extendable method performance. Our method is also expected to facilitate multi-task cases with a clear discrepancy in task difficulty, demonstrating its ideal scalability. The source code and dataset are available at https://github.com/GeWu-Lab/MMPareto_ICML2024.","sentences":["Multimodal learning methods with targeted unimodal learning objectives have exhibited their superior efficacy in alleviating the imbalanced multimodal learning problem.","However, in this paper, we identify the previously ignored gradient conflict between multimodal and unimodal learning objectives, potentially misleading the unimodal encoder optimization.","To well diminish these conflicts, we observe the discrepancy between multimodal loss and unimodal loss, where both gradient magnitude and covariance of the easier-to-learn multimodal loss are smaller than the unimodal one.","With this property, we analyze Pareto integration under our multimodal scenario and propose MMPareto algorithm, which could ensure a final gradient with direction that is common to all learning objectives and enhanced magnitude to improve generalization, providing innocent unimodal assistance.","Finally, experiments across multiple types of modalities and frameworks with dense cross-modal interaction indicate our superior and extendable method performance.","Our method is also expected to facilitate multi-task cases with a clear discrepancy in task difficulty, demonstrating its ideal scalability.","The source code and dataset are available at https://github.com/GeWu-Lab/MMPareto_ICML2024."],"url":"http://arxiv.org/abs/2405.17730v1","category":"cs.CV"}
{"created":"2024-05-28 01:07:06","title":"Facilitating Holistic Evaluations with LLMs: Insights from Scenario-Based Experiments","abstract":"Workshop courses designed to foster creativity are gaining popularity. However, achieving a holistic evaluation that accommodates diverse perspectives is challenging, even for experienced faculty teams. Adequate discussion is essential to integrate varied assessments, but faculty often lack the time for such deliberations. Deriving an average score without discussion undermines the purpose of a holistic evaluation. This paper explores the use of a Large Language Model (LLM) as a facilitator to integrate diverse faculty assessments. Scenario-based experiments were conducted to determine if the LLM could synthesize diverse evaluations and explain the underlying theories to faculty. The results were noteworthy, showing that the LLM effectively facilitated faculty discussions. Additionally, the LLM demonstrated the capability to generalize and create evaluation criteria from a single scenario based on its learned domain knowledge.","sentences":["Workshop courses designed to foster creativity are gaining popularity.","However, achieving a holistic evaluation that accommodates diverse perspectives is challenging, even for experienced faculty teams.","Adequate discussion is essential to integrate varied assessments, but faculty often lack the time for such deliberations.","Deriving an average score without discussion undermines the purpose of a holistic evaluation.","This paper explores the use of a Large Language Model (LLM) as a facilitator to integrate diverse faculty assessments.","Scenario-based experiments were conducted to determine if the LLM could synthesize diverse evaluations and explain the underlying theories to faculty.","The results were noteworthy, showing that the LLM effectively facilitated faculty discussions.","Additionally, the LLM demonstrated the capability to generalize and create evaluation criteria from a single scenario based on its learned domain knowledge."],"url":"http://arxiv.org/abs/2405.17728v1","category":"cs.CY"}
{"created":"2024-05-28 00:42:18","title":"ClavaDDPM: Multi-relational Data Synthesis with Cluster-guided Diffusion Models","abstract":"Recent research in tabular data synthesis has focused on single tables, whereas real-world applications often involve complex data with tens or hundreds of interconnected tables. Previous approaches to synthesizing multi-relational (multi-table) data fall short in two key aspects: scalability for larger datasets and capturing long-range dependencies, such as correlations between attributes spread across different tables. Inspired by the success of diffusion models in tabular data modeling, we introduce   $\\textbf{C}luster$ $\\textbf{La}tent$ $\\textbf{Va}riable$ $guided$ $\\textbf{D}enoising$ $\\textbf{D}iffusion$ $\\textbf{P}robabilistic$ $\\textbf{M}odels$ (ClavaDDPM). This novel approach leverages clustering labels as intermediaries to model relationships between tables, specifically focusing on foreign key constraints. ClavaDDPM leverages the robust generation capabilities of diffusion models while incorporating efficient algorithms to propagate the learned latent variables across tables. This enables ClavaDDPM to capture long-range dependencies effectively.   Extensive evaluations on multi-table datasets of varying sizes show that ClavaDDPM significantly outperforms existing methods for these long-range dependencies while remaining competitive on utility metrics for single-table data.","sentences":["Recent research in tabular data synthesis has focused on single tables, whereas real-world applications often involve complex data with tens or hundreds of interconnected tables.","Previous approaches to synthesizing multi-relational (multi-table) data fall short in two key aspects: scalability for larger datasets and capturing long-range dependencies, such as correlations between attributes spread across different tables.","Inspired by the success of diffusion models in tabular data modeling, we introduce   $\\textbf{C}luster$ $\\textbf{La}tent$ $\\textbf{Va}riable$ $guided$ $\\textbf{D}enoising$ $\\textbf{D}iffusion$ $\\textbf{P}robabilistic$ $\\textbf{M}odels$ (ClavaDDPM).","This novel approach leverages clustering labels as intermediaries to model relationships between tables, specifically focusing on foreign key constraints.","ClavaDDPM leverages the robust generation capabilities of diffusion models while incorporating efficient algorithms to propagate the learned latent variables across tables.","This enables ClavaDDPM to capture long-range dependencies effectively.   ","Extensive evaluations on multi-table datasets of varying sizes show that ClavaDDPM significantly outperforms existing methods for these long-range dependencies while remaining competitive on utility metrics for single-table data."],"url":"http://arxiv.org/abs/2405.17724v1","category":"cs.AI"}
{"created":"2024-05-28 00:36:25","title":"MindFormer: A Transformer Architecture for Multi-Subject Brain Decoding via fMRI","abstract":"Research efforts to understand neural signals have been ongoing for many years, with visual decoding from fMRI signals attracting considerable attention. Particularly, the advent of image diffusion models has advanced the reconstruction of images from fMRI data significantly. However, existing approaches often introduce inter- and intra- subject variations in the reconstructed images, which can compromise accuracy. To address current limitations in multi-subject brain decoding, we introduce a new Transformer architecture called MindFormer. This model is specifically designed to generate fMRI-conditioned feature vectors that can be used for conditioning Stable Diffusion model. More specifically, MindFormer incorporates two key innovations: 1) a novel training strategy based on the IP-Adapter to extract semantically meaningful features from fMRI signals, and 2) a subject specific token and linear layer that effectively capture individual differences in fMRI signals while synergistically combines multi subject fMRI data for training. Our experimental results demonstrate that Stable Diffusion, when integrated with MindFormer, produces semantically consistent images across different subjects. This capability significantly surpasses existing models in multi-subject brain decoding. Such advancements not only improve the accuracy of our reconstructions but also deepen our understanding of neural processing variations among individuals.","sentences":["Research efforts to understand neural signals have been ongoing for many years, with visual decoding from fMRI signals attracting considerable attention.","Particularly, the advent of image diffusion models has advanced the reconstruction of images from fMRI data significantly.","However, existing approaches often introduce inter- and intra- subject variations in the reconstructed images, which can compromise accuracy.","To address current limitations in multi-subject brain decoding, we introduce a new Transformer architecture called MindFormer.","This model is specifically designed to generate fMRI-conditioned feature vectors that can be used for conditioning Stable Diffusion model.","More specifically, MindFormer incorporates two key innovations: 1) a novel training strategy based on the IP-Adapter to extract semantically meaningful features from fMRI signals, and 2) a subject specific token and linear layer that effectively capture individual differences in fMRI signals while synergistically combines multi subject fMRI data for training.","Our experimental results demonstrate that Stable Diffusion, when integrated with MindFormer, produces semantically consistent images across different subjects.","This capability significantly surpasses existing models in multi-subject brain decoding.","Such advancements not only improve the accuracy of our reconstructions but also deepen our understanding of neural processing variations among individuals."],"url":"http://arxiv.org/abs/2405.17720v1","category":"cs.CV"}
{"created":"2024-05-28 00:08:46","title":"AI Alignment with Changing and Influenceable Reward Functions","abstract":"Existing AI alignment approaches assume that preferences are static, which is unrealistic: our preferences change, and may even be influenced by our interactions with AI systems themselves. To clarify the consequences of incorrectly assuming static preferences, we introduce Dynamic Reward Markov Decision Processes (DR-MDPs), which explicitly model preference changes and the AI's influence on them. We show that despite its convenience, the static-preference assumption may undermine the soundness of existing alignment techniques, leading them to implicitly reward AI systems for influencing user preferences in ways users may not truly want. We then explore potential solutions. First, we offer a unifying perspective on how an agent's optimization horizon may partially help reduce undesirable AI influence. Then, we formalize different notions of AI alignment that account for preference change from the outset. Comparing the strengths and limitations of 8 such notions of alignment, we find that they all either err towards causing undesirable AI influence, or are overly risk-averse, suggesting that a straightforward solution to the problems of changing preferences may not exist. As there is no avoiding grappling with changing preferences in real-world settings, this makes it all the more important to handle these issues with care, balancing risks and capabilities. We hope our work can provide conceptual clarity and constitute a first step towards AI alignment practices which explicitly account for (and contend with) the changing and influenceable nature of human preferences.","sentences":["Existing AI alignment approaches assume that preferences are static, which is unrealistic: our preferences change, and may even be influenced by our interactions with AI systems themselves.","To clarify the consequences of incorrectly assuming static preferences, we introduce Dynamic Reward Markov Decision Processes (DR-MDPs), which explicitly model preference changes and the AI's influence on them.","We show that despite its convenience, the static-preference assumption may undermine the soundness of existing alignment techniques, leading them to implicitly reward AI systems for influencing user preferences in ways users may not truly want.","We then explore potential solutions.","First, we offer a unifying perspective on how an agent's optimization horizon may partially help reduce undesirable AI influence.","Then, we formalize different notions of AI alignment that account for preference change from the outset.","Comparing the strengths and limitations of 8 such notions of alignment, we find that they all either err towards causing undesirable AI influence, or are overly risk-averse, suggesting that a straightforward solution to the problems of changing preferences may not exist.","As there is no avoiding grappling with changing preferences in real-world settings, this makes it all the more important to handle these issues with care, balancing risks and capabilities.","We hope our work can provide conceptual clarity and constitute a first step towards AI alignment practices which explicitly account for (and contend with) the changing and influenceable nature of human preferences."],"url":"http://arxiv.org/abs/2405.17713v1","category":"cs.AI"}
{"created":"2024-05-27 23:51:20","title":"OPERA: Automatic Offline Policy Evaluation with Re-weighted Aggregates of Multiple Estimators","abstract":"Offline policy evaluation (OPE) allows us to evaluate and estimate a new sequential decision-making policy's performance by leveraging historical interaction data collected from other policies. Evaluating a new policy online without a confident estimate of its performance can lead to costly, unsafe, or hazardous outcomes, especially in education and healthcare. Several OPE estimators have been proposed in the last decade, many of which have hyperparameters and require training. Unfortunately, choosing the best OPE algorithm for each task and domain is still unclear. In this paper, we propose a new algorithm that adaptively blends a set of OPE estimators given a dataset without relying on an explicit selection using a statistical procedure. We prove that our estimator is consistent and satisfies several desirable properties for policy evaluation. Additionally, we demonstrate that when compared to alternative approaches, our estimator can be used to select higher-performing policies in healthcare and robotics. Our work contributes to improving ease of use for a general-purpose, estimator-agnostic, off-policy evaluation framework for offline RL.","sentences":["Offline policy evaluation (OPE) allows us to evaluate and estimate a new sequential decision-making policy's performance by leveraging historical interaction data collected from other policies.","Evaluating a new policy online without a confident estimate of its performance can lead to costly, unsafe, or hazardous outcomes, especially in education and healthcare.","Several OPE estimators have been proposed in the last decade, many of which have hyperparameters and require training.","Unfortunately, choosing the best OPE algorithm for each task and domain is still unclear.","In this paper, we propose a new algorithm that adaptively blends a set of OPE estimators given a dataset without relying on an explicit selection using a statistical procedure.","We prove that our estimator is consistent and satisfies several desirable properties for policy evaluation.","Additionally, we demonstrate that when compared to alternative approaches, our estimator can be used to select higher-performing policies in healthcare and robotics.","Our work contributes to improving ease of use for a general-purpose, estimator-agnostic, off-policy evaluation framework for offline RL."],"url":"http://arxiv.org/abs/2405.17708v1","category":"cs.LG"}
{"created":"2024-05-27 23:39:17","title":"Video Enriched Retrieval Augmented Generation Using Aligned Video Captions","abstract":"In this work, we propose the use of \"aligned visual captions\" as a mechanism for integrating information contained within videos into retrieval augmented generation (RAG) based chat assistant systems. These captions are able to describe the visual and audio content of videos in a large corpus while having the advantage of being in a textual format that is both easy to reason about & incorporate into large language model (LLM) prompts, but also typically require less multimedia content to be inserted into the multimodal LLM context window, where typical configurations can aggressively fill up the context window by sampling video frames from the source video. Furthermore, visual captions can be adapted to specific use cases by prompting the original foundational model / captioner for particular visual details or fine tuning. In hopes of helping advancing progress in this area, we curate a dataset and describe automatic evaluation procedures on common RAG tasks.","sentences":["In this work, we propose the use of \"aligned visual captions\" as a mechanism for integrating information contained within videos into retrieval augmented generation (RAG) based chat assistant systems.","These captions are able to describe the visual and audio content of videos in a large corpus while having the advantage of being in a textual format that is both easy to reason about & incorporate into large language model (LLM) prompts, but also typically require less multimedia content to be inserted into the multimodal LLM context window, where typical configurations can aggressively fill up the context window by sampling video frames from the source video.","Furthermore, visual captions can be adapted to specific use cases by prompting the original foundational model / captioner for particular visual details or fine tuning.","In hopes of helping advancing progress in this area, we curate a dataset and describe automatic evaluation procedures on common RAG tasks."],"url":"http://arxiv.org/abs/2405.17706v1","category":"cs.AI"}
{"created":"2024-05-27 23:38:10","title":"DC-Gaussian: Improving 3D Gaussian Splatting for Reflective Dash Cam Videos","abstract":"We present DC-Gaussian, a new method for generating novel views from in-vehicle dash cam videos. While neural rendering techniques have made significant strides in driving scenarios, existing methods are primarily designed for videos collected by autonomous vehicles. However, these videos are limited in both quantity and diversity compared to dash cam videos, which are more widely used across various types of vehicles and capture a broader range of scenarios. Dash cam videos often suffer from severe obstructions such as reflections and occlusions on the windshields, which significantly impede the application of neural rendering techniques. To address this challenge, we develop DC-Gaussian based on the recent real-time neural rendering technique 3D Gaussian Splatting (3DGS). Our approach includes an adaptive image decomposition module to model reflections and occlusions in a unified manner. Additionally, we introduce illumination-aware obstruction modeling to manage reflections and occlusions under varying lighting conditions. Lastly, we employ a geometry-guided Gaussian enhancement strategy to improve rendering details by incorporating additional geometry priors. Experiments on self-captured and public dash cam videos show that our method not only achieves state-of-the-art performance in novel view synthesis, but also accurately reconstructing captured scenes getting rid of obstructions.","sentences":["We present DC-Gaussian, a new method for generating novel views from in-vehicle dash cam videos.","While neural rendering techniques have made significant strides in driving scenarios, existing methods are primarily designed for videos collected by autonomous vehicles.","However, these videos are limited in both quantity and diversity compared to dash cam videos, which are more widely used across various types of vehicles and capture a broader range of scenarios.","Dash cam videos often suffer from severe obstructions such as reflections and occlusions on the windshields, which significantly impede the application of neural rendering techniques.","To address this challenge, we develop DC-Gaussian based on the recent real-time neural rendering technique 3D Gaussian Splatting (3DGS).","Our approach includes an adaptive image decomposition module to model reflections and occlusions in a unified manner.","Additionally, we introduce illumination-aware obstruction modeling to manage reflections and occlusions under varying lighting conditions.","Lastly, we employ a geometry-guided Gaussian enhancement strategy to improve rendering details by incorporating additional geometry priors.","Experiments on self-captured and public dash cam videos show that our method not only achieves state-of-the-art performance in novel view synthesis, but also accurately reconstructing captured scenes getting rid of obstructions."],"url":"http://arxiv.org/abs/2405.17705v1","category":"cs.CV"}
{"created":"2024-05-27 23:09:37","title":"BaboonLand Dataset: Tracking Primates in the Wild and Automating Behaviour Recognition from Drone Videos","abstract":"Using drones to track multiple individuals simultaneously in their natural environment is a powerful approach for better understanding group primate behavior. Previous studies have demonstrated that it is possible to automate the classification of primate behavior from video data, but these studies have been carried out in captivity or from ground-based cameras. To understand group behavior and the self-organization of a collective, the whole troop needs to be seen at a scale where behavior can be seen in relation to the natural environment in which ecological decisions are made. This study presents a novel dataset from drone videos for baboon detection, tracking, and behavior recognition. The baboon detection dataset was created by manually annotating all baboons in drone videos with bounding boxes. A tiling method was subsequently applied to create a pyramid of images at various scales from the original 5.3K resolution images, resulting in approximately 30K images used for baboon detection. The tracking dataset is derived from the detection dataset, where all bounding boxes are assigned the same ID throughout the video. This process resulted in half an hour of very dense tracking data. The behavior recognition dataset was generated by converting tracks into mini-scenes, a video subregion centered on each animal; each mini-scene was manually annotated with 12 distinct behavior types, resulting in over 20 hours of data. Benchmark results show mean average precision (mAP) of 92.62\\% for the YOLOv8-X detection model, multiple object tracking precision (MOTA) of 63.81\\% for the BotSort tracking algorithm, and micro top-1 accuracy of 63.97\\% for the X3D behavior recognition model. Using deep learning to classify wildlife behavior from drone footage facilitates non-invasive insight into the collective behavior of an entire group.","sentences":["Using drones to track multiple individuals simultaneously in their natural environment is a powerful approach for better understanding group primate behavior.","Previous studies have demonstrated that it is possible to automate the classification of primate behavior from video data, but these studies have been carried out in captivity or from ground-based cameras.","To understand group behavior and the self-organization of a collective, the whole troop needs to be seen at a scale where behavior can be seen in relation to the natural environment in which ecological decisions are made.","This study presents a novel dataset from drone videos for baboon detection, tracking, and behavior recognition.","The baboon detection dataset was created by manually annotating all baboons in drone videos with bounding boxes.","A tiling method was subsequently applied to create a pyramid of images at various scales from the original 5.3K resolution images, resulting in approximately 30K images used for baboon detection.","The tracking dataset is derived from the detection dataset, where all bounding boxes are assigned the same ID throughout the video.","This process resulted in half an hour of very dense tracking data.","The behavior recognition dataset was generated by converting tracks into mini-scenes, a video subregion centered on each animal; each mini-scene was manually annotated with 12 distinct behavior types, resulting in over 20 hours of data.","Benchmark results show mean average precision (mAP) of 92.62\\% for the YOLOv8-X detection model, multiple object tracking precision (MOTA) of 63.81\\% for the BotSort tracking algorithm, and micro top-1 accuracy of 63.97\\% for the X3D behavior recognition model.","Using deep learning to classify wildlife behavior from drone footage facilitates non-invasive insight into the collective behavior of an entire group."],"url":"http://arxiv.org/abs/2405.17698v1","category":"cs.CV"}
{"created":"2024-05-27 23:02:28","title":"The Limit Space of Self-similar Groups and Schreier graphs","abstract":"The present paper investigates the limit $G$-space $\\mathcal{J}_{G}$ generated by the self-similar action of automatic groups on a regular rooted tree. The limit space $\\mathcal{J}_{G}$ is the Gromov-Hausdorff limit of the family of Schreier graphs $\\Gamma_{n}$; therefore, $\\mathcal{J}_{G}$ can be approximated by Schreier graphs on level $n$-th when $n$ tends to infinity. We propose a computer program whose code is written in Wolfram language computes the adjacency matrix of Schreier graph $\\Gamma_{n}$ at each specified level of the regular rooted tree. In this paper, the Schreier graphs corresponding to each automatic group is computed by applying the program to some collection of automata groups, including classic automatic groups.","sentences":["The present paper investigates the limit $G$-space $\\mathcal{J}_{G}$ generated by the self-similar action of automatic groups on a regular rooted tree.","The limit space $\\mathcal{J}_{G}$ is the Gromov-Hausdorff limit of the family of Schreier graphs $\\Gamma_{n}$; therefore, $\\mathcal{J}_{G}$ can be approximated by Schreier graphs on level $n$-th when $n$ tends to infinity.","We propose a computer program whose code is written in Wolfram language computes the adjacency matrix of Schreier graph $\\Gamma_{n}$ at each specified level of the regular rooted tree.","In this paper, the Schreier graphs corresponding to each automatic group is computed by applying the program to some collection of automata groups, including classic automatic groups."],"url":"http://arxiv.org/abs/2405.17695v1","category":"math.GR"}
{"created":"2024-05-27 22:52:23","title":"Ontology-Enhanced Decision-Making for Autonomous Agents in Dynamic and Partially Observable Environments","abstract":"Agents, whether software or hardware, perceive their environment through sensors and act using actuators, often operating in dynamic, partially observable settings. They face challenges like incomplete and noisy data, unforeseen situations, and the need to adapt goals in real-time. Traditional reasoning and ML methods, including Reinforcement Learning (RL), help but are limited by data needs, predefined goals, and extensive exploration periods. Ontologies offer a solution by integrating diverse information sources, enhancing decision-making in complex environments. This thesis introduces an ontology-enhanced decision-making model (OntoDeM) for autonomous agents. OntoDeM enriches agents' domain knowledge, allowing them to interpret unforeseen events, generate or adapt goals, and make better decisions. Key contributions include: 1. An ontology-based method to improve agents' real-time observations using prior knowledge. 2. The OntoDeM model for handling dynamic, unforeseen situations by evolving or generating new goals. 3. Implementation and evaluation in four real-world applications, demonstrating its effectiveness. Compared to traditional and advanced learning algorithms, OntoDeM shows superior performance in improving agents' observations and decision-making in dynamic, partially observable environments.","sentences":["Agents, whether software or hardware, perceive their environment through sensors and act using actuators, often operating in dynamic, partially observable settings.","They face challenges like incomplete and noisy data, unforeseen situations, and the need to adapt goals in real-time.","Traditional reasoning and ML methods, including Reinforcement Learning (RL), help but are limited by data needs, predefined goals, and extensive exploration periods.","Ontologies offer a solution by integrating diverse information sources, enhancing decision-making in complex environments.","This thesis introduces an ontology-enhanced decision-making model (OntoDeM) for autonomous agents.","OntoDeM enriches agents' domain knowledge, allowing them to interpret unforeseen events, generate or adapt goals, and make better decisions.","Key contributions include: 1.","An ontology-based method to improve agents' real-time observations using prior knowledge.","2.","The OntoDeM model for handling dynamic, unforeseen situations by evolving or generating new goals.","3. Implementation and evaluation in four real-world applications, demonstrating its effectiveness.","Compared to traditional and advanced learning algorithms, OntoDeM shows superior performance in improving agents' observations and decision-making in dynamic, partially observable environments."],"url":"http://arxiv.org/abs/2405.17691v1","category":"cs.AI"}
{"created":"2024-05-27 22:10:17","title":"TIMA: Text-Image Mutual Awareness for Balancing Zero-Shot Adversarial Robustness and Generalization Ability","abstract":"This work addresses the challenge of achieving zero-shot adversarial robustness while preserving zero-shot generalization in large-scale foundation models, with a focus on the popular Contrastive Language-Image Pre-training (CLIP). Although foundation models were reported to have exceptional zero-shot generalization, they are highly vulnerable to adversarial perturbations. Existing methods achieve a comparable good tradeoff between zero-shot adversarial robustness and generalization under small adversarial perturbations. However, they fail to achieve a good tradeoff under large adversarial perturbations. To this end, we propose a novel Text-Image Mutual Awareness (TIMA) method that strikes a balance between zero-shot adversarial robustness and generalization. More precisely, we propose an Image-Aware Text (IAT) tuning mechanism that increases the inter-class distance of text embeddings by incorporating the Minimum Hyperspherical Energy (MHE). Simultaneously, fixed pre-trained image embeddings are used as cross-modal auxiliary supervision to maintain the similarity between the MHE-tuned and original text embeddings by the knowledge distillation, preserving semantic information between different classes. Besides, we introduce a Text-Aware Image (TAI) tuning mechanism, which increases inter-class distance between image embeddings during the training stage by Text-distance based Adaptive Margin (TAM). Similarly, a knowledge distillation is utilized to retain the similarity between fine-tuned and pre-trained image embeddings. Extensive experimental results demonstrate the effectiveness of our approach, showing impressive zero-shot performance against a wide range of adversarial perturbations while preserving the zero-shot generalization capabilities of the original CLIP model.","sentences":["This work addresses the challenge of achieving zero-shot adversarial robustness while preserving zero-shot generalization in large-scale foundation models, with a focus on the popular Contrastive Language-Image Pre-training (CLIP).","Although foundation models were reported to have exceptional zero-shot generalization, they are highly vulnerable to adversarial perturbations.","Existing methods achieve a comparable good tradeoff between zero-shot adversarial robustness and generalization under small adversarial perturbations.","However, they fail to achieve a good tradeoff under large adversarial perturbations.","To this end, we propose a novel Text-Image Mutual Awareness (TIMA) method that strikes a balance between zero-shot adversarial robustness and generalization.","More precisely, we propose an Image-Aware Text (IAT) tuning mechanism that increases the inter-class distance of text embeddings by incorporating the Minimum Hyperspherical Energy (MHE).","Simultaneously, fixed pre-trained image embeddings are used as cross-modal auxiliary supervision to maintain the similarity between the MHE-tuned and original text embeddings by the knowledge distillation, preserving semantic information between different classes.","Besides, we introduce a Text-Aware Image (TAI) tuning mechanism, which increases inter-class distance between image embeddings during the training stage by Text-distance based Adaptive Margin (TAM).","Similarly, a knowledge distillation is utilized to retain the similarity between fine-tuned and pre-trained image embeddings.","Extensive experimental results demonstrate the effectiveness of our approach, showing impressive zero-shot performance against a wide range of adversarial perturbations while preserving the zero-shot generalization capabilities of the original CLIP model."],"url":"http://arxiv.org/abs/2405.17678v1","category":"cs.CV"}
{"created":"2024-05-27 22:03:26","title":"Utilising a Quantum Hybrid Solver for Bi-objective Quadratic Assignment Problems","abstract":"The intersection between quantum computing and optimisation has been an area of interest in recent years. There have been numerous studies exploring the application of quantum and quantum-hybrid solvers to various optimisation problems. This work explores scalarisation methods within the context of solving the bi-objective quadratic assignment problem using a quantum-hybrid solver. We show results that are consistent with previous research on a different Ising machine.","sentences":["The intersection between quantum computing and optimisation has been an area of interest in recent years.","There have been numerous studies exploring the application of quantum and quantum-hybrid solvers to various optimisation problems.","This work explores scalarisation methods within the context of solving the bi-objective quadratic assignment problem using a quantum-hybrid solver.","We show results that are consistent with previous research on a different Ising machine."],"url":"http://arxiv.org/abs/2405.17676v1","category":"quant-ph"}
{"created":"2024-05-27 21:49:57","title":"Exploring Loss Design Techniques For Decision Tree Robustness To Label Noise","abstract":"In the real world, data is often noisy, affecting not only the quality of features but also the accuracy of labels. Current research on mitigating label errors stems primarily from advances in deep learning, and a gap exists in exploring interpretable models, particularly those rooted in decision trees. In this study, we investigate whether ideas from deep learning loss design can be applied to improve the robustness of decision trees. In particular, we show that loss correction and symmetric losses, both standard approaches, are not effective. We argue that other directions need to be explored to improve the robustness of decision trees to label noise.","sentences":["In the real world, data is often noisy, affecting not only the quality of features but also the accuracy of labels.","Current research on mitigating label errors stems primarily from advances in deep learning, and a gap exists in exploring interpretable models, particularly those rooted in decision trees.","In this study, we investigate whether ideas from deep learning loss design can be applied to improve the robustness of decision trees.","In particular, we show that loss correction and symmetric losses, both standard approaches, are not effective.","We argue that other directions need to be explored to improve the robustness of decision trees to label noise."],"url":"http://arxiv.org/abs/2405.17672v1","category":"cs.LG"}
{"created":"2024-05-27 21:30:52","title":"Adaptive Device-Edge Collaboration on DNN Inference in AIoT: A Digital Twin-Assisted Approach","abstract":"Device-edge collaboration on deep neural network (DNN) inference is a promising approach to efficiently utilizing network resources for supporting artificial intelligence of things (AIoT) applications. In this paper, we propose a novel digital twin (DT)-assisted approach to device-edge collaboration on DNN inference that determines whether and when to stop local inference at a device and upload the intermediate results to complete the inference on an edge server. Instead of determining the collaboration for each DNN inference task only upon its generation, multi-step decision-making is performed during the on-device inference to adapt to the dynamic computing workload status at the device and the edge server. To enhance the adaptivity, a DT is constructed to evaluate all potential offloading decisions for each DNN inference task, which provides augmented training data for a machine learning-assisted decision-making algorithm. Then, another DT is constructed to estimate the inference status at the device to avoid frequently fetching the status information from the device, thus reducing the signaling overhead. We also derive necessary conditions for optimal offloading decisions to reduce the offloading decision space. Simulation results demon-strate the outstanding performance of our DT-assisted approach in terms of balancing the tradeoff among inference accuracy, delay, and energy consumption.","sentences":["Device-edge collaboration on deep neural network (DNN) inference is a promising approach to efficiently utilizing network resources for supporting artificial intelligence of things (AIoT) applications.","In this paper, we propose a novel digital twin (DT)-assisted approach to device-edge collaboration on DNN inference that determines whether and when to stop local inference at a device and upload the intermediate results to complete the inference on an edge server.","Instead of determining the collaboration for each DNN inference task only upon its generation, multi-step decision-making is performed during the on-device inference to adapt to the dynamic computing workload status at the device and the edge server.","To enhance the adaptivity, a DT is constructed to evaluate all potential offloading decisions for each DNN inference task, which provides augmented training data for a machine learning-assisted decision-making algorithm.","Then, another DT is constructed to estimate the inference status at the device to avoid frequently fetching the status information from the device, thus reducing the signaling overhead.","We also derive necessary conditions for optimal offloading decisions to reduce the offloading decision space.","Simulation results demon-strate the outstanding performance of our DT-assisted approach in terms of balancing the tradeoff among inference accuracy, delay, and energy consumption."],"url":"http://arxiv.org/abs/2405.17664v1","category":"cs.DC"}
{"created":"2024-05-27 20:59:47","title":"Robust Perception and Navigation of Autonomous Surface Vehicles in Challenging Environments","abstract":"Research on coastal regions traditionally involves methods like manual sampling, monitoring buoys, and remote sensing, but these methods face challenges in spatially and temporally diverse regions of interest. Autonomous surface vehicles (ASVs) with artificial intelligence (AI) are being explored, and recognized by the International Maritime Organization (IMO) as vital for future ecosystem understanding. However, there is not yet a mature technology for autonomous environmental monitoring due to typically complex coastal situations: (1) many static (e.g., buoy, dock) and dynamic (e.g., boats) obstacles not compliant with the rules of the road (COLREGs); (2) uncharted or uncertain information (e.g., non-updated nautical chart); and (3) high-cost ASVs not accessible to the community and citizen science while resulting in technology illiteracy. To address the above challenges, my research involves both system and algorithmic development: (1) a robotic boat system for stable and reliable in-water monitoring, (2) maritime perception to detect and track obstacles (such as buoys, and boats), and (3) navigational decision-making with multiple-obstacle avoidance and multi-objective optimization.","sentences":["Research on coastal regions traditionally involves methods like manual sampling, monitoring buoys, and remote sensing, but these methods face challenges in spatially and temporally diverse regions of interest.","Autonomous surface vehicles (ASVs) with artificial intelligence (AI) are being explored, and recognized by the International Maritime Organization (IMO) as vital for future ecosystem understanding.","However, there is not yet a mature technology for autonomous environmental monitoring due to typically complex coastal situations: (1) many static (e.g., buoy, dock) and dynamic (e.g., boats) obstacles not compliant with the rules of the road (COLREGs); (2) uncharted or uncertain information (e.g., non-updated nautical chart); and (3) high-cost ASVs not accessible to the community and citizen science while resulting in technology illiteracy.","To address the above challenges, my research involves both system and algorithmic development: (1) a robotic boat system for stable and reliable in-water monitoring, (2) maritime perception to detect and track obstacles (such as buoys, and boats), and (3) navigational decision-making with multiple-obstacle avoidance and multi-objective optimization."],"url":"http://arxiv.org/abs/2405.17657v1","category":"cs.RO"}
{"created":"2024-05-27 20:54:17","title":"Data-Driven Personalized Energy Consumption Range Estimation for Plug-in Hybrid Electric Vehicles in Urban Traffic","abstract":"In urban traffic environments, driver behaviors exhibit considerable diversity in vehicle operation, encompassing a range of acceleration and braking maneuvers as well as adherence to traffic regulations, such as speed limits. It is well-established that these intrinsic driving behaviors significantly influence vehicle energy consumption. Therefore, establishing a quantitative relationship between driver behavior and energy usage is essential for identifying energy-efficient driving practices and optimizing routes within urban traffic. This study introduces a data-driven approach to predict the equivalent fuel consumption of a plug-in hybrid electric vehicle (PHEV) based on an integrated model of driver behavior and vehicle energy consumption. Unlike traditional models that provide point predictions of fuel consumption, this approach uses Conformalized Quantile Regression (CQR) to offer prediction intervals that capture the variability and uncertainty in fuel consumption. These intervals reflect changes in fuel consumption, as well as variations in driver behavior, and vehicle and route conditions. To develop this model, driver-specific data were collected through a driver-in-the-loop simulator, which tested different human drivers responses. The CQR model was then trained and validated using the experimental data from the driver-in-the-loop simulator, augmented by the synthetic data generated from Monte Carlo simulations conducted using a calibrated microscopic driver behavior and vehicle energy model. The CQR model was evaluated by comparing its predictions of equivalent fuel consumption intervals with those of baseline prediction interval methods that rely solely on conformal prediction.","sentences":["In urban traffic environments, driver behaviors exhibit considerable diversity in vehicle operation, encompassing a range of acceleration and braking maneuvers as well as adherence to traffic regulations, such as speed limits.","It is well-established that these intrinsic driving behaviors significantly influence vehicle energy consumption.","Therefore, establishing a quantitative relationship between driver behavior and energy usage is essential for identifying energy-efficient driving practices and optimizing routes within urban traffic.","This study introduces a data-driven approach to predict the equivalent fuel consumption of a plug-in hybrid electric vehicle (PHEV) based on an integrated model of driver behavior and vehicle energy consumption.","Unlike traditional models that provide point predictions of fuel consumption, this approach uses Conformalized Quantile Regression (CQR) to offer prediction intervals that capture the variability and uncertainty in fuel consumption.","These intervals reflect changes in fuel consumption, as well as variations in driver behavior, and vehicle and route conditions.","To develop this model, driver-specific data were collected through a driver-in-the-loop simulator, which tested different human drivers responses.","The CQR model was then trained and validated using the experimental data from the driver-in-the-loop simulator, augmented by the synthetic data generated from Monte Carlo simulations conducted using a calibrated microscopic driver behavior and vehicle energy model.","The CQR model was evaluated by comparing its predictions of equivalent fuel consumption intervals with those of baseline prediction interval methods that rely solely on conformal prediction."],"url":"http://arxiv.org/abs/2405.17654v1","category":"eess.SY"}
{"created":"2024-05-27 20:32:09","title":"Unifying Perspectives: Plausible Counterfactual Explanations on Global, Group-wise, and Local Levels","abstract":"Growing regulatory and societal pressures demand increased transparency in AI, particularly in understanding the decisions made by complex machine learning models. Counterfactual Explanations (CFs) have emerged as a promising technique within Explainable AI (xAI), offering insights into individual model predictions. However, to understand the systemic biases and disparate impacts of AI models, it is crucial to move beyond local CFs and embrace global explanations, which offer a~holistic view across diverse scenarios and populations. Unfortunately, generating Global Counterfactual Explanations (GCEs) faces challenges in computational complexity, defining the scope of \"global,\" and ensuring the explanations are both globally representative and locally plausible. We introduce a novel unified approach for generating Local, Group-wise, and Global Counterfactual Explanations for differentiable classification models via gradient-based optimization to address these challenges. This framework aims to bridge the gap between individual and systemic insights, enabling a deeper understanding of model decisions and their potential impact on diverse populations. Our approach further innovates by incorporating a probabilistic plausibility criterion, enhancing actionability and trustworthiness. By offering a cohesive solution to the optimization and plausibility challenges in GCEs, our work significantly advances the interpretability and accountability of AI models, marking a step forward in the pursuit of transparent AI.","sentences":["Growing regulatory and societal pressures demand increased transparency in AI, particularly in understanding the decisions made by complex machine learning models.","Counterfactual Explanations (CFs) have emerged as a promising technique within Explainable AI (xAI), offering insights into individual model predictions.","However, to understand the systemic biases and disparate impacts of AI models, it is crucial to move beyond local CFs and embrace global explanations, which offer a~holistic view across diverse scenarios and populations.","Unfortunately, generating Global Counterfactual Explanations (GCEs) faces challenges in computational complexity, defining the scope of \"global,\" and ensuring the explanations are both globally representative and locally plausible.","We introduce a novel unified approach for generating Local, Group-wise, and Global Counterfactual Explanations for differentiable classification models via gradient-based optimization to address these challenges.","This framework aims to bridge the gap between individual and systemic insights, enabling a deeper understanding of model decisions and their potential impact on diverse populations.","Our approach further innovates by incorporating a probabilistic plausibility criterion, enhancing actionability and trustworthiness.","By offering a cohesive solution to the optimization and plausibility challenges in GCEs, our work significantly advances the interpretability and accountability of AI models, marking a step forward in the pursuit of transparent AI."],"url":"http://arxiv.org/abs/2405.17642v1","category":"cs.LG"}
{"created":"2024-05-27 20:24:03","title":"Probabilistically Plausible Counterfactual Explanations with Normalizing Flows","abstract":"We present PPCEF, a novel method for generating probabilistically plausible counterfactual explanations (CFs). PPCEF advances beyond existing methods by combining a probabilistic formulation that leverages the data distribution with the optimization of plausibility within a unified framework. Compared to reference approaches, our method enforces plausibility by directly optimizing the explicit density function without assuming a particular family of parametrized distributions. This ensures CFs are not only valid (i.e., achieve class change) but also align with the underlying data's probability density. For that purpose, our approach leverages normalizing flows as powerful density estimators to capture the complex high-dimensional data distribution. Furthermore, we introduce a novel loss that balances the trade-off between achieving class change and maintaining closeness to the original instance while also incorporating a probabilistic plausibility term. PPCEF's unconstrained formulation allows for efficient gradient-based optimization with batch processing, leading to orders of magnitude faster computation compared to prior methods. Moreover, the unconstrained formulation of PPCEF allows for the seamless integration of future constraints tailored to specific counterfactual properties. Finally, extensive evaluations demonstrate PPCEF's superiority in generating high-quality, probabilistically plausible counterfactual explanations in high-dimensional tabular settings. This makes PPCEF a powerful tool for not only interpreting complex machine learning models but also for improving fairness, accountability, and trust in AI systems.","sentences":["We present PPCEF, a novel method for generating probabilistically plausible counterfactual explanations (CFs).","PPCEF advances beyond existing methods by combining a probabilistic formulation that leverages the data distribution with the optimization of plausibility within a unified framework.","Compared to reference approaches, our method enforces plausibility by directly optimizing the explicit density function without assuming a particular family of parametrized distributions.","This ensures CFs are not only valid (i.e., achieve class change) but also align with the underlying data's probability density.","For that purpose, our approach leverages normalizing flows as powerful density estimators to capture the complex high-dimensional data distribution.","Furthermore, we introduce a novel loss that balances the trade-off between achieving class change and maintaining closeness to the original instance while also incorporating a probabilistic plausibility term.","PPCEF's unconstrained formulation allows for efficient gradient-based optimization with batch processing, leading to orders of magnitude faster computation compared to prior methods.","Moreover, the unconstrained formulation of PPCEF allows for the seamless integration of future constraints tailored to specific counterfactual properties.","Finally, extensive evaluations demonstrate PPCEF's superiority in generating high-quality, probabilistically plausible counterfactual explanations in high-dimensional tabular settings.","This makes PPCEF a powerful tool for not only interpreting complex machine learning models but also for improving fairness, accountability, and trust in AI systems."],"url":"http://arxiv.org/abs/2405.17640v1","category":"cs.LG"}
{"created":"2024-05-27 20:18:20","title":"The surprising efficiency of temporal difference learning for rare event prediction","abstract":"We quantify the efficiency of temporal difference (TD) learning over the direct, or Monte Carlo (MC), estimator for policy evaluation in reinforcement learning, with an emphasis on estimation of quantities related to rare events. Policy evaluation is complicated in the rare event setting by the long timescale of the event and by the need for \\emph{relative accuracy} in estimates of very small values. Specifically, we focus on least-squares TD (LSTD) prediction for finite state Markov chains, and show that LSTD can achieve relative accuracy far more efficiently than MC. We prove a central limit theorem for the LSTD estimator and upper bound the \\emph{relative asymptotic variance} by simple quantities characterizing the connectivity of states relative to the transition probabilities between them. Using this bound, we show that, even when both the timescale of the rare event and the relative accuracy of the MC estimator are exponentially large in the number of states, LSTD maintains a fixed level of relative accuracy with a total number of observed transitions of the Markov chain that is only \\emph{polynomially} large in the number of states.","sentences":["We quantify the efficiency of temporal difference (TD) learning over the direct, or Monte Carlo (MC), estimator for policy evaluation in reinforcement learning, with an emphasis on estimation of quantities related to rare events.","Policy evaluation is complicated in the rare event setting by the long timescale of the event and by the need for \\emph{relative accuracy} in estimates of very small values.","Specifically, we focus on least-squares TD (LSTD) prediction for finite state Markov chains, and show that LSTD can achieve relative accuracy far more efficiently than MC.","We prove a central limit theorem for the LSTD estimator and upper bound the \\emph{relative asymptotic variance} by simple quantities characterizing the connectivity of states relative to the transition probabilities between them.","Using this bound, we show that, even when both the timescale of the rare event and the relative accuracy of the MC estimator are exponentially large in the number of states, LSTD maintains a fixed level of relative accuracy with a total number of observed transitions of the Markov chain that is only \\emph{polynomially} large in the number of states."],"url":"http://arxiv.org/abs/2405.17638v1","category":"cs.LG"}
{"created":"2024-05-27 20:08:41","title":"The Economic Implications of Large Language Model Selection on Earnings and Return on Investment: A Decision Theoretic Model","abstract":"Selecting language models in business contexts requires a careful analysis of the final financial benefits of the investment. However, the emphasis of academia and industry analysis of LLM is solely on performance. This work introduces a framework to evaluate LLMs, focusing on the earnings and return on investment aspects that should be taken into account in business decision making. We use a decision-theoretic approach to compare the financial impact of different LLMs, considering variables such as the cost per token, the probability of success in the specific task, and the gain and losses associated with LLMs use. The study reveals how the superior accuracy of more expensive models can, under certain conditions, justify a greater investment through more significant earnings but not necessarily a larger RoI. This article provides a framework for companies looking to optimize their technology choices, ensuring that investment in cutting-edge technology aligns with strategic financial objectives. In addition, we discuss how changes in operational variables influence the economics of using LLMs, offering practical insights for enterprise settings, finding that the predicted gain and loss and the different probabilities of success and failure are the variables that most impact the sensitivity of the models.","sentences":["Selecting language models in business contexts requires a careful analysis of the final financial benefits of the investment.","However, the emphasis of academia and industry analysis of LLM is solely on performance.","This work introduces a framework to evaluate LLMs, focusing on the earnings and return on investment aspects that should be taken into account in business decision making.","We use a decision-theoretic approach to compare the financial impact of different LLMs, considering variables such as the cost per token, the probability of success in the specific task, and the gain and losses associated with LLMs use.","The study reveals how the superior accuracy of more expensive models can, under certain conditions, justify a greater investment through more significant earnings but not necessarily a larger RoI. This article provides a framework for companies looking to optimize their technology choices, ensuring that investment in cutting-edge technology aligns with strategic financial objectives.","In addition, we discuss how changes in operational variables influence the economics of using LLMs, offering practical insights for enterprise settings, finding that the predicted gain and loss and the different probabilities of success and failure are the variables that most impact the sensitivity of the models."],"url":"http://arxiv.org/abs/2405.17637v1","category":"cs.AI"}
{"created":"2024-05-27 20:00:38","title":"HEART-felt Narratives: Tracing Empathy and Narrative Style in Personal Stories with LLMs","abstract":"Empathy serves as a cornerstone in enabling prosocial behaviors, and can be evoked through sharing of personal experiences in stories. While empathy is influenced by narrative content, intuitively, people respond to the way a story is told as well, through narrative style. Yet the relationship between empathy and narrative style is not fully understood. In this work, we empirically examine and quantify this relationship between style and empathy using LLMs and large-scale crowdsourcing studies. We introduce a novel, theory-based taxonomy, HEART (Human Empathy and Narrative Taxonomy) that delineates elements of narrative style that can lead to empathy with the narrator of a story. We establish the performance of LLMs in extracting narrative elements from HEART, showing that prompting with our taxonomy leads to reasonable, human-level annotations beyond what prior lexicon-based methods can do. To show empirical use of our taxonomy, we collect a dataset of empathy judgments of stories via a large-scale crowdsourcing study with N=2,624 participants. We show that narrative elements extracted via LLMs, in particular, vividness of emotions and plot volume, can elucidate the pathways by which narrative style cultivates empathy towards personal stories. Our work suggests that such models can be used for narrative analyses that lead to human-centered social and behavioral insights.","sentences":["Empathy serves as a cornerstone in enabling prosocial behaviors, and can be evoked through sharing of personal experiences in stories.","While empathy is influenced by narrative content, intuitively, people respond to the way a story is told as well, through narrative style.","Yet the relationship between empathy and narrative style is not fully understood.","In this work, we empirically examine and quantify this relationship between style and empathy using LLMs and large-scale crowdsourcing studies.","We introduce a novel, theory-based taxonomy, HEART (Human Empathy and Narrative Taxonomy) that delineates elements of narrative style that can lead to empathy with the narrator of a story.","We establish the performance of LLMs in extracting narrative elements from HEART, showing that prompting with our taxonomy leads to reasonable, human-level annotations beyond what prior lexicon-based methods can do.","To show empirical use of our taxonomy, we collect a dataset of empathy judgments of stories via a large-scale crowdsourcing study with N=2,624 participants.","We show that narrative elements extracted via LLMs, in particular, vividness of emotions and plot volume, can elucidate the pathways by which narrative style cultivates empathy towards personal stories.","Our work suggests that such models can be used for narrative analyses that lead to human-centered social and behavioral insights."],"url":"http://arxiv.org/abs/2405.17633v1","category":"cs.CL"}
{"created":"2024-05-27 19:57:17","title":"BioDiscoveryAgent: An AI Agent for Designing Genetic Perturbation Experiments","abstract":"Agents based on large language models have shown great potential in accelerating scientific discovery by leveraging their rich background knowledge and reasoning capabilities. Here, we develop BioDiscoveryAgent, an agent that designs new experiments, reasons about their outcomes, and efficiently navigates the hypothesis space to reach desired solutions. We demonstrate our agent on the problem of designing genetic perturbation experiments, where the aim is to find a small subset out of many possible genes that, when perturbed, result in a specific phenotype (e.g., cell growth). Utilizing its biological knowledge, BioDiscoveryAgent can uniquely design new experiments without the need to train a machine learning model or explicitly design an acquisition function. Moreover, BioDiscoveryAgent achieves an average of 18% improvement in detecting desired phenotypes across five datasets, compared to existing Bayesian optimization baselines specifically trained for this task. Our evaluation includes one dataset that is unpublished, ensuring it is not part of the language model's training data. Additionally, BioDiscoveryAgent predicts gene combinations to perturb twice as accurately as a random baseline, a task so far not explored in the context of closed-loop experiment design. The agent also has access to tools for searching the biomedical literature, executing code to analyze biological datasets, and prompting another agent to critically evaluate its predictions. Overall, BioDiscoveryAgent is interpretable at every stage, representing an accessible new paradigm in the computational design of biological experiments with the potential to augment scientists' capabilities.","sentences":["Agents based on large language models have shown great potential in accelerating scientific discovery by leveraging their rich background knowledge and reasoning capabilities.","Here, we develop BioDiscoveryAgent, an agent that designs new experiments, reasons about their outcomes, and efficiently navigates the hypothesis space to reach desired solutions.","We demonstrate our agent on the problem of designing genetic perturbation experiments, where the aim is to find a small subset out of many possible genes that, when perturbed, result in a specific phenotype (e.g., cell growth).","Utilizing its biological knowledge, BioDiscoveryAgent can uniquely design new experiments without the need to train a machine learning model or explicitly design an acquisition function.","Moreover, BioDiscoveryAgent achieves an average of 18% improvement in detecting desired phenotypes across five datasets, compared to existing Bayesian optimization baselines specifically trained for this task.","Our evaluation includes one dataset that is unpublished, ensuring it is not part of the language model's training data.","Additionally, BioDiscoveryAgent predicts gene combinations to perturb twice as accurately as a random baseline, a task so far not explored in the context of closed-loop experiment design.","The agent also has access to tools for searching the biomedical literature, executing code to analyze biological datasets, and prompting another agent to critically evaluate its predictions.","Overall, BioDiscoveryAgent is interpretable at every stage, representing an accessible new paradigm in the computational design of biological experiments with the potential to augment scientists' capabilities."],"url":"http://arxiv.org/abs/2405.17631v1","category":"cs.AI"}
{"created":"2024-05-27 19:52:00","title":"Tensor Low-rank Approximation of Finite-horizon Value Functions","abstract":"The goal of reinforcement learning is estimating a policy that maps states to actions and maximizes the cumulative reward of a Markov Decision Process (MDP). This is oftentimes achieved by estimating first the optimal (reward) value function (VF) associated with each state-action pair. When the MDP has an infinite horizon, the optimal VFs and policies are stationary under mild conditions. However, in finite-horizon MDPs, the VFs (hence, the policies) vary with time. This poses a challenge since the number of VFs to estimate grows not only with the size of the state-action space but also with the time horizon. This paper proposes a non-parametric low-rank stochastic algorithm to approximate the VFs of finite-horizon MDPs. First, we represent the (unknown) VFs as a multi-dimensional array, or tensor, where time is one of the dimensions. Then, we use rewards sampled from the MDP to estimate the optimal VFs. More precisely, we use the (truncated) PARAFAC decomposition to design an online low-rank algorithm that recovers the entries of the tensor of VFs. The size of the low-rank PARAFAC model grows additively with respect to each of its dimensions, rendering our approach efficient, as demonstrated via numerical experiments.","sentences":["The goal of reinforcement learning is estimating a policy that maps states to actions and maximizes the cumulative reward of a Markov Decision Process (MDP).","This is oftentimes achieved by estimating first the optimal (reward) value function (VF) associated with each state-action pair.","When the MDP has an infinite horizon, the optimal VFs and policies are stationary under mild conditions.","However, in finite-horizon MDPs, the VFs (hence, the policies) vary with time.","This poses a challenge since the number of VFs to estimate grows not only with the size of the state-action space but also with the time horizon.","This paper proposes a non-parametric low-rank stochastic algorithm to approximate the VFs of finite-horizon MDPs.","First, we represent the (unknown) VFs as a multi-dimensional array, or tensor, where time is one of the dimensions.","Then, we use rewards sampled from the MDP to estimate the optimal VFs.","More precisely, we use the (truncated)","PARAFAC decomposition to design an online low-rank algorithm that recovers the entries of the tensor of VFs.","The size of the low-rank PARAFAC model grows additively with respect to each of its dimensions, rendering our approach efficient, as demonstrated via numerical experiments."],"url":"http://arxiv.org/abs/2405.17628v1","category":"cs.LG"}
{"created":"2024-05-27 19:49:08","title":"Matrix Low-Rank Approximation For Policy Gradient Methods","abstract":"Estimating a policy that maps states to actions is a central problem in reinforcement learning. Traditionally, policies are inferred from the so called value functions (VFs), but exact VF computation suffers from the curse of dimensionality. Policy gradient (PG) methods bypass this by learning directly a parametric stochastic policy. Typically, the parameters of the policy are estimated using neural networks (NNs) tuned via stochastic gradient descent. However, finding adequate NN architectures can be challenging, and convergence issues are common as well. In this paper, we put forth low-rank matrix-based models to estimate efficiently the parameters of PG algorithms. We collect the parameters of the stochastic policy into a matrix, and then, we leverage matrix-completion techniques to promote (enforce) low rank. We demonstrate via numerical studies how low-rank matrix-based policy models reduce the computational and sample complexities relative to NN models, while achieving a similar aggregated reward.","sentences":["Estimating a policy that maps states to actions is a central problem in reinforcement learning.","Traditionally, policies are inferred from the so called value functions (VFs), but exact VF computation suffers from the curse of dimensionality.","Policy gradient (PG) methods bypass this by learning directly a parametric stochastic policy.","Typically, the parameters of the policy are estimated using neural networks (NNs) tuned via stochastic gradient descent.","However, finding adequate NN architectures can be challenging, and convergence issues are common as well.","In this paper, we put forth low-rank matrix-based models to estimate efficiently the parameters of PG algorithms.","We collect the parameters of the stochastic policy into a matrix, and then, we leverage matrix-completion techniques to promote (enforce) low rank.","We demonstrate via numerical studies how low-rank matrix-based policy models reduce the computational and sample complexities relative to NN models, while achieving a similar aggregated reward."],"url":"http://arxiv.org/abs/2405.17626v1","category":"cs.LG"}
{"created":"2024-05-27 19:46:31","title":"Matrix Low-Rank Trust Region Policy Optimization","abstract":"Most methods in reinforcement learning use a Policy Gradient (PG) approach to learn a parametric stochastic policy that maps states to actions. The standard approach is to implement such a mapping via a neural network (NN) whose parameters are optimized using stochastic gradient descent. However, PG methods are prone to large policy updates that can render learning inefficient. Trust region algorithms, like Trust Region Policy Optimization (TRPO), constrain the policy update step, ensuring monotonic improvements. This paper introduces low-rank matrix-based models as an efficient alternative for estimating the parameters of TRPO algorithms. By gathering the stochastic policy's parameters into a matrix and applying matrix-completion techniques, we promote and enforce low rank. Our numerical studies demonstrate that low-rank matrix-based policy models effectively reduce both computational and sample complexities compared to NN models, while maintaining comparable aggregated rewards.","sentences":["Most methods in reinforcement learning use a Policy Gradient (PG) approach to learn a parametric stochastic policy that maps states to actions.","The standard approach is to implement such a mapping via a neural network (NN) whose parameters are optimized using stochastic gradient descent.","However, PG methods are prone to large policy updates that can render learning inefficient.","Trust region algorithms, like Trust Region Policy Optimization (TRPO), constrain the policy update step, ensuring monotonic improvements.","This paper introduces low-rank matrix-based models as an efficient alternative for estimating the parameters of TRPO algorithms.","By gathering the stochastic policy's parameters into a matrix and applying matrix-completion techniques, we promote and enforce low rank.","Our numerical studies demonstrate that low-rank matrix-based policy models effectively reduce both computational and sample complexities compared to NN models, while maintaining comparable aggregated rewards."],"url":"http://arxiv.org/abs/2405.17625v1","category":"cs.LG"}
{"created":"2024-05-27 19:28:33","title":"Symmetric Reinforcement Learning Loss for Robust Learning on Diverse Tasks and Model Scales","abstract":"Reinforcement learning (RL) training is inherently unstable due to factors such as moving targets and high gradient variance. Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF) can introduce additional difficulty. Differing preferences can complicate the alignment process, and prediction errors in a trained reward model can become more severe as the LLM generates unseen outputs. To enhance training robustness, RL has adopted techniques from supervised learning, such as ensembles and layer normalization. In this work, we improve the stability of RL training by adapting the reverse cross entropy (RCE) from supervised learning for noisy data to define a symmetric RL loss. We demonstrate performance improvements across various tasks and scales. We conduct experiments in discrete action tasks (Atari games) and continuous action space tasks (MuJoCo benchmark and Box2D) using Symmetric A2C (SA2C) and Symmetric PPO (SPPO), with and without added noise with especially notable performance in SPPO across different hyperparameters. Furthermore, we validate the benefits of the symmetric RL loss when using SPPO for large language models through improved performance in RLHF tasks, such as IMDB positive sentiment sentiment and TL;DR summarization tasks.","sentences":["Reinforcement learning (RL) training is inherently unstable due to factors such as moving targets and high gradient variance.","Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF) can introduce additional difficulty.","Differing preferences can complicate the alignment process, and prediction errors in a trained reward model can become more severe as the LLM generates unseen outputs.","To enhance training robustness, RL has adopted techniques from supervised learning, such as ensembles and layer normalization.","In this work, we improve the stability of RL training by adapting the reverse cross entropy (RCE) from supervised learning for noisy data to define a symmetric RL loss.","We demonstrate performance improvements across various tasks and scales.","We conduct experiments in discrete action tasks (Atari games) and continuous action space tasks (MuJoCo benchmark and Box2D) using Symmetric A2C (SA2C) and Symmetric PPO (SPPO), with and without added noise with especially notable performance in SPPO across different hyperparameters.","Furthermore, we validate the benefits of the symmetric RL loss when using SPPO for large language models through improved performance in RLHF tasks, such as IMDB positive sentiment sentiment and TL;DR summarization tasks."],"url":"http://arxiv.org/abs/2405.17618v1","category":"cs.LG"}
{"created":"2024-05-27 19:16:42","title":"Explainable machine learning multi-label classification of Spanish legal judgements","abstract":"Artificial Intelligence techniques such as Machine Learning (ML) have not been exploited to their maximum potential in the legal domain. This has been partially due to the insufficient explanations they provided about their decisions. Automatic expert systems with explanatory capabilities can be specially useful when legal practitioners search jurisprudence to gather contextual knowledge for their cases. Therefore, we propose a hybrid system that applies ML for multi-label classification of judgements (sentences) and visual and natural language descriptions for explanation purposes, boosted by Natural Language Processing techniques and deep legal reasoning to identify the entities, such as the parties, involved. We are not aware of any prior work on automatic multi-label classification of legal judgements also providing natural language explanations to the end-users with comparable overall quality. Our solution achieves over 85 % micro precision on a labelled data set annotated by legal experts. This endorses its interest to relieve human experts from monotonous labour-intensive legal classification tasks.","sentences":["Artificial Intelligence techniques such as Machine Learning (ML) have not been exploited to their maximum potential in the legal domain.","This has been partially due to the insufficient explanations they provided about their decisions.","Automatic expert systems with explanatory capabilities can be specially useful when legal practitioners search jurisprudence to gather contextual knowledge for their cases.","Therefore, we propose a hybrid system that applies ML for multi-label classification of judgements (sentences) and visual and natural language descriptions for explanation purposes, boosted by Natural Language Processing techniques and deep legal reasoning to identify the entities, such as the parties, involved.","We are not aware of any prior work on automatic multi-label classification of legal judgements also providing natural language explanations to the end-users with comparable overall quality.","Our solution achieves over 85 % micro precision on a labelled data set annotated by legal experts.","This endorses its interest to relieve human experts from monotonous labour-intensive legal classification tasks."],"url":"http://arxiv.org/abs/2405.17610v1","category":"cs.CL"}
{"created":"2024-05-27 19:12:53","title":"Advancing Cultural Inclusivity: Optimizing Embedding Spaces for Balanced Music Recommendations","abstract":"Popularity bias in music recommendation systems -- where artists and tracks with the highest listen counts are recommended more often -- can also propagate biases along demographic and cultural axes. In this work, we identify these biases in recommendations for artists from underrepresented cultural groups in prototype-based matrix factorization methods. Unlike traditional matrix factorization methods, prototype-based approaches are interpretable. This allows us to directly link the observed bias in recommendations for minority artists (the effect) to specific properties of the embedding space (the cause). We mitigate popularity bias in music recommendation through capturing both users' and songs' cultural nuances in the embedding space. To address these challenges while maintaining recommendation quality, we propose two novel enhancements to the embedding space: i) we propose an approach to filter-out the irrelevant prototypes used to represent each user and item to improve generalizability, and ii) we introduce regularization techniques to reinforce a more uniform distribution of prototypes within the embedding space. Our results demonstrate significant improvements in reducing popularity bias and enhancing demographic and cultural fairness in music recommendations while achieving competitive -- if not better -- overall performance.","sentences":["Popularity bias in music recommendation systems -- where artists and tracks with the highest listen counts are recommended more often -- can also propagate biases along demographic and cultural axes.","In this work, we identify these biases in recommendations for artists from underrepresented cultural groups in prototype-based matrix factorization methods.","Unlike traditional matrix factorization methods, prototype-based approaches are interpretable.","This allows us to directly link the observed bias in recommendations for minority artists (the effect) to specific properties of the embedding space (the cause).","We mitigate popularity bias in music recommendation through capturing both users' and songs' cultural nuances in the embedding space.","To address these challenges while maintaining recommendation quality, we propose two novel enhancements to the embedding space: i) we propose an approach to filter-out the irrelevant prototypes used to represent each user and item to improve generalizability, and ii) we introduce regularization techniques to reinforce a more uniform distribution of prototypes within the embedding space.","Our results demonstrate significant improvements in reducing popularity bias and enhancing demographic and cultural fairness in music recommendations while achieving competitive -- if not better -- overall performance."],"url":"http://arxiv.org/abs/2405.17607v1","category":"cs.IR"}
{"created":"2024-05-27 19:09:25","title":"Review of searches for vector-like quarks, vector-like leptons, and heavy neutral leptons in proton-proton collisions at $\\sqrt{s}$ = 13 TeV at the CMS experiment","abstract":"The LHC has provided an unprecedented amount of proton-proton collision data, bringing forth exciting opportunities to address fundamental open questions in particle physics. These questions can potentially be answered by performing searches for very rare processes predicted by models that attempt to extend the standard model of particle physics. The data collected by the CMS experiment in 2015-2018 at a center-of-mass energy of 13 TeV help to test the standard model at the highest precision ever and potentially discover new physics. An interesting opportunity is presented by the possibility of new fermions with masses ranging from the MeV to the TeV scale. Such new particles appear in many possible extensions of the standard model and are well motivated theoretically. They may explain the appearance of three generations of leptons and quarks, the mass hierarchy across the generations, and the nonzero neutrino masses. In this report, the status of searches targeting vector-like quarks, vector-like leptons, and heavy neutral leptons at the CMS experiment is discussed. A complete overview of final states is provided together with their complementarity and partial combination. The discovery potential for several of these searches at the High-Luminosity LHC is also discussed.","sentences":["The LHC has provided an unprecedented amount of proton-proton collision data, bringing forth exciting opportunities to address fundamental open questions in particle physics.","These questions can potentially be answered by performing searches for very rare processes predicted by models that attempt to extend the standard model of particle physics.","The data collected by the CMS experiment in 2015-2018 at a center-of-mass energy of 13 TeV help to test the standard model at the highest precision ever and potentially discover new physics.","An interesting opportunity is presented by the possibility of new fermions with masses ranging from the MeV to the TeV scale.","Such new particles appear in many possible extensions of the standard model and are well motivated theoretically.","They may explain the appearance of three generations of leptons and quarks, the mass hierarchy across the generations, and the nonzero neutrino masses.","In this report, the status of searches targeting vector-like quarks, vector-like leptons, and heavy neutral leptons at the CMS experiment is discussed.","A complete overview of final states is provided together with their complementarity and partial combination.","The discovery potential for several of these searches at the High-Luminosity LHC is also discussed."],"url":"http://arxiv.org/abs/2405.17605v1","category":"hep-ex"}
{"created":"2024-05-27 19:07:13","title":"LoRA-XS: Low-Rank Adaptation with Extremely Small Number of Parameters","abstract":"The recent trend in scaling language models has led to a growing demand for parameter-efficient tuning (PEFT) methods such as LoRA (Low-Rank Adaptation). LoRA consistently matches or surpasses the full fine-tuning baseline with fewer parameters. However, handling numerous task-specific or user-specific LoRA modules on top of a base model still presents significant storage challenges. To address this, we introduce LoRA-XS (Low-Rank Adaptation with eXtremely Small number of parameters), a novel approach leveraging Singular Value Decomposition (SVD) for parameter-efficient fine-tuning. LoRA-XS introduces a small r x r weight matrix between frozen LoRA matrices, which are constructed by SVD of the original weight matrix. Training only r x r weight matrices ensures independence from model dimensions, enabling more parameter-efficient fine-tuning, especially for larger models. LoRA-XS achieves a remarkable reduction of trainable parameters by over 100x in 7B models compared to LoRA. Our benchmarking across various scales, including GLUE, GSM8k, and MATH benchmarks, shows that our approach outperforms LoRA and recent state-of-the-art approaches like VeRA in terms of parameter efficiency while maintaining competitive performance.","sentences":["The recent trend in scaling language models has led to a growing demand for parameter-efficient tuning (PEFT) methods such as LoRA (Low-Rank Adaptation).","LoRA consistently matches or surpasses the full fine-tuning baseline with fewer parameters.","However, handling numerous task-specific or user-specific LoRA modules on top of a base model still presents significant storage challenges.","To address this, we introduce LoRA-XS (Low-Rank Adaptation with eXtremely Small number of parameters), a novel approach leveraging Singular Value Decomposition (SVD) for parameter-efficient fine-tuning.","LoRA-XS introduces a small r x r weight matrix between frozen LoRA matrices, which are constructed by SVD of the original weight matrix.","Training only r x r weight matrices ensures independence from model dimensions, enabling more parameter-efficient fine-tuning, especially for larger models.","LoRA-XS achieves a remarkable reduction of trainable parameters by over 100x in 7B models compared to LoRA.","Our benchmarking across various scales, including GLUE, GSM8k, and MATH benchmarks, shows that our approach outperforms LoRA and recent state-of-the-art approaches like VeRA in terms of parameter efficiency while maintaining competitive performance."],"url":"http://arxiv.org/abs/2405.17604v1","category":"cs.LG"}
{"created":"2024-05-27 19:00:57","title":"A Mobility Equity Metric for Multi-Modal Intelligent Transportation Systems","abstract":"In this paper, we introduce a metric to evaluate the equity in mobility and a routing framework to enhance the metric within multi-modal intelligent transportation systems. The mobility equity metric (MEM) simultaneously accounts for service accessibility and transportation costs to quantify the equity and fairness in a transportation network. Finally, we develop a system planner integrated with MEM that aims to distribute travel demand for the transportation network, resulting in a socially optimal mobility system. Our framework results in a transportation network that is efficient in terms of travel time, improves accessibility, and ensures equity in transportation.","sentences":["In this paper, we introduce a metric to evaluate the equity in mobility and a routing framework to enhance the metric within multi-modal intelligent transportation systems.","The mobility equity metric (MEM) simultaneously accounts for service accessibility and transportation costs to quantify the equity and fairness in a transportation network.","Finally, we develop a system planner integrated with MEM that aims to distribute travel demand for the transportation network, resulting in a socially optimal mobility system.","Our framework results in a transportation network that is efficient in terms of travel time, improves accessibility, and ensures equity in transportation."],"url":"http://arxiv.org/abs/2405.17599v1","category":"eess.SY"}
{"created":"2024-05-27 18:48:06","title":"Optimizing Layout of Recursive Datatypes with Marmoset","abstract":"While programmers know that the low-level memory representation of data structures can have significant effects on performance, compiler support to optimize the layout of those structures is an under-explored field. Prior work has optimized the layout of individual, non-recursive structures without considering how collections of those objects in linked or recursive data structures are laid out. This work introduces Marmoset, a compiler that optimizes the layouts of algebraic datatypes, with a special focus on producing highly optimized, packed data layouts where recursive structures can be traversed with minimal pointer chasing. Marmoset performs an analysis of how a recursive ADT is used across functions to choose a global layout that promotes simple, strided access for that ADT in memory. It does so by building and solving a constraint system to minimize an abstract cost model, yielding a predicted efficient layout for the ADT. Marmoset then builds on top of Gibbon, a prior compiler for packed, mostly-serial representations, to synthesize optimized ADTs. We show experimentally that Marmoset is able to choose optimal layouts across a series of microbenchmarks and case studies, outperforming both Gibbons baseline approach, as well as MLton, a Standard ML compiler that uses traditional pointer-heavy representations.","sentences":["While programmers know that the low-level memory representation of data structures can have significant effects on performance, compiler support to optimize the layout of those structures is an under-explored field.","Prior work has optimized the layout of individual, non-recursive structures without considering how collections of those objects in linked or recursive data structures are laid out.","This work introduces Marmoset, a compiler that optimizes the layouts of algebraic datatypes, with a special focus on producing highly optimized, packed data layouts where recursive structures can be traversed with minimal pointer chasing.","Marmoset performs an analysis of how a recursive ADT is used across functions to choose a global layout that promotes simple, strided access for that ADT in memory.","It does so by building and solving a constraint system to minimize an abstract cost model, yielding a predicted efficient layout for the ADT.","Marmoset then builds on top of Gibbon, a prior compiler for packed, mostly-serial representations, to synthesize optimized ADTs.","We show experimentally that Marmoset is able to choose optimal layouts across a series of microbenchmarks and case studies, outperforming both Gibbons baseline approach, as well as MLton, a Standard ML compiler that uses traditional pointer-heavy representations."],"url":"http://arxiv.org/abs/2405.17590v1","category":"cs.PL"}
{"created":"2024-05-27 18:43:42","title":"Two-dimensional hydrodynamic viscous electron flow in annular Corbino rings","abstract":"The concept of fluidic viscosity is ubiquitous in our everyday life and for it to arise the fluidic medium must necessarily form a continuum where macroscopic properties can emerge. While a powerful concept for tangible liquids, hydrodynamic manifestation of collective flow in electronic systems such as two-dimensional electron gases (2DEGs) has only been shown recently to occur in graphene and GaAs/AlGaAs. Here, we present nonlocal electronic transport measurements in concentric annular rings formed in high-mobility GaAs/AlGaAs 2DEGs and the resulting data strongly suggest that viscous hydrodynamic flow can occur far away from the source-drain current region. Our conclusion of viscous electronic transport is further corroborated by simulations of the Navier-Stokes equations that are found to be in agreement with our measurements below 1K temperature. Most importantly, our work emphasizes the key role played by viscosity via electron-electron (e-e) interaction when hydrodynamic transport is restricted radially, and for which a priori should not have played a major role.","sentences":["The concept of fluidic viscosity is ubiquitous in our everyday life and for it to arise the fluidic medium must necessarily form a continuum where macroscopic properties can emerge.","While a powerful concept for tangible liquids, hydrodynamic manifestation of collective flow in electronic systems such as two-dimensional electron gases (2DEGs) has only been shown recently to occur in graphene and GaAs/AlGaAs.","Here, we present nonlocal electronic transport measurements in concentric annular rings formed in high-mobility GaAs/AlGaAs 2DEGs and the resulting data strongly suggest that viscous hydrodynamic flow can occur far away from the source-drain current region.","Our conclusion of viscous electronic transport is further corroborated by simulations of the Navier-Stokes equations that are found to be in agreement with our measurements below 1K temperature.","Most importantly, our work emphasizes the key role played by viscosity via electron-electron (e-e) interaction when hydrodynamic transport is restricted radially, and for which a priori should not have played a major role."],"url":"http://arxiv.org/abs/2405.17588v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-27 18:32:36","title":"Building a temperature forecasting model for the city with the regression neural network (RNN)","abstract":"In recent years, a study by environmental organizations in the world and Vietnam shows that weather change is quite complex. global warming has become a serious problem in the modern world, which is a concern for scientists. last century, it was difficult to forecast the weather due to missing weather monitoring stations and technological limitations. this made it hard to collect data for building predictive models to make accurate simulations. in Vietnam, research on weather forecast models is a recent development, having only begun around 2000. along with advancements in computer science, mathematical models are being built and applied with machine learning techniques to create more accurate and reliable predictive models. this article will summarize the research and solutions for applying recurrent neural networks to forecast urban temperatures.","sentences":["In recent years, a study by environmental organizations in the world and Vietnam shows that weather change is quite complex.","global warming has become a serious problem in the modern world, which is a concern for scientists.","last century, it was difficult to forecast the weather due to missing weather monitoring stations and technological limitations.","this made it hard to collect data for building predictive models to make accurate simulations.","in Vietnam, research on weather forecast models is a recent development, having only begun around 2000.","along with advancements in computer science, mathematical models are being built and applied with machine learning techniques to create more accurate and reliable predictive models.","this article will summarize the research and solutions for applying recurrent neural networks to forecast urban temperatures."],"url":"http://arxiv.org/abs/2405.17582v1","category":"cs.LG"}
{"created":"2024-05-28 17:59:51","title":"On the Origin of Llamas: Model Tree Heritage Recovery","abstract":"The rapid growth of neural network models shared on the internet has made model weights an important data modality. However, this information is underutilized as the weights are uninterpretable, and publicly available models are disorganized. Inspired by Darwin's tree of life, we define the Model Tree which describes the origin of models i.e., the parent model that was used to fine-tune the target model. Similarly to the natural world, the tree structure is unknown. In this paper, we introduce the task of Model Tree Heritage Recovery (MoTHer Recovery) for discovering Model Trees in the ever-growing universe of neural networks. Our hypothesis is that model weights encode this information, the challenge is to decode the underlying tree structure given the weights. Beyond the immediate application of model authorship attribution, MoTHer recovery holds exciting long-term applications akin to indexing the internet by search engines. Practically, for each pair of models, this task requires: i) determining if they are related, and ii) establishing the direction of the relationship. We find that certain distributional properties of the weights evolve monotonically during training, which enables us to classify the relationship between two given models. MoTHer recovery reconstructs entire model hierarchies, represented by a directed tree, where a parent model gives rise to multiple child models through additional training. Our approach successfully reconstructs complex Model Trees, as well as the structure of \"in-the-wild\" model families such as Llama 2 and Stable Diffusion.","sentences":["The rapid growth of neural network models shared on the internet has made model weights an important data modality.","However, this information is underutilized as the weights are uninterpretable, and publicly available models are disorganized.","Inspired by Darwin's tree of life, we define the Model Tree which describes the origin of models i.e., the parent model that was used to fine-tune the target model.","Similarly to the natural world, the tree structure is unknown.","In this paper, we introduce the task of Model Tree Heritage Recovery (MoTHer Recovery) for discovering Model Trees in the ever-growing universe of neural networks.","Our hypothesis is that model weights encode this information, the challenge is to decode the underlying tree structure given the weights.","Beyond the immediate application of model authorship attribution, MoTHer recovery holds exciting long-term applications akin to indexing the internet by search engines.","Practically, for each pair of models, this task requires: i) determining if they are related, and ii) establishing the direction of the relationship.","We find that certain distributional properties of the weights evolve monotonically during training, which enables us to classify the relationship between two given models.","MoTHer recovery reconstructs entire model hierarchies, represented by a directed tree, where a parent model gives rise to multiple child models through additional training.","Our approach successfully reconstructs complex Model Trees, as well as the structure of \"in-the-wild\" model families such as Llama 2 and Stable Diffusion."],"url":"http://arxiv.org/abs/2405.18432v1","category":"cs.LG"}
{"created":"2024-05-28 17:58:52","title":"JWST-TST High Contrast: JWST/NIRCam observations of the young giant planet $\u03b2$ Pic b","abstract":"We present the first JWST/NIRCam observations of the directly-imaged gas giant exoplanet $\\beta$ Pic b. Observations in six filters using NIRCam's round coronagraphic masks provide a high signal-to-noise detection of $\\beta$ Pic b and the archetypal debris disk around $\\beta$ Pic over a wavelength range of $\\sim$1.7-5 $\\mu$m. This paper focuses on the detection of $\\beta$ Pic b and other potential point sources in the NIRCam data, following a paper by Rebollido et al. which presented the NIRCam and MIRI view of the debris disk around $\\beta$ Pic. We develop and validate approaches for obtaining accurate photometry of planets in the presence of bright, complex circumstellar backgrounds. By simultaneously fitting the planet's PSF and a geometric model for the disk, we obtain planet photometry that is in good agreement with previous measurements from the ground. The NIRCam data supports the cloudy nature of $\\beta$ Pic b's atmosphere and the discrepancy between its mass as inferred from evolutionary models and the dynamical mass reported in the literature. We further identify five additional localized sources in the data, but all of them are found to be background stars or galaxies based on their color or spatial extent. We can rule out additional planets in the disk midplane above 1 Jupiter mass outward of 2 arcsec ($\\sim$40 au) and away from the disk midplane above 0.05 Jupiter masses outward of 4 arcsec ($\\sim$80 au). The inner giant planet $\\beta$ Pic c remains undetected behind the coronagraphic masks of NIRCam in our observations.","sentences":["We present the first JWST/NIRCam observations of the directly-imaged gas giant exoplanet $\\beta$ Pic b. Observations in six filters using NIRCam's round coronagraphic masks provide a high signal-to-noise detection of $\\beta$ Pic b and the archetypal debris disk around $\\beta$ Pic over a wavelength range of $\\sim$1.7-5 $\\mu$m.","This paper focuses on the detection of $\\beta$ Pic b and other potential point sources in the NIRCam data, following a paper by Rebollido et al. which presented the NIRCam and MIRI view of the debris disk around $\\beta$ Pic.","We develop and validate approaches for obtaining accurate photometry of planets in the presence of bright, complex circumstellar backgrounds.","By simultaneously fitting the planet's PSF and a geometric model for the disk, we obtain planet photometry that is in good agreement with previous measurements from the ground.","The NIRCam data supports the cloudy nature of $\\beta$ Pic b's atmosphere and the discrepancy between its mass as inferred from evolutionary models and the dynamical mass reported in the literature.","We further identify five additional localized sources in the data, but all of them are found to be background stars or galaxies based on their color or spatial extent.","We can rule out additional planets in the disk midplane above 1 Jupiter mass outward of 2 arcsec ($\\sim$40 au) and away from the disk midplane above 0.05 Jupiter masses outward of 4 arcsec ($\\sim$80 au).","The inner giant planet $\\beta$ Pic c remains undetected behind the coronagraphic masks of NIRCam in our observations."],"url":"http://arxiv.org/abs/2405.18422v1","category":"astro-ph.EP"}
{"created":"2024-05-28 17:54:03","title":"Tensor Methods in High Dimensional Data Analysis: Opportunities and Challenges","abstract":"Large amount of multidimensional data represented by multiway arrays or tensors are prevalent in modern applications across various fields such as chemometrics, genomics, physics, psychology, and signal processing. The structural complexity of such data provides vast new opportunities for modeling and analysis, but efficiently extracting information content from them, both statistically and computationally, presents unique and fundamental challenges. Addressing these challenges requires an interdisciplinary approach that brings together tools and insights from statistics, optimization and numerical linear algebra among other fields. Despite these hurdles, significant progress has been made in the last decade. This review seeks to examine some of the key advancements and identify common threads among them, under eight different statistical settings.","sentences":["Large amount of multidimensional data represented by multiway arrays or tensors are prevalent in modern applications across various fields such as chemometrics, genomics, physics, psychology, and signal processing.","The structural complexity of such data provides vast new opportunities for modeling and analysis, but efficiently extracting information content from them, both statistically and computationally, presents unique and fundamental challenges.","Addressing these challenges requires an interdisciplinary approach that brings together tools and insights from statistics, optimization and numerical linear algebra among other fields.","Despite these hurdles, significant progress has been made in the last decade.","This review seeks to examine some of the key advancements and identify common threads among them, under eight different statistical settings."],"url":"http://arxiv.org/abs/2405.18412v1","category":"math.ST"}
{"created":"2024-05-28 17:54:00","title":"The adhesive contact problem for a piecewise-homogeneous orthotropic plate with an elastic patch","abstract":"A piecewise-homogeneous elastic orthotropic plate, reinforced with a finite patch of the wedgeshaped, which meets the interface at a right angle and is loaded with tangential and normal forces is considered. By using methods of the theory of analytic functions, the problem is reduced to the system of singular integro-differential equations (SIDE) with fixed singularity. Under tension-compression of patch by using an integral transformation a Riemann problem is obtained, the solution of which is presented in explicit form. The tangential contact stresses along the contact line are determined and their asymptotic behavior in the neighborhood of singular points is established.","sentences":["A piecewise-homogeneous elastic orthotropic plate, reinforced with a finite patch of the wedgeshaped, which meets the interface at a right angle and is loaded with tangential and normal forces is considered.","By using methods of the theory of analytic functions, the problem is reduced to the system of singular integro-differential equations (SIDE) with fixed singularity.","Under tension-compression of patch by using an integral transformation a Riemann problem is obtained, the solution of which is presented in explicit form.","The tangential contact stresses along the contact line are determined and their asymptotic behavior in the neighborhood of singular points is established."],"url":"http://arxiv.org/abs/2405.18411v1","category":"math-ph"}
{"created":"2024-05-28 17:44:02","title":"Antigenic Cooperation in Viral Populations: Redistribution of Loads Among Altruistic Viruses and Maximal Load per Altruist","abstract":"The paper continues the study of the phenomenon of local immunodeficiency (LI) in viral cross-immunoreactivity networks, with a focus on the roles and interactions between altruistic and persistent viral variants. As always, only the state of stable (i.e. observable) LI is analysed. First, we show that a single altruistic viral variant has an upper limit for the number of persistent viral variants that it can support. Our findings reveal that in viral cross-immunoreactivity networks, altruistic viruses act essentially autonomously from each other. Namely, connections between altruistic viruses do not change neither their qualitative roles, nor the quantitative values of the strengths of their connections in the CRNs. In other words, each altruistic virus does exactly the same actions and with the same strengths with or without presence of other altruistic viruses. However, having more altruistic viruses allows to keep sizes of populations of persistent viruses at the higher levels. Likewise, the strength of the immune response against any altruistic virus remains at the same constant level regardless of how many persistent viruses this altruistic virus supports, i.e. shields from the immune response of the host's immune system. It is also shown that viruses strongly compete with each other in order to become persistent in the state of stable LI. We also present an example for a CRN with stable LI that only consists of persistent viral variants.","sentences":["The paper continues the study of the phenomenon of local immunodeficiency (LI) in viral cross-immunoreactivity networks, with a focus on the roles and interactions between altruistic and persistent viral variants.","As always, only the state of stable (i.e. observable) LI is analysed.","First, we show that a single altruistic viral variant has an upper limit for the number of persistent viral variants that it can support.","Our findings reveal that in viral cross-immunoreactivity networks, altruistic viruses act essentially autonomously from each other.","Namely, connections between altruistic viruses do not change neither their qualitative roles, nor the quantitative values of the strengths of their connections in the CRNs.","In other words, each altruistic virus does exactly the same actions and with the same strengths with or without presence of other altruistic viruses.","However, having more altruistic viruses allows to keep sizes of populations of persistent viruses at the higher levels.","Likewise, the strength of the immune response against any altruistic virus remains at the same constant level regardless of how many persistent viruses this altruistic virus supports, i.e. shields from the immune response of the host's immune system.","It is also shown that viruses strongly compete with each other in order to become persistent in the state of stable LI.","We also present an example for a CRN with stable LI that only consists of persistent viral variants."],"url":"http://arxiv.org/abs/2405.18402v1","category":"q-bio.PE"}
{"created":"2024-05-28 17:39:37","title":"A simple, randomized algorithm for diagonalizing normal matrices","abstract":"We present and analyze a simple numerical method that diagonalizes a complex normal matrix A by diagonalizing the Hermitian matrix obtained from a random linear combination of the Hermitian and skew-Hermitian parts of A.","sentences":["We present and analyze a simple numerical method that diagonalizes a complex normal matrix A by diagonalizing the Hermitian matrix obtained from a random linear combination of the Hermitian and skew-Hermitian parts of A."],"url":"http://arxiv.org/abs/2405.18399v1","category":"math.NA"}
{"created":"2024-05-28 17:33:57","title":"A Critique of Snapshot Isolation","abstract":"The support for transactions is an essential part of a database management system (DBMS). Without this support, the developers are burdened with ensuring atomic execution of a transaction despite failures as well as concurrent accesses to the database by other transactions. Ideally, a transactional system provides serializability, which means that the outcome of concurrent transactions is equivalent to a serial execution of them. Based on experiences on lock-based implementations, nevertheless, serializability is known as an expensive feature that comes with high overhead and low concurrency. Commercial systems, hence, compromise serializability by implementing weaker guarantees such as snapshot isolation. The developers, therefore, are still burdened with the anomalies that could arise due to the lack of serializability.   There have been recent attempts to enrich large-scale data stores, such as HBase and BigTable, with transactional support. Not surprisingly, inspired by traditional database management systems, serializability is usually compromised for the benefit of efficiency. For example, Google Percolator, implements lock-based snapshot isolation on top of BigTable. We show in this paper that this compromise is not necessary in lock-free implementations of transactional support. We introduce write-snapshot isolation, a novel isolation level that has a performance comparable with that of snapshot isolation, and yet provides serializability. The main insight in write-snapshot isolation is to prevent read-write conflicts in contrast to write-write conflicts that are prevented by snapshot isolation.","sentences":["The support for transactions is an essential part of a database management system (DBMS).","Without this support, the developers are burdened with ensuring atomic execution of a transaction despite failures as well as concurrent accesses to the database by other transactions.","Ideally, a transactional system provides serializability, which means that the outcome of concurrent transactions is equivalent to a serial execution of them.","Based on experiences on lock-based implementations, nevertheless, serializability is known as an expensive feature that comes with high overhead and low concurrency.","Commercial systems, hence, compromise serializability by implementing weaker guarantees such as snapshot isolation.","The developers, therefore, are still burdened with the anomalies that could arise due to the lack of serializability.   ","There have been recent attempts to enrich large-scale data stores, such as HBase and BigTable, with transactional support.","Not surprisingly, inspired by traditional database management systems, serializability is usually compromised for the benefit of efficiency.","For example, Google Percolator, implements lock-based snapshot isolation on top of BigTable.","We show in this paper that this compromise is not necessary in lock-free implementations of transactional support.","We introduce write-snapshot isolation, a novel isolation level that has a performance comparable with that of snapshot isolation, and yet provides serializability.","The main insight in write-snapshot isolation is to prevent read-write conflicts in contrast to write-write conflicts that are prevented by snapshot isolation."],"url":"http://arxiv.org/abs/2405.18393v1","category":"cs.DB"}
{"created":"2024-05-28 17:31:15","title":"Untangling Climate's Complexity: Methodological Insights","abstract":"In this article, we review the interdisciplinary techniques (borrowed from physics, mathematics, statistics, machine-learning, etc.) and methodological framework that we have used to understand climate systems, which serve as examples of \"complex systems\". We believe that this would offer valuable insights to comprehend the complexity of climate variability and pave the way for drafting policies for action against climate change, etc. Our basic aim is to analyse time-series data structures across diverse climate parameters, extract Fourier-transformed features to recognize and model the trends/seasonalities in the climate variables using standard methods like detrended residual series analyses, correlation structures among climate parameters, Granger causal models, and other statistical machine-learning techniques. We cite and briefly explain two case studies: (i) the relationship between the Standardised Precipitation Index (SPI) and specific climate variables including Sea Surface Temperature (SST), El Ni\\~no Southern Oscillation (ENSO), and Indian Ocean Dipole (IOD), uncovering temporal shifts in correlations between SPI and these variables, and reveal complex patterns that drive drought and wet climate conditions in South-West Australia; (ii) the complex interactions of North Atlantic Oscillation (NAO) index, with SST and sea ice extent (SIE), potentially arising from positive feedback loops.","sentences":["In this article, we review the interdisciplinary techniques (borrowed from physics, mathematics, statistics, machine-learning, etc.) and methodological framework that we have used to understand climate systems, which serve as examples of \"complex systems\".","We believe that this would offer valuable insights to comprehend the complexity of climate variability and pave the way for drafting policies for action against climate change, etc.","Our basic aim is to analyse time-series data structures across diverse climate parameters, extract Fourier-transformed features to recognize and model the trends/seasonalities in the climate variables using standard methods like detrended residual series analyses, correlation structures among climate parameters, Granger causal models, and other statistical machine-learning techniques.","We cite and briefly explain two case studies: (i) the relationship between the Standardised Precipitation Index (SPI) and specific climate variables including Sea Surface Temperature (SST), El Ni\\~no Southern Oscillation (ENSO), and Indian Ocean Dipole (IOD), uncovering temporal shifts in correlations between SPI and these variables, and reveal complex patterns that drive drought and wet climate conditions in South-West Australia; (ii) the complex interactions of North Atlantic Oscillation (NAO) index, with SST and sea ice extent (SIE), potentially arising from positive feedback loops."],"url":"http://arxiv.org/abs/2405.18391v1","category":"physics.data-an"}
{"created":"2024-05-28 17:25:58","title":"Decentralized Picosecond Synchronization for Distributed Wireless Systems","abstract":"We demonstrate a wireless, decentralized time-alignment method for distributed antenna arrays and distributed wireless networks that achieves picosecond-level synchronization. Distributed antenna arrays consist of spatially separated antennas that coordinate their functionality at the wavelength level to achieve coherent operations such as distributed beamforming. Accurate time alignment (synchronization) of the local clocks on each node in the array is necessary to support accurate time-delay beamforming of modulated signals. In this work we combine a consensus averaging algorithm and a high-accuracy wireless two-way time transfer method to achieve decentralized time alignment, correcting for the time-varying bias of the clocks in a method that has no central node. Internode time transfer is based on a spectrally-sparse, two-tone signal achieving near-optimal time delay accuracy. We experimentally demonstrate the approach in a wireless four-node software-defined radio system, with various network connectivity graphs. We show that within 20 iterations all the nodes achieve convergence within a bias of less than 12 ps and a standard deviation of less than 3 ps. The performance is evaluated versus the bandwidth of the two-tone waveform, which impacts the synchronization error, and versus the signal-to-noise ratio.","sentences":["We demonstrate a wireless, decentralized time-alignment method for distributed antenna arrays and distributed wireless networks that achieves picosecond-level synchronization.","Distributed antenna arrays consist of spatially separated antennas that coordinate their functionality at the wavelength level to achieve coherent operations such as distributed beamforming.","Accurate time alignment (synchronization) of the local clocks on each node in the array is necessary to support accurate time-delay beamforming of modulated signals.","In this work we combine a consensus averaging algorithm and a high-accuracy wireless two-way time transfer method to achieve decentralized time alignment, correcting for the time-varying bias of the clocks in a method that has no central node.","Internode time transfer is based on a spectrally-sparse, two-tone signal achieving near-optimal time delay accuracy.","We experimentally demonstrate the approach in a wireless four-node software-defined radio system, with various network connectivity graphs.","We show that within 20 iterations all the nodes achieve convergence within a bias of less than 12 ps and a standard deviation of less than 3 ps.","The performance is evaluated versus the bandwidth of the two-tone waveform, which impacts the synchronization error, and versus the signal-to-noise ratio."],"url":"http://arxiv.org/abs/2405.18384v1","category":"eess.SP"}
{"created":"2024-05-28 17:23:02","title":"Learning a conserved mechanism for early neuroectoderm morphogenesis","abstract":"Morphogenesis is the process whereby the body of an organism develops its target shape. The morphogen BMP is known to play a conserved role across bilaterian organisms in determining the dorsoventral (DV) axis. Yet, how BMP governs the spatio-temporal dynamics of cytoskeletal proteins driving morphogenetic flow remains an open question. Here, we use machine learning to mine a morphodynamic atlas of Drosophila development, and construct a mathematical model capable of predicting the coupled dynamics of myosin, E-cadherin, and morphogenetic flow. Mutant analysis shows that BMP sets the initial condition of this dynamical system according to the following signaling cascade: BMP establishes DV pair-rule-gene patterns that set-up an E-cadherin gradient which in turn creates a myosin gradient in the opposite direction through mechanochemical feedbacks. Using neural tube organoids, we argue that BMP, and the signaling cascade it triggers, prime the conserved dynamics of neuroectoderm morphogenesis from fly to humans.","sentences":["Morphogenesis is the process whereby the body of an organism develops its target shape.","The morphogen BMP is known to play a conserved role across bilaterian organisms in determining the dorsoventral (DV) axis.","Yet, how BMP governs the spatio-temporal dynamics of cytoskeletal proteins driving morphogenetic flow remains an open question.","Here, we use machine learning to mine a morphodynamic atlas of Drosophila development, and construct a mathematical model capable of predicting the coupled dynamics of myosin, E-cadherin, and morphogenetic flow.","Mutant analysis shows that BMP sets the initial condition of this dynamical system according to the following signaling cascade: BMP establishes DV pair-rule-gene patterns that set-up an E-cadherin gradient which in turn creates a myosin gradient in the opposite direction through mechanochemical feedbacks.","Using neural tube organoids, we argue that BMP, and the signaling cascade it triggers, prime the conserved dynamics of neuroectoderm morphogenesis from fly to humans."],"url":"http://arxiv.org/abs/2405.18382v1","category":"physics.bio-ph"}
{"created":"2024-05-28 17:22:40","title":"Short-time Fokker-Planck propagator beyond the Gaussian approximation","abstract":"We present a perturbation approach to calculate the short-time propagator, or transition density, of the one-dimensional Fokker-Planck equation, to in principle arbitrary order in the time increment. Our approach preserves probability exactly and allows us to evaluate expectation values of analytical observables to in principle arbitrary accuracy; to showcase this, we derive perturbation expansions for the moments of the spatial increment, the finite-time Kramers-Moyal coefficients, and the mean medium entropy production rate. For an explicit multiplicative-noise system with available analytical solution, we validate all our perturbative results. Throughout, we compare our perturbative results to those obtained from the widely used Gaussian approximation of the short-time propagator; we demonstrate that this Gaussian propagator leads to errors that can be many orders of magnitude larger than those resulting from our perturbation approach. Potential applications of our results include parametrizing diffusive stochastic dynamics from observed time series, and sampling path spaces of stochastic trajectories numerically.","sentences":["We present a perturbation approach to calculate the short-time propagator, or transition density, of the one-dimensional Fokker-Planck equation, to in principle arbitrary order in the time increment.","Our approach preserves probability exactly and allows us to evaluate expectation values of analytical observables to in principle arbitrary accuracy; to showcase this, we derive perturbation expansions for the moments of the spatial increment, the finite-time Kramers-Moyal coefficients, and the mean medium entropy production rate.","For an explicit multiplicative-noise system with available analytical solution, we validate all our perturbative results.","Throughout, we compare our perturbative results to those obtained from the widely used Gaussian approximation of the short-time propagator; we demonstrate that this Gaussian propagator leads to errors that can be many orders of magnitude larger than those resulting from our perturbation approach.","Potential applications of our results include parametrizing diffusive stochastic dynamics from observed time series, and sampling path spaces of stochastic trajectories numerically."],"url":"http://arxiv.org/abs/2405.18381v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-28 17:22:15","title":"A Canonization Perspective on Invariant and Equivariant Learning","abstract":"In many applications, we desire neural networks to exhibit invariance or equivariance to certain groups due to symmetries inherent in the data. Recently, frame-averaging methods emerged to be a unified framework for attaining symmetries efficiently by averaging over input-dependent subsets of the group, i.e., frames. What we currently lack is a principled understanding of the design of frames. In this work, we introduce a canonization perspective that provides an essential and complete view of the design of frames. Canonization is a classic approach for attaining invariance by mapping inputs to their canonical forms. We show that there exists an inherent connection between frames and canonical forms. Leveraging this connection, we can efficiently compare the complexity of frames as well as determine the optimality of certain frames. Guided by this principle, we design novel frames for eigenvectors that are strictly superior to existing methods -- some are even optimal -- both theoretically and empirically. The reduction to the canonization perspective further uncovers equivalences between previous methods. These observations suggest that canonization provides a fundamental understanding of existing frame-averaging methods and unifies existing equivariant and invariant learning methods.","sentences":["In many applications, we desire neural networks to exhibit invariance or equivariance to certain groups due to symmetries inherent in the data.","Recently, frame-averaging methods emerged to be a unified framework for attaining symmetries efficiently by averaging over input-dependent subsets of the group, i.e., frames.","What we currently lack is a principled understanding of the design of frames.","In this work, we introduce a canonization perspective that provides an essential and complete view of the design of frames.","Canonization is a classic approach for attaining invariance by mapping inputs to their canonical forms.","We show that there exists an inherent connection between frames and canonical forms.","Leveraging this connection, we can efficiently compare the complexity of frames as well as determine the optimality of certain frames.","Guided by this principle, we design novel frames for eigenvectors that are strictly superior to existing methods -- some are even optimal -- both theoretically and empirically.","The reduction to the canonization perspective further uncovers equivalences between previous methods.","These observations suggest that canonization provides a fundamental understanding of existing frame-averaging methods and unifies existing equivariant and invariant learning methods."],"url":"http://arxiv.org/abs/2405.18378v1","category":"cs.LG"}
{"created":"2024-05-28 17:07:55","title":"The 2024 Brain Tumor Segmentation (BraTS) Challenge: Glioma Segmentation on Post-treatment MRI","abstract":"Gliomas are the most common malignant primary brain tumors in adults and one of the deadliest types of cancer. There are many challenges in treatment and monitoring due to the genetic diversity and high intrinsic heterogeneity in appearance, shape, histology, and treatment response. Treatments include surgery, radiation, and systemic therapies, with magnetic resonance imaging (MRI) playing a key role in treatment planning and post-treatment longitudinal assessment. The 2024 Brain Tumor Segmentation (BraTS) challenge on post-treatment glioma MRI will provide a community standard and benchmark for state-of-the-art automated segmentation models based on the largest expert-annotated post-treatment glioma MRI dataset. Challenge competitors will develop automated segmentation models to predict four distinct tumor sub-regions consisting of enhancing tissue (ET), surrounding non-enhancing T2/fluid-attenuated inversion recovery (FLAIR) hyperintensity (SNFH), non-enhancing tumor core (NETC), and resection cavity (RC). Models will be evaluated on separate validation and test datasets using standardized performance metrics utilized across the BraTS 2024 cluster of challenges, including lesion-wise Dice Similarity Coefficient and Hausdorff Distance. Models developed during this challenge will advance the field of automated MRI segmentation and contribute to their integration into clinical practice, ultimately enhancing patient care.","sentences":["Gliomas are the most common malignant primary brain tumors in adults and one of the deadliest types of cancer.","There are many challenges in treatment and monitoring due to the genetic diversity and high intrinsic heterogeneity in appearance, shape, histology, and treatment response.","Treatments include surgery, radiation, and systemic therapies, with magnetic resonance imaging (MRI) playing a key role in treatment planning and post-treatment longitudinal assessment.","The 2024 Brain Tumor Segmentation (BraTS) challenge on post-treatment glioma MRI will provide a community standard and benchmark for state-of-the-art automated segmentation models based on the largest expert-annotated post-treatment glioma MRI dataset.","Challenge competitors will develop automated segmentation models to predict four distinct tumor sub-regions consisting of enhancing tissue (ET), surrounding non-enhancing T2/fluid-attenuated inversion recovery (FLAIR) hyperintensity (SNFH), non-enhancing tumor core (NETC), and resection cavity (RC).","Models will be evaluated on separate validation and test datasets using standardized performance metrics utilized across the BraTS 2024 cluster of challenges, including lesion-wise Dice Similarity Coefficient and Hausdorff Distance.","Models developed during this challenge will advance the field of automated MRI segmentation and contribute to their integration into clinical practice, ultimately enhancing patient care."],"url":"http://arxiv.org/abs/2405.18368v1","category":"cs.CV"}
{"created":"2024-05-28 17:00:17","title":"Computational Characterization of Symmetry-Protected Topological Phases in Open Quantum Systems","abstract":"It is a challenging problem to correctly characterize the symmetry-protected topological (SPT) phases in open quantum systems. As the measurement-based quantum computation (MBQC) utilizes non-trivial edge states of the SPT phases as the logical qubit, its computational power is closely tied to the non-trivial topological nature of the phases. In this paper, we propose to use the gate fidelity which is a measure of the computational power of the MBQC to identify the SPT phases in mixed-state settings. Specifically, we investigate the robustness of the Haldane phase by considering the MBQC on the Affleck-Kennedy-Lieb-Tasaki state subject to different types of noises. To illustrate how our criterion works, we analytically and numerically calculated the gate fidelity to find that its behavior depends crucially on whether the noises satisfy a certain symmetry condition with respect to the on-site $\\mathbb{Z}_2 \\times \\mathbb{Z}_2$ symmetry. In particular, the fidelity for the identity gate, which is given by the sum of the non-local string order parameters, plays an important role. Furthermore, we demonstrate that a stronger symmetry conditions are required to be able to perform other (e.g., the $Z$-rotation gate) gates with high fidelity. By examining which unitary gates can be implemented with the MBQC on the decohered states, we can gain a useful insight into the richer structure of noisy SPT states that cannot be captured solely by the string order parameters.","sentences":["It is a challenging problem to correctly characterize the symmetry-protected topological (SPT) phases in open quantum systems.","As the measurement-based quantum computation (MBQC) utilizes non-trivial edge states of the SPT phases as the logical qubit, its computational power is closely tied to the non-trivial topological nature of the phases.","In this paper, we propose to use the gate fidelity which is a measure of the computational power of the MBQC to identify the SPT phases in mixed-state settings.","Specifically, we investigate the robustness of the Haldane phase by considering the MBQC on the Affleck-Kennedy-Lieb-Tasaki state subject to different types of noises.","To illustrate how our criterion works, we analytically and numerically calculated the gate fidelity to find that its behavior depends crucially on whether the noises satisfy a certain symmetry condition with respect to the on-site $\\mathbb{Z}_2 \\times \\mathbb{Z}_2$ symmetry.","In particular, the fidelity for the identity gate, which is given by the sum of the non-local string order parameters, plays an important role.","Furthermore, we demonstrate that a stronger symmetry conditions are required to be able to perform other (e.g., the $Z$-rotation gate) gates with high fidelity.","By examining which unitary gates can be implemented with the MBQC on the decohered states, we can gain a useful insight into the richer structure of noisy SPT states that cannot be captured solely by the string order parameters."],"url":"http://arxiv.org/abs/2405.18364v1","category":"quant-ph"}
{"created":"2024-05-28 16:58:49","title":"Enhanced fractional quantum Hall gaps in a two-dimensional electron gas coupled to a hovering split-ring resonator","abstract":"The magnetotransport of a high-mobility two-dimensional electron gas coupled to a hovering split-ring resonator with controllable distance is studied in the quantum Hall regime. The measurements reveal an enhancement by more than a factor 2 of the quantum Hall energy gaps at the fractional filling factors 4/3, 5/3, and 7/5, alongside a concurrent reduction in exchange splitting at odd integer filling factors. Theoretically, we show the strength of both effects to be quantitatively compatible with the emergence of an effective electron-electron long-range attractive interaction mediated by the exchange of virtual cavity photons in the presence of significant spatial gradients of the cavity electric vacuum fields. These results unveil a compelling interplay between cavity quantum electrodynamics and electronic correlations in two-dimensional systems, with profound implications for the manipulation and control of quantum phases in 2D materials.","sentences":["The magnetotransport of a high-mobility two-dimensional electron gas coupled to a hovering split-ring resonator with controllable distance is studied in the quantum Hall regime.","The measurements reveal an enhancement by more than a factor 2 of the quantum Hall energy gaps at the fractional filling factors 4/3, 5/3, and 7/5, alongside a concurrent reduction in exchange splitting at odd integer filling factors.","Theoretically, we show the strength of both effects to be quantitatively compatible with the emergence of an effective electron-electron long-range attractive interaction mediated by the exchange of virtual cavity photons in the presence of significant spatial gradients of the cavity electric vacuum fields.","These results unveil a compelling interplay between cavity quantum electrodynamics and electronic correlations in two-dimensional systems, with profound implications for the manipulation and control of quantum phases in 2D materials."],"url":"http://arxiv.org/abs/2405.18362v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-28 16:55:33","title":"Faithful Logical Reasoning via Symbolic Chain-of-Thought","abstract":"While the recent Chain-of-Thought (CoT) technique enhances the reasoning ability of large language models (LLMs) with the theory of mind, it might still struggle in handling logical reasoning that relies much on symbolic expressions and rigid deducing rules. To strengthen the logical reasoning capability of LLMs, we propose a novel Symbolic Chain-of-Thought, namely SymbCoT, a fully LLM-based framework that integrates symbolic expressions and logic rules with CoT prompting. Technically, building upon an LLM, SymbCoT 1) first translates the natural language context into the symbolic format, and then 2) derives a step-by-step plan to solve the problem with symbolic logical rules, 3) followed by a verifier to check the translation and reasoning chain. Via thorough evaluations on 5 standard datasets with both First-Order Logic and Constraint Optimization symbolic expressions, SymbCoT shows striking improvements over the CoT method consistently, meanwhile refreshing the current state-of-the-art performances. We further demonstrate that our system advances in more faithful, flexible, and explainable logical reasoning. To our knowledge, this is the first to combine symbolic expressions and rules into CoT for logical reasoning with LLMs. Code is open at https://github.com/Aiden0526/SymbCoT.","sentences":["While the recent Chain-of-Thought (CoT) technique enhances the reasoning ability of large language models (LLMs) with the theory of mind, it might still struggle in handling logical reasoning that relies much on symbolic expressions and rigid deducing rules.","To strengthen the logical reasoning capability of LLMs, we propose a novel Symbolic Chain-of-Thought, namely SymbCoT, a fully LLM-based framework that integrates symbolic expressions and logic rules with CoT prompting.","Technically, building upon an LLM, SymbCoT 1) first translates the natural language context into the symbolic format, and then 2) derives a step-by-step plan to solve the problem with symbolic logical rules, 3) followed by a verifier to check the translation and reasoning chain.","Via thorough evaluations on 5 standard datasets with both First-Order Logic and Constraint Optimization symbolic expressions, SymbCoT shows striking improvements over the CoT method consistently, meanwhile refreshing the current state-of-the-art performances.","We further demonstrate that our system advances in more faithful, flexible, and explainable logical reasoning.","To our knowledge, this is the first to combine symbolic expressions and rules into CoT for logical reasoning with LLMs.","Code is open at https://github.com/Aiden0526/SymbCoT."],"url":"http://arxiv.org/abs/2405.18357v1","category":"cs.CL"}
{"created":"2024-05-28 16:28:51","title":"SketchQL Demonstration: Zero-shot Video Moment Querying with Sketches","abstract":"In this paper, we will present SketchQL, a video database management system (VDBMS) for retrieving video moments with a sketch-based query interface. This novel interface allows users to specify object trajectory events with simple mouse drag-and-drop operations. Users can use trajectories of single objects as building blocks to compose complex events. Using a pre-trained model that encodes trajectory similarity, SketchQL achieves zero-shot video moments retrieval by performing similarity searches over the video to identify clips that are the most similar to the visual query. In this demonstration, we introduce the graphic user interface of SketchQL and detail its functionalities and interaction mechanisms. We also demonstrate the end-to-end usage of SketchQL from query composition to video moments retrieval using real-world scenarios.","sentences":["In this paper, we will present SketchQL, a video database management system (VDBMS) for retrieving video moments with a sketch-based query interface.","This novel interface allows users to specify object trajectory events with simple mouse drag-and-drop operations.","Users can use trajectories of single objects as building blocks to compose complex events.","Using a pre-trained model that encodes trajectory similarity, SketchQL achieves zero-shot video moments retrieval by performing similarity searches over the video to identify clips that are the most similar to the visual query.","In this demonstration, we introduce the graphic user interface of SketchQL and detail its functionalities and interaction mechanisms.","We also demonstrate the end-to-end usage of SketchQL from query composition to video moments retrieval using real-world scenarios."],"url":"http://arxiv.org/abs/2405.18334v1","category":"cs.DB"}
{"created":"2024-05-28 16:22:18","title":"Warm Start Marginal Likelihood Optimisation for Iterative Gaussian Processes","abstract":"Gaussian processes are a versatile probabilistic machine learning model whose effectiveness often depends on good hyperparameters, which are typically learned by maximising the marginal likelihood. In this work, we consider iterative methods, which use iterative linear system solvers to approximate marginal likelihood gradients up to a specified numerical precision, allowing a trade-off between compute time and accuracy of a solution. We introduce a three-level hierarchy of marginal likelihood optimisation for iterative Gaussian processes, and identify that the computational costs are dominated by solving sequential batches of large positive-definite systems of linear equations. We then propose to amortise computations by reusing solutions of linear system solvers as initialisations in the next step, providing a $\\textit{warm start}$. Finally, we discuss the necessary conditions and quantify the consequences of warm starts and demonstrate their effectiveness on regression tasks, where warm starts achieve the same results as the conventional procedure while providing up to a $16 \\times$ average speed-up among datasets.","sentences":["Gaussian processes are a versatile probabilistic machine learning model whose effectiveness often depends on good hyperparameters, which are typically learned by maximising the marginal likelihood.","In this work, we consider iterative methods, which use iterative linear system solvers to approximate marginal likelihood gradients up to a specified numerical precision, allowing a trade-off between compute time and accuracy of a solution.","We introduce a three-level hierarchy of marginal likelihood optimisation for iterative Gaussian processes, and identify that the computational costs are dominated by solving sequential batches of large positive-definite systems of linear equations.","We then propose to amortise computations by reusing solutions of linear system solvers as initialisations in the next step, providing a $\\textit{warm start}$.","Finally, we discuss the necessary conditions and quantify the consequences of warm starts and demonstrate their effectiveness on regression tasks, where warm starts achieve the same results as the conventional procedure while providing up to a $16 \\times$ average speed-up among datasets."],"url":"http://arxiv.org/abs/2405.18328v1","category":"cs.LG"}
{"created":"2024-05-28 16:11:36","title":"Long Short-Term Memory Networks for Anomaly Detection in Magnet Power Supplies of Particle Accelerators","abstract":"This research introduces a novel anomaly detection method designed to enhance the operational reliability of particle accelerators - complex machines that accelerate elementary particles to high speeds for various scientific applications. Our approach utilizes a Long Short-Term Memory (LSTM) neural network to predict the temperature of key components within the magnet power supplies (PSs) of these accelerators, such as heatsinks, capacitors, and resistors, based on the electrical current flowing through the PS. Anomalies are declared when there is a significant discrepancy between the LSTM-predicted temperatures and actual observations. Leveraging a custom-built test stand, we conducted comprehensive performance comparisons with a less sophisticated method, while also fine-tuning hyperparameters of both methods. This process not only optimized the LSTM model but also unequivocally demonstrated the superior efficacy of this new proposed method. The dedicated test stand also facilitated exploratory work on more advanced strategies for monitoring interior PS temperatures using infrared cameras. A proof-of-concept example is provided.","sentences":["This research introduces a novel anomaly detection method designed to enhance the operational reliability of particle accelerators - complex machines that accelerate elementary particles to high speeds for various scientific applications.","Our approach utilizes a Long Short-Term Memory (LSTM) neural network to predict the temperature of key components within the magnet power supplies (PSs) of these accelerators, such as heatsinks, capacitors, and resistors, based on the electrical current flowing through the PS.","Anomalies are declared when there is a significant discrepancy between the LSTM-predicted temperatures and actual observations.","Leveraging a custom-built test stand, we conducted comprehensive performance comparisons with a less sophisticated method, while also fine-tuning hyperparameters of both methods.","This process not only optimized the LSTM model but also unequivocally demonstrated the superior efficacy of this new proposed method.","The dedicated test stand also facilitated exploratory work on more advanced strategies for monitoring interior PS temperatures using infrared cameras.","A proof-of-concept example is provided."],"url":"http://arxiv.org/abs/2405.18321v1","category":"physics.acc-ph"}
{"created":"2024-05-28 16:10:22","title":"Hybrid Multi-Head Physics-informed Neural Network for Depth Estimation in Terahertz Imaging","abstract":"Terahertz (THz) imaging is one of the hotspots in the field of optics, where the depth information retrieval is a key factor to restore the three-dimensional appearance of objects. Impressive results for depth extraction in visible and infrared wave range have been demonstrated through deep learning (DL). Among them, most DL methods are merely data-driven, lacking relevant physical priors, which thus request for a large amount of experimental data to train the DL models.However, large training data acquirement in the THz domain is challenging due to the requirements of environmental and system stability, as well as the time-consuming data acquisition process. To overcome this limitation, this paper incorporates a complete physical model representing the THz image formation process into traditional DL networks to retrieve the depth information of objects. The most significant advantage is the ability to use it without pre-training, thereby eliminating the need for tens of thousands of labeled data. Through experiments validation, we demonstrate that by providing diffraction patterns of planar objects with their upper and lower halves individually masked, the proposed physics-informed neural network (NN) can automatically optimize and, ultimately, reconstruct the depth of the object through interaction between the NN and a physical model. The obtained results represent the initial steps towards achieving fast holographic THz imaging using reference-free beams and low-cost power detection.","sentences":["Terahertz (THz) imaging is one of the hotspots in the field of optics, where the depth information retrieval is a key factor to restore the three-dimensional appearance of objects.","Impressive results for depth extraction in visible and infrared wave range have been demonstrated through deep learning (DL).","Among them, most DL methods are merely data-driven, lacking relevant physical priors, which thus request for a large amount of experimental data to train the DL models.","However, large training data acquirement in the THz domain is challenging due to the requirements of environmental and system stability, as well as the time-consuming data acquisition process.","To overcome this limitation, this paper incorporates a complete physical model representing the THz image formation process into traditional DL networks to retrieve the depth information of objects.","The most significant advantage is the ability to use it without pre-training, thereby eliminating the need for tens of thousands of labeled data.","Through experiments validation, we demonstrate that by providing diffraction patterns of planar objects with their upper and lower halves individually masked, the proposed physics-informed neural network (NN) can automatically optimize and, ultimately, reconstruct the depth of the object through interaction between the NN and a physical model.","The obtained results represent the initial steps towards achieving fast holographic THz imaging using reference-free beams and low-cost power detection."],"url":"http://arxiv.org/abs/2405.18317v1","category":"physics.optics"}
{"created":"2024-05-28 16:07:17","title":"Deriving Causal Order from Single-Variable Interventions: Guarantees & Algorithm","abstract":"Targeted and uniform interventions to a system are crucial for unveiling causal relationships. While several methods have been developed to leverage interventional data for causal structure learning, their practical application in real-world scenarios often remains challenging. Recent benchmark studies have highlighted these difficulties, even when large numbers of single-variable intervention samples are available. In this work, we demonstrate, both theoretically and empirically, that such datasets contain a wealth of causal information that can be effectively extracted under realistic assumptions about the data distribution. More specifically, we introduce the notion of interventional faithfulness, which relies on comparisons between the marginal distributions of each variable across observational and interventional settings, and we introduce a score on causal orders. Under this assumption, we are able to prove strong theoretical guarantees on the optimum of our score that also hold for large-scale settings. To empirically verify our theory, we introduce Intersort, an algorithm designed to infer the causal order from datasets containing large numbers of single-variable interventions by approximately optimizing our score. Intersort outperforms baselines (GIES, PC and EASE) on almost all simulated data settings replicating common benchmarks in the field. Our proposed novel approach to modeling interventional datasets thus offers a promising avenue for advancing causal inference, highlighting significant potential for further enhancements under realistic assumptions.","sentences":["Targeted and uniform interventions to a system are crucial for unveiling causal relationships.","While several methods have been developed to leverage interventional data for causal structure learning, their practical application in real-world scenarios often remains challenging.","Recent benchmark studies have highlighted these difficulties, even when large numbers of single-variable intervention samples are available.","In this work, we demonstrate, both theoretically and empirically, that such datasets contain a wealth of causal information that can be effectively extracted under realistic assumptions about the data distribution.","More specifically, we introduce the notion of interventional faithfulness, which relies on comparisons between the marginal distributions of each variable across observational and interventional settings, and we introduce a score on causal orders.","Under this assumption, we are able to prove strong theoretical guarantees on the optimum of our score that also hold for large-scale settings.","To empirically verify our theory, we introduce Intersort, an algorithm designed to infer the causal order from datasets containing large numbers of single-variable interventions by approximately optimizing our score.","Intersort outperforms baselines (GIES, PC and EASE) on almost all simulated data settings replicating common benchmarks in the field.","Our proposed novel approach to modeling interventional datasets thus offers a promising avenue for advancing causal inference, highlighting significant potential for further enhancements under realistic assumptions."],"url":"http://arxiv.org/abs/2405.18314v1","category":"cs.LG"}
{"created":"2024-05-28 15:57:58","title":"Deep Network Pruning: A Comparative Study on CNNs in Face Recognition","abstract":"The widespread use of mobile devices for all kind of transactions makes necessary reliable and real-time identity authentication, leading to the adoption of face recognition (FR) via the cameras embedded in such devices. Progress of deep Convolutional Neural Networks (CNNs) has provided substantial advances in FR. Nonetheless, the size of state-of-the-art architectures is unsuitable for mobile deployment, since they often encompass hundreds of megabytes and millions of parameters. We address this by studying methods for deep network compression applied to FR. In particular, we apply network pruning based on Taylor scores, where less important filters are removed iteratively. The method is tested on three networks based on the small SqueezeNet (1.24M parameters) and the popular MobileNetv2 (3.5M) and ResNet50 (23.5M) architectures. These have been selected to showcase the method on CNNs with different complexities and sizes. We observe that a substantial percentage of filters can be removed with minimal performance loss. Also, filters with the highest amount of output channels tend to be removed first, suggesting that high-dimensional spaces within popular CNNs are over-dimensionated.","sentences":["The widespread use of mobile devices for all kind of transactions makes necessary reliable and real-time identity authentication, leading to the adoption of face recognition (FR) via the cameras embedded in such devices.","Progress of deep Convolutional Neural Networks (CNNs) has provided substantial advances in FR.","Nonetheless, the size of state-of-the-art architectures is unsuitable for mobile deployment, since they often encompass hundreds of megabytes and millions of parameters.","We address this by studying methods for deep network compression applied to FR.","In particular, we apply network pruning based on Taylor scores, where less important filters are removed iteratively.","The method is tested on three networks based on the small SqueezeNet (1.24M parameters) and the popular MobileNetv2 (3.5M) and ResNet50 (23.5M) architectures.","These have been selected to showcase the method on CNNs with different complexities and sizes.","We observe that a substantial percentage of filters can be removed with minimal performance loss.","Also, filters with the highest amount of output channels tend to be removed first, suggesting that high-dimensional spaces within popular CNNs are over-dimensionated."],"url":"http://arxiv.org/abs/2405.18302v1","category":"cs.CV"}
{"created":"2024-05-28 15:55:36","title":"Large disks touching three sides of a quadrilateral","abstract":"We show that every Jordan quadrilateral $Q\\subset\\mathbb{C}$ contains a disk $D$ so that $\\partial D\\cap\\partial Q$ contains points of three different sides of $Q$. As a consequence, together with some modulus estimates from Lehto and Virtanen, we offer a short proof of the main result obtained by Chrontsios-Garitsis and Hinkkanen in 2024 and it also improves the bounds on their result.","sentences":["We show that every Jordan quadrilateral $Q\\subset\\mathbb{C}$ contains a disk $D$ so that $\\partial D\\cap\\partial Q$ contains points of three different sides of $Q$. As a consequence, together with some modulus estimates from Lehto and Virtanen, we offer a short proof of the main result obtained by Chrontsios-Garitsis and Hinkkanen in 2024 and it also improves the bounds on their result."],"url":"http://arxiv.org/abs/2405.18301v1","category":"math.CV"}
{"created":"2024-05-28 15:48:39","title":"Intent3D: 3D Object Detection in RGB-D Scans Based on Human Intention","abstract":"In real-life scenarios, humans seek out objects in the 3D world to fulfill their daily needs or intentions. This inspires us to introduce 3D intention grounding, a new task in 3D object detection employing RGB-D, based on human intention, such as \"I want something to support my back\". Closely related, 3D visual grounding focuses on understanding human reference. To achieve detection based on human intention, it relies on humans to observe the scene, reason out the target that aligns with their intention (\"pillow\" in this case), and finally provide a reference to the AI system, such as \"A pillow on the couch\". Instead, 3D intention grounding challenges AI agents to automatically observe, reason and detect the desired target solely based on human intention. To tackle this challenge, we introduce the new Intent3D dataset, consisting of 44,990 intention texts associated with 209 fine-grained classes from 1,042 scenes of the ScanNet dataset. We also establish several baselines based on different language-based 3D object detection models on our benchmark. Finally, we propose IntentNet, our unique approach, designed to tackle this intention-based detection problem. It focuses on three key aspects: intention understanding, reasoning to identify object candidates, and cascaded adaptive learning that leverages the intrinsic priority logic of different losses for multiple objective optimization.","sentences":["In real-life scenarios, humans seek out objects in the 3D world to fulfill their daily needs or intentions.","This inspires us to introduce 3D intention grounding, a new task in 3D object detection employing RGB-D, based on human intention, such as \"I want something to support my back\".","Closely related, 3D visual grounding focuses on understanding human reference.","To achieve detection based on human intention, it relies on humans to observe the scene, reason out the target that aligns with their intention (\"pillow\" in this case), and finally provide a reference to the AI system, such as \"A pillow on the couch\".","Instead, 3D intention grounding challenges AI agents to automatically observe, reason and detect the desired target solely based on human intention.","To tackle this challenge, we introduce the new Intent3D dataset, consisting of 44,990 intention texts associated with 209 fine-grained classes from 1,042 scenes of the ScanNet dataset.","We also establish several baselines based on different language-based 3D object detection models on our benchmark.","Finally, we propose IntentNet, our unique approach, designed to tackle this intention-based detection problem.","It focuses on three key aspects: intention understanding, reasoning to identify object candidates, and cascaded adaptive learning that leverages the intrinsic priority logic of different losses for multiple objective optimization."],"url":"http://arxiv.org/abs/2405.18295v1","category":"cs.CV"}
{"created":"2024-05-28 15:48:27","title":"CF-OPT: Counterfactual Explanations for Structured Prediction","abstract":"Optimization layers in deep neural networks have enjoyed a growing popularity in structured learning, improving the state of the art on a variety of applications. Yet, these pipelines lack interpretability since they are made of two opaque layers: a highly non-linear prediction model, such as a deep neural network, and an optimization layer, which is typically a complex black-box solver. Our goal is to improve the transparency of such methods by providing counterfactual explanations. We build upon variational autoencoders a principled way of obtaining counterfactuals: working in the latent space leads to a natural notion of plausibility of explanations. We finally introduce a variant of the classic loss for VAE training that improves their performance in our specific structured context. These provide the foundations of CF-OPT, a first-order optimization algorithm that can find counterfactual explanations for a broad class of structured learning architectures. Our numerical results show that both close and plausible explanations can be obtained for problems from the recent literature.","sentences":["Optimization layers in deep neural networks have enjoyed a growing popularity in structured learning, improving the state of the art on a variety of applications.","Yet, these pipelines lack interpretability since they are made of two opaque layers: a highly non-linear prediction model, such as a deep neural network, and an optimization layer, which is typically a complex black-box solver.","Our goal is to improve the transparency of such methods by providing counterfactual explanations.","We build upon variational autoencoders a principled way of obtaining counterfactuals: working in the latent space leads to a natural notion of plausibility of explanations.","We finally introduce a variant of the classic loss for VAE training that improves their performance in our specific structured context.","These provide the foundations of CF-OPT, a first-order optimization algorithm that can find counterfactual explanations for a broad class of structured learning architectures.","Our numerical results show that both close and plausible explanations can be obtained for problems from the recent literature."],"url":"http://arxiv.org/abs/2405.18293v1","category":"cs.LG"}
{"created":"2024-05-28 15:40:39","title":"Stagewise Boosting Distributional Regression","abstract":"Forward stagewise regression is a simple algorithm that can be used to estimate regularized models. The updating rule adds a small constant to a regression coefficient in each iteration, such that the underlying optimization problem is solved slowly with small improvements. This is similar to gradient boosting, with the essential difference that the step size is determined by the product of the gradient and a step length parameter in the latter algorithm. One often overlooked challenge in gradient boosting for distributional regression is the issue of a vanishing small gradient, which practically halts the algorithm's progress. We show that gradient boosting in this case oftentimes results in suboptimal models, especially for complex problems certain distributional parameters are never updated due to the vanishing gradient. Therefore, we propose a stagewise boosting-type algorithm for distributional regression, combining stagewise regression ideas with gradient boosting. Additionally, we extend it with a novel regularization method, correlation filtering, to provide additional stability when the problem involves a large number of covariates. Furthermore, the algorithm includes best-subset selection for parameters and can be applied to big data problems by leveraging stochastic approximations of the updating steps. Besides the advantage of processing large datasets, the stochastic nature of the approximations can lead to better results, especially for complex distributions, by reducing the risk of being trapped in a local optimum. The performance of our proposed stagewise boosting distributional regression approach is investigated in an extensive simulation study and by estimating a full probabilistic model for lightning counts with data of more than 9.1 million observations and 672 covariates.","sentences":["Forward stagewise regression is a simple algorithm that can be used to estimate regularized models.","The updating rule adds a small constant to a regression coefficient in each iteration, such that the underlying optimization problem is solved slowly with small improvements.","This is similar to gradient boosting, with the essential difference that the step size is determined by the product of the gradient and a step length parameter in the latter algorithm.","One often overlooked challenge in gradient boosting for distributional regression is the issue of a vanishing small gradient, which practically halts the algorithm's progress.","We show that gradient boosting in this case oftentimes results in suboptimal models, especially for complex problems certain distributional parameters are never updated due to the vanishing gradient.","Therefore, we propose a stagewise boosting-type algorithm for distributional regression, combining stagewise regression ideas with gradient boosting.","Additionally, we extend it with a novel regularization method, correlation filtering, to provide additional stability when the problem involves a large number of covariates.","Furthermore, the algorithm includes best-subset selection for parameters and can be applied to big data problems by leveraging stochastic approximations of the updating steps.","Besides the advantage of processing large datasets, the stochastic nature of the approximations can lead to better results, especially for complex distributions, by reducing the risk of being trapped in a local optimum.","The performance of our proposed stagewise boosting distributional regression approach is investigated in an extensive simulation study and by estimating a full probabilistic model for lightning counts with data of more than 9.1 million observations and 672 covariates."],"url":"http://arxiv.org/abs/2405.18288v1","category":"stat.ME"}
{"created":"2024-05-28 15:40:06","title":"Stable finiteness of monoid algebras and surjunctivity","abstract":"A monoid $M$ is said to be surjunctive if every injective cellular automaton with finite alphabet over $M$ is surjective. We show that monoid algebras of surjunctive monoids are stably finite. In other words, given any field $K$ and any surjunctive monoid $M$, every one-sided invertible square matrix with entries in the monoid algebra $K[M]$ is two-sided invertible. Our proof uses first-order model theory.","sentences":["A monoid $M$ is said to be surjunctive if every injective cellular automaton with finite alphabet over $M$ is surjective.","We show that monoid algebras of surjunctive monoids are stably finite.","In other words, given any field $K$ and any surjunctive monoid $M$, every one-sided invertible square matrix with entries in the monoid algebra $K[M]$ is two-sided invertible.","Our proof uses first-order model theory."],"url":"http://arxiv.org/abs/2405.18287v1","category":"math.RA"}
{"created":"2024-05-28 15:33:38","title":"Diameter estimates in K\u00e4hler geometry II: removing the small degeneracy assumption","abstract":"In this short note, we remove the small degeneracy assumption in our earlier works [10, 11]. This is achieved by a technical improvement of Corollary 5.1 in [10]. As a consequence, we establish the same geometric estimates for diameter, Green's functions and Sobolev inequalities under an entropy bound for the K\\\"ahler metrics, without any small degeneracy assumption.","sentences":["In this short note, we remove the small degeneracy assumption in our earlier works [10, 11].","This is achieved by a technical improvement of Corollary 5.1 in [10].","As a consequence, we establish the same geometric estimates for diameter, Green's functions and Sobolev inequalities under an entropy bound for the K\\\"ahler metrics, without any small degeneracy assumption."],"url":"http://arxiv.org/abs/2405.18280v1","category":"math.DG"}
{"created":"2024-05-28 15:24:48","title":"The Round Complexity of Proofs in the Bounded Quantum Storage Model","abstract":"The round complexity of interactive proof systems is a key question of practical and theoretical relevance in complexity theory and cryptography. Moreover, results such as QIP = QIP(3) (STOC'00) show that quantum resources significantly help in such a task.   In this work, we initiate the study of round compression of protocols in the bounded quantum storage model (BQSM). In this model, the malicious parties have a bounded quantum memory and they cannot store the all the qubits that are transmitted in the protocol.   Our main results in this setting are the following:   1. There is a non-interactive (statistical) witness indistinguishable proof for any language in NP (and even QMA) in BQSM in the plain model. We notice that in this protocol, only the memory of the verifier is bounded.   2. Any classical proof system can be compressed in a two-message quantum proof system in BQSM. Moreover, if the original proof system is zero-knowledge, the quantum protocol is zero-knowledge too. In this result, we assume that the prover has bounded memory.   Finally, we give evidence towards the \"tightness\" of our results. First, we show that NIZK in the plain model against BQS adversaries is unlikely with standard techniques. Second, we prove that without the BQS model there is no 2-message zero-knowledge quantum interactive proof, even under computational assumptions.","sentences":["The round complexity of interactive proof systems is a key question of practical and theoretical relevance in complexity theory and cryptography.","Moreover, results such as QIP = QIP(3) (STOC'00) show that quantum resources significantly help in such a task.   ","In this work, we initiate the study of round compression of protocols in the bounded quantum storage model (BQSM).","In this model, the malicious parties have a bounded quantum memory and they cannot store the all the qubits that are transmitted in the protocol.   ","Our main results in this setting are the following:   1.","There is a non-interactive (statistical) witness indistinguishable proof for any language in NP (and even QMA) in BQSM in the plain model.","We notice that in this protocol, only the memory of the verifier is bounded.   ","2.","Any classical proof system can be compressed in a two-message quantum proof system in BQSM.","Moreover, if the original proof system is zero-knowledge, the quantum protocol is zero-knowledge too.","In this result, we assume that the prover has bounded memory.   ","Finally, we give evidence towards the \"tightness\" of our results.","First, we show that NIZK in the plain model against BQS adversaries is unlikely with standard techniques.","Second, we prove that without the BQS model there is no 2-message zero-knowledge quantum interactive proof, even under computational assumptions."],"url":"http://arxiv.org/abs/2405.18275v1","category":"quant-ph"}
{"created":"2024-05-28 15:24:30","title":"Synchronization on circles and spheres with nonlinear interactions","abstract":"We consider the dynamics of $n$ points on a sphere in $\\mathbb{R}^d$ ($d \\geq 2$) which attract each other according to a function $\\varphi$ of their inner products. When $\\varphi$ is linear ($\\varphi(t) = t$), the points converge to a common value (i.e., synchronize) in various connectivity scenarios: this is part of classical work on Kuramoto oscillator networks. When $\\varphi$ is exponential ($\\varphi(t) = e^{\\beta t}$), these dynamics correspond to a limit of how idealized transformers process data, as described by Geshkovski et al. (2024). Accordingly, they ask whether synchronization occurs for exponential $\\varphi$.   In the context of consensus for multi-agent control, Markdahl et al. (2018) show that for $d \\geq 3$ (spheres), if the interaction graph is connected and $\\varphi$ is increasing and convex, then the system synchronizes. What is the situation on circles ($d=2$)? First, we show that $\\varphi$ being increasing and convex is no longer sufficient. Then we identify a new condition (that the Taylor coefficients of $\\varphi'$ are decreasing) under which we do have synchronization on the circle. In so doing, we provide some answers to the open problems posed by Geshkovski et al. (2024).","sentences":["We consider the dynamics of $n$ points on a sphere in $\\mathbb{R}^d$ ($d \\geq 2$) which attract each other according to a function $\\varphi$ of their inner products.","When $\\varphi$ is linear ($\\varphi(t) = t$), the points converge to a common value (i.e., synchronize) in various connectivity scenarios: this is part of classical work on Kuramoto oscillator networks.","When $\\varphi$ is exponential ($\\varphi(t) = e^{\\beta t}$), these dynamics correspond to a limit of how idealized transformers process data, as described by Geshkovski et al. (2024).","Accordingly, they ask whether synchronization occurs for exponential $\\varphi$.   In the context of consensus for multi-agent control, Markdahl et al.","(2018) show that for $d \\geq 3$ (spheres), if the interaction graph is connected and $\\varphi$ is increasing and convex, then the system synchronizes.","What is the situation on circles ($d=2$)?","First, we show that $\\varphi$ being increasing and convex is no longer sufficient.","Then we identify a new condition (that the Taylor coefficients of $\\varphi'$ are decreasing) under which we do have synchronization on the circle.","In so doing, we provide some answers to the open problems posed by Geshkovski et al. (2024)."],"url":"http://arxiv.org/abs/2405.18273v1","category":"math.OC"}
{"created":"2024-05-28 15:19:00","title":"Continuous Transition between Bosonic Fractional Chern Insulator and Superfluid","abstract":"The properties of fractional Chern insulator (FCI) phase and the phase transitions between FCI and Mott insulators (MI) in bosonic systems are well studied. The continuous transitions between FCI and superfluid (SF), however, despite the inspiring field theoretical predictions, have not been directly verified. The existing numerical results of FCI-SF transition are either indirect or clearly first-order. Here, by simply tuning the bandwidth of Haldane honeycomb lattice model, we find direct transitions from a bosonic FCI at $\\nu$ = 1/2 filling of a flat Chern band to two SF states with bosons condensed at momenta $M$ and $\\Gamma$, respectively. While the FCI-SF($M$) transition is first-order, the FCI-SF($\\Gamma$) transition is found continuous. Through finite size criticality analysis, the obtained critical exponents $\\beta\\approx$0.35(5) and $\\nu\\approx$0.62(12) are both compatible with those of the 3D XY universality class and more exotic beyond-Landau ones. Our work thence presents a direct numerical evidence of a continuous FCI-SF transition between topological ordered phase and spontaneous continuous symmetry-breaking phase, and further indicates the zero-field bosonic FCI might be realized from a SF state by gradually flattening the dispersion of Chern band, through the (quasi)adiabatic preparation in ultracold atom systems.","sentences":["The properties of fractional Chern insulator (FCI) phase and the phase transitions between FCI and Mott insulators (MI) in bosonic systems are well studied.","The continuous transitions between FCI and superfluid (SF), however, despite the inspiring field theoretical predictions, have not been directly verified.","The existing numerical results of FCI-SF transition are either indirect or clearly first-order.","Here, by simply tuning the bandwidth of Haldane honeycomb lattice model, we find direct transitions from a bosonic FCI at $\\nu$ = 1/2 filling of a flat Chern band to two SF states with bosons condensed at momenta $M$ and $\\Gamma$, respectively.","While the FCI-SF($M$) transition is first-order, the FCI-SF($\\Gamma$) transition is found continuous.","Through finite size criticality analysis, the obtained critical exponents $\\beta\\approx$0.35(5) and $\\nu\\approx$0.62(12) are both compatible with those of the 3D XY universality class and more exotic beyond-Landau ones.","Our work thence presents a direct numerical evidence of a continuous FCI-SF transition between topological ordered phase and spontaneous continuous symmetry-breaking phase, and further indicates the zero-field bosonic FCI might be realized from a SF state by gradually flattening the dispersion of Chern band, through the (quasi)adiabatic preparation in ultracold atom systems."],"url":"http://arxiv.org/abs/2405.18269v1","category":"cond-mat.str-el"}
{"created":"2024-05-28 15:07:50","title":"Channel Reciprocity Based Attack Detection for Securing UWB Ranging by Autoencoder","abstract":"A variety of ranging threats represented by Ghost Peak attack have raised concerns regarding the security performance of Ultra-Wide Band (UWB) systems with the finalization of the IEEE 802.15.4z standard. Based on channel reciprocity, this paper proposes a low complexity attack detection scheme that compares Channel Impulse Response (CIR) features of both ranging sides utilizing an autoencoder with the capability of data compression and feature extraction. Taking Ghost Peak attack as an example, this paper demonstrates the effectiveness, feasibility and generalizability of the proposed attack detection scheme through simulation and experimental validation. The proposed scheme achieves an attack detection success rate of over 99% and can be implemented in current systems at low cost.","sentences":["A variety of ranging threats represented by Ghost Peak attack have raised concerns regarding the security performance of Ultra-Wide Band (UWB) systems with the finalization of the IEEE 802.15.4z standard.","Based on channel reciprocity, this paper proposes a low complexity attack detection scheme that compares Channel Impulse Response (CIR) features of both ranging sides utilizing an autoencoder with the capability of data compression and feature extraction.","Taking Ghost Peak attack as an example, this paper demonstrates the effectiveness, feasibility and generalizability of the proposed attack detection scheme through simulation and experimental validation.","The proposed scheme achieves an attack detection success rate of over 99% and can be implemented in current systems at low cost."],"url":"http://arxiv.org/abs/2405.18255v1","category":"cs.CR"}
{"created":"2024-05-28 15:02:09","title":"Sensor-Based Distributionally Robust Control for Safe Robot Navigation in Dynamic Environments","abstract":"We introduce a novel method for safe mobile robot navigation in dynamic, unknown environments, utilizing onboard sensing to impose safety constraints without the need for accurate map reconstruction. Traditional methods typically rely on detailed map information to synthesize safe stabilizing controls for mobile robots, which can be computationally demanding and less effective, particularly in dynamic operational conditions. By leveraging recent advances in distributionally robust optimization, we develop a distributionally robust control barrier function (DR-CBF) constraint that directly processes range sensor data to impose safety constraints. Coupling this with a control Lyapunov function (CLF) for path tracking, we demonstrate that our CLF-DR-CBF control synthesis method achieves safe, efficient, and robust navigation in uncertain dynamic environments. We demonstrate the effectiveness of our approach in simulated and real autonomous robot navigation experiments, marking a substantial advancement in real-time safety guarantees for mobile robots.","sentences":["We introduce a novel method for safe mobile robot navigation in dynamic, unknown environments, utilizing onboard sensing to impose safety constraints without the need for accurate map reconstruction.","Traditional methods typically rely on detailed map information to synthesize safe stabilizing controls for mobile robots, which can be computationally demanding and less effective, particularly in dynamic operational conditions.","By leveraging recent advances in distributionally robust optimization, we develop a distributionally robust control barrier function (DR-CBF) constraint that directly processes range sensor data to impose safety constraints.","Coupling this with a control Lyapunov function (CLF) for path tracking, we demonstrate that our CLF-DR-CBF control synthesis method achieves safe, efficient, and robust navigation in uncertain dynamic environments.","We demonstrate the effectiveness of our approach in simulated and real autonomous robot navigation experiments, marking a substantial advancement in real-time safety guarantees for mobile robots."],"url":"http://arxiv.org/abs/2405.18251v1","category":"cs.RO"}
{"created":"2024-05-28 14:46:03","title":"Position Paper: Think Globally, React Locally -- Bringing Real-time Reference-based Website Phishing Detection on macOS","abstract":"Background. The recent surge in phishing attacks keeps undermining the effectiveness of the traditional anti-phishing blacklist approaches. On-device anti-phishing solutions are gaining popularity as they offer faster phishing detection locally. Aim. We aim to eliminate the delay in recognizing and recording phishing campaigns in databases via on-device solutions that identify phishing sites immediately when encountered by the user rather than waiting for a web crawler's scan to finish. Additionally, utilizing operating system-specific resources and frameworks, we aim to minimize the impact on system performance and depend on local processing to protect user privacy. Method. We propose a phishing detection solution that uses a combination of computer vision and on-device machine learning models to analyze websites in real time. Our reference-based approach analyzes the visual content of webpages, identifying phishing attempts through layout analysis, credential input areas detection, and brand impersonation criteria combination. Results. Our case study shows it's feasible to perform background processing on-device continuously, for the case of the web browser requiring the resource use of 16% of a single CPU core and less than 84MB of RAM on Apple M1 while maintaining the accuracy of brand logo detection at 46.6% (comparable with baselines), and of Credential Requiring Page detection at 98.1% (improving the baseline by 3.1%), within the test dataset. Conclusions. Our results demonstrate the potential of on-device, real-time phishing detection systems to enhance cybersecurity defensive technologies and extend the scope of phishing detection to more similar regions of interest, e.g., email clients and messenger windows.","sentences":["Background.","The recent surge in phishing attacks keeps undermining the effectiveness of the traditional anti-phishing blacklist approaches.","On-device anti-phishing solutions are gaining popularity as they offer faster phishing detection locally.","Aim.","We aim to eliminate the delay in recognizing and recording phishing campaigns in databases via on-device solutions that identify phishing sites immediately when encountered by the user rather than waiting for a web crawler's scan to finish.","Additionally, utilizing operating system-specific resources and frameworks, we aim to minimize the impact on system performance and depend on local processing to protect user privacy.","Method.","We propose a phishing detection solution that uses a combination of computer vision and on-device machine learning models to analyze websites in real time.","Our reference-based approach analyzes the visual content of webpages, identifying phishing attempts through layout analysis, credential input areas detection, and brand impersonation criteria combination.","Results.","Our case study shows it's feasible to perform background processing on-device continuously, for the case of the web browser requiring the resource use of 16% of a single CPU core and less than 84MB of RAM on Apple M1 while maintaining the accuracy of brand logo detection at 46.6% (comparable with baselines), and of Credential Requiring Page detection at 98.1% (improving the baseline by 3.1%), within the test dataset.","Conclusions.","Our results demonstrate the potential of on-device, real-time phishing detection systems to enhance cybersecurity defensive technologies and extend the scope of phishing detection to more similar regions of interest, e.g., email clients and messenger windows."],"url":"http://arxiv.org/abs/2405.18236v1","category":"cs.CR"}
{"created":"2024-05-28 14:45:00","title":"Cooperative Relative Localization in MAV Swarms with Ultra-wideband Ranging","abstract":"Relative localization (RL) is essential for the successful operation of micro air vehicle (MAV) swarms. Achieving accurate 3-D RL in infrastructure-free and GPS-denied environments with only distance information is a challenging problem that has not been satisfactorily solved. In this work, based on the range-based peer-to-peer RL using the ultra-wideband (UWB) ranging technique, we develop a novel UWB-based cooperative relative localization (CRL) solution that integrates the relative motion dynamics of each host-neighbor pair to build a unified dynamic model and takes the distances between the neighbors as \\textit{bonus information}. Observability analysis using differential geometry shows that the proposed CRL scheme can expand the observable subspace compared to other alternatives using only direct distances between the host agent and its neighbors. In addition, we apply the kernel-induced extended Kalman filter (EKF) to the CRL state estimation problem with the novel-designed Logarithmic-Versoria (LV) kernel to tackle heavy-tailed UWB noise. Sufficient conditions for the convergence of the fixed-point iteration involved in the estimation algorithm are also derived. Comparative Monte Carlo simulations demonstrate that the proposed CRL scheme combined with the LV-kernel EKF significantly improves the estimation accuracy owing to its robustness against both measurement outliers and incorrect measurement covariance matrix initialization. Moreover, with the LV kernel, the estimation is still satisfactory when performing the fixed-point iteration only once for reduced computational complexity.","sentences":["Relative localization (RL) is essential for the successful operation of micro air vehicle (MAV) swarms.","Achieving accurate 3-D RL in infrastructure-free and GPS-denied environments with only distance information is a challenging problem that has not been satisfactorily solved.","In this work, based on the range-based peer-to-peer RL using the ultra-wideband (UWB) ranging technique, we develop a novel UWB-based cooperative relative localization (CRL) solution that integrates the relative motion dynamics of each host-neighbor pair to build a unified dynamic model and takes the distances between the neighbors as \\textit{bonus information}.","Observability analysis using differential geometry shows that the proposed CRL scheme can expand the observable subspace compared to other alternatives using only direct distances between the host agent and its neighbors.","In addition, we apply the kernel-induced extended Kalman filter (EKF) to the CRL state estimation problem with the novel-designed Logarithmic-Versoria (LV) kernel to tackle heavy-tailed UWB noise.","Sufficient conditions for the convergence of the fixed-point iteration involved in the estimation algorithm are also derived.","Comparative Monte Carlo simulations demonstrate that the proposed CRL scheme combined with the LV-kernel EKF significantly improves the estimation accuracy owing to its robustness against both measurement outliers and incorrect measurement covariance matrix initialization.","Moreover, with the LV kernel, the estimation is still satisfactory when performing the fixed-point iteration only once for reduced computational complexity."],"url":"http://arxiv.org/abs/2405.18234v1","category":"cs.RO"}
{"created":"2024-05-28 14:34:51","title":"SSLChange: A Self-supervised Change Detection Framework Based on Domain Adaptation","abstract":"In conventional remote sensing change detection (RS CD) procedures, extensive manual labeling for bi-temporal images is first required to maintain the performance of subsequent fully supervised training. However, pixel-level labeling for CD tasks is very complex and time-consuming. In this paper, we explore a novel self-supervised contrastive framework applicable to the RS CD task, which promotes the model to accurately capture spatial, structural, and semantic information through domain adapter and hierarchical contrastive head. The proposed SSLChange framework accomplishes self-learning only by taking a single-temporal sample and can be flexibly transferred to main-stream CD baselines. With self-supervised contrastive learning, feature representation pre-training can be performed directly based on the original data even without labeling. After a certain amount of labels are subsequently obtained, the pre-trained features will be aligned with the labels for fully supervised fine-tuning. Without introducing any additional data or labels, the performance of downstream baselines will experience a significant enhancement. Experimental results on 2 entire datasets and 6 diluted datasets show that our proposed SSLChange improves the performance and stability of CD baseline in data-limited situations. The code of SSLChange will be released at \\url{https://github.com/MarsZhaoYT/SSLChange}","sentences":["In conventional remote sensing change detection (RS CD) procedures, extensive manual labeling for bi-temporal images is first required to maintain the performance of subsequent fully supervised training.","However, pixel-level labeling for CD tasks is very complex and time-consuming.","In this paper, we explore a novel self-supervised contrastive framework applicable to the RS CD task, which promotes the model to accurately capture spatial, structural, and semantic information through domain adapter and hierarchical contrastive head.","The proposed SSLChange framework accomplishes self-learning only by taking a single-temporal sample and can be flexibly transferred to main-stream CD baselines.","With self-supervised contrastive learning, feature representation pre-training can be performed directly based on the original data even without labeling.","After a certain amount of labels are subsequently obtained, the pre-trained features will be aligned with the labels for fully supervised fine-tuning.","Without introducing any additional data or labels, the performance of downstream baselines will experience a significant enhancement.","Experimental results on 2 entire datasets and 6 diluted datasets show that our proposed SSLChange improves the performance and stability of CD baseline in data-limited situations.","The code of SSLChange will be released at \\url{https://github.com/MarsZhaoYT/SSLChange}"],"url":"http://arxiv.org/abs/2405.18224v1","category":"cs.CV"}
{"created":"2024-05-28 14:29:31","title":"Recurrent Natural Policy Gradient for POMDPs","abstract":"In this paper, we study a natural policy gradient method based on recurrent neural networks (RNNs) for partially-observable Markov decision processes, whereby RNNs are used for policy parameterization and policy evaluation to address curse of dimensionality in non-Markovian reinforcement learning. We present finite-time and finite-width analyses for both the critic (recurrent temporal difference learning), and correspondingly-operated recurrent natural policy gradient method in the near-initialization regime. Our analysis demonstrates the efficiency of RNNs for problems with short-term memory with explicit bounds on the required network widths and sample complexity, and points out the challenges in the case of long-term dependencies.","sentences":["In this paper, we study a natural policy gradient method based on recurrent neural networks (RNNs) for partially-observable Markov decision processes, whereby RNNs are used for policy parameterization and policy evaluation to address curse of dimensionality in non-Markovian reinforcement learning.","We present finite-time and finite-width analyses for both the critic (recurrent temporal difference learning), and correspondingly-operated recurrent natural policy gradient method in the near-initialization regime.","Our analysis demonstrates the efficiency of RNNs for problems with short-term memory with explicit bounds on the required network widths and sample complexity, and points out the challenges in the case of long-term dependencies."],"url":"http://arxiv.org/abs/2405.18221v1","category":"math.OC"}
{"created":"2024-05-28 14:20:49","title":"Understanding Inter-Concept Relationships in Concept-Based Models","abstract":"Concept-based explainability methods provide insight into deep learning systems by constructing explanations using human-understandable concepts. While the literature on human reasoning demonstrates that we exploit relationships between concepts when solving tasks, it is unclear whether concept-based methods incorporate the rich structure of inter-concept relationships. We analyse the concept representations learnt by concept-based models to understand whether these models correctly capture inter-concept relationships. First, we empirically demonstrate that state-of-the-art concept-based models produce representations that lack stability and robustness, and such methods fail to capture inter-concept relationships. Then, we develop a novel algorithm which leverages inter-concept relationships to improve concept intervention accuracy, demonstrating how correctly capturing inter-concept relationships can improve downstream tasks.","sentences":["Concept-based explainability methods provide insight into deep learning systems by constructing explanations using human-understandable concepts.","While the literature on human reasoning demonstrates that we exploit relationships between concepts when solving tasks, it is unclear whether concept-based methods incorporate the rich structure of inter-concept relationships.","We analyse the concept representations learnt by concept-based models to understand whether these models correctly capture inter-concept relationships.","First, we empirically demonstrate that state-of-the-art concept-based models produce representations that lack stability and robustness, and such methods fail to capture inter-concept relationships.","Then, we develop a novel algorithm which leverages inter-concept relationships to improve concept intervention accuracy, demonstrating how correctly capturing inter-concept relationships can improve downstream tasks."],"url":"http://arxiv.org/abs/2405.18217v1","category":"cs.LG"}
{"created":"2024-05-28 14:18:22","title":"Tactile-Driven Non-Prehensile Object Manipulation via Extrinsic Contact Mode Control","abstract":"In this paper, we consider the problem of non-prehensile manipulation using grasped objects. This problem is a superset of many common manipulation skills including instances of tool-use (e.g., grasped spatula flipping a burger) and assembly (e.g., screwdriver tightening a screw). Here, we present an algorithmic approach for non-prehensile manipulation leveraging a gripper with highly compliant and high-resolution tactile sensors. Our approach solves for robot actions that drive object poses and forces to desired values while obeying the complex dynamics induced by the sensors as well as the constraints imposed by static equilibrium, object kinematics, and frictional contact. Our method is able to produce a variety of manipulation skills and is amenable to gradient-based optimization by exploiting differentiability within contact modes (e.g., specifications of sticking or sliding contacts). We evaluate 4 variants of controllers that attempt to realize these plans and demonstrate a number of complex skills including non-prehensile planar sliding and pivoting on a variety of object geometries. The perception and controls capabilities that drive these skills are the building blocks towards dexterous and reactive autonomy in unstructured environments.","sentences":["In this paper, we consider the problem of non-prehensile manipulation using grasped objects.","This problem is a superset of many common manipulation skills including instances of tool-use (e.g., grasped spatula flipping a burger) and assembly (e.g., screwdriver tightening a screw).","Here, we present an algorithmic approach for non-prehensile manipulation leveraging a gripper with highly compliant and high-resolution tactile sensors.","Our approach solves for robot actions that drive object poses and forces to desired values while obeying the complex dynamics induced by the sensors as well as the constraints imposed by static equilibrium, object kinematics, and frictional contact.","Our method is able to produce a variety of manipulation skills and is amenable to gradient-based optimization by exploiting differentiability within contact modes (e.g., specifications of sticking or sliding contacts).","We evaluate 4 variants of controllers that attempt to realize these plans and demonstrate a number of complex skills including non-prehensile planar sliding and pivoting on a variety of object geometries.","The perception and controls capabilities that drive these skills are the building blocks towards dexterous and reactive autonomy in unstructured environments."],"url":"http://arxiv.org/abs/2405.18214v1","category":"cs.RO"}
{"created":"2024-05-28 14:15:18","title":"Safe Multi-Agent Reinforcement Learning with Bilevel Optimization in Autonomous Driving","abstract":"Ensuring safety in MARL, particularly when deploying it in real-world applications such as autonomous driving, emerges as a critical challenge. To address this challenge, traditional safe MARL methods extend MARL approaches to incorporate safety considerations, aiming to minimize safety risk values. However, these safe MARL algorithms often fail to model other agents and lack convergence guarantees, particularly in dynamically complex environments. In this study, we propose a safe MARL method grounded in a Stackelberg model with bi-level optimization, for which convergence analysis is provided. Derived from our theoretical analysis, we develop two practical algorithms, namely Constrained Stackelberg Q-learning (CSQ) and Constrained Stackelberg Multi-Agent Deep Deterministic Policy Gradient (CS-MADDPG), designed to facilitate MARL decision-making in autonomous driving applications. To evaluate the effectiveness of our algorithms, we developed a safe MARL autonomous driving benchmark and conducted experiments on challenging autonomous driving scenarios, such as merges, roundabouts, intersections, and racetracks. The experimental results indicate that our algorithms, CSQ and CS-MADDPG, outperform several strong MARL baselines, such as Bi-AC, MACPO, and MAPPO-L, regarding reward and safety performance. The demos and source code are available at {https://github.com/SafeRL-Lab/Safe-MARL-in-Autonomous-Driving.git}.","sentences":["Ensuring safety in MARL, particularly when deploying it in real-world applications such as autonomous driving, emerges as a critical challenge.","To address this challenge, traditional safe MARL methods extend MARL approaches to incorporate safety considerations, aiming to minimize safety risk values.","However, these safe MARL algorithms often fail to model other agents and lack convergence guarantees, particularly in dynamically complex environments.","In this study, we propose a safe MARL method grounded in a Stackelberg model with bi-level optimization, for which convergence analysis is provided.","Derived from our theoretical analysis, we develop two practical algorithms, namely Constrained Stackelberg Q-learning (CSQ) and Constrained Stackelberg Multi-Agent Deep Deterministic Policy Gradient (CS-MADDPG), designed to facilitate MARL decision-making in autonomous driving applications.","To evaluate the effectiveness of our algorithms, we developed a safe MARL autonomous driving benchmark and conducted experiments on challenging autonomous driving scenarios, such as merges, roundabouts, intersections, and racetracks.","The experimental results indicate that our algorithms, CSQ and CS-MADDPG, outperform several strong MARL baselines, such as Bi-AC, MACPO, and MAPPO-L, regarding reward and safety performance.","The demos and source code are available at {https://github.com/SafeRL-Lab/Safe-MARL-in-Autonomous-Driving.git}."],"url":"http://arxiv.org/abs/2405.18209v1","category":"cs.RO"}
{"created":"2024-05-28 14:12:54","title":"Space-Filling Input Design for Nonlinear State-Space Identification","abstract":"The quality of a model resulting from (black-box) system identification is highly dependent on the quality of the data that is used during the identification procedure. Designing experiments for linear time-invariant systems is well understood and mainly focuses on the power spectrum of the input signal. Performing experiment design for nonlinear system identification on the other hand remains an open challenge as informativity of the data depends both on the frequency-domain content and on the time-domain evolution of the input signal. Furthermore, as nonlinear system identification is much more sensitive to modeling and extrapolation errors, having experiments that explore the considered operation range of interest is of high importance. Hence, this paper focuses on designing space-filling experiments i.e., experiments that cover the full operation range of interest, for nonlinear dynamical systems that can be represented in a state-space form using a broad set of input signals. The presented experiment design approach can straightforwardly be extended to a wider range of system classes (e.g., NARMAX). The effectiveness of the proposed approach is illustrated on the experiment design for a nonlinear mass-spring-damper system, using a multisine input signal.","sentences":["The quality of a model resulting from (black-box) system identification is highly dependent on the quality of the data that is used during the identification procedure.","Designing experiments for linear time-invariant systems is well understood and mainly focuses on the power spectrum of the input signal.","Performing experiment design for nonlinear system identification on the other hand remains an open challenge as informativity of the data depends both on the frequency-domain content and on the time-domain evolution of the input signal.","Furthermore, as nonlinear system identification is much more sensitive to modeling and extrapolation errors, having experiments that explore the considered operation range of interest is of high importance.","Hence, this paper focuses on designing space-filling experiments i.e., experiments that cover the full operation range of interest, for nonlinear dynamical systems that can be represented in a state-space form using a broad set of input signals.","The presented experiment design approach can straightforwardly be extended to a wider range of system classes (e.g., NARMAX).","The effectiveness of the proposed approach is illustrated on the experiment design for a nonlinear mass-spring-damper system, using a multisine input signal."],"url":"http://arxiv.org/abs/2405.18207v1","category":"eess.SY"}
{"created":"2024-05-28 14:08:22","title":"Propagation of chaos and phase transition in a stochastic model for a social network","abstract":"We consider a model for a social network with N interacting social actors. This model is a system of interacting marked point processes in which each point process indicates the successive times in which a social actor expresses a \"favorable\" (+1) or \"contrary\" (-1) opinion. The orientation and the rate at which an actor expresses an opinion is influenced by the social pressure exerted on this actor. The social pressure of an actor is reset to 0 when the actor expresses an opinion, and simultaneously the social pressures on all the other actors change by h/N in the direction of the opinion that was just expressed.   We prove propagation of chaos of the system, as N diverges to infinity, to a limit nonlinear jumping stochastic differential equation. Moreover, we prove that under certain conditions the limit system exhibits a phase transition described as follows. If h is smaller or equal than a certain threshold, the limit system has only the null Dirac measure as an invariant probability measure, corresponding to a vanishing social pressure on all actors. However, if h is greater than the threshold, the system has two additional non-trivial invariant probability measures. One of these measures has support on the positive real numbers and the other is obtained by symmetrization with respect to 0, having thus support on the negative real numbers.","sentences":["We consider a model for a social network with N interacting social actors.","This model is a system of interacting marked point processes in which each point process indicates the successive times in which a social actor expresses a \"favorable\" (+1) or \"contrary\" (-1) opinion.","The orientation and the rate at which an actor expresses an opinion is influenced by the social pressure exerted on this actor.","The social pressure of an actor is reset to 0 when the actor expresses an opinion, and simultaneously the social pressures on all the other actors change by h/N in the direction of the opinion that was just expressed.   ","We prove propagation of chaos of the system, as N diverges to infinity, to a limit nonlinear jumping stochastic differential equation.","Moreover, we prove that under certain conditions the limit system exhibits a phase transition described as follows.","If h is smaller or equal than a certain threshold, the limit system has only the null Dirac measure as an invariant probability measure, corresponding to a vanishing social pressure on all actors.","However, if h is greater than the threshold, the system has two additional non-trivial invariant probability measures.","One of these measures has support on the positive real numbers and the other is obtained by symmetrization with respect to 0, having thus support on the negative real numbers."],"url":"http://arxiv.org/abs/2405.18200v1","category":"math.PR"}
{"created":"2024-05-28 14:08:04","title":"Adam with model exponential moving average is effective for nonconvex optimization","abstract":"In this work, we offer a theoretical analysis of two modern optimization techniques for training large and complex models: (i) adaptive optimization algorithms, such as Adam, and (ii) the model exponential moving average (EMA). Specifically, we demonstrate that a clipped version of Adam with model EMA achieves the optimal convergence rates in various nonconvex optimization settings, both smooth and nonsmooth. Moreover, when the scale varies significantly across different coordinates, we demonstrate that the coordinate-wise adaptivity of Adam is provably advantageous. Notably, unlike previous analyses of Adam, our analysis crucially relies on its core elements -- momentum and discounting factors -- as well as model EMA, motivating their wide applications in practice.","sentences":["In this work, we offer a theoretical analysis of two modern optimization techniques for training large and complex models: (i) adaptive optimization algorithms, such as Adam, and (ii) the model exponential moving average (EMA).","Specifically, we demonstrate that a clipped version of Adam with model EMA achieves the optimal convergence rates in various nonconvex optimization settings, both smooth and nonsmooth.","Moreover, when the scale varies significantly across different coordinates, we demonstrate that the coordinate-wise adaptivity of Adam is provably advantageous.","Notably, unlike previous analyses of Adam, our analysis crucially relies on its core elements -- momentum and discounting factors -- as well as model EMA, motivating their wide applications in practice."],"url":"http://arxiv.org/abs/2405.18199v1","category":"cs.LG"}
{"created":"2024-05-28 14:06:36","title":"The dynamic of the positons for the reverse space-time nonlocal short pulse equation","abstract":"In this paper, the Darboux transformation (DT) of the reverse space-time (RST) nonlocal short pulse equation is constructed by a hodograph transformation and the eigenfunctions of its Lax pair. The multi-soliton solutions of the RST nonlocal short pulse equation are produced through the DT, which can be expressed in terms of determinant representation. By taking different values of eigenvalues, bounded soliton solutions and unbounded soliton solutions can be obtained. In addition, based on the degenerate Darboux transformation, the $N$-positon solutions of the RST nonlocal short pulse equation are computed from the determinant expression of the multi-soliton solution. Furthermore, different kinds of mixed solutions are also presented, and the interaction properties between positons and solitons are investigated.","sentences":["In this paper, the Darboux transformation (DT) of the reverse space-time (RST) nonlocal short pulse equation is constructed by a hodograph transformation and the eigenfunctions of its Lax pair.","The multi-soliton solutions of the RST nonlocal short pulse equation are produced through the DT, which can be expressed in terms of determinant representation.","By taking different values of eigenvalues, bounded soliton solutions and unbounded soliton solutions can be obtained.","In addition, based on the degenerate Darboux transformation, the $N$-positon solutions of the RST nonlocal short pulse equation are computed from the determinant expression of the multi-soliton solution.","Furthermore, different kinds of mixed solutions are also presented, and the interaction properties between positons and solitons are investigated."],"url":"http://arxiv.org/abs/2405.18197v1","category":"nlin.SI"}
{"created":"2024-05-28 14:05:13","title":"Importance Sampling for counting statistics in one-dimensional systems","abstract":"In this paper, we consider the problem of numerical investigation of the counting statistics for a class of one-dimensional systems. Importance sampling, the cornerstone technique usually implemented for such problems, critically hinges on selecting an appropriate biased distribution. While exponential tilt in the observable stands as the conventional choice for various problems, its efficiency in the context of counting statistics may be significantly hindered by the genuine discreteness of the observable. To address this challenge, we propose an alternative strategy which we call importance sampling with the local tilt. We demonstrate the efficiency of the proposed approach through the analysis of three prototypical examples: a set of independent Gaussian random variables, Dyson gas, and Symmetric Simple Exclusion Process (SSEP) with a steplike initial condition.","sentences":["In this paper, we consider the problem of numerical investigation of the counting statistics for a class of one-dimensional systems.","Importance sampling, the cornerstone technique usually implemented for such problems, critically hinges on selecting an appropriate biased distribution.","While exponential tilt in the observable stands as the conventional choice for various problems, its efficiency in the context of counting statistics may be significantly hindered by the genuine discreteness of the observable.","To address this challenge, we propose an alternative strategy which we call importance sampling with the local tilt.","We demonstrate the efficiency of the proposed approach through the analysis of three prototypical examples: a set of independent Gaussian random variables, Dyson gas, and Symmetric Simple Exclusion Process (SSEP) with a steplike initial condition."],"url":"http://arxiv.org/abs/2405.18195v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-28 14:01:36","title":"Characterizing dynamical criticality of many-body localization transitions from the Fock-space perspective","abstract":"Characterizing the nature of many-body localization transitions (MBLTs) and their potential critical behaviors has remained a challenging problem. In this work, we study the dynamics of the displacement, quantifying the spread of the radial probability distribution in the Fock space, for systems with MBLTs, and perform a finite-size scaling analysis. We find that the scaling exponents satisfy theoretical bounds, and can identify universality classes. We show that reliable extrapolations to the thermodynamic limit for the MBLT induced by quasiperiodic fields is possible even for computationally accessible system sizes. Our work highlights that the displacement is a valuable tool for studying MBLTs, as relevant to ongoing experimental efforts.","sentences":["Characterizing the nature of many-body localization transitions (MBLTs) and their potential critical behaviors has remained a challenging problem.","In this work, we study the dynamics of the displacement, quantifying the spread of the radial probability distribution in the Fock space, for systems with MBLTs, and perform a finite-size scaling analysis.","We find that the scaling exponents satisfy theoretical bounds, and can identify universality classes.","We show that reliable extrapolations to the thermodynamic limit for the MBLT induced by quasiperiodic fields is possible even for computationally accessible system sizes.","Our work highlights that the displacement is a valuable tool for studying MBLTs, as relevant to ongoing experimental efforts."],"url":"http://arxiv.org/abs/2405.18188v1","category":"quant-ph"}
{"created":"2024-05-28 14:01:03","title":"AlignIQL: Policy Alignment in Implicit Q-Learning through Constrained Optimization","abstract":"Implicit Q-learning (IQL) serves as a strong baseline for offline RL, which learns the value function using only dataset actions through quantile regression. However, it is unclear how to recover the implicit policy from the learned implicit Q-function and why IQL can utilize weighted regression for policy extraction. IDQL reinterprets IQL as an actor-critic method and gets weights of implicit policy, however, this weight only holds for the optimal value function. In this work, we introduce a different way to solve the implicit policy-finding problem (IPF) by formulating this problem as an optimization problem. Based on this optimization problem, we further propose two practical algorithms AlignIQL and AlignIQL-hard, which inherit the advantages of decoupling actor from critic in IQL and provide insights into why IQL can use weighted regression for policy extraction. Compared with IQL and IDQL, we find our method keeps the simplicity of IQL and solves the implicit policy-finding problem. Experimental results on D4RL datasets show that our method achieves competitive or superior results compared with other SOTA offline RL methods. Especially in complex sparse reward tasks like Antmaze and Adroit, our method outperforms IQL and IDQL by a significant margin.","sentences":["Implicit Q-learning (IQL) serves as a strong baseline for offline RL, which learns the value function using only dataset actions through quantile regression.","However, it is unclear how to recover the implicit policy from the learned implicit Q-function and why IQL can utilize weighted regression for policy extraction.","IDQL reinterprets IQL as an actor-critic method and gets weights of implicit policy, however, this weight only holds for the optimal value function.","In this work, we introduce a different way to solve the implicit policy-finding problem (IPF) by formulating this problem as an optimization problem.","Based on this optimization problem, we further propose two practical algorithms AlignIQL and AlignIQL-hard, which inherit the advantages of decoupling actor from critic in IQL and provide insights into why IQL can use weighted regression for policy extraction.","Compared with IQL and IDQL, we find our method keeps the simplicity of IQL and solves the implicit policy-finding problem.","Experimental results on D4RL datasets show that our method achieves competitive or superior results compared with other SOTA offline RL methods.","Especially in complex sparse reward tasks like Antmaze and Adroit, our method outperforms IQL and IDQL by a significant margin."],"url":"http://arxiv.org/abs/2405.18187v1","category":"cs.LG"}
{"created":"2024-05-28 13:48:36","title":"Towards Practicable Algorithms for Rewriting Graph Queries beyond DL-Lite","abstract":"Despite the many advantages that ontology-based data access (OBDA) has brought to a range of application domains, state-of-the-art OBDA systems still do not support popular graph database management systems such as Neo4j. Algorithms for query rewriting focus on languages like conjunctive queries and their unions, which are fragments of first-order logic and were developed for relational data. Such query languages are poorly suited for querying graph data. Moreover, they also limit the expressiveness of the ontology languages that admit rewritings, restricting them to those where the data complexity of reasoning is not higher than it is in first-order logic. In this paper, we propose a technique for rewriting a family of navigational queries for a suitably restricted fragment of ELHI that extends DL-Lite and that is NL-complete in data complexity. We implemented a proof-of-concept prototype that rewrites into Cypher queries, and tested it on a real-world cognitive neuroscience use case with promising results.","sentences":["Despite the many advantages that ontology-based data access (OBDA) has brought to a range of application domains, state-of-the-art OBDA systems still do not support popular graph database management systems such as Neo4j.","Algorithms for query rewriting focus on languages like conjunctive queries and their unions, which are fragments of first-order logic and were developed for relational data.","Such query languages are poorly suited for querying graph data.","Moreover, they also limit the expressiveness of the ontology languages that admit rewritings, restricting them to those where the data complexity of reasoning is not higher than it is in first-order logic.","In this paper, we propose a technique for rewriting a family of navigational queries for a suitably restricted fragment of ELHI that extends DL-Lite and that is NL-complete in data complexity.","We implemented a proof-of-concept prototype that rewrites into Cypher queries, and tested it on a real-world cognitive neuroscience use case with promising results."],"url":"http://arxiv.org/abs/2405.18181v1","category":"cs.DB"}
{"created":"2024-05-28 13:39:36","title":"Bulk viscosity of the rigid rotor one-component plasma","abstract":"Bulk viscosity of a plasma consisting of strongly coupled diatomic ions is computed using molecular dynamics simulations. The simulations are based on the rigid rotor one-component plasma, which is introduced as a model system that adds two degrees of molecular rotation to the traditional one-component plasma. It is characterized by two parameters: the Coulomb coupling parameter, $\\Gamma$, and the bond length parameter, $\\Omega$. Results show that the long-range nature of the Coulomb potential can lead to long rotational relaxation times, which in turn yield large values for bulk viscosity. The bulk-to-shear viscosity ratio is found to span from small to large values depending on the values of $\\Gamma$ and $\\Omega$. Although bulk viscosity is often neglected in plasma modeling, these results motivate that it can be large in molecular plasmas with rotational degrees of freedom.","sentences":["Bulk viscosity of a plasma consisting of strongly coupled diatomic ions is computed using molecular dynamics simulations.","The simulations are based on the rigid rotor one-component plasma, which is introduced as a model system that adds two degrees of molecular rotation to the traditional one-component plasma.","It is characterized by two parameters: the Coulomb coupling parameter, $\\Gamma$, and the bond length parameter, $\\Omega$. Results show that the long-range nature of the Coulomb potential can lead to long rotational relaxation times, which in turn yield large values for bulk viscosity.","The bulk-to-shear viscosity ratio is found to span from small to large values depending on the values of $\\Gamma$ and $\\Omega$. Although bulk viscosity is often neglected in plasma modeling, these results motivate that it can be large in molecular plasmas with rotational degrees of freedom."],"url":"http://arxiv.org/abs/2405.18175v1","category":"physics.plasm-ph"}
{"created":"2024-05-28 13:31:55","title":"Computing hydration free energies of small molecules with first principles accuracy","abstract":"Free energies play a central role in characterising the behaviour of chemical systems and are among the most important quantities that can be calculated by molecular dynamics simulations. The free energy of hydration in particular is a well-studied physicochemical property of drug-like molecules and is commonly used to assess and optimise the accuracy of nonbonded parameters in empirical forcefields, and as a fast-to-compute surrogate of performance for protein-ligand binding free energy estimation. Machine learned potentials (MLPs) show great promise as more accurate alternatives to empirical forcefields, but are not readily decomposed into physically motivated functional forms, which has thus far rendered them incompatible with standard alchemical free energy methods that manipulate individual pairwise interaction terms. However, since the accuracy of free energy calculations is highly sensitive to the forcefield, this is a key area in which MLPs have the potential to address the shortcomings of empirical forcefields. In this work, we introduce an efficient alchemical free energy method compatible with MLPs, enabling, for the first time, calculations of biomolecular free energy with \\textit{ab initio} accuracy. Using a pretrained, transferrable, alchemically equipped MACE model, we demonstrate sub-chemical accuracy for the hydration free energies of organic molecules.","sentences":["Free energies play a central role in characterising the behaviour of chemical systems and are among the most important quantities that can be calculated by molecular dynamics simulations.","The free energy of hydration in particular is a well-studied physicochemical property of drug-like molecules and is commonly used to assess and optimise the accuracy of nonbonded parameters in empirical forcefields, and as a fast-to-compute surrogate of performance for protein-ligand binding free energy estimation.","Machine learned potentials (MLPs) show great promise as more accurate alternatives to empirical forcefields, but are not readily decomposed into physically motivated functional forms, which has thus far rendered them incompatible with standard alchemical free energy methods that manipulate individual pairwise interaction terms.","However, since the accuracy of free energy calculations is highly sensitive to the forcefield, this is a key area in which MLPs have the potential to address the shortcomings of empirical forcefields.","In this work, we introduce an efficient alchemical free energy method compatible with MLPs, enabling, for the first time, calculations of biomolecular free energy with \\textit{ab initio} accuracy.","Using a pretrained, transferrable, alchemically equipped MACE model, we demonstrate sub-chemical accuracy for the hydration free energies of organic molecules."],"url":"http://arxiv.org/abs/2405.18171v1","category":"physics.chem-ph"}
{"created":"2024-05-28 13:29:31","title":"Efficient Adaptable Streaming Aggregation Engine","abstract":"Aggregation queries are a series of computationally-demanding analytics operations on grouped and/or time series (streaming) data. They include tasks such as summation or finding the mean among the items of a group (sharing a group ID) or within the last N observed tuples. They have a wide range of applications including in database analytics, operating systems, bank security and medical sensors. Existing challenges include the increased hardware utilisation and random memory access patterns that result from hash-based approaches or multi-tasking as a way to introduce parallelism. There are also challenges relating to the degree of which the function can be calculated incrementally for sliding windows, such as with overlapping windows. This paper presents a pipelined and reconfigurable approach for calculating a wide range of aggregation queries with minimal hardware overhead.","sentences":["Aggregation queries are a series of computationally-demanding analytics operations on grouped and/or time series (streaming) data.","They include tasks such as summation or finding the mean among the items of a group (sharing a group ID) or within the last N observed tuples.","They have a wide range of applications including in database analytics, operating systems, bank security and medical sensors.","Existing challenges include the increased hardware utilisation and random memory access patterns that result from hash-based approaches or multi-tasking as a way to introduce parallelism.","There are also challenges relating to the degree of which the function can be calculated incrementally for sliding windows, such as with overlapping windows.","This paper presents a pipelined and reconfigurable approach for calculating a wide range of aggregation queries with minimal hardware overhead."],"url":"http://arxiv.org/abs/2405.18168v1","category":"cs.AR"}
{"created":"2024-05-28 13:24:25","title":"NegGS: Negative Gaussian Splatting","abstract":"One of the key advantages of 3D rendering is its ability to simulate intricate scenes accurately. One of the most widely used methods for this purpose is Gaussian Splatting, a novel approach that is known for its rapid training and inference capabilities. In essence, Gaussian Splatting involves incorporating data about the 3D objects of interest into a series of Gaussian distributions, each of which can then be depicted in 3D in a manner analogous to traditional meshes. It is regrettable that the use of Gaussians in Gaussian Splatting is currently somewhat restrictive due to their perceived linear nature. In practice, 3D objects are often composed of complex curves and highly nonlinear structures. This issue can to some extent be alleviated by employing a multitude of Gaussian components to reflect the complex, nonlinear structures accurately. However, this approach results in a considerable increase in time complexity. This paper introduces the concept of negative Gaussians, which are interpreted as items with negative colors. The rationale behind this approach is based on the density distribution created by dividing the probability density functions (PDFs) of two Gaussians, which we refer to as Diff-Gaussian. Such a distribution can be used to approximate structures such as donut and moon-shaped datasets. Experimental findings indicate that the application of these techniques enhances the modeling of high-frequency elements with rapid color transitions. Additionally, it improves the representation of shadows. To the best of our knowledge, this is the first paper to extend the simple elipsoid shapes of Gaussian Splatting to more complex nonlinear structures.","sentences":["One of the key advantages of 3D rendering is its ability to simulate intricate scenes accurately.","One of the most widely used methods for this purpose is Gaussian Splatting, a novel approach that is known for its rapid training and inference capabilities.","In essence, Gaussian Splatting involves incorporating data about the 3D objects of interest into a series of Gaussian distributions, each of which can then be depicted in 3D in a manner analogous to traditional meshes.","It is regrettable that the use of Gaussians in Gaussian Splatting is currently somewhat restrictive due to their perceived linear nature.","In practice, 3D objects are often composed of complex curves and highly nonlinear structures.","This issue can to some extent be alleviated by employing a multitude of Gaussian components to reflect the complex, nonlinear structures accurately.","However, this approach results in a considerable increase in time complexity.","This paper introduces the concept of negative Gaussians, which are interpreted as items with negative colors.","The rationale behind this approach is based on the density distribution created by dividing the probability density functions (PDFs) of two Gaussians, which we refer to as Diff-Gaussian.","Such a distribution can be used to approximate structures such as donut and moon-shaped datasets.","Experimental findings indicate that the application of these techniques enhances the modeling of high-frequency elements with rapid color transitions.","Additionally, it improves the representation of shadows.","To the best of our knowledge, this is the first paper to extend the simple elipsoid shapes of Gaussian Splatting to more complex nonlinear structures."],"url":"http://arxiv.org/abs/2405.18163v1","category":"cs.GR"}
{"created":"2024-05-28 13:10:49","title":"Momentum-resolved electronic structures and strong electronic correlations in graphene-like nitride superconductors","abstract":"Although transition-metal nitrides have been widely applied for several decades, experimental investigations of their high-resolution electronic band structures are rare due to the lack of high-quality single-crystalline samples. Here, we report on the first momentum-resolved electronic band structures of titanium nitride (TiN) films, a remarkable nitride superconductor. The measurements of crystal structures and electrical transport properties confirmed the high quality of these films. More importantly, with a combination of high-resolution angle-resolved photoelectron spectroscopy and the first-principles calculations, the extracted Coulomb interaction strength of TiN films can be as large as 8.5 eV, whereas resonant photoemission spectroscopy yields a value of 6.26 eV. These large values of Coulomb interaction strength indicate that superconducting TiN is a strongly correlated system. Our results uncover the unexpected electronic correlations in transition-metal nitrides, potentially providing a perspective not only to understand their emergent quantum states but also to develop their applications in quantum devices.","sentences":["Although transition-metal nitrides have been widely applied for several decades, experimental investigations of their high-resolution electronic band structures are rare due to the lack of high-quality single-crystalline samples.","Here, we report on the first momentum-resolved electronic band structures of titanium nitride (TiN) films, a remarkable nitride superconductor.","The measurements of crystal structures and electrical transport properties confirmed the high quality of these films.","More importantly, with a combination of high-resolution angle-resolved photoelectron spectroscopy and the first-principles calculations, the extracted Coulomb interaction strength of TiN films can be as large as 8.5 eV, whereas resonant photoemission spectroscopy yields a value of 6.26 eV. These large values of Coulomb interaction strength indicate that superconducting TiN is a strongly correlated system.","Our results uncover the unexpected electronic correlations in transition-metal nitrides, potentially providing a perspective not only to understand their emergent quantum states but also to develop their applications in quantum devices."],"url":"http://arxiv.org/abs/2405.18150v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-28 13:06:55","title":"All-mirror wavefront division interferometer for mid-infrared spectrometry","abstract":"In this contribution, we address the core of any Fourier transform (FT) spectrometer$\\unicode{x2013}$the interferometer$\\unicode{x2013}$in perspective of the recent emergence of spatially coherent broadband infrared (IR) sources. As a result, we report on the design of a wavefront-division interferometer for spectroscopic applications in the mid-IR and beyond. The theoretical framework of the proposed wavefront division interferometer is discussed, and an analytical solution to determine the far-field interference pattern is derived. The solution is verified by both optical propagation simulations and experimentally. In view of the practical significance, we apply the wavefront division interferometer for FTIR spectroscopy. It features a simple architecture, ultra-broad achromaticity (limited only by the spectral profiles of the mirrors), high optical throughput, variable arms split ratio, and a two-fold increase in scan length and spectral resolution (demonstrated up to 0.2 $cm^{-1}$), respectively. Further, the employed design inherently enables the measurement of the complex refractive index. Experimental verification of the mentioned properties is provided by coupling the spectrometer with a mid-IR supercontinuum source for various applied spectroscopic studies: high-resolution transmission measurements of polymers (polypropylene) and gas (methane), as well as reflectance measurements of dried pharmaceuticals (insulin products on a metal surface).","sentences":["In this contribution, we address the core of any Fourier transform (FT) spectrometer$\\unicode{x2013}$the interferometer$\\unicode{x2013}$in perspective of the recent emergence of spatially coherent broadband infrared (IR) sources.","As a result, we report on the design of a wavefront-division interferometer for spectroscopic applications in the mid-IR and beyond.","The theoretical framework of the proposed wavefront division interferometer is discussed, and an analytical solution to determine the far-field interference pattern is derived.","The solution is verified by both optical propagation simulations and experimentally.","In view of the practical significance, we apply the wavefront division interferometer for FTIR spectroscopy.","It features a simple architecture, ultra-broad achromaticity (limited only by the spectral profiles of the mirrors), high optical throughput, variable arms split ratio, and a two-fold increase in scan length and spectral resolution (demonstrated up to 0.2 $cm^{-1}$), respectively.","Further, the employed design inherently enables the measurement of the complex refractive index.","Experimental verification of the mentioned properties is provided by coupling the spectrometer with a mid-IR supercontinuum source for various applied spectroscopic studies: high-resolution transmission measurements of polymers (polypropylene) and gas (methane), as well as reflectance measurements of dried pharmaceuticals (insulin products on a metal surface)."],"url":"http://arxiv.org/abs/2405.18147v1","category":"physics.optics"}
{"created":"2024-05-28 13:03:37","title":"Quantum subsystem codes, CFTs and their $\\mathbb{Z}_2$-gaugings","abstract":"We construct Narain conformal field theories (CFTs) from quantum subsystem codes, a more comprehensive class of quantum error-correcting codes than quantum stabilizer codes, for qudit systems of prime dimensions. The resulting code CFTs exhibit a global $\\mathbb{Z}_2$ symmetry, enabling us to perform the $\\mathbb{Z}_2$-gauging to derive their orbifolded and fermionized theories when the symmetry is non-anomalous. We classify a subset of these subsystem code CFTs using weighted oriented graphs and enumerate those with small central charges. Consequently, we identify several bosonic code CFTs self-dual under the $\\mathbb{Z}_2$-orbifold, new supersymmetric code CFTs, and a few fermionic code CFTs with spontaneously broken supersymmetry.","sentences":["We construct Narain conformal field theories (CFTs) from quantum subsystem codes, a more comprehensive class of quantum error-correcting codes than quantum stabilizer codes, for qudit systems of prime dimensions.","The resulting code CFTs exhibit a global $\\mathbb{Z}_2$ symmetry, enabling us to perform the $\\mathbb{Z}_2$-gauging to derive their orbifolded and fermionized theories when the symmetry is non-anomalous.","We classify a subset of these subsystem code CFTs using weighted oriented graphs and enumerate those with small central charges.","Consequently, we identify several bosonic code CFTs self-dual under the $\\mathbb{Z}_2$-orbifold, new supersymmetric code CFTs, and a few fermionic code CFTs with spontaneously broken supersymmetry."],"url":"http://arxiv.org/abs/2405.18145v1","category":"hep-th"}
{"created":"2024-05-28 13:02:54","title":"Simon algorithm in measurement-based quantum computing","abstract":"Simon's hidden subgroup algorithm was the first quantum algorithm to prove the superiority of quantum computing over classical computing in terms of complexity. Measurement-based quantum computing (MBQC) is a formulation of quantum computing that, while equivalent in terms of computational power, can be advantageous in experiments and in displaying the core mechanics of quantum algorithms. We present a reformulation of the Simon algorithm into the language of MBQC -- in detail for two qubits and schematically for $n$ qubits. We utilize the framework of ZX-calculus, a graphical tensor description of quantum states and operators, to translate the circuit description of the algorithm into a form concordant with MBQC. The result for the two-qubit Simon algorithm is a ten-qubit cluster state on which single-qubit measurements suffice to extract the desired information. Additionally, we show that the $n$-qubit version of the Simon algorithm can be formulated in MBQC as cluster state graph with $2n$ nodes and $n^2$ edges. This is an example of the MBQC formulation of a quantum algorithm that is exponentially faster than its classical counterpart. As such, this formulation should aid in understanding the core mechanics of such an established algorithm and could serve as a blueprint for experimental implementation.","sentences":["Simon's hidden subgroup algorithm was the first quantum algorithm to prove the superiority of quantum computing over classical computing in terms of complexity.","Measurement-based quantum computing (MBQC) is a formulation of quantum computing that, while equivalent in terms of computational power, can be advantageous in experiments and in displaying the core mechanics of quantum algorithms.","We present a reformulation of the Simon algorithm into the language of MBQC -- in detail for two qubits and schematically for $n$ qubits.","We utilize the framework of ZX-calculus, a graphical tensor description of quantum states and operators, to translate the circuit description of the algorithm into a form concordant with MBQC.","The result for the two-qubit Simon algorithm is a ten-qubit cluster state on which single-qubit measurements suffice to extract the desired information.","Additionally, we show that the $n$-qubit version of the Simon algorithm can be formulated in MBQC as cluster state graph with $2n$ nodes and $n^2$ edges.","This is an example of the MBQC formulation of a quantum algorithm that is exponentially faster than its classical counterpart.","As such, this formulation should aid in understanding the core mechanics of such an established algorithm and could serve as a blueprint for experimental implementation."],"url":"http://arxiv.org/abs/2405.18143v1","category":"quant-ph"}
{"created":"2024-05-28 12:48:47","title":"Bringing Rust to Safety-Critical Systems in Space","abstract":"The development of safety-critical aerospace systems is traditionally dominated by the C language. Its language characteristics make it trivial to accidentally introduce memory safety issues resulting in undefined behavior or security vulnerabilities. The Rust language aims to drastically reduce the chance of introducing bugs and consequently produces overall more secure and safer code. However, due to its relatively short lifespan, industry adaption in safety-critical environments is still lacking. This work provides a set of recommendations for the development of safety-critical space systems in Rust. Our recommendations are based on insights from our multi-fold contributions towards safer and more secure aerospace systems: We provide a comprehensive overview of ongoing efforts to adapt Rust for safety-critical system programming, highlighting its potential to enhance system robustness. Next, we introduce a procedure for partially rewriting C-based systems in Rust, offering a pragmatic pathway to improving safety without necessitating a full system overhaul. During the execution of our rewriting case study, we identify and fix three previously undiscovered vulnerabilities in a popular open-source satellite communication protocol. Finally, we introduce a new Rust compiler target configuration for bare metal PowerPC. With this, we aim to broaden Rust's applicability in space-oriented projects, as the architecture is commonly encountered in the domain, e.g., in the James Webb Space Telescope.","sentences":["The development of safety-critical aerospace systems is traditionally dominated by the C language.","Its language characteristics make it trivial to accidentally introduce memory safety issues resulting in undefined behavior or security vulnerabilities.","The Rust language aims to drastically reduce the chance of introducing bugs and consequently produces overall more secure and safer code.","However, due to its relatively short lifespan, industry adaption in safety-critical environments is still lacking.","This work provides a set of recommendations for the development of safety-critical space systems in Rust.","Our recommendations are based on insights from our multi-fold contributions towards safer and more secure aerospace systems: We provide a comprehensive overview of ongoing efforts to adapt Rust for safety-critical system programming, highlighting its potential to enhance system robustness.","Next, we introduce a procedure for partially rewriting C-based systems in Rust, offering a pragmatic pathway to improving safety without necessitating a full system overhaul.","During the execution of our rewriting case study, we identify and fix three previously undiscovered vulnerabilities in a popular open-source satellite communication protocol.","Finally, we introduce a new Rust compiler target configuration for bare metal PowerPC.","With this, we aim to broaden Rust's applicability in space-oriented projects, as the architecture is commonly encountered in the domain, e.g., in the James Webb Space Telescope."],"url":"http://arxiv.org/abs/2405.18135v1","category":"cs.CR"}
{"created":"2024-05-28 12:27:30","title":"Acoustic wake in an isothermal profile: dynamical friction and gravitational wave emission","abstract":"We consider the motion of a circularly-moving perturber in a self-gravitating, collisional system with spherically symmetric density profile. We concentrate on the singular isothermal sphere which, despite its pathological features, admits a simple polarization function in linear response theory. This allows us to solve for the acoustic wake trailing the perturber and the resulting dynamical friction, in the limit where the self-gravity of the response can be ignored. In steady-state and for subsonic velocities $v_p<c_s$, the dynamical friction torque $F_\\varphi\\propto v_p^3$ is suppressed for perturbers orbiting in an isothermal sphere relative to the infinite, homogeneous medium expectation $F_\\varphi\\propto v_p$. For highly supersonic motions, both expectations agree and are consistent with a local approximation to the gravitational torque. At fixed resolution (a given Coulomb logarithm), the response of the system is maximal for Mach numbers near the constant circular velocity of the singular isothermal profile. This resonance maximizes the gravitational wave (GW) emission produced by the trailing acoustic wake. For an inspiral around a massive black hole of mass $10^6 M_\\odot$ located at the center of a (truncated) isothermal sphere, this GW signal could be comparable to the vacuum GW emission of the black hole binary at sub-nanohertz frequencies when the small black hole enters the Bondi sphere of the massive one. The exact magnitude of this effect depends on departures from hydrostatic equilibrium and on the viscosity present in any realistic astrophysical fluid, which are not included in our simplified description.","sentences":["We consider the motion of a circularly-moving perturber in a self-gravitating, collisional system with spherically symmetric density profile.","We concentrate on the singular isothermal sphere which, despite its pathological features, admits a simple polarization function in linear response theory.","This allows us to solve for the acoustic wake trailing the perturber and the resulting dynamical friction, in the limit where the self-gravity of the response can be ignored.","In steady-state and for subsonic velocities $v_p<c_s$, the dynamical friction torque $F_\\varphi\\propto v_p^3$ is suppressed for perturbers orbiting in an isothermal sphere relative to the infinite, homogeneous medium expectation $F_\\varphi\\propto v_p$. For highly supersonic motions, both expectations agree and are consistent with a local approximation to the gravitational torque.","At fixed resolution (a given Coulomb logarithm), the response of the system is maximal for Mach numbers near the constant circular velocity of the singular isothermal profile.","This resonance maximizes the gravitational wave (GW) emission produced by the trailing acoustic wake.","For an inspiral around a massive black hole of mass $10^6 M_\\odot$ located at the center of a (truncated) isothermal sphere, this GW signal could be comparable to the vacuum GW emission of the black hole binary at sub-nanohertz frequencies when the small black hole enters the Bondi sphere of the massive one.","The exact magnitude of this effect depends on departures from hydrostatic equilibrium and on the viscosity present in any realistic astrophysical fluid, which are not included in our simplified description."],"url":"http://arxiv.org/abs/2405.18117v1","category":"astro-ph.GA"}
{"created":"2024-05-28 12:26:02","title":"Emergent Inequalities in a Primitive Agent-Based Good-Exchange Model","abstract":"Rising inequalities around the globe bring into question our economic systems and the origin of such inequalities. Here we propose a toy agent-based model where each entity is simultaneously producing and consuming indivisible goods. We find that the system exhibits a non-trivial phase transition beyond which a market clearing equilibrium exists but becomes dynamically unreachable. When production capacity exceeds a threshold and adapts too slowly, some agents cannot sell all their goods. This leads to global price deflation and induces strong wealth inequalities, with the spontaneous separation of the population into a rich class and a poor class. We explore ways to alleviate poverty in this model and whether they have real life significance.","sentences":["Rising inequalities around the globe bring into question our economic systems and the origin of such inequalities.","Here we propose a toy agent-based model where each entity is simultaneously producing and consuming indivisible goods.","We find that the system exhibits a non-trivial phase transition beyond which a market clearing equilibrium exists but becomes dynamically unreachable.","When production capacity exceeds a threshold and adapts too slowly, some agents cannot sell all their goods.","This leads to global price deflation and induces strong wealth inequalities, with the spontaneous separation of the population into a rich class and a poor class.","We explore ways to alleviate poverty in this model and whether they have real life significance."],"url":"http://arxiv.org/abs/2405.18116v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-28 12:15:05","title":"Strong coupling at room temperature with a centimeter-scale quartz crystal","abstract":"Brillouin-based optomechanical systems with high-frequency acoustic modes provide a promising platform for implementing quantum-information processing and wavelength conversion applications, and for probing macroscopic quantum effects. Achieving strong coupling through electrostrictive Brillouin interaction is essential for coupling the massive mechanical mode to an optical field, thereby controlling and characterizing the mechanical state. However, achieving strong coupling at room temperature has proven challenging due to fast mechanical decay rates, which increase the pumping power required to surpass the coupling threshold. Here, we report an optomechanical system with independent control over pumping power and frequency detuning to achieve and characterize the strong-coupling regime of a bulk acoustic-wave resonator. Through spectral analysis of the cavity reflectivity, we identify clear signatures of strong coupling, i.e., normal-mode splitting and an avoided crossing in the detuned spectra, while estimating the mechanical linewidth $\\Gamma_m/2\\pi~=~7.13MHz$ and the single-photon coupling rate $g_0/2\\pi~=~7.76Hz$ of our system. Our results provide valuable insights into the performances of room-temperature macroscopic mechanical systems and their applications in hybrid quantum devices.","sentences":["Brillouin-based optomechanical systems with high-frequency acoustic modes provide a promising platform for implementing quantum-information processing and wavelength conversion applications, and for probing macroscopic quantum effects.","Achieving strong coupling through electrostrictive Brillouin interaction is essential for coupling the massive mechanical mode to an optical field, thereby controlling and characterizing the mechanical state.","However, achieving strong coupling at room temperature has proven challenging due to fast mechanical decay rates, which increase the pumping power required to surpass the coupling threshold.","Here, we report an optomechanical system with independent control over pumping power and frequency detuning to achieve and characterize the strong-coupling regime of a bulk acoustic-wave resonator.","Through spectral analysis of the cavity reflectivity, we identify clear signatures of strong coupling, i.e., normal-mode splitting and an avoided crossing in the detuned spectra, while estimating the mechanical linewidth $\\Gamma_m/2\\pi~=~7.13MHz$ and the single-photon coupling rate $g_0/2\\pi~=~7.76Hz$ of our system.","Our results provide valuable insights into the performances of room-temperature macroscopic mechanical systems and their applications in hybrid quantum devices."],"url":"http://arxiv.org/abs/2405.18107v1","category":"quant-ph"}
{"created":"2024-05-28 11:46:18","title":"Network Diffusion -- Framework to Simulate Spreading Processes in Complex Networks","abstract":"With the advancement of computational network science, its research scope has significantly expanded beyond static graphs to encompass more complex structures. The introduction of streaming, temporal, multilayer, and hypernetwork approaches has brought new possibilities and imposed additional requirements. For instance, by utilising these advancements, one can model structures such as social networks in a much more refined manner, which is particularly relevant in simulations of the spreading processes. Unfortunately, the pace of advancement is often too rapid for existing computational packages to keep up with the functionality updates. This results in a significant proliferation of tools used by researchers and, consequently, a lack of a universally accepted technological stack that would standardise experimental methods (as seen, e.g. in machine learning). This article addresses that issue by presenting an extended version of the Network Diffusion library. First, a survey of the existing approaches and toolkits for simulating spreading phenomena is shown and then, an overview of the framework functionalities. Finally, we report four case studies conducted with the package to demonstrate its usefulness: the impact of sanitary measures on the spread of COVID-19, the comparison of information diffusion on two temporal network models, and the effectiveness of seed selection methods in the task of influence maximisation in multilayer networks. We conclude the paper with a critical assessment of the library and the outline of still awaiting challenges to standardise research environments in computational network science.","sentences":["With the advancement of computational network science, its research scope has significantly expanded beyond static graphs to encompass more complex structures.","The introduction of streaming, temporal, multilayer, and hypernetwork approaches has brought new possibilities and imposed additional requirements.","For instance, by utilising these advancements, one can model structures such as social networks in a much more refined manner, which is particularly relevant in simulations of the spreading processes.","Unfortunately, the pace of advancement is often too rapid for existing computational packages to keep up with the functionality updates.","This results in a significant proliferation of tools used by researchers and, consequently, a lack of a universally accepted technological stack that would standardise experimental methods (as seen, e.g. in machine learning).","This article addresses that issue by presenting an extended version of the Network Diffusion library.","First, a survey of the existing approaches and toolkits for simulating spreading phenomena is shown and then, an overview of the framework functionalities.","Finally, we report four case studies conducted with the package to demonstrate its usefulness: the impact of sanitary measures on the spread of COVID-19, the comparison of information diffusion on two temporal network models, and the effectiveness of seed selection methods in the task of influence maximisation in multilayer networks.","We conclude the paper with a critical assessment of the library and the outline of still awaiting challenges to standardise research environments in computational network science."],"url":"http://arxiv.org/abs/2405.18085v1","category":"cs.SI"}
{"created":"2024-05-28 11:41:41","title":"HarmoDT: Harmony Multi-Task Decision Transformer for Offline Reinforcement Learning","abstract":"The purpose of offline multi-task reinforcement learning (MTRL) is to develop a unified policy applicable to diverse tasks without the need for online environmental interaction. Recent advancements approach this through sequence modeling, leveraging the Transformer architecture's scalability and the benefits of parameter sharing to exploit task similarities. However, variations in task content and complexity pose significant challenges in policy formulation, necessitating judicious parameter sharing and management of conflicting gradients for optimal policy performance. In this work, we introduce the Harmony Multi-Task Decision Transformer (HarmoDT), a novel solution designed to identify an optimal harmony subspace of parameters for each task. We approach this as a bi-level optimization problem, employing a meta-learning framework that leverages gradient-based techniques. The upper level of this framework is dedicated to learning a task-specific mask that delineates the harmony subspace, while the inner level focuses on updating parameters to enhance the overall performance of the unified policy. Empirical evaluations on a series of benchmarks demonstrate the superiority of HarmoDT, verifying the effectiveness of our approach.","sentences":["The purpose of offline multi-task reinforcement learning (MTRL) is to develop a unified policy applicable to diverse tasks without the need for online environmental interaction.","Recent advancements approach this through sequence modeling, leveraging the Transformer architecture's scalability and the benefits of parameter sharing to exploit task similarities.","However, variations in task content and complexity pose significant challenges in policy formulation, necessitating judicious parameter sharing and management of conflicting gradients for optimal policy performance.","In this work, we introduce the Harmony Multi-Task Decision Transformer (HarmoDT), a novel solution designed to identify an optimal harmony subspace of parameters for each task.","We approach this as a bi-level optimization problem, employing a meta-learning framework that leverages gradient-based techniques.","The upper level of this framework is dedicated to learning a task-specific mask that delineates the harmony subspace, while the inner level focuses on updating parameters to enhance the overall performance of the unified policy.","Empirical evaluations on a series of benchmarks demonstrate the superiority of HarmoDT, verifying the effectiveness of our approach."],"url":"http://arxiv.org/abs/2405.18080v1","category":"cs.LG"}
{"created":"2024-05-28 11:37:10","title":"Prediction of energy consumption in hotels using ANN","abstract":"The increase in travelers and stays in tourist destinations is leading hotels to be aware of their ecological management and the need for efficient energy consumption. To achieve this, hotels are increasingly using digitalized systems and more frequent measurements are made of the variables that affect their management. Electricity can play a significant role, predicting electricity usage in hotels, which in turn can enhance their circularity - an approach aimed at sustainable and efficient resource use. In this study, neural networks are trained to predict electricity usage patterns in two hotels based on historical data. The results indicate that the predictions have a good accuracy level of around 2.5% in MAPE, showing the potential of using these techniques for electricity forecasting in hotels. Additionally, neural network models can use climatological data to improve predictions. By accurately forecasting energy demand, hotels can optimize their energy procurement and usage, moving energy-intensive activities to off-peak hours to reduce costs and strain on the grid, assisting in the better integration of renewable energy sources, or identifying patterns and anomalies in energy consumption, suggesting areas for efficiency improvements, among other. Hence, by optimizing the allocation of resources, reducing waste and improving efficiency these models can improve hotel's circularity.","sentences":["The increase in travelers and stays in tourist destinations is leading hotels to be aware of their ecological management and the need for efficient energy consumption.","To achieve this, hotels are increasingly using digitalized systems and more frequent measurements are made of the variables that affect their management.","Electricity can play a significant role, predicting electricity usage in hotels, which in turn can enhance their circularity - an approach aimed at sustainable and efficient resource use.","In this study, neural networks are trained to predict electricity usage patterns in two hotels based on historical data.","The results indicate that the predictions have a good accuracy level of around 2.5% in MAPE, showing the potential of using these techniques for electricity forecasting in hotels.","Additionally, neural network models can use climatological data to improve predictions.","By accurately forecasting energy demand, hotels can optimize their energy procurement and usage, moving energy-intensive activities to off-peak hours to reduce costs and strain on the grid, assisting in the better integration of renewable energy sources, or identifying patterns and anomalies in energy consumption, suggesting areas for efficiency improvements, among other.","Hence, by optimizing the allocation of resources, reducing waste and improving efficiency these models can improve hotel's circularity."],"url":"http://arxiv.org/abs/2405.18076v1","category":"stat.CO"}
{"created":"2024-05-28 11:14:15","title":"ReChorus2.0: A Modular and Task-Flexible Recommendation Library","abstract":"With the applications of recommendation systems rapidly expanding, an increasing number of studies have focused on every aspect of recommender systems with different data inputs, models, and task settings. Therefore, a flexible library is needed to help researchers implement the experimental strategies they require. Existing open libraries for recommendation scenarios have enabled reproducing various recommendation methods and provided standard implementations. However, these libraries often impose certain restrictions on data and seldom support the same model to perform different tasks and input formats, limiting users from customized explorations. To fill the gap, we propose ReChorus2.0, a modular and task-flexible library for recommendation researchers. Based on ReChorus, we upgrade the supported input formats, models, and training&evaluation strategies to help realize more recommendation tasks with more data types. The main contributions of ReChorus2.0 include: (1) Realization of complex and practical tasks, including reranking and CTR prediction tasks; (2) Inclusion of various context-aware and rerank recommenders; (3) Extension of existing and new models to support different tasks with the same models; (4) Support of highly-customized input with impression logs, negative items, or click labels, as well as user, item, and situation contexts. To summarize, ReChorus2.0 serves as a comprehensive and flexible library better aligning with the practical problems in the recommendation scenario and catering to more diverse research needs. The implementation and detailed tutorials of ReChorus2.0 can be found at https://github.com/THUwangcy/ReChorus.","sentences":["With the applications of recommendation systems rapidly expanding, an increasing number of studies have focused on every aspect of recommender systems with different data inputs, models, and task settings.","Therefore, a flexible library is needed to help researchers implement the experimental strategies they require.","Existing open libraries for recommendation scenarios have enabled reproducing various recommendation methods and provided standard implementations.","However, these libraries often impose certain restrictions on data and seldom support the same model to perform different tasks and input formats, limiting users from customized explorations.","To fill the gap, we propose ReChorus2.0, a modular and task-flexible library for recommendation researchers.","Based on ReChorus, we upgrade the supported input formats, models, and training&evaluation strategies to help realize more recommendation tasks with more data types.","The main contributions of ReChorus2.0 include: (1) Realization of complex and practical tasks, including reranking and CTR prediction tasks; (2) Inclusion of various context-aware and rerank recommenders; (3) Extension of existing and new models to support different tasks with the same models; (4) Support of highly-customized input with impression logs, negative items, or click labels, as well as user, item, and situation contexts.","To summarize, ReChorus2.0 serves as a comprehensive and flexible library better aligning with the practical problems in the recommendation scenario and catering to more diverse research needs.","The implementation and detailed tutorials of ReChorus2.0 can be found at https://github.com/THUwangcy/ReChorus."],"url":"http://arxiv.org/abs/2405.18058v1","category":"cs.IR"}
{"created":"2024-05-28 11:09:39","title":"Dimension-free uniform concentration bound for logistic regression","abstract":"We provide a novel dimension-free uniform concentration bound for the empirical risk function of constrained logistic regression. Our bound yields a milder sufficient condition for a uniform law of large numbers than conditions derived by the Rademacher complexity argument and McDiarmid's inequality. The derivation is based on the PAC-Bayes approach with second-order expansion and Rademacher-complexity-based bounds for the residual term of the expansion.","sentences":["We provide a novel dimension-free uniform concentration bound for the empirical risk function of constrained logistic regression.","Our bound yields a milder sufficient condition for a uniform law of large numbers than conditions derived by the Rademacher complexity argument and McDiarmid's inequality.","The derivation is based on the PAC-Bayes approach with second-order expansion and Rademacher-complexity-based bounds for the residual term of the expansion."],"url":"http://arxiv.org/abs/2405.18055v1","category":"math.ST"}
{"created":"2024-05-28 11:02:22","title":"Expectation in Stochastic Window Mean-Payoff Games","abstract":"Stochastic two-player games model systems with an environment that is both adversarial and stochastic. In this paper, we study the expected value of the window mean-payoff measure in stochastic games. The window mean-payoff measure strengthens the classical mean-payoff measure by measuring the mean-payoff over a window of bounded length that slides along an infinite path. Two variants have been considered: in one variant, the maximum window length is fixed and given, while in the other, it is not fixed but is required to be bounded. For both variants, we show that the decision problem to check if the expected value is at least a given threshold is in NP $\\cap$ coNP. The result follows from guessing the expected values of the vertices, partitioning them into so-called value classes, and proving that a short certificate for the expected values exists. Finally, we also show that the memory required by the players to play optimally is no more than that in non-stochastic two-player games with the corresponding window objectives.","sentences":["Stochastic two-player games model systems with an environment that is both adversarial and stochastic.","In this paper, we study the expected value of the window mean-payoff measure in stochastic games.","The window mean-payoff measure strengthens the classical mean-payoff measure by measuring the mean-payoff over a window of bounded length that slides along an infinite path.","Two variants have been considered: in one variant, the maximum window length is fixed and given, while in the other, it is not fixed but is required to be bounded.","For both variants, we show that the decision problem to check if the expected value is at least a given threshold is in NP $\\cap$ coNP.","The result follows from guessing the expected values of the vertices, partitioning them into so-called value classes, and proving that a short certificate for the expected values exists.","Finally, we also show that the memory required by the players to play optimally is no more than that in non-stochastic two-player games with the corresponding window objectives."],"url":"http://arxiv.org/abs/2405.18048v1","category":"cs.GT"}
{"created":"2024-05-28 10:28:45","title":"Lower Bounds and Optimal Algorithms for Non-Smooth Convex Decentralized Optimization over Time-Varying Networks","abstract":"We consider the task of minimizing the sum of convex functions stored in a decentralized manner across the nodes of a communication network. This problem is relatively well-studied in the scenario when the objective functions are smooth, or the links of the network are fixed in time, or both. In particular, lower bounds on the number of decentralized communications and (sub)gradient computations required to solve the problem have been established, along with matching optimal algorithms. However, the remaining and most challenging setting of non-smooth decentralized optimization over time-varying networks is largely underexplored, as neither lower bounds nor optimal algorithms are known in the literature. We resolve this fundamental gap with the following contributions: (i) we establish the first lower bounds on the communication and subgradient computation complexities of solving non-smooth convex decentralized optimization problems over time-varying networks; (ii) we develop the first optimal algorithm that matches these lower bounds and offers substantially improved theoretical performance compared to the existing state of the art.","sentences":["We consider the task of minimizing the sum of convex functions stored in a decentralized manner across the nodes of a communication network.","This problem is relatively well-studied in the scenario when the objective functions are smooth, or the links of the network are fixed in time, or both.","In particular, lower bounds on the number of decentralized communications and (sub)gradient computations required to solve the problem have been established, along with matching optimal algorithms.","However, the remaining and most challenging setting of non-smooth decentralized optimization over time-varying networks is largely underexplored, as neither lower bounds nor optimal algorithms are known in the literature.","We resolve this fundamental gap with the following contributions: (i) we establish the first lower bounds on the communication and subgradient computation complexities of solving non-smooth convex decentralized optimization problems over time-varying networks; (ii) we develop the first optimal algorithm that matches these lower bounds and offers substantially improved theoretical performance compared to the existing state of the art."],"url":"http://arxiv.org/abs/2405.18031v1","category":"math.OC"}
{"created":"2024-05-28 10:26:37","title":"Modeling and Controlling Many-Core HPC Processors: an Alternative to PID and Moving Average Algorithms","abstract":"The race towards performance increase and computing power has led to chips with heterogeneous and complex designs, integrating an ever-growing number of cores on the same monolithic chip or chiplet silicon die. Higher integration density, compounded with the slowdown of technology-driven power reduction, implies that power and thermal management become increasingly relevant. Unfortunately, existing research lacks a detailed analysis and modeling of thermal, power, and electrical coupling effects and how they have to be jointly considered to perform dynamic control of complex and heterogeneous Multi-Processor System on Chips (MPSoCs). To close the gap, in this work, we first provide a detailed thermal and power model targeting a modern High Performance Computing (HPC) MPSoC. We consider real-world coupling effects such as actuators' non-idealities and the exponential relation between the dissipated power, the temperature state, and the voltage level in a single processing element. We analyze how these factors affect the control algorithm behavior and the type of challenges that they pose. Based on the analysis, we propose a thermal capping strategy inspired by Fuzzy control theory to replace the state-of-the-art PID controller, as well as a root-finding iterative method to optimally choose the shared voltage value among cores grouped in the same voltage domain. We evaluate the proposed controller with model-in-the-loop and hardware-in-the-loop co-simulations. We show an improvement over state-of-the-art methods of up to 5x the maximum exceeded temperature while providing an average of 3.56% faster application execution runtime across all the evaluation scenarios.","sentences":["The race towards performance increase and computing power has led to chips with heterogeneous and complex designs, integrating an ever-growing number of cores on the same monolithic chip or chiplet silicon die.","Higher integration density, compounded with the slowdown of technology-driven power reduction, implies that power and thermal management become increasingly relevant.","Unfortunately, existing research lacks a detailed analysis and modeling of thermal, power, and electrical coupling effects and how they have to be jointly considered to perform dynamic control of complex and heterogeneous Multi-Processor System on Chips (MPSoCs).","To close the gap, in this work, we first provide a detailed thermal and power model targeting a modern High Performance Computing (HPC) MPSoC. We consider real-world coupling effects such as actuators' non-idealities and the exponential relation between the dissipated power, the temperature state, and the voltage level in a single processing element.","We analyze how these factors affect the control algorithm behavior and the type of challenges that they pose.","Based on the analysis, we propose a thermal capping strategy inspired by Fuzzy control theory to replace the state-of-the-art PID controller, as well as a root-finding iterative method to optimally choose the shared voltage value among cores grouped in the same voltage domain.","We evaluate the proposed controller with model-in-the-loop and hardware-in-the-loop co-simulations.","We show an improvement over state-of-the-art methods of up to 5x the maximum exceeded temperature while providing an average of 3.56% faster application execution runtime across all the evaluation scenarios."],"url":"http://arxiv.org/abs/2405.18030v1","category":"eess.SY"}
{"created":"2024-05-28 10:09:49","title":"MULi-Ev: Maintaining Unperturbed LiDAR-Event Calibration","abstract":"Despite the increasing interest in enhancing perception systems for autonomous vehicles, the online calibration between event cameras and LiDAR - two sensors pivotal in capturing comprehensive environmental information - remains unexplored. We introduce MULi-Ev, the first online, deep learning-based framework tailored for the extrinsic calibration of event cameras with LiDAR. This advancement is instrumental for the seamless integration of LiDAR and event cameras, enabling dynamic, real-time calibration adjustments that are essential for maintaining optimal sensor alignment amidst varying operational conditions. Rigorously evaluated against the real-world scenarios presented in the DSEC dataset, MULi-Ev not only achieves substantial improvements in calibration accuracy but also sets a new standard for integrating LiDAR with event cameras in mobile platforms. Our findings reveal the potential of MULi-Ev to bolster the safety, reliability, and overall performance of event-based perception systems in autonomous driving, marking a significant step forward in their real-world deployment and effectiveness.","sentences":["Despite the increasing interest in enhancing perception systems for autonomous vehicles, the online calibration between event cameras and LiDAR - two sensors pivotal in capturing comprehensive environmental information - remains unexplored.","We introduce MULi-Ev, the first online, deep learning-based framework tailored for the extrinsic calibration of event cameras with LiDAR.","This advancement is instrumental for the seamless integration of LiDAR and event cameras, enabling dynamic, real-time calibration adjustments that are essential for maintaining optimal sensor alignment amidst varying operational conditions.","Rigorously evaluated against the real-world scenarios presented in the DSEC dataset, MULi-Ev not only achieves substantial improvements in calibration accuracy but also sets a new standard for integrating LiDAR with event cameras in mobile platforms.","Our findings reveal the potential of MULi-Ev to bolster the safety, reliability, and overall performance of event-based perception systems in autonomous driving, marking a significant step forward in their real-world deployment and effectiveness."],"url":"http://arxiv.org/abs/2405.18021v1","category":"cs.CV"}
{"created":"2024-05-28 09:53:11","title":"Rethinking Recommender Systems: Cluster-based Algorithm Selection","abstract":"Cluster-based algorithm selection deals with selecting recommendation algorithms on clusters of users to obtain performance gains. No studies have been attempted for many combinations of clustering approaches and recommendation algorithms. We want to show that clustering users prior to algorithm selection increases the performance of recommendation algorithms. Our study covers eight datasets, four clustering approaches, and eight recommendation algorithms. We select the best performing recommendation algorithm for each cluster. Our work shows that cluster-based algorithm selection is an effective technique for optimizing recommendation algorithm performance. For five out of eight datasets, we report an increase in nDCG@10 between 19.28% (0.032) and 360.38% (0.191) compared to algorithm selection without prior clustering.","sentences":["Cluster-based algorithm selection deals with selecting recommendation algorithms on clusters of users to obtain performance gains.","No studies have been attempted for many combinations of clustering approaches and recommendation algorithms.","We want to show that clustering users prior to algorithm selection increases the performance of recommendation algorithms.","Our study covers eight datasets, four clustering approaches, and eight recommendation algorithms.","We select the best performing recommendation algorithm for each cluster.","Our work shows that cluster-based algorithm selection is an effective technique for optimizing recommendation algorithm performance.","For five out of eight datasets, we report an increase in nDCG@10 between 19.28% (0.032) and 360.38% (0.191) compared to algorithm selection without prior clustering."],"url":"http://arxiv.org/abs/2405.18011v1","category":"cs.IR"}
{"created":"2024-05-28 09:48:52","title":"Mathematical models of the Arabidopsis circadian oscillator","abstract":"We review the construction and evolution of mathematical models of the Arabidopsis circadian clock, structuring the discussion into two distinct historical phases of modeling strategies: extension and reduction. The extension phase explores the bottom-up assembly of regulatory networks introducing as many components and interactions as possible in order to capture the oscillatory nature of the clock. The reduction phase deals with functional decomposition, distilling complex models to their essential dynamical repertoire. Current challenges in this field, including the integration of spatial considerations and environmental influences like light and temperature, are also discussed. The review emphasizes the ongoing need for models that balance molecular detail with practical simplicity.","sentences":["We review the construction and evolution of mathematical models of the Arabidopsis circadian clock, structuring the discussion into two distinct historical phases of modeling strategies: extension and reduction.","The extension phase explores the bottom-up assembly of regulatory networks introducing as many components and interactions as possible in order to capture the oscillatory nature of the clock.","The reduction phase deals with functional decomposition, distilling complex models to their essential dynamical repertoire.","Current challenges in this field, including the integration of spatial considerations and environmental influences like light and temperature, are also discussed.","The review emphasizes the ongoing need for models that balance molecular detail with practical simplicity."],"url":"http://arxiv.org/abs/2405.18006v1","category":"physics.bio-ph"}
{"created":"2024-05-28 09:36:22","title":"A Passive and Asynchronous Wake-up Receiver for Acoustic Underwater Communication","abstract":"Establishing reliable data exchange in an underwater domain using energy and power-efficient communication methods is crucial and challenging. Radio frequencies are absorbed by the salty and mineral-rich water and optical signals are obstructed and scattered after short distances. In contrast, acoustic communication benefits from low absorption and enables communication over long distances. Underwater communication must match low power and energy requirements as underwater sensor systems must have a long battery lifetime and need to work reliably due to their deployment and maintenance cost. For long-term deployments, the sensors' overall power consumption is determined by the power consumption during idle state. It can be reduced by integrating asynchronous always-on wake-up circuits with nano-watt power consumption. However, this approach does reduce but not eliminate idle power consumption, leaving a margin for improvement. This paper presents a passive and asynchronous wake-up receiver for acoustic underwater communication enabling zero-power always-on listening. Zero-power listening is achieved by combining energy and information transmission using a low-power wake-up receiver that extracts energy out of the acoustic signal and eliminates radio frontend idle consumption. In-field evaluations demonstrate that the wake-up circuit requires only 63 uW to detect and compare an 8-bit UUID at a data rate of 200 bps up to a distance of 5 m and that the needed energy can directly be extracted from the acoustic signal.","sentences":["Establishing reliable data exchange in an underwater domain using energy and power-efficient communication methods is crucial and challenging.","Radio frequencies are absorbed by the salty and mineral-rich water and optical signals are obstructed and scattered after short distances.","In contrast, acoustic communication benefits from low absorption and enables communication over long distances.","Underwater communication must match low power and energy requirements as underwater sensor systems must have a long battery lifetime and need to work reliably due to their deployment and maintenance cost.","For long-term deployments, the sensors' overall power consumption is determined by the power consumption during idle state.","It can be reduced by integrating asynchronous always-on wake-up circuits with nano-watt power consumption.","However, this approach does reduce but not eliminate idle power consumption, leaving a margin for improvement.","This paper presents a passive and asynchronous wake-up receiver for acoustic underwater communication enabling zero-power always-on listening.","Zero-power listening is achieved by combining energy and information transmission using a low-power wake-up receiver that extracts energy out of the acoustic signal and eliminates radio frontend idle consumption.","In-field evaluations demonstrate that the wake-up circuit requires only 63 uW to detect and compare an 8-bit UUID at a data rate of 200 bps up to a distance of 5 m and that the needed energy can directly be extracted from the acoustic signal."],"url":"http://arxiv.org/abs/2405.18000v1","category":"eess.SY"}
{"created":"2024-05-28 09:28:35","title":"Emergent Oscillating bound states in a semi-infinite linear waveguide with a point-like $\u039b$-type quantum emitter driven by a classical field","abstract":"An oscillating bound state is a phenomenon where excitations mediated by the continuum modes oscillate persistently. Although it is generated by the superposition of two bound states in the continuum (BICs), such phenomenon is said to be unique to giant atoms. We present the phenomenon of an oscillating bound state with an alternative waveguide QED system, which is a one-dimensional semi-infinite waveguide coupled to a \\textit{point-like} quantum emitter. This \\textit{point-like} quantum emitter is $\\Lambda$-type quantum system with one transition driven by a classical field.","sentences":["An oscillating bound state is a phenomenon where excitations mediated by the continuum modes oscillate persistently.","Although it is generated by the superposition of two bound states in the continuum (BICs), such phenomenon is said to be unique to giant atoms.","We present the phenomenon of an oscillating bound state with an alternative waveguide QED system, which is a one-dimensional semi-infinite waveguide coupled to a \\textit{point-like} quantum emitter.","This \\textit{point-like} quantum emitter is $\\Lambda$-type quantum system with one transition driven by a classical field."],"url":"http://arxiv.org/abs/2405.17994v1","category":"quant-ph"}
{"created":"2024-05-28 09:22:21","title":"Bistatic Sensing at THz Frequencies via a Two-Stage Ultra-Wideband MIMO-OFDM System","abstract":"Only the chairs can edit The availability of abundant bandwidth at terahertz (THz) frequencies holds promise for significantly enhancing the sensing performance of integrated sensing and communication (ISAC) systems in the next-generation wireless systems, enabling high accuracy and resolution for precise target localization. In orthogonal frequency-division multiplexing (OFDM) systems, wide bandwidth can be achieved by increasing the subcarrier spacing rather than the number of subcarriers, thereby keeping the complexity of the sensing system low. However, this approach may lead to an ambiguity problem in target range estimation. To address this issue, this work proposes a two-stage maximum likelihood method for estimating target position in an ultra-wideband bistatic multiple-antenna OFDM-based ISAC system operating at THz frequencies. Numerical results show that the proposed estimation approach effectively resolves the ambiguity problem while achieving high resolution and accuracy target position estimation at a very low signal-to-noise ratio regime.","sentences":["Only the chairs can edit The availability of abundant bandwidth at terahertz (THz) frequencies holds promise for significantly enhancing the sensing performance of integrated sensing and communication (ISAC) systems in the next-generation wireless systems, enabling high accuracy and resolution for precise target localization.","In orthogonal frequency-division multiplexing (OFDM) systems, wide bandwidth can be achieved by increasing the subcarrier spacing rather than the number of subcarriers, thereby keeping the complexity of the sensing system low.","However, this approach may lead to an ambiguity problem in target range estimation.","To address this issue, this work proposes a two-stage maximum likelihood method for estimating target position in an ultra-wideband bistatic multiple-antenna OFDM-based ISAC system operating at THz frequencies.","Numerical results show that the proposed estimation approach effectively resolves the ambiguity problem while achieving high resolution and accuracy target position estimation at a very low signal-to-noise ratio regime."],"url":"http://arxiv.org/abs/2405.17990v1","category":"eess.SP"}
{"created":"2024-05-28 09:22:18","title":"Extraction of Physical Properties of Interstellar Medium from the Observed Line Profiles","abstract":"Since molecules are ubiquitous in space, the study of the 'Molecular Universe' could unfold the mystery of the existing Interstellar medium. Star formation is linked to the chemical evolution processes. Thus, an analysis of the formation of stars coupled with the chemical evolution would give a clear insight into the entire process. For example, various evolutionary stages of star formation could be probed by observing various molecules. Chemical diagnostics of these regions could be used to extract the physical properties (e.g., density, temperature, ionization degree, etc.) of these regions. Radiative transfer calculations are worthwhile in estimating physical parameters of the region where molecules are detected. However, the radiative transfer calculations are limited due to insufficient molecular data, such as spectroscopic information or collisional excitation probabilities of many interstellar species. Complex organic molecules are detected in various environments ranging from the cold gas in prestellar cores to the warm gas on solar system scales close to individual protostars. A comparative study of the relative abundances of molecules could provide insights into the beginning of chemical complexity and the link to our solar system. In my thesis, I would mainly investigate the physical properties and kinematics of different star-forming regions using radiative transfer modeling. The observed spatial differentiation between various key molecules is used to explain their physical structure or evolution and various microphysical effects. In addition, some key molecules are used to study the various evolutionary phases. This simulated data is useful for interpreting the observed data of different telescopes like IRAM 30m, GBT, ALMA, Herschel, SOFIA, etc.","sentences":["Since molecules are ubiquitous in space, the study of the 'Molecular Universe' could unfold the mystery of the existing Interstellar medium.","Star formation is linked to the chemical evolution processes.","Thus, an analysis of the formation of stars coupled with the chemical evolution would give a clear insight into the entire process.","For example, various evolutionary stages of star formation could be probed by observing various molecules.","Chemical diagnostics of these regions could be used to extract the physical properties (e.g., density, temperature, ionization degree, etc.) of these regions.","Radiative transfer calculations are worthwhile in estimating physical parameters of the region where molecules are detected.","However, the radiative transfer calculations are limited due to insufficient molecular data, such as spectroscopic information or collisional excitation probabilities of many interstellar species.","Complex organic molecules are detected in various environments ranging from the cold gas in prestellar cores to the warm gas on solar system scales close to individual protostars.","A comparative study of the relative abundances of molecules could provide insights into the beginning of chemical complexity and the link to our solar system.","In my thesis, I would mainly investigate the physical properties and kinematics of different star-forming regions using radiative transfer modeling.","The observed spatial differentiation between various key molecules is used to explain their physical structure or evolution and various microphysical effects.","In addition, some key molecules are used to study the various evolutionary phases.","This simulated data is useful for interpreting the observed data of different telescopes like IRAM 30m, GBT, ALMA, Herschel, SOFIA, etc."],"url":"http://arxiv.org/abs/2405.17989v1","category":"astro-ph.GA"}
{"created":"2024-05-28 09:19:05","title":"Representing the dissipation of infinite-dimensional linear port-Hamiltonian systems","abstract":"It is well known that linear and non-linear dissipative port-Hamiltonian systems in finite dimensions admit an energy balance, relating the energy increase in the system with the supplied energy and the dissipated energy. The integrand in the dissipation term is then a function of the state variable. In this note, we answer the question of when this is possible for linear port-Hamiltonian systems in infinite dimensions.","sentences":["It is well known that linear and non-linear dissipative port-Hamiltonian systems in finite dimensions admit an energy balance, relating the energy increase in the system with the supplied energy and the dissipated energy.","The integrand in the dissipation term is then a function of the state variable.","In this note, we answer the question of when this is possible for linear port-Hamiltonian systems in infinite dimensions."],"url":"http://arxiv.org/abs/2405.17986v1","category":"math.AP"}
{"created":"2024-05-28 09:17:58","title":"Cross-Context Backdoor Attacks against Graph Prompt Learning","abstract":"Graph Prompt Learning (GPL) bridges significant disparities between pretraining and downstream applications to alleviate the knowledge transfer bottleneck in real-world graph learning. While GPL offers superior effectiveness in graph knowledge transfer and computational efficiency, the security risks posed by backdoor poisoning effects embedded in pretrained models remain largely unexplored. Our study provides a comprehensive analysis of GPL's vulnerability to backdoor attacks. We introduce \\textit{CrossBA}, the first cross-context backdoor attack against GPL, which manipulates only the pretraining phase without requiring knowledge of downstream applications. Our investigation reveals both theoretically and empirically that tuning trigger graphs, combined with prompt transformations, can seamlessly transfer the backdoor threat from pretrained encoders to downstream applications. Through extensive experiments involving 3 representative GPL methods across 5 distinct cross-context scenarios and 5 benchmark datasets of node and graph classification tasks, we demonstrate that \\textit{CrossBA} consistently achieves high attack success rates while preserving the functionality of downstream applications over clean input. We also explore potential countermeasures against \\textit{CrossBA} and conclude that current defenses are insufficient to mitigate \\textit{CrossBA}. Our study highlights the persistent backdoor threats to GPL systems, raising trustworthiness concerns in the practices of GPL techniques.","sentences":["Graph Prompt Learning (GPL) bridges significant disparities between pretraining and downstream applications to alleviate the knowledge transfer bottleneck in real-world graph learning.","While GPL offers superior effectiveness in graph knowledge transfer and computational efficiency, the security risks posed by backdoor poisoning effects embedded in pretrained models remain largely unexplored.","Our study provides a comprehensive analysis of GPL's vulnerability to backdoor attacks.","We introduce \\textit{CrossBA}, the first cross-context backdoor attack against GPL, which manipulates only the pretraining phase without requiring knowledge of downstream applications.","Our investigation reveals both theoretically and empirically that tuning trigger graphs, combined with prompt transformations, can seamlessly transfer the backdoor threat from pretrained encoders to downstream applications.","Through extensive experiments involving 3 representative GPL methods across 5 distinct cross-context scenarios and 5 benchmark datasets of node and graph classification tasks, we demonstrate that \\textit{CrossBA} consistently achieves high attack success rates while preserving the functionality of downstream applications over clean input.","We also explore potential countermeasures against \\textit{CrossBA} and conclude that current defenses are insufficient to mitigate \\textit{CrossBA}.","Our study highlights the persistent backdoor threats to GPL systems, raising trustworthiness concerns in the practices of GPL techniques."],"url":"http://arxiv.org/abs/2405.17984v1","category":"cs.LG"}
{"created":"2024-05-28 09:16:08","title":"Reinforced Model Predictive Control via Trust-Region Quasi-Newton Policy Optimization","abstract":"Model predictive control can optimally deal with nonlinear systems under consideration of constraints. The control performance depends on the model accuracy and the prediction horizon. Recent advances propose to use reinforcement learning applied to a parameterized model predictive controller to recover the optimal control performance even if an imperfect model or short prediction horizons are used. However, common reinforcement learning algorithms rely on first order updates, which only have a linear convergence rate and hence need an excessive amount of dynamic data. Higher order updates are typically intractable if the policy is approximated with neural networks due to the large number of parameters.   In this work, we use a parameterized model predictive controller as policy, and leverage the small amount of necessary parameters to propose a trust-region constrained Quasi-Newton training algorithm for policy optimization with a superlinear convergence rate. We show that the required second order derivative information can be calculated by the solution of a linear system of equations. A simulation study illustrates that the proposed training algorithm outperforms other algorithms in terms of data efficiency and accuracy.","sentences":["Model predictive control can optimally deal with nonlinear systems under consideration of constraints.","The control performance depends on the model accuracy and the prediction horizon.","Recent advances propose to use reinforcement learning applied to a parameterized model predictive controller to recover the optimal control performance even if an imperfect model or short prediction horizons are used.","However, common reinforcement learning algorithms rely on first order updates, which only have a linear convergence rate and hence need an excessive amount of dynamic data.","Higher order updates are typically intractable if the policy is approximated with neural networks due to the large number of parameters.   ","In this work, we use a parameterized model predictive controller as policy, and leverage the small amount of necessary parameters to propose a trust-region constrained Quasi-Newton training algorithm for policy optimization with a superlinear convergence rate.","We show that the required second order derivative information can be calculated by the solution of a linear system of equations.","A simulation study illustrates that the proposed training algorithm outperforms other algorithms in terms of data efficiency and accuracy."],"url":"http://arxiv.org/abs/2405.17983v1","category":"cs.LG"}
{"created":"2024-05-28 09:02:06","title":"Symmetries of linear and nonlinear partial differential equations","abstract":"We consider higher symmetries and operator symmetries of linear partial differential equations. The higher symmetries form a Lie algebra, and operator ones form an associative algebra. The relationship between these symmetries is established. We show that symmetries of linear equations sometimes generate symmetries of nonlinear ones. New symmetries of two-dimensional stationary equations of gas dynamics are found.","sentences":["We consider higher symmetries and operator symmetries of linear partial differential equations.","The higher symmetries form a Lie algebra, and operator ones form an associative algebra.","The relationship between these symmetries is established.","We show that symmetries of linear equations sometimes generate symmetries of nonlinear ones.","New symmetries of two-dimensional stationary equations of gas dynamics are found."],"url":"http://arxiv.org/abs/2405.17973v1","category":"nlin.SI"}
{"created":"2024-05-28 08:59:38","title":"Inference for the stochastic FitzHugh-Nagumo model from real action potential data via approximate Bayesian computation","abstract":"The stochastic FitzHugh-Nagumo (FHN) model considered here is a two-dimensional nonlinear stochastic differential equation with additive degenerate noise, whose first component, the only one observed, describes the membrane voltage evolution of a single neuron. Due to its low dimensionality, its analytical and numerical tractability, and its neuronal interpretation, it has been used as a case study to test the performance of different statistical methods in estimating the underlying model parameters. Existing methods, however, often require complete observations, non-degeneracy of the noise or a complex architecture (e.g., to estimate the transition density of the process, \"recovering\" the unobserved second component), and they may not (satisfactorily) estimate all model parameters simultaneously. Moreover, these studies lack real data applications for the stochastic FHN model. Here, we tackle all challenges (non-globally Lipschitz drift, non-explicit solution, lack of available transition density, degeneracy of the noise, and partial observations) via an intuitive and easy-to-implement sequential Monte Carlo approximate Bayesian computation algorithm. The proposed method relies on a recent computationally efficient and structure-preserving numerical splitting scheme for synthetic data generation, and on summary statistics exploiting the structural properties of the process. We succeed in estimating all model parameters from simulated data and, more remarkably, real action potential data of rats. The presented novel real-data fit may broaden the scope and credibility of this classic and widely used neuronal model.","sentences":["The stochastic FitzHugh-Nagumo (FHN) model considered here is a two-dimensional nonlinear stochastic differential equation with additive degenerate noise, whose first component, the only one observed, describes the membrane voltage evolution of a single neuron.","Due to its low dimensionality, its analytical and numerical tractability, and its neuronal interpretation, it has been used as a case study to test the performance of different statistical methods in estimating the underlying model parameters.","Existing methods, however, often require complete observations, non-degeneracy of the noise or a complex architecture (e.g., to estimate the transition density of the process, \"recovering\" the unobserved second component), and they may not (satisfactorily) estimate all model parameters simultaneously.","Moreover, these studies lack real data applications for the stochastic FHN model.","Here, we tackle all challenges (non-globally Lipschitz drift, non-explicit solution, lack of available transition density, degeneracy of the noise, and partial observations) via an intuitive and easy-to-implement sequential Monte Carlo approximate Bayesian computation algorithm.","The proposed method relies on a recent computationally efficient and structure-preserving numerical splitting scheme for synthetic data generation, and on summary statistics exploiting the structural properties of the process.","We succeed in estimating all model parameters from simulated data and, more remarkably, real action potential data of rats.","The presented novel real-data fit may broaden the scope and credibility of this classic and widely used neuronal model."],"url":"http://arxiv.org/abs/2405.17972v1","category":"stat.CO"}
{"created":"2024-05-28 08:55:02","title":"Matroid Semi-Bandits in Sublinear Time","abstract":"We study the matroid semi-bandits problem, where at each round the learner plays a subset of $K$ arms from a feasible set, and the goal is to maximize the expected cumulative linear rewards. Existing algorithms have per-round time complexity at least $\\Omega(K)$, which becomes expensive when $K$ is large. To address this computational issue, we propose FasterCUCB whose sampling rule takes time sublinear in $K$ for common classes of matroids: $O(D\\text{ polylog}(K)\\text{ polylog}(T))$ for uniform matroids, partition matroids, and graphical matroids, and $O(D\\sqrt{K}\\text{ polylog}(T))$ for transversal matroids. Here, $D$ is the maximum number of elements in any feasible subset of arms, and $T$ is the horizon. Our technique is based on dynamic maintenance of an approximate maximum-weight basis over inner-product weights. Although the introduction of an approximate maximum-weight basis presents a challenge in regret analysis, we can still guarantee an upper bound on regret as tight as CUCB in the sense that it matches the gap-dependent lower bound by Kveton et al. (2014a) asymptotically.","sentences":["We study the matroid semi-bandits problem, where at each round the learner plays a subset of $K$ arms from a feasible set, and the goal is to maximize the expected cumulative linear rewards.","Existing algorithms have per-round time complexity at least $\\Omega(K)$, which becomes expensive when $K$ is large.","To address this computational issue, we propose FasterCUCB whose sampling rule takes time sublinear in $K$ for common classes of matroids: $O(D\\text{ polylog}(K)\\text{ polylog}(T))$ for uniform matroids, partition matroids, and graphical matroids, and $O(D\\sqrt{K}\\text{ polylog}(T))$ for transversal matroids.","Here, $D$ is the maximum number of elements in any feasible subset of arms, and $T$ is the horizon.","Our technique is based on dynamic maintenance of an approximate maximum-weight basis over inner-product weights.","Although the introduction of an approximate maximum-weight basis presents a challenge in regret analysis, we can still guarantee an upper bound on regret as tight as CUCB in the sense that it matches the gap-dependent lower bound by Kveton et al. (2014a) asymptotically."],"url":"http://arxiv.org/abs/2405.17968v1","category":"cs.LG"}
{"created":"2024-05-28 08:47:28","title":"Band geometry induced electro-optic effect and polarization rotation","abstract":"Electric field-induced modulation of the optical properties is crucial for amplitude and phase modulators used in photonic devices. Here, we present a comprehensive study of the band geometry-induced electro-optic effect, specifically focusing on the Fermi surface and disorder-induced contributions. These contributions are crucial for metallic and semimetallic systems and for optical frequencies comparable to or smaller than the scattering rates. We highlight the importance of the quantum metric and metric connection in generating the electro-optic effect in parity-time reversal ($\\mathcal{PT}$) symmetric systems such as CuMnAs. Our findings establish the electro-optic effect as a novel tool to probe band geometric effects and open new avenues to design electrically controlled efficient amplitude and phase modulators for photonic applications.","sentences":["Electric field-induced modulation of the optical properties is crucial for amplitude and phase modulators used in photonic devices.","Here, we present a comprehensive study of the band geometry-induced electro-optic effect, specifically focusing on the Fermi surface and disorder-induced contributions.","These contributions are crucial for metallic and semimetallic systems and for optical frequencies comparable to or smaller than the scattering rates.","We highlight the importance of the quantum metric and metric connection in generating the electro-optic effect in parity-time reversal ($\\mathcal{PT}$) symmetric systems such as CuMnAs.","Our findings establish the electro-optic effect as a novel tool to probe band geometric effects and open new avenues to design electrically controlled efficient amplitude and phase modulators for photonic applications."],"url":"http://arxiv.org/abs/2405.17963v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-28 08:43:03","title":"Detection of carbon monoxide using a polarization-multiplexed erbium dual-comb fiber laser","abstract":"We present a compact, reliable, and robust free-running all-polarization-maintaining erbium (Er) single-cavity dual-comb laser generated via polarization multiplexing with gain sharing. Polarization multiplexing exploits the fast and slow axes of the fiber, while modelocking is achieved through a nonlinear amplifying loop mirror scheme using readily available components. The laser operates at a repetition rate of around 74.74 MHz with a tuning capability in the difference in repetition rates from 500 Hz to 200 kHz. This tunability makes the system more flexible for dual-comb spectroscopy experiments. Consequently, using this laser, we demonstrated a proof-of-principle dual-comb spectroscopy of carbon monoxide (CO), operating without any active stabilization.","sentences":["We present a compact, reliable, and robust free-running all-polarization-maintaining erbium (Er) single-cavity dual-comb laser generated via polarization multiplexing with gain sharing.","Polarization multiplexing exploits the fast and slow axes of the fiber, while modelocking is achieved through a nonlinear amplifying loop mirror scheme using readily available components.","The laser operates at a repetition rate of around 74.74 MHz with a tuning capability in the difference in repetition rates from 500 Hz to 200 kHz.","This tunability makes the system more flexible for dual-comb spectroscopy experiments.","Consequently, using this laser, we demonstrated a proof-of-principle dual-comb spectroscopy of carbon monoxide (CO), operating without any active stabilization."],"url":"http://arxiv.org/abs/2405.17962v1","category":"physics.optics"}
{"created":"2024-05-28 08:41:09","title":"Elementary Flux Modes as CRN Gears for Free Energy Transduction","abstract":"We demonstrate that, for a chemical reaction network (CRN) engaged in energy transduction, its optimal operation from a thermodynamic efficiency standpoint is contingent upon its working conditions. Analogously to the bicycle gear system, CRNs have at their disposal several transducing mechanisms characterized by different yields. We highlight the critical role of the CRN's elementary flux modes in determining this \"gearing\" and their impact on maximizing energy transduction efficiency. Furthermore, we introduce an enzymatically regulated CRN, engineered to autonomously adjust its \"gear\", thereby optimizing its efficiency under different external conditions.","sentences":["We demonstrate that, for a chemical reaction network (CRN) engaged in energy transduction, its optimal operation from a thermodynamic efficiency standpoint is contingent upon its working conditions.","Analogously to the bicycle gear system, CRNs have at their disposal several transducing mechanisms characterized by different yields.","We highlight the critical role of the CRN's elementary flux modes in determining this \"gearing\" and their impact on maximizing energy transduction efficiency.","Furthermore, we introduce an enzymatically regulated CRN, engineered to autonomously adjust its \"gear\", thereby optimizing its efficiency under different external conditions."],"url":"http://arxiv.org/abs/2405.17960v1","category":"q-bio.MN"}
{"created":"2024-05-28 08:34:23","title":"Comparison of predictive values with paired samples","abstract":"Positive predictive value and negative predictive value are two widely used parameters to assess the clinical usefulness of a medical diagnostic test. When there are two diagnostic tests, it is recommendable to make a comparative assessment of the values of these two parameters after applying the two tests to the same subjects (paired samples). The objective is then to make individual or global inferences about the difference or the ratio of the predictive value of the two diagnostic tests. These inferences are usually based on complex and not very intuitive expressions, some of which have subsequently been reformulated. We define the two properties of symmetry which any inference method must verify - symmetry in diagnoses and symmetry in the tests -, we propose new inference methods, and we define them with simple expressions. All of the methods are compared with each other, selecting the optimal method: (a) to obtain a confidence interval for the difference or ratio; (b) to perform an individual homogeneity test of the two predictive values; and (c) to carry out a global homogeneity test of the two predictive values.","sentences":["Positive predictive value and negative predictive value are two widely used parameters to assess the clinical usefulness of a medical diagnostic test.","When there are two diagnostic tests, it is recommendable to make a comparative assessment of the values of these two parameters after applying the two tests to the same subjects (paired samples).","The objective is then to make individual or global inferences about the difference or the ratio of the predictive value of the two diagnostic tests.","These inferences are usually based on complex and not very intuitive expressions, some of which have subsequently been reformulated.","We define the two properties of symmetry which any inference method must verify - symmetry in diagnoses and symmetry in the tests -, we propose new inference methods, and we define them with simple expressions.","All of the methods are compared with each other, selecting the optimal method: (a) to obtain a confidence interval for the difference or ratio; (b) to perform an individual homogeneity test of the two predictive values; and (c) to carry out a global homogeneity test of the two predictive values."],"url":"http://arxiv.org/abs/2405.17954v1","category":"stat.ME"}
{"created":"2024-05-28 08:34:03","title":"Graph Threading with Turn Costs","abstract":"How should we thread a single string through a set of tubes so that pulling the string taut self-assembles the tubes into a desired graph? While prior work [ITCS 2024] solves this problem with the goal of minimizing the length of string, we study here the objective of minimizing the total turn cost. The frictional force required to pull the string through the tubes grows exponentially with the total absolute turn angles (by the Capstan equation), so this metric often dominates the friction in real-world applications such as deployable structures. We show that minimum-turn threading is NP-hard, even for graphs of maximum degree 4, and even when restricted to some special cases of threading. On the other hand, we show that these special cases can in fact be solved efficiently for graphs of maximum degree 4, thereby fully characterizing their dependence on maximum degree. We further provide polynomial-time exact and approximation algorithms for variants of turn-cost threading: restricting to threading each edge exactly twice, and on rectangular grid graphs.","sentences":["How should we thread a single string through a set of tubes so that pulling the string taut self-assembles the tubes into a desired graph?","While prior work [ITCS 2024] solves this problem with the goal of minimizing the length of string, we study here the objective of minimizing the total turn cost.","The frictional force required to pull the string through the tubes grows exponentially with the total absolute turn angles (by the Capstan equation), so this metric often dominates the friction in real-world applications such as deployable structures.","We show that minimum-turn threading is NP-hard, even for graphs of maximum degree 4, and even when restricted to some special cases of threading.","On the other hand, we show that these special cases can in fact be solved efficiently for graphs of maximum degree 4, thereby fully characterizing their dependence on maximum degree.","We further provide polynomial-time exact and approximation algorithms for variants of turn-cost threading: restricting to threading each edge exactly twice, and on rectangular grid graphs."],"url":"http://arxiv.org/abs/2405.17953v1","category":"cs.DS"}
{"created":"2024-05-28 08:28:18","title":"Efficient Time Series Processing for Transformers and State-Space Models through Token Merging","abstract":"Transformer architectures have shown promising results in time series processing. However, despite recent advances in subquadratic attention mechanisms or state-space models, processing very long sequences still imposes significant computational requirements. Token merging, which involves replacing multiple tokens with a single one calculated as their linear combination, has shown to considerably improve the throughput of vision transformer architectures while maintaining accuracy. In this work, we go beyond computer vision and perform the first investigations of token merging in time series analysis on both time series transformers and state-space models. To effectively scale token merging to long sequences, we introduce local merging, a domain-specific token merging algorithm that selectively combines tokens within a local neighborhood, adjusting the computational complexity from linear to quadratic based on the neighborhood size. Our comprehensive empirical evaluation demonstrates that token merging offers substantial computational benefits with minimal impact on accuracy across various models and datasets. On the recently proposed Chronos foundation model, we achieve accelerations up to 5400% with only minor accuracy degradations.","sentences":["Transformer architectures have shown promising results in time series processing.","However, despite recent advances in subquadratic attention mechanisms or state-space models, processing very long sequences still imposes significant computational requirements.","Token merging, which involves replacing multiple tokens with a single one calculated as their linear combination, has shown to considerably improve the throughput of vision transformer architectures while maintaining accuracy.","In this work, we go beyond computer vision and perform the first investigations of token merging in time series analysis on both time series transformers and state-space models.","To effectively scale token merging to long sequences, we introduce local merging, a domain-specific token merging algorithm that selectively combines tokens within a local neighborhood, adjusting the computational complexity from linear to quadratic based on the neighborhood size.","Our comprehensive empirical evaluation demonstrates that token merging offers substantial computational benefits with minimal impact on accuracy across various models and datasets.","On the recently proposed Chronos foundation model, we achieve accelerations up to 5400% with only minor accuracy degradations."],"url":"http://arxiv.org/abs/2405.17951v1","category":"cs.LG"}
{"created":"2024-05-28 08:26:07","title":"Attosecond spectroscopy using vacuum-ultraviolet pulses emitted from laser-driven semiconductors","abstract":"Strongly laser-driven semiconductor crystals offer substantial advantages for the study of many-body physics and ultrafast optoelectronics via the high harmonic generation process. While this phenomenon has been employed to investigate the dynamics of solids in the presence of strong laser fields, its potential to be utilized as an attosecond light source has remained unexploited. Here, we demonstrate that the high harmonics generated through the interaction of mid--infrared pulses with a ZnO crystal leads to the production of attosecond pulses, that can be used to trace the ultrafast ionization dynamics of alkali metals. In a cross--correlation approach, we photoionize Cesium atoms with the vacuum-ultraviolet (VUV) high-harmonics in the presence of a mid-infrared laser field. We observe strong oscillations of the photoelectron yield originating from the instantaneous polarization of the atoms by the laser field. The phase of the oscillations encodes the attosecond synchronization of the ionizing high-harmonics and is used for attosecond pulse metrology. This light source opens a new spectral window for attosecond spectroscopy, paving the way for studies of systems with low ionization potentials including neutral atoms, molecules and solids. Additionally, our results highlight the significance of the source for generating non--classical massively entangled light states in the visible--VUV spectral region.","sentences":["Strongly laser-driven semiconductor crystals offer substantial advantages for the study of many-body physics and ultrafast optoelectronics via the high harmonic generation process.","While this phenomenon has been employed to investigate the dynamics of solids in the presence of strong laser fields, its potential to be utilized as an attosecond light source has remained unexploited.","Here, we demonstrate that the high harmonics generated through the interaction of mid--infrared pulses with a ZnO crystal leads to the production of attosecond pulses, that can be used to trace the ultrafast ionization dynamics of alkali metals.","In a cross--correlation approach, we photoionize Cesium atoms with the vacuum-ultraviolet (VUV) high-harmonics in the presence of a mid-infrared laser field.","We observe strong oscillations of the photoelectron yield originating from the instantaneous polarization of the atoms by the laser field.","The phase of the oscillations encodes the attosecond synchronization of the ionizing high-harmonics and is used for attosecond pulse metrology.","This light source opens a new spectral window for attosecond spectroscopy, paving the way for studies of systems with low ionization potentials including neutral atoms, molecules and solids.","Additionally, our results highlight the significance of the source for generating non--classical massively entangled light states in the visible--VUV spectral region."],"url":"http://arxiv.org/abs/2405.17949v1","category":"physics.optics"}
{"created":"2024-05-28 08:21:36","title":"The communication power of a noisy qubit","abstract":"A fundamental property of quantum mechanics is that a single qubit can carry at most 1 bit of classical information. For an important class of quantum communication channels, known as entanglement-breaking, this limitation remains valid even if the sender and receiver share entangled particles before the start of the communication: for every entanglement-breaking channel, the rate at which classical messages can be reliably communicated cannot exceed 1 bit per transmitted qubit even with the assistance of quantum entanglement. But does this mean that, for the purpose of communicating classical messages, a noisy entanglement-breaking qubit channel can be replaced by a noisy bit channel? Here we answer the question in the negative. We introduce a game where a player (the sender) assists another player (the receiver) in finding a prize hidden into one of four possible boxes, while avoiding a bomb hidden in one of the three remaining boxes. In this game, the bomb cannot be avoided with certainty if the players communicate through a noisy bit channel. In contrast, the players can deterministically avoid the bomb and find the prize with a guaranteed 1/3 probability if they communicate through an entanglement-breaking qubit channel known as the universal NOT channel. We show that the features of the quantum strategy can be simulated with a noiseless bit channel, but this simulation requires the transmission to be assisted by shared randomness: without shared randomness, even the noiseless transmission of a three-level classical system cannot match the transmission of a single noisy qubit.","sentences":["A fundamental property of quantum mechanics is that a single qubit can carry at most 1 bit of classical information.","For an important class of quantum communication channels, known as entanglement-breaking, this limitation remains valid even if the sender and receiver share entangled particles before the start of the communication: for every entanglement-breaking channel, the rate at which classical messages can be reliably communicated cannot exceed 1 bit per transmitted qubit even with the assistance of quantum entanglement.","But does this mean that, for the purpose of communicating classical messages, a noisy entanglement-breaking qubit channel can be replaced by a noisy bit channel?","Here we answer the question in the negative.","We introduce a game where a player (the sender) assists another player (the receiver) in finding a prize hidden into one of four possible boxes, while avoiding a bomb hidden in one of the three remaining boxes.","In this game, the bomb cannot be avoided with certainty if the players communicate through a noisy bit channel.","In contrast, the players can deterministically avoid the bomb and find the prize with a guaranteed 1/3 probability if they communicate through an entanglement-breaking qubit channel known as the universal NOT channel.","We show that the features of the quantum strategy can be simulated with a noiseless bit channel, but this simulation requires the transmission to be assisted by shared randomness: without shared randomness, even the noiseless transmission of a three-level classical system cannot match the transmission of a single noisy qubit."],"url":"http://arxiv.org/abs/2405.17946v1","category":"quant-ph"}
{"created":"2024-05-28 08:04:01","title":"An empirical study of bloated dependencies in CommonJS packages","abstract":"JavaScript packages are notoriously prone to bloat, a factor that significantly impacts the performance and maintainability of web applications. While web bundlers and tree-shaking can mitigate this issue in client-side applications at the function level, they cannot effectively detect and remove bloat in server-side applications. In this paper, we conduct an empirical study to investigate the bloated dependencies that are entirely unused within server-side applications. Our study focuses on applications built with the widely used and highly dynamic CommonJS module system. We propose a trace-based dynamic analysis that monitors file access, to determine which dependencies are not accessed during runtime. To conduct our study, we curate an original dataset of 92 CommonJS packages with a median test coverage of 96.9% and a total of 50,661 dependencies. Our dynamic analysis identifies and successfully removes 50.7% of these dependencies while maintaining the correct build of all packages. Furthermore, we find that 14.9% of directly used dependencies and 51.3% of indirect dependencies are bloated. A key insight is that focusing on removing only the direct bloated dependencies by cleaning the package.json file, also removes a significant share of unnecessary bloated indirect dependencies. Compared to the state-of-the-art dynamic debloating technique, our analysis based on file accesses has fewer false positives, and demonstrates higher accuracy in detecting bloated dependencies. Our findings suggest that native support for dependency debloating in package managers could significantly alleviate the burden of maintaining dependencies.","sentences":["JavaScript packages are notoriously prone to bloat, a factor that significantly impacts the performance and maintainability of web applications.","While web bundlers and tree-shaking can mitigate this issue in client-side applications at the function level, they cannot effectively detect and remove bloat in server-side applications.","In this paper, we conduct an empirical study to investigate the bloated dependencies that are entirely unused within server-side applications.","Our study focuses on applications built with the widely used and highly dynamic CommonJS module system.","We propose a trace-based dynamic analysis that monitors file access, to determine which dependencies are not accessed during runtime.","To conduct our study, we curate an original dataset of 92 CommonJS packages with a median test coverage of 96.9% and a total of 50,661 dependencies.","Our dynamic analysis identifies and successfully removes 50.7% of these dependencies while maintaining the correct build of all packages.","Furthermore, we find that 14.9% of directly used dependencies and 51.3% of indirect dependencies are bloated.","A key insight is that focusing on removing only the direct bloated dependencies by cleaning the package.json file, also removes a significant share of unnecessary bloated indirect dependencies.","Compared to the state-of-the-art dynamic debloating technique, our analysis based on file accesses has fewer false positives, and demonstrates higher accuracy in detecting bloated dependencies.","Our findings suggest that native support for dependency debloating in package managers could significantly alleviate the burden of maintaining dependencies."],"url":"http://arxiv.org/abs/2405.17939v1","category":"cs.SE"}
{"created":"2024-05-28 07:48:10","title":"SarcNet: A Novel AI-based Framework to Automatically Analyze and Score Sarcomere Organizations in Fluorescently Tagged hiPSC-CMs","abstract":"Quantifying sarcomere structure organization in human-induced pluripotent stem cell-derived cardiomyocytes (hiPSC-CMs) is crucial for understanding cardiac disease pathology, improving drug screening, and advancing regenerative medicine. Traditional methods, such as manual annotation and Fourier transform analysis, are labor-intensive, error-prone, and lack high-throughput capabilities. In this study, we present a novel deep learning-based framework that leverages cell images and integrates cell features to automatically evaluate the sarcomere structure of hiPSC-CMs from the onset of differentiation. This framework overcomes the limitations of traditional methods through automated, high-throughput analysis, providing consistent, reliable results while accurately detecting complex sarcomere patterns across diverse samples. The proposed framework contains the SarcNet, a linear layers-added ResNet-18 module, to output a continuous score ranging from one to five that captures the level of sarcomere structure organization. It is trained and validated on an open-source dataset of hiPSC-CMs images with the endogenously GFP-tagged alpha-actinin-2 structure developed by the Allen Institute for Cell Science (AICS). SarcNet achieves a Spearman correlation of 0.831 with expert evaluations, demonstrating superior performance and an improvement of 0.075 over the current state-of-the-art approach, which uses Linear Regression. Our results also show a consistent pattern of increasing organization from day 18 to day 32 of differentiation, aligning with expert evaluations. By integrating the quantitative features calculated directly from the images with the visual features learned during the deep learning model, our framework offers a more comprehensive and accurate assessment, thereby enhancing the further utility of hiPSC-CMs in medical research and therapy development.","sentences":["Quantifying sarcomere structure organization in human-induced pluripotent stem cell-derived cardiomyocytes (hiPSC-CMs) is crucial for understanding cardiac disease pathology, improving drug screening, and advancing regenerative medicine.","Traditional methods, such as manual annotation and Fourier transform analysis, are labor-intensive, error-prone, and lack high-throughput capabilities.","In this study, we present a novel deep learning-based framework that leverages cell images and integrates cell features to automatically evaluate the sarcomere structure of hiPSC-CMs from the onset of differentiation.","This framework overcomes the limitations of traditional methods through automated, high-throughput analysis, providing consistent, reliable results while accurately detecting complex sarcomere patterns across diverse samples.","The proposed framework contains the SarcNet, a linear layers-added ResNet-18 module, to output a continuous score ranging from one to five that captures the level of sarcomere structure organization.","It is trained and validated on an open-source dataset of hiPSC-CMs images with the endogenously GFP-tagged alpha-actinin-2 structure developed by the Allen Institute for Cell Science (AICS).","SarcNet achieves a Spearman correlation of 0.831 with expert evaluations, demonstrating superior performance and an improvement of 0.075 over the current state-of-the-art approach, which uses Linear Regression.","Our results also show a consistent pattern of increasing organization from day 18 to day 32 of differentiation, aligning with expert evaluations.","By integrating the quantitative features calculated directly from the images with the visual features learned during the deep learning model, our framework offers a more comprehensive and accurate assessment, thereby enhancing the further utility of hiPSC-CMs in medical research and therapy development."],"url":"http://arxiv.org/abs/2405.17926v1","category":"cs.CV"}
{"created":"2024-05-28 07:47:28","title":"A real/fast-time simulator for impact assessment of spoofing & jamming attacks on GNSS receivers","abstract":"In aviation, the impact of threats is becoming increasingly significant, particularly for global navigation satellite system (GNSS). Two relevant GNSS threats are represented by jamming and spoofing. In order to evaluate the technological solutions to counter GNSS attacks, such attacks should be assessed by means of a proper GNSS threat simulator. This work shows the implementation and the testing results of a GNSS security impact simulator which injects the desired threat scenarios as a deviations on the GNSS actual measurements. The proposed simulator can be integrated in both real- and fast-time simulation environments. The provided results confirm the effectiveness of the simulator, and include in-flight demonstrations by means of a flight experimental vehicle.","sentences":["In aviation, the impact of threats is becoming increasingly significant, particularly for global navigation satellite system (GNSS).","Two relevant GNSS threats are represented by jamming and spoofing.","In order to evaluate the technological solutions to counter GNSS attacks, such attacks should be assessed by means of a proper GNSS threat simulator.","This work shows the implementation and the testing results of a GNSS security impact simulator which injects the desired threat scenarios as a deviations on the GNSS actual measurements.","The proposed simulator can be integrated in both real- and fast-time simulation environments.","The provided results confirm the effectiveness of the simulator, and include in-flight demonstrations by means of a flight experimental vehicle."],"url":"http://arxiv.org/abs/2405.17925v1","category":"eess.SP"}
{"created":"2024-05-28 07:33:25","title":"Lateral migration and bouncing of a deformable bubble rising near a vertical wall. Part 1. Moderately inertial regimes","abstract":"The buoyancy-driven motion of a deformable bubble rising near a vertical hydrophilic wall is studied numerically. We focus on moderately inertial regimes in which the bubble undergoes low-to-moderate deformations and would rise in a straight line in the absence of the wall. Three different types of near-wall motion are observed, depending on the buoyancy-to-viscous and buoyancy-to-capillary force ratios defining the Galilei ($Ga$) and Bond ($Bo$) numbers of the system, respectively. For low enough $Ga$ or large enough $Bo$, bubbles consistently migrate away from the wall. Conversely, for large enough $Ga$ and low enough $Bo$, they perform periodic near-wall bounces. At intermediate $Ga$ and $Bo$, they are first attracted to the wall down to a certain critical distance, and then perform bounces with a decreasing amplitude before stabilizing at this critical separation. Periodic bounces are accompanied by the shedding of a pair of streamwise vortices in the wake, the formation of which is governed by the near-wall shear resulting from the no-slip condition. These vortices provide a repulsive force that overcomes the viscous resistance of the fluid to the departing motion, making the bubble capable of returning to the region where it is attracted again to the wall. Although periodic, the shedding/regeneration cycle of these vortices is highly asymmetric with respect to the lateral bubble displacements, vortices being shed when the gap left between the bubble and the wall reaches its maximum, and reborn only when this gap comes back to its minimum.","sentences":["The buoyancy-driven motion of a deformable bubble rising near a vertical hydrophilic wall is studied numerically.","We focus on moderately inertial regimes in which the bubble undergoes low-to-moderate deformations and would rise in a straight line in the absence of the wall.","Three different types of near-wall motion are observed, depending on the buoyancy-to-viscous and buoyancy-to-capillary force ratios defining the Galilei ($Ga$) and Bond ($Bo$) numbers of the system, respectively.","For low enough $Ga$ or large enough $Bo$, bubbles consistently migrate away from the wall.","Conversely, for large enough $Ga$ and low enough $Bo$, they perform periodic near-wall bounces.","At intermediate $Ga$ and $Bo$, they are first attracted to the wall down to a certain critical distance, and then perform bounces with a decreasing amplitude before stabilizing at this critical separation.","Periodic bounces are accompanied by the shedding of a pair of streamwise vortices in the wake, the formation of which is governed by the near-wall shear resulting from the no-slip condition.","These vortices provide a repulsive force that overcomes the viscous resistance of the fluid to the departing motion, making the bubble capable of returning to the region where it is attracted again to the wall.","Although periodic, the shedding/regeneration cycle of these vortices is highly asymmetric with respect to the lateral bubble displacements, vortices being shed when the gap left between the bubble and the wall reaches its maximum, and reborn only when this gap comes back to its minimum."],"url":"http://arxiv.org/abs/2405.17912v1","category":"physics.flu-dyn"}
{"created":"2024-05-28 07:31:13","title":"Memory-induced absolute negative mobility","abstract":"Non-Markovian systems form a broad area of physics that remains greatly unexplored despite years of intensive investigations. The spotlight is on memory as a source of effects that are absent in their Markovian counterparts. In this work we dive into this problem and analyze a driven Brownian particle moving in a spatially periodic potential and exposed to correlated thermal noise. We show that the absolute negative mobility effect, in which the net movement of the particle is in direction opposite to the average force acting on it, may be induced by the memory of the setup. To explain the origin of this phenomenon we resort to the recently developed effective mass approach to dynamics of non-Markovian systems.","sentences":["Non-Markovian systems form a broad area of physics that remains greatly unexplored despite years of intensive investigations.","The spotlight is on memory as a source of effects that are absent in their Markovian counterparts.","In this work we dive into this problem and analyze a driven Brownian particle moving in a spatially periodic potential and exposed to correlated thermal noise.","We show that the absolute negative mobility effect, in which the net movement of the particle is in direction opposite to the average force acting on it, may be induced by the memory of the setup.","To explain the origin of this phenomenon we resort to the recently developed effective mass approach to dynamics of non-Markovian systems."],"url":"http://arxiv.org/abs/2405.17911v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-28 07:28:55","title":"Biunits of ternary algebra of hypermatrices","abstract":"In this paper we study ternary algebras of third-order hypermatrices. By hypermatrix we mean a complex-valued variable with three indices, which is also called a three-dimensional matrix or spatial matrix. We assume that a hypermatrix is defined in three-dimensional Euclidean space and when this space is rotated, it transforms as a SO(3)-tensor. We consider two ternary multiplications of hypermatrices, which have the property of generalized associativity. We explore the geometric meaning of two independent SO(3)-invariants of hypermatrices and show that one of them defines a Hermitian metric. We study the 10-dimensional subspace of hypermatrices, known in the theory of representations of the rotation group, as the space of the weight 2 tensor representation of the rotation group. It is proved that the elements of this subspace that satisfy the regularity condition are right biunits of the ternary algebra of hypermatrices. The motivation for studying biunits of ternary algebra of hypermatrices was a ternary generalization of the Pauli exclusion principle.","sentences":["In this paper we study ternary algebras of third-order hypermatrices.","By hypermatrix we mean a complex-valued variable with three indices, which is also called a three-dimensional matrix or spatial matrix.","We assume that a hypermatrix is defined in three-dimensional Euclidean space and when this space is rotated, it transforms as a SO(3)-tensor.","We consider two ternary multiplications of hypermatrices, which have the property of generalized associativity.","We explore the geometric meaning of two independent SO(3)-invariants of hypermatrices and show that one of them defines a Hermitian metric.","We study the 10-dimensional subspace of hypermatrices, known in the theory of representations of the rotation group, as the space of the weight 2 tensor representation of the rotation group.","It is proved that the elements of this subspace that satisfy the regularity condition are right biunits of the ternary algebra of hypermatrices.","The motivation for studying biunits of ternary algebra of hypermatrices was a ternary generalization of the Pauli exclusion principle."],"url":"http://arxiv.org/abs/2405.17907v1","category":"math.RA"}
{"created":"2024-05-28 07:17:57","title":"The expected evolution of the binary system PTF J2238+743015.1","abstract":"Binary systems made by a low-mass CO WD and a He-donor represent possible progenitors of explosive events via He-detonation, producing low-luminosity thermonuclear Supernovae with a peculiar nucleosynthetis. Recently, the binary system PTF J223857.11+743015.1 has been suggested as one. We investigate the evolution of the PTF J223857.11+743015.1 system, composed by a 0.75Msun CO WD and a 0.390Msun subdwarf, capped by a thin H-rich layer, considering rotation of the WD component. We compute the evolution of two stars simultaneously, accounting for the possible evolution of the orbital parameters, as determined by mass transfer between components and by mass ejection from the system during RLOF episodes. We consider that the WD gains angular momentum due to accretion and we follow the evolution of the angular velocity profile as due to angular momentum transport via convection and rotation-induced instabilities. As the donor H-rich envelope is transferred, the WD experiences recurrent very strong H-flashes triggering RLOF episodes during which the entire accreted matter is lost from the system. Due to mixing of chemicals by rotation-induced instabilities during the accretion phase, H-flashes occur inside the original WD. Hence, pulse-by pulse, the accretor mass is reduced down to 0.7453Msun. When He-rich matter is transferred, He-detonation does not occur in the rotating WD, which undergoes 6 very strong He-flashes and subsequent RLOF episodes. Also in this case, due to rotation-induced mixing of the accreted layers with the underlying core, the WD is eroded. Finally, when the mass transfer rate from the donor decreases, a massive He-buffer is piled-up onto the accretor which ends its life as a cooling WD. The binary system PTF J2238+743015.1 as all those binaries having similar components masses and orbital parameters are not good candidates as thermonuclear explosions progenitors.","sentences":["Binary systems made by a low-mass CO WD and a He-donor represent possible progenitors of explosive events via He-detonation, producing low-luminosity thermonuclear Supernovae with a peculiar nucleosynthetis.","Recently, the binary system PTF J223857.11+743015.1 has been suggested as one.","We investigate the evolution of the PTF J223857.11+743015.1 system, composed by a 0.75Msun CO WD and a 0.390Msun subdwarf, capped by a thin H-rich layer, considering rotation of the WD component.","We compute the evolution of two stars simultaneously, accounting for the possible evolution of the orbital parameters, as determined by mass transfer between components and by mass ejection from the system during RLOF episodes.","We consider that the WD gains angular momentum due to accretion and we follow the evolution of the angular velocity profile as due to angular momentum transport via convection and rotation-induced instabilities.","As the donor H-rich envelope is transferred, the WD experiences recurrent very strong H-flashes triggering RLOF episodes during which the entire accreted matter is lost from the system.","Due to mixing of chemicals by rotation-induced instabilities during the accretion phase, H-flashes occur inside the original WD.","Hence, pulse-by pulse, the accretor mass is reduced down to 0.7453Msun.","When He-rich matter is transferred, He-detonation does not occur in the rotating WD, which undergoes 6 very strong He-flashes and subsequent RLOF episodes.","Also in this case, due to rotation-induced mixing of the accreted layers with the underlying core, the WD is eroded.","Finally, when the mass transfer rate from the donor decreases, a massive He-buffer is piled-up onto the accretor which ends its life as a cooling WD.","The binary system PTF J2238+743015.1 as all those binaries having similar components masses and orbital parameters are not good candidates as thermonuclear explosions progenitors."],"url":"http://arxiv.org/abs/2405.17896v1","category":"astro-ph.SR"}
{"created":"2024-05-28 07:17:43","title":"Enhanced dissipation and temporal decay in the Euler-Poisson-Navier-Stokes equations","abstract":"This paper investigates the global well-posedness and large-time behavior of solutions for a coupled fluid model in $\\mathbb{R}^3$ consisting of the isothermal compressible Euler-Poisson system and incompressible Navier-Stokes equations coupled through the drag force. Notably, we exploit the dissipation effects inherent in the Poisson equation to achieve a faster decay of fluid density compared to velocities. This strategic utilization of dissipation, together with the influence of the electric field and the damping structure induced by the drag force, leads to a remarkable decay behavior: the fluid density converges to equilibrium at a rate of $(1+t)^{-11/4}$, significantly faster than the decay rates of velocity differences $(1+t)^{-7/4}$ and velocities themselves $(1+t)^{-3/4}$ in the $L^2$ norm. Furthermore, under the condition of vanishing coupled incompressible flow, we demonstrate an exponential decay to a constant state for the solution of the corresponding system, the damped Euler-Poisson system.","sentences":["This paper investigates the global well-posedness and large-time behavior of solutions for a coupled fluid model in $\\mathbb{R}^3$ consisting of the isothermal compressible Euler-Poisson system and incompressible Navier-Stokes equations coupled through the drag force.","Notably, we exploit the dissipation effects inherent in the Poisson equation to achieve a faster decay of fluid density compared to velocities.","This strategic utilization of dissipation, together with the influence of the electric field and the damping structure induced by the drag force, leads to a remarkable decay behavior: the fluid density converges to equilibrium at a rate of $(1+t)^{-11/4}$, significantly faster than the decay rates of velocity differences $(1+t)^{-7/4}$ and velocities themselves $(1+t)^{-3/4}$ in the $L^2$ norm.","Furthermore, under the condition of vanishing coupled incompressible flow, we demonstrate an exponential decay to a constant state for the solution of the corresponding system, the damped Euler-Poisson system."],"url":"http://arxiv.org/abs/2405.17895v1","category":"math.AP"}
{"created":"2024-05-28 07:12:29","title":"Data-Driven Predictive Control and MPC: Do we achieve optimality?","abstract":"In this paper, we explore the interplay between Predictive Control and closed-loop optimality, spanning from Model Predictive Control to Data-Driven Predictive Control. Predictive Control in general relies on some form of prediction scheme on the real system trajectories. However, these predictions may not accurately capture the real system dynamics, for e.g., due to stochasticity, resulting in sub-optimal control policies. This lack of optimality is a critical issue in case of problems with economic objectives. We address this by providing sufficient conditions on the underlying prediction scheme such that a Predictive Controller can achieve closed-loop optimality. However, these conditions do not readily extend to Data-Driven Predictive Control. In this context of closed-loop optimality, we conclude that the factor distinguishing the approaches within Data-Driven Predictive Control is if they can be cast as a sequential decision-making process or not, rather than the dichotomy of model-based vs. model-free. Furthermore, we show that the conventional approach of improving the prediction accuracy from data may not guarantee optimality.","sentences":["In this paper, we explore the interplay between Predictive Control and closed-loop optimality, spanning from Model Predictive Control to Data-Driven Predictive Control.","Predictive Control in general relies on some form of prediction scheme on the real system trajectories.","However, these predictions may not accurately capture the real system dynamics, for e.g., due to stochasticity, resulting in sub-optimal control policies.","This lack of optimality is a critical issue in case of problems with economic objectives.","We address this by providing sufficient conditions on the underlying prediction scheme such that a Predictive Controller can achieve closed-loop optimality.","However, these conditions do not readily extend to Data-Driven Predictive Control.","In this context of closed-loop optimality, we conclude that the factor distinguishing the approaches within Data-Driven Predictive Control is if they can be cast as a sequential decision-making process or not, rather than the dichotomy of model-based vs. model-free.","Furthermore, we show that the conventional approach of improving the prediction accuracy from data may not guarantee optimality."],"url":"http://arxiv.org/abs/2405.17892v1","category":"math.OC"}
{"created":"2024-05-28 07:12:06","title":"SLMRec: Empowering Small Language Models for Sequential Recommendation","abstract":"The sequential Recommendation (SR) task involves predicting the next item a user is likely to interact with, given their past interactions. The SR models examine the sequence of a user's actions to discern more complex behavioral patterns and temporal dynamics. Recent research demonstrates the great impact of LLMs on sequential recommendation systems, either viewing sequential recommendation as language modeling or serving as the backbone for user representation. Although these methods deliver outstanding performance, there is scant evidence of the necessity of a large language model and how large the language model is needed, especially in the sequential recommendation scene. Meanwhile, due to the huge size of LLMs, it is inefficient and impractical to apply a LLM-based model in real-world platforms that often need to process billions of traffic logs daily. In this paper, we explore the influence of LLMs' depth by conducting extensive experiments on large-scale industry datasets. Surprisingly, we discover that most intermediate layers of LLMs are redundant. Motivated by this insight, we empower small language models for SR, namely SLMRec, which adopt a simple yet effective knowledge distillation method. Moreover, SLMRec is orthogonal to other post-training efficiency techniques, such as quantization and pruning, so that they can be leveraged in combination. Comprehensive experimental results illustrate that the proposed SLMRec model attains the best performance using only 13% of the parameters found in LLM-based recommendation models, while simultaneously achieving up to 6.6x and 8.0x speedups in training and inference time costs, respectively.","sentences":["The sequential Recommendation (SR) task involves predicting the next item a user is likely to interact with, given their past interactions.","The SR models examine the sequence of a user's actions to discern more complex behavioral patterns and temporal dynamics.","Recent research demonstrates the great impact of LLMs on sequential recommendation systems, either viewing sequential recommendation as language modeling or serving as the backbone for user representation.","Although these methods deliver outstanding performance, there is scant evidence of the necessity of a large language model and how large the language model is needed, especially in the sequential recommendation scene.","Meanwhile, due to the huge size of LLMs, it is inefficient and impractical to apply a LLM-based model in real-world platforms that often need to process billions of traffic logs daily.","In this paper, we explore the influence of LLMs' depth by conducting extensive experiments on large-scale industry datasets.","Surprisingly, we discover that most intermediate layers of LLMs are redundant.","Motivated by this insight, we empower small language models for SR, namely SLMRec, which adopt a simple yet effective knowledge distillation method.","Moreover, SLMRec is orthogonal to other post-training efficiency techniques, such as quantization and pruning, so that they can be leveraged in combination.","Comprehensive experimental results illustrate that the proposed SLMRec model attains the best performance using only 13% of the parameters found in LLM-based recommendation models, while simultaneously achieving up to 6.6x and 8.0x speedups in training and inference time costs, respectively."],"url":"http://arxiv.org/abs/2405.17890v1","category":"cs.IR"}
{"created":"2024-05-28 07:09:42","title":"Graphomotor and Handwriting Disabilities Rating Scale (GHDRS):towards complex and objective assessment","abstract":"Graphomotor and handwriting disabilities (GD and HD, respectively) could significantly reduce children's quality of life. Effective remediation depends on proper diagnosis; however, current approaches to diagnosis and assessment of GD and HD have several limitations and knowledge gaps, e.g. they are subjective, they do not facilitate identification of specific manifestations, etc. The aim of this work is to introduce a new scale (GHDRS Graphomotor and Handwriting Disabilities Rating Scale) that will enable experts to perform objective and complex computeraided diagnosis and assessment of GD and HD. The scale supports quantification of 17 manifestations associated with the process/product of drawing/ handwriting. The whole methodology of GHDRS design is made maximally transparent so that it could be adapted for other languages.","sentences":["Graphomotor and handwriting disabilities (GD and HD, respectively) could significantly reduce children's quality of life.","Effective remediation depends on proper diagnosis; however, current approaches to diagnosis and assessment of GD and HD have several limitations and knowledge gaps, e.g. they are subjective, they do not facilitate identification of specific manifestations, etc.","The aim of this work is to introduce a new scale (GHDRS Graphomotor and Handwriting Disabilities Rating Scale) that will enable experts to perform objective and complex computeraided diagnosis and assessment of GD and HD.","The scale supports quantification of 17 manifestations associated with the process/product of drawing/ handwriting.","The whole methodology of GHDRS design is made maximally transparent so that it could be adapted for other languages."],"url":"http://arxiv.org/abs/2405.17886v1","category":"cs.CV"}
{"created":"2024-05-28 07:08:45","title":"On basic velocity estimates for the plane steady-state Navier-Stokes system and its applications","abstract":"We consider some new estimates for general steady Navier-Stokes solutions in plane domains. According to our main result, if the domain is convex, then the difference between mean values of the velocity over two concentric circles is bounded (up to a constant factor) by the square-root of the Dirichlet integral in the annulus between the circles. The constant factor in this inequality is universal and does not depend on the ratio of the circle radii. Several applications of these formulas are discussed.","sentences":["We consider some new estimates for general steady Navier-Stokes solutions in plane domains.","According to our main result, if the domain is convex, then the difference between mean values of the velocity over two concentric circles is bounded (up to a constant factor) by the square-root of the Dirichlet integral in the annulus between the circles.","The constant factor in this inequality is universal and does not depend on the ratio of the circle radii.","Several applications of these formulas are discussed."],"url":"http://arxiv.org/abs/2405.17884v1","category":"math.AP"}
{"created":"2024-05-28 07:03:49","title":"Crystal-LSBO: Automated Design of De Novo Crystals with Latent Space Bayesian Optimization","abstract":"Generative modeling of crystal structures is significantly challenged by the complexity of input data, which constrains the ability of these models to explore and discover novel crystals. This complexity often confines de novo design methodologies to merely small perturbations of known crystals and hampers the effective application of advanced optimization techniques. One such optimization technique, Latent Space Bayesian Optimization (LSBO) has demonstrated promising results in uncovering novel objects across various domains, especially when combined with Variational Autoencoders (VAEs). Recognizing LSBO's potential and the critical need for innovative crystal discovery, we introduce Crystal-LSBO, a de novo design framework for crystals specifically tailored to enhance explorability within LSBO frameworks. Crystal-LSBO employs multiple VAEs, each dedicated to a distinct aspect of crystal structure: lattice, coordinates, and chemical elements, orchestrated by an integrative model that synthesizes these components into a cohesive output. This setup not only streamlines the learning process but also produces explorable latent spaces thanks to the decreased complexity of the learning task for each model, enabling LSBO approaches to operate. Our study pioneers the use of LSBO for de novo crystal design, demonstrating its efficacy through optimization tasks focused mainly on formation energy values. Our results highlight the effectiveness of our methodology, offering a new perspective for de novo crystal discovery.","sentences":["Generative modeling of crystal structures is significantly challenged by the complexity of input data, which constrains the ability of these models to explore and discover novel crystals.","This complexity often confines de novo design methodologies to merely small perturbations of known crystals and hampers the effective application of advanced optimization techniques.","One such optimization technique, Latent Space Bayesian Optimization (LSBO) has demonstrated promising results in uncovering novel objects across various domains, especially when combined with Variational Autoencoders (VAEs).","Recognizing LSBO's potential and the critical need for innovative crystal discovery, we introduce Crystal-LSBO, a de novo design framework for crystals specifically tailored to enhance explorability within LSBO frameworks.","Crystal-LSBO employs multiple VAEs, each dedicated to a distinct aspect of crystal structure: lattice, coordinates, and chemical elements, orchestrated by an integrative model that synthesizes these components into a cohesive output.","This setup not only streamlines the learning process but also produces explorable latent spaces thanks to the decreased complexity of the learning task for each model, enabling LSBO approaches to operate.","Our study pioneers the use of LSBO for de novo crystal design, demonstrating its efficacy through optimization tasks focused mainly on formation energy values.","Our results highlight the effectiveness of our methodology, offering a new perspective for de novo crystal discovery."],"url":"http://arxiv.org/abs/2405.17881v1","category":"cs.LG"}
{"created":"2024-05-28 06:52:17","title":"BO4IO: A Bayesian optimization approach to inverse optimization with uncertainty quantification","abstract":"This work addresses data-driven inverse optimization (IO), where the goal is to estimate unknown parameters in an optimization model from observed decisions that can be assumed to be optimal or near-optimal solutions to the optimization problem. The IO problem is commonly formulated as a large-scale bilevel program that is notoriously difficult to solve. Deviating from traditional exact solution methods, we propose a derivative-free optimization approach based on Bayesian optimization, which we call BO4IO, to solve general IO problems. We treat the IO loss function as a black box and approximate it with a Gaussian process model. Using the predicted posterior function, an acquisition function is minimized at each iteration to query new candidate solutions and sequentially converge to the optimal parameter estimates. The main advantages of using Bayesian optimization for IO are two-fold: (i) it circumvents the need of complex reformulations of the bilevel program or specialized algorithms and can hence enable computational tractability even when the underlying optimization problem is nonconvex or involves discrete variables, and (ii) it allows approximations of the profile likelihood, which provide uncertainty quantification on the IO parameter estimates. We apply the proposed method to three computational case studies, covering different classes of forward optimization problems ranging from convex nonlinear to nonconvex mixed-integer nonlinear programs. Our extensive computational results demonstrate the efficacy and robustness of BO4IO to accurately estimate unknown model parameters from small and noisy datasets. In addition, the proposed profile likelihood analysis has proven to be effective in providing good approximations of the confidence intervals on the parameter estimates and assessing the identifiability of the unknown parameters.","sentences":["This work addresses data-driven inverse optimization (IO), where the goal is to estimate unknown parameters in an optimization model from observed decisions that can be assumed to be optimal or near-optimal solutions to the optimization problem.","The IO problem is commonly formulated as a large-scale bilevel program that is notoriously difficult to solve.","Deviating from traditional exact solution methods, we propose a derivative-free optimization approach based on Bayesian optimization, which we call BO4IO, to solve general IO problems.","We treat the IO loss function as a black box and approximate it with a Gaussian process model.","Using the predicted posterior function, an acquisition function is minimized at each iteration to query new candidate solutions and sequentially converge to the optimal parameter estimates.","The main advantages of using Bayesian optimization for IO are two-fold: (i) it circumvents the need of complex reformulations of the bilevel program or specialized algorithms and can hence enable computational tractability even when the underlying optimization problem is nonconvex or involves discrete variables, and (ii) it allows approximations of the profile likelihood, which provide uncertainty quantification on the IO parameter estimates.","We apply the proposed method to three computational case studies, covering different classes of forward optimization problems ranging from convex nonlinear to nonconvex mixed-integer nonlinear programs.","Our extensive computational results demonstrate the efficacy and robustness of BO4IO to accurately estimate unknown model parameters from small and noisy datasets.","In addition, the proposed profile likelihood analysis has proven to be effective in providing good approximations of the confidence intervals on the parameter estimates and assessing the identifiability of the unknown parameters."],"url":"http://arxiv.org/abs/2405.17875v1","category":"math.OC"}
{"created":"2024-05-28 06:44:09","title":"Full-Stack Allreduce on Multi-Rail Networks","abstract":"The high communication costs impede scalability in distributed systems. Multimodal models like Sora exacerbate this issue by requiring more resources than current networks can support. However, existing network architectures fail to address this gap. In this paper, we provide full-stack support for allreduce on multi-rail networks, aiming to overcome the scalability limitations of large-scale networks by facilitating collaborative data transfer across various networks. To achieve this, we propose the Nezha system, which integrates TCP, in-network computing protocol SHARP, and RDMA-based protocol GLEX. To maximize data transfer rates, Nezha incorporates a load balancing data allocation scheme based on cost feedback and combines exception handling to achieve reliable data transmission. Our experiments on a six-node cluster demonstrate that Nezha significantly enhances allreduce performance by 58\\% to 87\\% in homogeneous dual-rail configurations and offers considerable acceleration in heterogeneous settings, contingent on the performance variance among networks.","sentences":["The high communication costs impede scalability in distributed systems.","Multimodal models like Sora exacerbate this issue by requiring more resources than current networks can support.","However, existing network architectures fail to address this gap.","In this paper, we provide full-stack support for allreduce on multi-rail networks, aiming to overcome the scalability limitations of large-scale networks by facilitating collaborative data transfer across various networks.","To achieve this, we propose the Nezha system, which integrates TCP, in-network computing protocol SHARP, and RDMA-based protocol GLEX.","To maximize data transfer rates, Nezha incorporates a load balancing data allocation scheme based on cost feedback and combines exception handling to achieve reliable data transmission.","Our experiments on a six-node cluster demonstrate that Nezha significantly enhances allreduce performance by 58\\% to 87\\% in homogeneous dual-rail configurations and offers considerable acceleration in heterogeneous settings, contingent on the performance variance among networks."],"url":"http://arxiv.org/abs/2405.17870v1","category":"cs.DC"}
{"created":"2024-05-28 06:33:43","title":"Nonreciprocal singularities dominated by the dissipative photon-magnon coupling in non-Hermitian systems","abstract":"We investigated the magnon-photon coupling in an open cavity magnonic system, which leads to two different nonreciprocal singularities dominated by the dissipative coupling. One type of singularity is the exceptional point, which is just on the exceptional surface in parameter space. The other type of singularity is the bound state in the continuum discovered in the level-attraction-like coupling, which is above the exceptional surface. In experiment, we realized the two different singularities with nonreciprocity and selectivity in an open cavity magnonic system with suitable dissipation rating. Our results can be understood well with the pseudo-Hermitian theory of magnon-polariton system.","sentences":["We investigated the magnon-photon coupling in an open cavity magnonic system, which leads to two different nonreciprocal singularities dominated by the dissipative coupling.","One type of singularity is the exceptional point, which is just on the exceptional surface in parameter space.","The other type of singularity is the bound state in the continuum discovered in the level-attraction-like coupling, which is above the exceptional surface.","In experiment, we realized the two different singularities with nonreciprocity and selectivity in an open cavity magnonic system with suitable dissipation rating.","Our results can be understood well with the pseudo-Hermitian theory of magnon-polariton system."],"url":"http://arxiv.org/abs/2405.17869v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-28 06:33:21","title":"An algorithm applied the Turing pattern model to control active swarm robots using only information from neighboring modules","abstract":"Swarm robots, inspired by the emergence of animal herds, are robots that assemble a large number of modules and self-organize themselves to form specific morphologies and exhibit specific functions. These modular robots perform relatively simple actions and controls, and create macroscopic morphologies and functions through the interaction of a large number of modular robots. This research focuses on such self-organizing robots or swarm robots. The proposed algorithm is a model that applies the Turing pattern, one of the self-organization models, to make a group of modules accumulate and stay within a certain region. The proposed method utilizes the area within the spots of the Turing pattern as the aggregation region of the modules. Furthermore, it considers the value corresponding to the concentration distribution within the spotted pattern of the Turing pattern model (referred to as the potential value in this research), identifies the center of the region (spotted pattern), and makes it the center of the module group. By controlling the modules in the direction of the higher potential value, it succeeds in maintaining the shape of the module group as a whole while moving. The algorithm was validated using a two-dimensional simulation model. The unit module robot was assumed to have the following properties: 1) limited self-drive, 2) no module identifier, 3) information exchange only with adjacent modules, 4) no coordinate system, and 5) only simple arithmetic and memory functions. Using these modules, the devised algorithm was able to achieve not only the creation of static forms but also the realization of the following movements: 1) modules accumulate and grow, 2) modules move to the light source, 3) exit the gap while maintaining its shape, and 4) self-replication.","sentences":["Swarm robots, inspired by the emergence of animal herds, are robots that assemble a large number of modules and self-organize themselves to form specific morphologies and exhibit specific functions.","These modular robots perform relatively simple actions and controls, and create macroscopic morphologies and functions through the interaction of a large number of modular robots.","This research focuses on such self-organizing robots or swarm robots.","The proposed algorithm is a model that applies the Turing pattern, one of the self-organization models, to make a group of modules accumulate and stay within a certain region.","The proposed method utilizes the area within the spots of the Turing pattern as the aggregation region of the modules.","Furthermore, it considers the value corresponding to the concentration distribution within the spotted pattern of the Turing pattern model (referred to as the potential value in this research), identifies the center of the region (spotted pattern), and makes it the center of the module group.","By controlling the modules in the direction of the higher potential value, it succeeds in maintaining the shape of the module group as a whole while moving.","The algorithm was validated using a two-dimensional simulation model.","The unit module robot was assumed to have the following properties: 1) limited self-drive, 2) no module identifier, 3) information exchange only with adjacent modules, 4) no coordinate system, and 5) only simple arithmetic and memory functions.","Using these modules, the devised algorithm was able to achieve not only the creation of static forms but also the realization of the following movements: 1) modules accumulate and grow, 2) modules move to the light source, 3) exit the gap while maintaining its shape, and 4) self-replication."],"url":"http://arxiv.org/abs/2405.17868v1","category":"cs.RO"}
{"created":"2024-05-28 06:25:05","title":"Quantum Integrable Systems on a Classical Integrable Background","abstract":"In this paper, we develop the framework for quantum integrable systems on an integrable classical background. We call them hybrid quantum integrable systems (hybrid integrable systems), and we show that they occur naturally in the semiclassical limit of quantum integrable systems. We start with an outline of the concept of hybrid dynamical systems. Then we give several examples of hybrid integrable systems. The first series of examples is a class of hybrid integrable systems that appear in the semiclassical limit of quantum spin chains. Then we look at the semiclassical limit of the quantum spin Calogero--Moser system. The result is a hybrid integrable system driven by usual classical Calogero--Moser (CM) dynamics. This system at the fixed point of the multi-time classical dynamics CM system gives commuting spin Hamiltonians of Haldane--Shastry model.","sentences":["In this paper, we develop the framework for quantum integrable systems on an integrable classical background.","We call them hybrid quantum integrable systems (hybrid integrable systems), and we show that they occur naturally in the semiclassical limit of quantum integrable systems.","We start with an outline of the concept of hybrid dynamical systems.","Then we give several examples of hybrid integrable systems.","The first series of examples is a class of hybrid integrable systems that appear in the semiclassical limit of quantum spin chains.","Then we look at the semiclassical limit of the quantum spin Calogero--Moser system.","The result is a hybrid integrable system driven by usual classical Calogero--Moser (CM) dynamics.","This system at the fixed point of the multi-time classical dynamics CM system gives commuting spin Hamiltonians of Haldane--Shastry model."],"url":"http://arxiv.org/abs/2405.17865v1","category":"math-ph"}
{"created":"2024-05-28 06:15:09","title":"The structure of Ferroelectric BaBiO$_3$/BaTiO$_3$ Interfaces grown by Molecular Beam Epitaxy","abstract":"We investigate the lattice structure of heterostructures comprising of ferroelectric BaTiO$_3$ (BTO) thin films and BaBiO$_3$ (BBO), the insulating parent-compound of the high Tc superconductor. Motivated by theoretical predictions of exotic phenomena in BBO-based heterostructures including interfacial conductivity, superconductivity and topologically-protected states, we synthesize BTO/BBO heterostructures by molecular beam epitaxy and characterize their structural properties by in-situ reflection high energy electron diffraction and ex-situ high-resolution synchrotron X-ray diffraction, Raman spectroscopy and piezoforce microscopy. For heterostructures with 4 uc BBO layers, reciprocal space maps indicate strain relaxation relative to the SrTiO$_3$ substrate. We observe a strong tetragonal distortion for heterostructures with 2 unit cells BBO coherently strained to the BTO layers. Raman spectroscopy measurements indicate a suppression of the breathing mode distortion associated with the charge density wave in bulk BBO. The ferroelectric properties of the system are confirmed by piezoforce microscopy measurements. The coupling between the ferroelectric polarization and the electronic states in BBO may potentially serve as a starting point for tunable electrostatic doping to realize the novel predicted states in the atomically-thin BBO layers.","sentences":["We investigate the lattice structure of heterostructures comprising of ferroelectric BaTiO$_3$ (BTO) thin films and BaBiO$_3$ (BBO), the insulating parent-compound of the high Tc superconductor.","Motivated by theoretical predictions of exotic phenomena in BBO-based heterostructures including interfacial conductivity, superconductivity and topologically-protected states, we synthesize BTO/BBO heterostructures by molecular beam epitaxy and characterize their structural properties by in-situ reflection high energy electron diffraction and ex-situ high-resolution synchrotron X-ray diffraction, Raman spectroscopy and piezoforce microscopy.","For heterostructures with 4 uc BBO layers, reciprocal space maps indicate strain relaxation relative to the SrTiO$_3$ substrate.","We observe a strong tetragonal distortion for heterostructures with 2 unit cells BBO coherently strained to the BTO layers.","Raman spectroscopy measurements indicate a suppression of the breathing mode distortion associated with the charge density wave in bulk BBO.","The ferroelectric properties of the system are confirmed by piezoforce microscopy measurements.","The coupling between the ferroelectric polarization and the electronic states in BBO may potentially serve as a starting point for tunable electrostatic doping to realize the novel predicted states in the atomically-thin BBO layers."],"url":"http://arxiv.org/abs/2405.17856v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-28 05:55:02","title":"Generic quartic solitons in optical media","abstract":"Our analysis suggests strongly that stationary pulses exist in nonlinear media with second-, third-, and fourth-order dispersion. A theory, based on the variational approach, is developed for finding approximate parameters of such solitons. It is obtained that the soliton velocity in the retarded reference frame can be different from the inverse of the group velocity of linear waves. It is shown that the interaction of the pulse spectrum with that of linear waves can affect the existence of stationary solitons. These theoretical results are supported by numerical simulations. Transformations between solitons of different systems are derived. A generalization for solitons in media with the highest even-order dispersion is suggested.","sentences":["Our analysis suggests strongly that stationary pulses exist in nonlinear media with second-, third-, and fourth-order dispersion.","A theory, based on the variational approach, is developed for finding approximate parameters of such solitons.","It is obtained that the soliton velocity in the retarded reference frame can be different from the inverse of the group velocity of linear waves.","It is shown that the interaction of the pulse spectrum with that of linear waves can affect the existence of stationary solitons.","These theoretical results are supported by numerical simulations.","Transformations between solitons of different systems are derived.","A generalization for solitons in media with the highest even-order dispersion is suggested."],"url":"http://arxiv.org/abs/2405.17848v1","category":"nlin.PS"}
{"created":"2024-05-28 05:49:32","title":"Multi-Wheeled Passive Sliding with Fully-Actuated Aerial Robots: Tip-Over Recovery and Avoidance","abstract":"Recently, the utilization of aerial manipulators for performing pushing tasks in non-destructive testing (NDT) applications has seen significant growth. Such operations entail physical interactions between the aerial robotic system and the environment. End-effectors with multiple contact points are often used for placing NDT sensors in contact with a surface to be inspected. Aligning the NDT sensor and the work surface while preserving contact, requires that all available contact points at the end-effector tip are in contact with the work surface. With a standard full-pose controller, attitude errors often occur due to perturbations caused by modeling uncertainties, sensor noise, and environmental uncertainties. Even small attitude errors can cause a loss of contact points between the end-effector tip and the work surface. To preserve full alignment amidst these uncertainties, we propose a control strategy which selectively deactivates angular motion control and enables direct force control in specific directions. In particular, we derive two essential conditions to be met, such that the robot can passively align with flat work surfaces achieving full alignment through the rotation along non-actively controlled axes. Additionally, these conditions serve as hardware design and control guidelines for effectively integrating the proposed control method for practical usage. Real world experiments are conducted to validate both the control design and the guidelines.","sentences":["Recently, the utilization of aerial manipulators for performing pushing tasks in non-destructive testing (NDT) applications has seen significant growth.","Such operations entail physical interactions between the aerial robotic system and the environment.","End-effectors with multiple contact points are often used for placing NDT sensors in contact with a surface to be inspected.","Aligning the NDT sensor and the work surface while preserving contact, requires that all available contact points at the end-effector tip are in contact with the work surface.","With a standard full-pose controller, attitude errors often occur due to perturbations caused by modeling uncertainties, sensor noise, and environmental uncertainties.","Even small attitude errors can cause a loss of contact points between the end-effector tip and the work surface.","To preserve full alignment amidst these uncertainties, we propose a control strategy which selectively deactivates angular motion control and enables direct force control in specific directions.","In particular, we derive two essential conditions to be met, such that the robot can passively align with flat work surfaces achieving full alignment through the rotation along non-actively controlled axes.","Additionally, these conditions serve as hardware design and control guidelines for effectively integrating the proposed control method for practical usage.","Real world experiments are conducted to validate both the control design and the guidelines."],"url":"http://arxiv.org/abs/2405.17844v1","category":"cs.RO"}
{"created":"2024-05-28 17:57:15","title":"A physics-inspired evolutionary machine learning method: from the Schr\u00f6dinger equation to an orbital-free-DFT kinetic energy functional","abstract":"We introduce a machine learning (ML) supervised model function that is inspired by the variational principle of physics. This ML hypothesis evolutionary method, termed ML-Omega, allows us to go from data to differential equation(s) underlying the physical (chemical, engineering, etc.) phenomena the data are derived from. The fundamental equations of physics can be derived from this ML-Omega evolutionary method when provided the proper training data. By training the ML-Omega model function with only three hydrogen-like atom energies, the method can find Schr\\\"odinger's exact functional and, from it, Schr\\\"odinger's fundamental equation. Then, in the field of density functional theory (DFT), when the model function is trained with the energies from the known Thomas-Fermi (TF) formula E = -0.7687Z^7/3, it correctly finds the exact TF functional. Finally, the method is applied to find a local orbital-free (OF) functional expression of the independent electron kinetic energy functional Ts based on the gamma-TF-lambda-vW model. By considering the theoretical energies of only 5 atoms (He, Be, Ne, Mg, Ar) as the training set, the evolutionary ML-Omega method finds an ML-Omega-OF-DFT local Ts functional (gamma-TF-lambda-vW (0.964, 1/4)) that outperforms all the OF- DFT functionals of a representative group. Moreover, our ML-Omega-OF functional overcomes the LDA's and some local GGA-DFT's functionals' difficulty to describe the stretched bond region at the correct spin configuration of diatomic molecules. Although our evolutionary ML-Omega model function can work without an explicit prior-form functional, by using the techniques of symbolic regression, in this work we exploit prior-form functional expressions to make the training process faster in the example problems presented here.","sentences":["We introduce a machine learning (ML) supervised model function that is inspired by the variational principle of physics.","This ML hypothesis evolutionary method, termed ML-Omega, allows us to go from data to differential equation(s) underlying the physical (chemical, engineering, etc.)","phenomena the data are derived from.","The fundamental equations of physics can be derived from this ML-Omega evolutionary method when provided the proper training data.","By training the ML-Omega model function with only three hydrogen-like atom energies, the method can find Schr\\\"odinger's exact functional and, from it, Schr\\\"odinger's fundamental equation.","Then, in the field of density functional theory (DFT), when the model function is trained with the energies from the known Thomas-Fermi (TF) formula E = -0.7687Z^7/3, it correctly finds the exact TF functional.","Finally, the method is applied to find a local orbital-free (OF) functional expression of the independent electron kinetic energy functional Ts based on the gamma-TF-lambda-vW model.","By considering the theoretical energies of only 5 atoms (He, Be, Ne, Mg, Ar) as the training set, the evolutionary ML-Omega method finds an ML-Omega-OF-DFT local Ts functional (gamma-TF-lambda-vW (0.964, 1/4)) that outperforms all the OF- DFT functionals of a representative group.","Moreover, our ML-Omega-OF functional overcomes the LDA's and some local GGA-DFT's functionals' difficulty to describe the stretched bond region at the correct spin configuration of diatomic molecules.","Although our evolutionary ML-Omega model function can work without an explicit prior-form functional, by using the techniques of symbolic regression, in this work we exploit prior-form functional expressions to make the training process faster in the example problems presented here."],"url":"http://arxiv.org/abs/2405.18417v1","category":"physics.chem-ph"}
{"created":"2024-05-28 17:11:34","title":"A Hessian-Aware Stochastic Differential Equation for Modelling SGD","abstract":"Continuous-time approximation of Stochastic Gradient Descent (SGD) is a crucial tool to study its escaping behaviors from stationary points. However, existing stochastic differential equation (SDE) models fail to fully capture these behaviors, even for simple quadratic objectives. Built on a novel stochastic backward error analysis framework, we derive the Hessian-Aware Stochastic Modified Equation (HA-SME), an SDE that incorporates Hessian information of the objective function into both its drift and diffusion terms. Our analysis shows that HA-SME matches the order-best approximation error guarantee among existing SDE models in the literature, while achieving a significantly reduced dependence on the smoothness parameter of the objective. Further, for quadratic objectives, under mild conditions, HA-SME is proved to be the first SDE model that recovers exactly the SGD dynamics in the distributional sense. Consequently, when the local landscape near a stationary point can be approximated by quadratics, HA-SME is expected to accurately predict the local escaping behaviors of SGD.","sentences":["Continuous-time approximation of Stochastic Gradient Descent (SGD) is a crucial tool to study its escaping behaviors from stationary points.","However, existing stochastic differential equation (SDE) models fail to fully capture these behaviors, even for simple quadratic objectives.","Built on a novel stochastic backward error analysis framework, we derive the Hessian-Aware Stochastic Modified Equation (HA-SME), an SDE that incorporates Hessian information of the objective function into both its drift and diffusion terms.","Our analysis shows that HA-SME matches the order-best approximation error guarantee among existing SDE models in the literature, while achieving a significantly reduced dependence on the smoothness parameter of the objective.","Further, for quadratic objectives, under mild conditions, HA-SME is proved to be the first SDE model that recovers exactly the SGD dynamics in the distributional sense.","Consequently, when the local landscape near a stationary point can be approximated by quadratics, HA-SME is expected to accurately predict the local escaping behaviors of SGD."],"url":"http://arxiv.org/abs/2405.18373v1","category":"stat.ML"}
{"created":"2024-05-28 16:44:02","title":"Can Automatic Metrics Assess High-Quality Translations?","abstract":"Automatic metrics for evaluating translation quality are typically validated by measuring how well they correlate with human assessments. However, correlation methods tend to capture only the ability of metrics to differentiate between good and bad source-translation pairs, overlooking their reliability in distinguishing alternative translations for the same source. In this paper, we confirm that this is indeed the case by showing that current metrics are insensitive to nuanced differences in translation quality. This effect is most pronounced when the quality is high and the variance among alternatives is low. Given this finding, we shift towards detecting high-quality correct translations, an important problem in practical decision-making scenarios where a binary check of correctness is prioritized over a nuanced evaluation of quality. Using the MQM framework as the gold standard, we systematically stress-test the ability of current metrics to identify translations with no errors as marked by humans. Our findings reveal that current metrics often over or underestimate translation quality, indicating significant room for improvement in automatic evaluation methods.","sentences":["Automatic metrics for evaluating translation quality are typically validated by measuring how well they correlate with human assessments.","However, correlation methods tend to capture only the ability of metrics to differentiate between good and bad source-translation pairs, overlooking their reliability in distinguishing alternative translations for the same source.","In this paper, we confirm that this is indeed the case by showing that current metrics are insensitive to nuanced differences in translation quality.","This effect is most pronounced when the quality is high and the variance among alternatives is low.","Given this finding, we shift towards detecting high-quality correct translations, an important problem in practical decision-making scenarios where a binary check of correctness is prioritized over a nuanced evaluation of quality.","Using the MQM framework as the gold standard, we systematically stress-test the ability of current metrics to identify translations with no errors as marked by humans.","Our findings reveal that current metrics often over or underestimate translation quality, indicating significant room for improvement in automatic evaluation methods."],"url":"http://arxiv.org/abs/2405.18348v1","category":"cs.CL"}
{"created":"2024-05-28 16:20:33","title":"Value Alignment and Trust in Human-Robot Interaction: Insights from Simulation and User Study","abstract":"With the advent of AI technologies, humans and robots are increasingly teaming up to perform collaborative tasks. To enable smooth and effective collaboration, the topic of value alignment (operationalized herein as the degree of dynamic goal alignment within a task) between the robot and the human is gaining increasing research attention. Prior literature on value alignment makes an inherent assumption that aligning the values of the robot with that of the human benefits the team. This assumption, however, has not been empirically verified. Moreover, prior literature does not account for human's trust in the robot when analyzing human-robot value alignment. Thus, a research gap needs to be bridged by answering two questions: How does alignment of values affect trust? Is it always beneficial to align the robot's values with that of the human? We present a simulation study and a human-subject study to answer these questions. Results from the simulation study show that alignment of values is important for trust when the overall risk level of the task is high. We also present an adaptive strategy for the robot that uses Inverse Reinforcement Learning (IRL) to match the values of the robot with those of the human during interaction. Our simulations suggest that such an adaptive strategy is able to maintain trust across the full spectrum of human values. We also present results from an empirical study that validate these findings from simulation. Results indicate that real-time personalized value alignment is beneficial to trust and perceived performance by the human when the robot does not have a good prior on the human's values.","sentences":["With the advent of AI technologies, humans and robots are increasingly teaming up to perform collaborative tasks.","To enable smooth and effective collaboration, the topic of value alignment (operationalized herein as the degree of dynamic goal alignment within a task) between the robot and the human is gaining increasing research attention.","Prior literature on value alignment makes an inherent assumption that aligning the values of the robot with that of the human benefits the team.","This assumption, however, has not been empirically verified.","Moreover, prior literature does not account for human's trust in the robot when analyzing human-robot value alignment.","Thus, a research gap needs to be bridged by answering two questions: How does alignment of values affect trust?","Is it always beneficial to align the robot's values with that of the human?","We present a simulation study and a human-subject study to answer these questions.","Results from the simulation study show that alignment of values is important for trust when the overall risk level of the task is high.","We also present an adaptive strategy for the robot that uses Inverse Reinforcement Learning (IRL) to match the values of the robot with those of the human during interaction.","Our simulations suggest that such an adaptive strategy is able to maintain trust across the full spectrum of human values.","We also present results from an empirical study that validate these findings from simulation.","Results indicate that real-time personalized value alignment is beneficial to trust and perceived performance by the human when the robot does not have a good prior on the human's values."],"url":"http://arxiv.org/abs/2405.18324v1","category":"cs.RO"}
{"created":"2024-05-28 16:11:09","title":"Impact of the radial profile of atomic nuclei on observables in high-energy collisions","abstract":"In heavy-ion phenomenology, the nucleon density distribution in colliding nuclei is commonly described by a two-parameter Woods-Saxon (WS) distribution. However, this approach omits the detailed radial structure in the density distribution that arises from quantal filling patterns of neutrons and protons. These fine structures, as estimated by the Skyrme-Hartree-Fock density functional, cause small deviations in heavy-ion observables from the WS baseline, which cannot be captured by simply readjusting the WS parameters. These deviations are dependent on centrality and observable but often exhibit similar shapes for different nuclei. Such fine structures may introduce up to a 25% uncertainty in the measured differences in heavy-ion observables between the $^{96}$Ru+$^{96}$Ru and $^{96}$Zr+$^{96}$Zr mid-central collisions from the STAR Collaboration.","sentences":["In heavy-ion phenomenology, the nucleon density distribution in colliding nuclei is commonly described by a two-parameter Woods-Saxon (WS) distribution.","However, this approach omits the detailed radial structure in the density distribution that arises from quantal filling patterns of neutrons and protons.","These fine structures, as estimated by the Skyrme-Hartree-Fock density functional, cause small deviations in heavy-ion observables from the WS baseline, which cannot be captured by simply readjusting the WS parameters.","These deviations are dependent on centrality and observable but often exhibit similar shapes for different nuclei.","Such fine structures may introduce up to a 25% uncertainty in the measured differences in heavy-ion observables between the $^{96}$Ru+$^{96}$Ru and $^{96}$Zr+$^{96}$Zr mid-central collisions from the STAR Collaboration."],"url":"http://arxiv.org/abs/2405.18318v1","category":"nucl-th"}
{"created":"2024-05-28 16:02:11","title":"Deterministic and statistical calibration of constitutive models from full-field data with parametric physics-informed neural networks","abstract":"The calibration of constitutive models from full-field data has recently gained increasing interest due to improvements in full-field measurement capabilities. In addition to the experimental characterization of novel materials, continuous structural health monitoring is another application that is of great interest. However, monitoring is usually associated with severe time constraints, difficult to meet with standard numerical approaches. Therefore, parametric physics-informed neural networks (PINNs) for constitutive model calibration from full-field displacement data are investigated. In an offline stage, a parametric PINN can be trained to learn a parameterized solution of the underlying partial differential equation. In the subsequent online stage, the parametric PINN then acts as a surrogate for the parameters-to-state map in calibration. We test the proposed approach for the deterministic least-squares calibration of a linear elastic as well as a hyperelastic constitutive model from noisy synthetic displacement data. We further carry out Markov chain Monte Carlo-based Bayesian inference to quantify the uncertainty. A proper statistical evaluation of the results underlines the high accuracy of the deterministic calibration and that the estimated uncertainty is valid. Finally, we consider experimental data and show that the results are in good agreement with a Finite Element Method-based calibration. Due to the fast evaluation of PINNs, calibration can be performed in near real-time. This advantage is particularly evident in many-query applications such as Markov chain Monte Carlo-based Bayesian inference.","sentences":["The calibration of constitutive models from full-field data has recently gained increasing interest due to improvements in full-field measurement capabilities.","In addition to the experimental characterization of novel materials, continuous structural health monitoring is another application that is of great interest.","However, monitoring is usually associated with severe time constraints, difficult to meet with standard numerical approaches.","Therefore, parametric physics-informed neural networks (PINNs) for constitutive model calibration from full-field displacement data are investigated.","In an offline stage, a parametric PINN can be trained to learn a parameterized solution of the underlying partial differential equation.","In the subsequent online stage, the parametric PINN then acts as a surrogate for the parameters-to-state map in calibration.","We test the proposed approach for the deterministic least-squares calibration of a linear elastic as well as a hyperelastic constitutive model from noisy synthetic displacement data.","We further carry out Markov chain Monte Carlo-based Bayesian inference to quantify the uncertainty.","A proper statistical evaluation of the results underlines the high accuracy of the deterministic calibration and that the estimated uncertainty is valid.","Finally, we consider experimental data and show that the results are in good agreement with a Finite Element Method-based calibration.","Due to the fast evaluation of PINNs, calibration can be performed in near real-time.","This advantage is particularly evident in many-query applications such as Markov chain Monte Carlo-based Bayesian inference."],"url":"http://arxiv.org/abs/2405.18311v1","category":"cs.LG"}
{"created":"2024-05-28 16:00:23","title":"Learning Staged Trees from Incomplete Data","abstract":"Staged trees are probabilistic graphical models capable of representing any class of non-symmetric independence via a coloring of its vertices. Several structural learning routines have been defined and implemented to learn staged trees from data, under the frequentist or Bayesian paradigm. They assume a data set has been observed fully and, in practice, observations with missing entries are either dropped or imputed before learning the model. Here, we introduce the first algorithms for staged trees that handle missingness within the learning of the model. To this end, we characterize the likelihood of staged tree models in the presence of missing data and discuss pseudo-likelihoods that approximate it. A structural expectation-maximization algorithm estimating the model directly from the full likelihood is also implemented and evaluated. A computational experiment showcases the performance of the novel learning algorithms, demonstrating that it is feasible to account for different missingness patterns when learning staged trees.","sentences":["Staged trees are probabilistic graphical models capable of representing any class of non-symmetric independence via a coloring of its vertices.","Several structural learning routines have been defined and implemented to learn staged trees from data, under the frequentist or Bayesian paradigm.","They assume a data set has been observed fully and, in practice, observations with missing entries are either dropped or imputed before learning the model.","Here, we introduce the first algorithms for staged trees that handle missingness within the learning of the model.","To this end, we characterize the likelihood of staged tree models in the presence of missing data and discuss pseudo-likelihoods that approximate it.","A structural expectation-maximization algorithm estimating the model directly from the full likelihood is also implemented and evaluated.","A computational experiment showcases the performance of the novel learning algorithms, demonstrating that it is feasible to account for different missingness patterns when learning staged trees."],"url":"http://arxiv.org/abs/2405.18306v1","category":"stat.ML"}
{"created":"2024-05-28 15:48:27","title":"Three quantitative versions of the P\u00e1l inequality","abstract":"The P\\'al inequality is a classical result which asserts that among all planar convex sets of given width the equilateral triangle is the one of minimal area. In this paper we prove three quantitative versions of this inequality, by quantifying how the closeness of the area of a convex set, of certain width, to the minimal value implies its closeness to the equilateral triangle. As a by-product, we also present a novel result concerning a quantitative inequality for the inradius of a set, under minimal width constraint.","sentences":["The P\\'al inequality is a classical result which asserts that among all planar convex sets of given width the equilateral triangle is the one of minimal area.","In this paper we prove three quantitative versions of this inequality, by quantifying how the closeness of the area of a convex set, of certain width, to the minimal value implies its closeness to the equilateral triangle.","As a by-product, we also present a novel result concerning a quantitative inequality for the inradius of a set, under minimal width constraint."],"url":"http://arxiv.org/abs/2405.18294v1","category":"math.MG"}
{"created":"2024-05-28 15:47:11","title":"Semantic are Beacons: A Semantic Perspective for Unveiling Parameter-Efficient Fine-Tuning in Knowledge Learning","abstract":"Parameter-Efficient Fine-Tuning (PEFT) methods enable efficient adaptation of Large Language Models (LLMs) to various downstream applications. However, the effectiveness of the PEFT diminishes notably when downstream tasks require accurate learning of factual knowledge. In this paper, we adopt a semantic perspective to investigate this phenomenon, uncovering the reasons behind PEFT's limitations in knowledge learning task. Our findings reveal that: (1) PEFT presents a notable risk of pushing the model away from the intended knowledge target; (2) multiple knowledge interfere with each other, and such interference suppresses the learning and expression of knowledge features. Based on these insights, we introduce a data filtering strategy to exclude data that is detrimental to knowledge learning and a re-weighted learning strategy to make the model attentive to semantic distance during knowledge learning. Experimental results demonstrate the effectiveness of the proposed method on open-source large language model, further validate the semantic challenge in PEFT, thus paving the way for future research.","sentences":["Parameter-Efficient Fine-Tuning (PEFT) methods enable efficient adaptation of Large Language Models (LLMs) to various downstream applications.","However, the effectiveness of the PEFT diminishes notably when downstream tasks require accurate learning of factual knowledge.","In this paper, we adopt a semantic perspective to investigate this phenomenon, uncovering the reasons behind PEFT's limitations in knowledge learning task.","Our findings reveal that: (1) PEFT presents a notable risk of pushing the model away from the intended knowledge target; (2) multiple knowledge interfere with each other, and such interference suppresses the learning and expression of knowledge features.","Based on these insights, we introduce a data filtering strategy to exclude data that is detrimental to knowledge learning and a re-weighted learning strategy to make the model attentive to semantic distance during knowledge learning.","Experimental results demonstrate the effectiveness of the proposed method on open-source large language model, further validate the semantic challenge in PEFT, thus paving the way for future research."],"url":"http://arxiv.org/abs/2405.18292v1","category":"cs.CL"}
{"created":"2024-05-28 15:29:40","title":"NotPlaNET: Removing False Positives from Planet Hunters TESS with Machine Learning","abstract":"Differentiating between real transit events and false positive signals in photometric time series data is a bottleneck in the identification of transiting exoplanets, particularly long-period planets. This differentiation typically requires visual inspection of a large number of transit-like signals to rule out instrumental and astrophysical false positives that mimic planetary transit signals. We build a one-dimensional convolutional neural network (CNN) to separate eclipsing binaries and other false positives from potential planet candidates, reducing the number of light curves that require human vetting. Our CNN is trained using the TESS light curves that were identified by Planet Hunters citizen scientists as likely containing a transit. We also include the background flux and centroid information. The light curves are visually inspected and labeled by project scientists and are minimally pre-processed, with only normalization and data augmentation taking place before training. The median percentage of contaminants flagged across the test sectors is 18% with a maximum of 37% and a minimum of 10%. Our model keeps 100% of the planets for 16 of the 18 test sectors, while incorrectly flagging one planet candidate (0.3%) for one sector and two (0.6%) for the remaining sector. Our method shows potential to reduce the number of light curves requiring manual vetting by up to a third with minimal misclassification of planet candidates.","sentences":["Differentiating between real transit events and false positive signals in photometric time series data is a bottleneck in the identification of transiting exoplanets, particularly long-period planets.","This differentiation typically requires visual inspection of a large number of transit-like signals to rule out instrumental and astrophysical false positives that mimic planetary transit signals.","We build a one-dimensional convolutional neural network (CNN) to separate eclipsing binaries and other false positives from potential planet candidates, reducing the number of light curves that require human vetting.","Our CNN is trained using the TESS light curves that were identified by Planet Hunters citizen scientists as likely containing a transit.","We also include the background flux and centroid information.","The light curves are visually inspected and labeled by project scientists and are minimally pre-processed, with only normalization and data augmentation taking place before training.","The median percentage of contaminants flagged across the test sectors is 18% with a maximum of 37% and a minimum of 10%.","Our model keeps 100% of the planets for 16 of the 18 test sectors, while incorrectly flagging one planet candidate (0.3%) for one sector and two (0.6%) for the remaining sector.","Our method shows potential to reduce the number of light curves requiring manual vetting by up to a third with minimal misclassification of planet candidates."],"url":"http://arxiv.org/abs/2405.18278v1","category":"astro-ph.EP"}
{"created":"2024-05-28 15:17:58","title":"CT-based brain ventricle segmentation via diffusion Schr\u00f6dinger Bridge without target domain ground truths","abstract":"Efficient and accurate brain ventricle segmentation from clinical CT scans is critical for emergency surgeries like ventriculostomy. With the challenges in poor soft tissue contrast and a scarcity of well-annotated databases for clinical brain CTs, we introduce a novel uncertainty-aware ventricle segmentation technique without the need of CT segmentation ground truths by leveraging diffusion-model-based domain adaptation. Specifically, our method employs the diffusion Schr\\\"odinger Bridge and an attention recurrent residual U-Net to capitalize on unpaired CT and MRI scans to derive automatic CT segmentation from those of the MRIs, which are more accessible. Importantly, we propose an end-to-end, joint training framework of image translation and segmentation tasks, and demonstrate its benefit over training individual tasks separately. By comparing the proposed method against similar setups using two different GAN models for domain adaptation (CycleGAN and CUT), we also reveal the advantage of diffusion models towards improved segmentation and image translation quality. With a Dice score of 0.78$\\pm$0.27, our proposed method outperformed the compared methods, including SynSeg-Net, while providing intuitive uncertainty measures to further facilitate quality control of the automatic segmentation outcomes.","sentences":["Efficient and accurate brain ventricle segmentation from clinical CT scans is critical for emergency surgeries like ventriculostomy.","With the challenges in poor soft tissue contrast and a scarcity of well-annotated databases for clinical brain CTs, we introduce a novel uncertainty-aware ventricle segmentation technique without the need of CT segmentation ground truths by leveraging diffusion-model-based domain adaptation.","Specifically, our method employs the diffusion Schr\\\"odinger Bridge and an attention recurrent residual U-Net to capitalize on unpaired CT and MRI scans to derive automatic CT segmentation from those of the MRIs, which are more accessible.","Importantly, we propose an end-to-end, joint training framework of image translation and segmentation tasks, and demonstrate its benefit over training individual tasks separately.","By comparing the proposed method against similar setups using two different GAN models for domain adaptation (CycleGAN and CUT), we also reveal the advantage of diffusion models towards improved segmentation and image translation quality.","With a Dice score of 0.78$\\pm$0.27, our proposed method outperformed the compared methods, including SynSeg-Net, while providing intuitive uncertainty measures to further facilitate quality control of the automatic segmentation outcomes."],"url":"http://arxiv.org/abs/2405.18267v1","category":"eess.IV"}
{"created":"2024-05-28 14:50:12","title":"MSPE: Multi-Scale Patch Embedding Prompts Vision Transformers to Any Resolution","abstract":"Although Vision Transformers (ViTs) have recently advanced computer vision tasks significantly, an important real-world problem was overlooked: adapting to variable input resolutions. Typically, images are resized to a fixed resolution, such as 224x224, for efficiency during training and inference. However, uniform input size conflicts with real-world scenarios where images naturally vary in resolution. Modifying the preset resolution of a model may severely degrade the performance. In this work, we propose to enhance the model adaptability to resolution variation by optimizing the patch embedding. The proposed method, called Multi-Scale Patch Embedding (MSPE), substitutes the standard patch embedding with multiple variable-sized patch kernels and selects the best parameters for different resolutions, eliminating the need to resize the original image. Our method does not require high-cost training or modifications to other parts, making it easy to apply to most ViT models. Experiments in image classification, segmentation, and detection tasks demonstrate the effectiveness of MSPE, yielding superior performance on low-resolution inputs and performing comparably on high-resolution inputs with existing methods.","sentences":["Although Vision Transformers (ViTs) have recently advanced computer vision tasks significantly, an important real-world problem was overlooked: adapting to variable input resolutions.","Typically, images are resized to a fixed resolution, such as 224x224, for efficiency during training and inference.","However, uniform input size conflicts with real-world scenarios where images naturally vary in resolution.","Modifying the preset resolution of a model may severely degrade the performance.","In this work, we propose to enhance the model adaptability to resolution variation by optimizing the patch embedding.","The proposed method, called Multi-Scale Patch Embedding (MSPE), substitutes the standard patch embedding with multiple variable-sized patch kernels and selects the best parameters for different resolutions, eliminating the need to resize the original image.","Our method does not require high-cost training or modifications to other parts, making it easy to apply to most ViT models.","Experiments in image classification, segmentation, and detection tasks demonstrate the effectiveness of MSPE, yielding superior performance on low-resolution inputs and performing comparably on high-resolution inputs with existing methods."],"url":"http://arxiv.org/abs/2405.18240v1","category":"cs.CV"}
{"created":"2024-05-28 14:46:20","title":"Unveiling the Cycloid Trajectory of EM Iterations in Mixed Linear Regression","abstract":"We study the trajectory of iterations and the convergence rates of the Expectation-Maximization (EM) algorithm for two-component Mixed Linear Regression (2MLR). The fundamental goal of MLR is to learn the regression models from unlabeled observations. The EM algorithm finds extensive applications in solving the mixture of linear regressions. Recent results have established the super-linear convergence of EM for 2MLR in the noiseless and high SNR settings under some assumptions and its global convergence rate with random initialization has been affirmed. However, the exponent of convergence has not been theoretically estimated and the geometric properties of the trajectory of EM iterations are not well-understood. In this paper, first, using Bessel functions we provide explicit closed-form expressions for the EM updates under all SNR regimes. Then, in the noiseless setting, we completely characterize the behavior of EM iterations by deriving a recurrence relation at the population level and notably show that all the iterations lie on a certain cycloid. Based on this new trajectory-based analysis, we exhibit the theoretical estimate for the exponent of super-linear convergence and further improve the statistical error bound at the finite-sample level. Our analysis provides a new framework for studying the behavior of EM for Mixed Linear Regression.","sentences":["We study the trajectory of iterations and the convergence rates of the Expectation-Maximization (EM) algorithm for two-component Mixed Linear Regression (2MLR).","The fundamental goal of MLR is to learn the regression models from unlabeled observations.","The EM algorithm finds extensive applications in solving the mixture of linear regressions.","Recent results have established the super-linear convergence of EM for 2MLR in the noiseless and high SNR settings under some assumptions and its global convergence rate with random initialization has been affirmed.","However, the exponent of convergence has not been theoretically estimated and the geometric properties of the trajectory of EM iterations are not well-understood.","In this paper, first, using Bessel functions we provide explicit closed-form expressions for the EM updates under all SNR regimes.","Then, in the noiseless setting, we completely characterize the behavior of EM iterations by deriving a recurrence relation at the population level and notably show that all the iterations lie on a certain cycloid.","Based on this new trajectory-based analysis, we exhibit the theoretical estimate for the exponent of super-linear convergence and further improve the statistical error bound at the finite-sample level.","Our analysis provides a new framework for studying the behavior of EM for Mixed Linear Regression."],"url":"http://arxiv.org/abs/2405.18237v1","category":"cs.LG"}
{"created":"2024-05-28 14:43:34","title":"First Eigenvalue of Jacobi operator and Rigidity Results for Constant Mean Curvature Hypersurfaces","abstract":"In this paper, we obtain geometric upper bounds for the first eigenvalue $\\lambda_1^J$ of the Jacobi operator for both closed and compact with boundary hypersurfaces having constant mean curvature (CMC). As an application, we derive new rigidity results for the area of CMC hypersurfaces under suitable conditions on $\\lambda_1^J$ and the curvature of the ambient space. We also address the Jacobi-Steklov problem, proving geometric upper bounds for its first eigenvalue $\\sigma_1^J$ and deriving rigidity results related to the length of the boundary. Additionally, we present some results in higher dimensions related to the Yamabe invariants.","sentences":["In this paper, we obtain geometric upper bounds for the first eigenvalue $\\lambda_1^J$ of the Jacobi operator for both closed and compact with boundary hypersurfaces having constant mean curvature (CMC).","As an application, we derive new rigidity results for the area of CMC hypersurfaces under suitable conditions on $\\lambda_1^J$ and the curvature of the ambient space.","We also address the Jacobi-Steklov problem, proving geometric upper bounds for its first eigenvalue $\\sigma_1^J$ and deriving rigidity results related to the length of the boundary.","Additionally, we present some results in higher dimensions related to the Yamabe invariants."],"url":"http://arxiv.org/abs/2405.18233v1","category":"math.DG"}
{"created":"2024-05-28 14:40:51","title":"Guidelines and Best Practices to Share Deidentified Data and Code","abstract":"In 2022, the Journal of Statistics and Data Science Education (JSDSE) instituted augmented requirements for authors to post deidentified data and code underlying their papers. These changes were prompted by an increased focus on reproducibility and open science (NASEM 2019). A recent review of data availability practices noted that \"such policies help increase the reproducibility of the published literature, as well as make a larger body of data available for reuse and re-analysis\" (PLOS ONE, 2024). JSDSE values accessibility as it endeavors to share knowledge that can improve educational approaches to teaching statistics and data science. Because institution, environment, and students differ across readers of the journal, it is especially important to facilitate the transfer of a journal article's findings to new contexts. This process may require digging into more of the details, including the deidentified data and code. Our goal is to provide our readers and authors with a review of why the requirements for code and data sharing were instituted, summarize ongoing trends and developments in open science, discuss options for data and code sharing, and share advice for authors.","sentences":["In 2022, the Journal of Statistics and Data Science Education (JSDSE) instituted augmented requirements for authors to post deidentified data and code underlying their papers.","These changes were prompted by an increased focus on reproducibility and open science (NASEM 2019).","A recent review of data availability practices noted that \"such policies help increase the reproducibility of the published literature, as well as make a larger body of data available for reuse and re-analysis\" (PLOS ONE, 2024).","JSDSE values accessibility as it endeavors to share knowledge that can improve educational approaches to teaching statistics and data science.","Because institution, environment, and students differ across readers of the journal, it is especially important to facilitate the transfer of a journal article's findings to new contexts.","This process may require digging into more of the details, including the deidentified data and code.","Our goal is to provide our readers and authors with a review of why the requirements for code and data sharing were instituted, summarize ongoing trends and developments in open science, discuss options for data and code sharing, and share advice for authors."],"url":"http://arxiv.org/abs/2405.18232v1","category":"stat.OT"}
{"created":"2024-05-28 14:21:15","title":"FinerCut: Finer-grained Interpretable Layer Pruning for Large Language Models","abstract":"Overparametrized transformer networks are the state-of-the-art architecture for Large Language Models (LLMs). However, such models contain billions of parameters making large compute a necessity, while raising environmental concerns. To address these issues, we propose FinerCut, a new form of fine-grained layer pruning, which in contrast to prior work at the transformer block level, considers all self-attention and feed-forward network (FFN) layers within blocks as individual pruning candidates. FinerCut prunes layers whose removal causes minimal alternation to the model's output -- contributing to a new, lean, interpretable, and task-agnostic pruning method. Tested across 9 benchmarks, our approach retains 90% performance of Llama3-8B with 25% layers removed, and 95% performance of Llama3-70B with 30% layers removed, all without fine-tuning or post-pruning reconstruction. Strikingly, we observe intriguing results with FinerCut: 42% (34 out of 80) of the self-attention layers in Llama3-70B can be removed while preserving 99% of its performance -- without additional fine-tuning after removal. Moreover, FinerCut provides a tool to inspect the types and locations of pruned layers, allowing to observe interesting pruning behaviors. For instance, we observe a preference for pruning self-attention layers, often at deeper consecutive decoder layers. We hope our insights inspire future efficient LLM architecture designs.","sentences":["Overparametrized transformer networks are the state-of-the-art architecture for Large Language Models (LLMs).","However, such models contain billions of parameters making large compute a necessity, while raising environmental concerns.","To address these issues, we propose FinerCut, a new form of fine-grained layer pruning, which in contrast to prior work at the transformer block level, considers all self-attention and feed-forward network (FFN) layers within blocks as individual pruning candidates.","FinerCut prunes layers whose removal causes minimal alternation to the model's output -- contributing to a new, lean, interpretable, and task-agnostic pruning method.","Tested across 9 benchmarks, our approach retains 90% performance of Llama3-8B with 25% layers removed, and 95% performance of Llama3-70B with 30% layers removed, all without fine-tuning or post-pruning reconstruction.","Strikingly, we observe intriguing results with FinerCut: 42% (34 out of 80) of the self-attention layers in Llama3-70B can be removed while preserving 99% of its performance -- without additional fine-tuning after removal.","Moreover, FinerCut provides a tool to inspect the types and locations of pruned layers, allowing to observe interesting pruning behaviors.","For instance, we observe a preference for pruning self-attention layers, often at deeper consecutive decoder layers.","We hope our insights inspire future efficient LLM architecture designs."],"url":"http://arxiv.org/abs/2405.18218v1","category":"cs.LG"}
{"created":"2024-05-28 13:50:07","title":"Feature-Based Online Bilateral Trade","abstract":"Bilateral trade models the problem of facilitating trades between a seller and a buyer having private valuations for the item being sold. In the online version of the problem, the learner faces a new seller and buyer at each time step, and has to post a price for each of the two parties without any knowledge of their valuations. We consider a scenario where, at each time step, before posting prices the learner observes a context vector containing information about the features of the item for sale. The valuations of both the seller and the buyer follow an unknown linear function of the context. In this setting, the learner could leverage previous transactions in an attempt to estimate private valuations. We characterize the regret regimes of different settings, taking as a baseline the best context-dependent prices in hindsight. First, in the setting in which the learner has two-bit feedback and strong budget balance constraints, we propose an algorithm with $O(\\log T)$ regret. Then, we study the same set-up with noisy valuations, providing a tight $\\widetilde O(T^{\\frac23})$ regret upper bound. Finally, we show that loosening budget balance constraints allows the learner to operate under more restrictive feedback. Specifically, we show how to address the one-bit, global budget balance setting through a reduction from the two-bit, strong budget balance setup. This established a fundamental trade-off between the quality of the feedback and the strictness of the budget constraints.","sentences":["Bilateral trade models the problem of facilitating trades between a seller and a buyer having private valuations for the item being sold.","In the online version of the problem, the learner faces a new seller and buyer at each time step, and has to post a price for each of the two parties without any knowledge of their valuations.","We consider a scenario where, at each time step, before posting prices the learner observes a context vector containing information about the features of the item for sale.","The valuations of both the seller and the buyer follow an unknown linear function of the context.","In this setting, the learner could leverage previous transactions in an attempt to estimate private valuations.","We characterize the regret regimes of different settings, taking as a baseline the best context-dependent prices in hindsight.","First, in the setting in which the learner has two-bit feedback and strong budget balance constraints, we propose an algorithm with $O(\\log T)$ regret.","Then, we study the same set-up with noisy valuations, providing a tight $\\widetilde O(T^{\\frac23})$ regret upper bound.","Finally, we show that loosening budget balance constraints allows the learner to operate under more restrictive feedback.","Specifically, we show how to address the one-bit, global budget balance setting through a reduction from the two-bit, strong budget balance setup.","This established a fundamental trade-off between the quality of the feedback and the strictness of the budget constraints."],"url":"http://arxiv.org/abs/2405.18183v1","category":"cs.GT"}
{"created":"2024-05-28 13:33:42","title":"Life span of solutions to a semilinear parabolic equation on locally finite graphs","abstract":"Let $G=(V,E)$ be a locally finite connected graph. We develop the first eigenvalue method on $G$ introduced in 1963 by Kaplan \\cite{Kaplan} on Euclidean space, the discrete Phragm\\'{e}n-Lindel\\\"{o}f principle of parabolic equations and upper and lower solutions method on $G$. Using these methods, we establish the estimates and asymptotic behaviour of the life span of solutions to a semilinear heat equation with initial data $\\lambda\\psi(x)$ for different scales of $\\lambda$ on $G$ under some different conditions. Our results are different from the continuous case, which is related to the structure of the graph $G$.","sentences":["Let $G=(V,E)$ be a locally finite connected graph.","We develop the first eigenvalue method on $G$ introduced in 1963 by Kaplan \\cite{Kaplan} on Euclidean space, the discrete Phragm\\'{e}n-Lindel\\\"{o}f principle of parabolic equations and upper and lower solutions method on $G$. Using these methods, we establish the estimates and asymptotic behaviour of the life span of solutions to a semilinear heat equation with initial data $\\lambda\\psi(x)$ for different scales of $\\lambda$ on $G$ under some different conditions.","Our results are different from the continuous case, which is related to the structure of the graph $G$."],"url":"http://arxiv.org/abs/2405.18173v1","category":"math.AP"}
{"created":"2024-05-28 13:24:07","title":"A note on locating sets in twin-free graphs","abstract":"In this short note, we prove that every twin-free graph on $n$ vertices contains a locating-dominating set of size at most $\\lceil\\frac{5}{8}n\\rceil$. This improves the earlier bound of $\\lfloor\\frac{2}{3}n\\rfloor$ due to Foucaud, Henning, L\\\"owenstein and Sasse from 2016, and makes some progress towards the well-studied locating-dominating conjecture of Garijo, Gonz\\'alez and M\\'arquez.","sentences":["In this short note, we prove that every twin-free graph on $n$ vertices contains a locating-dominating set of size at most $\\lceil\\frac{5}{8}n\\rceil$. This improves the earlier bound of $\\lfloor\\frac{2}{3}n\\rfloor$ due to Foucaud, Henning, L\\\"owenstein and Sasse from 2016, and makes some progress towards the well-studied locating-dominating conjecture of Garijo, Gonz\\'alez and M\\'arquez."],"url":"http://arxiv.org/abs/2405.18162v1","category":"math.CO"}
{"created":"2024-05-28 13:16:09","title":"On the Laplace operator with a weak magnetic field in exterior domains","abstract":"We study the magnetic Laplacian in a two-dimensional exterior domain with Neumann boundary condition and uniform magnetic field. For the exterior of the disk we establish accurate asymptotics of the low-lying eigenvalues in the weak magnetic field limit. For the exterior of a star-shaped domain, we obtain an asymptotic upper bound on the lowest eigenvalue in the weak field limit, involving the $4$-moment, and optimal for the case of the disk. Moreover, we prove that, for moderate magnetic fields, the exterior of the disk is a local maximizer for the lowest eigenvalue under a $p$-moment constraint.","sentences":["We study the magnetic Laplacian in a two-dimensional exterior domain with Neumann boundary condition and uniform magnetic field.","For the exterior of the disk we establish accurate asymptotics of the low-lying eigenvalues in the weak magnetic field limit.","For the exterior of a star-shaped domain, we obtain an asymptotic upper bound on the lowest eigenvalue in the weak field limit, involving the $4$-moment, and optimal for the case of the disk.","Moreover, we prove that, for moderate magnetic fields, the exterior of the disk is a local maximizer for the lowest eigenvalue under a $p$-moment constraint."],"url":"http://arxiv.org/abs/2405.18154v1","category":"math.SP"}
{"created":"2024-05-28 12:40:16","title":"On the fibbinary numbers and the Wythoffarray","abstract":"This paper defines the set fib of fibbinary numbers and displays its structure in the form of a table of a specialised type, and in array form. It uses the Zeckendorf representation $n \\in \\mathbf{N}$ to define a bijection $\\mathcal{Z}$ between $\\mathbf{N}$ and fib. It is proved that the fibbinary array is the image under $\\mathcal{Z}$ of the famous Wythoff array. The fibbinary table proves useful pictorial insight into the fractal defined by the Wythoff array. The Wythoff table, obtained as the image under the inverse of $\\mathcal{Z}$ of the fibbinary table, leads to a simpler view of the fractal, and may be compared with the (1938) Steinhaus tree.","sentences":["This paper defines the set fib of fibbinary numbers and displays its structure in the form of a table of a specialised type, and in array form.","It uses the Zeckendorf representation $n \\in \\mathbf{N}$ to define a bijection $\\mathcal{Z}$ between $\\mathbf{N}$ and fib.","It is proved that the fibbinary array is the image under $\\mathcal{Z}$ of the famous Wythoff array.","The fibbinary table proves useful pictorial insight into the fractal defined by the Wythoff array.","The Wythoff table, obtained as the image under the inverse of $\\mathcal{Z}$ of the fibbinary table, leads to a simpler view of the fractal, and may be compared with the (1938) Steinhaus tree."],"url":"http://arxiv.org/abs/2405.18128v1","category":"math.CO"}
{"created":"2024-05-28 12:15:15","title":"Simulation of Single-Phase Natural Circulation within the BEPU Framework: Sketching Scaling Uncertainty Principle by Multi-Scale CFD Approaches","abstract":"In order to enhance safety, nuclear reactors in the design phase consider natural circulation as a mean to remove residual power. The simulation of this passive mechanism must be qualified between the validation range and the scope of utilization (reactor case), introducing potential physical and numerical distortion effects. In this study, we simulate the flow of liquid sodium using the TrioCFD code, employing both higher-fidelity (HF) LES and lower-fidelity (LF) URANS models. We tackle respectively numerical uncertainties through the Grid Convergence Index method, and physical modelling uncertainties through the Polynomial Chaos Expansion method available on the URANIE platform. HF simulations are shown to exhibit a strong resilience to physical distortion effects, with numerical uncertainties being intricately correlated. Conversely, the LF approach, the only one applicable at the reactor scale, is likely to present a reduced predictability. If so, the HF approach should be effective in pinpointing the LF weaknesses: the concept of scaling uncertainty is inline introduced as the growth of the LF simulation uncertainty associated with distortion effects. Thus, the paper outlines that a specific methodology within the BEPU framework - leveraging both HF and LF approaches - could pragmatically enable correlating distortion effects with scaling uncertainty, thereby providing a metric principle.","sentences":["In order to enhance safety, nuclear reactors in the design phase consider natural circulation as a mean to remove residual power.","The simulation of this passive mechanism must be qualified between the validation range and the scope of utilization (reactor case), introducing potential physical and numerical distortion effects.","In this study, we simulate the flow of liquid sodium using the TrioCFD code, employing both higher-fidelity (HF) LES and lower-fidelity (LF) URANS models.","We tackle respectively numerical uncertainties through the Grid Convergence Index method, and physical modelling uncertainties through the Polynomial Chaos Expansion method available on the URANIE platform.","HF simulations are shown to exhibit a strong resilience to physical distortion effects, with numerical uncertainties being intricately correlated.","Conversely, the LF approach, the only one applicable at the reactor scale, is likely to present a reduced predictability.","If so, the HF approach should be effective in pinpointing the LF weaknesses: the concept of scaling uncertainty is inline introduced as the growth of the LF simulation uncertainty associated with distortion effects.","Thus, the paper outlines that a specific methodology within the BEPU framework - leveraging both HF and LF approaches - could pragmatically enable correlating distortion effects with scaling uncertainty, thereby providing a metric principle."],"url":"http://arxiv.org/abs/2405.18108v1","category":"physics.class-ph"}
{"created":"2024-05-28 12:02:07","title":"Raman analysis of the dehydrogenation process of hydrogenated monolayer graphene","abstract":"Creating defects in graphene by hydrogenation, either to achieve hydrogen chemisorption or partial etching, is a way to open an electronic band gap in graphene. Understanding the range of stability conditions of partially etched or hydrogenated graphene is crucial for application, as processing conditions (e.g. temperature) and quality control (characterization) conditions may result in modifying the material through partial or full dehydrogenation, and subsequent alteration of its electronic properties. This work reports a study of various dehydrogenation conditions of hydrogenated or hydrogen-etched monolayer graphene (1LG), either free-standing or supported by an interferential (SiO2/Si) substrate, using incremental annealing under nitrogen atmosphere up to 400 {\\textdegree}C. Materials were investigated by Raman spectroscopy. Indeed, it has been known since 2012 that the intensity ratio of two Raman bands activated by double resonance, D over D' (ID/ID') can be used to identify the type of defects in defective graphene. It is shown that hydrogenated 1LG, characterized by a large ID/ID' ratio (~9-15), is stable provided annealing remains below 300 {\\textdegree}C. On the other hand, defective 1LG resulting from hydrogen etching remains stable up to 400 {\\textdegree}C, whether the 1LG is hydrogenated on one side or both sides, while a modification in the type and proportions of defects is likely. Experimental conditions for the safe use of Raman spectroscopy, otherwise able to induce specimen overheating because of the laser energy and power, are also determined and discussed.","sentences":["Creating defects in graphene by hydrogenation, either to achieve hydrogen chemisorption or partial etching, is a way to open an electronic band gap in graphene.","Understanding the range of stability conditions of partially etched or hydrogenated graphene is crucial for application, as processing conditions (e.g. temperature) and quality control (characterization) conditions may result in modifying the material through partial or full dehydrogenation, and subsequent alteration of its electronic properties.","This work reports a study of various dehydrogenation conditions of hydrogenated or hydrogen-etched monolayer graphene (1LG), either free-standing or supported by an interferential (SiO2/Si) substrate, using incremental annealing under nitrogen atmosphere up to 400 {\\textdegree}C. Materials were investigated by Raman spectroscopy.","Indeed, it has been known since 2012 that the intensity ratio of two Raman bands activated by double resonance, D over D' (ID/ID') can be used to identify the type of defects in defective graphene.","It is shown that hydrogenated 1LG, characterized by a large ID/ID' ratio (~9-15), is stable provided annealing remains below 300 {\\textdegree}C. On the other hand, defective 1LG resulting from hydrogen etching remains stable up to 400 {\\textdegree}C, whether the 1LG is hydrogenated on one side or both sides, while a modification in the type and proportions of defects is likely.","Experimental conditions for the safe use of Raman spectroscopy, otherwise able to induce specimen overheating because of the laser energy and power, are also determined and discussed."],"url":"http://arxiv.org/abs/2405.18096v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-28 12:01:52","title":"Is machine learning good or bad for the natural sciences?","abstract":"Machine learning (ML) methods are having a huge impact across all of the sciences. However, ML has a strong ontology - in which only the data exist - and a strong epistemology - in which a model is considered good if it performs well on held-out training data. These philosophies are in strong conflict with both standard practices and key philosophies in the natural sciences. Here, we identify some locations for ML in the natural sciences at which the ontology and epistemology are valuable. For example, when an expressive machine learning model is used in a causal inference to represent the effects of confounders, such as foregrounds, backgrounds, or instrument calibration parameters, the model capacity and loose philosophy of ML can make the results more trustworthy. We also show that there are contexts in which the introduction of ML introduces strong, unwanted statistical biases. For one, when ML models are used to emulate physical (or first-principles) simulations, they introduce strong confirmation biases. For another, when expressive regressions are used to label datasets, those labels cannot be used in downstream joint or ensemble analyses without taking on uncontrolled biases. The question in the title is being asked of all of the natural sciences; that is, we are calling on the scientific communities to take a step back and consider the role and value of ML in their fields; the (partial) answers we give here come from the particular perspective of physics.","sentences":["Machine learning (ML) methods are having a huge impact across all of the sciences.","However, ML has a strong ontology - in which only the data exist - and a strong epistemology - in which a model is considered good if it performs well on held-out training data.","These philosophies are in strong conflict with both standard practices and key philosophies in the natural sciences.","Here, we identify some locations for ML in the natural sciences at which the ontology and epistemology are valuable.","For example, when an expressive machine learning model is used in a causal inference to represent the effects of confounders, such as foregrounds, backgrounds, or instrument calibration parameters, the model capacity and loose philosophy of ML can make the results more trustworthy.","We also show that there are contexts in which the introduction of ML introduces strong, unwanted statistical biases.","For one, when ML models are used to emulate physical (or first-principles) simulations, they introduce strong confirmation biases.","For another, when expressive regressions are used to label datasets, those labels cannot be used in downstream joint or ensemble analyses without taking on uncontrolled biases.","The question in the title is being asked of all of the natural sciences; that is, we are calling on the scientific communities to take a step back and consider the role and value of ML in their fields; the (partial) answers we give here come from the particular perspective of physics."],"url":"http://arxiv.org/abs/2405.18095v1","category":"stat.ML"}
{"created":"2024-05-28 11:39:36","title":"Edge-guided and Class-balanced Active Learning for Semantic Segmentation of Aerial Images","abstract":"Semantic segmentation requires pixel-level annotation, which is time-consuming. Active Learning (AL) is a promising method for reducing data annotation costs. Due to the gap between aerial and natural images, the previous AL methods are not ideal, mainly caused by unreasonable labeling units and the neglect of class imbalance. Previous labeling units are based on images or regions, which does not consider the characteristics of segmentation tasks and aerial images, i.e., the segmentation network often makes mistakes in the edge region, and the edge of aerial images is often interlaced and irregular. Therefore, an edge-guided labeling unit is proposed and supplemented as the new unit. On the other hand, the class imbalance is severe, manifested in two aspects: the aerial image is seriously imbalanced, and the AL strategy does not fully consider the class balance. Both seriously affect the performance of AL in aerial images. We comprehensively ensure class balance from all steps that may occur imbalance, including initial labeled data, subsequent labeled data, and pseudo-labels. Through the two improvements, our method achieves more than 11.2\\% gains compared to state-of-the-art methods on three benchmark datasets, Deepglobe, Potsdam, and Vaihingen, and more than 18.6\\% gains compared to the baseline. Sufficient ablation studies show that every module is indispensable. Furthermore, we establish a fair and strong benchmark for future research on AL for aerial image segmentation.","sentences":["Semantic segmentation requires pixel-level annotation, which is time-consuming.","Active Learning (AL) is a promising method for reducing data annotation costs.","Due to the gap between aerial and natural images, the previous AL methods are not ideal, mainly caused by unreasonable labeling units and the neglect of class imbalance.","Previous labeling units are based on images or regions, which does not consider the characteristics of segmentation tasks and aerial images, i.e., the segmentation network often makes mistakes in the edge region, and the edge of aerial images is often interlaced and irregular.","Therefore, an edge-guided labeling unit is proposed and supplemented as the new unit.","On the other hand, the class imbalance is severe, manifested in two aspects: the aerial image is seriously imbalanced, and the AL strategy does not fully consider the class balance.","Both seriously affect the performance of AL in aerial images.","We comprehensively ensure class balance from all steps that may occur imbalance, including initial labeled data, subsequent labeled data, and pseudo-labels.","Through the two improvements, our method achieves more than 11.2\\% gains compared to state-of-the-art methods on three benchmark datasets, Deepglobe, Potsdam, and Vaihingen, and more than 18.6\\% gains compared to the baseline.","Sufficient ablation studies show that every module is indispensable.","Furthermore, we establish a fair and strong benchmark for future research on AL for aerial image segmentation."],"url":"http://arxiv.org/abs/2405.18078v1","category":"cs.CV"}
{"created":"2024-05-28 11:21:49","title":"Letter of Intent: Towards a Vacuum Birefringence Experiment at the Helmholtz International Beamline for Extreme Fields","abstract":"Quantum field theory predicts a nonlinear response of the vacuum to strong electromagnetic fields of macroscopic extent. This fundamental tenet has remained experimentally challenging and is yet to be tested in the laboratory. A particularly distinct signature of the resulting optical activity of the quantum vacuum is vacuum birefringence. This offers an excellent opportunity for a precision test of nonlinear quantum electrodynamics in an uncharted parameter regime. Recently, the operation of the high-intensity laser ReLaX provided by the Helmholtz International Beamline for Extreme Fields (HIBEF) has been inaugurated at the High Energy Density (HED) scientific instrument of the European XFEL. We make the case that this worldwide unique combination of an x-ray free-electron laser and an ultra-intense near-infrared laser together with recent advances in high-precision x-ray polarimetry, refinements of prospective discovery scenarios, and progress in their accurate theoretical modelling have set the stage for performing an actual discovery experiment of quantum vacuum nonlinearity.","sentences":["Quantum field theory predicts a nonlinear response of the vacuum to strong electromagnetic fields of macroscopic extent.","This fundamental tenet has remained experimentally challenging and is yet to be tested in the laboratory.","A particularly distinct signature of the resulting optical activity of the quantum vacuum is vacuum birefringence.","This offers an excellent opportunity for a precision test of nonlinear quantum electrodynamics in an uncharted parameter regime.","Recently, the operation of the high-intensity laser ReLaX provided by the Helmholtz International Beamline for Extreme Fields (HIBEF) has been inaugurated at the High Energy Density (HED) scientific instrument of the European XFEL.","We make the case that this worldwide unique combination of an x-ray free-electron laser and an ultra-intense near-infrared laser together with recent advances in high-precision x-ray polarimetry, refinements of prospective discovery scenarios, and progress in their accurate theoretical modelling have set the stage for performing an actual discovery experiment of quantum vacuum nonlinearity."],"url":"http://arxiv.org/abs/2405.18063v1","category":"physics.ins-det"}
{"created":"2024-05-28 11:07:14","title":"Water at negative pressure: Nuclear quantum effects","abstract":"Various condensed phases of water, spanning from the liquid state to multiple ice phases, have been systematically investigated under extreme conditions of pressure and temperature to delineate their stability boundaries. This study focuses on probing the mechanical stability of liquid water through path-integral molecular dynamics simulations, employing the q-TIP4P/F potential to model interatomic interactions in flexible water molecules. Temperature and pressure conditions ranging from 250 to 375 K and -0.3 to 1 GPa, respectively, are considered. This comprehensive approach enables a thorough exploration of nuclear quantum effects on various physical properties of water through direct comparisons with classical molecular dynamics results employing the same potential model. Key properties such as molar volume, intramolecular bond length, H--O--H angle, internal and kinetic energy are analyzed, with a specific focus on the effect of tensile stress. Particular attention is devoted to the liquid-gas spinodal pressure, representing the limit of mechanical stability for the liquid phase, at several temperatures. The quantum simulations reveal a spinodal pressure for water of -286 and -236 MPa at temperatures of 250 and 300 K, respectively. At these temperatures, the discernible shifts induced by nuclear quantum motion are quantified at 15 and 10 MPa, respectively. These findings contribute valuable insights into the interplay of quantum effects on the stability of liquid water under diverse thermodynamic conditions.","sentences":["Various condensed phases of water, spanning from the liquid state to multiple ice phases, have been systematically investigated under extreme conditions of pressure and temperature to delineate their stability boundaries.","This study focuses on probing the mechanical stability of liquid water through path-integral molecular dynamics simulations, employing the q-TIP4P/F potential to model interatomic interactions in flexible water molecules.","Temperature and pressure conditions ranging from 250 to 375 K and -0.3 to 1 GPa, respectively, are considered.","This comprehensive approach enables a thorough exploration of nuclear quantum effects on various physical properties of water through direct comparisons with classical molecular dynamics results employing the same potential model.","Key properties such as molar volume, intramolecular bond length, H--O--H angle, internal and kinetic energy are analyzed, with a specific focus on the effect of tensile stress.","Particular attention is devoted to the liquid-gas spinodal pressure, representing the limit of mechanical stability for the liquid phase, at several temperatures.","The quantum simulations reveal a spinodal pressure for water of -286 and -236 MPa at temperatures of 250 and 300 K, respectively.","At these temperatures, the discernible shifts induced by nuclear quantum motion are quantified at 15 and 10 MPa, respectively.","These findings contribute valuable insights into the interplay of quantum effects on the stability of liquid water under diverse thermodynamic conditions."],"url":"http://arxiv.org/abs/2405.18053v1","category":"physics.chem-ph"}
{"created":"2024-05-28 11:02:51","title":"Structural, electronic, and optical properties of 6H-SiC layers synthesized by implantation of carbon ions into silicon","abstract":"Systematic studies of the gradual fabrication by means of carbon ion-implantation of high-quality 6H-SiC layers on silicon surfaces have been carried out. The fluence of carbon ions varied from 5*10^15 cm-2 to 10^17 cm-2. Results of first-principle calculations, X-ray diffraction (XRD), and Raman spectroscopy demonstrate the amorphization of silicon substrate without any tendency to the segregation of carbon in the samples synthesized at low fluencies. The formation of a SiO2-like structure at this stage was also detected. X-ray photoelectron spectroscopy (XPS), XRD, and Raman spectroscopy demonstrate that an increase in carbon content at 10^17 cm-2 fluence leads to the growth of 6H-SiC films on the surface of the amorphous silicon substrate. Atomic force microscopy (AFM) data obtained also demonstrates the decreasing of surface roughens after the formation of SiC film. XPS and Raman spectra suggest that excessive carbon content leaves the SiC matrix via the formation of an insignificant amount of partially oxidized carbon nanostructures. Optical measurements also support the claim of high-quality 6H-SiC film formation in the samples synthesized at 10^17 cm-2 fluence and demonstrate the absence of any detectable contribution of nanostructures formed from excessive carbon on the optical properties of the material under study.","sentences":["Systematic studies of the gradual fabrication by means of carbon ion-implantation of high-quality 6H-SiC layers on silicon surfaces have been carried out.","The fluence of carbon ions varied from 5*10^15 cm-2 to 10^17 cm-2.","Results of first-principle calculations, X-ray diffraction (XRD), and Raman spectroscopy demonstrate the amorphization of silicon substrate without any tendency to the segregation of carbon in the samples synthesized at low fluencies.","The formation of a SiO2-like structure at this stage was also detected.","X-ray photoelectron spectroscopy (XPS), XRD, and Raman spectroscopy demonstrate that an increase in carbon content at 10^17 cm-2 fluence leads to the growth of 6H-SiC films on the surface of the amorphous silicon substrate.","Atomic force microscopy (AFM) data obtained also demonstrates the decreasing of surface roughens after the formation of SiC film.","XPS and Raman spectra suggest that excessive carbon content leaves the SiC matrix via the formation of an insignificant amount of partially oxidized carbon nanostructures.","Optical measurements also support the claim of high-quality 6H-SiC film formation in the samples synthesized at 10^17 cm-2 fluence and demonstrate the absence of any detectable contribution of nanostructures formed from excessive carbon on the optical properties of the material under study."],"url":"http://arxiv.org/abs/2405.18049v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-28 11:00:41","title":"Bridging Mini-Batch and Asymptotic Analysis in Contrastive Learning: From InfoNCE to Kernel-Based Losses","abstract":"What do different contrastive learning (CL) losses actually optimize for? Although multiple CL methods have demonstrated remarkable representation learning capabilities, the differences in their inner workings remain largely opaque. In this work, we analyse several CL families and prove that, under certain conditions, they admit the same minimisers when optimizing either their batch-level objectives or their expectations asymptotically. In both cases, an intimate connection with the hyperspherical energy minimisation (HEM) problem resurfaces. Drawing inspiration from this, we introduce a novel CL objective, coined Decoupled Hyperspherical Energy Loss (DHEL). DHEL simplifies the problem by decoupling the target hyperspherical energy from the alignment of positive examples while preserving the same theoretical guarantees. Going one step further, we show the same results hold for another relevant CL family, namely kernel contrastive learning (KCL), with the additional advantage of the expected loss being independent of batch size, thus identifying the minimisers in the non-asymptotic regime. Empirical results demonstrate improved downstream performance and robustness across combinations of different batch sizes and hyperparameters and reduced dimensionality collapse, on several computer vision datasets.","sentences":["What do different contrastive learning (CL) losses actually optimize for?","Although multiple CL methods have demonstrated remarkable representation learning capabilities, the differences in their inner workings remain largely opaque.","In this work, we analyse several CL families and prove that, under certain conditions, they admit the same minimisers when optimizing either their batch-level objectives or their expectations asymptotically.","In both cases, an intimate connection with the hyperspherical energy minimisation (HEM) problem resurfaces.","Drawing inspiration from this, we introduce a novel CL objective, coined Decoupled Hyperspherical Energy Loss (DHEL).","DHEL simplifies the problem by decoupling the target hyperspherical energy from the alignment of positive examples while preserving the same theoretical guarantees.","Going one step further, we show the same results hold for another relevant CL family, namely kernel contrastive learning (KCL), with the additional advantage of the expected loss being independent of batch size, thus identifying the minimisers in the non-asymptotic regime.","Empirical results demonstrate improved downstream performance and robustness across combinations of different batch sizes and hyperparameters and reduced dimensionality collapse, on several computer vision datasets."],"url":"http://arxiv.org/abs/2405.18045v1","category":"cs.LG"}
{"created":"2024-05-28 09:57:28","title":"MultiADE: A Multi-domain Benchmark for Adverse Drug Event Extraction","abstract":"Objective. Active adverse event surveillance monitors Adverse Drug Events (ADE) from different data sources, such as electronic health records, medical literature, social media and search engine logs. Over years, many datasets are created, and shared tasks are organised to facilitate active adverse event surveillance. However, most-if not all-datasets or shared tasks focus on extracting ADEs from a particular type of text. Domain generalisation-the ability of a machine learning model to perform well on new, unseen domains (text types)-is under-explored. Given the rapid advancements in natural language processing, one unanswered question is how far we are from having a single ADE extraction model that are effective on various types of text, such as scientific literature and social media posts}. Methods. We contribute to answering this question by building a multi-domain benchmark for adverse drug event extraction, which we named MultiADE. The new benchmark comprises several existing datasets sampled from different text types and our newly created dataset-CADECv2, which is an extension of CADEC (Karimi, et al., 2015), covering online posts regarding more diverse drugs than CADEC. Our new dataset is carefully annotated by human annotators following detailed annotation guidelines. Conclusion. Our benchmark results show that the generalisation of the trained models is far from perfect, making it infeasible to be deployed to process different types of text. In addition, although intermediate transfer learning is a promising approach to utilising existing resources, further investigation is needed on methods of domain adaptation, particularly cost-effective methods to select useful training instances.","sentences":["Objective.","Active adverse event surveillance monitors Adverse Drug Events (ADE) from different data sources, such as electronic health records, medical literature, social media and search engine logs.","Over years, many datasets are created, and shared tasks are organised to facilitate active adverse event surveillance.","However, most-if not all-datasets or shared tasks focus on extracting ADEs from a particular type of text.","Domain generalisation-the ability of a machine learning model to perform well on new, unseen domains (text types)-is under-explored.","Given the rapid advancements in natural language processing, one unanswered question is how far we are from having a single ADE extraction model that are effective on various types of text, such as scientific literature and social media posts}.","Methods.","We contribute to answering this question by building a multi-domain benchmark for adverse drug event extraction, which we named MultiADE.","The new benchmark comprises several existing datasets sampled from different text types and our newly created dataset-CADECv2, which is an extension of CADEC (Karimi, et al., 2015), covering online posts regarding more diverse drugs than CADEC.","Our new dataset is carefully annotated by human annotators following detailed annotation guidelines.","Conclusion.","Our benchmark results show that the generalisation of the trained models is far from perfect, making it infeasible to be deployed to process different types of text.","In addition, although intermediate transfer learning is a promising approach to utilising existing resources, further investigation is needed on methods of domain adaptation, particularly cost-effective methods to select useful training instances."],"url":"http://arxiv.org/abs/2405.18015v1","category":"cs.CL"}
{"created":"2024-05-28 09:50:29","title":"Potential to Identify the Neutrino Mass Ordering with Reactor Antineutrinos in JUNO","abstract":"The Jiangmen Underground Neutrino Observatory (JUNO) is a multi-purpose neutrino experiment under construction in South of China. This paper presents an updated estimate of JUNO's sensitivity to the neutrino mass ordering using the reactor antineutrinos emitted from eight nuclear reactor cores in the Taishan and Yangjiang nuclear power plants. This measurement is planned by studying the fine interference pattern caused by quasi-vacuum oscillations in the oscillated antineutrino spectrum at a baseline of 52.5~km and is completely independent of the CP violating phase and the neutrino mixing angle $\\theta_{23}$. The sensitivity is obtained through a joint analysis of JUNO and TAO detectors utilizing the best available knowledge to date about the location and overburden of the JUNO experimental site, the local and global nuclear reactors, the JUNO and TAO detectors responses, the expected event rates and spectra of signal and backgrounds, and the systematic uncertainties of the analysis inputs. It is found that a 3$\\sigma$ median sensitivity to reject the wrong mass ordering hypothesis can be reached with an exposure of about 6.5 years $\\times$ 26.6~GW thermal power.","sentences":["The Jiangmen Underground Neutrino Observatory (JUNO) is a multi-purpose neutrino experiment under construction in South of China.","This paper presents an updated estimate of JUNO's sensitivity to the neutrino mass ordering using the reactor antineutrinos emitted from eight nuclear reactor cores in the Taishan and Yangjiang nuclear power plants.","This measurement is planned by studying the fine interference pattern caused by quasi-vacuum oscillations in the oscillated antineutrino spectrum at a baseline of 52.5~km and is completely independent of the CP violating phase and the neutrino mixing angle $\\theta_{23}$.","The sensitivity is obtained through a joint analysis of JUNO and TAO detectors utilizing the best available knowledge to date about the location and overburden of the JUNO experimental site, the local and global nuclear reactors, the JUNO and TAO detectors responses, the expected event rates and spectra of signal and backgrounds, and the systematic uncertainties of the analysis inputs.","It is found that a 3$\\sigma$ median sensitivity to reject the wrong mass ordering hypothesis can be reached with an exposure of about 6.5 years $\\times$ 26.6~GW thermal power."],"url":"http://arxiv.org/abs/2405.18008v1","category":"hep-ex"}
{"created":"2024-05-28 09:08:56","title":"Performance of Slotted ALOHA in User-Centric Cell-Free Massive MIMO","abstract":"To efficiently utilize the scarce wireless resource, the random access scheme has been attaining renewed interest primarily in supporting the sporadic traffic of a large number of devices encountered in the Internet of Things (IoT). In this paper we investigate the performance of slotted ALOHA -- a simple and practical random access scheme -- in connection with the grant-free random access protocol applied for user-centric cell-free massive MIMO. More specifically, we provide the expression of the sum-throughput under the assumptions of the capture capability owned by the centralized detector in the uplink. Further, a comparative study of user-centric cell-free massive MIMO with other types of networks is provided, which allows us to identify its potential and possible limitation. Our numerical simulations show that the user-centric cell-free massive MIMO has a good trade-off between performance and fronthaul load, especially at low activation probability regime.","sentences":["To efficiently utilize the scarce wireless resource, the random access scheme has been attaining renewed interest primarily in supporting the sporadic traffic of a large number of devices encountered in the Internet of Things (IoT).","In this paper we investigate the performance of slotted ALOHA -- a simple and practical random access scheme -- in connection with the grant-free random access protocol applied for user-centric cell-free massive MIMO.","More specifically, we provide the expression of the sum-throughput under the assumptions of the capture capability owned by the centralized detector in the uplink.","Further, a comparative study of user-centric cell-free massive MIMO with other types of networks is provided, which allows us to identify its potential and possible limitation.","Our numerical simulations show that the user-centric cell-free massive MIMO has a good trade-off between performance and fronthaul load, especially at low activation probability regime."],"url":"http://arxiv.org/abs/2405.17979v1","category":"cs.IT"}
{"created":"2024-05-28 09:04:36","title":"Massive Black Hole Seeds","abstract":"The pathway(s) to seeding the massive black holes (MBHs) that exist at the heart of galaxies in the present and distant Universe remains an unsolved problem. Here we categorise, describe and quantitatively discuss the formation pathways of both $\\textit{light}$ and $\\textit{heavy}$ seeds. We emphasise that the most recent computational models suggest that rather than a bimodal-like mass spectrum between $\\textit{light}$ and $\\textit{heavy}$ seeds with $\\textit{light}$ at one end and $\\textit{heavy}$ at the other that instead a continuum exists. $\\textit{Light}$ seeds being more ubiquitous and the heavier seeds becoming less and less abundant due the rarer environmental conditions required for their formation. We therefore examine the different mechanisms that give rise to different seed mass spectrums. We show how and why the mechanisms that produce the $\\textit{heaviest}$ seeds are also among the rarest events in the Universe and are hence extremely unlikely to be the seeds for the vast majority of the MBH population. We quantify, within the limits of the current large uncertainties in the seeding processes, the expected number densities of the seed mass spectrum. We argue that $\\textit{light}$ seeds must be at least $10^{3}$ to $10^{5}$ times more numerous than $\\textit{heavy}$ seeds to explain the MBH population as a whole. Based on our current understanding of the seed population this makes $\\textit{heavy}$ seeds ($\\rm{M_{seed}} > 10^3$ M$_{\\odot}$) a significantly more likely pathway given that $\\textit{heavy}$ seeds have an abundance pattern than is close to and likely in excess of $10^{-4}$ compared to $\\textit{light}$ seeds. Finally, we examine the current state-of-the-art in numerical calculations and recent observations and plot a path forward for near-future advances in both domains.","sentences":["The pathway(s) to seeding the massive black holes (MBHs) that exist at the heart of galaxies in the present and distant Universe remains an unsolved problem.","Here we categorise, describe and quantitatively discuss the formation pathways of both $\\textit{light}$ and $\\textit{heavy}$ seeds.","We emphasise that the most recent computational models suggest that rather than a bimodal-like mass spectrum between $\\textit{light}$ and $\\textit{heavy}$ seeds with $\\textit{light}$ at one end and $\\textit{heavy}$ at the other that instead a continuum exists.","$\\textit{Light}$ seeds being more ubiquitous and the heavier seeds becoming less and less abundant due the rarer environmental conditions required for their formation.","We therefore examine the different mechanisms that give rise to different seed mass spectrums.","We show how and why the mechanisms that produce the $\\textit{heaviest}$ seeds are also among the rarest events in the Universe and are hence extremely unlikely to be the seeds for the vast majority of the MBH population.","We quantify, within the limits of the current large uncertainties in the seeding processes, the expected number densities of the seed mass spectrum.","We argue that $\\textit{light}$ seeds must be at least $10^{3}$ to $10^{5}$ times more numerous than $\\textit{heavy}$ seeds to explain the MBH population as a whole.","Based on our current understanding of the seed population this makes $\\textit{heavy}$ seeds ($\\rm{M_{seed}} > 10^3$ M$_{\\odot}$) a significantly more likely pathway given that $\\textit{heavy}$ seeds have an abundance pattern than is close to and likely in excess of $10^{-4}$ compared to $\\textit{light}$ seeds.","Finally, we examine the current state-of-the-art in numerical calculations and recent observations and plot a path forward for near-future advances in both domains."],"url":"http://arxiv.org/abs/2405.17975v1","category":"astro-ph.GA"}
{"created":"2024-05-28 08:34:41","title":"Efficient Prior Calibration From Indirect Data","abstract":"Bayesian inversion is central to the quantification of uncertainty within problems arising from numerous applications in science and engineering. To formulate the approach, four ingredients are required: a forward model mapping the unknown parameter to an element of a solution space, often the solution space for a differential equation; an observation operator mapping an element of the solution space to the data space; a noise model describing how noise pollutes the observations; and a prior model describing knowledge about the unknown parameter before the data is acquired. This paper is concerned with learning the prior model from data; in particular, learning the prior from multiple realizations of indirect data obtained through the noisy observation process. The prior is represented, using a generative model, as the pushforward of a Gaussian in a latent space; the pushforward map is learned by minimizing an appropriate loss function. A metric that is well-defined under empirical approximation is used to define the loss function for the pushforward map to make an implementable methodology. Furthermore, an efficient residual-based neural operator approximation of the forward model is proposed and it is shown that this may be learned concurrently with the pushforward map, using a bilevel optimization formulation of the problem; this use of neural operator approximation has the potential to make prior learning from indirect data more computationally efficient, especially when the observation process is expensive, non-smooth or not known. The ideas are illustrated with the Darcy flow inverse problem of finding permeability from piezometric head measurements.","sentences":["Bayesian inversion is central to the quantification of uncertainty within problems arising from numerous applications in science and engineering.","To formulate the approach, four ingredients are required: a forward model mapping the unknown parameter to an element of a solution space, often the solution space for a differential equation; an observation operator mapping an element of the solution space to the data space; a noise model describing how noise pollutes the observations; and a prior model describing knowledge about the unknown parameter before the data is acquired.","This paper is concerned with learning the prior model from data; in particular, learning the prior from multiple realizations of indirect data obtained through the noisy observation process.","The prior is represented, using a generative model, as the pushforward of a Gaussian in a latent space; the pushforward map is learned by minimizing an appropriate loss function.","A metric that is well-defined under empirical approximation is used to define the loss function for the pushforward map to make an implementable methodology.","Furthermore, an efficient residual-based neural operator approximation of the forward model is proposed and it is shown that this may be learned concurrently with the pushforward map, using a bilevel optimization formulation of the problem; this use of neural operator approximation has the potential to make prior learning from indirect data more computationally efficient, especially when the observation process is expensive, non-smooth or not known.","The ideas are illustrated with the Darcy flow inverse problem of finding permeability from piezometric head measurements."],"url":"http://arxiv.org/abs/2405.17955v1","category":"stat.ML"}
{"created":"2024-05-28 08:01:58","title":"The 4-Planes of Spin(7) Manifolds","abstract":"This paper investigates the geometric structures and properties of 8-dimensional manifolds with Spin(7)-holonomy. We focus on the characterization and implications of 4-planes within these manifolds, which are endowed with an almost symplectic structure compatible with the Spin(7)-structure. We provide a detailed analysis of the differential forms defining these structures, including the Cayley 4-form and its stabilization under the Spin(7) group actions. Furthermore, we explore the concept of mirror duality within the context of exceptional holonomy, elucidating the relationship between dual structures and their topological constraints. By examining the conditions under which mirror duality arises, we enhance the understanding of the interplay between Spin(7)-structures and symplectic geometry. This endeavor contributes to our deeper understanding of the intricate symmetries and topological properties of these remarkable manifolds.","sentences":["This paper investigates the geometric structures and properties of 8-dimensional manifolds with Spin(7)-holonomy.","We focus on the characterization and implications of 4-planes within these manifolds, which are endowed with an almost symplectic structure compatible with the Spin(7)-structure.","We provide a detailed analysis of the differential forms defining these structures, including the Cayley 4-form and its stabilization under the Spin(7) group actions.","Furthermore, we explore the concept of mirror duality within the context of exceptional holonomy, elucidating the relationship between dual structures and their topological constraints.","By examining the conditions under which mirror duality arises, we enhance the understanding of the interplay between Spin(7)-structures and symplectic geometry.","This endeavor contributes to our deeper understanding of the intricate symmetries and topological properties of these remarkable manifolds."],"url":"http://arxiv.org/abs/2405.17936v1","category":"math.DG"}
{"created":"2024-05-28 07:51:17","title":"Modified double brackets and a conjecture of S. Arthamonov","abstract":"Around 20 years ago, M. Van den Bergh introduced double Poisson brackets as operations on associative algebras inducing Poisson brackets under the representation functor. Weaker versions of these operations, called modified double Poisson brackets, were later introduced by S. Arthamonov in order to induce a Poisson bracket on moduli spaces of representations of the corresponding associative algebras. Moreover, he defined two operations that he conjectured to be modified double Poisson brackets. The first case of this conjecture was recently proved by M. Goncharov and V. Gubarev motivated by the theory of Rota-Baxter operators of nonzero weight. We settle the conjecture by realising the second case as part of a new family of modified double Poisson brackets.","sentences":["Around 20 years ago, M. Van den Bergh introduced double Poisson brackets as operations on associative algebras inducing Poisson brackets under the representation functor.","Weaker versions of these operations, called modified double Poisson brackets, were later introduced by S. Arthamonov in order to induce a Poisson bracket on moduli spaces of representations of the corresponding associative algebras.","Moreover, he defined two operations that he conjectured to be modified double Poisson brackets.","The first case of this conjecture was recently proved by M. Goncharov and V. Gubarev motivated by the theory of Rota-Baxter operators of nonzero weight.","We settle the conjecture by realising the second case as part of a new family of modified double Poisson brackets."],"url":"http://arxiv.org/abs/2405.17930v1","category":"math.RA"}
{"created":"2024-05-28 07:45:22","title":"Stochastic Optimization Schemes for Performative Prediction with Nonconvex Loss","abstract":"This paper studies a risk minimization problem with decision dependent data distribution. The problem pertains to the performative prediction setting where a trained model can affect the outcome that the model estimates. Such dependency creates a feedback loop that influences the stability of optimization algorithms such as stochastic gradient descent (SGD). We present the first study on performative prediction with smooth but possibly non-convex loss. We analyze a greedy deployment scheme with SGD (SGD-GD). Note that in the literature, SGD-GD is often studied with strongly convex loss. We first propose the definition of stationary performative stable (SPS) solutions through relaxing the popular performative stable condition. We then prove that SGD-GD converges to a biased SPS solution in expectation. We consider two conditions of sensitivity on the distribution shifts: (i) the sensitivity is characterized by Wasserstein-1 distance and the loss is Lipschitz w.r.t.~data samples, or (ii) the sensitivity is characterized by $\\chi^2$-divergence and the loss is bounded. In both conditions, the bias levels are proportional to stochastic gradient's variance and sensitivity level. Our analysis is extended to a lazy deployment scheme where models are deployed once per several SGD updates, and we show that it converges to a bias-free SPS solution. Numerical experiments corroborate our theories.","sentences":["This paper studies a risk minimization problem with decision dependent data distribution.","The problem pertains to the performative prediction setting where a trained model can affect the outcome that the model estimates.","Such dependency creates a feedback loop that influences the stability of optimization algorithms such as stochastic gradient descent (SGD).","We present the first study on performative prediction with smooth but possibly non-convex loss.","We analyze a greedy deployment scheme with SGD (SGD-GD).","Note that in the literature, SGD-GD is often studied with strongly convex loss.","We first propose the definition of stationary performative stable (SPS) solutions through relaxing the popular performative stable condition.","We then prove that SGD-GD converges to a biased SPS solution in expectation.","We consider two conditions of sensitivity on the distribution shifts: (i) the sensitivity is characterized by Wasserstein-1 distance and the loss is Lipschitz w.r.t.~data samples, or (ii) the sensitivity is characterized by $\\chi^2$-divergence and the loss is bounded.","In both conditions, the bias levels are proportional to stochastic gradient's variance and sensitivity level.","Our analysis is extended to a lazy deployment scheme where models are deployed once per several SGD updates, and we show that it converges to a bias-free SPS solution.","Numerical experiments corroborate our theories."],"url":"http://arxiv.org/abs/2405.17922v1","category":"math.OC"}
{"created":"2024-05-28 07:12:22","title":"A Refined 3D Gaussian Representation for High-Quality Dynamic Scene Reconstruction","abstract":"In recent years, Neural Radiance Fields (NeRF) has revolutionized three-dimensional (3D) reconstruction with its implicit representation. Building upon NeRF, 3D Gaussian Splatting (3D-GS) has departed from the implicit representation of neural networks and instead directly represents scenes as point clouds with Gaussian-shaped distributions. While this shift has notably elevated the rendering quality and speed of radiance fields but inevitably led to a significant increase in memory usage. Additionally, effectively rendering dynamic scenes in 3D-GS has emerged as a pressing challenge. To address these concerns, this paper purposes a refined 3D Gaussian representation for high-quality dynamic scene reconstruction. Firstly, we use a deformable multi-layer perceptron (MLP) network to capture the dynamic offset of Gaussian points and express the color features of points through hash encoding and a tiny MLP to reduce storage requirements. Subsequently, we introduce a learnable denoising mask coupled with denoising loss to eliminate noise points from the scene, thereby further compressing 3D Gaussian model. Finally, motion noise of points is mitigated through static constraints and motion consistency constraints. Experimental results demonstrate that our method surpasses existing approaches in rendering quality and speed, while significantly reducing the memory usage associated with 3D-GS, making it highly suitable for various tasks such as novel view synthesis, and dynamic mapping.","sentences":["In recent years, Neural Radiance Fields (NeRF) has revolutionized three-dimensional (3D) reconstruction with its implicit representation.","Building upon NeRF, 3D Gaussian Splatting (3D-GS) has departed from the implicit representation of neural networks and instead directly represents scenes as point clouds with Gaussian-shaped distributions.","While this shift has notably elevated the rendering quality and speed of radiance fields but inevitably led to a significant increase in memory usage.","Additionally, effectively rendering dynamic scenes in 3D-GS has emerged as a pressing challenge.","To address these concerns, this paper purposes a refined 3D Gaussian representation for high-quality dynamic scene reconstruction.","Firstly, we use a deformable multi-layer perceptron (MLP) network to capture the dynamic offset of Gaussian points and express the color features of points through hash encoding and a tiny MLP to reduce storage requirements.","Subsequently, we introduce a learnable denoising mask coupled with denoising loss to eliminate noise points from the scene, thereby further compressing 3D Gaussian model.","Finally, motion noise of points is mitigated through static constraints and motion consistency constraints.","Experimental results demonstrate that our method surpasses existing approaches in rendering quality and speed, while significantly reducing the memory usage associated with 3D-GS, making it highly suitable for various tasks such as novel view synthesis, and dynamic mapping."],"url":"http://arxiv.org/abs/2405.17891v1","category":"cs.CV"}
{"created":"2024-05-28 07:08:29","title":"When is exponential asymptotic optimality achievable in average-reward restless bandits?","abstract":"We consider the discrete-time infinite-horizon average-reward restless bandit problem. We propose a novel policy that maintains two dynamic subsets of arms: one subset of arms has a nearly optimal state distribution and takes actions according to an Optimal Local Control routine; the other subset of arms is driven towards the optimal state distribution and gradually merged into the first subset. We show that our policy is asymptotically optimal with an $O(\\exp(-C N))$ optimality gap for an $N$-armed problem, under the mild assumptions of aperiodic-unichain, non-degeneracy, and local stability. Our policy is the first to achieve exponential asymptotic optimality under the above set of easy-to-verify assumptions, whereas prior work either requires a strong Global Attractor assumption or only achieves an $O(1/\\sqrt{N})$ optimality gap. We further discuss the fundamental obstacles in significantly weakening our assumptions. In particular, we prove a lower bound showing that local stability is fundamental for exponential asymptotic optimality.","sentences":["We consider the discrete-time infinite-horizon average-reward restless bandit problem.","We propose a novel policy that maintains two dynamic subsets of arms: one subset of arms has a nearly optimal state distribution and takes actions according to an Optimal Local Control routine; the other subset of arms is driven towards the optimal state distribution and gradually merged into the first subset.","We show that our policy is asymptotically optimal with an $O(\\exp(-C N))$ optimality gap for an $N$-armed problem, under the mild assumptions of aperiodic-unichain, non-degeneracy, and local stability.","Our policy is the first to achieve exponential asymptotic optimality under the above set of easy-to-verify assumptions, whereas prior work either requires a strong Global Attractor assumption or only achieves an $O(1/\\sqrt{N})$ optimality gap.","We further discuss the fundamental obstacles in significantly weakening our assumptions.","In particular, we prove a lower bound showing that local stability is fundamental for exponential asymptotic optimality."],"url":"http://arxiv.org/abs/2405.17882v1","category":"cs.LG"}
{"created":"2024-05-28 07:00:28","title":"Diffusion Rejection Sampling","abstract":"Recent advances in powerful pre-trained diffusion models encourage the development of methods to improve the sampling performance under well-trained diffusion models. This paper introduces Diffusion Rejection Sampling (DiffRS), which uses a rejection sampling scheme that aligns the sampling transition kernels with the true ones at each timestep. The proposed method can be viewed as a mechanism that evaluates the quality of samples at each intermediate timestep and refines them with varying effort depending on the sample. Theoretical analysis shows that DiffRS can achieve a tighter bound on sampling error compared to pre-trained models. Empirical results demonstrate the state-of-the-art performance of DiffRS on the benchmark datasets and the effectiveness of DiffRS for fast diffusion samplers and large-scale text-to-image diffusion models. Our code is available at https://github.com/aailabkaist/DiffRS.","sentences":["Recent advances in powerful pre-trained diffusion models encourage the development of methods to improve the sampling performance under well-trained diffusion models.","This paper introduces Diffusion Rejection Sampling (DiffRS), which uses a rejection sampling scheme that aligns the sampling transition kernels with the true ones at each timestep.","The proposed method can be viewed as a mechanism that evaluates the quality of samples at each intermediate timestep and refines them with varying effort depending on the sample.","Theoretical analysis shows that DiffRS can achieve a tighter bound on sampling error compared to pre-trained models.","Empirical results demonstrate the state-of-the-art performance of DiffRS on the benchmark datasets and the effectiveness of DiffRS for fast diffusion samplers and large-scale text-to-image diffusion models.","Our code is available at https://github.com/aailabkaist/DiffRS."],"url":"http://arxiv.org/abs/2405.17880v1","category":"cs.LG"}
{"created":"2024-05-28 06:56:39","title":"Sparsity- and Hybridity-Inspired Visual Parameter-Efficient Fine-Tuning for Medical Diagnosis","abstract":"The success of Large Vision Models (LVMs) is accompanied by vast data volumes, which are prohibitively expensive in medical diagnosis.To address this, recent efforts exploit Parameter-Efficient Fine-Tuning (PEFT), which trains a small number of weights while freezing the rest.However, they typically assign trainable weights to the same positions in LVMs in a heuristic manner, regardless of task differences, making them suboptimal for professional applications like medical diagnosis.To address this, we statistically reveal the nature of sparsity and hybridity during diagnostic-targeted fine-tuning, i.e., a small portion of key weights significantly impacts performance, and these key weights are hybrid, including both task-specific and task-agnostic parts.Based on this, we propose a novel Sparsity- and Hybridity-inspired Parameter Efficient Fine-Tuning (SH-PEFT).It selects and trains a small portion of weights based on their importance, which is innovatively estimated by hybridizing both task-specific and task-agnostic strategies.Validated on six medical datasets of different modalities, we demonstrate that SH-PEFT achieves state-of-the-art performance in transferring LVMs to medical diagnosis in terms of accuracy. By tuning around 0.01% number of weights, it outperforms full model fine-tuning.Moreover, SH-PEFT also achieves comparable performance to other models deliberately optimized for specific medical tasks.Extensive experiments demonstrate the effectiveness of each design and reveal that large model transfer holds great potential in medical diagnosis.","sentences":["The success of Large Vision Models (LVMs) is accompanied by vast data volumes, which are prohibitively expensive in medical diagnosis.","To address this, recent efforts exploit Parameter-Efficient Fine-Tuning (PEFT), which trains a small number of weights while freezing the rest.","However, they typically assign trainable weights to the same positions in LVMs in a heuristic manner, regardless of task differences, making them suboptimal for professional applications like medical diagnosis.","To address this, we statistically reveal the nature of sparsity and hybridity during diagnostic-targeted fine-tuning, i.e., a small portion of key weights significantly impacts performance, and these key weights are hybrid, including both task-specific and task-agnostic parts.","Based on this, we propose a novel Sparsity- and Hybridity-inspired Parameter Efficient Fine-Tuning (SH-PEFT).It selects and trains a small portion of weights based on their importance, which is innovatively estimated by hybridizing both task-specific and task-agnostic strategies.","Validated on six medical datasets of different modalities, we demonstrate that SH-PEFT achieves state-of-the-art performance in transferring LVMs to medical diagnosis in terms of accuracy.","By tuning around 0.01% number of weights, it outperforms full model fine-tuning.","Moreover, SH-PEFT also achieves comparable performance to other models deliberately optimized for specific medical tasks.","Extensive experiments demonstrate the effectiveness of each design and reveal that large model transfer holds great potential in medical diagnosis."],"url":"http://arxiv.org/abs/2405.17877v1","category":"cs.CV"}
{"created":"2024-05-28 06:48:02","title":"HFGS: 4D Gaussian Splatting with Emphasis on Spatial and Temporal High-Frequency Components for Endoscopic Scene Reconstruction","abstract":"Robot-assisted minimally invasive surgery benefits from enhancing dynamic scene reconstruction, as it improves surgical outcomes. While Neural Radiance Fields (NeRF) have been effective in scene reconstruction, their slow inference speeds and lengthy training durations limit their applicability. To overcome these limitations, 3D Gaussian Splatting (3D-GS) based methods have emerged as a recent trend, offering rapid inference capabilities and superior 3D quality. However, these methods still struggle with under-reconstruction in both static and dynamic scenes. In this paper, we propose HFGS, a novel approach for deformable endoscopic reconstruction that addresses these challenges from spatial and temporal frequency perspectives. Our approach incorporates deformation fields to better handle dynamic scenes and introduces Spatial High-Frequency Emphasis Reconstruction (SHF) to minimize discrepancies in spatial frequency spectra between the rendered image and its ground truth. Additionally, we introduce Temporal High-Frequency Emphasis Reconstruction (THF) to enhance dynamic awareness in neural rendering by leveraging flow priors, focusing optimization on motion-intensive parts. Extensive experiments on two widely used benchmarks demonstrate that HFGS achieves superior rendering quality. Our code will be available.","sentences":["Robot-assisted minimally invasive surgery benefits from enhancing dynamic scene reconstruction, as it improves surgical outcomes.","While Neural Radiance Fields (NeRF) have been effective in scene reconstruction, their slow inference speeds and lengthy training durations limit their applicability.","To overcome these limitations, 3D Gaussian Splatting (3D-GS) based methods have emerged as a recent trend, offering rapid inference capabilities and superior 3D quality.","However, these methods still struggle with under-reconstruction in both static and dynamic scenes.","In this paper, we propose HFGS, a novel approach for deformable endoscopic reconstruction that addresses these challenges from spatial and temporal frequency perspectives.","Our approach incorporates deformation fields to better handle dynamic scenes and introduces Spatial High-Frequency Emphasis Reconstruction (SHF) to minimize discrepancies in spatial frequency spectra between the rendered image and its ground truth.","Additionally, we introduce Temporal High-Frequency Emphasis Reconstruction (THF) to enhance dynamic awareness in neural rendering by leveraging flow priors, focusing optimization on motion-intensive parts.","Extensive experiments on two widely used benchmarks demonstrate that HFGS achieves superior rendering quality.","Our code will be available."],"url":"http://arxiv.org/abs/2405.17872v1","category":"cs.CV"}
{"created":"2024-05-28 06:31:47","title":"Adjustable Robust Nonlinear Network Design under Demand Uncertainties","abstract":"We study network design problems for nonlinear and nonconvex flow models under demand uncertainties. To this end, we apply the concept of adjustable robust optimization to compute a network design that admits a feasible transport for all, possibly infinitely many, demand scenarios within a given uncertainty set. For solving the corresponding adjustable robust mixed-integer nonlinear optimization problem, we show that a given network design is robust feasible, i.e., it admits a feasible transport for all demand uncertainties, if and only if a finite number of worst-case demand scenarios can be routed through the network. We compute these worst-case scenarios by solving polynomially many nonlinear optimization problems. Embedding this result for robust feasibility in an adversarial approach leads to an exact algorithm that computes an optimal robust network design in a finite number of iterations. Since all of the results are valid for general potential-based flows, the approach can be applied to different utility networks such as gas, hydrogen, or water networks. We finally demonstrate the applicability of the method by computing robust gas networks that are protected from future demand fluctuations.","sentences":["We study network design problems for nonlinear and nonconvex flow models under demand uncertainties.","To this end, we apply the concept of adjustable robust optimization to compute a network design that admits a feasible transport for all, possibly infinitely many, demand scenarios within a given uncertainty set.","For solving the corresponding adjustable robust mixed-integer nonlinear optimization problem, we show that a given network design is robust feasible, i.e., it admits a feasible transport for all demand uncertainties, if and only if a finite number of worst-case demand scenarios can be routed through the network.","We compute these worst-case scenarios by solving polynomially many nonlinear optimization problems.","Embedding this result for robust feasibility in an adversarial approach leads to an exact algorithm that computes an optimal robust network design in a finite number of iterations.","Since all of the results are valid for general potential-based flows, the approach can be applied to different utility networks such as gas, hydrogen, or water networks.","We finally demonstrate the applicability of the method by computing robust gas networks that are protected from future demand fluctuations."],"url":"http://arxiv.org/abs/2405.17867v1","category":"math.OC"}
{"created":"2024-05-28 06:20:43","title":"The effect of interstellar medium on LVK's black holes","abstract":"Gravitational radiation alone is not efficient in hardening the orbit of a wide binary black hole (BBH). By employing a toy model for the interstellar medium (ISM) surrounding BBHs, here we discuss the effect of this baryonic medium on BBH dynamics. Depending on the BBH's mass, we show that a binary surrounded by an isotropic cold neutral medium (i.e., an asymptotic temperature $T_{\\infty} \\approx 100$ K) with a time-averaged particle density of $\\langle n_H \\rangle = \\mathcal{O}(1)$ cm$^{-3}$ can play a significant role in hardening the binary orbit over a $\\mathcal{O}(10^9)$ yr time scale. Additionally, this causes the black hole's mass to grow at a rate $\\propto m^2$. We thus discuss the impact of the ISM on the LIGO-Virgo-KAGRA (LVK) observables and quantify the properties of the ISM under which the latter could act as an additional important pathway for driving a subset of LVK's BBH mergers.","sentences":["Gravitational radiation alone is not efficient in hardening the orbit of a wide binary black hole (BBH).","By employing a toy model for the interstellar medium (ISM) surrounding BBHs, here we discuss the effect of this baryonic medium on BBH dynamics.","Depending on the BBH's mass, we show that a binary surrounded by an isotropic cold neutral medium (i.e., an asymptotic temperature $T_{\\infty} \\approx 100$ K) with a time-averaged particle density of $\\langle n_H \\rangle = \\mathcal{O}(1)$ cm$^{-3}$ can play a significant role in hardening the binary orbit over a $\\mathcal{O}(10^9)$ yr time scale.","Additionally, this causes the black hole's mass to grow at a rate $\\propto m^2$. We thus discuss the impact of the ISM on the LIGO-Virgo-KAGRA (LVK) observables and quantify the properties of the ISM under which the latter could act as an additional important pathway for driving a subset of LVK's BBH mergers."],"url":"http://arxiv.org/abs/2405.17863v1","category":"astro-ph.HE"}
{"created":"2024-05-28 06:20:14","title":"Towards robust prediction of material properties for nuclear reactor design under scarce data -- a study in creep rupture property","abstract":"Advances in Deep Learning bring further investigation into credibility and robustness, especially for safety-critical engineering applications such as the nuclear industry. The key challenges include the availability of data set (often scarce and sparse) and insufficient consideration of the uncertainty in the data, model, and prediction. This paper therefore presents a meta-learning based approach that is both uncertainty- and prior knowledge-informed, aiming at trustful predictions of material properties for the nuclear reactor design. It is suited for robust learning under limited data. Uncertainty has been accounted for where a distribution of predictor functions are produced for extrapolation. Results suggest it achieves superior performance than existing empirical methods in rupture life prediction, a case which is typically under a small data regime. While demonstrated herein with rupture properties, this learning approach is transferable to solve similar problems of data scarcity across the nuclear industry. It is of great importance to boosting the AI analytics in the nuclear industry by proving the applicability and robustness while providing tools that can be trusted.","sentences":["Advances in Deep Learning bring further investigation into credibility and robustness, especially for safety-critical engineering applications such as the nuclear industry.","The key challenges include the availability of data set (often scarce and sparse) and insufficient consideration of the uncertainty in the data, model, and prediction.","This paper therefore presents a meta-learning based approach that is both uncertainty- and prior knowledge-informed, aiming at trustful predictions of material properties for the nuclear reactor design.","It is suited for robust learning under limited data.","Uncertainty has been accounted for where a distribution of predictor functions are produced for extrapolation.","Results suggest it achieves superior performance than existing empirical methods in rupture life prediction, a case which is typically under a small data regime.","While demonstrated herein with rupture properties, this learning approach is transferable to solve similar problems of data scarcity across the nuclear industry.","It is of great importance to boosting the AI analytics in the nuclear industry by proving the applicability and robustness while providing tools that can be trusted."],"url":"http://arxiv.org/abs/2405.17862v1","category":"cs.LG"}
{"created":"2024-05-28 06:16:47","title":"Uncertainty quantification for damage mechanics models using the bootstrap method","abstract":"We quantify the uncertainty of the L\\\"ammer model of damage evolution when fitted to (noisy) observations of damage evolution in cyclic fatigue experiments with and without dwell time. We therefore develop a bootstrap method by sampling over blocks of load cycles in the experiments in order to quantify the uncertainty in the material parameters of the L\\\"ammer damage evolution equation. We first develop a resilient optimization algorithm for parameter identification based on numerical solutions of damage evolution. The uncertainty is quantified on three levels: distribution of parameters of the L\\\"ammer model, confidence bands for the solutions of damage evolution, and distributions of failure times. The method is tested on several data sets, committing considerable high-performance computing resources to the task.","sentences":["We quantify the uncertainty of the L\\\"ammer model of damage evolution when fitted to (noisy) observations of damage evolution in cyclic fatigue experiments with and without dwell time.","We therefore develop a bootstrap method by sampling over blocks of load cycles in the experiments in order to quantify the uncertainty in the material parameters of the L\\\"ammer damage evolution equation.","We first develop a resilient optimization algorithm for parameter identification based on numerical solutions of damage evolution.","The uncertainty is quantified on three levels: distribution of parameters of the L\\\"ammer model, confidence bands for the solutions of damage evolution, and distributions of failure times.","The method is tested on several data sets, committing considerable high-performance computing resources to the task."],"url":"http://arxiv.org/abs/2405.17858v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-28 06:11:58","title":"White dwarf magnetospheres: Shielding volatile content of icy objects and implications for volatile pollution scarcity","abstract":"Context. About 25% -- 50% of white dwarfs are found to be contaminated by heavy elements, which are believed to originate from external sources such as planetary materials. Elemental abundances suggest that most of the pollutants are rocky objects and only a small fraction of white dwarfs bear traces of volatile accretion.   Aims. In order to account for the scarcity of volatile pollution, we investigate the role of the white dwarfs' magnetospheres in shielding the volatile content of icy objects.   Methods. We estimated the volatile sublimation of inward-drifting exocomets. We assume the orbits of the exocomets are circularized by the Alfven wing drag that is effective for long-period comets.   Results. Volatile material can sublimate outside the corotation radius and be shielded by the magnetic field. {The two conditions for this volatile-shielded mechanism are that the magnetosphere radius must be larger than the corotation radius and that the volatiles are depleted outside the corotation radius, which requires a sufficiently slow orbital circularization process.} We applied our model to nine white dwarfs with known rotational periods, magnetic fields, and atmosphere compositions. Our volatile-shielded model may explain the excess of volatile elements such as C and S in the disk relative to the white dwarf atmosphere in WD2326+049 (G29-38). Nevertheless, given the sensitivity of our model to the circularization process and material properties of icy objects, there remains considerable uncertainty in our results.   Conclusions. Our work suggests a possible explanation for the scarcity of volatile-accretion signatures among white dwarfs. We also identify a correlation between the magnetic field strength, the spin period, and the composition of pollutants in white dwarf atmospheres.","sentences":["Context.","About 25% -- 50% of white dwarfs are found to be contaminated by heavy elements, which are believed to originate from external sources such as planetary materials.","Elemental abundances suggest that most of the pollutants are rocky objects and only a small fraction of white dwarfs bear traces of volatile accretion.   ","Aims.","In order to account for the scarcity of volatile pollution, we investigate the role of the white dwarfs' magnetospheres in shielding the volatile content of icy objects.   Methods.","We estimated the volatile sublimation of inward-drifting exocomets.","We assume the orbits of the exocomets are circularized by the Alfven wing drag that is effective for long-period comets.   Results.","Volatile material can sublimate outside the corotation radius and be shielded by the magnetic field.","{The two conditions for this volatile-shielded mechanism are that the magnetosphere radius must be larger than the corotation radius and that the volatiles are depleted outside the corotation radius, which requires a sufficiently slow orbital circularization process.}","We applied our model to nine white dwarfs with known rotational periods, magnetic fields, and atmosphere compositions.","Our volatile-shielded model may explain the excess of volatile elements such as C and S in the disk relative to the white dwarf atmosphere in WD2326+049 (G29-38).","Nevertheless, given the sensitivity of our model to the circularization process and material properties of icy objects, there remains considerable uncertainty in our results.   ","Conclusions.","Our work suggests a possible explanation for the scarcity of volatile-accretion signatures among white dwarfs.","We also identify a correlation between the magnetic field strength, the spin period, and the composition of pollutants in white dwarf atmospheres."],"url":"http://arxiv.org/abs/2405.17853v1","category":"astro-ph.EP"}
{"created":"2024-05-28 05:52:19","title":"Sparsification of Phylogenetic Covariance Matrices of $k$-Regular Trees","abstract":"Consider a tree $T=(V,E)$ with root $\\circ$ and edge length function $\\ell:E\\to\\mathbb{R}_+$. The phylogenetic covariance matrix of $T$ is the matrix $C$ with rows and columns indexed by $L$, the leaf set of $T$, with entries $C(i,j):=\\sum_{e\\in[i\\wedge j,o]}\\ell(e)$, for each $i,j\\in L$. Recent work [15] has shown that the phylogenetic covariance matrix of a large, random binary tree $T$ is significantly sparsified with overwhelmingly high probability under a change-of-basis with respect to the so-called Haar-like wavelets of $T$. This finding notably enables manipulating the spectrum of covariance matrices of large binary trees without the necessity to store them in computer memory but instead performing two post-order traversals of the tree. Building on the methods of [15], this manuscript further advances their sparsification result to encompass the broader class of $k$-regular trees, for any given $k\\ge2$. This extension is achieved by refining existing asymptotic formulas for the mean and variance of the internal path length of random $k$-regular trees, utilizing hypergeometric function properties and identities.","sentences":["Consider a tree $T=(V,E)$ with root $\\circ$ and edge length function $\\ell:E\\to\\mathbb{R}_+$. The phylogenetic covariance matrix of $T$ is the matrix $C$ with rows and columns indexed by $L$, the leaf set of $T$, with entries $C(i,j):=\\sum_{e\\in[i\\wedge j,o]}\\ell(e)$, for each $i,j\\in L$.","Recent work [15] has shown that the phylogenetic covariance matrix of a large, random binary tree $T$ is significantly sparsified with overwhelmingly high probability under a change-of-basis with respect to the so-called Haar-like wavelets of $T$. This finding notably enables manipulating the spectrum of covariance matrices of large binary trees without the necessity to store them in computer memory but instead performing two post-order traversals of the tree.","Building on the methods of [15], this manuscript further advances their sparsification result to encompass the broader class of $k$-regular trees, for any given $k\\ge2$.","This extension is achieved by refining existing asymptotic formulas for the mean and variance of the internal path length of random $k$-regular trees, utilizing hypergeometric function properties and identities."],"url":"http://arxiv.org/abs/2405.17847v1","category":"q-bio.PE"}
{"created":"2024-05-28 05:45:30","title":"Ai.llude: Encouraging Rewriting AI-Generated Text to Support Creative Expression","abstract":"In each step of the creative writing process, writers must grapple with their creative goals and individual perspectives. This process affects the writer's sense of authenticity and their engagement with the written output. Fluent text generation by AIs risks undermining the reflective loop of rewriting. We hypothesize that deliberately generating imperfect intermediate text can encourage rewriting and prompt higher level decision making. Using logs from 27 writing sessions using a text generation AI, we characterize how writers adapt and rewrite AI suggestions, and show that intermediate suggestions significantly motivate and increase rewriting. We discuss the implications of this finding, and future steps for investigating how to leverage intermediate text in AI writing support tools to support ownership over creative expression.","sentences":["In each step of the creative writing process, writers must grapple with their creative goals and individual perspectives.","This process affects the writer's sense of authenticity and their engagement with the written output.","Fluent text generation by AIs risks undermining the reflective loop of rewriting.","We hypothesize that deliberately generating imperfect intermediate text can encourage rewriting and prompt higher level decision making.","Using logs from 27 writing sessions using a text generation AI, we characterize how writers adapt and rewrite AI suggestions, and show that intermediate suggestions significantly motivate and increase rewriting.","We discuss the implications of this finding, and future steps for investigating how to leverage intermediate text in AI writing support tools to support ownership over creative expression."],"url":"http://arxiv.org/abs/2405.17843v1","category":"cs.HC"}
{"created":"2024-05-28 05:43:03","title":"Discriminator-Guided Cooperative Diffusion for Joint Audio and Video Generation","abstract":"In this study, we aim to construct an audio-video generative model with minimal computational cost by leveraging pre-trained single-modal generative models for audio and video. To achieve this, we propose a novel method that guides each single-modal model to cooperatively generate well-aligned samples across modalities. Specifically, given two pre-trained base diffusion models, we train a lightweight joint guidance module to adjust scores separately estimated by the base models to match the score of joint distribution over audio and video. We theoretically show that this guidance can be computed through the gradient of the optimal discriminator distinguishing real audio-video pairs from fake ones independently generated by the base models. On the basis of this analysis, we construct the joint guidance module by training this discriminator. Additionally, we adopt a loss function to make the gradient of the discriminator work as a noise estimator, as in standard diffusion models, stabilizing the gradient of the discriminator. Empirical evaluations on several benchmark datasets demonstrate that our method improves both single-modal fidelity and multi-modal alignment with a relatively small number of parameters.","sentences":["In this study, we aim to construct an audio-video generative model with minimal computational cost by leveraging pre-trained single-modal generative models for audio and video.","To achieve this, we propose a novel method that guides each single-modal model to cooperatively generate well-aligned samples across modalities.","Specifically, given two pre-trained base diffusion models, we train a lightweight joint guidance module to adjust scores separately estimated by the base models to match the score of joint distribution over audio and video.","We theoretically show that this guidance can be computed through the gradient of the optimal discriminator distinguishing real audio-video pairs from fake ones independently generated by the base models.","On the basis of this analysis, we construct the joint guidance module by training this discriminator.","Additionally, we adopt a loss function to make the gradient of the discriminator work as a noise estimator, as in standard diffusion models, stabilizing the gradient of the discriminator.","Empirical evaluations on several benchmark datasets demonstrate that our method improves both single-modal fidelity and multi-modal alignment with a relatively small number of parameters."],"url":"http://arxiv.org/abs/2405.17842v1","category":"cs.CV"}
{"created":"2024-05-28 05:39:45","title":"Constrained monotone mean--variance investment-reinsurance under the Cram\u00e9r--Lundberg model with random coefficients","abstract":"This paper studies an optimal investment-reinsurance problem for an insurer (she) under the Cram\\'er--Lundberg model with monotone mean--variance (MMV) criterion. At any time, the insurer can purchase reinsurance (or acquire new business) and invest in a security market consisting of a risk-free asset and multiple risky assets whose excess return rate and volatility rate are allowed to be random. The trading strategy is subject to a general convex cone constraint, encompassing no-shorting constraint as a special case. The optimal investment-reinsurance strategy and optimal value for the MMV problem are deduced by solving certain backward stochastic differential equations with jumps. In the literature, it is known that models with MMV criterion and mean--variance criterion lead to the same optimal strategy and optimal value when the wealth process is continuous. Our result shows that the conclusion remains true even if the wealth process has compensated Poisson jumps and the market coefficients are random.","sentences":["This paper studies an optimal investment-reinsurance problem for an insurer (she) under the Cram\\'er--Lundberg model with monotone mean--variance (MMV) criterion.","At any time, the insurer can purchase reinsurance (or acquire new business) and invest in a security market consisting of a risk-free asset and multiple risky assets whose excess return rate and volatility rate are allowed to be random.","The trading strategy is subject to a general convex cone constraint, encompassing no-shorting constraint as a special case.","The optimal investment-reinsurance strategy and optimal value for the MMV problem are deduced by solving certain backward stochastic differential equations with jumps.","In the literature, it is known that models with MMV criterion and mean--variance criterion lead to the same optimal strategy and optimal value when the wealth process is continuous.","Our result shows that the conclusion remains true even if the wealth process has compensated Poisson jumps and the market coefficients are random."],"url":"http://arxiv.org/abs/2405.17841v1","category":"q-fin.PM"}
{"created":"2024-05-28 05:11:05","title":"Revisiting Step-Size Assumptions in Stochastic Approximation","abstract":"Many machine learning and optimization algorithms are built upon the framework of stochastic approximation (SA), for which the selection of step-size (or learning rate) is essential for success. For the sake of clarity, this paper focuses on the special case $\\alpha_n = \\alpha_0 n^{-\\rho}$ at iteration $n$, with $\\rho \\in [0,1]$ and $\\alpha_0>0$ design parameters. It is most common in practice to take $\\rho=0$ (constant step-size), while in more theoretically oriented papers a vanishing step-size is preferred. In particular, with $\\rho \\in (1/2, 1)$ it is known that on applying the averaging technique of Polyak and Ruppert, the mean-squared error (MSE) converges at the optimal rate of $O(1/n)$ and the covariance in the central limit theorem (CLT) is minimal in a precise sense.   The paper revisits step-size selection in a general Markovian setting. Under readily verifiable assumptions, the following conclusions are obtained provided $0<\\rho<1$:   $\\bullet$ Parameter estimates converge with probability one, and also in $L_p$ for any $p\\ge 1$.   $\\bullet$ The MSE may converge very slowly for small $\\rho$, of order $O(\\alpha_n^2)$ even with averaging.   $\\bullet$ For linear stochastic approximation the source of slow convergence is identified: for any $\\rho\\in (0,1)$, averaging results in estimates for which the error $\\textit{covariance}$ vanishes at the optimal rate, and moreover the   CLT covariance is optimal in the sense of Polyak and Ruppert. However, necessary and sufficient conditions are obtained under which the $\\textit{bias}$ converges to zero at rate $O(\\alpha_n)$.   This is the first paper to obtain such strong conclusions while allowing for $\\rho \\le 1/2$. A major conclusion is that the choice of $\\rho =0$ or even $\\rho<1/2$ is justified only in select settings -- In general, bias may preclude fast convergence.","sentences":["Many machine learning and optimization algorithms are built upon the framework of stochastic approximation (SA), for which the selection of step-size (or learning rate) is essential for success.","For the sake of clarity, this paper focuses on the special case $\\alpha_n = \\alpha_0 n^{-\\rho}$ at iteration $n$, with $\\rho \\in","[0,1]$ and $\\alpha_0>0$ design parameters.","It is most common in practice to take $\\rho=0$ (constant step-size), while in more theoretically oriented papers a vanishing step-size is preferred.","In particular, with $\\rho \\in (1/2, 1)$ it is known that on applying the averaging technique of Polyak and Ruppert, the mean-squared error (MSE) converges at the optimal rate of $O(1/n)$ and the covariance in the central limit theorem (CLT) is minimal in a precise sense.   ","The paper revisits step-size selection in a general Markovian setting.","Under readily verifiable assumptions, the following conclusions are obtained provided $0<\\rho<1$:   $\\bullet$ Parameter estimates converge with probability one, and also in $L_p$ for any $p\\ge 1$.   $\\bullet$ The MSE may converge very slowly for small $\\rho$, of order $O(\\alpha_n^2)$ even with averaging.   ","$\\bullet$ For linear stochastic approximation the source of slow convergence is identified: for any $\\rho\\in (0,1)$, averaging results in estimates for which the error $\\textit{covariance}$ vanishes at the optimal rate, and moreover the   CLT covariance is optimal in the sense of Polyak and Ruppert.","However, necessary and sufficient conditions are obtained under which the $\\textit{bias}$ converges to zero at rate $O(\\alpha_n)$.   This is the first paper to obtain such strong conclusions while allowing for $\\rho \\le 1/2$. A major conclusion is that the choice of $\\rho =0$ or even $\\rho<1/2$ is justified only in select settings -- In general, bias may preclude fast convergence."],"url":"http://arxiv.org/abs/2405.17834v1","category":"math.ST"}
{"created":"2024-05-28 05:10:11","title":"Neutral phylogenetic models and their role in tree-based biodiversity measures","abstract":"A wide variety of stochastic models of cladogenesis (based on speciation and extinction) lead to an identical distribution on phylogenetic tree shapes once the edge lengths are ignored. By contrast, the distribution of the tree's edge lengths is generally quite sensitive to the underlying model. In this paper, we review the impact of different model choices on tree shape and edge length distribution, and its impact for studying the properties of phylogenetic diversity (PD) as a measure of biodiversity, and the loss of PD as species become extinct at the present. We also compare PD with a stochastic model of feature diversity, and investigate some mathematical links and inequalities between these two measures plus their predictions concerning the loss of biodiversity under extinction at the present.","sentences":["A wide variety of stochastic models of cladogenesis (based on speciation and extinction) lead to an identical distribution on phylogenetic tree shapes once the edge lengths are ignored.","By contrast, the distribution of the tree's edge lengths is generally quite sensitive to the underlying model.","In this paper, we review the impact of different model choices on tree shape and edge length distribution, and its impact for studying the properties of phylogenetic diversity (PD) as a measure of biodiversity, and the loss of PD as species become extinct at the present.","We also compare PD with a stochastic model of feature diversity, and investigate some mathematical links and inequalities between these two measures plus their predictions concerning the loss of biodiversity under extinction at the present."],"url":"http://arxiv.org/abs/2405.17833v1","category":"q-bio.PE"}
{"created":"2024-05-28 04:52:45","title":"On Robust Clustering of Temporal Point Process","abstract":"Clustering of event stream data is of great importance in many application scenarios, including but not limited to, e-commerce, electronic health, online testing, mobile music service, etc. Existing clustering algorithms fail to take outlier data into consideration and are implemented without theoretical guarantees. In this paper, we propose a robust temporal point processes clustering framework which works under mild assumptions and meanwhile addresses several important issues in the event stream clustering problem.Specifically, we introduce a computationally efficient model-free distance function to quantify the dissimilarity between different event streams so that the outliers can be detected and the good initial clusters could be obtained. We further consider an expectation-maximization-type algorithm incorporated with a Catoni's influence function for robust estimation and fine-tuning of clusters. We also establish the theoretical results including algorithmic convergence, estimation error bound, outlier detection, etc. Simulation results corroborate our theoretical findings and real data applications show the effectiveness of our proposed methodology.","sentences":["Clustering of event stream data is of great importance in many application scenarios, including but not limited to, e-commerce, electronic health, online testing, mobile music service, etc.","Existing clustering algorithms fail to take outlier data into consideration and are implemented without theoretical guarantees.","In this paper, we propose a robust temporal point processes clustering framework which works under mild assumptions and meanwhile addresses several important issues in the event stream clustering problem.","Specifically, we introduce a computationally efficient model-free distance function to quantify the dissimilarity between different event streams so that the outliers can be detected and the good initial clusters could be obtained.","We further consider an expectation-maximization-type algorithm incorporated with a Catoni's influence function for robust estimation and fine-tuning of clusters.","We also establish the theoretical results including algorithmic convergence, estimation error bound, outlier detection, etc. Simulation results corroborate our theoretical findings and real data applications show the effectiveness of our proposed methodology."],"url":"http://arxiv.org/abs/2405.17828v1","category":"stat.ME"}
{"created":"2024-05-28 04:24:38","title":"Pursuing Feature Separation based on Neural Collapse for Out-of-Distribution Detection","abstract":"In the open world, detecting out-of-distribution (OOD) data, whose labels are disjoint with those of in-distribution (ID) samples, is important for reliable deep neural networks (DNNs). To achieve better detection performance, one type of approach proposes to fine-tune the model with auxiliary OOD datasets to amplify the difference between ID and OOD data through a separation loss defined on model outputs. However, none of these studies consider enlarging the feature disparity, which should be more effective compared to outputs. The main difficulty lies in the diversity of OOD samples, which makes it hard to describe their feature distribution, let alone design losses to separate them from ID features. In this paper, we neatly fence off the problem based on an aggregation property of ID features named Neural Collapse (NC). NC means that the penultimate features of ID samples within a class are nearly identical to the last layer weight of the corresponding class. Based on this property, we propose a simple but effective loss called OrthLoss, which binds the features of OOD data in a subspace orthogonal to the principal subspace of ID features formed by NC. In this way, the features of ID and OOD samples are separated by different dimensions. By optimizing the feature separation loss rather than purely enlarging output differences, our detection achieves SOTA performance on CIFAR benchmarks without any additional data augmentation or sampling, demonstrating the importance of feature separation in OOD detection. The code will be published.","sentences":["In the open world, detecting out-of-distribution (OOD) data, whose labels are disjoint with those of in-distribution (ID) samples, is important for reliable deep neural networks (DNNs).","To achieve better detection performance, one type of approach proposes to fine-tune the model with auxiliary OOD datasets to amplify the difference between ID and OOD data through a separation loss defined on model outputs.","However, none of these studies consider enlarging the feature disparity, which should be more effective compared to outputs.","The main difficulty lies in the diversity of OOD samples, which makes it hard to describe their feature distribution, let alone design losses to separate them from ID features.","In this paper, we neatly fence off the problem based on an aggregation property of ID features named Neural Collapse (NC).","NC means that the penultimate features of ID samples within a class are nearly identical to the last layer weight of the corresponding class.","Based on this property, we propose a simple but effective loss called OrthLoss, which binds the features of OOD data in a subspace orthogonal to the principal subspace of ID features formed by NC.","In this way, the features of ID and OOD samples are separated by different dimensions.","By optimizing the feature separation loss rather than purely enlarging output differences, our detection achieves SOTA performance on CIFAR benchmarks without any additional data augmentation or sampling, demonstrating the importance of feature separation in OOD detection.","The code will be published."],"url":"http://arxiv.org/abs/2405.17816v1","category":"cs.CV"}
{"created":"2024-05-28 04:16:43","title":"The Impacts of Data, Ordering, and Intrinsic Dimensionality on Recall in Hierarchical Navigable Small Worlds","abstract":"Vector search systems, pivotal in AI applications, often rely on the Hierarchical Navigable Small Worlds (HNSW) algorithm. However, the behaviour of HNSW under real-world scenarios using vectors generated with deep learning models remains under-explored. Existing Approximate Nearest Neighbours (ANN) benchmarks and research typically has an over-reliance on simplistic datasets like MNIST or SIFT1M and fail to reflect the complexity of current use-cases. Our investigation focuses on HNSW's efficacy across a spectrum of datasets, including synthetic vectors tailored to mimic specific intrinsic dimensionalities, widely-used retrieval benchmarks with popular embedding models, and proprietary e-commerce image data with CLIP models. We survey the most popular HNSW vector databases and collate their default parameters to provide a realistic fixed parameterisation for the duration of the paper.   We discover that the recall of approximate HNSW search, in comparison to exact K Nearest Neighbours (KNN) search, is linked to the vector space's intrinsic dimensionality and significantly influenced by the data insertion sequence. Our methodology highlights how insertion order, informed by measurable properties such as the pointwise Local Intrinsic Dimensionality (LID) or known categories, can shift recall by up to 12 percentage points. We also observe that running popular benchmark datasets with HNSW instead of KNN can shift rankings by up to three positions for some models. This work underscores the need for more nuanced benchmarks and design considerations in developing robust vector search systems using approximate vector search algorithms. This study presents a number of scenarios with varying real world applicability which aim to better increase understanding and future development of ANN algorithms and embedding","sentences":["Vector search systems, pivotal in AI applications, often rely on the Hierarchical Navigable Small Worlds (HNSW) algorithm.","However, the behaviour of HNSW under real-world scenarios using vectors generated with deep learning models remains under-explored.","Existing Approximate Nearest Neighbours (ANN) benchmarks and research typically has an over-reliance on simplistic datasets like MNIST or SIFT1M and fail to reflect the complexity of current use-cases.","Our investigation focuses on HNSW's efficacy across a spectrum of datasets, including synthetic vectors tailored to mimic specific intrinsic dimensionalities, widely-used retrieval benchmarks with popular embedding models, and proprietary e-commerce image data with CLIP models.","We survey the most popular HNSW vector databases and collate their default parameters to provide a realistic fixed parameterisation for the duration of the paper.   ","We discover that the recall of approximate HNSW search, in comparison to exact K Nearest Neighbours (KNN) search, is linked to the vector space's intrinsic dimensionality and significantly influenced by the data insertion sequence.","Our methodology highlights how insertion order, informed by measurable properties such as the pointwise Local Intrinsic Dimensionality (LID) or known categories, can shift recall by up to 12 percentage points.","We also observe that running popular benchmark datasets with HNSW instead of KNN can shift rankings by up to three positions for some models.","This work underscores the need for more nuanced benchmarks and design considerations in developing robust vector search systems using approximate vector search algorithms.","This study presents a number of scenarios with varying real world applicability which aim to better increase understanding and future development of ANN algorithms and embedding"],"url":"http://arxiv.org/abs/2405.17813v1","category":"cs.IR"}
{"created":"2024-05-28 04:12:00","title":"A new class of evolution multivalued quasi-variational inequalities I: existence and nonsmooth optimal control","abstract":"In this paper, we consider a new kind of evolution multivalued quasi-variational inequalities with feedback effect and a nonlinear bifunction which contain several (evolution) quasi-variational/hemivariational inequalities as special cases. The main contribution of this paper is twofold. The first goal is to establish a novel framework for proving the existence of solutions and the compactness of solution set to the evolution multivalued quasi-variational inequalities, under quite mild assumptions. Whereas, the second contribution is to introduce and study a nonlinear and nonsmooth optimal control problem governed by an evolution multivalued quasi-variational inequality, and then to obtain the sufficient conditions for guaranteeing the solvability of the nonlinear and nonsmooth optimal control problem under consideration. Such nonlinear and nonsmooth optimal control problem could as a useful model to explore the simultaneous distributed-boundary optimal control problems driven by evolution multivalued quasi-variational inequalities, and optimal parameters identification for evolution multivalued quasi-variational inequalities.","sentences":["In this paper, we consider a new kind of evolution multivalued quasi-variational inequalities with feedback effect and a nonlinear bifunction which contain several (evolution) quasi-variational/hemivariational inequalities as special cases.","The main contribution of this paper is twofold.","The first goal is to establish a novel framework for proving the existence of solutions and the compactness of solution set to the evolution multivalued quasi-variational inequalities, under quite mild assumptions.","Whereas, the second contribution is to introduce and study a nonlinear and nonsmooth optimal control problem governed by an evolution multivalued quasi-variational inequality, and then to obtain the sufficient conditions for guaranteeing the solvability of the nonlinear and nonsmooth optimal control problem under consideration.","Such nonlinear and nonsmooth optimal control problem could as a useful model to explore the simultaneous distributed-boundary optimal control problems driven by evolution multivalued quasi-variational inequalities, and optimal parameters identification for evolution multivalued quasi-variational inequalities."],"url":"http://arxiv.org/abs/2405.17810v1","category":"math.FA"}
{"created":"2024-05-28 03:53:10","title":"Bandwidth Efficient Cache Selection and Content Advertisement","abstract":"Caching is extensively used in various networking environments to optimize performance by reducing latency, bandwidth, and energy consumption. To optimize performance, caches often advertise their content using indicators, which are data structures that trade space efficiency for accuracy. However, this tradeoff introduces the risk of false indications. Existing solutions for cache content advertisement and cache selection often lead to inefficiencies, failing to adapt to dynamic network conditions. This paper introduces SALSA2, a Scalable Adaptive and Learning-based Selection and Advertisement Algorithm, which addresses these limitations through a dynamic and adaptive approach. SALSA2 accurately estimates mis-indication probabilities by considering inter-cache dependencies and dynamically adjusts the size and frequency of indicator advertisements to minimize transmission overhead while maintaining high accuracy. Our extensive simulation study, conducted using a variety of real-world cache traces, demonstrates that SALSA2 achieves up to 84\\% bandwidth savings compared to the state-of-the-art solution and close-to-optimal service cost in most scenarios. These results highlight SALSA2's effectiveness in enhancing cache management, making it a robust and versatile solution for modern networking challenges.","sentences":["Caching is extensively used in various networking environments to optimize performance by reducing latency, bandwidth, and energy consumption.","To optimize performance, caches often advertise their content using indicators, which are data structures that trade space efficiency for accuracy.","However, this tradeoff introduces the risk of false indications.","Existing solutions for cache content advertisement and cache selection often lead to inefficiencies, failing to adapt to dynamic network conditions.","This paper introduces SALSA2, a Scalable Adaptive and Learning-based Selection and Advertisement Algorithm, which addresses these limitations through a dynamic and adaptive approach.","SALSA2 accurately estimates mis-indication probabilities by considering inter-cache dependencies and dynamically adjusts the size and frequency of indicator advertisements to minimize transmission overhead while maintaining high accuracy.","Our extensive simulation study, conducted using a variety of real-world cache traces, demonstrates that SALSA2 achieves up to 84\\% bandwidth savings compared to the state-of-the-art solution and close-to-optimal service cost in most scenarios.","These results highlight SALSA2's effectiveness in enhancing cache management, making it a robust and versatile solution for modern networking challenges."],"url":"http://arxiv.org/abs/2405.17801v1","category":"cs.NI"}
{"created":"2024-05-28 03:47:41","title":"Offline Oracle-Efficient Learning for Contextual MDPs via Layerwise Exploration-Exploitation Tradeoff","abstract":"Motivated by the recent discovery of a statistical and computational reduction from contextual bandits to offline regression (Simchi-Levi and Xu, 2021), we address the general (stochastic) Contextual Markov Decision Process (CMDP) problem with horizon H (as known as CMDP with H layers). In this paper, we introduce a reduction from CMDPs to offline density estimation under the realizability assumption, i.e., a model class M containing the true underlying CMDP is provided in advance. We develop an efficient, statistically near-optimal algorithm requiring only O(HlogT) calls to an offline density estimation algorithm (or oracle) across all T rounds of interaction. This number can be further reduced to O(HloglogT) if T is known in advance. Our results mark the first efficient and near-optimal reduction from CMDPs to offline density estimation without imposing any structural assumptions on the model class. A notable feature of our algorithm is the design of a layerwise exploration-exploitation tradeoff tailored to address the layerwise structure of CMDPs. Additionally, our algorithm is versatile and applicable to pure exploration tasks in reward-free reinforcement learning.","sentences":["Motivated by the recent discovery of a statistical and computational reduction from contextual bandits to offline regression (Simchi-Levi and Xu, 2021), we address the general (stochastic) Contextual Markov Decision Process (CMDP) problem with horizon H (as known as CMDP with H layers).","In this paper, we introduce a reduction from CMDPs to offline density estimation under the realizability assumption, i.e., a model class M containing the true underlying CMDP is provided in advance.","We develop an efficient, statistically near-optimal algorithm requiring only O(HlogT) calls to an offline density estimation algorithm (or oracle) across all T rounds of interaction.","This number can be further reduced to O(HloglogT) if T is known in advance.","Our results mark the first efficient and near-optimal reduction from CMDPs to offline density estimation without imposing any structural assumptions on the model class.","A notable feature of our algorithm is the design of a layerwise exploration-exploitation tradeoff tailored to address the layerwise structure of CMDPs.","Additionally, our algorithm is versatile and applicable to pure exploration tasks in reward-free reinforcement learning."],"url":"http://arxiv.org/abs/2405.17796v1","category":"cs.LG"}
{"created":"2024-05-28 03:45:32","title":"LNS2+RL: Combining Multi-agent Reinforcement Learning with Large Neighborhood Search in Multi-agent Path Finding","abstract":"Multi-Agent Path Finding (MAPF) is a critical component of logistics and warehouse management, which focuses on planning collision-free paths for a team of robots in a known environment. Recent work introduced a novel MAPF approach, LNS2, which proposed to repair a quickly-obtainable set of infeasible paths via iterative re-planning, by relying on a fast, yet lower-quality, priority-based planner. At the same time, there has been a recent push for Multi-Agent Reinforcement Learning (MARL) based MAPF algorithms, which let agents learn decentralized policies that exhibit improved cooperation over such priority planning, although inevitably remaining slower. In this paper, we introduce a new MAPF algorithm, LNS2+RL, which combines the distinct yet complementary characteristics of LNS2 and MARL to effectively balance their individual limitations and get the best from both worlds. During early iterations, LNS2+RL relies on MARL for low-level re-planning, which we show eliminates collisions much more than a priority-based planner. There, our MARL-based planner allows agents to reason about past and future/predicted information to gradually learn cooperative decision-making through a finely designed curriculum learning. At later stages of planning, LNS2+RL adaptively switches to priority-based planning to quickly resolve the remaining collisions, naturally trading-off solution quality and computational efficiency. Our comprehensive experiments on challenging tasks across various team sizes, world sizes, and map structures consistently demonstrate the superior performance of LNS2+RL compared to many MAPF algorithms, including LNS2, LaCAM, and EECBS, where LNS2+RL shows significantly better performance in complex scenarios. We finally experimentally validate our algorithm in a hybrid simulation of a warehouse mockup involving a team of 100 (real-world and simulated) robots.","sentences":["Multi-Agent Path Finding (MAPF) is a critical component of logistics and warehouse management, which focuses on planning collision-free paths for a team of robots in a known environment.","Recent work introduced a novel MAPF approach, LNS2, which proposed to repair a quickly-obtainable set of infeasible paths via iterative re-planning, by relying on a fast, yet lower-quality, priority-based planner.","At the same time, there has been a recent push for Multi-Agent Reinforcement Learning (MARL) based MAPF algorithms, which let agents learn decentralized policies that exhibit improved cooperation over such priority planning, although inevitably remaining slower.","In this paper, we introduce a new MAPF algorithm, LNS2+RL, which combines the distinct yet complementary characteristics of LNS2 and MARL to effectively balance their individual limitations and get the best from both worlds.","During early iterations, LNS2+RL relies on MARL for low-level re-planning, which we show eliminates collisions much more than a priority-based planner.","There, our MARL-based planner allows agents to reason about past and future/predicted information to gradually learn cooperative decision-making through a finely designed curriculum learning.","At later stages of planning, LNS2+RL adaptively switches to priority-based planning to quickly resolve the remaining collisions, naturally trading-off solution quality and computational efficiency.","Our comprehensive experiments on challenging tasks across various team sizes, world sizes, and map structures consistently demonstrate the superior performance of LNS2+RL compared to many MAPF algorithms, including LNS2, LaCAM, and EECBS, where LNS2+RL shows significantly better performance in complex scenarios.","We finally experimentally validate our algorithm in a hybrid simulation of a warehouse mockup involving a team of 100 (real-world and simulated) robots."],"url":"http://arxiv.org/abs/2405.17794v1","category":"cs.RO"}
{"created":"2024-05-28 03:41:36","title":"SafeguardGS: 3D Gaussian Primitive Pruning While Avoiding Catastrophic Scene Destruction","abstract":"3D Gaussian Splatting (3DGS) has made a significant stride in novel view synthesis, demonstrating top-notch rendering quality while achieving real-time rendering speed. However, the excessively large number of Gaussian primitives resulting from 3DGS' suboptimal densification process poses a major challenge, slowing down frame-per-second (FPS) and demanding considerable memory cost, making it unfavorable for low-end devices. To cope with this issue, many follow-up studies have suggested various pruning techniques, often in combination with different score functions, to optimize rendering performance. Nonetheless, a comprehensive discussion regarding their effectiveness and implications across all techniques is missing. In this paper, we first categorize 3DGS pruning techniques into two types: Cross-view pruning and pixel-wise pruning, which differ in their approaches to rank primitives. Our subsequent experiments reveal that while cross-view pruning leads to disastrous quality drops under extreme Gaussian primitives decimation, the pixel-wise pruning technique not only sustains relatively high rendering quality with minuscule performance degradation but also provides a reasonable minimum boundary for pruning. Building on this observation, we further propose multiple variations of score functions and empirically discover that the color-weighted score function outperforms others for discriminating insignificant primitives for rendering. We believe our research provides valuable insights for optimizing 3DGS pruning strategies for future works.","sentences":["3D Gaussian Splatting (3DGS) has made a significant stride in novel view synthesis, demonstrating top-notch rendering quality while achieving real-time rendering speed.","However, the excessively large number of Gaussian primitives resulting from 3DGS' suboptimal densification process poses a major challenge, slowing down frame-per-second (FPS) and demanding considerable memory cost, making it unfavorable for low-end devices.","To cope with this issue, many follow-up studies have suggested various pruning techniques, often in combination with different score functions, to optimize rendering performance.","Nonetheless, a comprehensive discussion regarding their effectiveness and implications across all techniques is missing.","In this paper, we first categorize 3DGS pruning techniques into two types: Cross-view pruning and pixel-wise pruning, which differ in their approaches to rank primitives.","Our subsequent experiments reveal that while cross-view pruning leads to disastrous quality drops under extreme Gaussian primitives decimation, the pixel-wise pruning technique not only sustains relatively high rendering quality with minuscule performance degradation but also provides a reasonable minimum boundary for pruning.","Building on this observation, we further propose multiple variations of score functions and empirically discover that the color-weighted score function outperforms others for discriminating insignificant primitives for rendering.","We believe our research provides valuable insights for optimizing 3DGS pruning strategies for future works."],"url":"http://arxiv.org/abs/2405.17793v1","category":"cs.CV"}
{"created":"2024-05-28 03:34:55","title":"Enhancing Road Safety: Real-Time Detection of Driver Distraction through Convolutional Neural Networks","abstract":"As we navigate our daily commutes, the threat posed by a distracted driver is at a large, resulting in a troubling rise in traffic accidents. Addressing this safety concern, our project harnesses the analytical power of Convolutional Neural Networks (CNNs), with a particular emphasis on the well-established models VGG16 and VGG19. These models are acclaimed for their precision in image recognition and are meticulously tested for their ability to detect nuances in driver behavior under varying environmental conditions. Through a comparative analysis against an array of CNN architectures, this study seeks to identify the most efficient model for real-time detection of driver distractions. The ultimate aim is to incorporate the findings into vehicle safety systems, significantly boosting their capability to prevent accidents triggered by inattention. This research not only enhances our understanding of automotive safety technologies but also marks a pivotal step towards creating vehicles that are intuitively aligned with driver behaviors, ensuring safer roads for all.","sentences":["As we navigate our daily commutes, the threat posed by a distracted driver is at a large, resulting in a troubling rise in traffic accidents.","Addressing this safety concern, our project harnesses the analytical power of Convolutional Neural Networks (CNNs), with a particular emphasis on the well-established models VGG16 and VGG19.","These models are acclaimed for their precision in image recognition and are meticulously tested for their ability to detect nuances in driver behavior under varying environmental conditions.","Through a comparative analysis against an array of CNN architectures, this study seeks to identify the most efficient model for real-time detection of driver distractions.","The ultimate aim is to incorporate the findings into vehicle safety systems, significantly boosting their capability to prevent accidents triggered by inattention.","This research not only enhances our understanding of automotive safety technologies but also marks a pivotal step towards creating vehicles that are intuitively aligned with driver behaviors, ensuring safer roads for all."],"url":"http://arxiv.org/abs/2405.17788v1","category":"cs.CV"}
{"created":"2024-05-28 03:32:54","title":"Effect of insulator end cap thickness on time-dependent Hartmann flow in a rotating mirror","abstract":"We present a framework for analyzing plasma flow in a rotating mirror. By making a series of physical assumptions, we reduce the magnetohydrodynamic (MHD) equations in a three-dimensional cylindrical system to a one-dimensional system in a shallow, cuboidal channel within a transverse magnetic field, similar to the Hartmann flow in the ducts. We then solve the system both numerically and analytically for a range of values of the Hartmann number and calculate the dependence of the plasma flow speed on the thickness of the insulating end cap. We observe that the mean flow overshoots and decelerates before achieving a steady-state value, a phenomenon that the analytical model cannot capture. This overshoot is directly proportional to the thickness of the insulating end cap and the external electric field, with a weak dependence on the external magnetic field. Our simplified model can act as a benchmark for future simulations of the supersonic mirror device Compact Magnetic Fusion Experiment (CMFX), which will employ more sophisticated physics and realistic magnetic field geometries.","sentences":["We present a framework for analyzing plasma flow in a rotating mirror.","By making a series of physical assumptions, we reduce the magnetohydrodynamic (MHD) equations in a three-dimensional cylindrical system to a one-dimensional system in a shallow, cuboidal channel within a transverse magnetic field, similar to the Hartmann flow in the ducts.","We then solve the system both numerically and analytically for a range of values of the Hartmann number and calculate the dependence of the plasma flow speed on the thickness of the insulating end cap.","We observe that the mean flow overshoots and decelerates before achieving a steady-state value, a phenomenon that the analytical model cannot capture.","This overshoot is directly proportional to the thickness of the insulating end cap and the external electric field, with a weak dependence on the external magnetic field.","Our simplified model can act as a benchmark for future simulations of the supersonic mirror device Compact Magnetic Fusion Experiment (CMFX), which will employ more sophisticated physics and realistic magnetic field geometries."],"url":"http://arxiv.org/abs/2405.17786v1","category":"physics.plasm-ph"}
{"created":"2024-05-28 03:26:00","title":"Post-Fair Federated Learning: Achieving Group and Community Fairness in Federated Learning via Post-processing","abstract":"Federated Learning (FL) is a distributed machine learning framework in which a set of local communities collaboratively learn a shared global model while retaining all training data locally within each community. Two notions of fairness have recently emerged as important issues for federated learning: group fairness and community fairness. Group fairness requires that a model's decisions do not favor any particular group based on a set of legally protected attributes such as race or gender. Community fairness requires that global models exhibit similar levels of performance (accuracy) across all collaborating communities. Both fairness concepts can coexist within an FL framework, but the existing literature has focused on either one concept or the other. This paper proposes and analyzes a post-processing fair federated learning (FFL) framework called post-FFL. Post-FFL uses a linear program to simultaneously enforce group and community fairness while maximizing the utility of the global model. Because Post-FFL is a post-processing approach, it can be used with existing FL training pipelines whose convergence properties are well understood. This paper uses post-FFL on real-world datasets to mimic how hospital networks, for example, use federated learning to deliver community health care. Theoretical results bound the accuracy lost when post-FFL enforces both notion of fairness. Experimental results illustrate that post-FFL simultaneously improves both group and community fairness in FL. Moreover, post-FFL outperforms the existing in-processing fair federated learning in terms of improving both notions of fairness, communication efficiency and computation cost.","sentences":["Federated Learning (FL) is a distributed machine learning framework in which a set of local communities collaboratively learn a shared global model while retaining all training data locally within each community.","Two notions of fairness have recently emerged as important issues for federated learning: group fairness and community fairness.","Group fairness requires that a model's decisions do not favor any particular group based on a set of legally protected attributes such as race or gender.","Community fairness requires that global models exhibit similar levels of performance (accuracy) across all collaborating communities.","Both fairness concepts can coexist within an FL framework, but the existing literature has focused on either one concept or the other.","This paper proposes and analyzes a post-processing fair federated learning (FFL) framework called post-FFL.","Post-FFL uses a linear program to simultaneously enforce group and community fairness while maximizing the utility of the global model.","Because Post-FFL is a post-processing approach, it can be used with existing FL training pipelines whose convergence properties are well understood.","This paper uses post-FFL on real-world datasets to mimic how hospital networks, for example, use federated learning to deliver community health care.","Theoretical results bound the accuracy lost when post-FFL enforces both notion of fairness.","Experimental results illustrate that post-FFL simultaneously improves both group and community fairness in FL.","Moreover, post-FFL outperforms the existing in-processing fair federated learning in terms of improving both notions of fairness, communication efficiency and computation cost."],"url":"http://arxiv.org/abs/2405.17782v1","category":"cs.LG"}
{"created":"2024-05-28 03:25:31","title":"Dissipation-induced bound states as a two-level system","abstract":"Potential wells are employed to constrain quantum particles into forming discrete energy levels, acting as artificial few-level systems. In contrast, an anti-parity-time ($\\mathcal{PT}$) symmetric system can have a single pair of real energy levels, while all the remaining levels are unstable due to the negative imaginary part of the energy. In this work, we investigate the formation of bound states in a tight-binding chain induced by a harmonic imaginary potential. Exact solutions show that the real parts of energy levels are equidistant, while the imaginary parts are semi-negative definite and equidistant. This allows for the formation of an effective two-level system. For a given initial state with a wide range of profiles, the evolved state always converges to a superposition of two stable eigenstates. In addition, these two states are orthogonal under the Dirac inner product and can be mutually switched by applying a $\\pi$ pulse of a linear field. Our finding provides an alternative method for fabricating quantum devices through dissipation.","sentences":["Potential wells are employed to constrain quantum particles into forming discrete energy levels, acting as artificial few-level systems.","In contrast, an anti-parity-time ($\\mathcal{PT}$) symmetric system can have a single pair of real energy levels, while all the remaining levels are unstable due to the negative imaginary part of the energy.","In this work, we investigate the formation of bound states in a tight-binding chain induced by a harmonic imaginary potential.","Exact solutions show that the real parts of energy levels are equidistant, while the imaginary parts are semi-negative definite and equidistant.","This allows for the formation of an effective two-level system.","For a given initial state with a wide range of profiles, the evolved state always converges to a superposition of two stable eigenstates.","In addition, these two states are orthogonal under the Dirac inner product and can be mutually switched by applying a $\\pi$ pulse of a linear field.","Our finding provides an alternative method for fabricating quantum devices through dissipation."],"url":"http://arxiv.org/abs/2405.17781v1","category":"quant-ph"}
{"created":"2024-05-28 03:20:05","title":"Unmasking Vulnerabilities: Cardinality Sketches under Adaptive Inputs","abstract":"Cardinality sketches are popular data structures that enhance the efficiency of working with large data sets. The sketches are randomized representations of sets that are only of logarithmic size but can support set merges and approximate cardinality (i.e., distinct count) queries. When queries are not adaptive, that is, they do not depend on preceding query responses, the design provides strong guarantees of correctly answering a number of queries exponential in the sketch size $k$.   In this work, we investigate the performance of cardinality sketches in adaptive settings and unveil inherent vulnerabilities. We design an attack against the ``standard'' estimators that constructs an adversarial input by post-processing responses to a set of simple non-adaptive queries of size linear in the sketch size $k$. Empirically, our attack used only $4k$ queries with the widely used HyperLogLog (HLL++)~\\citep{hyperloglog:2007,hyperloglogpractice:EDBT2013} sketch. The simple attack technique suggests it can be effective with post-processed natural workloads. Finally and importantly, we demonstrate that the vulnerability is inherent as \\emph{any} estimator applied to known sketch structures can be attacked using a number of queries that is quadratic in $k$, matching a generic upper bound.","sentences":["Cardinality sketches are popular data structures that enhance the efficiency of working with large data sets.","The sketches are randomized representations of sets that are only of logarithmic size but can support set merges and approximate cardinality (i.e., distinct count) queries.","When queries are not adaptive, that is, they do not depend on preceding query responses, the design provides strong guarantees of correctly answering a number of queries exponential in the sketch size $k$.   In this work, we investigate the performance of cardinality sketches in adaptive settings and unveil inherent vulnerabilities.","We design an attack against the ``standard'' estimators that constructs an adversarial input by post-processing responses to a set of simple non-adaptive queries of size linear in the sketch size $k$. Empirically, our attack used only $4k$ queries with the widely used HyperLogLog (HLL++)~\\citep{hyperloglog:2007,hyperloglogpractice:EDBT2013} sketch.","The simple attack technique suggests it can be effective with post-processed natural workloads.","Finally and importantly, we demonstrate that the vulnerability is inherent as \\emph{any} estimator applied to known sketch structures can be attacked using a number of queries that is quadratic in $k$, matching a generic upper bound."],"url":"http://arxiv.org/abs/2405.17780v1","category":"cs.DS"}
{"created":"2024-05-28 02:46:11","title":"Linguistic Collapse: Neural Collapse in (Large) Language Models","abstract":"Neural collapse ($\\mathcal{NC}$) is a phenomenon observed in classification tasks where top-layer representations collapse into their class means, which become equinorm, equiangular and aligned with the classifiers. These behaviors -- associated with generalization and robustness -- would manifest under specific conditions: models are trained towards zero loss, with noise-free labels belonging to balanced classes, which do not outnumber the model's hidden dimension. Recent studies have explored $\\mathcal{NC}$ in the absence of one or more of these conditions to extend and capitalize on the associated benefits of ideal geometries. Language modeling presents a curious frontier, as \\textit{training by token prediction} constitutes a classification task where none of the conditions exist: the vocabulary is imbalanced and exceeds the embedding dimension; different tokens might correspond to similar contextual embeddings; and large language models (LLMs) in particular are typically only trained for a few epochs. This paper empirically investigates the impact of scaling the architectures and training of causal language models (CLMs) on their progression towards $\\mathcal{NC}$. We find that $\\mathcal{NC}$ properties that develop with scaling are linked to generalization. Moreover, there is evidence of some relationship between $\\mathcal{NC}$ and generalization independent of scale. Our work therefore underscores the generality of $\\mathcal{NC}$ as it extends to the novel and more challenging setting of language modeling. Downstream, we seek to inspire further research on the phenomenon to deepen our understanding of LLMs -- and neural networks at large -- and improve existing architectures based on $\\mathcal{NC}$-related properties.","sentences":["Neural collapse ($\\mathcal{NC}$) is a phenomenon observed in classification tasks where top-layer representations collapse into their class means, which become equinorm, equiangular and aligned with the classifiers.","These behaviors -- associated with generalization and robustness -- would manifest under specific conditions: models are trained towards zero loss, with noise-free labels belonging to balanced classes, which do not outnumber the model's hidden dimension.","Recent studies have explored $\\mathcal{NC}$ in the absence of one or more of these conditions to extend and capitalize on the associated benefits of ideal geometries.","Language modeling presents a curious frontier, as \\textit{training by token prediction} constitutes a classification task where none of the conditions exist: the vocabulary is imbalanced and exceeds the embedding dimension; different tokens might correspond to similar contextual embeddings; and large language models (LLMs) in particular are typically only trained for a few epochs.","This paper empirically investigates the impact of scaling the architectures and training of causal language models (CLMs) on their progression towards $\\mathcal{NC}$. We find that $\\mathcal{NC}$ properties that develop with scaling are linked to generalization.","Moreover, there is evidence of some relationship between $\\mathcal{NC}$ and generalization independent of scale.","Our work therefore underscores the generality of $\\mathcal{NC}$ as it extends to the novel and more challenging setting of language modeling.","Downstream, we seek to inspire further research on the phenomenon to deepen our understanding of LLMs -- and neural networks at large -- and improve existing architectures based on $\\mathcal{NC}$-related properties."],"url":"http://arxiv.org/abs/2405.17767v1","category":"cs.LG"}
{"created":"2024-05-28 02:29:12","title":"Tuition too high? Blame competition","abstract":"We develop a feedback theory that includes reinforcing and balancing feedback effects that emerge when colleges compete for reputation, applicants, and tuition revenue. The feedback theory is replicated in a formal duopoly model consisting of two competing colleges. An independent ranking entity determines the relative order of the colleges. College applicants choose between the two colleges based on the rankings and the financial aid offered by the colleges. Contrary to the conventional wisdom that competition lowers prices and benefits consumers, our simulations show that competition between academic institutions for resources and reputation leads to tuition escalation that negatively affects students and their families. Four of the five scenarios -- rankings, a capital campaign, facilities improvements, and an excellence campaign -- increase college tuition, institutional debt, and expenditures per student; only the scenario of ignoring the rankings decreases these measures. By referring to the feedback structure of academic competition, the article makes several recommendations for controlling tuition inflation. This article contributes to the literature on the economics of higher education and illustrates the value of feedback economics in developing economic theory.","sentences":["We develop a feedback theory that includes reinforcing and balancing feedback effects that emerge when colleges compete for reputation, applicants, and tuition revenue.","The feedback theory is replicated in a formal duopoly model consisting of two competing colleges.","An independent ranking entity determines the relative order of the colleges.","College applicants choose between the two colleges based on the rankings and the financial aid offered by the colleges.","Contrary to the conventional wisdom that competition lowers prices and benefits consumers, our simulations show that competition between academic institutions for resources and reputation leads to tuition escalation that negatively affects students and their families.","Four of the five scenarios -- rankings, a capital campaign, facilities improvements, and an excellence campaign -- increase college tuition, institutional debt, and expenditures per student; only the scenario of ignoring the rankings decreases these measures.","By referring to the feedback structure of academic competition, the article makes several recommendations for controlling tuition inflation.","This article contributes to the literature on the economics of higher education and illustrates the value of feedback economics in developing economic theory."],"url":"http://arxiv.org/abs/2405.17762v1","category":"econ.GN"}
{"created":"2024-05-28 02:23:48","title":"Wireless Federated Learning over Resource-Constrained Networks: Digital versus Analog Transmissions","abstract":"To enable wireless federated learning (FL) in communication resource-constrained networks, two communication schemes, i.e., digital and analog ones, are effective solutions. In this paper, we quantitatively compare these two techniques, highlighting their essential differences as well as respectively suitable scenarios. We first examine both digital and analog transmission schemes, together with a unified and fair comparison framework under imbalanced device sampling, strict latency targets, and transmit power constraints. A universal convergence analysis under various imperfections is established for evaluating the performance of FL over wireless networks. These analytical results reveal that the fundamental difference between the digital and analog communications lies in whether communication and computation are jointly designed or not. The digital scheme decouples the communication design from FL computing tasks, making it difficult to support uplink transmission from massive devices with limited bandwidth and hence the performance is mainly communication-limited. In contrast, the analog communication allows over-the-air computation (AirComp) and achieves better spectrum utilization. However, the computation-oriented analog transmission reduces power efficiency, and its performance is sensitive to computation errors from imperfect channel state information (CSI). Furthermore, device sampling for both schemes are optimized and differences in sampling optimization are analyzed. Numerical results verify the theoretical analysis and affirm the superior performance of the sampling optimization.","sentences":["To enable wireless federated learning (FL) in communication resource-constrained networks, two communication schemes, i.e., digital and analog ones, are effective solutions.","In this paper, we quantitatively compare these two techniques, highlighting their essential differences as well as respectively suitable scenarios.","We first examine both digital and analog transmission schemes, together with a unified and fair comparison framework under imbalanced device sampling, strict latency targets, and transmit power constraints.","A universal convergence analysis under various imperfections is established for evaluating the performance of FL over wireless networks.","These analytical results reveal that the fundamental difference between the digital and analog communications lies in whether communication and computation are jointly designed or not.","The digital scheme decouples the communication design from FL computing tasks, making it difficult to support uplink transmission from massive devices with limited bandwidth and hence the performance is mainly communication-limited.","In contrast, the analog communication allows over-the-air computation (AirComp) and achieves better spectrum utilization.","However, the computation-oriented analog transmission reduces power efficiency, and its performance is sensitive to computation errors from imperfect channel state information (CSI).","Furthermore, device sampling for both schemes are optimized and differences in sampling optimization are analyzed.","Numerical results verify the theoretical analysis and affirm the superior performance of the sampling optimization."],"url":"http://arxiv.org/abs/2405.17759v1","category":"cs.IT"}
{"created":"2024-05-28 02:23:18","title":"Charge transport through the multiple end zigzag edge states of armchair graphene nanoribbons and heterojunctions","abstract":"This comprehensive study investigates charge transport through the multiple end zigzag edge states of finite-size armchair graphene nanoribbons/boron nitride nanoribbons (n-AGNR/w-BNNR) junctions under a longitudinal electric field, where n and w denote the widths of the AGNRs and the BNNRs, respectively. In 13-atom wide AGNR segments, the edge states exhibit a blue Stark shift in response to the electric field, with only the long decay length zigzag edge states showing significant interaction with the red Stark shift subband states. Charge tunneling through such edge states assisted by the subband states is elucidated in the spectra of the transmission coefficient. In the 13-AGNR/6-BNNR heterojunction, notable influences on the energy levels of the end zigzag edge states of 13-AGNRs induced by BNNR segments are observed. We demonstrate the modulation of these energy levels in resonant tunneling situations, as depicted by bias-dependent transmission coefficient spectra. Intriguing nonthermal broadening of tunneling current shows a significant peak-to-valley ratio. Our findings highlight the promising potential of n-AGNR/w-BNNR heterojunctions with long decay length edge states in the realm of GNR-based single electron transistors at room temperature.","sentences":["This comprehensive study investigates charge transport through the multiple end zigzag edge states of finite-size armchair graphene nanoribbons/boron nitride nanoribbons (n-AGNR/w-BNNR) junctions under a longitudinal electric field, where n and w denote the widths of the AGNRs and the BNNRs, respectively.","In 13-atom wide AGNR segments, the edge states exhibit a blue Stark shift in response to the electric field, with only the long decay length zigzag edge states showing significant interaction with the red Stark shift subband states.","Charge tunneling through such edge states assisted by the subband states is elucidated in the spectra of the transmission coefficient.","In the 13-AGNR/6-BNNR heterojunction, notable influences on the energy levels of the end zigzag edge states of 13-AGNRs induced by BNNR segments are observed.","We demonstrate the modulation of these energy levels in resonant tunneling situations, as depicted by bias-dependent transmission coefficient spectra.","Intriguing nonthermal broadening of tunneling current shows a significant peak-to-valley ratio.","Our findings highlight the promising potential of n-AGNR/w-BNNR heterojunctions with long decay length edge states in the realm of GNR-based single electron transistors at room temperature."],"url":"http://arxiv.org/abs/2405.17758v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-28 02:16:35","title":"Motion-Informed Deep Learning for Brain MR Image Reconstruction Framework","abstract":"Motion artifacts in Magnetic Resonance Imaging (MRI) are one of the frequently occurring artifacts due to patient movements during scanning. Motion is estimated to be present in approximately 30% of clinical MRI scans; however, motion has not been explicitly modeled within deep learning image reconstruction models. Deep learning (DL) algorithms have been demonstrated to be effective for both the image reconstruction task and the motion correction task, but the two tasks are considered separately. The image reconstruction task involves removing undersampling artifacts such as noise and aliasing artifacts, whereas motion correction involves removing artifacts including blurring, ghosting, and ringing. In this work, we propose a novel method to simultaneously accelerate imaging and correct motion. This is achieved by integrating a motion module into the deep learning-based MRI reconstruction process, enabling real-time detection and correction of motion. We model motion as a tightly integrated auxiliary layer in the deep learning model during training, making the deep learning model 'motion-informed'. During inference, image reconstruction is performed from undersampled raw k-space data using a trained motion-informed DL model. Experimental results demonstrate that the proposed motion-informed deep learning image reconstruction network outperformed the conventional image reconstruction network for motion-degraded MRI datasets.","sentences":["Motion artifacts in Magnetic Resonance Imaging (MRI) are one of the frequently occurring artifacts due to patient movements during scanning.","Motion is estimated to be present in approximately 30% of clinical MRI scans; however, motion has not been explicitly modeled within deep learning image reconstruction models.","Deep learning (DL) algorithms have been demonstrated to be effective for both the image reconstruction task and the motion correction task, but the two tasks are considered separately.","The image reconstruction task involves removing undersampling artifacts such as noise and aliasing artifacts, whereas motion correction involves removing artifacts including blurring, ghosting, and ringing.","In this work, we propose a novel method to simultaneously accelerate imaging and correct motion.","This is achieved by integrating a motion module into the deep learning-based MRI reconstruction process, enabling real-time detection and correction of motion.","We model motion as a tightly integrated auxiliary layer in the deep learning model during training, making the deep learning model 'motion-informed'.","During inference, image reconstruction is performed from undersampled raw k-space data using a trained motion-informed DL model.","Experimental results demonstrate that the proposed motion-informed deep learning image reconstruction network outperformed the conventional image reconstruction network for motion-degraded MRI datasets."],"url":"http://arxiv.org/abs/2405.17756v1","category":"eess.IV"}
{"created":"2024-05-28 02:12:16","title":"Differential Voltage Analysis and Patterns in Parallel-Connected Pairs of Imbalanced Cells","abstract":"Diagnosing imbalances in capacity and resistance within parallel-connected cells in battery packs is critical for battery management and fault detection, but it is challenging given that individual currents flowing into each cell are often unmeasured. This work introduces a novel method useful for identifying imbalances in capacity and resistance within a pair of parallel-connected cells using only voltage and current measurements from the pair. Our method utilizes differential voltage analysis (DVA) when the pair is under constant current discharge and demonstrates that features of the pair's differential voltage curve (dV/dQ), namely its mid-to-high SOC dV/dQ peak's height and skewness, are sensitive to imbalances in capacity and resistance. We analyze and explain how and why these dV/dQ peak shape features change in response to these imbalances, highlighting that the underlying current imbalance dynamics resulting from these imbalances contribute to these changes. Ultimately, we demonstrate that dV/dQ peak shape features can identify the product of capacity imbalance and resistance imbalance, but cannot uniquely identify the imbalances. This work lays the groundwork for identifying imbalances in capacity and resistance in parallel-connected cell groups in battery packs, where commonly only a single current sensor is placed for each parallel cell group.","sentences":["Diagnosing imbalances in capacity and resistance within parallel-connected cells in battery packs is critical for battery management and fault detection, but it is challenging given that individual currents flowing into each cell are often unmeasured.","This work introduces a novel method useful for identifying imbalances in capacity and resistance within a pair of parallel-connected cells using only voltage and current measurements from the pair.","Our method utilizes differential voltage analysis (DVA) when the pair is under constant current discharge and demonstrates that features of the pair's differential voltage curve (dV/dQ), namely its mid-to-high SOC dV/dQ peak's height and skewness, are sensitive to imbalances in capacity and resistance.","We analyze and explain how and why these dV/dQ peak shape features change in response to these imbalances, highlighting that the underlying current imbalance dynamics resulting from these imbalances contribute to these changes.","Ultimately, we demonstrate that dV/dQ peak shape features can identify the product of capacity imbalance and resistance imbalance, but cannot uniquely identify the imbalances.","This work lays the groundwork for identifying imbalances in capacity and resistance in parallel-connected cell groups in battery packs, where commonly only a single current sensor is placed for each parallel cell group."],"url":"http://arxiv.org/abs/2405.17754v1","category":"eess.SY"}
{"created":"2024-05-28 02:11:21","title":"Regression Equilibrium in Electricity Markets","abstract":"Renewable power producers participating in electricity markets build forecasting models independently, relying on their own data, model and feature preferences. In this paper, we argue that in renewable-dominated markets, such an uncoordinated approach to forecasting results in substantial opportunity costs for stochastic producers and additional operating costs for the power system. As a solution, we introduce Regression Equilibrium--a welfare-optimal state of electricity markets under uncertainty, where profit-seeking stochastic producers do not benefit by unilaterally deviating from their equilibrium forecast models. While the regression equilibrium maximizes the private welfare, i.e., the average profit of stochastic producers across the day-ahead and real-time markets, it also aligns with the socially optimal, least-cost dispatch solution for the system. We base the equilibrium analysis on the theory of variational inequalities, providing results on the existence and uniqueness of regression equilibrium in energy-only markets. We also devise two methods for computing the regression equilibrium: centralized optimization and a decentralized ADMM-based algorithm that preserves the privacy of regression datasets.","sentences":["Renewable power producers participating in electricity markets build forecasting models independently, relying on their own data, model and feature preferences.","In this paper, we argue that in renewable-dominated markets, such an uncoordinated approach to forecasting results in substantial opportunity costs for stochastic producers and additional operating costs for the power system.","As a solution, we introduce Regression Equilibrium--a welfare-optimal state of electricity markets under uncertainty, where profit-seeking stochastic producers do not benefit by unilaterally deviating from their equilibrium forecast models.","While the regression equilibrium maximizes the private welfare, i.e., the average profit of stochastic producers across the day-ahead and real-time markets, it also aligns with the socially optimal, least-cost dispatch solution for the system.","We base the equilibrium analysis on the theory of variational inequalities, providing results on the existence and uniqueness of regression equilibrium in energy-only markets.","We also devise two methods for computing the regression equilibrium: centralized optimization and a decentralized ADMM-based algorithm that preserves the privacy of regression datasets."],"url":"http://arxiv.org/abs/2405.17753v1","category":"eess.SY"}
{"created":"2024-05-28 01:48:05","title":"The HTTP Garden: Discovering Parsing Vulnerabilities in HTTP/1.1 Implementations by Differential Fuzzing of Request Streams","abstract":"HTTP/1.1 parsing discrepancies have been the basis for numerous classes of attacks against web servers. Previous techniques for discovering HTTP parsing discrepancies have focused on blackbox differential testing of HTTP gateway servers, despite evidence that the most significant parsing anomalies occur within origin servers. While these techniques can detect some vulnerabilities, not all parsing discrepancy-related vulnerabilities are detectable by examining a gateway server's output alone. Our system, the HTTP Garden, examines both origin servers' interpretations and gateway servers' transformations of HTTP requests. It also includes a coverage-guided differential fuzzer for HTTP/1.1 origin servers that is capable of mutating all components of a request stream, paired with an interactive REPL that facilitates the automatic discovery of meaningful HTTP parsing discrepancies and the rapid development of those discrepancies into attack payloads. Using our tool, we have discovered and reported over 100 HTTP parsing bugs in popular web servers, of which 68 have been fixed following our reports. We designate 39 of these to be exploitable. We release the HTTP Garden to the public on GitHub under a free software license to allow researchers to further explore new parser discrepancy-based attacks against HTTP/1.1 servers.","sentences":["HTTP/1.1 parsing discrepancies have been the basis for numerous classes of attacks against web servers.","Previous techniques for discovering HTTP parsing discrepancies have focused on blackbox differential testing of HTTP gateway servers, despite evidence that the most significant parsing anomalies occur within origin servers.","While these techniques can detect some vulnerabilities, not all parsing discrepancy-related vulnerabilities are detectable by examining a gateway server's output alone.","Our system, the HTTP Garden, examines both origin servers' interpretations and gateway servers' transformations of HTTP requests.","It also includes a coverage-guided differential fuzzer for HTTP/1.1 origin servers that is capable of mutating all components of a request stream, paired with an interactive REPL that facilitates the automatic discovery of meaningful HTTP parsing discrepancies and the rapid development of those discrepancies into attack payloads.","Using our tool, we have discovered and reported over 100 HTTP parsing bugs in popular web servers, of which 68 have been fixed following our reports.","We designate 39 of these to be exploitable.","We release the HTTP Garden to the public on GitHub under a free software license to allow researchers to further explore new parser discrepancy-based attacks against HTTP/1.1 servers."],"url":"http://arxiv.org/abs/2405.17737v1","category":"cs.CR"}
{"created":"2024-05-28 00:45:35","title":"Color Shift Estimation-and-Correction for Image Enhancement","abstract":"Images captured under sub-optimal illumination conditions may contain both over- and under-exposures. Current approaches mainly focus on adjusting image brightness, which may exacerbate the color tone distortion in under-exposed areas and fail to restore accurate colors in over-exposed regions. We observe that under-exposed and over-exposed regions display opposite color tone distribution shifts with respect to each other, which may not be easily normalized in joint modeling as they usually do not have ``normal-exposed'' regions/pixels as reference. In this paper, we propose a novel method to enhance images with both over- and under-exposures by learning to estimate and correct such color shifts. Specifically, we first derive the color feature maps of the brightened and darkened versions of the input image via a UNet-based network, followed by a pseudo-normal feature generator to produce pseudo-normal color feature maps. We then propose a novel COlor Shift Estimation (COSE) module to estimate the color shifts between the derived brightened (or darkened) color feature maps and the pseudo-normal color feature maps. The COSE module corrects the estimated color shifts of the over- and under-exposed regions separately. We further propose a novel COlor MOdulation (COMO) module to modulate the separately corrected colors in the over- and under-exposed regions to produce the enhanced image. Comprehensive experiments show that our method outperforms existing approaches. Project webpage: https://github.com/yiyulics/CSEC.","sentences":["Images captured under sub-optimal illumination conditions may contain both over- and under-exposures.","Current approaches mainly focus on adjusting image brightness, which may exacerbate the color tone distortion in under-exposed areas and fail to restore accurate colors in over-exposed regions.","We observe that under-exposed and over-exposed regions display opposite color tone distribution shifts with respect to each other, which may not be easily normalized in joint modeling as they usually do not have ``normal-exposed'' regions/pixels as reference.","In this paper, we propose a novel method to enhance images with both over- and under-exposures by learning to estimate and correct such color shifts.","Specifically, we first derive the color feature maps of the brightened and darkened versions of the input image via a UNet-based network, followed by a pseudo-normal feature generator to produce pseudo-normal color feature maps.","We then propose a novel COlor Shift Estimation (COSE) module to estimate the color shifts between the derived brightened (or darkened) color feature maps and the pseudo-normal color feature maps.","The COSE module corrects the estimated color shifts of the over- and under-exposed regions separately.","We further propose a novel COlor MOdulation (COMO) module to modulate the separately corrected colors in the over- and under-exposed regions to produce the enhanced image.","Comprehensive experiments show that our method outperforms existing approaches.","Project webpage: https://github.com/yiyulics/CSEC."],"url":"http://arxiv.org/abs/2405.17725v1","category":"cs.CV"}
{"created":"2024-05-27 23:16:52","title":"Learning Social Welfare Functions","abstract":"Is it possible to understand or imitate a policy maker's rationale by looking at past decisions they made? We formalize this question as the problem of learning social welfare functions belonging to the well-studied family of power mean functions. We focus on two learning tasks; in the first, the input is vectors of utilities of an action (decision or policy) for individuals in a group and their associated social welfare as judged by a policy maker, whereas in the second, the input is pairwise comparisons between the welfares associated with a given pair of utility vectors. We show that power mean functions are learnable with polynomial sample complexity in both cases, even if the comparisons are social welfare information is noisy. Finally, we design practical algorithms for these tasks and evaluate their performance.","sentences":["Is it possible to understand or imitate a policy maker's rationale by looking at past decisions they made?","We formalize this question as the problem of learning social welfare functions belonging to the well-studied family of power mean functions.","We focus on two learning tasks; in the first, the input is vectors of utilities of an action (decision or policy) for individuals in a group and their associated social welfare as judged by a policy maker, whereas in the second, the input is pairwise comparisons between the welfares associated with a given pair of utility vectors.","We show that power mean functions are learnable with polynomial sample complexity in both cases, even if the comparisons are social welfare information is noisy.","Finally, we design practical algorithms for these tasks and evaluate their performance."],"url":"http://arxiv.org/abs/2405.17700v1","category":"cs.GT"}
{"created":"2024-05-27 23:00:40","title":"Tamed Langevin sampling under weaker conditions","abstract":"Motivated by applications to deep learning which often fail standard Lipschitz smoothness requirements, we examine the problem of sampling from distributions that are not log-concave and are only weakly dissipative, with log-gradients allowed to grow superlinearly at infinity. In terms of structure, we only assume that the target distribution satisfies either a log-Sobolev or a Poincar\\'e inequality and a local Lipschitz smoothness assumption with modulus growing possibly polynomially at infinity. This set of assumptions greatly exceeds the operational limits of the \"vanilla\" unadjusted Langevin algorithm (ULA), making sampling from such distributions a highly involved affair. To account for this, we introduce a taming scheme which is tailored to the growth and decay properties of the target distribution, and we provide explicit non-asymptotic guarantees for the proposed sampler in terms of the Kullback-Leibler (KL) divergence, total variation, and Wasserstein distance to the target distribution.","sentences":["Motivated by applications to deep learning which often fail standard Lipschitz smoothness requirements, we examine the problem of sampling from distributions that are not log-concave and are only weakly dissipative, with log-gradients allowed to grow superlinearly at infinity.","In terms of structure, we only assume that the target distribution satisfies either a log-Sobolev or a Poincar\\'e inequality and a local Lipschitz smoothness assumption with modulus growing possibly polynomially at infinity.","This set of assumptions greatly exceeds the operational limits of the \"vanilla\" unadjusted Langevin algorithm (ULA), making sampling from such distributions a highly involved affair.","To account for this, we introduce a taming scheme which is tailored to the growth and decay properties of the target distribution, and we provide explicit non-asymptotic guarantees for the proposed sampler in terms of the Kullback-Leibler (KL) divergence, total variation, and Wasserstein distance to the target distribution."],"url":"http://arxiv.org/abs/2405.17693v1","category":"stat.ML"}
{"created":"2024-05-27 22:46:17","title":"Data Makes Better Data Scientists","abstract":"With the goal of identifying common practices in data science projects, this paper proposes a framework for logging and understanding incremental code executions in Jupyter notebooks. This framework aims to allow reasoning about how insights are generated in data science and extract key observations into best data science practices in the wild. In this paper, we show an early prototype of this framework and ran an experiment to log a machine learning project for 25 undergraduate students.","sentences":["With the goal of identifying common practices in data science projects, this paper proposes a framework for logging and understanding incremental code executions in Jupyter notebooks.","This framework aims to allow reasoning about how insights are generated in data science and extract key observations into best data science practices in the wild.","In this paper, we show an early prototype of this framework and ran an experiment to log a machine learning project for 25 undergraduate students."],"url":"http://arxiv.org/abs/2405.17690v1","category":"cs.HC"}
{"created":"2024-05-27 22:03:05","title":"$J/\u03c8$ photoproduction: threshold to very high energy","abstract":"A reaction model for $\\gamma + p \\to J/\\psi + p$ photoproduction, which exposes the $c \\bar c$ content of the photon in making the transition $\\gamma\\to c\\bar c + \\mathbb P \\to J/\\psi$ and couples the intermediate $c \\bar c$ system to the proton's valence quarks via Pomeron ($\\mathbb P $) exchange, is used to deliver a description of available data, viz. both differential and total cross sections from near threshold, where data has newly been acquired, to invariant mass $W \\approx 300\\,$GeV. The study suggests that it is premature to link existing $\\gamma + p \\to J/\\psi + p$ data with, for instance, in-proton gluon distributions, the quantum chromodynamics trace anomaly, or pentaquark production. Further developments in reaction theory and higher precision data are necessary before the validity of any such connections can be assessed.","sentences":["A reaction model for $\\gamma + p \\to J/\\psi + p$ photoproduction, which exposes the $c \\bar c$ content of the photon in making the transition $\\gamma\\to c\\bar c","+ \\mathbb P \\to J/\\psi$ and couples the intermediate $c \\bar c$ system to the proton's valence quarks via Pomeron ($\\mathbb P $) exchange, is used to deliver a description of available data, viz.","both differential and total cross sections from near threshold, where data has newly been acquired, to invariant mass $W \\approx 300\\,$GeV.","The study suggests that it is premature to link existing $\\gamma + p \\to J/\\psi + p$ data with, for instance, in-proton gluon distributions, the quantum chromodynamics trace anomaly, or pentaquark production.","Further developments in reaction theory and higher precision data are necessary before the validity of any such connections can be assessed."],"url":"http://arxiv.org/abs/2405.17675v1","category":"hep-ph"}
{"created":"2024-05-27 21:47:41","title":"Bayesian Nonparametrics for Principal Stratification with Continuous Post-Treatment Variables","abstract":"Principal stratification provides a causal inference framework that allows adjustment for confounded post-treatment variables when comparing treatments. Although the literature has focused mainly on binary post-treatment variables, there is a growing interest in principal stratification involving continuous post-treatment variables. However, characterizing the latent principal strata with a continuous post-treatment presents a significant challenge, which is further complicated in observational studies where the treatment is not randomized. In this paper, we introduce the Confounders-Aware SHared atoms BAyesian mixture (CASBAH), a novel approach for principal stratification with continuous post-treatment variables that can be directly applied to observational studies. CASBAH leverages a dependent Dirichlet process, utilizing shared atoms across treatment levels, to effectively control for measured confounders and facilitate information sharing between treatment groups in the identification of principal strata membership. CASBAH also offers a comprehensive quantification of uncertainty surrounding the membership of the principal strata. Through Monte Carlo simulations, we show that the proposed methodology has excellent performance in characterizing the latent principal strata and estimating the effects of treatment on post-treatment variables and outcomes. Finally, CASBAH is applied to a case study in which we estimate the causal effects of US national air quality regulations on pollution levels and health outcomes.","sentences":["Principal stratification provides a causal inference framework that allows adjustment for confounded post-treatment variables when comparing treatments.","Although the literature has focused mainly on binary post-treatment variables, there is a growing interest in principal stratification involving continuous post-treatment variables.","However, characterizing the latent principal strata with a continuous post-treatment presents a significant challenge, which is further complicated in observational studies where the treatment is not randomized.","In this paper, we introduce the Confounders-Aware SHared atoms BAyesian mixture (CASBAH), a novel approach for principal stratification with continuous post-treatment variables that can be directly applied to observational studies.","CASBAH leverages a dependent Dirichlet process, utilizing shared atoms across treatment levels, to effectively control for measured confounders and facilitate information sharing between treatment groups in the identification of principal strata membership.","CASBAH also offers a comprehensive quantification of uncertainty surrounding the membership of the principal strata.","Through Monte Carlo simulations, we show that the proposed methodology has excellent performance in characterizing the latent principal strata and estimating the effects of treatment on post-treatment variables and outcomes.","Finally, CASBAH is applied to a case study in which we estimate the causal effects of US national air quality regulations on pollution levels and health outcomes."],"url":"http://arxiv.org/abs/2405.17669v1","category":"stat.ME"}
{"created":"2024-05-27 21:44:14","title":"Hunting for Polluted White Dwarfs and Other Treasures with Gaia XP Spectra and Unsupervised Machine Learning","abstract":"White dwarfs (WDs) polluted by exoplanetary material provide the unprecedented opportunity to directly observe the interiors of exoplanets. However, spectroscopic surveys are often limited by brightness constraints, and WDs tend to be very faint, making detections of large populations of polluted WDs difficult. In this paper, we aim to increase considerably the number of WDs with multiple metals in their atmospheres. Using 96,134 WDs with Gaia DR3 BP/RP (XP) spectra, we constructed a 2D map using an unsupervised machine learning technique called Uniform Manifold Approximation and Projection (UMAP) to organize the WDs into identifiable spectral regions. The polluted WDs are among the distinct spectral groups identified in our map. We have shown that this selection method could potentially increase the number of known WDs with 5 or more metal species in their atmospheres by an order of magnitude. Such systems are essential for characterizing exoplanet diversity and geology.","sentences":["White dwarfs (WDs) polluted by exoplanetary material provide the unprecedented opportunity to directly observe the interiors of exoplanets.","However, spectroscopic surveys are often limited by brightness constraints, and WDs tend to be very faint, making detections of large populations of polluted WDs difficult.","In this paper, we aim to increase considerably the number of WDs with multiple metals in their atmospheres.","Using 96,134 WDs with Gaia DR3 BP/RP (XP) spectra, we constructed a 2D map using an unsupervised machine learning technique called Uniform Manifold Approximation and Projection (UMAP) to organize the WDs into identifiable spectral regions.","The polluted WDs are among the distinct spectral groups identified in our map.","We have shown that this selection method could potentially increase the number of known WDs with 5 or more metal species in their atmospheres by an order of magnitude.","Such systems are essential for characterizing exoplanet diversity and geology."],"url":"http://arxiv.org/abs/2405.17667v1","category":"astro-ph.SR"}
{"created":"2024-05-27 21:40:31","title":"Structured Partial Stochasticity in Bayesian Neural Networks","abstract":"Bayesian neural network posterior distributions have a great number of modes that correspond to the same network function. The abundance of such modes can make it difficult for approximate inference methods to do their job. Recent work has demonstrated the benefits of partial stochasticity for approximate inference in Bayesian neural networks; inference can be less costly and performance can sometimes be improved. I propose a structured way to select the deterministic subset of weights that removes neuron permutation symmetries, and therefore the corresponding redundant posterior modes. With a drastically simplified posterior distribution, the performance of existing approximate inference schemes is found to be greatly improved.","sentences":["Bayesian neural network posterior distributions have a great number of modes that correspond to the same network function.","The abundance of such modes can make it difficult for approximate inference methods to do their job.","Recent work has demonstrated the benefits of partial stochasticity for approximate inference in Bayesian neural networks; inference can be less costly and performance can sometimes be improved.","I propose a structured way to select the deterministic subset of weights that removes neuron permutation symmetries, and therefore the corresponding redundant posterior modes.","With a drastically simplified posterior distribution, the performance of existing approximate inference schemes is found to be greatly improved."],"url":"http://arxiv.org/abs/2405.17666v1","category":"stat.ML"}
{"created":"2024-05-27 21:33:56","title":"Enhanced Robot Arm at the Edge with NLP and Vision Systems","abstract":"This paper introduces a \"proof of concept\" for a new approach to assistive robotics, integrating edge computing with Natural Language Processing (NLP) and computer vision to enhance the interaction between humans and robotic systems. Our \"proof of concept\" demonstrates the feasibility of using large language models (LLMs) and vision systems in tandem for interpreting and executing complex commands conveyed through natural language. This integration aims to improve the intuitiveness and accessibility of assistive robotic systems, making them more adaptable to the nuanced needs of users with disabilities. By leveraging the capabilities of edge computing, our system has the potential to minimize latency and support offline capability, enhancing the autonomy and responsiveness of assistive robots. Experimental results from our implementation on a robotic arm show promising outcomes in terms of accurate intent interpretation and object manipulation based on verbal commands. This research lays the groundwork for future developments in assistive robotics, focusing on creating highly responsive, user-centric systems that can significantly improve the quality of life for individuals with disabilities.","sentences":["This paper introduces a \"proof of concept\" for a new approach to assistive robotics, integrating edge computing with Natural Language Processing (NLP) and computer vision to enhance the interaction between humans and robotic systems.","Our \"proof of concept\" demonstrates the feasibility of using large language models (LLMs) and vision systems in tandem for interpreting and executing complex commands conveyed through natural language.","This integration aims to improve the intuitiveness and accessibility of assistive robotic systems, making them more adaptable to the nuanced needs of users with disabilities.","By leveraging the capabilities of edge computing, our system has the potential to minimize latency and support offline capability, enhancing the autonomy and responsiveness of assistive robots.","Experimental results from our implementation on a robotic arm show promising outcomes in terms of accurate intent interpretation and object manipulation based on verbal commands.","This research lays the groundwork for future developments in assistive robotics, focusing on creating highly responsive, user-centric systems that can significantly improve the quality of life for individuals with disabilities."],"url":"http://arxiv.org/abs/2405.17665v1","category":"cs.RO"}
{"created":"2024-05-27 21:23:20","title":"RefDrop: Controllable Consistency in Image or Video Generation via Reference Feature Guidance","abstract":"There is a rapidly growing interest in controlling consistency across multiple generated images using diffusion models. Among various methods, recent works have found that simply manipulating attention modules by concatenating features from multiple reference images provides an efficient approach to enhancing consistency without fine-tuning. Despite its popularity and success, few studies have elucidated the underlying mechanisms that contribute to its effectiveness. In this work, we reveal that the popular approach is a linear interpolation of image self-attention and cross-attention between synthesized content and reference features, with a constant rank-1 coefficient. Motivated by this observation, we find that a rank-1 coefficient is not necessary and simplifies the controllable generation mechanism. The resulting algorithm, which we coin as RefDrop, allows users to control the influence of reference context in a direct and precise manner. Besides further enhancing consistency in single-subject image generation, our method also enables more interesting applications, such as the consistent generation of multiple subjects, suppressing specific features to encourage more diverse content, and high-quality personalized video generation by boosting temporal consistency. Even compared with state-of-the-art image-prompt-based generators, such as IP-Adapter, RefDrop is competitive in terms of controllability and quality while avoiding the need to train a separate image encoder for feature injection from reference images, making it a versatile plug-and-play solution for any image or video diffusion model.","sentences":["There is a rapidly growing interest in controlling consistency across multiple generated images using diffusion models.","Among various methods, recent works have found that simply manipulating attention modules by concatenating features from multiple reference images provides an efficient approach to enhancing consistency without fine-tuning.","Despite its popularity and success, few studies have elucidated the underlying mechanisms that contribute to its effectiveness.","In this work, we reveal that the popular approach is a linear interpolation of image self-attention and cross-attention between synthesized content and reference features, with a constant rank-1 coefficient.","Motivated by this observation, we find that a rank-1 coefficient is not necessary and simplifies the controllable generation mechanism.","The resulting algorithm, which we coin as RefDrop, allows users to control the influence of reference context in a direct and precise manner.","Besides further enhancing consistency in single-subject image generation, our method also enables more interesting applications, such as the consistent generation of multiple subjects, suppressing specific features to encourage more diverse content, and high-quality personalized video generation by boosting temporal consistency.","Even compared with state-of-the-art image-prompt-based generators, such as IP-Adapter, RefDrop is competitive in terms of controllability and quality while avoiding the need to train a separate image encoder for feature injection from reference images, making it a versatile plug-and-play solution for any image or video diffusion model."],"url":"http://arxiv.org/abs/2405.17661v1","category":"cs.CV"}
{"created":"2024-05-27 21:04:43","title":"Enhancing Global Sensitivity and Uncertainty Quantification in Medical Image Reconstruction with Monte Carlo Arbitrary-Masked Mamba","abstract":"Deep learning has been extensively applied in medical image reconstruction, where Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) represent the predominant paradigms, each possessing distinct advantages and inherent limitations: CNNs exhibit linear complexity with local sensitivity, whereas ViTs demonstrate quadratic complexity with global sensitivity. The emerging Mamba has shown superiority in learning visual representation, which combines the advantages of linear scalability and global sensitivity. In this study, we introduce MambaMIR, an Arbitrary-Masked Mamba-based model with wavelet decomposition for joint medical image reconstruction and uncertainty estimation. A novel Arbitrary Scan Masking (ASM) mechanism ``masks out'' redundant information to introduce randomness for further uncertainty estimation. Compared to the commonly used Monte Carlo (MC) dropout, our proposed MC-ASM provides an uncertainty map without the need for hyperparameter tuning and mitigates the performance drop typically observed when applying dropout to low-level tasks. For further texture preservation and better perceptual quality, we employ the wavelet transformation into MambaMIR and explore its variant based on the Generative Adversarial Network, namely MambaMIR-GAN. Comprehensive experiments have been conducted for multiple representative medical image reconstruction tasks, demonstrating that the proposed MambaMIR and MambaMIR-GAN outperform other baseline and state-of-the-art methods in different reconstruction tasks, where MambaMIR achieves the best reconstruction fidelity and MambaMIR-GAN has the best perceptual quality. In addition, our MC-ASM provides uncertainty maps as an additional tool for clinicians, while mitigating the typical performance drop caused by the commonly used dropout.","sentences":["Deep learning has been extensively applied in medical image reconstruction, where Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) represent the predominant paradigms, each possessing distinct advantages and inherent limitations: CNNs exhibit linear complexity with local sensitivity, whereas ViTs demonstrate quadratic complexity with global sensitivity.","The emerging Mamba has shown superiority in learning visual representation, which combines the advantages of linear scalability and global sensitivity.","In this study, we introduce MambaMIR, an Arbitrary-Masked Mamba-based model with wavelet decomposition for joint medical image reconstruction and uncertainty estimation.","A novel Arbitrary Scan Masking (ASM) mechanism ``masks out'' redundant information to introduce randomness for further uncertainty estimation.","Compared to the commonly used Monte Carlo (MC) dropout, our proposed MC-ASM provides an uncertainty map without the need for hyperparameter tuning and mitigates the performance drop typically observed when applying dropout to low-level tasks.","For further texture preservation and better perceptual quality, we employ the wavelet transformation into MambaMIR and explore its variant based on the Generative Adversarial Network, namely MambaMIR-GAN.","Comprehensive experiments have been conducted for multiple representative medical image reconstruction tasks, demonstrating that the proposed MambaMIR and MambaMIR-GAN outperform other baseline and state-of-the-art methods in different reconstruction tasks, where MambaMIR achieves the best reconstruction fidelity and MambaMIR-GAN has the best perceptual quality.","In addition, our MC-ASM provides uncertainty maps as an additional tool for clinicians, while mitigating the typical performance drop caused by the commonly used dropout."],"url":"http://arxiv.org/abs/2405.17659v1","category":"eess.IV"}
{"created":"2024-05-27 20:55:10","title":"Active flow control for drag reduction through multi-agent reinforcement learning on a turbulent cylinder at $Re_D=3900$","abstract":"This study presents novel active-flow-control (AFC) strategies aimed at achieving drag reduction for a three-dimensional cylinder immersed in a flow at a Reynolds number based on freestream velocity and cylinder diameter of (Re_D=3900). The cylinder in this subcritical flow regime has been extensively studied in the literature and is considered a classic case of turbulent flow arising from a bluff body. The strategies presented are explored through the use of deep reinforcement learning. The cylinder is equipped with 10 independent zero-net-mass-flux jet pairs, distributed on the top and bottom surfaces, which define the AFC setup. The method is based on the coupling between a computational-fluid-dynamics solver and a multi-agent reinforcement-learning (MARL) framework using the proximal-policy-optimization algorithm. Thanks to the acceleration in training facilitated by exploiting the local invariants with MARL, a drag reduction of (8\\%) was achieved, with a mass cost efficiency two orders of magnitude lower than those of the existing classical controls in the literature. This development represents a significant advancement in active flow control, particularly in turbulent regimes critical to industrial applications.","sentences":["This study presents novel active-flow-control (AFC) strategies aimed at achieving drag reduction for a three-dimensional cylinder immersed in a flow at a Reynolds number based on freestream velocity and cylinder diameter of (Re_D=3900).","The cylinder in this subcritical flow regime has been extensively studied in the literature and is considered a classic case of turbulent flow arising from a bluff body.","The strategies presented are explored through the use of deep reinforcement learning.","The cylinder is equipped with 10 independent zero-net-mass-flux jet pairs, distributed on the top and bottom surfaces, which define the AFC setup.","The method is based on the coupling between a computational-fluid-dynamics solver and a multi-agent reinforcement-learning (MARL) framework using the proximal-policy-optimization algorithm.","Thanks to the acceleration in training facilitated by exploiting the local invariants with MARL, a drag reduction of (8\\%) was achieved, with a mass cost efficiency two orders of magnitude lower than those of the existing classical controls in the literature.","This development represents a significant advancement in active flow control, particularly in turbulent regimes critical to industrial applications."],"url":"http://arxiv.org/abs/2405.17655v1","category":"physics.flu-dyn"}
{"created":"2024-05-27 20:45:14","title":"The $e$-positivity of the chromatic symmetric function for twinned paths and cycles","abstract":"The operation of twinning a graph at a vertex was introduced by Foley, Ho\\`ang, and Merkel (2019), who conjectured that twinning preserves $e$-positivity of the chromatic symmetric function. A counterexample to this conjecture was given by Li, Li, Wang, and Yang (2021). In this paper, we prove that $e$-positivity is preserved by the twinning operation on cycles, by giving an $e$-positive generating function for the chromatic symmetric function, as well as an $e$-positive recurrence. We derive similar $e$-positive generating functions and recurrences for twins of paths. Our methods make use of the important triple deletion formulas of Orellana and Scott (2014), as well as new symmetric function identities.","sentences":["The operation of twinning a graph at a vertex was introduced by Foley, Ho\\`ang, and Merkel (2019), who conjectured that twinning preserves $e$-positivity of the chromatic symmetric function.","A counterexample to this conjecture was given by Li, Li, Wang, and Yang (2021).","In this paper, we prove that $e$-positivity is preserved by the twinning operation on cycles, by giving an $e$-positive generating function for the chromatic symmetric function, as well as an $e$-positive recurrence.","We derive similar $e$-positive generating functions and recurrences for twins of paths.","Our methods make use of the important triple deletion formulas of Orellana and Scott (2014), as well as new symmetric function identities."],"url":"http://arxiv.org/abs/2405.17649v1","category":"math.CO"}
{"created":"2024-05-27 20:36:14","title":"A model-independent treatment of cosmic ladder calibration and $\u03a9_k$ measurement through low-$z$ observations","abstract":"Looking at the well-known Hubble tension as a tension in the calibrators of the cosmic distance ladder, i.e. the absolute magnitude $M$ of standard candles such as supernovae of Type Ia (SNIa) and the standard ruler represented by the comoving sound horizon at the baryon-drag epoch, $r_d$, we propose a model-independent method to measure these distance calibrators independently from the cosmic microwave background and the first rungs of the direct distance ladder. To do so, we leverage state-of-the-art data on cosmic chronometers (CCH), SNIa and baryon acoustic oscillations (BAO) from various galaxy surveys. Taking advantage of the Gaussian Processes Bayesian technique, we reconstruct $M(z)$, $\\Omega_k(z)$ and $r_d(z)$ at $z\\lesssim2$ and check that no significant statistical evolution is preferred at 68\\% C.L. This allows us to treat them as constants and constrain them assuming the metric description of gravity, the cosmological principle and the validity of CCH as reliable cosmic clocks, and SNIa and BAO as optimal standard candles and standard rulers, respectively, but otherwise in a model-independent way. We obtain: $\\Omega_k=-0.07^{+0.12}_{-0.15}$, $M=(-19.314^{+0.086}_{-0.108})$ mag and $r_d=(142.3\\pm 5.3)$ Mpc. At present, the uncertainties derived are still too large to arbitrate the tension but this is bound to change in the near future with the advent of upcoming surveys and data.","sentences":["Looking at the well-known Hubble tension as a tension in the calibrators of the cosmic distance ladder, i.e. the absolute magnitude $M$ of standard candles such as supernovae of Type Ia (SNIa) and the standard ruler represented by the comoving sound horizon at the baryon-drag epoch, $r_d$, we propose a model-independent method to measure these distance calibrators independently from the cosmic microwave background and the first rungs of the direct distance ladder.","To do so, we leverage state-of-the-art data on cosmic chronometers (CCH), SNIa and baryon acoustic oscillations (BAO) from various galaxy surveys.","Taking advantage of the Gaussian Processes Bayesian technique, we reconstruct $M(z)$, $\\Omega_k(z)$ and $r_d(z)$ at $z\\lesssim2$ and check that no significant statistical evolution is preferred at 68\\% C.L.","This allows us to treat them as constants and constrain them assuming the metric description of gravity, the cosmological principle and the validity of CCH as reliable cosmic clocks, and SNIa and BAO as optimal standard candles and standard rulers, respectively, but otherwise in a model-independent way.","We obtain: $\\Omega_k=-0.07^{+0.12}_{-0.15}$, $M=(-19.314^{+0.086}_{-0.108})$ mag and $r_d=(142.3\\pm 5.3)$ Mpc.","At present, the uncertainties derived are still too large to arbitrate the tension but this is bound to change in the near future with the advent of upcoming surveys and data."],"url":"http://arxiv.org/abs/2405.17643v1","category":"astro-ph.CO"}
{"created":"2024-05-27 20:03:07","title":"On the Growth of the Extremal and Cluster Level Sets in Branching Brownian Motion","abstract":"We study the limiting extremal and cluster point processes of branching Brownian motion. The former records the heights of all extreme values of the process, while the latter records the relative heights of extreme values in a genealogical neighborhood of order unity around a local maximum thereof. For the extremal point process, we show that the mass of upper level sets $[-v, \\infty)$ grows as $C_\\star Z v e^{\\sqrt{2} v}(1+o(1))$ as $v \\to \\infty$, almost surely, where $Z$ is the limit of the associated derivative martingale and $C_\\star \\in (0, \\infty)$ is a universal constant. For the cluster point process, we show that the logarithm of the mass of $[-v, \\infty)$ grow as $\\sqrt{2}v$ minus random fluctuations of order $v^{2/3}$, which are governed by an explicit law in the limit. The first result improves upon the works of Cortines et al. (arXiv:1703.06529) and Mytnik et al. (arXiv:2009.02042) in which asymptotics are shown in probability, while the second makes rigorous the derivation in the physics literature by Mueller et al. (arXiv:1910.06382) and Le et al. (arXiv:2207.07672) and resolves a conjecture thereof.","sentences":["We study the limiting extremal and cluster point processes of branching Brownian motion.","The former records the heights of all extreme values of the process, while the latter records the relative heights of extreme values in a genealogical neighborhood of order unity around a local maximum thereof.","For the extremal point process, we show that the mass of upper level sets $[-v, \\infty)$ grows as $C_\\star Z v e^{\\sqrt{2} v}(1+o(1))$ as $v \\to \\infty$, almost surely, where $Z$ is the limit of the associated derivative martingale and $C_\\star \\in (0, \\infty)$ is a universal constant.","For the cluster point process, we show that the logarithm of the mass of $[-v, \\infty)$ grow as $\\sqrt{2}v$ minus random fluctuations of order $v^{2/3}$, which are governed by an explicit law in the limit.","The first result improves upon the works of Cortines et al.","(arXiv:1703.06529) and Mytnik et al. (arXiv:2009.02042) in which asymptotics are shown in probability, while the second makes rigorous the derivation in the physics literature by Mueller et al.","(arXiv:1910.06382) and Le et al.","(arXiv:2207.07672) and resolves a conjecture thereof."],"url":"http://arxiv.org/abs/2405.17634v1","category":"math.PR"}
{"created":"2024-05-27 19:52:37","title":"HERMES: Gamma Ray Burst and Gravitational Wave counterpart hunter","abstract":"Gamma Ray Bursts (GRBs) bridge relativistic astrophysics and multi-messenger astronomy. Space-based gamma/X-ray wide field detectors have proven essential to detect and localize the highly variable GRB prompt emission, which is also a counterpart of gravitational wave events. We study the capabilities to detect long and short GRBs by the High Energy Rapid Modular Ensemble of Satellites (HERMES) Pathfinder (HP) and SpIRIT, namely a swarm of six 3U CubeSats to be launched in early 2025, and a 6U CubeSat launched on December 1st 2023. We also study the capabilities of two advanced configurations of swarms of >8 satellites with improved detector performances (HERMES Constellations). The HERMES detectors, sensitive down to ~2-3 keV, will be able to detect faint/soft GRBs which comprise X-ray flashes and high redshift bursts. By combining state-of-the-art long and short GRB population models with a description of the single module performance, we estimate that HP will detect ~195^{+22}_{-21} long GRBs (3.4^{+0.3}_{-0.8} at redshift z>6) and ~19^{+5}_{-3} short GRBs per year. The larger HERMES Constellations under study can detect between ~1300 and ~3000 long GRBs per year and between ~160 and ~400 short GRBs per year, depending on the chosen configuration, with a rate of long GRBs above z>6 between 30 and 75 per year. Finally, we explore the capabilities of HERMES to detect short GRBs as electromagnetic counterparts of binary neutron star (BNS) mergers detected as gravitational signals by current and future ground-based interferometers. Under the assumption that the GRB jets are structured, we estimate that HP can provide up to 1 (14) yr^{-1} joint detections during the fifth LIGO-Virgo-KAGRA observing run (Einstein Telescope single triangle 10 km arm configuration). These numbers become 4 (100) yr^{-1}, respectively, for the HERMES Constellation configuration.","sentences":["Gamma Ray Bursts (GRBs) bridge relativistic astrophysics and multi-messenger astronomy.","Space-based gamma/X-ray wide field detectors have proven essential to detect and localize the highly variable GRB prompt emission, which is also a counterpart of gravitational wave events.","We study the capabilities to detect long and short GRBs by the High Energy Rapid Modular Ensemble of Satellites (HERMES) Pathfinder (HP) and SpIRIT, namely a swarm of six 3U CubeSats to be launched in early 2025, and a 6U CubeSat launched on December 1st 2023.","We also study the capabilities of two advanced configurations of swarms of >8 satellites with improved detector performances (HERMES Constellations).","The HERMES detectors, sensitive down to ~2-3 keV, will be able to detect faint/soft GRBs which comprise X-ray flashes and high redshift bursts.","By combining state-of-the-art long and short GRB population models with a description of the single module performance, we estimate that HP will detect ~195^{+22}_{-21} long GRBs (3.4^{+0.3}_{-0.8} at redshift z>6) and ~19^{+5}_{-3} short GRBs per year.","The larger HERMES Constellations under study can detect between ~1300 and ~3000 long GRBs per year and between ~160 and ~400 short GRBs per year, depending on the chosen configuration, with a rate of long GRBs above z>6 between 30 and 75 per year.","Finally, we explore the capabilities of HERMES to detect short GRBs as electromagnetic counterparts of binary neutron star (BNS) mergers detected as gravitational signals by current and future ground-based interferometers.","Under the assumption that the GRB jets are structured, we estimate that HP can provide up to 1 (14) yr^{-1} joint detections during the fifth LIGO-Virgo-KAGRA observing run (Einstein Telescope single triangle 10 km arm configuration).","These numbers become 4 (100) yr^{-1}, respectively, for the HERMES Constellation configuration."],"url":"http://arxiv.org/abs/2405.17630v1","category":"astro-ph.HE"}
{"created":"2024-05-27 19:28:55","title":"The significance of fuzzy boundaries of the barrier regions in single-molecule measurements of failed barrier crossing attempts","abstract":"A recent experimental study reports on measuring the temporal duration and the spatial extent of failed attempts to cross an activation barrier (i.e., \"loops\") for a folding transition in a single molecule and for a Brownian particle trapped within a bistable potential. Within the model of diffusive dynamics, however, both of these quantities are, on the average, exactly zero because of the recrossings of the barrier region boundary. That is, an observer endowed with infinite spatial and temporal resolution would find that finite loops do not exist (or, more precisely, form a set of measure zero). Here we develop a description of the experiment that takes finite experimental resolution into account and show how the experimental uncertainty of localizing the point, in time and space, where the barrier is crossed leads to observable distributions of loop times and sizes. Although these distributions generally depend on the experimental resolution, this dependence, in certain cases, may amount to a simple resolution-dependent factor and thus the experiments do probe inherent properties of barrier crossing dynamics.","sentences":["A recent experimental study reports on measuring the temporal duration and the spatial extent of failed attempts to cross an activation barrier (i.e., \"loops\") for a folding transition in a single molecule and for a Brownian particle trapped within a bistable potential.","Within the model of diffusive dynamics, however, both of these quantities are, on the average, exactly zero because of the recrossings of the barrier region boundary.","That is, an observer endowed with infinite spatial and temporal resolution would find that finite loops do not exist (or, more precisely, form a set of measure zero).","Here we develop a description of the experiment that takes finite experimental resolution into account and show how the experimental uncertainty of localizing the point, in time and space, where the barrier is crossed leads to observable distributions of loop times and sizes.","Although these distributions generally depend on the experimental resolution, this dependence, in certain cases, may amount to a simple resolution-dependent factor and thus the experiments do probe inherent properties of barrier crossing dynamics."],"url":"http://arxiv.org/abs/2405.17620v1","category":"physics.chem-ph"}
{"created":"2024-05-28 17:18:17","title":"Empowering Source-Free Domain Adaptation with MLLM-driven Curriculum Learning","abstract":"Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to a target domain using only unlabeled target data. Current SFDA methods face challenges in effectively leveraging pre-trained knowledge and exploiting target domain data. Multimodal Large Language Models (MLLMs) offer remarkable capabilities in understanding visual and textual information, but their applicability to SFDA poses challenges such as instruction-following failures, intensive computational demands, and difficulties in performance measurement prior to adaptation. To alleviate these issues, we propose Reliability-based Curriculum Learning (RCL), a novel framework that integrates multiple MLLMs for knowledge exploitation via pseudo-labeling in SFDA. Our framework incorporates proposed Reliable Knowledge Transfer, Self-correcting and MLLM-guided Knowledge Expansion, and Multi-hot Masking Refinement to progressively exploit unlabeled data in the target domain. RCL achieves state-of-the-art (SOTA) performance on multiple SFDA benchmarks, e.g., $\\textbf{+9.4%}$ on DomainNet, demonstrating its effectiveness in enhancing adaptability and robustness without requiring access to source data. Code: https://github.com/Dong-Jie-Chen/RCL.","sentences":["Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to a target domain using only unlabeled target data.","Current SFDA methods face challenges in effectively leveraging pre-trained knowledge and exploiting target domain data.","Multimodal Large Language Models (MLLMs) offer remarkable capabilities in understanding visual and textual information, but their applicability to SFDA poses challenges such as instruction-following failures, intensive computational demands, and difficulties in performance measurement prior to adaptation.","To alleviate these issues, we propose Reliability-based Curriculum Learning (RCL), a novel framework that integrates multiple MLLMs for knowledge exploitation via pseudo-labeling in SFDA.","Our framework incorporates proposed Reliable Knowledge Transfer, Self-correcting and MLLM-guided Knowledge Expansion, and Multi-hot Masking Refinement to progressively exploit unlabeled data in the target domain.","RCL achieves state-of-the-art (SOTA) performance on multiple SFDA benchmarks, e.g., $\\textbf{+9.4%}$ on DomainNet, demonstrating its effectiveness in enhancing adaptability and robustness without requiring access to source data.","Code: https://github.com/Dong-Jie-Chen/RCL."],"url":"http://arxiv.org/abs/2405.18376v1","category":"cs.LG"}
{"created":"2024-05-28 16:52:52","title":"Simulating infinite-dimensional nonlinear diffusion bridges","abstract":"The diffusion bridge is a type of diffusion process that conditions on hitting a specific state within a finite time period. It has broad applications in fields such as Bayesian inference, financial mathematics, control theory, and shape analysis. However, simulating the diffusion bridge for natural data can be challenging due to both the intractability of the drift term and continuous representations of the data. Although several methods are available to simulate finite-dimensional diffusion bridges, infinite-dimensional cases remain unresolved. In the paper, we present a solution to this problem by merging score-matching techniques with operator learning, enabling a direct approach to score-matching for the infinite-dimensional bridge. We construct the score to be discretization invariant, which is natural given the underlying spatially continuous process. We conduct a series of experiments, ranging from synthetic examples with closed-form solutions to the stochastic nonlinear evolution of real-world biological shape data, and our method demonstrates high efficacy, particularly due to its ability to adapt to any resolution without extra training.","sentences":["The diffusion bridge is a type of diffusion process that conditions on hitting a specific state within a finite time period.","It has broad applications in fields such as Bayesian inference, financial mathematics, control theory, and shape analysis.","However, simulating the diffusion bridge for natural data can be challenging due to both the intractability of the drift term and continuous representations of the data.","Although several methods are available to simulate finite-dimensional diffusion bridges, infinite-dimensional cases remain unresolved.","In the paper, we present a solution to this problem by merging score-matching techniques with operator learning, enabling a direct approach to score-matching for the infinite-dimensional bridge.","We construct the score to be discretization invariant, which is natural given the underlying spatially continuous process.","We conduct a series of experiments, ranging from synthetic examples with closed-form solutions to the stochastic nonlinear evolution of real-world biological shape data, and our method demonstrates high efficacy, particularly due to its ability to adapt to any resolution without extra training."],"url":"http://arxiv.org/abs/2405.18353v1","category":"cs.LG"}
{"created":"2024-05-28 16:25:43","title":"Lattice ultrasensitivity produces large gain in E. coli chemosensing","abstract":"E. coli use a regular lattice of receptors and attached kinases to detect and amplify faint chemical signals. Kinase output is characterized by precise adaptation to a wide range of background ligand levels and large gain in response to small relative changes in ligand concentration. These characteristics are well described by models which achieve their gain through equilibrium cooperativity. But these models are challenged by two experimental results. First, neither adaptation nor large gain are present in receptor binding assays. Second, in cells lacking adaptation machinery, fluctuations can sometimes be enormous, with essentially all kinases transitioning together. Here we introduce a far-from equilibrium model in which receptors gate the spread of activity between neighboring kinases. This model achieves large gain through a mechanism we term lattice ultrasensitivity (LU). In our LU model, kinase and receptor states are separate degrees of freedom, and kinase kinetics are dominated by chemical rates far-from-equilibrium rather than by equilibrium allostery. The model recapitulates the successes of past models, but also matches the challenging experimental findings. Importantly, unlike past lattice critical models, our LU model does not require parameters to be fine tuned for function.","sentences":["E. coli use a regular lattice of receptors and attached kinases to detect and amplify faint chemical signals.","Kinase output is characterized by precise adaptation to a wide range of background ligand levels and large gain in response to small relative changes in ligand concentration.","These characteristics are well described by models which achieve their gain through equilibrium cooperativity.","But these models are challenged by two experimental results.","First, neither adaptation nor large gain are present in receptor binding assays.","Second, in cells lacking adaptation machinery, fluctuations can sometimes be enormous, with essentially all kinases transitioning together.","Here we introduce a far-from equilibrium model in which receptors gate the spread of activity between neighboring kinases.","This model achieves large gain through a mechanism we term lattice ultrasensitivity (LU).","In our LU model, kinase and receptor states are separate degrees of freedom, and kinase kinetics are dominated by chemical rates far-from-equilibrium rather than by equilibrium allostery.","The model recapitulates the successes of past models, but also matches the challenging experimental findings.","Importantly, unlike past lattice critical models, our LU model does not require parameters to be fine tuned for function."],"url":"http://arxiv.org/abs/2405.18331v1","category":"physics.bio-ph"}
{"created":"2024-05-28 15:17:03","title":"Population III star formation in the presence of turbulence, magnetic fields and ionizing radiation feedback","abstract":"Turbulence, magnetic fields and radiation feedback are key components that shape the formation of stars, especially in the metal-free environments at high redshifts where Population III stars form. Yet no 3D numerical simulations exist that simultaneously take all of these into account. We present the first suite of radiation-magnetohydrodynamics (RMHD) simulations of Population III star formation using the adaptive mesh refinement (AMR) code FLASH. We include both turbulent magnetic fields and ionizing radiation feedback coupled to primordial chemistry, and resolve the collapse of primordial clouds down to few au. We find that dynamically strong magnetic fields significantly slow down accretion onto protostars, while ionizing feedback is largely unable to regulate gas accretion because the partially ionized \\ion{H}{ii} region gets trapped near the star due to insufficient radiative outputs from the star. The maximum stellar mass in the HD and RHD simulations that only yield one star exceeds $100\\,\\rm{M_{\\odot}}$ within the first $5000\\,\\rm{yr}$. However, in the corresponding MHD and RMHD runs, the maximum mass of Population III star is only $60\\,\\rm{M_{\\odot}}$. In other realizations where we observe widespread fragmentation leading to the formation of Population III star clusters, the maximum stellar mass is further reduced by a factor of few due to fragmentation-induced starvation. We thus conclude that magnetic fields are more important than ionizing feedback in regulating the mass of the star, at least during the earliest stages of Population III star formation, in typical dark matter minihaloes at $z \\approx 30$.","sentences":["Turbulence, magnetic fields and radiation feedback are key components that shape the formation of stars, especially in the metal-free environments at high redshifts where Population III stars form.","Yet no 3D numerical simulations exist that simultaneously take all of these into account.","We present the first suite of radiation-magnetohydrodynamics (RMHD) simulations of Population III star formation using the adaptive mesh refinement (AMR) code FLASH.","We include both turbulent magnetic fields and ionizing radiation feedback coupled to primordial chemistry, and resolve the collapse of primordial clouds down to few au.","We find that dynamically strong magnetic fields significantly slow down accretion onto protostars, while ionizing feedback is largely unable to regulate gas accretion because the partially ionized \\ion{H}{ii} region gets trapped near the star due to insufficient radiative outputs from the star.","The maximum stellar mass in the HD and RHD simulations that only yield one star exceeds $100\\,\\rm{M_{\\odot}}$ within the first $5000\\,\\rm{yr}$. However, in the corresponding MHD and RMHD runs, the maximum mass of Population III star is only $60\\,\\rm{M_{\\odot}}$. In other realizations where we observe widespread fragmentation leading to the formation of Population III star clusters, the maximum stellar mass is further reduced by a factor of few due to fragmentation-induced starvation.","We thus conclude that magnetic fields are more important than ionizing feedback in regulating the mass of the star, at least during the earliest stages of Population III star formation, in typical dark matter minihaloes at $z \\approx 30$."],"url":"http://arxiv.org/abs/2405.18265v1","category":"astro-ph.GA"}
{"created":"2024-05-28 13:24:48","title":"Imaging, counting, and positioning single interstitial atoms in solids","abstract":"Interstitial atoms are ubiquitous in solids and they are widely incorporated into materials to tune their lattice structure, electronic transportation, and mechanical properties. Because the distribution of interstitial atoms in matrix materials is usually disordered and most of them are light atoms with weak scattering ability, it remains a challenge to directly image single interstitial atoms and measure their geometrical positions. In this work, direct imaging and measuring of single interstitial atoms have been realized with adaptive-propagator ptychography. The measurement of their three-dimensional coordinates enables quantitative analysis of the pair distribution function of the interstitial atoms and reveals the anisotropic occupation of oxygen in the interstitial sites in titanium. The current work paves the way for the determination of interstitial atoms in materials, and for the correlation between the atomic-scale behavior of interstitial atoms and the physical properties of materials.","sentences":["Interstitial atoms are ubiquitous in solids and they are widely incorporated into materials to tune their lattice structure, electronic transportation, and mechanical properties.","Because the distribution of interstitial atoms in matrix materials is usually disordered and most of them are light atoms with weak scattering ability, it remains a challenge to directly image single interstitial atoms and measure their geometrical positions.","In this work, direct imaging and measuring of single interstitial atoms have been realized with adaptive-propagator ptychography.","The measurement of their three-dimensional coordinates enables quantitative analysis of the pair distribution function of the interstitial atoms and reveals the anisotropic occupation of oxygen in the interstitial sites in titanium.","The current work paves the way for the determination of interstitial atoms in materials, and for the correlation between the atomic-scale behavior of interstitial atoms and the physical properties of materials."],"url":"http://arxiv.org/abs/2405.18164v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-28 13:06:32","title":"Unified Low-rank Compression Framework for Click-through Rate Prediction","abstract":"Deep Click-Through Rate (CTR) prediction models play an important role in modern industrial recommendation scenarios. However, high memory overhead and computational costs limit their deployment in resource-constrained environments. Low-rank approximation is an effective method for computer vision and natural language processing models, but its application in compressing CTR prediction models has been less explored. Due to the limited memory and computing resources, compression of CTR prediction models often confronts three fundamental challenges, i.e., (1). How to reduce the model sizes to adapt to edge devices? (2). How to speed up CTR prediction model inference? (3). How to retain the capabilities of original models after compression? Previous low-rank compression research mostly uses tensor decomposition, which can achieve a high parameter compression ratio, but brings in AUC degradation and additional computing overhead. To address these challenges, we propose a unified low-rank decomposition framework for compressing CTR prediction models. We find that even with the most classic matrix decomposition SVD method, our framework can achieve better performance than the original model. To further improve the effectiveness of our framework, we locally compress the output features instead of compressing the model weights. Our unified low-rank compression framework can be applied to embedding tables and MLP layers in various CTR prediction models. Extensive experiments on two academic datasets and one real industrial benchmark demonstrate that, with 3-5x model size reduction, our compressed models can achieve both faster inference and higher AUC than the uncompressed original models. Our code is at https://github.com/yuhao318/Atomic_Feature_Mimicking.","sentences":["Deep Click-Through Rate (CTR) prediction models play an important role in modern industrial recommendation scenarios.","However, high memory overhead and computational costs limit their deployment in resource-constrained environments.","Low-rank approximation is an effective method for computer vision and natural language processing models, but its application in compressing CTR prediction models has been less explored.","Due to the limited memory and computing resources, compression of CTR prediction models often confronts three fundamental challenges, i.e., (1).","How to reduce the model sizes to adapt to edge devices?","(2).","How to speed up CTR prediction model inference?","(3).","How to retain the capabilities of original models after compression?","Previous low-rank compression research mostly uses tensor decomposition, which can achieve a high parameter compression ratio, but brings in AUC degradation and additional computing overhead.","To address these challenges, we propose a unified low-rank decomposition framework for compressing CTR prediction models.","We find that even with the most classic matrix decomposition SVD method, our framework can achieve better performance than the original model.","To further improve the effectiveness of our framework, we locally compress the output features instead of compressing the model weights.","Our unified low-rank compression framework can be applied to embedding tables and MLP layers in various CTR prediction models.","Extensive experiments on two academic datasets and one real industrial benchmark demonstrate that, with 3-5x model size reduction, our compressed models can achieve both faster inference and higher AUC than the uncompressed original models.","Our code is at https://github.com/yuhao318/Atomic_Feature_Mimicking."],"url":"http://arxiv.org/abs/2405.18146v1","category":"cs.IR"}
{"created":"2024-05-28 11:57:29","title":"An adaptive transfer learning perspective on classification in non-stationary environments","abstract":"We consider a semi-supervised classification problem with non-stationary label-shift in which we observe a labelled data set followed by a sequence of unlabelled covariate vectors in which the marginal probabilities of the class labels may change over time. Our objective is to predict the corresponding class-label for each covariate vector, without ever observing the ground-truth labels, beyond the initial labelled data set. Previous work has demonstrated the potential of sophisticated variants of online gradient descent to perform competitively with the optimal dynamic strategy (Bai et al. 2022). In this work we explore an alternative approach grounded in statistical methods for adaptive transfer learning. We demonstrate the merits of this alternative methodology by establishing a high-probability regret bound on the test error at any given individual test-time, which adapt automatically to the unknown dynamics of the marginal label probabilities. Further more, we give bounds on the average dynamic regret which match the average guarantees of the online learning perspective for any given time interval.","sentences":["We consider a semi-supervised classification problem with non-stationary label-shift in which we observe a labelled data set followed by a sequence of unlabelled covariate vectors in which the marginal probabilities of the class labels may change over time.","Our objective is to predict the corresponding class-label for each covariate vector, without ever observing the ground-truth labels, beyond the initial labelled data set.","Previous work has demonstrated the potential of sophisticated variants of online gradient descent to perform competitively with the optimal dynamic strategy (Bai et al. 2022).","In this work we explore an alternative approach grounded in statistical methods for adaptive transfer learning.","We demonstrate the merits of this alternative methodology by establishing a high-probability regret bound on the test error at any given individual test-time, which adapt automatically to the unknown dynamics of the marginal label probabilities.","Further more, we give bounds on the average dynamic regret which match the average guarantees of the online learning perspective for any given time interval."],"url":"http://arxiv.org/abs/2405.18091v1","category":"math.ST"}
{"created":"2024-05-28 11:29:25","title":"An Empirical Analysis of Forgetting in Pre-trained Models with Incremental Low-Rank Updates","abstract":"Broad, open source availability of large pretrained foundation models on the internet through platforms such as HuggingFace has taken the world of practical deep learning by storm. A classical pipeline for neural network training now typically consists of finetuning these pretrained network on a small target dataset instead of training from scratch. In the case of large models this can be done even on modest hardware using a low rank training technique known as Low-Rank Adaptation (LoRA). While Low Rank training has already been studied in the continual learning setting, existing works often consider storing the learned adapter along with the existing model but rarely attempt to modify the weights of the pretrained model by merging the LoRA with the existing weights after finishing the training of each task. In this article we investigate this setting and study the impact of LoRA rank on the forgetting of the pretraining foundation task and on the plasticity and forgetting of subsequent ones. We observe that this rank has an important impact on forgetting of both the pretraining and downstream tasks. We also observe that vision transformers finetuned in that way exhibit a sort of ``contextual'' forgetting, a behaviour that we do not observe for residual networks and that we believe has not been observed yet in previous continual learning works.","sentences":["Broad, open source availability of large pretrained foundation models on the internet through platforms such as HuggingFace has taken the world of practical deep learning by storm.","A classical pipeline for neural network training now typically consists of finetuning these pretrained network on a small target dataset instead of training from scratch.","In the case of large models this can be done even on modest hardware using a low rank training technique known as Low-Rank Adaptation (LoRA).","While Low Rank training has already been studied in the continual learning setting, existing works often consider storing the learned adapter along with the existing model but rarely attempt to modify the weights of the pretrained model by merging the LoRA with the existing weights after finishing the training of each task.","In this article we investigate this setting and study the impact of LoRA rank on the forgetting of the pretraining foundation task and on the plasticity and forgetting of subsequent ones.","We observe that this rank has an important impact on forgetting of both the pretraining and downstream tasks.","We also observe that vision transformers finetuned in that way exhibit a sort of ``contextual'' forgetting, a behaviour that we do not observe for residual networks and that we believe has not been observed yet in previous continual learning works."],"url":"http://arxiv.org/abs/2405.18069v1","category":"cs.LG"}
{"created":"2024-05-28 09:30:44","title":"Local boundedness of cone multipliers","abstract":"We show that the cone multiplier satisfies local $L^p$-$L^q$ bounds only in the trivial range $1\\leq q\\leq 2\\leq p\\leq\\infty$. To do so, we suitably adapt to this setting the proof of Fefferman for the ball multiplier. As a consequence we answer negatively a question by B\\'ekoll\\'e and Bonami (Colloq. Math. 68, 1995, 81-100), regarding the continuity from $L^p\\to L^q$ of the Cauchy-Szeg\\\"o projections associated with a class of bounded symmetric domains in $\\mathbb{C}^n$ with rank $r\\geq2$.","sentences":["We show that the cone multiplier satisfies local $L^p$-$L^q$ bounds only in the trivial range $1\\leq q\\leq 2\\leq p\\leq\\infty$. To do so, we suitably adapt to this setting the proof of Fefferman for the ball multiplier.","As a consequence we answer negatively a question by B\\'ekoll\\'e and Bonami (Colloq.","Math. 68, 1995, 81-100), regarding the continuity from $L^p\\to L^q$ of the Cauchy-Szeg\\\"o projections associated with a class of bounded symmetric domains in $\\mathbb{C}^n$ with rank $r\\geq2$."],"url":"http://arxiv.org/abs/2405.17997v1","category":"math.AP"}
{"created":"2024-05-28 09:20:14","title":"Multi spacecraft study with the Icarus model: Modelling the propagation of CMEs to Mercury and Earth","abstract":"Coronal Mass Ejections (CMEs) are the main drivers of the disturbances in interplanetary space. Understanding the CME interior magnetic structure is crucial for advancing space weather studies. Assessing the capabilities of a numerical heliospheric model is crucial, as understanding the nature and extent of its limitations can be used for improving the model and the space weather predictions based on it.   The present paper aims to test the capabilities of the recently developed heliospheric model Icarus and the linear force-free spheromak model that has been implemented in it.   To validate the Icarus space weather modeling tool, two CME events were selected that were observed by two spacecraft located near Mercury and Earth, respectively. This enables testing the heliospheric model computed with Icarus at two distant locations. The source regions for the CMEs were identified, and the CME parameters were determined and later optimized. Different adaptive mesh refinement levels were applied in the simulations to assess its performance by comparing the simulation results to in-situ measurements.   The first CME event erupted on SOL2013-07-09T15:24. The modeled time series were in good agreement with the observations both at MESSENGER and ACE. The second CME event started on SOL2014-02-16T10:24 and was more complicated, as three CME interactions occurred in this event. It was impossible to recover the observed profiles without modeling the other two CMEs that were observed, one before the main CME and one afterward. For both CME studies, AMR level 3 was sufficient to reconstruct small-scale features near Mercury, while at Earth, AMR level 4 was necessary due to the radially stretched grid that was used.","sentences":["Coronal Mass Ejections (CMEs) are the main drivers of the disturbances in interplanetary space.","Understanding the CME interior magnetic structure is crucial for advancing space weather studies.","Assessing the capabilities of a numerical heliospheric model is crucial, as understanding the nature and extent of its limitations can be used for improving the model and the space weather predictions based on it.   ","The present paper aims to test the capabilities of the recently developed heliospheric model Icarus and the linear force-free spheromak model that has been implemented in it.   ","To validate the Icarus space weather modeling tool, two CME events were selected that were observed by two spacecraft located near Mercury and Earth, respectively.","This enables testing the heliospheric model computed with Icarus at two distant locations.","The source regions for the CMEs were identified, and the CME parameters were determined and later optimized.","Different adaptive mesh refinement levels were applied in the simulations to assess its performance by comparing the simulation results to in-situ measurements.   ","The first CME event erupted on SOL2013-07-09T15:24.","The modeled time series were in good agreement with the observations both at MESSENGER and ACE.","The second CME event started on SOL2014-02-16T10:24 and was more complicated, as three CME interactions occurred in this event.","It was impossible to recover the observed profiles without modeling the other two CMEs that were observed, one before the main CME and one afterward.","For both CME studies, AMR level 3 was sufficient to reconstruct small-scale features near Mercury, while at Earth, AMR level 4 was necessary due to the radially stretched grid that was used."],"url":"http://arxiv.org/abs/2405.17988v1","category":"astro-ph.SR"}
{"created":"2024-05-28 09:19:52","title":"BlueSWAT: A Lightweight State-Aware Security Framework for Bluetooth Low Energy","abstract":"Bluetooth Low Energy (BLE) is a short-range wireless communication technology for resource-constrained IoT devices. Unfortunately, BLE is vulnerable to session-based attacks, where previous packets construct exploitable conditions for subsequent packets to compromise connections. Defending against session-based attacks is challenging because each step in the attack sequence is legitimate when inspected individually. In this paper, we present BlueSWAT, a lightweight state-aware security framework for protecting BLE devices. To perform inspection on the session level rather than individual packets, BlueSWAT leverages a finite state machine (FSM) to monitor sequential actions of connections at runtime. Patterns of session-based attacks are modeled as malicious transition paths in the FSM. To overcome the heterogeneous IoT environment, we develop a lightweight eBPF framework to facilitate universal patch distribution across different BLE architectures and stacks, without requiring device reboot. We implement BlueSWAT on 5 real-world devices with different chips and stacks to demonstrate its cross-device adaptability. On our dataset with 101 real-world BLE vulnerabilities, BlueSWAT can mitigate 76.1% of session-based attacks, outperforming other defense frameworks. In our end-to-end application evaluation, BlueSWAT patches introduce an average of 0.073% memory overhead and negligible latency.","sentences":["Bluetooth Low Energy (BLE) is a short-range wireless communication technology for resource-constrained IoT devices.","Unfortunately, BLE is vulnerable to session-based attacks, where previous packets construct exploitable conditions for subsequent packets to compromise connections.","Defending against session-based attacks is challenging because each step in the attack sequence is legitimate when inspected individually.","In this paper, we present BlueSWAT, a lightweight state-aware security framework for protecting BLE devices.","To perform inspection on the session level rather than individual packets, BlueSWAT leverages a finite state machine (FSM) to monitor sequential actions of connections at runtime.","Patterns of session-based attacks are modeled as malicious transition paths in the FSM.","To overcome the heterogeneous IoT environment, we develop a lightweight eBPF framework to facilitate universal patch distribution across different BLE architectures and stacks, without requiring device reboot.","We implement BlueSWAT on 5 real-world devices with different chips and stacks to demonstrate its cross-device adaptability.","On our dataset with 101 real-world BLE vulnerabilities, BlueSWAT can mitigate 76.1% of session-based attacks, outperforming other defense frameworks.","In our end-to-end application evaluation, BlueSWAT patches introduce an average of 0.073% memory overhead and negligible latency."],"url":"http://arxiv.org/abs/2405.17987v1","category":"cs.CR"}
{"created":"2024-05-28 09:13:12","title":"Explicit formulae for the mean value of products of values of Dirichlet $L$-functions at positive integers","abstract":"Let $m\\ge 1$ be a rational integer. We give an explicit formula for the mean value $$\\frac{2}{\\phi(f)}\\sum_{\\chi (-1)=(-1)^m}\\vert L(m,\\chi )\\vert^2,$$ where $\\chi$ ranges over the $\\phi (f)/2$ Dirichlet characters modulo $f>2$ with the same parity as $m$. We then adapt our proof to obtain explicit means values for products of the form $L(m_1,\\chi_1)\\cdots L(m_{n-1},\\chi_{n-1})\\overline{L(m_n,\\chi_1\\cdots\\chi_{n-1})}$.","sentences":["Let $m\\ge 1$ be a rational integer.","We give an explicit formula for the mean value $$\\frac{2}{\\phi(f)}\\sum_{\\chi (-1)=(-1)^m}\\vert L(m,\\chi )\\vert^2,$$ where $\\chi$ ranges over the $\\phi (f)/2$ Dirichlet characters modulo $f>2$ with the same parity as $m$. We then adapt our proof to obtain explicit means values for products of the form $L(m_1,\\chi_1)\\cdots L(m_{n-1},\\chi_{n-1})\\overline{L(m_n,\\chi_1\\cdots\\chi_{n-1})}$."],"url":"http://arxiv.org/abs/2405.17981v1","category":"math.NT"}
{"created":"2024-05-28 08:40:14","title":"FreeSplat: Generalizable 3D Gaussian Splatting Towards Free-View Synthesis of Indoor Scenes","abstract":"Empowering 3D Gaussian Splatting with generalization ability is appealing. However, existing generalizable 3D Gaussian Splatting methods are largely confined to narrow-range interpolation between stereo images due to their heavy backbones, thus lacking the ability to accurately localize 3D Gaussian and support free-view synthesis across wide view range. In this paper, we present a novel framework FreeSplat that is capable of reconstructing geometrically consistent 3D scenes from long sequence input towards free-view synthesis.Specifically, we firstly introduce Low-cost Cross-View Aggregation achieved by constructing adaptive cost volumes among nearby views and aggregating features using a multi-scale structure. Subsequently, we present the Pixel-wise Triplet Fusion to eliminate redundancy of 3D Gaussians in overlapping view regions and to aggregate features observed across multiple views. Additionally, we propose a simple but effective free-view training strategy that ensures robust view synthesis across broader view range regardless of the number of views. Our empirical results demonstrate state-of-the-art novel view synthesis peformances in both novel view rendered color maps quality and depth maps accuracy across different numbers of input views. We also show that FreeSplat performs inference more efficiently and can effectively reduce redundant Gaussians, offering the possibility of feed-forward large scene reconstruction without depth priors.","sentences":["Empowering 3D Gaussian Splatting with generalization ability is appealing.","However, existing generalizable 3D Gaussian Splatting methods are largely confined to narrow-range interpolation between stereo images due to their heavy backbones, thus lacking the ability to accurately localize 3D Gaussian and support free-view synthesis across wide view range.","In this paper, we present a novel framework FreeSplat that is capable of reconstructing geometrically consistent 3D scenes from long sequence input towards free-view synthesis.","Specifically, we firstly introduce Low-cost Cross-View Aggregation achieved by constructing adaptive cost volumes among nearby views and aggregating features using a multi-scale structure.","Subsequently, we present the Pixel-wise Triplet Fusion to eliminate redundancy of 3D Gaussians in overlapping view regions and to aggregate features observed across multiple views.","Additionally, we propose a simple but effective free-view training strategy that ensures robust view synthesis across broader view range regardless of the number of views.","Our empirical results demonstrate state-of-the-art novel view synthesis peformances in both novel view rendered color maps quality and depth maps accuracy across different numbers of input views.","We also show that FreeSplat performs inference more efficiently and can effectively reduce redundant Gaussians, offering the possibility of feed-forward large scene reconstruction without depth priors."],"url":"http://arxiv.org/abs/2405.17958v1","category":"cs.CV"}
{"created":"2024-05-28 07:58:33","title":"ToonCrafter: Generative Cartoon Interpolation","abstract":"We introduce ToonCrafter, a novel approach that transcends traditional correspondence-based cartoon video interpolation, paving the way for generative interpolation. Traditional methods, that implicitly assume linear motion and the absence of complicated phenomena like dis-occlusion, often struggle with the exaggerated non-linear and large motions with occlusion commonly found in cartoons, resulting in implausible or even failed interpolation results. To overcome these limitations, we explore the potential of adapting live-action video priors to better suit cartoon interpolation within a generative framework. ToonCrafter effectively addresses the challenges faced when applying live-action video motion priors to generative cartoon interpolation. First, we design a toon rectification learning strategy that seamlessly adapts live-action video priors to the cartoon domain, resolving the domain gap and content leakage issues. Next, we introduce a dual-reference-based 3D decoder to compensate for lost details due to the highly compressed latent prior spaces, ensuring the preservation of fine details in interpolation results. Finally, we design a flexible sketch encoder that empowers users with interactive control over the interpolation results. Experimental results demonstrate that our proposed method not only produces visually convincing and more natural dynamics, but also effectively handles dis-occlusion. The comparative evaluation demonstrates the notable superiority of our approach over existing competitors.","sentences":["We introduce ToonCrafter, a novel approach that transcends traditional correspondence-based cartoon video interpolation, paving the way for generative interpolation.","Traditional methods, that implicitly assume linear motion and the absence of complicated phenomena like dis-occlusion, often struggle with the exaggerated non-linear and large motions with occlusion commonly found in cartoons, resulting in implausible or even failed interpolation results.","To overcome these limitations, we explore the potential of adapting live-action video priors to better suit cartoon interpolation within a generative framework.","ToonCrafter effectively addresses the challenges faced when applying live-action video motion priors to generative cartoon interpolation.","First, we design a toon rectification learning strategy that seamlessly adapts live-action video priors to the cartoon domain, resolving the domain gap and content leakage issues.","Next, we introduce a dual-reference-based 3D decoder to compensate for lost details due to the highly compressed latent prior spaces, ensuring the preservation of fine details in interpolation results.","Finally, we design a flexible sketch encoder that empowers users with interactive control over the interpolation results.","Experimental results demonstrate that our proposed method not only produces visually convincing and more natural dynamics, but also effectively handles dis-occlusion.","The comparative evaluation demonstrates the notable superiority of our approach over existing competitors."],"url":"http://arxiv.org/abs/2405.17933v1","category":"cs.CV"}
{"created":"2024-05-28 07:56:49","title":"Towards Communication-efficient Federated Learning via Sparse and Aligned Adaptive Optimization","abstract":"Adaptive moment estimation (Adam), as a Stochastic Gradient Descent (SGD) variant, has gained widespread popularity in federated learning (FL) due to its fast convergence. However, federated Adam (FedAdam) algorithms suffer from a threefold increase in uplink communication overhead compared to federated SGD (FedSGD) algorithms, which arises from the necessity to transmit both local model updates and first and second moment estimates from distributed devices to the centralized server for aggregation. Driven by this issue, we propose a novel sparse FedAdam algorithm called FedAdam-SSM, wherein distributed devices sparsify the updates of local model parameters and moment estimates and subsequently upload the sparse representations to the centralized server. To further reduce the communication overhead, the updates of local model parameters and moment estimates incorporate a shared sparse mask (SSM) into the sparsification process, eliminating the need for three separate sparse masks. Theoretically, we develop an upper bound on the divergence between the local model trained by FedAdam-SSM and the desired model trained by centralized Adam, which is related to sparsification error and imbalanced data distribution. By minimizing the divergence bound between the model trained by FedAdam-SSM and centralized Adam, we optimize the SSM to mitigate the learning performance degradation caused by sparsification error. Additionally, we provide convergence bounds for FedAdam-SSM in both convex and non-convex objective function settings, and investigate the impact of local epoch, learning rate and sparsification ratio on the convergence rate of FedAdam-SSM. Experimental results show that FedAdam-SSM outperforms baselines in terms of convergence rate (over 1.1$\\times$ faster than the sparse FedAdam baselines) and test accuracy (over 14.5\\% ahead of the quantized FedAdam baselines).","sentences":["Adaptive moment estimation (Adam), as a Stochastic Gradient Descent (SGD) variant, has gained widespread popularity in federated learning (FL) due to its fast convergence.","However, federated Adam (FedAdam) algorithms suffer from a threefold increase in uplink communication overhead compared to federated SGD (FedSGD) algorithms, which arises from the necessity to transmit both local model updates and first and second moment estimates from distributed devices to the centralized server for aggregation.","Driven by this issue, we propose a novel sparse FedAdam algorithm called FedAdam-SSM, wherein distributed devices sparsify the updates of local model parameters and moment estimates and subsequently upload the sparse representations to the centralized server.","To further reduce the communication overhead, the updates of local model parameters and moment estimates incorporate a shared sparse mask (SSM) into the sparsification process, eliminating the need for three separate sparse masks.","Theoretically, we develop an upper bound on the divergence between the local model trained by FedAdam-SSM and the desired model trained by centralized Adam, which is related to sparsification error and imbalanced data distribution.","By minimizing the divergence bound between the model trained by FedAdam-SSM and centralized Adam, we optimize the SSM to mitigate the learning performance degradation caused by sparsification error.","Additionally, we provide convergence bounds for FedAdam-SSM in both convex and non-convex objective function settings, and investigate the impact of local epoch, learning rate and sparsification ratio on the convergence rate of FedAdam-SSM.","Experimental results show that FedAdam-SSM outperforms baselines in terms of convergence rate (over 1.1$\\times$ faster than the sparse FedAdam baselines) and test accuracy (over 14.5\\% ahead of the quantized FedAdam baselines)."],"url":"http://arxiv.org/abs/2405.17932v1","category":"cs.LG"}
{"created":"2024-05-28 07:45:32","title":"Unraveling friction forces of droplets on non-wetting surface","abstract":"This paper explores the friction forces encountered by droplets on non-wetting surfaces, specifically focusing on superhydrophobic and superheated substrates. Employing a combination of experimental techniques, including inclined plane tests and cantilever force sensor measurements, we quantify friction forces across a broad range of velocities and surface types. Our results demonstrate that friction forces vary significantly with changes in droplet velocity and surface characteristics, transitioning from contact line pinning to viscous dissipation in the bulk of the droplet. We propose a universal scaling law that accounts for contact angle hysteresis, viscous dissipation, and aerodynamic drag, providing a comprehensive framework for understanding droplet dynamics on non-wetting surfaces. These findings offer valuable insights for optimizing surface designs in fluid transport and microfluidic applications, paving the way for enhanced efficiency and innovation in these technologies.","sentences":["This paper explores the friction forces encountered by droplets on non-wetting surfaces, specifically focusing on superhydrophobic and superheated substrates.","Employing a combination of experimental techniques, including inclined plane tests and cantilever force sensor measurements, we quantify friction forces across a broad range of velocities and surface types.","Our results demonstrate that friction forces vary significantly with changes in droplet velocity and surface characteristics, transitioning from contact line pinning to viscous dissipation in the bulk of the droplet.","We propose a universal scaling law that accounts for contact angle hysteresis, viscous dissipation, and aerodynamic drag, providing a comprehensive framework for understanding droplet dynamics on non-wetting surfaces.","These findings offer valuable insights for optimizing surface designs in fluid transport and microfluidic applications, paving the way for enhanced efficiency and innovation in these technologies."],"url":"http://arxiv.org/abs/2405.17923v1","category":"physics.flu-dyn"}
{"created":"2024-05-28 07:24:56","title":"Reliable Object Tracking by Multimodal Hybrid Feature Extraction and Transformer-Based Fusion","abstract":"Visual object tracking, which is primarily based on visible light image sequences, encounters numerous challenges in complicated scenarios, such as low light conditions, high dynamic ranges, and background clutter. To address these challenges, incorporating the advantages of multiple visual modalities is a promising solution for achieving reliable object tracking. However, the existing approaches usually integrate multimodal inputs through adaptive local feature interactions, which cannot leverage the full potential of visual cues, thus resulting in insufficient feature modeling. In this study, we propose a novel multimodal hybrid tracker (MMHT) that utilizes frame-event-based data for reliable single object tracking. The MMHT model employs a hybrid backbone consisting of an artificial neural network (ANN) and a spiking neural network (SNN) to extract dominant features from different visual modalities and then uses a unified encoder to align the features across different domains. Moreover, we propose an enhanced transformer-based module to fuse multimodal features using attention mechanisms. With these methods, the MMHT model can effectively construct a multiscale and multidimensional visual feature space and achieve discriminative feature modeling. Extensive experiments demonstrate that the MMHT model exhibits competitive performance in comparison with that of other state-of-the-art methods. Overall, our results highlight the effectiveness of the MMHT model in terms of addressing the challenges faced in visual object tracking tasks.","sentences":["Visual object tracking, which is primarily based on visible light image sequences, encounters numerous challenges in complicated scenarios, such as low light conditions, high dynamic ranges, and background clutter.","To address these challenges, incorporating the advantages of multiple visual modalities is a promising solution for achieving reliable object tracking.","However, the existing approaches usually integrate multimodal inputs through adaptive local feature interactions, which cannot leverage the full potential of visual cues, thus resulting in insufficient feature modeling.","In this study, we propose a novel multimodal hybrid tracker (MMHT) that utilizes frame-event-based data for reliable single object tracking.","The MMHT model employs a hybrid backbone consisting of an artificial neural network (ANN) and a spiking neural network (SNN) to extract dominant features from different visual modalities and then uses a unified encoder to align the features across different domains.","Moreover, we propose an enhanced transformer-based module to fuse multimodal features using attention mechanisms.","With these methods, the MMHT model can effectively construct a multiscale and multidimensional visual feature space and achieve discriminative feature modeling.","Extensive experiments demonstrate that the MMHT model exhibits competitive performance in comparison with that of other state-of-the-art methods.","Overall, our results highlight the effectiveness of the MMHT model in terms of addressing the challenges faced in visual object tracking tasks."],"url":"http://arxiv.org/abs/2405.17903v1","category":"cs.CV"}
{"created":"2024-05-28 06:16:57","title":"Adapting Pre-Trained Vision Models for Novel Instance Detection and Segmentation","abstract":"Novel Instance Detection and Segmentation (NIDS) aims at detecting and segmenting novel object instances given a few examples of each instance. We propose a unified framework (NIDS-Net) comprising object proposal generation, embedding creation for both instance templates and proposal regions, and embedding matching for instance label assignment. Leveraging recent advancements in large vision methods, we utilize the Grounding DINO and Segment Anything Model (SAM) to obtain object proposals with accurate bounding boxes and masks. Central to our approach is the generation of high-quality instance embeddings. We utilize foreground feature averages of patch embeddings from the DINOv2 ViT backbone, followed by refinement through a weight adapter mechanism that we introduce. We show experimentally that our weight adapter can adjust the embeddings locally within their feature space and effectively limit overfitting. This methodology enables a straightforward matching strategy, resulting in significant performance gains. Our framework surpasses current state-of-the-art methods, demonstrating notable improvements of 22.3, 46.2, 10.3, and 24.0 in average precision (AP) across four detection datasets. In instance segmentation tasks on seven core datasets of the BOP challenge, our method outperforms the top RGB methods by 3.6 AP and remains competitive with the best RGB-D method. Code is available at: https://github.com/YoungSean/NIDS-Net","sentences":["Novel Instance Detection and Segmentation (NIDS) aims at detecting and segmenting novel object instances given a few examples of each instance.","We propose a unified framework (NIDS-Net) comprising object proposal generation, embedding creation for both instance templates and proposal regions, and embedding matching for instance label assignment.","Leveraging recent advancements in large vision methods, we utilize the Grounding DINO and Segment Anything Model (SAM) to obtain object proposals with accurate bounding boxes and masks.","Central to our approach is the generation of high-quality instance embeddings.","We utilize foreground feature averages of patch embeddings from the DINOv2 ViT backbone, followed by refinement through a weight adapter mechanism that we introduce.","We show experimentally that our weight adapter can adjust the embeddings locally within their feature space and effectively limit overfitting.","This methodology enables a straightforward matching strategy, resulting in significant performance gains.","Our framework surpasses current state-of-the-art methods, demonstrating notable improvements of 22.3, 46.2, 10.3, and 24.0 in average precision (AP) across four detection datasets.","In instance segmentation tasks on seven core datasets of the BOP challenge, our method outperforms the top RGB methods by 3.6 AP and remains competitive with the best RGB-D method.","Code is available at: https://github.com/YoungSean/NIDS-Net"],"url":"http://arxiv.org/abs/2405.17859v1","category":"cs.CV"}
{"created":"2024-05-28 04:13:21","title":"Mani-GS: Gaussian Splatting Manipulation with Triangular Mesh","abstract":"Neural 3D representations such as Neural Radiance Fields (NeRF), excel at producing photo-realistic rendering results but lack the flexibility for manipulation and editing which is crucial for content creation. Previous works have attempted to address this issue by deforming a NeRF in canonical space or manipulating the radiance field based on an explicit mesh. However, manipulating NeRF is not highly controllable and requires a long training and inference time. With the emergence of 3D Gaussian Splatting (3DGS), extremely high-fidelity novel view synthesis can be achieved using an explicit point-based 3D representation with much faster training and rendering speed. However, there is still a lack of effective means to manipulate 3DGS freely while maintaining rendering quality. In this work, we aim to tackle the challenge of achieving manipulable photo-realistic rendering. We propose to utilize a triangular mesh to manipulate 3DGS directly with self-adaptation. This approach reduces the need to design various algorithms for different types of Gaussian manipulation. By utilizing a triangle shape-aware Gaussian binding and adapting method, we can achieve 3DGS manipulation and preserve high-fidelity rendering after manipulation. Our approach is capable of handling large deformations, local manipulations, and soft body simulations while keeping high-quality rendering. Furthermore, we demonstrate that our method is also effective with inaccurate meshes extracted from 3DGS. Experiments conducted demonstrate the effectiveness of our method and its superiority over baseline approaches.","sentences":["Neural 3D representations such as Neural Radiance Fields (NeRF), excel at producing photo-realistic rendering results but lack the flexibility for manipulation and editing which is crucial for content creation.","Previous works have attempted to address this issue by deforming a NeRF in canonical space or manipulating the radiance field based on an explicit mesh.","However, manipulating NeRF is not highly controllable and requires a long training and inference time.","With the emergence of 3D Gaussian Splatting (3DGS), extremely high-fidelity novel view synthesis can be achieved using an explicit point-based 3D representation with much faster training and rendering speed.","However, there is still a lack of effective means to manipulate 3DGS freely while maintaining rendering quality.","In this work, we aim to tackle the challenge of achieving manipulable photo-realistic rendering.","We propose to utilize a triangular mesh to manipulate 3DGS directly with self-adaptation.","This approach reduces the need to design various algorithms for different types of Gaussian manipulation.","By utilizing a triangle shape-aware Gaussian binding and adapting method, we can achieve 3DGS manipulation and preserve high-fidelity rendering after manipulation.","Our approach is capable of handling large deformations, local manipulations, and soft body simulations while keeping high-quality rendering.","Furthermore, we demonstrate that our method is also effective with inaccurate meshes extracted from 3DGS.","Experiments conducted demonstrate the effectiveness of our method and its superiority over baseline approaches."],"url":"http://arxiv.org/abs/2405.17811v1","category":"cs.GR"}
{"created":"2024-05-28 03:33:26","title":"Dyadic Regression with Sample Selection","abstract":"This paper addresses the sample selection problem in panel dyadic regression analysis. Dyadic data often include many zeros in the main outcomes due to the underlying network formation process. This not only contaminates popular estimators used in practice but also complicates the inference due to the dyadic dependence structure. We extend Kyriazidou (1997)'s approach to dyadic data and characterize the asymptotic distribution of our proposed estimator. The convergence rates are $\\sqrt{n}$ or $\\sqrt{n^{2}h_{n}}$, depending on the degeneracy of the H\\'{a}jek projection part of the estimator, where $n$ is the number of nodes and $h_{n}$ is a bandwidth. We propose a bias-corrected confidence interval and a variance estimator that adapts to the degeneracy. A Monte Carlo simulation shows the good finite performance of our estimator and highlights the importance of bias correction in both asymptotic regimes when the fraction of zeros in outcomes varies. We illustrate our procedure using data from Moretti and Wilson (2017)'s paper on migration.","sentences":["This paper addresses the sample selection problem in panel dyadic regression analysis.","Dyadic data often include many zeros in the main outcomes due to the underlying network formation process.","This not only contaminates popular estimators used in practice but also complicates the inference due to the dyadic dependence structure.","We extend Kyriazidou (1997)'s approach to dyadic data and characterize the asymptotic distribution of our proposed estimator.","The convergence rates are $\\sqrt{n}$ or $\\sqrt{n^{2}h_{n}}$, depending on the degeneracy of the H\\'{a}jek projection part of the estimator, where $n$ is the number of nodes and $h_{n}$ is a bandwidth.","We propose a bias-corrected confidence interval and a variance estimator that adapts to the degeneracy.","A Monte Carlo simulation shows the good finite performance of our estimator and highlights the importance of bias correction in both asymptotic regimes when the fraction of zeros in outcomes varies.","We illustrate our procedure using data from Moretti and Wilson (2017)'s paper on migration."],"url":"http://arxiv.org/abs/2405.17787v1","category":"econ.EM"}
{"created":"2024-05-28 03:03:32","title":"Gradually Vanishing Gap in Prototypical Network for Unsupervised Domain Adaptation","abstract":"Unsupervised domain adaptation (UDA) is a critical problem for transfer learning, which aims to transfer the semantic information from labeled source domain to unlabeled target domain. Recent advancements in UDA models have demonstrated significant generalization capabilities on the target domain. However, the generalization boundary of UDA models remains unclear. When the domain discrepancy is too large, the model can not preserve the distribution structure, leading to distribution collapse during the alignment. To address this challenge, we propose an efficient UDA framework named Gradually Vanishing Gap in Prototypical Network (GVG-PN), which achieves transfer learning from both global and local perspectives. From the global alignment standpoint, our model generates a domain-biased intermediate domain that helps preserve the distribution structures. By entangling cross-domain features, our model progressively reduces the risk of distribution collapse. However, only relying on global alignment is insufficient to preserve the distribution structure. To further enhance the inner relationships of features, we introduce the local perspective. We utilize the graph convolutional network (GCN) as an intuitive method to explore the internal relationships between features, ensuring the preservation of manifold structures and generating domain-biased prototypes. Additionally, we consider the discriminability of the inner relationships between features. We propose a pro-contrastive loss to enhance the discriminability at the prototype level by separating hard negative pairs. By incorporating both GCN and the pro-contrastive loss, our model fully explores fine-grained semantic relationships. Experiments on several UDA benchmarks validated that the proposed GVG-PN can clearly outperform the SOTA models.","sentences":["Unsupervised domain adaptation (UDA) is a critical problem for transfer learning, which aims to transfer the semantic information from labeled source domain to unlabeled target domain.","Recent advancements in UDA models have demonstrated significant generalization capabilities on the target domain.","However, the generalization boundary of UDA models remains unclear.","When the domain discrepancy is too large, the model can not preserve the distribution structure, leading to distribution collapse during the alignment.","To address this challenge, we propose an efficient UDA framework named Gradually Vanishing Gap in Prototypical Network (GVG-PN), which achieves transfer learning from both global and local perspectives.","From the global alignment standpoint, our model generates a domain-biased intermediate domain that helps preserve the distribution structures.","By entangling cross-domain features, our model progressively reduces the risk of distribution collapse.","However, only relying on global alignment is insufficient to preserve the distribution structure.","To further enhance the inner relationships of features, we introduce the local perspective.","We utilize the graph convolutional network (GCN) as an intuitive method to explore the internal relationships between features, ensuring the preservation of manifold structures and generating domain-biased prototypes.","Additionally, we consider the discriminability of the inner relationships between features.","We propose a pro-contrastive loss to enhance the discriminability at the prototype level by separating hard negative pairs.","By incorporating both GCN and the pro-contrastive loss, our model fully explores fine-grained semantic relationships.","Experiments on several UDA benchmarks validated that the proposed GVG-PN can clearly outperform the SOTA models."],"url":"http://arxiv.org/abs/2405.17774v1","category":"cs.CV"}
{"created":"2024-05-28 02:55:40","title":"Risk-Neutral Generative Networks","abstract":"We present a functional generative approach to extract risk-neutral densities from market prices of options. Specifically, we model the log-returns on the time-to-maturity continuum as a stochastic curve driven by standard normal. We then use neural nets to represent the term structures of the location, the scale, and the higher-order moments, and impose stringent conditions on the learning process to ensure the neural net-based curve representation is free of static arbitrage. This specification is structurally clear in that it separates the modeling of randomness from the modeling of the term structures of the parameters. It is data adaptive in that we use neural nets to represent the shape of the stochastic curve. It is also generative in that the functional form of the stochastic curve, although parameterized by neural nets, is an explicit and deterministic function of the standard normal. This explicitness allows for the efficient generation of samples to price options across strikes and maturities, without compromising data adaptability. We have validated the effectiveness of this approach by benchmarking it against a comprehensive set of baseline models. Experiments show that the extracted risk-neutral densities accommodate a diverse range of shapes. Its accuracy significantly outperforms the extensive set of baseline models--including three parametric models and nine stochastic process models--in terms of accuracy and stability. The success of this approach is attributed to its capacity to offer flexible term structures for risk-neutral skewness and kurtosis.","sentences":["We present a functional generative approach to extract risk-neutral densities from market prices of options.","Specifically, we model the log-returns on the time-to-maturity continuum as a stochastic curve driven by standard normal.","We then use neural nets to represent the term structures of the location, the scale, and the higher-order moments, and impose stringent conditions on the learning process to ensure the neural net-based curve representation is free of static arbitrage.","This specification is structurally clear in that it separates the modeling of randomness from the modeling of the term structures of the parameters.","It is data adaptive in that we use neural nets to represent the shape of the stochastic curve.","It is also generative in that the functional form of the stochastic curve, although parameterized by neural nets, is an explicit and deterministic function of the standard normal.","This explicitness allows for the efficient generation of samples to price options across strikes and maturities, without compromising data adaptability.","We have validated the effectiveness of this approach by benchmarking it against a comprehensive set of baseline models.","Experiments show that the extracted risk-neutral densities accommodate a diverse range of shapes.","Its accuracy significantly outperforms the extensive set of baseline models--including three parametric models and nine stochastic process models--in terms of accuracy and stability.","The success of this approach is attributed to its capacity to offer flexible term structures for risk-neutral skewness and kurtosis."],"url":"http://arxiv.org/abs/2405.17770v1","category":"q-fin.MF"}
{"created":"2024-05-28 02:03:13","title":"Bi-directional models of `radically synthetic' differential geometry","abstract":"The radically synthetic foundation for smooth geometry formulated in [Law11] postulates a space T with the property that it has a unique point and, out of the monoid T^T of endomorphisms, it extracts a submonoid R which, in many cases, is the (commutative) multiplication of a rig structure. The rig R is said to be bi-directional if its subobject of invertible elements has two connected components. In this case, R may be equipped with a pre-order compatible with the rig structure. We adjust the construction of `well-adapted' models of Synthetic Differential Geometry in order to build the first pre-cohesive toposes with a bi-directional R. We also show that, in one of these pre-cohesive variants, the pre-order on R, derived radically synthetically from bi-directionality, coincides with that defined in the original model.","sentences":["The radically synthetic foundation for smooth geometry formulated in [Law11] postulates a space T with the property that it has a unique point and, out of the monoid T^T of endomorphisms, it extracts a submonoid R which, in many cases, is the (commutative) multiplication of a rig structure.","The rig R is said to be bi-directional if its subobject of invertible elements has two connected components.","In this case, R may be equipped with a pre-order compatible with the rig structure.","We adjust the construction of `well-adapted' models of Synthetic Differential Geometry in order to build the first pre-cohesive toposes with a bi-directional R. We also show that, in one of these pre-cohesive variants, the pre-order on R, derived radically synthetically from bi-directionality, coincides with that defined in the original model."],"url":"http://arxiv.org/abs/2405.17748v1","category":"math.CT"}
{"created":"2024-05-28 00:25:41","title":"AdapNet: Adaptive Noise-Based Network for Low-Quality Image Retrieval","abstract":"Image retrieval aims to identify visually similar images within a database using a given query image. Traditional methods typically employ both global and local features extracted from images for matching, and may also apply re-ranking techniques to enhance accuracy. However, these methods often fail to account for the noise present in query images, which can stem from natural or human-induced factors, thereby negatively impacting retrieval performance. To mitigate this issue, we introduce a novel setting for low-quality image retrieval, and propose an Adaptive Noise-Based Network (AdapNet) to learn robust abstract representations. Specifically, we devise a quality compensation block trained to compensate for various low-quality factors in input images. Besides, we introduce an innovative adaptive noise-based loss function, which dynamically adjusts its focus on the gradient in accordance with image quality, thereby augmenting the learning of unknown noisy samples during training and enhancing intra-class compactness. To assess the performance, we construct two datasets with low-quality queries, which is built by applying various types of noise on clean query images on the standard Revisited Oxford and Revisited Paris datasets. Comprehensive experimental results illustrate that AdapNet surpasses state-of-the-art methods on the Noise Revisited Oxford and Noise Revisited Paris benchmarks, while maintaining competitive performance on high-quality datasets. The code and constructed datasets will be made available.","sentences":["Image retrieval aims to identify visually similar images within a database using a given query image.","Traditional methods typically employ both global and local features extracted from images for matching, and may also apply re-ranking techniques to enhance accuracy.","However, these methods often fail to account for the noise present in query images, which can stem from natural or human-induced factors, thereby negatively impacting retrieval performance.","To mitigate this issue, we introduce a novel setting for low-quality image retrieval, and propose an Adaptive Noise-Based Network (AdapNet) to learn robust abstract representations.","Specifically, we devise a quality compensation block trained to compensate for various low-quality factors in input images.","Besides, we introduce an innovative adaptive noise-based loss function, which dynamically adjusts its focus on the gradient in accordance with image quality, thereby augmenting the learning of unknown noisy samples during training and enhancing intra-class compactness.","To assess the performance, we construct two datasets with low-quality queries, which is built by applying various types of noise on clean query images on the standard Revisited Oxford and Revisited Paris datasets.","Comprehensive experimental results illustrate that AdapNet surpasses state-of-the-art methods on the Noise Revisited Oxford and Noise Revisited Paris benchmarks, while maintaining competitive performance on high-quality datasets.","The code and constructed datasets will be made available."],"url":"http://arxiv.org/abs/2405.17718v1","category":"cs.CV"}
{"created":"2024-05-27 23:32:06","title":"Consistency Regularisation for Unsupervised Domain Adaptation in Monocular Depth Estimation","abstract":"In monocular depth estimation, unsupervised domain adaptation has recently been explored to relax the dependence on large annotated image-based depth datasets. However, this comes at the cost of training multiple models or requiring complex training protocols. We formulate unsupervised domain adaptation for monocular depth estimation as a consistency-based semi-supervised learning problem by assuming access only to the source domain ground truth labels. To this end, we introduce a pairwise loss function that regularises predictions on the source domain while enforcing perturbation consistency across multiple augmented views of the unlabelled target samples. Importantly, our approach is simple and effective, requiring only training of a single model in contrast to the prior work. In our experiments, we rely on the standard depth estimation benchmarks KITTI and NYUv2 to demonstrate state-of-the-art results compared to related approaches. Furthermore, we analyse the simplicity and effectiveness of our approach in a series of ablation studies. The code is available at \\url{https://github.com/AmirMaEl/SemiSupMDE}.","sentences":["In monocular depth estimation, unsupervised domain adaptation has recently been explored to relax the dependence on large annotated image-based depth datasets.","However, this comes at the cost of training multiple models or requiring complex training protocols.","We formulate unsupervised domain adaptation for monocular depth estimation as a consistency-based semi-supervised learning problem by assuming access only to the source domain ground truth labels.","To this end, we introduce a pairwise loss function that regularises predictions on the source domain while enforcing perturbation consistency across multiple augmented views of the unlabelled target samples.","Importantly, our approach is simple and effective, requiring only training of a single model in contrast to the prior work.","In our experiments, we rely on the standard depth estimation benchmarks KITTI and NYUv2 to demonstrate state-of-the-art results compared to related approaches.","Furthermore, we analyse the simplicity and effectiveness of our approach in a series of ablation studies.","The code is available at \\url{https://github.com/AmirMaEl/SemiSupMDE}."],"url":"http://arxiv.org/abs/2405.17704v1","category":"cs.CV"}
{"created":"2024-05-27 23:03:21","title":"Physics-guided Full Waveform Inversion using Encoder-Solver Convolutional Neural Networks","abstract":"Full Waveform Inversion (FWI) is an inverse problem for estimating the wave velocity distribution in a given domain, based on observed data on the boundaries. The inversion is computationally demanding because we are required to solve multiple forward problems, either in time or frequency domains, to simulate data that are then iteratively fitted to the observed data. We consider FWI in the frequency domain, where the Helmholtz equation is used as a forward model, and its repeated solution is the main computational bottleneck of the inversion process. To ease this cost, we integrate a learning process of an encoder-solver preconditioner that is based on convolutional neural networks (CNNs). The encoder-solver is trained to effectively precondition the discretized Helmholtz operator given velocity medium parameters. Then, by re-training the CNN between the iterations of the optimization process, the encoder-solver is adapted to the iteratively evolving velocity medium as part of the inversion. Without retraining, the performance of the solver deteriorates as the medium changes. Using our light retraining procedures, we obtain the forward simulations effectively throughout the process. We demonstrate our approach to solving FWI problems using 2D geophysical models with high-frequency data.","sentences":["Full Waveform Inversion (FWI) is an inverse problem for estimating the wave velocity distribution in a given domain, based on observed data on the boundaries.","The inversion is computationally demanding because we are required to solve multiple forward problems, either in time or frequency domains, to simulate data that are then iteratively fitted to the observed data.","We consider FWI in the frequency domain, where the Helmholtz equation is used as a forward model, and its repeated solution is the main computational bottleneck of the inversion process.","To ease this cost, we integrate a learning process of an encoder-solver preconditioner that is based on convolutional neural networks (CNNs).","The encoder-solver is trained to effectively precondition the discretized Helmholtz operator given velocity medium parameters.","Then, by re-training the CNN between the iterations of the optimization process, the encoder-solver is adapted to the iteratively evolving velocity medium as part of the inversion.","Without retraining, the performance of the solver deteriorates as the medium changes.","Using our light retraining procedures, we obtain the forward simulations effectively throughout the process.","We demonstrate our approach to solving FWI problems using 2D geophysical models with high-frequency data."],"url":"http://arxiv.org/abs/2405.17696v1","category":"cs.LG"}
{"created":"2024-05-27 22:15:23","title":"Deciphering Movement: Unified Trajectory Generation Model for Multi-Agent","abstract":"Understanding multi-agent behavior is critical across various fields. The conventional approach involves analyzing agent movements through three primary tasks: trajectory prediction, imputation, and spatial-temporal recovery. Considering the unique input formulation and constraint of these tasks, most existing methods are tailored to address only one specific task. However, in real-world applications, these scenarios frequently occur simultaneously. Consequently, methods designed for one task often fail to adapt to others, resulting in performance drops. To overcome this limitation, we propose a Unified Trajectory Generation model, UniTraj, that processes arbitrary trajectories as masked inputs, adaptable to diverse scenarios. Specifically, we introduce a Ghost Spatial Masking (GSM) module embedded within a Transformer encoder for spatial feature extraction. We further extend recent successful State Space Models (SSMs), particularly the Mamba model, into a Bidirectional Temporal Mamba to effectively capture temporal dependencies. Additionally, we incorporate a Bidirectional Temporal Scaled (BTS) module to comprehensively scan trajectories while maintaining the temporal missing relationships within the sequence. We curate and benchmark three practical sports game datasets, Basketball-U, Football-U, and Soccer-U, for evaluation. Extensive experiments demonstrate the superior performance of our model. To the best of our knowledge, this is the first work that addresses this unified problem through a versatile generative framework, thereby enhancing our understanding of multi-agent movement. Our datasets, code, and model weights are available at https://github.com/colorfulfuture/UniTraj-pytorch.","sentences":["Understanding multi-agent behavior is critical across various fields.","The conventional approach involves analyzing agent movements through three primary tasks: trajectory prediction, imputation, and spatial-temporal recovery.","Considering the unique input formulation and constraint of these tasks, most existing methods are tailored to address only one specific task.","However, in real-world applications, these scenarios frequently occur simultaneously.","Consequently, methods designed for one task often fail to adapt to others, resulting in performance drops.","To overcome this limitation, we propose a Unified Trajectory Generation model, UniTraj, that processes arbitrary trajectories as masked inputs, adaptable to diverse scenarios.","Specifically, we introduce a Ghost Spatial Masking (GSM) module embedded within a Transformer encoder for spatial feature extraction.","We further extend recent successful State Space Models (SSMs), particularly the Mamba model, into a Bidirectional Temporal Mamba to effectively capture temporal dependencies.","Additionally, we incorporate a Bidirectional Temporal Scaled (BTS) module to comprehensively scan trajectories while maintaining the temporal missing relationships within the sequence.","We curate and benchmark three practical sports game datasets, Basketball-U, Football-U, and Soccer-U, for evaluation.","Extensive experiments demonstrate the superior performance of our model.","To the best of our knowledge, this is the first work that addresses this unified problem through a versatile generative framework, thereby enhancing our understanding of multi-agent movement.","Our datasets, code, and model weights are available at https://github.com/colorfulfuture/UniTraj-pytorch."],"url":"http://arxiv.org/abs/2405.17680v1","category":"cs.CV"}
{"created":"2024-05-27 22:06:42","title":"Understanding differences in applying DETR to natural and medical images","abstract":"Transformer-based detectors have shown success in computer vision tasks with natural images. These models, exemplified by the Deformable DETR, are optimized through complex engineering strategies tailored to the typical characteristics of natural scenes. However, medical imaging data presents unique challenges such as extremely large image sizes, fewer and smaller regions of interest, and object classes which can be differentiated only through subtle differences. This study evaluates the applicability of these transformer-based design choices when applied to a screening mammography dataset that represents these distinct medical imaging data characteristics. Our analysis reveals that common design choices from the natural image domain, such as complex encoder architectures, multi-scale feature fusion, query initialization, and iterative bounding box refinement, do not improve and sometimes even impair object detection performance in medical imaging. In contrast, simpler and shallower architectures often achieve equal or superior results. This finding suggests that the adaptation of transformer models for medical imaging data requires a reevaluation of standard practices, potentially leading to more efficient and specialized frameworks for medical diagnosis.","sentences":["Transformer-based detectors have shown success in computer vision tasks with natural images.","These models, exemplified by the Deformable DETR, are optimized through complex engineering strategies tailored to the typical characteristics of natural scenes.","However, medical imaging data presents unique challenges such as extremely large image sizes, fewer and smaller regions of interest, and object classes which can be differentiated only through subtle differences.","This study evaluates the applicability of these transformer-based design choices when applied to a screening mammography dataset that represents these distinct medical imaging data characteristics.","Our analysis reveals that common design choices from the natural image domain, such as complex encoder architectures, multi-scale feature fusion, query initialization, and iterative bounding box refinement, do not improve and sometimes even impair object detection performance in medical imaging.","In contrast, simpler and shallower architectures often achieve equal or superior results.","This finding suggests that the adaptation of transformer models for medical imaging data requires a reevaluation of standard practices, potentially leading to more efficient and specialized frameworks for medical diagnosis."],"url":"http://arxiv.org/abs/2405.17677v1","category":"cs.CV"}
{"created":"2024-05-27 21:28:26","title":"What's the Opposite of a Face? Finding Shared Decodable Concepts and their Negations in the Brain","abstract":"Prior work has offered evidence for functional localization in the brain; different anatomical regions preferentially activate for certain types of visual input. For example, the fusiform face area preferentially activates for visual stimuli that include a face. However, the spectrum of visual semantics is extensive, and only a few semantically-tuned patches of cortex have so far been identified in the human brain. Using a multimodal (natural language and image) neural network architecture (CLIP) we train a highly accurate contrastive model that maps brain responses during naturalistic image viewing to CLIP embeddings. We then use a novel adaptation of the DBSCAN clustering algorithm to cluster the parameters of these participant-specific contrastive models. This reveals what we call Shared Decodable Concepts (SDCs): clusters in CLIP space that are decodable from common sets of voxels across multiple participants.   Examining the images most and least associated with each SDC cluster gives us additional insight into the semantic properties of each SDC. We note SDCs for previously reported visual features (e.g. orientation tuning in early visual cortex) as well as visual semantic concepts such as faces, places and bodies. In cases where our method finds multiple clusters for a visuo-semantic concept, the least associated images allow us to dissociate between confounding factors. For example, we discovered two clusters of food images, one driven by color, the other by shape. We also uncover previously unreported areas such as regions of extrastriate body area (EBA) tuned for legs/hands and sensitivity to numerosity in right intraparietal sulcus, and more. Thus, our contrastive-learning methodology better characterizes new and existing visuo-semantic representations in the brain by leveraging multimodal neural network representations and a novel adaptation of clustering algorithms.","sentences":["Prior work has offered evidence for functional localization in the brain; different anatomical regions preferentially activate for certain types of visual input.","For example, the fusiform face area preferentially activates for visual stimuli that include a face.","However, the spectrum of visual semantics is extensive, and only a few semantically-tuned patches of cortex have so far been identified in the human brain.","Using a multimodal (natural language and image) neural network architecture (CLIP) we train a highly accurate contrastive model that maps brain responses during naturalistic image viewing to CLIP embeddings.","We then use a novel adaptation of the DBSCAN clustering algorithm to cluster the parameters of these participant-specific contrastive models.","This reveals what we call Shared Decodable Concepts (SDCs): clusters in CLIP space that are decodable from common sets of voxels across multiple participants.   ","Examining the images most and least associated with each SDC cluster gives us additional insight into the semantic properties of each SDC.","We note SDCs for previously reported visual features (e.g. orientation tuning in early visual cortex) as well as visual semantic concepts such as faces, places and bodies.","In cases where our method finds multiple clusters for a visuo-semantic concept, the least associated images allow us to dissociate between confounding factors.","For example, we discovered two clusters of food images, one driven by color, the other by shape.","We also uncover previously unreported areas such as regions of extrastriate body area (EBA) tuned for legs/hands and sensitivity to numerosity in right intraparietal sulcus, and more.","Thus, our contrastive-learning methodology better characterizes new and existing visuo-semantic representations in the brain by leveraging multimodal neural network representations and a novel adaptation of clustering algorithms."],"url":"http://arxiv.org/abs/2405.17663v1","category":"cs.LG"}
{"created":"2024-05-27 20:57:19","title":"Alignment is Key for Applying Diffusion Models to Retrosynthesis","abstract":"Retrosynthesis, the task of identifying precursors for a given molecule, can be naturally framed as a conditional graph generation task. Diffusion models are a particularly promising modelling approach, enabling post-hoc conditioning and trading off quality for speed during generation. We show mathematically that permutation equivariant denoisers severely limit the expressiveness of graph diffusion models and thus their adaptation to retrosynthesis. To address this limitation, we relax the equivariance requirement such that it only applies to aligned permutations of the conditioning and the generated graphs obtained through atom mapping. Our new denoiser achieves the highest top-$1$ accuracy ($54.7$\\%) across template-free and template-based methods on USPTO-50k. We also demonstrate the ability for flexible post-training conditioning and good sample quality with small diffusion step counts, highlighting the potential for interactive applications and additional controls for multi-step planning.","sentences":["Retrosynthesis, the task of identifying precursors for a given molecule, can be naturally framed as a conditional graph generation task.","Diffusion models are a particularly promising modelling approach, enabling post-hoc conditioning and trading off quality for speed during generation.","We show mathematically that permutation equivariant denoisers severely limit the expressiveness of graph diffusion models and thus their adaptation to retrosynthesis.","To address this limitation, we relax the equivariance requirement such that it only applies to aligned permutations of the conditioning and the generated graphs obtained through atom mapping.","Our new denoiser achieves the highest top-$1$ accuracy ($54.7$\\%) across template-free and template-based methods on USPTO-50k.","We also demonstrate the ability for flexible post-training conditioning and good sample quality with small diffusion step counts, highlighting the potential for interactive applications and additional controls for multi-step planning."],"url":"http://arxiv.org/abs/2405.17656v1","category":"cs.LG"}
{"created":"2024-05-27 18:32:03","title":"Tleco: A Toolkit for Modeling Radiative Signatures from Relativistic Outflows","abstract":"A wide range of astrophysical sources exhibit extreme and rapidly varying electromagnetic emission indicative of efficient non-thermal particle acceleration. Understanding these sources often involves comparing data with a broad range of theoretical scenarios. To this end, it is beneficial to have tools that enable not only fast and efficient parametric investigation of the predictions of a specific scenario but also the flexibility to explore different theoretical ideas. In this paper, we introduce \\texttt{Tleco}, a versatile and lightweight toolkit for developing numerical models of relativistic outflows, including their particle acceleration mechanisms and resultant electromagnetic signature. Built on the Rust programming language and wrapped into a Python library, \\texttt{Tleco} offers efficient algorithms for evolving relativistic particle distributions and for solving the resulting emissions in a customizable fashion. \\texttt{Tleco} uses a fully implicit discretization algorithm to solve the Fokker-Planck (FP) equation with user-defined diffusion, advection, cooling, injection, and escape, and offers prescriptions for radiative emission and cooling. These include, but are not limited to, synchrotron, inverse-Compton, and self-synchrotron absorption. \\texttt{Tleco} is designed to be user-friendly and adaptable to model particle acceleration and the resulting electromagnetic spectrum and temporal variability in a wide variety of astrophysical scenarios, including, but not limited to, gamma-ray bursts, pulsar wind nebulae, and jets from active galactic nuclei. In this work, we outline the core algorithms and proceed to evaluate and demonstrate their effectiveness. The code is open-source and available in the GitHub repository: \\href{https://github.com/zkdavis/Tleco","sentences":["A wide range of astrophysical sources exhibit extreme and rapidly varying electromagnetic emission indicative of efficient non-thermal particle acceleration.","Understanding these sources often involves comparing data with a broad range of theoretical scenarios.","To this end, it is beneficial to have tools that enable not only fast and efficient parametric investigation of the predictions of a specific scenario but also the flexibility to explore different theoretical ideas.","In this paper, we introduce \\texttt{Tleco}, a versatile and lightweight toolkit for developing numerical models of relativistic outflows, including their particle acceleration mechanisms and resultant electromagnetic signature.","Built on the Rust programming language and wrapped into a Python library, \\texttt{Tleco} offers efficient algorithms for evolving relativistic particle distributions and for solving the resulting emissions in a customizable fashion.","\\texttt{Tleco} uses a fully implicit discretization algorithm to solve the Fokker-Planck (FP) equation with user-defined diffusion, advection, cooling, injection, and escape, and offers prescriptions for radiative emission and cooling.","These include, but are not limited to, synchrotron, inverse-Compton, and self-synchrotron absorption.","\\texttt{Tleco} is designed to be user-friendly and adaptable to model particle acceleration and the resulting electromagnetic spectrum and temporal variability in a wide variety of astrophysical scenarios, including, but not limited to, gamma-ray bursts, pulsar wind nebulae, and jets from active galactic nuclei.","In this work, we outline the core algorithms and proceed to evaluate and demonstrate their effectiveness.","The code is open-source and available in the GitHub repository: \\href{https://github.com/zkdavis/Tleco"],"url":"http://arxiv.org/abs/2405.17581v1","category":"astro-ph.HE"}
{"created":"2024-05-27 18:15:05","title":"Hamiltonian Mechanics of Feature Learning: Bottleneck Structure in Leaky ResNets","abstract":"We study Leaky ResNets, which interpolate between ResNets ($\\tilde{L}=0$) and Fully-Connected nets ($\\tilde{L}\\to\\infty$) depending on an 'effective depth' hyper-parameter $\\tilde{L}$. In the infinite depth limit, we study 'representation geodesics' $A_{p}$: continuous paths in representation space (similar to NeuralODEs) from input $p=0$ to output $p=1$ that minimize the parameter norm of the network. We give a Lagrangian and Hamiltonian reformulation, which highlight the importance of two terms: a kinetic energy which favors small layer derivatives $\\partial_{p}A_{p}$ and a potential energy that favors low-dimensional representations, as measured by the 'Cost of Identity'. The balance between these two forces offers an intuitive understanding of feature learning in ResNets. We leverage this intuition to explain the emergence of a bottleneck structure, as observed in previous work: for large $\\tilde{L}$ the potential energy dominates and leads to a separation of timescales, where the representation jumps rapidly from the high dimensional inputs to a low-dimensional representation, move slowly inside the space of low-dimensional representations, before jumping back to the potentially high-dimensional outputs. Inspired by this phenomenon, we train with an adaptive layer step-size to adapt to the separation of timescales.","sentences":["We study Leaky ResNets, which interpolate between ResNets ($\\tilde{L}=0$) and Fully-Connected nets ($\\tilde{L}\\to\\infty$) depending on an 'effective depth' hyper-parameter $\\tilde{L}$. In the infinite depth limit, we study 'representation geodesics' $A_{p}$: continuous paths in representation space (similar to NeuralODEs) from input $p=0$ to output $p=1$ that minimize the parameter norm of the network.","We give a Lagrangian and Hamiltonian reformulation, which highlight the importance of two terms: a kinetic energy which favors small layer derivatives $\\partial_{p}A_{p}$ and a potential energy that favors low-dimensional representations, as measured by the 'Cost of Identity'.","The balance between these two forces offers an intuitive understanding of feature learning in ResNets.","We leverage this intuition to explain the emergence of a bottleneck structure, as observed in previous work: for large $\\tilde{L}$ the potential energy dominates and leads to a separation of timescales, where the representation jumps rapidly from the high dimensional inputs to a low-dimensional representation, move slowly inside the space of low-dimensional representations, before jumping back to the potentially high-dimensional outputs.","Inspired by this phenomenon, we train with an adaptive layer step-size to adapt to the separation of timescales."],"url":"http://arxiv.org/abs/2405.17573v1","category":"stat.ML"}
{"created":"2024-05-27 18:03:00","title":"Strategic Code: A Unified Spatio-Temporal Framework for Quantum Error-Correction","abstract":"Quantum error-correcting code (QECC) is the central ingredient in fault-tolerant quantum information processing. An emerging paradigm of dynamical QECC shows that one can robustly encode logical quantum information both temporally and spatially in a more resource-efficient manner than traditional QECCs. Nevertheless, an overarching theory of how dynamical QECCs achieve fault-tolerance is lacking. In this work, we bridge this gap by proposing a unified spatio-temporal QECC framework called the ``strategic code'' built around an ``interrogator'' device which sequentially measures and evolves the spatial QECC in an adaptive manner based on the ``quantum combs'' formalism, a generalization of the channel-state duality. The strategic code covers all existing dynamical and static QECC, as well as all physically plausible QECCs to be discovered in the future, including those that involve adaptivity in its operational dynamics. Within this framework, we show an algebraic and an information-theoretic necessary and sufficient error-correction conditions for a strategic code, which consider spatially and temporally correlated errors. These conditions include the analogous known static QECC conditions as a special case. Lastly, we also propose an optimization-theoretic approach to obtain an approximate strategic code adapting to a correlated error.","sentences":["Quantum error-correcting code (QECC) is the central ingredient in fault-tolerant quantum information processing.","An emerging paradigm of dynamical QECC shows that one can robustly encode logical quantum information both temporally and spatially in a more resource-efficient manner than traditional QECCs.","Nevertheless, an overarching theory of how dynamical QECCs achieve fault-tolerance is lacking.","In this work, we bridge this gap by proposing a unified spatio-temporal QECC framework called the ``strategic code'' built around an ``interrogator'' device which sequentially measures and evolves the spatial QECC in an adaptive manner based on the ``quantum combs'' formalism, a generalization of the channel-state duality.","The strategic code covers all existing dynamical and static QECC, as well as all physically plausible QECCs to be discovered in the future, including those that involve adaptivity in its operational dynamics.","Within this framework, we show an algebraic and an information-theoretic necessary and sufficient error-correction conditions for a strategic code, which consider spatially and temporally correlated errors.","These conditions include the analogous known static QECC conditions as a special case.","Lastly, we also propose an optimization-theoretic approach to obtain an approximate strategic code adapting to a correlated error."],"url":"http://arxiv.org/abs/2405.17567v1","category":"quant-ph"}
{"created":"2024-05-27 17:59:23","title":"Hardness-Aware Scene Synthesis for Semi-Supervised 3D Object Detection","abstract":"3D object detection aims to recover the 3D information of concerning objects and serves as the fundamental task of autonomous driving perception. Its performance greatly depends on the scale of labeled training data, yet it is costly to obtain high-quality annotations for point cloud data. While conventional methods focus on generating pseudo-labels for unlabeled samples as supplements for training, the structural nature of 3D point cloud data facilitates the composition of objects and backgrounds to synthesize realistic scenes. Motivated by this, we propose a hardness-aware scene synthesis (HASS) method to generate adaptive synthetic scenes to improve the generalization of the detection models. We obtain pseudo-labels for unlabeled objects and generate diverse scenes with different compositions of objects and backgrounds. As the scene synthesis is sensitive to the quality of pseudo-labels, we further propose a hardness-aware strategy to reduce the effect of low-quality pseudo-labels and maintain a dynamic pseudo-database to ensure the diversity and quality of synthetic scenes. Extensive experimental results on the widely used KITTI and Waymo datasets demonstrate the superiority of the proposed HASS method, which outperforms existing semi-supervised learning methods on 3D object detection. Code: https://github.com/wzzheng/HASS.","sentences":["3D object detection aims to recover the 3D information of concerning objects and serves as the fundamental task of autonomous driving perception.","Its performance greatly depends on the scale of labeled training data, yet it is costly to obtain high-quality annotations for point cloud data.","While conventional methods focus on generating pseudo-labels for unlabeled samples as supplements for training, the structural nature of 3D point cloud data facilitates the composition of objects and backgrounds to synthesize realistic scenes.","Motivated by this, we propose a hardness-aware scene synthesis (HASS) method to generate adaptive synthetic scenes to improve the generalization of the detection models.","We obtain pseudo-labels for unlabeled objects and generate diverse scenes with different compositions of objects and backgrounds.","As the scene synthesis is sensitive to the quality of pseudo-labels, we further propose a hardness-aware strategy to reduce the effect of low-quality pseudo-labels and maintain a dynamic pseudo-database to ensure the diversity and quality of synthetic scenes.","Extensive experimental results on the widely used KITTI and Waymo datasets demonstrate the superiority of the proposed HASS method, which outperforms existing semi-supervised learning methods on 3D object detection.","Code: https://github.com/wzzheng/HASS."],"url":"http://arxiv.org/abs/2405.17422v1","category":"cs.CV"}
{"created":"2024-05-27 17:58:48","title":"Self-Corrected Multimodal Large Language Model for End-to-End Robot Manipulation","abstract":"Robot manipulation policies have shown unsatisfactory action performance when confronted with novel task or object instances. Hence, the capability to automatically detect and self-correct failure action is essential for a practical robotic system. Recently, Multimodal Large Language Models (MLLMs) have shown promise in visual instruction following and demonstrated strong reasoning abilities in various tasks. To unleash general MLLMs as an end-to-end robotic agent, we introduce a Self-Corrected (SC)-MLLM, equipping our model not only to predict end-effector poses but also to autonomously recognize and correct failure actions. Specifically, we first conduct parameter-efficient fine-tuning to empower MLLM with pose prediction ability, which is reframed as a language modeling problem. When facing execution failures, our model learns to identify low-level action error causes (i.e., position and rotation errors) and adaptively seeks prompt feedback from experts. Based on the feedback, SC-MLLM rethinks the current failure scene and generates the corrected actions. Furthermore, we design a continuous policy learning method for successfully corrected samples, enhancing the model's adaptability to the current scene configuration and reducing the frequency of expert intervention. To evaluate our SC-MLLM, we conduct extensive experiments in both simulation and real-world settings. SC-MLLM agent significantly improve manipulation accuracy compared to previous state-of-the-art robotic MLLM (ManipLLM), increasing from 57\\% to 79\\% on seen object categories and from 47\\% to 69\\% on unseen novel categories.","sentences":["Robot manipulation policies have shown unsatisfactory action performance when confronted with novel task or object instances.","Hence, the capability to automatically detect and self-correct failure action is essential for a practical robotic system.","Recently, Multimodal Large Language Models (MLLMs) have shown promise in visual instruction following and demonstrated strong reasoning abilities in various tasks.","To unleash general MLLMs as an end-to-end robotic agent, we introduce a Self-Corrected (SC)-MLLM, equipping our model not only to predict end-effector poses but also to autonomously recognize and correct failure actions.","Specifically, we first conduct parameter-efficient fine-tuning to empower MLLM with pose prediction ability, which is reframed as a language modeling problem.","When facing execution failures, our model learns to identify low-level action error causes (i.e., position and rotation errors) and adaptively seeks prompt feedback from experts.","Based on the feedback, SC-MLLM rethinks the current failure scene and generates the corrected actions.","Furthermore, we design a continuous policy learning method for successfully corrected samples, enhancing the model's adaptability to the current scene configuration and reducing the frequency of expert intervention.","To evaluate our SC-MLLM, we conduct extensive experiments in both simulation and real-world settings.","SC-MLLM agent significantly improve manipulation accuracy compared to previous state-of-the-art robotic MLLM (ManipLLM), increasing from 57\\% to 79\\% on seen object categories and from 47\\% to 69\\% on unseen novel categories."],"url":"http://arxiv.org/abs/2405.17418v1","category":"cs.CV"}
{"created":"2024-05-27 17:53:32","title":"SMR: State Memory Replay for Long Sequence Modeling","abstract":"Despite the promising performance of state space models (SSMs) in long sequence modeling, limitations still exist. Advanced SSMs like S5 and S6 (Mamba) in addressing non-uniform sampling, their recursive structures impede efficient SSM computation via convolution. To overcome compatibility limitations in parallel convolutional computation, this paper proposes a novel non-recursive non-uniform sample processing strategy. Theoretical analysis of SSMs through the lens of Event-Triggered Control (ETC) theory reveals the Non-Stable State (NSS) problem, where deviations from sampling point requirements lead to error transmission and accumulation, causing the divergence of the SSM's hidden state. Our analysis further reveals that adjustments of input sequences with early memories can mitigate the NSS problem, achieving Sampling Step Adaptation (SSA). Building on this insight, we introduce a simple yet effective plug-and-play mechanism, State Memory Replay (SMR), which utilizes learnable memories to adjust the current state with multi-step information for generalization at sampling points different from those in the training data. This enables SSMs to stably model varying sampling points. Experiments on long-range modeling tasks in autoregressive language modeling and Long Range Arena demonstrate the general effectiveness of the SMR mechanism for a series of SSM models.","sentences":["Despite the promising performance of state space models (SSMs) in long sequence modeling, limitations still exist.","Advanced SSMs like S5 and S6 (Mamba) in addressing non-uniform sampling, their recursive structures impede efficient SSM computation via convolution.","To overcome compatibility limitations in parallel convolutional computation, this paper proposes a novel non-recursive non-uniform sample processing strategy.","Theoretical analysis of SSMs through the lens of Event-Triggered Control (ETC) theory reveals the Non-Stable State (NSS) problem, where deviations from sampling point requirements lead to error transmission and accumulation, causing the divergence of the SSM's hidden state.","Our analysis further reveals that adjustments of input sequences with early memories can mitigate the NSS problem, achieving Sampling Step Adaptation (SSA).","Building on this insight, we introduce a simple yet effective plug-and-play mechanism, State Memory Replay (SMR), which utilizes learnable memories to adjust the current state with multi-step information for generalization at sampling points different from those in the training data.","This enables SSMs to stably model varying sampling points.","Experiments on long-range modeling tasks in autoregressive language modeling and Long Range Arena demonstrate the general effectiveness of the SMR mechanism for a series of SSM models."],"url":"http://arxiv.org/abs/2405.17534v1","category":"cs.LG"}
{"created":"2024-05-27 17:51:24","title":"THREAD: Thinking Deeper with Recursive Spawning","abstract":"Large language models (LLMs) have shown impressive capabilities across diverse settings, but still struggle as the length and complexity of the context increases. To address this challenge, we propose Thinking Recursively and Dynamically (ThReaD). THREAD frames model generation as a thread of execution that, based on the context, can run to completion or dynamically spawn new threads. By spawning, threads can offload work (e.g., thinking, retrieving information) to child threads, which only return tokens needed for the parent thread to do its work. In effect, this enables the model to adapt, as needed, the amount of intermediate work used to produce tokens. We apply THREAD in the settings of LLM task solving and question answering, where the dynamic threading allows the model to recursively decompose the given task or question into progressively simpler sub-problems that can be solved by separate child threads. We test THREAD, implemented using a few-shot learning approach, on diverse benchmarks for agent tasks and data-grounded question answering. THREAD achieves state-of-the-art performance with GPT-4 and GPT-3.5 on these benchmarks, including ALFWorld, TextCraft, and WebShop, along with two new benchmarks, DataCommons QA and MIMIC-III ICU QA. In addition, THREAD outperforms existing frameworks by 10% to 50% absolute points with smaller models, including Llama-3-8b and CodeLlama-7b.","sentences":["Large language models (LLMs) have shown impressive capabilities across diverse settings, but still struggle as the length and complexity of the context increases.","To address this challenge, we propose Thinking Recursively and Dynamically (ThReaD).","THREAD frames model generation as a thread of execution that, based on the context, can run to completion or dynamically spawn new threads.","By spawning, threads can offload work (e.g., thinking, retrieving information) to child threads, which only return tokens needed for the parent thread to do its work.","In effect, this enables the model to adapt, as needed, the amount of intermediate work used to produce tokens.","We apply THREAD in the settings of LLM task solving and question answering, where the dynamic threading allows the model to recursively decompose the given task or question into progressively simpler sub-problems that can be solved by separate child threads.","We test THREAD, implemented using a few-shot learning approach, on diverse benchmarks for agent tasks and data-grounded question answering.","THREAD achieves state-of-the-art performance with GPT-4 and GPT-3.5 on these benchmarks, including ALFWorld, TextCraft, and WebShop, along with two new benchmarks, DataCommons QA and MIMIC-III ICU QA.","In addition, THREAD outperforms existing frameworks by 10% to 50% absolute points with smaller models, including Llama-3-8b and CodeLlama-7b."],"url":"http://arxiv.org/abs/2405.17402v1","category":"cs.CL"}
{"created":"2024-05-27 17:51:08","title":"RB-Modulation: Training-Free Personalization of Diffusion Models using Stochastic Optimal Control","abstract":"We propose Reference-Based Modulation (RB-Modulation), a new plug-and-play solution for training-free personalization of diffusion models. Existing training-free approaches exhibit difficulties in (a) style extraction from reference images in the absence of additional style or content text descriptions, (b) unwanted content leakage from reference style images, and (c) effective composition of style and content. RB-Modulation is built on a novel stochastic optimal controller where a style descriptor encodes the desired attributes through a terminal cost. The resulting drift not only overcomes the difficulties above, but also ensures high fidelity to the reference style and adheres to the given text prompt. We also introduce a cross-attention-based feature aggregation scheme that allows RB-Modulation to decouple content and style from the reference image. With theoretical justification and empirical evidence, our framework demonstrates precise extraction and control of content and style in a training-free manner. Further, our method allows a seamless composition of content and style, which marks a departure from the dependency on external adapters or ControlNets.","sentences":["We propose Reference-Based Modulation (RB-Modulation), a new plug-and-play solution for training-free personalization of diffusion models.","Existing training-free approaches exhibit difficulties in (a) style extraction from reference images in the absence of additional style or content text descriptions, (b) unwanted content leakage from reference style images, and (c) effective composition of style and content.","RB-Modulation is built on a novel stochastic optimal controller where a style descriptor encodes the desired attributes through a terminal cost.","The resulting drift not only overcomes the difficulties above, but also ensures high fidelity to the reference style and adheres to the given text prompt.","We also introduce a cross-attention-based feature aggregation scheme that allows RB-Modulation to decouple content and style from the reference image.","With theoretical justification and empirical evidence, our framework demonstrates precise extraction and control of content and style in a training-free manner.","Further, our method allows a seamless composition of content and style, which marks a departure from the dependency on external adapters or ControlNets."],"url":"http://arxiv.org/abs/2405.17401v1","category":"cs.LG"}
{"created":"2024-05-27 17:46:22","title":"EASI-Tex: Edge-Aware Mesh Texturing from Single Image","abstract":"We present a novel approach for single-image mesh texturing, which employs a diffusion model with judicious conditioning to seamlessly transfer an object's texture from a single RGB image to a given 3D mesh object. We do not assume that the two objects belong to the same category, and even if they do, there can be significant discrepancies in their geometry and part proportions. Our method aims to rectify the discrepancies by conditioning a pre-trained Stable Diffusion generator with edges describing the mesh through ControlNet, and features extracted from the input image using IP-Adapter to generate textures that respect the underlying geometry of the mesh and the input texture without any optimization or training. We also introduce Image Inversion, a novel technique to quickly personalize the diffusion model for a single concept using a single image, for cases where the pre-trained IP-Adapter falls short in capturing all the details from the input image faithfully. Experimental results demonstrate the efficiency and effectiveness of our edge-aware single-image mesh texturing approach, coined EASI-Tex, in preserving the details of the input texture on diverse 3D objects, while respecting their geometry.","sentences":["We present a novel approach for single-image mesh texturing, which employs a diffusion model with judicious conditioning to seamlessly transfer an object's texture from a single RGB image to a given 3D mesh object.","We do not assume that the two objects belong to the same category, and even if they do, there can be significant discrepancies in their geometry and part proportions.","Our method aims to rectify the discrepancies by conditioning a pre-trained Stable Diffusion generator with edges describing the mesh through ControlNet, and features extracted from the input image using IP-Adapter to generate textures that respect the underlying geometry of the mesh and the input texture without any optimization or training.","We also introduce Image Inversion, a novel technique to quickly personalize the diffusion model for a single concept using a single image, for cases where the pre-trained IP-Adapter falls short in capturing all the details from the input image faithfully.","Experimental results demonstrate the efficiency and effectiveness of our edge-aware single-image mesh texturing approach, coined EASI-Tex, in preserving the details of the input texture on diverse 3D objects, while respecting their geometry."],"url":"http://arxiv.org/abs/2405.17393v1","category":"cs.CV"}
{"created":"2024-05-27 17:42:00","title":"Batteryless BLE and Light-based IoT Sensor Nodes for Reliable Environmental Sensing","abstract":"The sustainable design of Internet of Things (IoT) networks encompasses considerations related to energy efficiency and autonomy as well as considerations related to reliable communications, ensuring no energy is wasted on undelivered data. Under these considerations, this work proposes the design and implementation of energy-efficient Bluetooth Low Energy (BLE) and Light-based IoT (LIoT) batteryless IoT sensor nodes powered by an indoor light Energy Harvesting Unit (EHU). Our design intends to integrate these nodes into a sensing network to improve its reliability by combining both technologies and taking advantage of their features. The nodes incorporate state-of-the-art components, such as low-power sensors and efficient System-on-Chips (SoCs). Moreover, we design a strategy for adaptive switching between active and sleep cycles as a function of the available energy, allowing the IoT nodes to continuously operate without batteries. Our results show that by adapting the duty cycle of the BLE and LIoT nodes depending on the environment's light intensity, we can ensure a continuous and reliable node operation. In particular, measurements show that our proposed BLE and LIoT node designs are able to communicate with an IoT gateway in a bidirectional way, every 19.3 and 624.6 seconds, respectively, in an energy-autonomous and reliable manner.","sentences":["The sustainable design of Internet of Things (IoT) networks encompasses considerations related to energy efficiency and autonomy as well as considerations related to reliable communications, ensuring no energy is wasted on undelivered data.","Under these considerations, this work proposes the design and implementation of energy-efficient Bluetooth Low Energy (BLE) and Light-based IoT (LIoT) batteryless IoT sensor nodes powered by an indoor light Energy Harvesting Unit (EHU).","Our design intends to integrate these nodes into a sensing network to improve its reliability by combining both technologies and taking advantage of their features.","The nodes incorporate state-of-the-art components, such as low-power sensors and efficient System-on-Chips (SoCs).","Moreover, we design a strategy for adaptive switching between active and sleep cycles as a function of the available energy, allowing the IoT nodes to continuously operate without batteries.","Our results show that by adapting the duty cycle of the BLE and LIoT nodes depending on the environment's light intensity, we can ensure a continuous and reliable node operation.","In particular, measurements show that our proposed BLE and LIoT node designs are able to communicate with an IoT gateway in a bidirectional way, every 19.3 and 624.6 seconds, respectively, in an energy-autonomous and reliable manner."],"url":"http://arxiv.org/abs/2405.17387v1","category":"eess.SP"}
{"created":"2024-05-27 17:40:00","title":"Evolutive Rendering Models","abstract":"The landscape of computer graphics has undergone significant transformations with the recent advances of differentiable rendering models. These rendering models often rely on heuristic designs that may not fully align with the final rendering objectives. We address this gap by pioneering \\textit{evolutive rendering models}, a methodology where rendering models possess the ability to evolve and adapt dynamically throughout the rendering process. In particular, we present a comprehensive learning framework that enables the optimization of three principal rendering elements, including the gauge transformations, the ray sampling mechanisms, and the primitive organization. Central to this framework is the development of differentiable versions of these rendering elements, allowing for effective gradient backpropagation from the final rendering objectives. A detailed analysis of gradient characteristics is performed to facilitate a stable and goal-oriented elements evolution. Our extensive experiments demonstrate the large potential of evolutive rendering models for enhancing the rendering performance across various domains, including static and dynamic scene representations, generative modeling, and texture mapping.","sentences":["The landscape of computer graphics has undergone significant transformations with the recent advances of differentiable rendering models.","These rendering models often rely on heuristic designs that may not fully align with the final rendering objectives.","We address this gap by pioneering \\textit{evolutive rendering models}, a methodology where rendering models possess the ability to evolve and adapt dynamically throughout the rendering process.","In particular, we present a comprehensive learning framework that enables the optimization of three principal rendering elements, including the gauge transformations, the ray sampling mechanisms, and the primitive organization.","Central to this framework is the development of differentiable versions of these rendering elements, allowing for effective gradient backpropagation from the final rendering objectives.","A detailed analysis of gradient characteristics is performed to facilitate a stable and goal-oriented elements evolution.","Our extensive experiments demonstrate the large potential of evolutive rendering models for enhancing the rendering performance across various domains, including static and dynamic scene representations, generative modeling, and texture mapping."],"url":"http://arxiv.org/abs/2405.17531v1","category":"cs.CV"}
{"created":"2024-05-27 17:32:37","title":"Federating Dynamic Models using Early-Exit Architectures for Automatic Speech Recognition on Heterogeneous Clients","abstract":"Automatic speech recognition models require large amounts of speech recordings for training. However, the collection of such data often is cumbersome and leads to privacy concerns. Federated learning has been widely used as an effective decentralized technique that collaboratively learns a shared prediction model while keeping the data local on different clients. Unfortunately, client devices often feature limited computation and communication resources leading to practical difficulties for large models. In addition, the heterogeneity that characterizes edge devices makes it sub-optimal to generate a single model that fits all of them. Differently from the recent literature, where multiple models with different architectures are used, in this work, we propose using dynamical architectures which, employing early-exit solutions, can adapt their processing (i.e. traversed layers) depending on the input and on the operation conditions. This solution falls in the realm of partial training methods and brings two benefits: a single model is used on a variety of devices; federating the models after local training is straightforward. Experiments on public datasets show that our proposed approach is effective and can be combined with basic federated learning strategies.","sentences":["Automatic speech recognition models require large amounts of speech recordings for training.","However, the collection of such data often is cumbersome and leads to privacy concerns.","Federated learning has been widely used as an effective decentralized technique that collaboratively learns a shared prediction model while keeping the data local on different clients.","Unfortunately, client devices often feature limited computation and communication resources leading to practical difficulties for large models.","In addition, the heterogeneity that characterizes edge devices makes it sub-optimal to generate a single model that fits all of them.","Differently from the recent literature, where multiple models with different architectures are used, in this work, we propose using dynamical architectures which, employing early-exit solutions, can adapt their processing (i.e. traversed layers) depending on the input and on the operation conditions.","This solution falls in the realm of partial training methods and brings two benefits: a single model is used on a variety of devices; federating the models after local training is straightforward.","Experiments on public datasets show that our proposed approach is effective and can be combined with basic federated learning strategies."],"url":"http://arxiv.org/abs/2405.17376v1","category":"cs.CL"}
{"created":"2024-05-27 17:02:27","title":"DoRA: Enhancing Parameter-Efficient Fine-Tuning with Dynamic Rank Distribution","abstract":"Fine-tuning large-scale pre-trained models is inherently a resource-intensive task. While it can enhance the capabilities of the model, it also incurs substantial computational costs, posing challenges to the practical application of downstream tasks. Existing parameter-efficient fine-tuning (PEFT) methods such as Low-Rank Adaptation (LoRA) rely on a bypass framework that ignores the differential parameter budget requirements across weight matrices, which may lead to suboptimal fine-tuning outcomes. To address this issue, we introduce the Dynamic Low-Rank Adaptation (DoRA) method. DoRA decomposes high-rank LoRA layers into structured single-rank components, allowing for dynamic pruning of parameter budget based on their importance to specific tasks during training, which makes the most of the limited parameter budget. Experimental results demonstrate that DoRA can achieve competitive performance compared with LoRA and full model fine-tuning, and outperform various strong baselines with the same storage parameter budget. Our code is available at https://github.com/MIkumikumi0116/DoRA","sentences":["Fine-tuning large-scale pre-trained models is inherently a resource-intensive task.","While it can enhance the capabilities of the model, it also incurs substantial computational costs, posing challenges to the practical application of downstream tasks.","Existing parameter-efficient fine-tuning (PEFT) methods such as Low-Rank Adaptation (LoRA) rely on a bypass framework that ignores the differential parameter budget requirements across weight matrices, which may lead to suboptimal fine-tuning outcomes.","To address this issue, we introduce the Dynamic Low-Rank Adaptation (DoRA) method.","DoRA decomposes high-rank LoRA layers into structured single-rank components, allowing for dynamic pruning of parameter budget based on their importance to specific tasks during training, which makes the most of the limited parameter budget.","Experimental results demonstrate that DoRA can achieve competitive performance compared with LoRA and full model fine-tuning, and outperform various strong baselines with the same storage parameter budget.","Our code is available at https://github.com/MIkumikumi0116/DoRA"],"url":"http://arxiv.org/abs/2405.17357v2","category":"cs.CL"}
{"created":"2024-05-27 16:53:42","title":"Kondo-Zeno crossover in the dynamics of a monitored quantum dot","abstract":"We study the dynamics of a quantum dot coupled to a metallic bath and subject to continuous monitoring of its charge density. The dynamics averaged over measurement noise is described by a dissipative Anderson impurity model with local Markovian dephasing, that we solve using an extension of the Non-Crossing Approximation in the vectorized Hilbert space. We show that the decay time scale of an initially polarised spin which is suddenly coupled to the bath and to the monitoring protocol displays a crossover from Kondo screening, with a lifetime controlled by interactions, to Quantum Zeno effect, with a lifetime which decreases with bare dissipation as the dephasing or monitoring rate is increased. Using a Schrieffer-Wolff transformation on the Lindbladian we derive an effective model for the long-time dynamics which is described at weak dissipation by a non-Hermitian Kondo model with complex-valued spin-spin exchange. As the dephasing is increased heating due to doublon production takes over and control the spin decay.","sentences":["We study the dynamics of a quantum dot coupled to a metallic bath and subject to continuous monitoring of its charge density.","The dynamics averaged over measurement noise is described by a dissipative Anderson impurity model with local Markovian dephasing, that we solve using an extension of the Non-Crossing Approximation in the vectorized Hilbert space.","We show that the decay time scale of an initially polarised spin which is suddenly coupled to the bath and to the monitoring protocol displays a crossover from Kondo screening, with a lifetime controlled by interactions, to Quantum Zeno effect, with a lifetime which decreases with bare dissipation as the dephasing or monitoring rate is increased.","Using a Schrieffer-Wolff transformation on the Lindbladian we derive an effective model for the long-time dynamics which is described at weak dissipation by a non-Hermitian Kondo model with complex-valued spin-spin exchange.","As the dephasing is increased heating due to doublon production takes over and control the spin decay."],"url":"http://arxiv.org/abs/2405.17348v1","category":"cond-mat.str-el"}
{"created":"2024-05-27 16:42:51","title":"Physics-Informed Real NVP for Satellite Power System Fault Detection","abstract":"The unique challenges posed by the space environment, characterized by extreme conditions and limited accessibility, raise the need for robust and reliable techniques to identify and prevent satellite faults. Fault detection methods in the space sector are required to ensure mission success and to protect valuable assets. In this context, this paper proposes an Artificial Intelligence (AI) based fault detection methodology and evaluates its performance on ADAPT (Advanced Diagnostics and Prognostics Testbed), an Electrical Power System (EPS) dataset, crafted in laboratory by NASA.   Our study focuses on the application of a physics-informed (PI) real-valued non-volume preserving (Real NVP) model for fault detection in space systems. The efficacy of this method is systematically compared against other AI approaches such as Gated Recurrent Unit (GRU) and Autoencoder-based techniques.   Results show that our physics-informed approach outperforms existing methods of fault detection, demonstrating its suitability for addressing the unique challenges of satellite EPS sub-system faults. Furthermore, we unveil the competitive advantage of physics-informed loss in AI models to address specific space needs, namely robustness, reliability, and power constraints, crucial for space exploration and satellite missions.","sentences":["The unique challenges posed by the space environment, characterized by extreme conditions and limited accessibility, raise the need for robust and reliable techniques to identify and prevent satellite faults.","Fault detection methods in the space sector are required to ensure mission success and to protect valuable assets.","In this context, this paper proposes an Artificial Intelligence (AI) based fault detection methodology and evaluates its performance on ADAPT (Advanced Diagnostics and Prognostics Testbed), an Electrical Power System (EPS) dataset, crafted in laboratory by NASA.   ","Our study focuses on the application of a physics-informed (PI) real-valued non-volume preserving (Real NVP) model for fault detection in space systems.","The efficacy of this method is systematically compared against other AI approaches such as Gated Recurrent Unit (GRU) and Autoencoder-based techniques.   ","Results show that our physics-informed approach outperforms existing methods of fault detection, demonstrating its suitability for addressing the unique challenges of satellite EPS sub-system faults.","Furthermore, we unveil the competitive advantage of physics-informed loss in AI models to address specific space needs, namely robustness, reliability, and power constraints, crucial for space exploration and satellite missions."],"url":"http://arxiv.org/abs/2405.17339v1","category":"cs.LG"}
{"created":"2024-05-27 16:31:15","title":"Emergent time crystal from a fractional Langevin equation with white and colored noise","abstract":"We study the fractional Langevin equation with fractional $\\alpha$-order and linear friction terms of a system coupled to white and colored thermal baths using both analytical and numerical methods. We find analytical expressions for the position and the mean squared displacement (MSD) of the system using the Prabhakar-Mittag-Leffler function. The MSD exhibits long-term sub-diffusive regimes $t^{\\alpha}$ driven by colored noise and $t^{2\\alpha-1}$ driven by white noise. When the linear friction is neglected, periodic ordered phases emerge for small fractional orders $\\alpha \\lesssim 0.1$. In particular, the zero-linear friction system driven only by colored noise manifests the properties of a time crystal, with a ground state satisfying the fluctuation-dissipation theorem and a periodicity proportional to $2\\pi$. On the other hand, the zero-linear friction system driven only by white noise displays an out-of-equilibrium time glass phase with periodicity proportional to $\\pi$. A mixed phase with contributions from both ground and out-of-equilibrium states is encountered when the system couples to both baths. In that case, the periodicity deviates from $2\\pi$ due to damping effects. We test all the analytical results numerically by implementing a discrete recursive expression, where the random forces of the system are modeled as the derivative of the fractional Brownian motion. A microscopic description for the system is also provided by an extension of the Caldeira-Leggett model.","sentences":["We study the fractional Langevin equation with fractional $\\alpha$-order and linear friction terms of a system coupled to white and colored thermal baths using both analytical and numerical methods.","We find analytical expressions for the position and the mean squared displacement (MSD) of the system using the Prabhakar-Mittag-Leffler function.","The MSD exhibits long-term sub-diffusive regimes $t^{\\alpha}$ driven by colored noise and $t^{2\\alpha-1}$ driven by white noise.","When the linear friction is neglected, periodic ordered phases emerge for small fractional orders $\\alpha \\lesssim 0.1$.","In particular, the zero-linear friction system driven only by colored noise manifests the properties of a time crystal, with a ground state satisfying the fluctuation-dissipation theorem and a periodicity proportional to $2\\pi$. On the other hand, the zero-linear friction system driven only by white noise displays an out-of-equilibrium time glass phase with periodicity proportional to $\\pi$. A mixed phase with contributions from both ground and out-of-equilibrium states is encountered when the system couples to both baths.","In that case, the periodicity deviates from $2\\pi$ due to damping effects.","We test all the analytical results numerically by implementing a discrete recursive expression, where the random forces of the system are modeled as the derivative of the fractional Brownian motion.","A microscopic description for the system is also provided by an extension of the Caldeira-Leggett model."],"url":"http://arxiv.org/abs/2405.17331v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-27 16:22:38","title":"Tracking Small Birds by Detection Candidate Region Filtering and Detection History-aware Association","abstract":"This paper focuses on tracking birds that appear small in a panoramic video. When the size of the tracked object is small in the image (small object tracking) and move quickly, object detection and association suffers. To address these problems, we propose Adaptive Slicing Aided Hyper Inference (Adaptive SAHI), which reduces the candidate regions to apply detection, and Detection History-aware Similarity Criterion (DHSC), which accurately associates objects in consecutive frames based on the detection history. Experiments on the NUBird2022 dataset verifies the effectiveness of the proposed method by showing improvements in both accuracy and speed.","sentences":["This paper focuses on tracking birds that appear small in a panoramic video.","When the size of the tracked object is small in the image (small object tracking) and move quickly, object detection and association suffers.","To address these problems, we propose Adaptive Slicing Aided Hyper Inference (Adaptive SAHI), which reduces the candidate regions to apply detection, and Detection History-aware Similarity Criterion (DHSC), which accurately associates objects in consecutive frames based on the detection history.","Experiments on the NUBird2022 dataset verifies the effectiveness of the proposed method by showing improvements in both accuracy and speed."],"url":"http://arxiv.org/abs/2405.17323v1","category":"cs.CV"}
{"created":"2024-05-27 16:11:39","title":"Exact dynamics of quantum dissipative $XX$ models: Wannier-Stark localization in the fragmented operator space","abstract":"We address dissipative dynamics of the one-dimensional nearest-neighbour $XX$ spin-$1/2$ chain governed by the Gorini-Kossakowski-Sudarshan-Lindblad (GKSL) equation. In the absence of dissipation the model is integrable. We identify a broad class of dissipative terms that generically destroy integrability but leave the operator space of the model fragmented into an extensive number of dynamically disjoint subspaces of varying dimensions. In sufficiently small subspaces the GKSL equation in the Heisenberg representation can be easily solved, sometimes in a closed analytical form. We provide an example of such an exact solution for a specific choice of dissipative terms. It is found that observables experience the Wannier-Stark localization in the corresponding operator subspace. As a result, the expectation values of the observables are linear combinations of essentially a few discrete decay modes, the long time dynamics being governed by the slowest mode. We examine the complex Liouvillian eigenvalue corresponding to this latter mode as a function of the dissipation strength. We find an exceptional point at a critical dissipation strength that separates oscillating and non-oscillating decay. We also describe a different type of dissipation that leads to a single decay mode in the whole operator subspace. Finally, we point out that our exact solutions of the GKSL equation entail exact solutions of the Schr\\\"odinger equation describing the quench dynamics in closed spin ladders dual to the dissipative spin chains.","sentences":["We address dissipative dynamics of the one-dimensional nearest-neighbour $XX$ spin-$1/2$ chain governed by the Gorini-Kossakowski-Sudarshan-Lindblad (GKSL) equation.","In the absence of dissipation the model is integrable.","We identify a broad class of dissipative terms that generically destroy integrability but leave the operator space of the model fragmented into an extensive number of dynamically disjoint subspaces of varying dimensions.","In sufficiently small subspaces the GKSL equation in the Heisenberg representation can be easily solved, sometimes in a closed analytical form.","We provide an example of such an exact solution for a specific choice of dissipative terms.","It is found that observables experience the Wannier-Stark localization in the corresponding operator subspace.","As a result, the expectation values of the observables are linear combinations of essentially a few discrete decay modes, the long time dynamics being governed by the slowest mode.","We examine the complex Liouvillian eigenvalue corresponding to this latter mode as a function of the dissipation strength.","We find an exceptional point at a critical dissipation strength that separates oscillating and non-oscillating decay.","We also describe a different type of dissipation that leads to a single decay mode in the whole operator subspace.","Finally, we point out that our exact solutions of the GKSL equation entail exact solutions of the Schr\\\"odinger equation describing the quench dynamics in closed spin ladders dual to the dissipative spin chains."],"url":"http://arxiv.org/abs/2405.17310v1","category":"cond-mat.str-el"}
{"created":"2024-05-27 16:10:49","title":"Survey of Graph Neural Network for Internet of Things and NextG Networks","abstract":"The exponential increase in Internet of Things (IoT) devices coupled with 6G pushing towards higher data rates and connected devices has sparked a surge in data. Consequently, harnessing the full potential of data-driven machine learning has become one of the important thrusts. In addition to the advancement in wireless technology, it is important to efficiently use the resources available and meet the users' requirements. Graph Neural Networks (GNNs) have emerged as a promising paradigm for effectively modeling and extracting insights which inherently exhibit complex network structures due to its high performance and accuracy, scalability, adaptability, and resource efficiency. There is a lack of a comprehensive survey that focuses on the applications and advances GNN has made in the context of IoT and Next Generation (NextG) networks. To bridge that gap, this survey starts by providing a detailed description of GNN's terminologies, architecture, and the different types of GNNs. Then we provide a comprehensive survey of the advancements in applying GNNs for IoT from the perspective of data fusion and intrusion detection. Thereafter, we survey the impact GNN has made in improving spectrum awareness. Next, we provide a detailed account of how GNN has been leveraged for networking and tactical systems. Through this survey, we aim to provide a comprehensive resource for researchers to learn more about GNN in the context of wireless networks, and understand its state-of-the-art use cases while contrasting to other machine learning approaches. Finally, we also discussed the challenges and wide range of future research directions to further motivate the use of GNN for IoT and NextG Networks.","sentences":["The exponential increase in Internet of Things (IoT) devices coupled with 6G pushing towards higher data rates and connected devices has sparked a surge in data.","Consequently, harnessing the full potential of data-driven machine learning has become one of the important thrusts.","In addition to the advancement in wireless technology, it is important to efficiently use the resources available and meet the users' requirements.","Graph Neural Networks (GNNs) have emerged as a promising paradigm for effectively modeling and extracting insights which inherently exhibit complex network structures due to its high performance and accuracy, scalability, adaptability, and resource efficiency.","There is a lack of a comprehensive survey that focuses on the applications and advances GNN has made in the context of IoT and Next Generation (NextG) networks.","To bridge that gap, this survey starts by providing a detailed description of GNN's terminologies, architecture, and the different types of GNNs.","Then we provide a comprehensive survey of the advancements in applying GNNs for IoT from the perspective of data fusion and intrusion detection.","Thereafter, we survey the impact GNN has made in improving spectrum awareness.","Next, we provide a detailed account of how GNN has been leveraged for networking and tactical systems.","Through this survey, we aim to provide a comprehensive resource for researchers to learn more about GNN in the context of wireless networks, and understand its state-of-the-art use cases while contrasting to other machine learning approaches.","Finally, we also discussed the challenges and wide range of future research directions to further motivate the use of GNN for IoT and NextG Networks."],"url":"http://arxiv.org/abs/2405.17309v1","category":"cs.LG"}
{"created":"2024-05-27 15:52:53","title":"On The Implicit Large Eddy Simulation of Turbomachinery Flows Using The Flux Reconstruction Method","abstract":"A high order flux reconstruction solver has been developed and validated to perform implicit large eddy simulations of industrially representative turbomachinery flows. The T106c low-pressure turbine and VKI LS89 high-pressure turbine cases are studied. The solver uses the Rusanov Riemann solver to compute the inviscid fluxes on the wall boundaries, and HLLC or Roe to evaluate inviscid fluxes for internal faces. The impact of Riemann solvers is demonstrated in terms of accuracy and non-linear stability for turbomachinery flows. It is found that HLLC is more robust than Roe, but both Riemann solvers produce very similar results if stable solutions can be obtained. For non-linear stabilization, a local modal filter, which combines a smooth indicator and a modal filter, is used to stabilize the solution. This approach requires a tuning parameter for the smoothness criterion. Detailed analysis has been provided to guide the selection of a suitable value for different spatial orders of accuracy. This local-modal filter is also compared with the recent positivity-preserving entropy filter in terms of accuracy and stability for the LS89 turbine case. The entropy filter could stabilize the computation but is more dissipative than the local modal filter. Regarding the spanwise spacing of the grid, the case of the LS89 turbine shows that a $z^+$ of approximately $45 - 60$ is suitable for obtaining a satisfactory prediction of the heat transfer coefficient of the mean flow. This would allow for a coarse grid spacing in the spanwise direction and a cost-effective ILES aerothermal simulation for turbomachinery flows.","sentences":["A high order flux reconstruction solver has been developed and validated to perform implicit large eddy simulations of industrially representative turbomachinery flows.","The T106c low-pressure turbine and VKI LS89 high-pressure turbine cases are studied.","The solver uses the Rusanov Riemann solver to compute the inviscid fluxes on the wall boundaries, and HLLC or Roe to evaluate inviscid fluxes for internal faces.","The impact of Riemann solvers is demonstrated in terms of accuracy and non-linear stability for turbomachinery flows.","It is found that HLLC is more robust than Roe, but both Riemann solvers produce very similar results if stable solutions can be obtained.","For non-linear stabilization, a local modal filter, which combines a smooth indicator and a modal filter, is used to stabilize the solution.","This approach requires a tuning parameter for the smoothness criterion.","Detailed analysis has been provided to guide the selection of a suitable value for different spatial orders of accuracy.","This local-modal filter is also compared with the recent positivity-preserving entropy filter in terms of accuracy and stability for the LS89 turbine case.","The entropy filter could stabilize the computation but is more dissipative than the local modal filter.","Regarding the spanwise spacing of the grid, the case of the LS89 turbine shows that a $z^+$ of approximately $45 - 60$ is suitable for obtaining a satisfactory prediction of the heat transfer coefficient of the mean flow.","This would allow for a coarse grid spacing in the spanwise direction and a cost-effective ILES aerothermal simulation for turbomachinery flows."],"url":"http://arxiv.org/abs/2405.17288v1","category":"physics.flu-dyn"}
{"created":"2024-05-27 15:44:06","title":"A Library for Automatic Natural Language Generation of Spanish Texts","abstract":"In this article we present a novel system for natural language generation (NLG) of Spanish sentences from a minimum set of meaningful words (such as nouns, verbs and adjectives) which, unlike other state-of-the-art solutions, performs the NLG task in a fully automatic way, exploiting both knowledge-based and statistical approaches. Relying on its linguistic knowledge of vocabulary and grammar, the system is able to generate complete, coherent and correctly spelled sentences from the main word sets presented by the user. The system, which was designed to be integrable, portable and efficient, can be easily adapted to other languages by design and can feasibly be integrated in a wide range of digital devices. During its development we also created a supplementary lexicon for Spanish, aLexiS, with wide coverage and high precision, as well as syntactic trees from a freely available definite-clause grammar. The resulting NLG library has been evaluated both automatically and manually (annotation). The system can potentially be used in different application domains such as augmentative communication and automatic generation of administrative reports or news.","sentences":["In this article we present a novel system for natural language generation (NLG) of Spanish sentences from a minimum set of meaningful words (such as nouns, verbs and adjectives) which, unlike other state-of-the-art solutions, performs the NLG task in a fully automatic way, exploiting both knowledge-based and statistical approaches.","Relying on its linguistic knowledge of vocabulary and grammar, the system is able to generate complete, coherent and correctly spelled sentences from the main word sets presented by the user.","The system, which was designed to be integrable, portable and efficient, can be easily adapted to other languages by design and can feasibly be integrated in a wide range of digital devices.","During its development we also created a supplementary lexicon for Spanish, aLexiS, with wide coverage and high precision, as well as syntactic trees from a freely available definite-clause grammar.","The resulting NLG library has been evaluated both automatically and manually (annotation).","The system can potentially be used in different application domains such as augmentative communication and automatic generation of administrative reports or news."],"url":"http://arxiv.org/abs/2405.17280v1","category":"cs.CL"}
{"created":"2024-05-27 15:15:08","title":"$\\textit{Trans-LoRA}$: towards data-free Transferable Parameter Efficient Finetuning","abstract":"Low-rank adapters (LoRA) and their variants are popular parameter-efficient fine-tuning (PEFT) techniques that closely match full model fine-tune performance while requiring only a small number of additional parameters. These additional LoRA parameters are specific to the base model being adapted. When the base model needs to be deprecated and replaced with a new one, all the associated LoRA modules need to be re-trained. Such re-training requires access to the data used to train the LoRA for the original base model. This is especially problematic for commercial cloud applications where the LoRA modules and the base models are hosted by service providers who may not be allowed to host proprietary client task data. To address this challenge, we propose $\\textit{Trans-LoRA}$ -- a novel method for lossless, nearly data-free transfer of LoRAs across base models. Our approach relies on synthetic data to transfer LoRA modules. Using large language models, we design a synthetic data generator to approximate the data-generating process of the $\\textit{observed}$ task data subset. Training on the resulting synthetic dataset transfers LoRA modules to new models. We show the effectiveness of our approach using both LLama and Gemma model families. Our approach achieves lossless (mostly improved) LoRA transfer between models within and across different base model families, and even between different PEFT methods, on a wide variety of tasks.","sentences":["Low-rank adapters (LoRA) and their variants are popular parameter-efficient fine-tuning (PEFT) techniques that closely match full model fine-tune performance while requiring only a small number of additional parameters.","These additional LoRA parameters are specific to the base model being adapted.","When the base model needs to be deprecated and replaced with a new one, all the associated LoRA modules need to be re-trained.","Such re-training requires access to the data used to train the LoRA for the original base model.","This is especially problematic for commercial cloud applications where the LoRA modules and the base models are hosted by service providers who may not be allowed to host proprietary client task data.","To address this challenge, we propose $\\textit{Trans-LoRA}$ -- a novel method for lossless, nearly data-free transfer of LoRAs across base models.","Our approach relies on synthetic data to transfer LoRA modules.","Using large language models, we design a synthetic data generator to approximate the data-generating process of the $\\textit{observed}$ task data subset.","Training on the resulting synthetic dataset transfers LoRA modules to new models.","We show the effectiveness of our approach using both LLama and Gemma model families.","Our approach achieves lossless (mostly improved)","LoRA transfer between models within and across different base model families, and even between different PEFT methods, on a wide variety of tasks."],"url":"http://arxiv.org/abs/2405.17258v1","category":"cs.LG"}
{"created":"2024-05-27 15:10:26","title":"Classifying post-Minkowskian geometries for gravitational waves via loop-by-loop Baikov","abstract":"We use the loop-by-loop Baikov representation to investigate the geometries in Feynman integrals contributing to the classical dynamics of a black-hole two-body system in the post-Minkowskian expansion of general relativity. These geometries determine the spaces of functions to which the corresponding Feynman diagrams evaluate. As a proof of principle, we provide a full classification of the geometries appearing up to three loops, i.e. fourth post-Minkowskian order, for all diagrams relevant to the conservative as well as the dissipative dynamics, finding full agreement with the literature. Moreover, we show that the non-planar top topology at four loops, which is the most complicated sector with respect to integration-byparts identities, has an algebraic leading singularity and thus can only depend on non-trivial geometries through its subsectors.","sentences":["We use the loop-by-loop Baikov representation to investigate the geometries in Feynman integrals contributing to the classical dynamics of a black-hole two-body system in the post-Minkowskian expansion of general relativity.","These geometries determine the spaces of functions to which the corresponding Feynman diagrams evaluate.","As a proof of principle, we provide a full classification of the geometries appearing up to three loops, i.e. fourth post-Minkowskian order, for all diagrams relevant to the conservative as well as the dissipative dynamics, finding full agreement with the literature.","Moreover, we show that the non-planar top topology at four loops, which is the most complicated sector with respect to integration-byparts identities, has an algebraic leading singularity and thus can only depend on non-trivial geometries through its subsectors."],"url":"http://arxiv.org/abs/2405.17255v1","category":"hep-th"}
{"created":"2024-05-27 14:58:24","title":"Surprise-Adaptive Intrinsic Motivation for Unsupervised Reinforcement Learning","abstract":"Both entropy-minimizing and entropy-maximizing (curiosity) objectives for unsupervised reinforcement learning (RL) have been shown to be effective in different environments, depending on the environment's level of natural entropy. However, neither method alone results in an agent that will consistently learn intelligent behavior across environments. In an effort to find a single entropy-based method that will encourage emergent behaviors in any environment, we propose an agent that can adapt its objective online, depending on the entropy conditions by framing the choice as a multi-armed bandit problem. We devise a novel intrinsic feedback signal for the bandit, which captures the agent's ability to control the entropy in its environment. We demonstrate that such agents can learn to control entropy and exhibit emergent behaviors in both high- and low-entropy regimes and can learn skillful behaviors in benchmark tasks. Videos of the trained agents and summarized findings can be found on our project page https://sites.google.com/view/surprise-adaptive-agents","sentences":["Both entropy-minimizing and entropy-maximizing (curiosity) objectives for unsupervised reinforcement learning (RL) have been shown to be effective in different environments, depending on the environment's level of natural entropy.","However, neither method alone results in an agent that will consistently learn intelligent behavior across environments.","In an effort to find a single entropy-based method that will encourage emergent behaviors in any environment, we propose an agent that can adapt its objective online, depending on the entropy conditions by framing the choice as a multi-armed bandit problem.","We devise a novel intrinsic feedback signal for the bandit, which captures the agent's ability to control the entropy in its environment.","We demonstrate that such agents can learn to control entropy and exhibit emergent behaviors in both high- and low-entropy regimes and can learn skillful behaviors in benchmark tasks.","Videos of the trained agents and summarized findings can be found on our project page https://sites.google.com/view/surprise-adaptive-agents"],"url":"http://arxiv.org/abs/2405.17243v1","category":"cs.LG"}
{"created":"2024-05-27 14:49:39","title":"CLAQ: Pushing the Limits of Low-Bit Post-Training Quantization for LLMs","abstract":"Parameter quantization for Large Language Models (LLMs) has attracted increasing attentions recently in reducing memory costs and improving computational efficiency. Early approaches have been widely adopted. However, the existing methods suffer from poor performance in low-bit (such as 2 to 3 bits) scenarios. In this paper, we present a novel and effective Column-Level Adaptive weight Quantization (CLAQ) framework by introducing three different types of adaptive strategies for LLM quantization. Firstly, a K-Means clustering based algorithm is proposed that allows dynamic generation of quantization centroids for each column of a parameter matrix. Secondly, we design an outlier-guided adaptive precision search strategy which can dynamically assign varying bit-widths to different columns. Finally, a dynamic outlier reservation scheme is developed to retain some parameters in their original float point precision, in trade off of boosted model performance. Experiments on various mainstream open source LLMs including LLaMA-1, LLaMA-2 and Yi demonstrate that our methods achieve the state-of-the-art results across different bit settings, especially in extremely low-bit scenarios. Code will be released soon.","sentences":["Parameter quantization for Large Language Models (LLMs) has attracted increasing attentions recently in reducing memory costs and improving computational efficiency.","Early approaches have been widely adopted.","However, the existing methods suffer from poor performance in low-bit (such as 2 to 3 bits) scenarios.","In this paper, we present a novel and effective Column-Level Adaptive weight Quantization (CLAQ) framework by introducing three different types of adaptive strategies for LLM quantization.","Firstly, a K-Means clustering based algorithm is proposed that allows dynamic generation of quantization centroids for each column of a parameter matrix.","Secondly, we design an outlier-guided adaptive precision search strategy which can dynamically assign varying bit-widths to different columns.","Finally, a dynamic outlier reservation scheme is developed to retain some parameters in their original float point precision, in trade off of boosted model performance.","Experiments on various mainstream open source LLMs including LLaMA-1, LLaMA-2 and Yi demonstrate that our methods achieve the state-of-the-art results across different bit settings, especially in extremely low-bit scenarios.","Code will be released soon."],"url":"http://arxiv.org/abs/2405.17233v1","category":"cs.LG"}
{"created":"2024-05-27 14:44:07","title":"Learning Generic and Dynamic Locomotion of Humanoids Across Discrete Terrains","abstract":"This paper addresses the challenge of terrain-adaptive dynamic locomotion in humanoid robots, a problem traditionally tackled by optimization-based methods or reinforcement learning (RL). Optimization-based methods, such as model-predictive control, excel in finding optimal reaction forces and achieving agile locomotion, especially in quadruped, but struggle with the nonlinear hybrid dynamics of legged systems and the real-time computation of step location, timing, and reaction forces. Conversely, RL-based methods show promise in navigating dynamic and rough terrains but are limited by their extensive data requirements. We introduce a novel locomotion architecture that integrates a neural network policy, trained through RL in simplified environments, with a state-of-the-art motion controller combining model-predictive control (MPC) and whole-body impulse control (WBIC). The policy efficiently learns high-level locomotion strategies, such as gait selection and step positioning, without the need for full dynamics simulations. This control architecture enables humanoid robots to dynamically navigate discrete terrains, making strategic locomotion decisions (e.g., walking, jumping, and leaping) based on ground height maps. Our results demonstrate that this integrated control architecture achieves dynamic locomotion with significantly fewer training samples than conventional RL-based methods and can be transferred to different humanoid platforms without additional training. The control architecture has been extensively tested in dynamic simulations, accomplishing terrain height-based dynamic locomotion for three different robots.","sentences":["This paper addresses the challenge of terrain-adaptive dynamic locomotion in humanoid robots, a problem traditionally tackled by optimization-based methods or reinforcement learning (RL).","Optimization-based methods, such as model-predictive control, excel in finding optimal reaction forces and achieving agile locomotion, especially in quadruped, but struggle with the nonlinear hybrid dynamics of legged systems and the real-time computation of step location, timing, and reaction forces.","Conversely, RL-based methods show promise in navigating dynamic and rough terrains but are limited by their extensive data requirements.","We introduce a novel locomotion architecture that integrates a neural network policy, trained through RL in simplified environments, with a state-of-the-art motion controller combining model-predictive control (MPC) and whole-body impulse control (WBIC).","The policy efficiently learns high-level locomotion strategies, such as gait selection and step positioning, without the need for full dynamics simulations.","This control architecture enables humanoid robots to dynamically navigate discrete terrains, making strategic locomotion decisions (e.g., walking, jumping, and leaping) based on ground height maps.","Our results demonstrate that this integrated control architecture achieves dynamic locomotion with significantly fewer training samples than conventional RL-based methods and can be transferred to different humanoid platforms without additional training.","The control architecture has been extensively tested in dynamic simulations, accomplishing terrain height-based dynamic locomotion for three different robots."],"url":"http://arxiv.org/abs/2405.17227v1","category":"cs.RO"}
{"created":"2024-05-27 14:42:04","title":"Quantifying the Reliance of Black-Box Decision-Makers on Variables of Interest","abstract":"This paper introduces a framework for measuring how much black-box decision-makers rely on variables of interest. The framework adapts a permutation-based measure of variable importance from the explainable machine learning literature. With an emphasis on applicability, I present some of the framework's theoretical and computational properties, explain how reliance computations have policy implications, and work through an illustrative example. In the empirical application to interruptions by Supreme Court Justices during oral argument, I find that the effect of gender is more muted compared to the existing literature's estimate; I then use this paper's framework to compare Justices' reliance on gender and alignment to their reliance on experience, which are incomparable using regression coefficients.","sentences":["This paper introduces a framework for measuring how much black-box decision-makers rely on variables of interest.","The framework adapts a permutation-based measure of variable importance from the explainable machine learning literature.","With an emphasis on applicability, I present some of the framework's theoretical and computational properties, explain how reliance computations have policy implications, and work through an illustrative example.","In the empirical application to interruptions by Supreme Court Justices during oral argument, I find that the effect of gender is more muted compared to the existing literature's estimate; I then use this paper's framework to compare Justices' reliance on gender and alignment to their reliance on experience, which are incomparable using regression coefficients."],"url":"http://arxiv.org/abs/2405.17225v1","category":"econ.EM"}
{"created":"2024-05-27 14:40:03","title":"A Retrospective of the Tutorial on Opportunities and Challenges of Online Deep Learning","abstract":"Machine learning algorithms have become indispensable in today's world. They support and accelerate the way we make decisions based on the data at hand. This acceleration means that data structures that were valid at one moment could no longer be valid in the future. With these changing data structures, it is necessary to adapt machine learning (ML) systems incrementally to the new data. This is done with the use of online learning or continuous ML technologies. While deep learning technologies have shown exceptional performance on predefined datasets, they have not been widely applied to online, streaming, and continuous learning. In this retrospective of our tutorial titled Opportunities and Challenges of Online Deep Learning held at ECML PKDD 2023, we provide a brief overview of the opportunities but also the potential pitfalls for the application of neural networks in online learning environments using the frameworks River and Deep-River.","sentences":["Machine learning algorithms have become indispensable in today's world.","They support and accelerate the way we make decisions based on the data at hand.","This acceleration means that data structures that were valid at one moment could no longer be valid in the future.","With these changing data structures, it is necessary to adapt machine learning (ML) systems incrementally to the new data.","This is done with the use of online learning or continuous ML technologies.","While deep learning technologies have shown exceptional performance on predefined datasets, they have not been widely applied to online, streaming, and continuous learning.","In this retrospective of our tutorial titled Opportunities and Challenges of Online Deep Learning held at ECML PKDD 2023, we provide a brief overview of the opportunities but also the potential pitfalls for the application of neural networks in online learning environments using the frameworks River and Deep-River."],"url":"http://arxiv.org/abs/2405.17222v2","category":"cs.LG"}
{"created":"2024-05-27 14:35:03","title":"Modelling between- and within-season trajectories in elite athletic performance data","abstract":"Athletic performance follows a typical pattern of improvement and decline during a career. This pattern is also often observed within-seasons as athlete aims for their performance to peak at key events such as the Olympic Games or World Championships. A Bayesian hierarchical model is developed to analyse the evolution of athletic sporting performance throughout an athlete's career and separate these effects whilst allowing for confounding factors such as environmental conditions. Our model works in continuous time and estimates both the average performance level of the population, $g(t)$, at age $t$ and how each $i$-th athlete differs from the average $f_i(t)$. We further decompose $f_i(t)$ into changes from season-to-season, termed the between-season performance trajectory, and within-season performance trajectories which are modelled by a constrained Bernstein polynomial. Hence, the specific focus of this project is to identify the differences in performance that exist both between and within-seasons for each athlete. For the implementation of the model an adaptive Metropolis-within-Gibbs algorithm is used. An illustration of algorithm's performance on 100 metres and 200 metres freestyle swimming in both female and male athletes is presented.","sentences":["Athletic performance follows a typical pattern of improvement and decline during a career.","This pattern is also often observed within-seasons as athlete aims for their performance to peak at key events such as the Olympic Games or World Championships.","A Bayesian hierarchical model is developed to analyse the evolution of athletic sporting performance throughout an athlete's career and separate these effects whilst allowing for confounding factors such as environmental conditions.","Our model works in continuous time and estimates both the average performance level of the population, $g(t)$, at age $t$ and how each $i$-th athlete differs from the average $f_i(t)$. We further decompose $f_i(t)$ into changes from season-to-season, termed the between-season performance trajectory, and within-season performance trajectories which are modelled by a constrained Bernstein polynomial.","Hence, the specific focus of this project is to identify the differences in performance that exist both between and within-seasons for each athlete.","For the implementation of the model an adaptive Metropolis-within-Gibbs algorithm is used.","An illustration of algorithm's performance on 100 metres and 200 metres freestyle swimming in both female and male athletes is presented."],"url":"http://arxiv.org/abs/2405.17214v1","category":"stat.AP"}
{"created":"2024-05-27 14:30:09","title":"Flow control of three-dimensional cylinders transitioning to turbulence via multi-agent reinforcement learning","abstract":"Designing active-flow-control (AFC) strategies for three-dimensional (3D) bluff bodies is a challenging task with critical industrial implications. In this study we explore the potential of discovering novel control strategies for drag reduction using deep reinforcement learning. We introduce a high-dimensional AFC setup on a 3D cylinder, considering Reynolds numbers ($Re_D$) from $100$ to $400$, which is a range including the transition to 3D wake instabilities. The setup involves multiple zero-net-mass-flux jets positioned on the top and bottom surfaces, aligned into two slots. The method relies on coupling the computational-fluid-dynamics solver with a multi-agent reinforcement-learning (MARL) framework based on the proximal-policy-optimization algorithm. MARL offers several advantages: it exploits local invariance, adaptable control across geometries, facilitates transfer learning and cross-application of agents, and results in a significant training speedup. For instance, our results demonstrate $21\\%$ drag reduction for $Re_D=300$, outperforming classical periodic control, which yields up to $6\\%$ reduction. To the authors' knowledge, the present MARL-based framework represents the first time where training is conducted in 3D cylinders. This breakthrough paves the way for conducting AFC on progressively more complex turbulent-flow configurations.","sentences":["Designing active-flow-control (AFC) strategies for three-dimensional (3D) bluff bodies is a challenging task with critical industrial implications.","In this study we explore the potential of discovering novel control strategies for drag reduction using deep reinforcement learning.","We introduce a high-dimensional AFC setup on a 3D cylinder, considering Reynolds numbers ($Re_D$) from $100$ to $400$, which is a range including the transition to 3D wake instabilities.","The setup involves multiple zero-net-mass-flux jets positioned on the top and bottom surfaces, aligned into two slots.","The method relies on coupling the computational-fluid-dynamics solver with a multi-agent reinforcement-learning (MARL) framework based on the proximal-policy-optimization algorithm.","MARL offers several advantages: it exploits local invariance, adaptable control across geometries, facilitates transfer learning and cross-application of agents, and results in a significant training speedup.","For instance, our results demonstrate $21\\%$ drag reduction for $Re_D=300$, outperforming classical periodic control, which yields up to $6\\%$ reduction.","To the authors' knowledge, the present MARL-based framework represents the first time where training is conducted in 3D cylinders.","This breakthrough paves the way for conducting AFC on progressively more complex turbulent-flow configurations."],"url":"http://arxiv.org/abs/2405.17210v1","category":"physics.flu-dyn"}
{"created":"2024-05-27 14:25:05","title":"Numerical solution of the boundary value problem of elliptic equation by Levi function scheme","abstract":"For boundary value problem of an elliptic equation with variable coefficients describing the physical field distribution in inhomogeneous media, the Levi function can represent the solution in terms of volume and surface potentials, with the drawback that the volume potential involving in the solution expression requires heavy computational costs as well as the solvability of the integral equations with respect to the density pair. We introduce an modified integral expression for the solution to an elliptic equation in divergence form under the Levi function framework. The well-posedness of the linear integral system with respect to the density functions to be determined is rigorously proved. Based on the singularity decomposition for the Levi function, we propose two schemes to deal with the volume integrals so that the density functions can be solved efficiently. One method is an adaptive discretization scheme (ADS) for computing the integrals with continuous integrands, leading to the uniform accuracy of the integrals in the whole domain, and consequently the efficient computations for the density functions. The other method is the dual reciprocity method (DRM) which is a meshless approach converting the volume integrals into boundary integrals equivalently by expressing the volume density as the combination of the radial basis functions determined by the interior grids. The proposed schemes are justified numerically to be of satisfactory computation costs. Numerical examples in 2-dimensional and 3-dimensional cases are presented to show the validity of the proposed schemes.","sentences":["For boundary value problem of an elliptic equation with variable coefficients describing the physical field distribution in inhomogeneous media, the Levi function can represent the solution in terms of volume and surface potentials, with the drawback that the volume potential involving in the solution expression requires heavy computational costs as well as the solvability of the integral equations with respect to the density pair.","We introduce an modified integral expression for the solution to an elliptic equation in divergence form under the Levi function framework.","The well-posedness of the linear integral system with respect to the density functions to be determined is rigorously proved.","Based on the singularity decomposition for the Levi function, we propose two schemes to deal with the volume integrals so that the density functions can be solved efficiently.","One method is an adaptive discretization scheme (ADS) for computing the integrals with continuous integrands, leading to the uniform accuracy of the integrals in the whole domain, and consequently the efficient computations for the density functions.","The other method is the dual reciprocity method (DRM) which is a meshless approach converting the volume integrals into boundary integrals equivalently by expressing the volume density as the combination of the radial basis functions determined by the interior grids.","The proposed schemes are justified numerically to be of satisfactory computation costs.","Numerical examples in 2-dimensional and 3-dimensional cases are presented to show the validity of the proposed schemes."],"url":"http://arxiv.org/abs/2405.17204v1","category":"math.NA"}
{"created":"2024-05-27 14:20:37","title":"Physically Consistent Modeling & Identification of Nonlinear Friction with Dissipative Gaussian Processes","abstract":"Friction modeling has always been a challenging problem due to the complexity of real physical systems. Although a few state-of-the-art structured data-driven methods show their efficiency in nonlinear system modeling, deterministic passivity as one of the significant characteristics of friction is rarely considered in these methods. To address this issue, we propose a Gaussian Process based model that preserves the inherent structural properties such as passivity. A matrix-vector physical structure is considered in our approaches to ensure physical consistency, in particular, enabling a guarantee of positive semi-definiteness of the damping matrix. An aircraft benchmark simulation is employed to demonstrate the efficacy of our methodology. Estimation accuracy and data efficiency are increased substantially by considering and enforcing more structured physical knowledge. Also, the fulfillment of the dissipative nature of the aerodynamics is validated numerically.","sentences":["Friction modeling has always been a challenging problem due to the complexity of real physical systems.","Although a few state-of-the-art structured data-driven methods show their efficiency in nonlinear system modeling, deterministic passivity as one of the significant characteristics of friction is rarely considered in these methods.","To address this issue, we propose a Gaussian Process based model that preserves the inherent structural properties such as passivity.","A matrix-vector physical structure is considered in our approaches to ensure physical consistency, in particular, enabling a guarantee of positive semi-definiteness of the damping matrix.","An aircraft benchmark simulation is employed to demonstrate the efficacy of our methodology.","Estimation accuracy and data efficiency are increased substantially by considering and enforcing more structured physical knowledge.","Also, the fulfillment of the dissipative nature of the aerodynamics is validated numerically."],"url":"http://arxiv.org/abs/2405.17199v1","category":"eess.SY"}
{"created":"2024-05-27 14:17:36","title":"Anisotropic Gauss Reconstruction for Unoriented Point Clouds","abstract":"Unoriented surface reconstructions based on the Gauss formula have attracted much attention due to their elegant mathematical formulation and excellent performance. However, the isotropic characteristics of the formulation limit their capacity to leverage the anisotropic information within the point cloud. In this work, we propose a novel anisotropic formulation by introducing a convection term in the original Laplace operator. By choosing different velocity vectors, the anisotropic feature can be exploited to construct more effective linear equations. Moreover, an adaptive selection strategy is introduced for the velocity vector to further enhance the orientation and reconstruction performance of thin structures. Extensive experiments demonstrate that our method achieves state-of-the-art performance and manages various challenging situations, especially for models with thin structures or small holes. The source code will be released on GitHub.","sentences":["Unoriented surface reconstructions based on the Gauss formula have attracted much attention due to their elegant mathematical formulation and excellent performance.","However, the isotropic characteristics of the formulation limit their capacity to leverage the anisotropic information within the point cloud.","In this work, we propose a novel anisotropic formulation by introducing a convection term in the original Laplace operator.","By choosing different velocity vectors, the anisotropic feature can be exploited to construct more effective linear equations.","Moreover, an adaptive selection strategy is introduced for the velocity vector to further enhance the orientation and reconstruction performance of thin structures.","Extensive experiments demonstrate that our method achieves state-of-the-art performance and manages various challenging situations, especially for models with thin structures or small holes.","The source code will be released on GitHub."],"url":"http://arxiv.org/abs/2405.17193v1","category":"cs.GR"}
{"created":"2024-05-27 14:14:07","title":"SoK: Leveraging Transformers for Malware Analysis","abstract":"The introduction of transformers has been an important breakthrough for AI research and application as transformers are the foundation behind Generative AI. A promising application domain for transformers is cybersecurity, in particular the malware domain analysis. The reason is the flexibility of the transformer models in handling long sequential features and understanding contextual relationships. However, as the use of transformers for malware analysis is still in the infancy stage, it is critical to evaluate, systematize, and contextualize existing literature to foster future research. This Systematization of Knowledge (SoK) paper aims to provide a comprehensive analysis of transformer-based approaches designed for malware analysis. Based on our systematic analysis of existing knowledge, we structure and propose taxonomies based on: (a) how different transformers are adapted, organized, and modified across various use cases; and (b) how diverse feature types and their representation capabilities are reflected. We also provide an inventory of datasets used to explore multiple research avenues in the use of transformers for malware analysis and discuss open challenges with future research directions. We believe that this SoK paper will assist the research community in gaining detailed insights from existing work and will serve as a foundational resource for implementing novel research using transformers for malware analysis.","sentences":["The introduction of transformers has been an important breakthrough for AI research and application as transformers are the foundation behind Generative AI.","A promising application domain for transformers is cybersecurity, in particular the malware domain analysis.","The reason is the flexibility of the transformer models in handling long sequential features and understanding contextual relationships.","However, as the use of transformers for malware analysis is still in the infancy stage, it is critical to evaluate, systematize, and contextualize existing literature to foster future research.","This Systematization of Knowledge (SoK) paper aims to provide a comprehensive analysis of transformer-based approaches designed for malware analysis.","Based on our systematic analysis of existing knowledge, we structure and propose taxonomies based on: (a) how different transformers are adapted, organized, and modified across various use cases; and (b) how diverse feature types and their representation capabilities are reflected.","We also provide an inventory of datasets used to explore multiple research avenues in the use of transformers for malware analysis and discuss open challenges with future research directions.","We believe that this SoK paper will assist the research community in gaining detailed insights from existing work and will serve as a foundational resource for implementing novel research using transformers for malware analysis."],"url":"http://arxiv.org/abs/2405.17190v1","category":"cs.CR"}
{"created":"2024-05-27 14:05:51","title":"A Pioneering Roadmap for ML-Driven Algorithmic Advancements in Electrical Networks","abstract":"To advance control, operation and planning tools of electrical networks with ML is not straightforward. 110 experts were surveyed showing where and how ML algorithmis could advance. This paper assesses this survey and research environment. Then it develops an innovation roadmap that helps align our research community towards a goal-oriented realisation of the opportunities that AI upholds. This paper finds that the R\\&D environment of system operators (and the surrounding research ecosystem) needs adaptation to enable faster developments with AI while maintaining high testing quality and safety. This roadmap may interest research centre managers in system operators, academics, and labs dedicated to advancing the next generation of tooling for electrical networks.","sentences":["To advance control, operation and planning tools of electrical networks with ML is not straightforward.","110 experts were surveyed showing where and how ML algorithmis could advance.","This paper assesses this survey and research environment.","Then it develops an innovation roadmap that helps align our research community towards a goal-oriented realisation of the opportunities that AI upholds.","This paper finds that the R\\&D environment of system operators (and the surrounding research ecosystem) needs adaptation to enable faster developments with AI while maintaining high testing quality and safety.","This roadmap may interest research centre managers in system operators, academics, and labs dedicated to advancing the next generation of tooling for electrical networks."],"url":"http://arxiv.org/abs/2405.17184v2","category":"eess.SY"}
{"created":"2024-05-27 13:36:50","title":"Injecting Hamiltonian Architectural Bias into Deep Graph Networks for Long-Range Propagation","abstract":"The dynamics of information diffusion within graphs is a critical open issue that heavily influences graph representation learning, especially when considering long-range propagation. This calls for principled approaches that control and regulate the degree of propagation and dissipation of information throughout the neural flow. Motivated by this, we introduce (port-)Hamiltonian Deep Graph Networks, a novel framework that models neural information flow in graphs by building on the laws of conservation of Hamiltonian dynamical systems. We reconcile under a single theoretical and practical framework both non-dissipative long-range propagation and non-conservative behaviors, introducing tools from mechanical systems to gauge the equilibrium between the two components. Our approach can be applied to general message-passing architectures, and it provides theoretical guarantees on information conservation in time. Empirical results prove the effectiveness of our port-Hamiltonian scheme in pushing simple graph convolutional architectures to state-of-the-art performance in long-range benchmarks.","sentences":["The dynamics of information diffusion within graphs is a critical open issue that heavily influences graph representation learning, especially when considering long-range propagation.","This calls for principled approaches that control and regulate the degree of propagation and dissipation of information throughout the neural flow.","Motivated by this, we introduce (port-)Hamiltonian Deep Graph Networks, a novel framework that models neural information flow in graphs by building on the laws of conservation of Hamiltonian dynamical systems.","We reconcile under a single theoretical and practical framework both non-dissipative long-range propagation and non-conservative behaviors, introducing tools from mechanical systems to gauge the equilibrium between the two components.","Our approach can be applied to general message-passing architectures, and it provides theoretical guarantees on information conservation in time.","Empirical results prove the effectiveness of our port-Hamiltonian scheme in pushing simple graph convolutional architectures to state-of-the-art performance in long-range benchmarks."],"url":"http://arxiv.org/abs/2405.17163v1","category":"cs.LG"}
{"created":"2024-05-27 13:34:35","title":"Gravitational Wave Cosmology: Be Careful of the Black Hole Mass Spectrum","abstract":"Gravitational waves (GWs) from compact binary coalescences (CBCs) offer insights into the universe expansion. The spectral siren method, used without electromagnetic counterparts (EMC), infers cosmic expansion (Hubble constant) by relating detector and source frame masses of black hole (BH) mergers. However, heuristic mass models (broken power law, power law plus peak, multipeak) may introduce biases in the Hubble constant estimation, potentially up to 3 sigma with 2000 detected GW mergers. These biases stem from the models inability to consider redshift evolution and unexpected mass features. Future GW cosmology studies should employ adaptable source mass models to address these issues.","sentences":["Gravitational waves (GWs) from compact binary coalescences (CBCs) offer insights into the universe expansion.","The spectral siren method, used without electromagnetic counterparts (EMC), infers cosmic expansion (Hubble constant) by relating detector and source frame masses of black hole (BH) mergers.","However, heuristic mass models (broken power law, power law plus peak, multipeak) may introduce biases in the Hubble constant estimation, potentially up to 3 sigma with 2000 detected GW mergers.","These biases stem from the models inability to consider redshift evolution and unexpected mass features.","Future GW cosmology studies should employ adaptable source mass models to address these issues."],"url":"http://arxiv.org/abs/2405.17161v1","category":"gr-qc"}
{"created":"2024-05-27 13:31:46","title":"PatchScaler: An Efficient Patch-independent Diffusion Model for Super-Resolution","abstract":"Diffusion models significantly improve the quality of super-resolved images with their impressive content generation capabilities. However, the huge computational costs limit the applications of these methods.Recent efforts have explored reasonable inference acceleration to reduce the number of sampling steps, but the computational cost remains high as each step is performed on the entire image.This paper introduces PatchScaler, a patch-independent diffusion-based single image super-resolution (SR) method, designed to enhance the efficiency of the inference process.The proposed method is motivated by the observation that not all the image patches within an image need the same sampling steps for reconstructing high-resolution images.Based on this observation, we thus develop a Patch-adaptive Group Sampling (PGS) to divide feature patches into different groups according to the patch-level reconstruction difficulty and dynamically assign an appropriate sampling configuration for each group so that the inference speed can be better accelerated.In addition, to improve the denoising ability at each step of the sampling, we develop a texture prompt to guide the estimations of the diffusion model by retrieving high-quality texture priors from a patch-independent reference texture memory.Experiments show that our PatchScaler achieves favorable performance in both quantitative and qualitative evaluations with fast inference speed.Our code and model are available at \\url{https://github.com/yongliuy/PatchScaler}.","sentences":["Diffusion models significantly improve the quality of super-resolved images with their impressive content generation capabilities.","However, the huge computational costs limit the applications of these methods.","Recent efforts have explored reasonable inference acceleration to reduce the number of sampling steps, but the computational cost remains high as each step is performed on the entire image.","This paper introduces PatchScaler, a patch-independent diffusion-based single image super-resolution (SR) method, designed to enhance the efficiency of the inference process.","The proposed method is motivated by the observation that not all the image patches within an image need the same sampling steps for reconstructing high-resolution images.","Based on this observation, we thus develop a Patch-adaptive Group Sampling (PGS) to divide feature patches into different groups according to the patch-level reconstruction difficulty and dynamically assign an appropriate sampling configuration for each group so that the inference speed can be better accelerated.","In addition, to improve the denoising ability at each step of the sampling, we develop a texture prompt to guide the estimations of the diffusion model by retrieving high-quality texture priors from a patch-independent reference texture memory.","Experiments show that our PatchScaler achieves favorable performance in both quantitative and qualitative evaluations with fast inference speed.","Our code and model are available at \\url{https://github.com/yongliuy/PatchScaler}."],"url":"http://arxiv.org/abs/2405.17158v1","category":"cs.CV"}
{"created":"2024-05-27 13:26:59","title":"CoSLight: Co-optimizing Collaborator Selection and Decision-making to Enhance Traffic Signal Control","abstract":"Effective multi-intersection collaboration is pivotal for reinforcement-learning-based traffic signal control to alleviate congestion. Existing work mainly chooses neighboring intersections as collaborators. However, quite an amount of congestion, even some wide-range congestion, is caused by non-neighbors failing to collaborate. To address these issues, we propose to separate the collaborator selection as a second policy to be learned, concurrently being updated with the original signal-controlling policy. Specifically, the selection policy in real-time adaptively selects the best teammates according to phase- and intersection-level features. Empirical results on both synthetic and real-world datasets provide robust validation for the superiority of our approach, offering significant improvements over existing state-of-the-art methods. The code is available at https://github.com/AnonymousAccountss/CoSLight.","sentences":["Effective multi-intersection collaboration is pivotal for reinforcement-learning-based traffic signal control to alleviate congestion.","Existing work mainly chooses neighboring intersections as collaborators.","However, quite an amount of congestion, even some wide-range congestion, is caused by non-neighbors failing to collaborate.","To address these issues, we propose to separate the collaborator selection as a second policy to be learned, concurrently being updated with the original signal-controlling policy.","Specifically, the selection policy in real-time adaptively selects the best teammates according to phase- and intersection-level features.","Empirical results on both synthetic and real-world datasets provide robust validation for the superiority of our approach, offering significant improvements over existing state-of-the-art methods.","The code is available at https://github.com/AnonymousAccountss/CoSLight."],"url":"http://arxiv.org/abs/2405.17152v1","category":"cs.MA"}
{"created":"2024-05-27 13:04:29","title":"Second Law of Thermodynamics without Einstein Relation","abstract":"Materials that are constantly driven out of thermodynamic equilibrium, such as active and living systems, typically violate the Einstein relation. This may arise from active contributions to particle fluctuations which are unrelated to the dissipative resistance of the surrounding medium. We show that in these cases the widely used relation between informatic entropy production and heat dissipation does not hold. Consequently, fluctuation relations for the mechanical work, such as the Jarzynski and Crooks theorems, are invalid. We relate the breaking of the correspondence between entropy production and heat dissipation to the breaking of the fluctuation-dissipation theorem. We propose a temperature-like variable which restores this correspondence and gives rise to a generalized second law of thermodynamics, whereby the dissipated heat is necessarily nonnegative and vanishes at equilibrium. The Clausius inequality, Carnot maximum efficiency theorem, and relation between the maximum extractable work and the change of free energy are recovered as well.","sentences":["Materials that are constantly driven out of thermodynamic equilibrium, such as active and living systems, typically violate the Einstein relation.","This may arise from active contributions to particle fluctuations which are unrelated to the dissipative resistance of the surrounding medium.","We show that in these cases the widely used relation between informatic entropy production and heat dissipation does not hold.","Consequently, fluctuation relations for the mechanical work, such as the Jarzynski and Crooks theorems, are invalid.","We relate the breaking of the correspondence between entropy production and heat dissipation to the breaking of the fluctuation-dissipation theorem.","We propose a temperature-like variable which restores this correspondence and gives rise to a generalized second law of thermodynamics, whereby the dissipated heat is necessarily nonnegative and vanishes at equilibrium.","The Clausius inequality, Carnot maximum efficiency theorem, and relation between the maximum extractable work and the change of free energy are recovered as well."],"url":"http://arxiv.org/abs/2405.17142v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-27 12:59:46","title":"SDL-MVS: View Space and Depth Deformable Learning Paradigm for Multi-View Stereo Reconstruction in Remote Sensing","abstract":"Research on multi-view stereo based on remote sensing images has promoted the development of large-scale urban 3D reconstruction. However, remote sensing multi-view image data suffers from the problems of occlusion and uneven brightness between views during acquisition, which leads to the problem of blurred details in depth estimation. To solve the above problem, we re-examine the deformable learning method in the Multi-View Stereo task and propose a novel paradigm based on view Space and Depth deformable Learning (SDL-MVS), aiming to learn deformable interactions of features in different view spaces and deformably model the depth ranges and intervals to enable high accurate depth estimation. Specifically, to solve the problem of view noise caused by occlusion and uneven brightness, we propose a Progressive Space deformable Sampling (PSS) mechanism, which performs deformable learning of sampling points in the 3D frustum space and the 2D image space in a progressive manner to embed source features to the reference feature adaptively. To further optimize the depth, we introduce Depth Hypothesis deformable Discretization (DHD), which achieves precise positioning of the depth prior by adaptively adjusting the depth range hypothesis and performing deformable discretization of the depth interval hypothesis. Finally, our SDL-MVS achieves explicit modeling of occlusion and uneven brightness faced in multi-view stereo through the deformable learning paradigm of view space and depth, achieving accurate multi-view depth estimation. Extensive experiments on LuoJia-MVS and WHU datasets show that our SDL-MVS reaches state-of-the-art performance. It is worth noting that our SDL-MVS achieves an MAE error of 0.086, an accuracy of 98.9% for <0.6m, and 98.9% for <3-interval on the LuoJia-MVS dataset under the premise of three views as input.","sentences":["Research on multi-view stereo based on remote sensing images has promoted the development of large-scale urban 3D reconstruction.","However, remote sensing multi-view image data suffers from the problems of occlusion and uneven brightness between views during acquisition, which leads to the problem of blurred details in depth estimation.","To solve the above problem, we re-examine the deformable learning method in the Multi-View Stereo task and propose a novel paradigm based on view Space and Depth deformable Learning (SDL-MVS), aiming to learn deformable interactions of features in different view spaces and deformably model the depth ranges and intervals to enable high accurate depth estimation.","Specifically, to solve the problem of view noise caused by occlusion and uneven brightness, we propose a Progressive Space deformable Sampling (PSS) mechanism, which performs deformable learning of sampling points in the 3D frustum space and the 2D image space in a progressive manner to embed source features to the reference feature adaptively.","To further optimize the depth, we introduce Depth Hypothesis deformable Discretization (DHD), which achieves precise positioning of the depth prior by adaptively adjusting the depth range hypothesis and performing deformable discretization of the depth interval hypothesis.","Finally, our SDL-MVS achieves explicit modeling of occlusion and uneven brightness faced in multi-view stereo through the deformable learning paradigm of view space and depth, achieving accurate multi-view depth estimation.","Extensive experiments on LuoJia-MVS and WHU datasets show that our SDL-MVS reaches state-of-the-art performance.","It is worth noting that our SDL-MVS achieves an MAE error of 0.086, an accuracy of 98.9% for <0.6m, and 98.9% for <3-interval on the LuoJia-MVS dataset under the premise of three views as input."],"url":"http://arxiv.org/abs/2405.17140v1","category":"cs.CV"}
{"created":"2024-05-27 12:59:35","title":"Synergy and Diversity in CLIP: Enhancing Performance Through Adaptive Backbone Ensembling","abstract":"Contrastive Language-Image Pretraining (CLIP) stands out as a prominent method for image representation learning. Various architectures, from vision transformers (ViTs) to convolutional networks (ResNets) have been trained with CLIP to serve as general solutions to diverse vision tasks. This paper explores the differences across various CLIP-trained vision backbones. Despite using the same data and training objective, we find that these architectures have notably different representations, different classification performance across datasets, and different robustness properties to certain types of image perturbations. Our findings indicate a remarkable possible synergy across backbones by leveraging their respective strengths. In principle, classification accuracy could be improved by over 40 percentage with an informed selection of the optimal backbone per test example.Using this insight, we develop a straightforward yet powerful approach to adaptively ensemble multiple backbones. The approach uses as few as one labeled example per class to tune the adaptive combination of backbones. On a large collection of datasets, the method achieves a remarkable increase in accuracy of up to 39.1% over the best single backbone, well beyond traditional ensembles","sentences":["Contrastive Language-Image Pretraining (CLIP) stands out as a prominent method for image representation learning.","Various architectures, from vision transformers (ViTs) to convolutional networks (ResNets) have been trained with CLIP to serve as general solutions to diverse vision tasks.","This paper explores the differences across various CLIP-trained vision backbones.","Despite using the same data and training objective, we find that these architectures have notably different representations, different classification performance across datasets, and different robustness properties to certain types of image perturbations.","Our findings indicate a remarkable possible synergy across backbones by leveraging their respective strengths.","In principle, classification accuracy could be improved by over 40 percentage with an informed selection of the optimal backbone per test example.","Using this insight, we develop a straightforward yet powerful approach to adaptively ensemble multiple backbones.","The approach uses as few as one labeled example per class to tune the adaptive combination of backbones.","On a large collection of datasets, the method achieves a remarkable increase in accuracy of up to 39.1% over the best single backbone, well beyond traditional ensembles"],"url":"http://arxiv.org/abs/2405.17139v1","category":"cs.CV"}
{"created":"2024-05-27 12:49:07","title":"Your decision path does matter in pre-training industrial recommenders with multi-source behaviors","abstract":"Online service platforms offering a wide range of services through miniapps have become crucial for users who visit these platforms with clear intentions to find services they are interested in. Aiming at effective content delivery, cross-domain recommendation are introduced to learn high-quality representations by transferring behaviors from data-rich scenarios. However, these methods overlook the impact of the decision path that users take when conduct behaviors, that is, users ultimately exhibit different behaviors based on various intents. To this end, we propose HIER, a novel Hierarchical decIsion path Enhanced Representation learning for cross-domain recommendation. With the help of graph neural networks for high-order topological information of the knowledge graph between multi-source behaviors, we further adaptively learn decision paths through well-designed exemplar-level and information bottleneck based contrastive learning. Extensive experiments in online and offline environments show the superiority of HIER.","sentences":["Online service platforms offering a wide range of services through miniapps have become crucial for users who visit these platforms with clear intentions to find services they are interested in.","Aiming at effective content delivery, cross-domain recommendation are introduced to learn high-quality representations by transferring behaviors from data-rich scenarios.","However, these methods overlook the impact of the decision path that users take when conduct behaviors, that is, users ultimately exhibit different behaviors based on various intents.","To this end, we propose HIER, a novel Hierarchical decIsion path Enhanced Representation learning for cross-domain recommendation.","With the help of graph neural networks for high-order topological information of the knowledge graph between multi-source behaviors, we further adaptively learn decision paths through well-designed exemplar-level and information bottleneck based contrastive learning.","Extensive experiments in online and offline environments show the superiority of HIER."],"url":"http://arxiv.org/abs/2405.17132v1","category":"cs.LG"}
{"created":"2024-05-27 12:39:08","title":"Entanglement signature in quantum work statistics in the slow-driving regime","abstract":"In slowly driven classical systems, work is a stochastic quantity and its probability distribution is known to satisfy the work fluctuation-dissipation relation, which states that the mean and variance of the dissipated work are linearly related. Recently, it was shown that generation of quantum coherence in the instantaneous energy eigenbasis leads to a correction to this linear relation in the slow-driving regime. Here, we go even further by investigating nonclassical features of work fluctuations in setups with more than one system. To do this, we first generalize slow control protocols to encompass multipartite systems, allowing for the generation of quantum correlations during the driving process. Then, focussing on two-qubit systems, we show that entanglement generation leads to a positive contribution to the dissipated work, which is distinct from the quantum correction due to local coherence generation known from previous work. Our results show that entanglement generated during slow control protocols, e.g. as an unavoidable consequence of qubit crosstalk, comes at the cost of increased dissipation.","sentences":["In slowly driven classical systems, work is a stochastic quantity and its probability distribution is known to satisfy the work fluctuation-dissipation relation, which states that the mean and variance of the dissipated work are linearly related.","Recently, it was shown that generation of quantum coherence in the instantaneous energy eigenbasis leads to a correction to this linear relation in the slow-driving regime.","Here, we go even further by investigating nonclassical features of work fluctuations in setups with more than one system.","To do this, we first generalize slow control protocols to encompass multipartite systems, allowing for the generation of quantum correlations during the driving process.","Then, focussing on two-qubit systems, we show that entanglement generation leads to a positive contribution to the dissipated work, which is distinct from the quantum correction due to local coherence generation known from previous work.","Our results show that entanglement generated during slow control protocols, e.g. as an unavoidable consequence of qubit crosstalk, comes at the cost of increased dissipation."],"url":"http://arxiv.org/abs/2405.17121v1","category":"quant-ph"}
{"created":"2024-05-27 12:23:45","title":"Commutator-based operator splitting for linear port-Hamiltonian systems","abstract":"The port-Hamiltonian approach offers a modeling of dynamic systems with an energy-conserving and a dissipative part. Port-Hamiltonian (pH) systems are passive. That means no energy can be generated within the system. A passive system cannot store more energy than it receives. The exact solution of the pH system fulfills the dissipation inequality. In this paper, we deal with operator splitting that considers the energy-conserving and dissipative parts separately. We aim at high-order splitting schemes that preserve the dissipation inequality. Fourth-order methods for linear pHs-ODE are derived and an extension to sixth-order methods is discussed.","sentences":["The port-Hamiltonian approach offers a modeling of dynamic systems with an energy-conserving and a dissipative part.","Port-Hamiltonian (pH) systems are passive.","That means no energy can be generated within the system.","A passive system cannot store more energy than it receives.","The exact solution of the pH system fulfills the dissipation inequality.","In this paper, we deal with operator splitting that considers the energy-conserving and dissipative parts separately.","We aim at high-order splitting schemes that preserve the dissipation inequality.","Fourth-order methods for linear pHs-ODE are derived and an extension to sixth-order methods is discussed."],"url":"http://arxiv.org/abs/2405.17106v1","category":"math-ph"}
{"created":"2024-05-27 12:17:47","title":"Efficient Model Compression for Hierarchical Federated Learning","abstract":"Federated learning (FL), as an emerging collaborative learning paradigm, has garnered significant attention due to its capacity to preserve privacy within distributed learning systems. In these systems, clients collaboratively train a unified neural network model using their local datasets and share model parameters rather than raw data, enhancing privacy. Predominantly, FL systems are designed for mobile and edge computing environments where training typically occurs over wireless networks. Consequently, as model sizes increase, the conventional FL frameworks increasingly consume substantial communication resources. To address this challenge and improve communication efficiency, this paper introduces a novel hierarchical FL framework that integrates the benefits of clustered FL and model compression. We present an adaptive clustering algorithm that identifies a core client and dynamically organizes clients into clusters. Furthermore, to enhance transmission efficiency, each core client implements a local aggregation with compression (LC aggregation) algorithm after collecting compressed models from other clients within the same cluster. Simulation results affirm that our proposed algorithms not only maintain comparable predictive accuracy but also significantly reduce energy consumption relative to existing FL mechanisms.","sentences":["Federated learning (FL), as an emerging collaborative learning paradigm, has garnered significant attention due to its capacity to preserve privacy within distributed learning systems.","In these systems, clients collaboratively train a unified neural network model using their local datasets and share model parameters rather than raw data, enhancing privacy.","Predominantly, FL systems are designed for mobile and edge computing environments where training typically occurs over wireless networks.","Consequently, as model sizes increase, the conventional FL frameworks increasingly consume substantial communication resources.","To address this challenge and improve communication efficiency, this paper introduces a novel hierarchical FL framework that integrates the benefits of clustered FL and model compression.","We present an adaptive clustering algorithm that identifies a core client and dynamically organizes clients into clusters.","Furthermore, to enhance transmission efficiency, each core client implements a local aggregation with compression (LC aggregation)","algorithm after collecting compressed models from other clients within the same cluster.","Simulation results affirm that our proposed algorithms not only maintain comparable predictive accuracy but also significantly reduce energy consumption relative to existing FL mechanisms."],"url":"http://arxiv.org/abs/2405.17522v1","category":"cs.LG"}
{"created":"2024-05-27 12:10:07","title":"Dual feature reduction for the sparse-group lasso and its adaptive variant","abstract":"The sparse-group lasso performs both variable and group selection, making simultaneous use of the strengths of the lasso and group lasso. It has found widespread use in genetics, a field that regularly involves the analysis of high-dimensional data, due to its sparse-group penalty, which allows it to utilize grouping information. However, the sparse-group lasso can be computationally more expensive than both the lasso and group lasso, due to the added shrinkage complexity, and its additional hyper-parameter that needs tuning. In this paper a novel dual feature reduction method, Dual Feature Reduction (DFR), is presented that uses strong screening rules for the sparse-group lasso and the adaptive sparse-group lasso to reduce their input space before optimization. DFR applies two layers of screening and is based on the dual norms of the sparse-group lasso and adaptive sparse-group lasso. Through synthetic and real numerical studies, it is shown that the proposed feature reduction approach is able to drastically reduce the computational cost in many different scenarios.","sentences":["The sparse-group lasso performs both variable and group selection, making simultaneous use of the strengths of the lasso and group lasso.","It has found widespread use in genetics, a field that regularly involves the analysis of high-dimensional data, due to its sparse-group penalty, which allows it to utilize grouping information.","However, the sparse-group lasso can be computationally more expensive than both the lasso and group lasso, due to the added shrinkage complexity, and its additional hyper-parameter that needs tuning.","In this paper a novel dual feature reduction method, Dual Feature Reduction (DFR), is presented that uses strong screening rules for the sparse-group lasso and the adaptive sparse-group lasso to reduce their input space before optimization.","DFR applies two layers of screening and is based on the dual norms of the sparse-group lasso and adaptive sparse-group lasso.","Through synthetic and real numerical studies, it is shown that the proposed feature reduction approach is able to drastically reduce the computational cost in many different scenarios."],"url":"http://arxiv.org/abs/2405.17094v1","category":"stat.ML"}
{"created":"2024-05-27 12:04:36","title":"Phase Transitions in the Output Distribution of Large Language Models","abstract":"In a physical system, changing parameters such as temperature can induce a phase transition: an abrupt change from one state of matter to another. Analogous phenomena have recently been observed in large language models. Typically, the task of identifying phase transitions requires human analysis and some prior understanding of the system to narrow down which low-dimensional properties to monitor and analyze. Statistical methods for the automated detection of phase transitions from data have recently been proposed within the physics community. These methods are largely system agnostic and, as shown here, can be adapted to study the behavior of large language models. In particular, we quantify distributional changes in the generated output via statistical distances, which can be efficiently estimated with access to the probability distribution over next-tokens. This versatile approach is capable of discovering new phases of behavior and unexplored transitions -- an ability that is particularly exciting in light of the rapid development of language models and their emergent capabilities.","sentences":["In a physical system, changing parameters such as temperature can induce a phase transition: an abrupt change from one state of matter to another.","Analogous phenomena have recently been observed in large language models.","Typically, the task of identifying phase transitions requires human analysis and some prior understanding of the system to narrow down which low-dimensional properties to monitor and analyze.","Statistical methods for the automated detection of phase transitions from data have recently been proposed within the physics community.","These methods are largely system agnostic and, as shown here, can be adapted to study the behavior of large language models.","In particular, we quantify distributional changes in the generated output via statistical distances, which can be efficiently estimated with access to the probability distribution over next-tokens.","This versatile approach is capable of discovering new phases of behavior and unexplored transitions -- an ability that is particularly exciting in light of the rapid development of language models and their emergent capabilities."],"url":"http://arxiv.org/abs/2405.17088v1","category":"cs.LG"}
{"created":"2024-05-27 12:04:01","title":"Symbolic dynamics for the Kuramoto-Sivashinsky PDE on the line II","abstract":"We present a new algorithm for the rigorous integration of the variational equation (i.e. producing $\\mathcal C^1$ estimates) for a class of dissipative PDEs on the torus. As an application for some parameter value for the Kuramoto-Sivashinsky PDE on the line with odd and periodic boundary conditions we prove the existence of infinite number of homo- and heteroclinic orbits to two periodic orbits. The proof is computer assisted.","sentences":["We present a new algorithm for the rigorous integration of the variational equation (i.e. producing $\\mathcal C^1$ estimates) for a class of dissipative PDEs on the torus.","As an application for some parameter value for the Kuramoto-Sivashinsky PDE on the line with odd and periodic boundary conditions we prove the existence of infinite number of homo- and heteroclinic orbits to two periodic orbits.","The proof is computer assisted."],"url":"http://arxiv.org/abs/2405.17087v1","category":"math.DS"}
{"created":"2024-05-27 11:55:35","title":"Ensembling Diffusion Models via Adaptive Feature Aggregation","abstract":"The success of the text-guided diffusion model has inspired the development and release of numerous powerful diffusion models within the open-source community. These models are typically fine-tuned on various expert datasets, showcasing diverse denoising capabilities. Leveraging multiple high-quality models to produce stronger generation ability is valuable, but has not been extensively studied. Existing methods primarily adopt parameter merging strategies to produce a new static model. However, they overlook the fact that the divergent denoising capabilities of the models may dynamically change across different states, such as when experiencing different prompts, initial noises, denoising steps, and spatial locations. In this paper, we propose a novel ensembling method, Adaptive Feature Aggregation (AFA), which dynamically adjusts the contributions of multiple models at the feature level according to various states (i.e., prompts, initial noises, denoising steps, and spatial locations), thereby keeping the advantages of multiple diffusion models, while suppressing their disadvantages. Specifically, we design a lightweight Spatial-Aware Block-Wise (SABW) feature aggregator that adaptive aggregates the block-wise intermediate features from multiple U-Net denoisers into a unified one. The core idea lies in dynamically producing an individual attention map for each model's features by comprehensively considering various states. It is worth noting that only SABW is trainable with about 50 million parameters, while other models are frozen. Both the quantitative and qualitative experiments demonstrate the effectiveness of our proposed Adaptive Feature Aggregation method. The code is available at https://github.com/tenvence/afa/.","sentences":["The success of the text-guided diffusion model has inspired the development and release of numerous powerful diffusion models within the open-source community.","These models are typically fine-tuned on various expert datasets, showcasing diverse denoising capabilities.","Leveraging multiple high-quality models to produce stronger generation ability is valuable, but has not been extensively studied.","Existing methods primarily adopt parameter merging strategies to produce a new static model.","However, they overlook the fact that the divergent denoising capabilities of the models may dynamically change across different states, such as when experiencing different prompts, initial noises, denoising steps, and spatial locations.","In this paper, we propose a novel ensembling method, Adaptive Feature Aggregation (AFA), which dynamically adjusts the contributions of multiple models at the feature level according to various states (i.e., prompts, initial noises, denoising steps, and spatial locations), thereby keeping the advantages of multiple diffusion models, while suppressing their disadvantages.","Specifically, we design a lightweight Spatial-Aware Block-Wise (SABW) feature aggregator that adaptive aggregates the block-wise intermediate features from multiple U-Net denoisers into a unified one.","The core idea lies in dynamically producing an individual attention map for each model's features by comprehensively considering various states.","It is worth noting that only SABW is trainable with about 50 million parameters, while other models are frozen.","Both the quantitative and qualitative experiments demonstrate the effectiveness of our proposed Adaptive Feature Aggregation method.","The code is available at https://github.com/tenvence/afa/."],"url":"http://arxiv.org/abs/2405.17082v1","category":"cs.CV"}
{"created":"2024-05-27 11:52:24","title":"Learning with User-Level Local Differential Privacy","abstract":"User-level privacy is important in distributed systems. Previous research primarily focuses on the central model, while the local models have received much less attention. Under the central model, user-level DP is strictly stronger than the item-level one. However, under the local model, the relationship between user-level and item-level LDP becomes more complex, thus the analysis is crucially different. In this paper, we first analyze the mean estimation problem and then apply it to stochastic optimization, classification, and regression. In particular, we propose adaptive strategies to achieve optimal performance at all privacy levels. Moreover, we also obtain information-theoretic lower bounds, which show that the proposed methods are minimax optimal up to logarithmic factors. Unlike the central DP model, where user-level DP always leads to slower convergence, our result shows that under the local model, the convergence rates are nearly the same between user-level and item-level cases for distributions with bounded support. For heavy-tailed distributions, the user-level rate is even faster than the item-level one.","sentences":["User-level privacy is important in distributed systems.","Previous research primarily focuses on the central model, while the local models have received much less attention.","Under the central model, user-level DP is strictly stronger than the item-level one.","However, under the local model, the relationship between user-level and item-level LDP becomes more complex, thus the analysis is crucially different.","In this paper, we first analyze the mean estimation problem and then apply it to stochastic optimization, classification, and regression.","In particular, we propose adaptive strategies to achieve optimal performance at all privacy levels.","Moreover, we also obtain information-theoretic lower bounds, which show that the proposed methods are minimax optimal up to logarithmic factors.","Unlike the central DP model, where user-level DP always leads to slower convergence, our result shows that under the local model, the convergence rates are nearly the same between user-level and item-level cases for distributions with bounded support.","For heavy-tailed distributions, the user-level rate is even faster than the item-level one."],"url":"http://arxiv.org/abs/2405.17079v1","category":"stat.ML"}
{"created":"2024-05-27 11:37:20","title":"Model-Driven Engineering for Quantum Programming: A Case Study on Ground State Energy Calculation","abstract":"This study introduces a novel framework that brings together two main Quantum Programming methodologies, gate-based Quantum Computing and Quantum Annealing, by applying the Model-Driven Engineering principles. This aims to enhance the adaptability, design and scalability of quantum programs, facilitating their design and operation across diverse computing platforms. A notable achievement of this research is the development of a mapping method for programs between gate-based quantum computers and quantum annealers which can lead to the automatic transformation of these programs. Specifically, this method is applied to the Variational Quantum Eigensolver Algorithm and Quantum Anneling Ising Model, targeting ground state solutions. Finding ground-state solutions is crucial for a wide range of scientific applications, ranging from simulating chemistry lab experiments to medical applications, such as vaccine development. The success of this application demonstrates Model-Driven Engineering for Quantum Programming frameworks's practical viability and sets a clear path for quantum Computing's broader use in solving intricate problems.","sentences":["This study introduces a novel framework that brings together two main Quantum Programming methodologies, gate-based Quantum Computing and Quantum Annealing, by applying the Model-Driven Engineering principles.","This aims to enhance the adaptability, design and scalability of quantum programs, facilitating their design and operation across diverse computing platforms.","A notable achievement of this research is the development of a mapping method for programs between gate-based quantum computers and quantum annealers which can lead to the automatic transformation of these programs.","Specifically, this method is applied to the Variational Quantum Eigensolver Algorithm and Quantum Anneling Ising Model, targeting ground state solutions.","Finding ground-state solutions is crucial for a wide range of scientific applications, ranging from simulating chemistry lab experiments to medical applications, such as vaccine development.","The success of this application demonstrates Model-Driven Engineering for Quantum Programming frameworks's practical viability and sets a clear path for quantum Computing's broader use in solving intricate problems."],"url":"http://arxiv.org/abs/2405.17065v1","category":"quant-ph"}
{"created":"2024-05-27 11:29:54","title":"Comparative Study of Machine Learning Algorithms in Detecting Cardiovascular Diseases","abstract":"The detection of cardiovascular diseases (CVD) using machine learning techniques represents a significant advancement in medical diagnostics, aiming to enhance early detection, accuracy, and efficiency. This study explores a comparative analysis of various machine learning algorithms, including Logistic Regression, Decision Tree, Random Forest, Gradient Boosting, Support Vector Machine (SVM), K-Nearest Neighbors (KNN), and XGBoost. By utilising a structured workflow encompassing data collection, preprocessing, model selection and hyperparameter tuning, training, evaluation, and choice of the optimal model, this research addresses the critical need for improved diagnostic tools. The findings highlight the efficacy of ensemble methods and advanced algorithms in providing reliable predictions, thereby offering a comprehensive framework for CVD detection that can be readily implemented and adapted in clinical settings.","sentences":["The detection of cardiovascular diseases (CVD) using machine learning techniques represents a significant advancement in medical diagnostics, aiming to enhance early detection, accuracy, and efficiency.","This study explores a comparative analysis of various machine learning algorithms, including Logistic Regression, Decision Tree, Random Forest, Gradient Boosting, Support Vector Machine (SVM), K-Nearest Neighbors (KNN), and XGBoost.","By utilising a structured workflow encompassing data collection, preprocessing, model selection and hyperparameter tuning, training, evaluation, and choice of the optimal model, this research addresses the critical need for improved diagnostic tools.","The findings highlight the efficacy of ensemble methods and advanced algorithms in providing reliable predictions, thereby offering a comprehensive framework for CVD detection that can be readily implemented and adapted in clinical settings."],"url":"http://arxiv.org/abs/2405.17059v1","category":"cs.LG"}
{"created":"2024-05-27 11:18:25","title":"WirelessLLM: Empowering Large Language Models Towards Wireless Intelligence","abstract":"The rapid evolution of wireless technologies and the growing complexity of network infrastructures necessitate a paradigm shift in how communication networks are designed, configured, and managed. Recent advancements in Large Language Models (LLMs) have sparked interest in their potential to revolutionize wireless communication systems. However, existing studies on LLMs for wireless systems are limited to a direct application for telecom language understanding. To empower LLMs with knowledge and expertise in the wireless domain, this paper proposes WirelessLLM, a comprehensive framework for adapting and enhancing LLMs to address the unique challenges and requirements of wireless communication networks. We first identify three foundational principles that underpin WirelessLLM: knowledge alignment, knowledge fusion, and knowledge evolution. Then, we investigate the enabling technologies to build WirelessLLM, including prompt engineering, retrieval augmented generation, tool usage, multi-modal pre-training, and domain-specific fine-tuning. Moreover, we present three case studies to demonstrate the practical applicability and benefits of WirelessLLM for solving typical problems in wireless networks. Finally, we conclude this paper by highlighting key challenges and outlining potential avenues for future research.","sentences":["The rapid evolution of wireless technologies and the growing complexity of network infrastructures necessitate a paradigm shift in how communication networks are designed, configured, and managed.","Recent advancements in Large Language Models (LLMs) have sparked interest in their potential to revolutionize wireless communication systems.","However, existing studies on LLMs for wireless systems are limited to a direct application for telecom language understanding.","To empower LLMs with knowledge and expertise in the wireless domain, this paper proposes WirelessLLM, a comprehensive framework for adapting and enhancing LLMs to address the unique challenges and requirements of wireless communication networks.","We first identify three foundational principles that underpin WirelessLLM: knowledge alignment, knowledge fusion, and knowledge evolution.","Then, we investigate the enabling technologies to build WirelessLLM, including prompt engineering, retrieval augmented generation, tool usage, multi-modal pre-training, and domain-specific fine-tuning.","Moreover, we present three case studies to demonstrate the practical applicability and benefits of WirelessLLM for solving typical problems in wireless networks.","Finally, we conclude this paper by highlighting key challenges and outlining potential avenues for future research."],"url":"http://arxiv.org/abs/2405.17053v1","category":"cs.NI"}
{"created":"2024-05-27 11:14:55","title":"SelfCP: Compressing Long Prompt to 1/12 Using the Frozen Large Language Model Itself","abstract":"Long prompt leads to huge hardware costs when using Large Language Models (LLMs). Unfortunately, many tasks, such as summarization, inevitably introduce long task-inputs, and the wide application of in-context learning easily makes the prompt length explode. Inspired by the language understanding ability of LLMs, this paper proposes SelfCP, which uses the LLM \\textbf{itself} to \\textbf{C}ompress long \\textbf{P}rompt into compact virtual tokens. SelfCP applies a general frozen LLM twice, first as an encoder to compress the prompt and then as a decoder to generate responses. Specifically, given a long prompt, we place special tokens within the lengthy segment for compression and signal the LLM to generate $k$ virtual tokens. Afterward, the virtual tokens concatenate with the uncompressed prompt and are fed into the same LLM to generate the response. In general, SelfCP facilitates the unconditional and conditional compression of prompts, fitting both standard tasks and those with specific objectives. Since the encoder and decoder are frozen, SelfCP only contains 17M trainable parameters and allows for convenient adaptation across various backbones. We implement SelfCP with two LLM backbones and evaluate it in both in- and out-domain tasks. Results show that the compressed virtual tokens can substitute $12 \\times$ larger original prompts effectively","sentences":["Long prompt leads to huge hardware costs when using Large Language Models (LLMs).","Unfortunately, many tasks, such as summarization, inevitably introduce long task-inputs, and the wide application of in-context learning easily makes the prompt length explode.","Inspired by the language understanding ability of LLMs, this paper proposes SelfCP, which uses the LLM \\textbf{itself} to \\textbf{C}ompress long \\textbf{P}rompt into compact virtual tokens.","SelfCP applies a general frozen LLM twice, first as an encoder to compress the prompt and then as a decoder to generate responses.","Specifically, given a long prompt, we place special tokens within the lengthy segment for compression and signal the LLM to generate $k$ virtual tokens.","Afterward, the virtual tokens concatenate with the uncompressed prompt and are fed into the same LLM to generate the response.","In general, SelfCP facilitates the unconditional and conditional compression of prompts, fitting both standard tasks and those with specific objectives.","Since the encoder and decoder are frozen, SelfCP only contains 17M trainable parameters and allows for convenient adaptation across various backbones.","We implement SelfCP with two LLM backbones and evaluate it in both in- and out-domain tasks.","Results show that the compressed virtual tokens can substitute $12 \\times$ larger original prompts effectively"],"url":"http://arxiv.org/abs/2405.17052v1","category":"cs.CL"}
{"created":"2024-05-27 10:30:21","title":"Supervised Batch Normalization","abstract":"Batch Normalization (BN), a widely-used technique in neural networks, enhances generalization and expedites training by normalizing each mini-batch to the same mean and variance. However, its effectiveness diminishes when confronted with diverse data distributions. To address this challenge, we propose Supervised Batch Normalization (SBN), a pioneering approach. We expand normalization beyond traditional single mean and variance parameters, enabling the identification of data modes prior to training. This ensures effective normalization for samples sharing common features. We define contexts as modes, categorizing data with similar characteristics. These contexts are explicitly defined, such as domains in domain adaptation or modalities in multimodal systems, or implicitly defined through clustering algorithms based on data similarity. We illustrate the superiority of our approach over BN and other commonly employed normalization techniques through various experiments on both single and multi-task datasets. Integrating SBN with Vision Transformer results in a remarkable \\textit{15.13}\\% accuracy enhancement on CIFAR-100. Additionally, in domain adaptation scenarios, employing AdaMatch demonstrates an impressive \\textit{22.25}\\% accuracy improvement on MNIST and SVHN compared to BN.","sentences":["Batch Normalization (BN), a widely-used technique in neural networks, enhances generalization and expedites training by normalizing each mini-batch to the same mean and variance.","However, its effectiveness diminishes when confronted with diverse data distributions.","To address this challenge, we propose Supervised Batch Normalization (SBN), a pioneering approach.","We expand normalization beyond traditional single mean and variance parameters, enabling the identification of data modes prior to training.","This ensures effective normalization for samples sharing common features.","We define contexts as modes, categorizing data with similar characteristics.","These contexts are explicitly defined, such as domains in domain adaptation or modalities in multimodal systems, or implicitly defined through clustering algorithms based on data similarity.","We illustrate the superiority of our approach over BN and other commonly employed normalization techniques through various experiments on both single and multi-task datasets.","Integrating SBN with Vision Transformer results in a remarkable \\textit{15.13}\\% accuracy enhancement on CIFAR-100.","Additionally, in domain adaptation scenarios, employing AdaMatch demonstrates an impressive \\textit{22.25}\\% accuracy improvement on MNIST and SVHN compared to BN."],"url":"http://arxiv.org/abs/2405.17027v1","category":"cs.LG"}
{"created":"2024-05-27 10:15:35","title":"From Compliant to Rigid Contact Simulation: a Unified and Efficient Approach","abstract":"Whether rigid or compliant, contact interactions are inherent to robot motions, enabling them to move or manipulate things. Contact interactions result from complex physical phenomena, that can be mathematically cast as Nonlinear Complementarity Problems (NCPs) in the context of rigid or compliant point contact interactions. Such a class of complementarity problems is, in general, difficult to solve both from an optimization and numerical perspective. Over the past decades, dedicated and specialized contact solvers, implemented in modern robotics simulators (e.g., Bullet, Drake, MuJoCo, DART, Raisim) have emerged. Yet, most of these solvers tend either to solve a relaxed formulation of the original contact problems (at the price of physical inconsistencies) or to scale poorly with the problem dimension or its numerical conditioning (e.g., a robotic hand manipulating a paper sheet). In this paper, we introduce a unified and efficient approach to solving NCPs in the context of contact simulation. It relies on a sound combination of the Alternating Direction Method of Multipliers (ADMM) and proximal algorithms to account for both compliant and rigid contact interfaces in a unified way. To handle ill-conditioned problems and accelerate the convergence rate, we also propose an efficient update strategy to adapt the ADMM hyperparameters automatically. By leveraging proximal methods, we also propose new algorithmic solutions to efficiently evaluate the inverse dynamics involving rigid and compliant contact interactions, extending the approach developed in MuJoCo. We validate the efficiency and robustness of our contact solver against several alternative contact methods of the literature and benchmark them on various robotics and granular mechanics scenarios. Our code is made open-source at https://github.com/Simple-Robotics/Simple.","sentences":["Whether rigid or compliant, contact interactions are inherent to robot motions, enabling them to move or manipulate things.","Contact interactions result from complex physical phenomena, that can be mathematically cast as Nonlinear Complementarity Problems (NCPs) in the context of rigid or compliant point contact interactions.","Such a class of complementarity problems is, in general, difficult to solve both from an optimization and numerical perspective.","Over the past decades, dedicated and specialized contact solvers, implemented in modern robotics simulators (e.g., Bullet, Drake, MuJoCo, DART, Raisim) have emerged.","Yet, most of these solvers tend either to solve a relaxed formulation of the original contact problems (at the price of physical inconsistencies) or to scale poorly with the problem dimension or its numerical conditioning (e.g., a robotic hand manipulating a paper sheet).","In this paper, we introduce a unified and efficient approach to solving NCPs in the context of contact simulation.","It relies on a sound combination of the Alternating Direction Method of Multipliers (ADMM) and proximal algorithms to account for both compliant and rigid contact interfaces in a unified way.","To handle ill-conditioned problems and accelerate the convergence rate, we also propose an efficient update strategy to adapt the ADMM hyperparameters automatically.","By leveraging proximal methods, we also propose new algorithmic solutions to efficiently evaluate the inverse dynamics involving rigid and compliant contact interactions, extending the approach developed in MuJoCo.","We validate the efficiency and robustness of our contact solver against several alternative contact methods of the literature and benchmark them on various robotics and granular mechanics scenarios.","Our code is made open-source at https://github.com/Simple-Robotics/Simple."],"url":"http://arxiv.org/abs/2405.17020v1","category":"cs.RO"}
{"created":"2024-05-27 09:57:51","title":"MotionLLM: Multimodal Motion-Language Learning with Large Language Models","abstract":"Recent advancements in Multimodal Large Language Models (MM-LLMs) have demonstrated promising potential in terms of generalization and robustness when applied to different modalities. While previous works have already achieved 3D human motion generation using various approaches including language modeling, they mostly % are mostly carefully designed use specialized architecture and are restricted to single-human motion generation. Inspired by the success of MM-LLMs, we propose MotionLLM, a simple and general framework that can achieve single-human, multi-human motion generation, and motion captioning by fine-tuning pre-trained LLMs. Specifically, we encode and quantize motions into discrete LLM-understandable tokens, which results in a unified vocabulary consisting of both motion and text tokens. With only 1--3% parameters of the LLMs trained by using adapters, our single-human motion generation achieves comparable results to those diffusion models and other trained-from-scratch transformer-based models. Additionally, we show that our approach is scalable and flexible, allowing easy extension to multi-human motion generation through autoregressive generation of single-human motions. Project page: https://knoxzhao.github.io/MotionLLM","sentences":["Recent advancements in Multimodal Large Language Models (MM-LLMs) have demonstrated promising potential in terms of generalization and robustness when applied to different modalities.","While previous works have already achieved 3D human motion generation using various approaches including language modeling, they mostly % are mostly carefully designed use specialized architecture and are restricted to single-human motion generation.","Inspired by the success of MM-LLMs, we propose MotionLLM, a simple and general framework that can achieve single-human, multi-human motion generation, and motion captioning by fine-tuning pre-trained LLMs.","Specifically, we encode and quantize motions into discrete LLM-understandable tokens, which results in a unified vocabulary consisting of both motion and text tokens.","With only 1--3% parameters of the LLMs trained by using adapters, our single-human motion generation achieves comparable results to those diffusion models and other trained-from-scratch transformer-based models.","Additionally, we show that our approach is scalable and flexible, allowing easy extension to multi-human motion generation through autoregressive generation of single-human motions.","Project page: https://knoxzhao.github.io/MotionLLM"],"url":"http://arxiv.org/abs/2405.17013v2","category":"cs.CV"}
{"created":"2024-05-27 09:54:50","title":"Position: Foundation Agents as the Paradigm Shift for Decision Making","abstract":"Decision making demands intricate interplay between perception, memory, and reasoning to discern optimal policies. Conventional approaches to decision making face challenges related to low sample efficiency and poor generalization. In contrast, foundation models in language and vision have showcased rapid adaptation to diverse new tasks. Therefore, we advocate for the construction of foundation agents as a transformative shift in the learning paradigm of agents. This proposal is underpinned by the formulation of foundation agents with their fundamental characteristics and challenges motivated by the success of large language models (LLMs). Moreover, we specify the roadmap of foundation agents from large interactive data collection or generation, to self-supervised pretraining and adaptation, and knowledge and value alignment with LLMs. Lastly, we pinpoint critical research questions derived from the formulation and delineate trends for foundation agents supported by real-world use cases, addressing both technical and theoretical aspects to propel the field towards a more comprehensive and impactful future.","sentences":["Decision making demands intricate interplay between perception, memory, and reasoning to discern optimal policies.","Conventional approaches to decision making face challenges related to low sample efficiency and poor generalization.","In contrast, foundation models in language and vision have showcased rapid adaptation to diverse new tasks.","Therefore, we advocate for the construction of foundation agents as a transformative shift in the learning paradigm of agents.","This proposal is underpinned by the formulation of foundation agents with their fundamental characteristics and challenges motivated by the success of large language models (LLMs).","Moreover, we specify the roadmap of foundation agents from large interactive data collection or generation, to self-supervised pretraining and adaptation, and knowledge and value alignment with LLMs.","Lastly, we pinpoint critical research questions derived from the formulation and delineate trends for foundation agents supported by real-world use cases, addressing both technical and theoretical aspects to propel the field towards a more comprehensive and impactful future."],"url":"http://arxiv.org/abs/2405.17009v2","category":"cs.AI"}
{"created":"2024-05-27 09:47:09","title":"Graph Condensation for Open-World Graph Learning","abstract":"The burgeoning volume of graph data presents significant computational challenges in training graph neural networks (GNNs), critically impeding their efficiency in various applications. To tackle this challenge, graph condensation (GC) has emerged as a promising acceleration solution, focusing on the synthesis of a compact yet representative graph for efficiently training GNNs while retaining performance. Despite the potential to promote scalable use of GNNs, existing GC methods are limited to aligning the condensed graph with merely the observed static graph distribution. This limitation significantly restricts the generalization capacity of condensed graphs, particularly in adapting to dynamic distribution changes. In real-world scenarios, however, graphs are dynamic and constantly evolving, with new nodes and edges being continually integrated. Consequently, due to the limited generalization capacity of condensed graphs, applications that employ GC for efficient GNN training end up with sub-optimal GNNs when confronted with evolving graph structures and distributions in dynamic real-world situations. To overcome this issue, we propose open-world graph condensation (OpenGC), a robust GC framework that integrates structure-aware distribution shift to simulate evolving graph patterns and exploit the temporal environments for invariance condensation. This approach is designed to extract temporal invariant patterns from the original graph, thereby enhancing the generalization capabilities of the condensed graph and, subsequently, the GNNs trained on it. Extensive experiments on both real-world and synthetic evolving graphs demonstrate that OpenGC outperforms state-of-the-art (SOTA) GC methods in adapting to dynamic changes in open-world graph environments.","sentences":["The burgeoning volume of graph data presents significant computational challenges in training graph neural networks (GNNs), critically impeding their efficiency in various applications.","To tackle this challenge, graph condensation (GC) has emerged as a promising acceleration solution, focusing on the synthesis of a compact yet representative graph for efficiently training GNNs while retaining performance.","Despite the potential to promote scalable use of GNNs, existing GC methods are limited to aligning the condensed graph with merely the observed static graph distribution.","This limitation significantly restricts the generalization capacity of condensed graphs, particularly in adapting to dynamic distribution changes.","In real-world scenarios, however, graphs are dynamic and constantly evolving, with new nodes and edges being continually integrated.","Consequently, due to the limited generalization capacity of condensed graphs, applications that employ GC for efficient GNN training end up with sub-optimal GNNs when confronted with evolving graph structures and distributions in dynamic real-world situations.","To overcome this issue, we propose open-world graph condensation (OpenGC), a robust GC framework that integrates structure-aware distribution shift to simulate evolving graph patterns and exploit the temporal environments for invariance condensation.","This approach is designed to extract temporal invariant patterns from the original graph, thereby enhancing the generalization capabilities of the condensed graph and, subsequently, the GNNs trained on it.","Extensive experiments on both real-world and synthetic evolving graphs demonstrate that OpenGC outperforms state-of-the-art (SOTA) GC methods in adapting to dynamic changes in open-world graph environments."],"url":"http://arxiv.org/abs/2405.17003v1","category":"cs.LG"}
{"created":"2024-05-27 09:35:22","title":"Uncertainty Learning for High-dimensional Mean-variance Portfolio","abstract":"Accounting for uncertainty in Data quality is important for accurate statistical inference. We aim to an optimal conservative allocation for a large universe of assets in mean-variance portfolio (MVP), which is the worst choice within uncertainty in data distribution. Unlike the low dimensional MVP studied in Blanchet et al. (2022, Management Science), the large number of assets raises a challenging problem in quantifying the uncertainty, due to the big deviation of the sample covariance matrix from the population version. To overcome this difficulty, we propose a data-adaptive method to quantify the uncertainty with the help of a factor structure. Monte-Carlo Simulation is conducted to show the superiority of our method in high-dimensional cases, that, avoiding the over-conservative results in Blanchet et al. (2022), our allocation is closer to the oracle version in terms of risk minimization and expected portfolio return controlling.","sentences":["Accounting for uncertainty in Data quality is important for accurate statistical inference.","We aim to an optimal conservative allocation for a large universe of assets in mean-variance portfolio (MVP), which is the worst choice within uncertainty in data distribution.","Unlike the low dimensional MVP studied in Blanchet et al. (2022, Management Science), the large number of assets raises a challenging problem in quantifying the uncertainty, due to the big deviation of the sample covariance matrix from the population version.","To overcome this difficulty, we propose a data-adaptive method to quantify the uncertainty with the help of a factor structure.","Monte-Carlo Simulation is conducted to show the superiority of our method in high-dimensional cases, that, avoiding the over-conservative results in Blanchet et al. (2022), our allocation is closer to the oracle version in terms of risk minimization and expected portfolio return controlling."],"url":"http://arxiv.org/abs/2405.16989v1","category":"stat.ME"}
{"created":"2024-05-27 08:55:22","title":"Blind Data Adaptation to tackle Covariate Shift in Operational Steganalysis","abstract":"The proliferation of image manipulation for unethical purposes poses significant challenges in social networks. One particularly concerning method is Image Steganography, allowing individuals to hide illegal information in digital images without arousing suspicions. Such a technique pose severe security risks, making it crucial to develop effective steganalysis methods enabling to detect manipulated images for clandestine communications. Although significant advancements have been achieved with machine learning models, a critical issue remains: the disparity between the controlled datasets used to train steganalysis models against real-world datasets of forensic practitioners, undermining severely the practical effectiveness of standardized steganalysis models. In this paper, we address this issue focusing on a realistic scenario where practitioners lack crucial information about the limited target set of images under analysis, including details about their development process and even whereas it contains manipulated images or not. By leveraging geometric alignment and distribution matching of source and target residuals, we develop TADA (Target Alignment through Data Adaptation), a novel methodology enabling to emulate sources aligned with specific targets in steganalysis, which is also relevant for highly unbalanced targets. The emulator is represented by a light convolutional network trained to align distributions of image residuals. Experimental validation demonstrates the potential of our strategy over traditional methods fighting covariate shift in steganalysis.","sentences":["The proliferation of image manipulation for unethical purposes poses significant challenges in social networks.","One particularly concerning method is Image Steganography, allowing individuals to hide illegal information in digital images without arousing suspicions.","Such a technique pose severe security risks, making it crucial to develop effective steganalysis methods enabling to detect manipulated images for clandestine communications.","Although significant advancements have been achieved with machine learning models, a critical issue remains: the disparity between the controlled datasets used to train steganalysis models against real-world datasets of forensic practitioners, undermining severely the practical effectiveness of standardized steganalysis models.","In this paper, we address this issue focusing on a realistic scenario where practitioners lack crucial information about the limited target set of images under analysis, including details about their development process and even whereas it contains manipulated images or not.","By leveraging geometric alignment and distribution matching of source and target residuals, we develop TADA (Target Alignment through Data Adaptation), a novel methodology enabling to emulate sources aligned with specific targets in steganalysis, which is also relevant for highly unbalanced targets.","The emulator is represented by a light convolutional network trained to align distributions of image residuals.","Experimental validation demonstrates the potential of our strategy over traditional methods fighting covariate shift in steganalysis."],"url":"http://arxiv.org/abs/2405.16961v1","category":"eess.IV"}
{"created":"2024-05-27 08:55:17","title":"DCPI-Depth: Explicitly Infusing Dense Correspondence Prior to Unsupervised Monocular Depth Estimation","abstract":"There has been a recent surge of interest in learning to perceive depth from monocular videos in an unsupervised fashion. A key challenge in this field is achieving robust and accurate depth estimation in challenging scenarios, particularly in regions with weak textures or where dynamic objects are present. This study makes three major contributions by delving deeply into dense correspondence priors to provide existing frameworks with explicit geometric constraints. The first novelty is a contextual-geometric depth consistency loss, which employs depth maps triangulated from dense correspondences based on estimated ego-motion to guide the learning of depth perception from contextual information, since explicitly triangulated depth maps capture accurate relative distances among pixels. The second novelty arises from the observation that there exists an explicit, deducible relationship between optical flow divergence and depth gradient. A differential property correlation loss is, therefore, designed to refine depth estimation with a specific emphasis on local variations. The third novelty is a bidirectional stream co-adjustment strategy that enhances the interaction between rigid and optical flows, encouraging the former towards more accurate correspondence and making the latter more adaptable across various scenarios under the static scene hypotheses. DCPI-Depth, a framework that incorporates all these innovative components and couples two bidirectional and collaborative streams, achieves state-of-the-art performance and generalizability across multiple public datasets, outperforming all existing prior arts. Specifically, it demonstrates accurate depth estimation in texture-less and dynamic regions, and shows more reasonable smoothness.","sentences":["There has been a recent surge of interest in learning to perceive depth from monocular videos in an unsupervised fashion.","A key challenge in this field is achieving robust and accurate depth estimation in challenging scenarios, particularly in regions with weak textures or where dynamic objects are present.","This study makes three major contributions by delving deeply into dense correspondence priors to provide existing frameworks with explicit geometric constraints.","The first novelty is a contextual-geometric depth consistency loss, which employs depth maps triangulated from dense correspondences based on estimated ego-motion to guide the learning of depth perception from contextual information, since explicitly triangulated depth maps capture accurate relative distances among pixels.","The second novelty arises from the observation that there exists an explicit, deducible relationship between optical flow divergence and depth gradient.","A differential property correlation loss is, therefore, designed to refine depth estimation with a specific emphasis on local variations.","The third novelty is a bidirectional stream co-adjustment strategy that enhances the interaction between rigid and optical flows, encouraging the former towards more accurate correspondence and making the latter more adaptable across various scenarios under the static scene hypotheses.","DCPI-Depth, a framework that incorporates all these innovative components and couples two bidirectional and collaborative streams, achieves state-of-the-art performance and generalizability across multiple public datasets, outperforming all existing prior arts.","Specifically, it demonstrates accurate depth estimation in texture-less and dynamic regions, and shows more reasonable smoothness."],"url":"http://arxiv.org/abs/2405.16960v1","category":"cs.CV"}
{"created":"2024-05-27 08:52:56","title":"Jan Veth's paintings of Jacobus Kapteyn","abstract":"Jacobus C. Kapteyn is regarded as one of the coryfees of the University of Groningen. Part of his legacy is two paintings of him by Dutch painter Jan Pieter Veth. One, showing him at his desk, decorates the Kapteyn Room in the Kapteyn Astronomical Institute, and the other one, displaying him in academic attire, is in the University's gallery of professors in the central Academy Building. The first was offered to the Kapteyns on the occasion of his 40-th anniversary as professor in 1918 and the second to the University after his retirement in 1921.   It has been suggested that there must have been a third portrait that now is lost. Former director Adriaan Blaauw has proposed that the one in the Academy Building actually was first offered in 1918, but at Mrs. Kapteyn's request replaced by the one now in the Kapteyn Room. The first version was then later adapted to the requirements of the gallery of professors by Veth himself by overpainting it with academic attire. A preliminary trial version by Veth, in the possession of Kapteyn's greatgrandson, shows what it would have looked like before the adaption.   The following reports on new evidence: the biography of Jan Veth that historian Johan Huizinga, friend of Veth, wrote, and letters Veth wrote to his wife while he was working on these paintings. This provides strong support of Blaauw's sequence of events with a few modifications. No third painting has ever been produced.","sentences":["Jacobus C. Kapteyn is regarded as one of the coryfees of the University of Groningen.","Part of his legacy is two paintings of him by Dutch painter Jan Pieter Veth.","One, showing him at his desk, decorates the Kapteyn Room in the Kapteyn Astronomical Institute, and the other one, displaying him in academic attire, is in the University's gallery of professors in the central Academy Building.","The first was offered to the Kapteyns on the occasion of his 40-th anniversary as professor in 1918 and the second to the University after his retirement in 1921.   ","It has been suggested that there must have been a third portrait that now is lost.","Former director Adriaan Blaauw has proposed that the one in the Academy Building actually was first offered in 1918, but at Mrs. Kapteyn's request replaced by the one now in the Kapteyn Room.","The first version was then later adapted to the requirements of the gallery of professors by Veth himself by overpainting it with academic attire.","A preliminary trial version by Veth, in the possession of Kapteyn's greatgrandson, shows what it would have looked like before the adaption.   ","The following reports on new evidence: the biography of Jan Veth that historian Johan Huizinga, friend of Veth, wrote, and letters Veth wrote to his wife while he was working on these paintings.","This provides strong support of Blaauw's sequence of events with a few modifications.","No third painting has ever been produced."],"url":"http://arxiv.org/abs/2405.16957v1","category":"physics.hist-ph"}
{"created":"2024-05-27 08:39:38","title":"Zero-Shot Video Semantic Segmentation based on Pre-Trained Diffusion Models","abstract":"We introduce the first zero-shot approach for Video Semantic Segmentation (VSS) based on pre-trained diffusion models. A growing research direction attempts to employ diffusion models to perform downstream vision tasks by exploiting their deep understanding of image semantics. Yet, the majority of these approaches have focused on image-related tasks like semantic correspondence and segmentation, with less emphasis on video tasks such as VSS. Ideally, diffusion-based image semantic segmentation approaches can be applied to videos in a frame-by-frame manner. However, we find their performance on videos to be subpar due to the absence of any modeling of temporal information inherent in the video data. To this end, we tackle this problem and introduce a framework tailored for VSS based on pre-trained image and video diffusion models. We propose building a scene context model based on the diffusion features, where the model is autoregressively updated to adapt to scene changes. This context model predicts per-frame coarse segmentation maps that are temporally consistent. To refine these maps further, we propose a correspondence-based refinement strategy that aggregates predictions temporally, resulting in more confident predictions. Finally, we introduce a masked modulation approach to upsample the coarse maps to the full resolution at a high quality. Experiments show that our proposed approach outperforms existing zero-shot image semantic segmentation approaches significantly on various VSS benchmarks without any training or fine-tuning. Moreover, it rivals supervised VSS approaches on the VSPW dataset despite not being explicitly trained for VSS.","sentences":["We introduce the first zero-shot approach for Video Semantic Segmentation (VSS) based on pre-trained diffusion models.","A growing research direction attempts to employ diffusion models to perform downstream vision tasks by exploiting their deep understanding of image semantics.","Yet, the majority of these approaches have focused on image-related tasks like semantic correspondence and segmentation, with less emphasis on video tasks such as VSS.","Ideally, diffusion-based image semantic segmentation approaches can be applied to videos in a frame-by-frame manner.","However, we find their performance on videos to be subpar due to the absence of any modeling of temporal information inherent in the video data.","To this end, we tackle this problem and introduce a framework tailored for VSS based on pre-trained image and video diffusion models.","We propose building a scene context model based on the diffusion features, where the model is autoregressively updated to adapt to scene changes.","This context model predicts per-frame coarse segmentation maps that are temporally consistent.","To refine these maps further, we propose a correspondence-based refinement strategy that aggregates predictions temporally, resulting in more confident predictions.","Finally, we introduce a masked modulation approach to upsample the coarse maps to the full resolution at a high quality.","Experiments show that our proposed approach outperforms existing zero-shot image semantic segmentation approaches significantly on various VSS benchmarks without any training or fine-tuning.","Moreover, it rivals supervised VSS approaches on the VSPW dataset despite not being explicitly trained for VSS."],"url":"http://arxiv.org/abs/2405.16947v1","category":"cs.CV"}
{"created":"2024-05-27 08:12:00","title":"VoCoT: Unleashing Visually Grounded Multi-Step Reasoning in Large Multi-Modal Models","abstract":"While large multi-modal models (LMMs) have exhibited impressive capabilities across diverse tasks, their effectiveness in handling complex tasks has been limited by the prevailing single-step reasoning paradigm. To this end, this paper proposes VoCoT, a multi-step Visually grounded object-centric Chain-of-Thought reasoning framework tailored for inference with LMMs. VoCoT is characterized by two key features: (1) object-centric reasoning paths that revolve around cross-modal shared object-level information, and (2) visually grounded representation of object concepts in a multi-modal interleaved and aligned manner, which effectively bridges the modality gap within LMMs during long-term generation. Additionally, we construct an instruction dataset to facilitate LMMs in adapting to reasoning with VoCoT. By introducing VoCoT into the prevalent open-source LMM architecture, we introduce VolCano. With only 7B parameters and limited input resolution, VolCano demonstrates excellent performance across various scenarios, surpassing SOTA models, including GPT-4V, in tasks requiring complex reasoning. Our code, data and model will be available at https://github.com/RupertLuo/VoCoT.","sentences":["While large multi-modal models (LMMs) have exhibited impressive capabilities across diverse tasks, their effectiveness in handling complex tasks has been limited by the prevailing single-step reasoning paradigm.","To this end, this paper proposes VoCoT, a multi-step Visually grounded object-centric Chain-of-Thought reasoning framework tailored for inference with LMMs.","VoCoT is characterized by two key features: (1) object-centric reasoning paths that revolve around cross-modal shared object-level information, and (2) visually grounded representation of object concepts in a multi-modal interleaved and aligned manner, which effectively bridges the modality gap within LMMs during long-term generation.","Additionally, we construct an instruction dataset to facilitate LMMs in adapting to reasoning with VoCoT. By introducing VoCoT into the prevalent open-source LMM architecture, we introduce VolCano.","With only 7B parameters and limited input resolution, VolCano demonstrates excellent performance across various scenarios, surpassing SOTA models, including GPT-4V, in tasks requiring complex reasoning.","Our code, data and model will be available at https://github.com/RupertLuo/VoCoT."],"url":"http://arxiv.org/abs/2405.16919v2","category":"cs.CV"}
{"created":"2024-05-27 07:46:36","title":"Partial Models for Building Adaptive Model-Based Reinforcement Learning Agents","abstract":"In neuroscience, one of the key behavioral tests for determining whether a subject of study exhibits model-based behavior is to study its adaptiveness to local changes in the environment. In reinforcement learning, however, recent studies have shown that modern model-based agents display poor adaptivity to such changes. The main reason for this is that modern agents are typically designed to improve sample efficiency in single task settings and thus do not take into account the challenges that can arise in other settings. In local adaptation settings, one particularly important challenge is in quickly building and maintaining a sufficiently accurate model after a local change. This is challenging for deep model-based agents as their models and replay buffers are monolithic structures lacking distribution shift handling capabilities. In this study, we show that the conceptually simple idea of partial models can allow deep model-based agents to overcome this challenge and thus allow for building locally adaptive model-based agents. By modeling the different parts of the state space through different models, the agent can not only maintain a model that is accurate across the state space, but it can also quickly adapt it in the presence of a local change in the environment. We demonstrate this by showing that the use of partial models in agents such as deep Dyna-Q, PlaNet and Dreamer can allow for them to effectively adapt to the local changes in their environments.","sentences":["In neuroscience, one of the key behavioral tests for determining whether a subject of study exhibits model-based behavior is to study its adaptiveness to local changes in the environment.","In reinforcement learning, however, recent studies have shown that modern model-based agents display poor adaptivity to such changes.","The main reason for this is that modern agents are typically designed to improve sample efficiency in single task settings and thus do not take into account the challenges that can arise in other settings.","In local adaptation settings, one particularly important challenge is in quickly building and maintaining a sufficiently accurate model after a local change.","This is challenging for deep model-based agents as their models and replay buffers are monolithic structures lacking distribution shift handling capabilities.","In this study, we show that the conceptually simple idea of partial models can allow deep model-based agents to overcome this challenge and thus allow for building locally adaptive model-based agents.","By modeling the different parts of the state space through different models, the agent can not only maintain a model that is accurate across the state space, but it can also quickly adapt it in the presence of a local change in the environment.","We demonstrate this by showing that the use of partial models in agents such as deep Dyna-Q, PlaNet and Dreamer can allow for them to effectively adapt to the local changes in their environments."],"url":"http://arxiv.org/abs/2405.16899v1","category":"cs.LG"}
{"created":"2024-05-27 07:37:43","title":"On Fairness of Low-Rank Adaptation of Large Models","abstract":"Low-rank adaptation of large models, particularly LoRA, has gained traction due to its computational efficiency. This efficiency, contrasted with the prohibitive costs of full-model fine-tuning, means that practitioners often turn to LoRA and sometimes without a complete understanding of its ramifications. In this study, we focus on fairness and ask whether LoRA has an unexamined impact on utility, calibration, and resistance to membership inference across different subgroups (e.g., genders, races, religions) compared to a full-model fine-tuning baseline. We present extensive experiments across vision and language domains and across classification and generation tasks using ViT-Base, Swin-v2-Large, Llama-2 7B, and Mistral 7B. Intriguingly, experiments suggest that while one can isolate cases where LoRA exacerbates model bias across subgroups, the pattern is inconsistent -- in many cases, LoRA has equivalent or even improved fairness compared to the base model or its full fine-tuning baseline. We also examine the complications of evaluating fine-tuning fairness relating to task design and model token bias, calling for more careful fairness evaluations in future work.","sentences":["Low-rank adaptation of large models, particularly LoRA, has gained traction due to its computational efficiency.","This efficiency, contrasted with the prohibitive costs of full-model fine-tuning, means that practitioners often turn to LoRA and sometimes without a complete understanding of its ramifications.","In this study, we focus on fairness and ask whether LoRA has an unexamined impact on utility, calibration, and resistance to membership inference across different subgroups (e.g., genders, races, religions) compared to a full-model fine-tuning baseline.","We present extensive experiments across vision and language domains and across classification and generation tasks using ViT-Base, Swin-v2-Large, Llama-2 7B, and Mistral 7B. Intriguingly, experiments suggest that while one can isolate cases where LoRA exacerbates model bias across subgroups, the pattern is inconsistent -- in many cases, LoRA has equivalent or even improved fairness compared to the base model or its full fine-tuning baseline.","We also examine the complications of evaluating fine-tuning fairness relating to task design and model token bias, calling for more careful fairness evaluations in future work."],"url":"http://arxiv.org/abs/2405.17512v1","category":"cs.LG"}
{"created":"2024-05-27 06:59:20","title":"Scorch: A Library for Sparse Deep Learning","abstract":"The rapid growth in the size of deep learning models strains the capabilities of traditional dense computation paradigms. Leveraging sparse computation has become increasingly popular for training and deploying large-scale models, but existing deep learning frameworks lack extensive support for sparse operations. To bridge this gap, we introduce Scorch, a library that seamlessly integrates efficient sparse tensor computation into the PyTorch ecosystem, with an initial focus on inference workloads on CPUs. Scorch provides a flexible and intuitive interface for sparse tensors, supporting diverse sparse data structures. Scorch introduces a compiler stack that automates key optimizations, including automatic loop ordering, tiling, and format inference. Combined with a runtime that adapts its execution to both dense and sparse data, Scorch delivers substantial speedups over hand-written PyTorch Sparse (torch.sparse) operations without sacrificing usability. More importantly, Scorch enables efficient computation of complex sparse operations that lack hand-optimized PyTorch implementations. This flexibility is crucial for exploring novel sparse architectures. We demonstrate Scorch's ease of use and performance gains on diverse deep learning models across multiple domains. With only minimal code changes, Scorch achieves 1.05-5.78x speedups over PyTorch Sparse on end-to-end tasks. Scorch's seamless integration and performance gains make it a valuable addition to the PyTorch ecosystem. We believe Scorch will enable wider exploration of sparsity as a tool for scaling deep learning and inform the development of other sparse libraries.","sentences":["The rapid growth in the size of deep learning models strains the capabilities of traditional dense computation paradigms.","Leveraging sparse computation has become increasingly popular for training and deploying large-scale models, but existing deep learning frameworks lack extensive support for sparse operations.","To bridge this gap, we introduce Scorch, a library that seamlessly integrates efficient sparse tensor computation into the PyTorch ecosystem, with an initial focus on inference workloads on CPUs.","Scorch provides a flexible and intuitive interface for sparse tensors, supporting diverse sparse data structures.","Scorch introduces a compiler stack that automates key optimizations, including automatic loop ordering, tiling, and format inference.","Combined with a runtime that adapts its execution to both dense and sparse data, Scorch delivers substantial speedups over hand-written PyTorch Sparse (torch.sparse) operations without sacrificing usability.","More importantly, Scorch enables efficient computation of complex sparse operations that lack hand-optimized PyTorch implementations.","This flexibility is crucial for exploring novel sparse architectures.","We demonstrate Scorch's ease of use and performance gains on diverse deep learning models across multiple domains.","With only minimal code changes, Scorch achieves 1.05-5.78x speedups over PyTorch Sparse on end-to-end tasks.","Scorch's seamless integration and performance gains make it a valuable addition to the PyTorch ecosystem.","We believe Scorch will enable wider exploration of sparsity as a tool for scaling deep learning and inform the development of other sparse libraries."],"url":"http://arxiv.org/abs/2405.16883v1","category":"cs.LG"}
{"created":"2024-05-27 06:47:14","title":"CoCoGesture: Toward Coherent Co-speech 3D Gesture Generation in the Wild","abstract":"Deriving co-speech 3D gestures has seen tremendous progress in virtual avatar animation. Yet, the existing methods often produce stiff and unreasonable gestures with unseen human speech inputs due to the limited 3D speech-gesture data. In this paper, we propose CoCoGesture, a novel framework enabling vivid and diverse gesture synthesis from unseen human speech prompts. Our key insight is built upon the custom-designed pretrain-fintune training paradigm. At the pretraining stage, we aim to formulate a large generalizable gesture diffusion model by learning the abundant postures manifold. Therefore, to alleviate the scarcity of 3D data, we first construct a large-scale co-speech 3D gesture dataset containing more than 40M meshed posture instances across 4.3K speakers, dubbed GES-X. Then, we scale up the large unconditional diffusion model to 1B parameters and pre-train it to be our gesture experts. At the finetune stage, we present the audio ControlNet that incorporates the human voice as condition prompts to guide the gesture generation. Here, we construct the audio ControlNet through a trainable copy of our pre-trained diffusion model. Moreover, we design a novel Mixture-of-Gesture-Experts (MoGE) block to adaptively fuse the audio embedding from the human speech and the gesture features from the pre-trained gesture experts with a routing mechanism. Such an effective manner ensures audio embedding is temporal coordinated with motion features while preserving the vivid and diverse gesture generation. Extensive experiments demonstrate that our proposed CoCoGesture outperforms the state-of-the-art methods on the zero-shot speech-to-gesture generation. The dataset will be publicly available at: https://mattie-e.github.io/GES-X/","sentences":["Deriving co-speech 3D gestures has seen tremendous progress in virtual avatar animation.","Yet, the existing methods often produce stiff and unreasonable gestures with unseen human speech inputs due to the limited 3D speech-gesture data.","In this paper, we propose CoCoGesture, a novel framework enabling vivid and diverse gesture synthesis from unseen human speech prompts.","Our key insight is built upon the custom-designed pretrain-fintune training paradigm.","At the pretraining stage, we aim to formulate a large generalizable gesture diffusion model by learning the abundant postures manifold.","Therefore, to alleviate the scarcity of 3D data, we first construct a large-scale co-speech 3D gesture dataset containing more than 40M meshed posture instances across 4.3K speakers, dubbed GES-X.","Then, we scale up the large unconditional diffusion model to 1B parameters and pre-train it to be our gesture experts.","At the finetune stage, we present the audio ControlNet that incorporates the human voice as condition prompts to guide the gesture generation.","Here, we construct the audio ControlNet through a trainable copy of our pre-trained diffusion model.","Moreover, we design a novel Mixture-of-Gesture-Experts (MoGE) block to adaptively fuse the audio embedding from the human speech and the gesture features from the pre-trained gesture experts with a routing mechanism.","Such an effective manner ensures audio embedding is temporal coordinated with motion features while preserving the vivid and diverse gesture generation.","Extensive experiments demonstrate that our proposed CoCoGesture outperforms the state-of-the-art methods on the zero-shot speech-to-gesture generation.","The dataset will be publicly available at: https://mattie-e.github.io/GES-X/"],"url":"http://arxiv.org/abs/2405.16874v1","category":"cs.CV"}
{"created":"2024-05-27 06:36:17","title":"Mixture of Modality Knowledge Experts for Robust Multi-modal Knowledge Graph Completion","abstract":"Multi-modal knowledge graph completion (MMKGC) aims to automatically discover new knowledge triples in the given multi-modal knowledge graphs (MMKGs), which is achieved by collaborative modeling the structural information concealed in massive triples and the multi-modal features of the entities. Existing methods tend to focus on crafting elegant entity-wise multi-modal fusion strategies, yet they overlook the utilization of multi-perspective features concealed within the modalities under diverse relational contexts. To address this issue, we introduce a novel MMKGC framework with Mixture of Modality Knowledge experts (MoMoK for short) to learn adaptive multi-modal embedding under intricate relational contexts. We design relation-guided modality knowledge experts to acquire relation-aware modality embeddings and integrate the predictions from multi-modalities to achieve comprehensive decisions. Additionally, we disentangle the experts by minimizing their mutual information. Experiments on four public MMKG benchmarks demonstrate the outstanding performance of MoMoK under complex scenarios.","sentences":["Multi-modal knowledge graph completion (MMKGC) aims to automatically discover new knowledge triples in the given multi-modal knowledge graphs (MMKGs), which is achieved by collaborative modeling the structural information concealed in massive triples and the multi-modal features of the entities.","Existing methods tend to focus on crafting elegant entity-wise multi-modal fusion strategies, yet they overlook the utilization of multi-perspective features concealed within the modalities under diverse relational contexts.","To address this issue, we introduce a novel MMKGC framework with Mixture of Modality Knowledge experts (MoMoK for short) to learn adaptive multi-modal embedding under intricate relational contexts.","We design relation-guided modality knowledge experts to acquire relation-aware modality embeddings and integrate the predictions from multi-modalities to achieve comprehensive decisions.","Additionally, we disentangle the experts by minimizing their mutual information.","Experiments on four public MMKG benchmarks demonstrate the outstanding performance of MoMoK under complex scenarios."],"url":"http://arxiv.org/abs/2405.16869v1","category":"cs.AI"}
{"created":"2024-05-27 06:32:36","title":"Hierarchical Rank-One Sequence Convexification for the Relaxation of Variational Problems with Microstructures","abstract":"This paper presents an efficient algorithm for the approximation of the rank-one convex hull in the context of nonlinear solid mechanics. It is based on hierarchical rank-one sequences and simultaneously provides first and second derivative information essential for the calculation of mechanical stresses and the computational minimization of discretized energies. For materials, whose microstructure can be well approximated in terms of laminates and where each laminate stage achieves energetic optimality with respect to the current stage, the approximate envelope coincides with the rank-one convex envelope. Although the proposed method provides only an upper bound for the rank-one convex hull, a careful examination of the resulting constraints shows a decent applicability in mechanical problems. Various aspects of the algorithm are discussed, including the restoration of rotational invariance, microstructure reconstruction, comparisons with other semi-convexification algorithms, and mesh independency. Overall, this paper demonstrates the efficiency of the algorithm for both, well-established mathematical benchmark problems as well as nonconvex isotropic finite-strain continuum damage models in two and three dimensions. Thereby, for the first time, a feasible concurrent numerical relaxation is established for an incremental, dissipative large-strain model with relevant applications in engineering problems.","sentences":["This paper presents an efficient algorithm for the approximation of the rank-one convex hull in the context of nonlinear solid mechanics.","It is based on hierarchical rank-one sequences and simultaneously provides first and second derivative information essential for the calculation of mechanical stresses and the computational minimization of discretized energies.","For materials, whose microstructure can be well approximated in terms of laminates and where each laminate stage achieves energetic optimality with respect to the current stage, the approximate envelope coincides with the rank-one convex envelope.","Although the proposed method provides only an upper bound for the rank-one convex hull, a careful examination of the resulting constraints shows a decent applicability in mechanical problems.","Various aspects of the algorithm are discussed, including the restoration of rotational invariance, microstructure reconstruction, comparisons with other semi-convexification algorithms, and mesh independency.","Overall, this paper demonstrates the efficiency of the algorithm for both, well-established mathematical benchmark problems as well as nonconvex isotropic finite-strain continuum damage models in two and three dimensions.","Thereby, for the first time, a feasible concurrent numerical relaxation is established for an incremental, dissipative large-strain model with relevant applications in engineering problems."],"url":"http://arxiv.org/abs/2405.16866v1","category":"cs.CE"}
{"created":"2024-05-27 04:49:41","title":"Kernel-based optimally weighted conformal prediction intervals","abstract":"Conformal prediction has been a popular distribution-free framework for uncertainty quantification. In this paper, we present a novel conformal prediction method for time-series, which we call Kernel-based Optimally Weighted Conformal Prediction Intervals (KOWCPI). Specifically, KOWCPI adapts the classic Reweighted Nadaraya-Watson (RNW) estimator for quantile regression on dependent data and learns optimal data-adaptive weights. Theoretically, we tackle the challenge of establishing a conditional coverage guarantee for non-exchangeable data under strong mixing conditions on the non-conformity scores. We demonstrate the superior performance of KOWCPI on real time-series against state-of-the-art methods, where KOWCPI achieves narrower confidence intervals without losing coverage.","sentences":["Conformal prediction has been a popular distribution-free framework for uncertainty quantification.","In this paper, we present a novel conformal prediction method for time-series, which we call Kernel-based Optimally Weighted Conformal Prediction Intervals (KOWCPI).","Specifically, KOWCPI adapts the classic Reweighted Nadaraya-Watson (RNW) estimator for quantile regression on dependent data and learns optimal data-adaptive weights.","Theoretically, we tackle the challenge of establishing a conditional coverage guarantee for non-exchangeable data under strong mixing conditions on the non-conformity scores.","We demonstrate the superior performance of KOWCPI on real time-series against state-of-the-art methods, where KOWCPI achieves narrower confidence intervals without losing coverage."],"url":"http://arxiv.org/abs/2405.16828v1","category":"cs.LG"}
{"created":"2024-05-27 04:38:10","title":"Laboratory-Scale AI: Open-Weight Models are Competitive with ChatGPT Even in Low-Resource Settings","abstract":"The rapid proliferation of generative AI has raised questions about the competitiveness of lower-parameter, locally tunable, open-weight models relative to high-parameter, API-guarded, closed-weight models in terms of performance, domain adaptation, cost, and generalization. Centering under-resourced yet risk-intolerant settings in government, research, and healthcare, we see for-profit closed-weight models as incompatible with requirements for transparency, privacy, adaptability, and standards of evidence. Yet the performance penalty in using open-weight models, especially in low-data and low-resource settings, is unclear.   We assess the feasibility of using smaller, open-weight models to replace GPT-4-Turbo in zero-shot, few-shot, and fine-tuned regimes, assuming access to only a single, low-cost GPU. We assess value-sensitive issues around bias, privacy, and abstention on three additional tasks relevant to those topics. We find that with relatively low effort, very low absolute monetary cost, and relatively little data for fine-tuning, small open-weight models can achieve competitive performance in domain-adapted tasks without sacrificing generality. We then run experiments considering practical issues in bias, privacy, and hallucination risk, finding that open models offer several benefits over closed models. We intend this work as a case study in understanding the opportunity cost of reproducibility and transparency over for-profit state-of-the-art zero shot performance, finding this cost to be marginal under realistic settings.","sentences":["The rapid proliferation of generative AI has raised questions about the competitiveness of lower-parameter, locally tunable, open-weight models relative to high-parameter, API-guarded, closed-weight models in terms of performance, domain adaptation, cost, and generalization.","Centering under-resourced yet risk-intolerant settings in government, research, and healthcare, we see for-profit closed-weight models as incompatible with requirements for transparency, privacy, adaptability, and standards of evidence.","Yet the performance penalty in using open-weight models, especially in low-data and low-resource settings, is unclear.   ","We assess the feasibility of using smaller, open-weight models to replace GPT-4-Turbo in zero-shot, few-shot, and fine-tuned regimes, assuming access to only a single, low-cost GPU.","We assess value-sensitive issues around bias, privacy, and abstention on three additional tasks relevant to those topics.","We find that with relatively low effort, very low absolute monetary cost, and relatively little data for fine-tuning, small open-weight models can achieve competitive performance in domain-adapted tasks without sacrificing generality.","We then run experiments considering practical issues in bias, privacy, and hallucination risk, finding that open models offer several benefits over closed models.","We intend this work as a case study in understanding the opportunity cost of reproducibility and transparency over for-profit state-of-the-art zero shot performance, finding this cost to be marginal under realistic settings."],"url":"http://arxiv.org/abs/2405.16820v1","category":"cs.LG"}
{"created":"2024-05-27 04:33:53","title":"Automatic Domain Adaptation by Transformers in In-Context Learning","abstract":"Selecting or designing an appropriate domain adaptation algorithm for a given problem remains challenging. This paper presents a Transformer model that can provably approximate and opt for domain adaptation methods for a given dataset in the in-context learning framework, where a foundation model performs new tasks without updating its parameters at test time. Specifically, we prove that Transformers can approximate instance-based and feature-based unsupervised domain adaptation algorithms and automatically select an algorithm suited for a given dataset. Numerical results indicate that in-context learning demonstrates an adaptive domain adaptation surpassing existing methods.","sentences":["Selecting or designing an appropriate domain adaptation algorithm for a given problem remains challenging.","This paper presents a Transformer model that can provably approximate and opt for domain adaptation methods for a given dataset in the in-context learning framework, where a foundation model performs new tasks without updating its parameters at test time.","Specifically, we prove that Transformers can approximate instance-based and feature-based unsupervised domain adaptation algorithms and automatically select an algorithm suited for a given dataset.","Numerical results indicate that in-context learning demonstrates an adaptive domain adaptation surpassing existing methods."],"url":"http://arxiv.org/abs/2405.16819v1","category":"cs.LG"}
{"created":"2024-05-27 03:54:09","title":"Extreme Compression of Adaptive Neural Images","abstract":"Implicit Neural Representations (INRs) and Neural Fields are a novel paradigm for signal representation, from images and audio to 3D scenes and videos. The fundamental idea is to represent a signal as a continuous and differentiable neural network. This idea offers unprecedented benefits such as continuous resolution and memory efficiency, enabling new compression techniques. However, representing data as neural networks poses new challenges. For instance, given a 2D image as a neural network, how can we further compress such a neural image?. In this work, we present a novel analysis on compressing neural fields, with the focus on images. We also introduce Adaptive Neural Images (ANI), an efficient neural representation that enables adaptation to different inference or transmission requirements. Our proposed method allows to reduce the bits-per-pixel (bpp) of the neural image by 4x, without losing sensitive details or harming fidelity. We achieve this thanks to our successful implementation of 4-bit neural representations. Our work offers a new framework for developing compressed neural fields.","sentences":["Implicit Neural Representations (INRs) and Neural Fields are a novel paradigm for signal representation, from images and audio to 3D scenes and videos.","The fundamental idea is to represent a signal as a continuous and differentiable neural network.","This idea offers unprecedented benefits such as continuous resolution and memory efficiency, enabling new compression techniques.","However, representing data as neural networks poses new challenges.","For instance, given a 2D image as a neural network, how can we further compress such a neural image?.","In this work, we present a novel analysis on compressing neural fields, with the focus on images.","We also introduce Adaptive Neural Images (ANI), an efficient neural representation that enables adaptation to different inference or transmission requirements.","Our proposed method allows to reduce the bits-per-pixel (bpp) of the neural image by 4x, without losing sensitive details or harming fidelity.","We achieve this thanks to our successful implementation of 4-bit neural representations.","Our work offers a new framework for developing compressed neural fields."],"url":"http://arxiv.org/abs/2405.16807v1","category":"cs.CV"}
{"created":"2024-05-27 03:29:06","title":"Physics-informed Inverse Design of Multi-bit Programmable Metasurfaces","abstract":"Emerging reconfigurable metasurfaces offer various possibilities in programmatically manipulating electromagnetic waves across spatial, spectral, and temporal domains, showcasing great potential for enhancing terahertz applications. However, they are hindered by limited tunability, particularly evident in relatively small phase tuning over 270o, due to the design constraints with time-intensive forward design methodologies. Here, we demonstrate a multi-bit programmable metasurface capable of terahertz beam steering, facilitated by a developed physics-informed inverse design (PIID) approach. Through integrating a modified coupled mode theory (MCMT) into residual neural networks, our PIID algorithm not only significantly increases the design accuracy compared to conventional neural networks but also elucidates the intricate physical relations between the geometry and the modes. Without decreasing the reflection intensity, our method achieves the enhanced phase tuning as large as 300o. Additionally, we experimentally validate the inverse designed programmable beam steering metasurface, which is adaptable across 1-bit, 2-bit, and tri-state coding schemes, yielding a deflection angle up to 68o and broadened steering coverage. Our demonstration provides a promising pathway for rapidly exploring advanced metasurface devices, with potentially great impact on communication and imaging technologies.","sentences":["Emerging reconfigurable metasurfaces offer various possibilities in programmatically manipulating electromagnetic waves across spatial, spectral, and temporal domains, showcasing great potential for enhancing terahertz applications.","However, they are hindered by limited tunability, particularly evident in relatively small phase tuning over 270o, due to the design constraints with time-intensive forward design methodologies.","Here, we demonstrate a multi-bit programmable metasurface capable of terahertz beam steering, facilitated by a developed physics-informed inverse design (PIID) approach.","Through integrating a modified coupled mode theory (MCMT) into residual neural networks, our PIID algorithm not only significantly increases the design accuracy compared to conventional neural networks but also elucidates the intricate physical relations between the geometry and the modes.","Without decreasing the reflection intensity, our method achieves the enhanced phase tuning as large as 300o.","Additionally, we experimentally validate the inverse designed programmable beam steering metasurface, which is adaptable across 1-bit, 2-bit, and tri-state coding schemes, yielding a deflection angle up to 68o and broadened steering coverage.","Our demonstration provides a promising pathway for rapidly exploring advanced metasurface devices, with potentially great impact on communication and imaging technologies."],"url":"http://arxiv.org/abs/2405.16795v1","category":"physics.optics"}
{"created":"2024-05-27 03:13:28","title":"PromptFix: You Prompt and We Fix the Photo","abstract":"Diffusion models equipped with language models demonstrate excellent controllability in image generation tasks, allowing image processing to adhere to human instructions. However, the lack of diverse instruction-following data hampers the development of models that effectively recognize and execute user-customized instructions, particularly in low-level tasks. Moreover, the stochastic nature of the diffusion process leads to deficiencies in image generation or editing tasks that require the detailed preservation of the generated images. To address these limitations, we propose PromptFix, a comprehensive framework that enables diffusion models to follow human instructions to perform a wide variety of image-processing tasks. First, we construct a large-scale instruction-following dataset that covers comprehensive image-processing tasks, including low-level tasks, image editing, and object creation. Next, we propose a high-frequency guidance sampling method to explicitly control the denoising process and preserve high-frequency details in unprocessed areas. Finally, we design an auxiliary prompting adapter, utilizing Vision-Language Models (VLMs) to enhance text prompts and improve the model's task generalization. Experimental results show that PromptFix outperforms previous methods in various image-processing tasks. Our proposed model also achieves comparable inference efficiency with these baseline models and exhibits superior zero-shot capabilities in blind restoration and combination tasks. The dataset and code will be aviliable at https://github.com/yeates/PromptFix.","sentences":["Diffusion models equipped with language models demonstrate excellent controllability in image generation tasks, allowing image processing to adhere to human instructions.","However, the lack of diverse instruction-following data hampers the development of models that effectively recognize and execute user-customized instructions, particularly in low-level tasks.","Moreover, the stochastic nature of the diffusion process leads to deficiencies in image generation or editing tasks that require the detailed preservation of the generated images.","To address these limitations, we propose PromptFix, a comprehensive framework that enables diffusion models to follow human instructions to perform a wide variety of image-processing tasks.","First, we construct a large-scale instruction-following dataset that covers comprehensive image-processing tasks, including low-level tasks, image editing, and object creation.","Next, we propose a high-frequency guidance sampling method to explicitly control the denoising process and preserve high-frequency details in unprocessed areas.","Finally, we design an auxiliary prompting adapter, utilizing Vision-Language Models (VLMs) to enhance text prompts and improve the model's task generalization.","Experimental results show that PromptFix outperforms previous methods in various image-processing tasks.","Our proposed model also achieves comparable inference efficiency with these baseline models and exhibits superior zero-shot capabilities in blind restoration and combination tasks.","The dataset and code will be aviliable at https://github.com/yeates/PromptFix."],"url":"http://arxiv.org/abs/2405.16785v1","category":"cs.CV"}
{"created":"2024-05-27 02:27:28","title":"Reframing the Relationship in Out-of-Distribution Detection","abstract":"The remarkable achievements of Large Language Models (LLMs) have captivated the attention of both academia and industry, transcending their initial role in dialogue generation. The utilization of LLMs as intermediary agents in various tasks has yielded promising results, sparking a wave of innovation in artificial intelligence. Building on these breakthroughs, we introduce a novel approach that integrates the agent paradigm into the Out-of-distribution (OOD) detection task, aiming to enhance its robustness and adaptability. Our proposed method, Concept Matching with Agent (CMA), employs neutral prompts as agents to augment the CLIP-based OOD detection process. These agents function as dynamic observers and communication hubs, interacting with both In-distribution (ID) labels and data inputs to form vector triangle relationships. This triangular framework offers a more nuanced approach than the traditional binary relationship, allowing for better separation and identification of ID and OOD inputs. Our extensive experimental results showcase the superior performance of CMA over both zero-shot and training-required methods in a diverse array of real-world scenarios.","sentences":["The remarkable achievements of Large Language Models (LLMs) have captivated the attention of both academia and industry, transcending their initial role in dialogue generation.","The utilization of LLMs as intermediary agents in various tasks has yielded promising results, sparking a wave of innovation in artificial intelligence.","Building on these breakthroughs, we introduce a novel approach that integrates the agent paradigm into the Out-of-distribution (OOD) detection task, aiming to enhance its robustness and adaptability.","Our proposed method, Concept Matching with Agent (CMA), employs neutral prompts as agents to augment the CLIP-based OOD detection process.","These agents function as dynamic observers and communication hubs, interacting with both In-distribution (ID) labels and data inputs to form vector triangle relationships.","This triangular framework offers a more nuanced approach than the traditional binary relationship, allowing for better separation and identification of ID and OOD inputs.","Our extensive experimental results showcase the superior performance of CMA over both zero-shot and training-required methods in a diverse array of real-world scenarios."],"url":"http://arxiv.org/abs/2405.16766v1","category":"cs.CV"}
{"created":"2024-05-27 01:54:16","title":"CHESS: Contextual Harnessing for Efficient SQL Synthesis","abstract":"Utilizing large language models (LLMs) for transforming natural language questions into SQL queries (text-to-SQL) is a promising yet challenging approach, particularly when applied to real-world databases with complex and extensive schemas. In particular, effectively incorporating data catalogs and database values for SQL generation remains an obstacle, leading to suboptimal solutions. We address this problem by proposing a new pipeline that effectively retrieves relevant data and context, selects an efficient schema, and synthesizes correct and efficient SQL queries. To increase retrieval precision, our pipeline introduces a hierarchical retrieval method leveraging model-generated keywords, locality-sensitive hashing indexing, and vector databases. Additionally, we have developed an adaptive schema pruning technique that adjusts based on the complexity of the problem and the model's context size. Our approach generalizes to both frontier proprietary models like GPT-4 and open-source models such as Llama-3-70B. Through a series of ablation studies, we demonstrate the effectiveness of each component of our pipeline and its impact on the end-to-end performance. Our method achieves new state-of-the-art performance on the cross-domain challenging BIRD dataset.","sentences":["Utilizing large language models (LLMs) for transforming natural language questions into SQL queries (text-to-SQL) is a promising yet challenging approach, particularly when applied to real-world databases with complex and extensive schemas.","In particular, effectively incorporating data catalogs and database values for SQL generation remains an obstacle, leading to suboptimal solutions.","We address this problem by proposing a new pipeline that effectively retrieves relevant data and context, selects an efficient schema, and synthesizes correct and efficient SQL queries.","To increase retrieval precision, our pipeline introduces a hierarchical retrieval method leveraging model-generated keywords, locality-sensitive hashing indexing, and vector databases.","Additionally, we have developed an adaptive schema pruning technique that adjusts based on the complexity of the problem and the model's context size.","Our approach generalizes to both frontier proprietary models like GPT-4 and open-source models such as Llama-3-70B. Through a series of ablation studies, we demonstrate the effectiveness of each component of our pipeline and its impact on the end-to-end performance.","Our method achieves new state-of-the-art performance on the cross-domain challenging BIRD dataset."],"url":"http://arxiv.org/abs/2405.16755v1","category":"cs.LG"}
{"created":"2024-05-27 01:54:07","title":"Adaptive VIO: Deep Visual-Inertial Odometry with Online Continual Learning","abstract":"Visual-inertial odometry (VIO) has demonstrated remarkable success due to its low-cost and complementary sensors. However, existing VIO methods lack the generalization ability to adjust to different environments and sensor attributes. In this paper, we propose Adaptive VIO, a new monocular visual-inertial odometry that combines online continual learning with traditional nonlinear optimization. Adaptive VIO comprises two networks to predict visual correspondence and IMU bias. Unlike end-to-end approaches that use networks to fuse the features from two modalities (camera and IMU) and predict poses directly, we combine neural networks with visual-inertial bundle adjustment in our VIO system. The optimized estimates will be fed back to the visual and IMU bias networks, refining the networks in a self-supervised manner. Such a learning-optimization-combined framework and feedback mechanism enable the system to perform online continual learning. Experiments demonstrate that our Adaptive VIO manifests adaptive capability on EuRoC and TUM-VI datasets. The overall performance exceeds the currently known learning-based VIO methods and is comparable to the state-of-the-art optimization-based methods.","sentences":["Visual-inertial odometry (VIO) has demonstrated remarkable success due to its low-cost and complementary sensors.","However, existing VIO methods lack the generalization ability to adjust to different environments and sensor attributes.","In this paper, we propose Adaptive VIO, a new monocular visual-inertial odometry that combines online continual learning with traditional nonlinear optimization.","Adaptive VIO comprises two networks to predict visual correspondence and IMU bias.","Unlike end-to-end approaches that use networks to fuse the features from two modalities (camera and IMU) and predict poses directly, we combine neural networks with visual-inertial bundle adjustment in our VIO system.","The optimized estimates will be fed back to the visual and IMU bias networks, refining the networks in a self-supervised manner.","Such a learning-optimization-combined framework and feedback mechanism enable the system to perform online continual learning.","Experiments demonstrate that our Adaptive VIO manifests adaptive capability on EuRoC and TUM-VI datasets.","The overall performance exceeds the currently known learning-based VIO methods and is comparable to the state-of-the-art optimization-based methods."],"url":"http://arxiv.org/abs/2405.16754v1","category":"cs.RO"}
{"created":"2024-05-27 01:47:14","title":"LLM-Based Cooperative Agents using Information Relevance and Plan Validation","abstract":"We address the challenge of multi-agent cooperation, where agents achieve a common goal by interacting with a 3D scene and cooperating with decentralized agents under complex partial observations. This involves managing communication costs and optimizing interaction trajectories in dynamic environments. Our research focuses on three primary limitations of existing cooperative agent systems. Firstly, current systems demonstrate inefficiency in managing acquired information through observation, resulting in declining planning performance as the environment becomes more complex with additional objects or goals. Secondly, the neglect of false plans in partially observable settings leads to suboptimal cooperative performance, as agents struggle to adapt to environmental changes influenced by the unseen actions of other agents. Lastly, the failure to incorporate spatial data into decision-making processes restricts the agent's ability to construct optimized trajectories. To overcome these limitations, we propose the RElevance and Validation-Enhanced Cooperative Language Agent (REVECA), a novel cognitive architecture powered by GPT-3.5. REVECA leverages relevance assessment, plan validation, and spatial information to enhance the efficiency and robustness of agent cooperation in dynamic and partially observable environments while minimizing continuous communication costs and effectively managing irrelevant dummy objects. Our extensive experiments demonstrate the superiority of REVECA over previous approaches, including those driven by GPT-4.0. Additionally, a user study highlights REVECA's potential for achieving trustworthy human-AI cooperation. We expect that REVECA will have significant applications in gaming, XR applications, educational tools, and humanoid robots, contributing to substantial economic, commercial, and academic advancements.","sentences":["We address the challenge of multi-agent cooperation, where agents achieve a common goal by interacting with a 3D scene and cooperating with decentralized agents under complex partial observations.","This involves managing communication costs and optimizing interaction trajectories in dynamic environments.","Our research focuses on three primary limitations of existing cooperative agent systems.","Firstly, current systems demonstrate inefficiency in managing acquired information through observation, resulting in declining planning performance as the environment becomes more complex with additional objects or goals.","Secondly, the neglect of false plans in partially observable settings leads to suboptimal cooperative performance, as agents struggle to adapt to environmental changes influenced by the unseen actions of other agents.","Lastly, the failure to incorporate spatial data into decision-making processes restricts the agent's ability to construct optimized trajectories.","To overcome these limitations, we propose the RElevance and Validation-Enhanced Cooperative Language Agent (REVECA), a novel cognitive architecture powered by GPT-3.5.","REVECA leverages relevance assessment, plan validation, and spatial information to enhance the efficiency and robustness of agent cooperation in dynamic and partially observable environments while minimizing continuous communication costs and effectively managing irrelevant dummy objects.","Our extensive experiments demonstrate the superiority of REVECA over previous approaches, including those driven by GPT-4.0.","Additionally, a user study highlights REVECA's potential for achieving trustworthy human-AI cooperation.","We expect that REVECA will have significant applications in gaming, XR applications, educational tools, and humanoid robots, contributing to substantial economic, commercial, and academic advancements."],"url":"http://arxiv.org/abs/2405.16751v1","category":"cs.AI"}
{"created":"2024-05-27 01:31:40","title":"Understanding Linear Probing then Fine-tuning Language Models from NTK Perspective","abstract":"The two-stage fine-tuning (FT) method, linear probing then fine-tuning (LP-FT), consistently outperforms linear probing (LP) and FT alone in terms of accuracy for both in-distribution (ID) and out-of-distribution (OOD) data. This success is largely attributed to the preservation of pre-trained features, achieved through a near-optimal linear head obtained during LP. However, despite the widespread use of large language models, the exploration of complex architectures such as Transformers remains limited. In this paper, we analyze the training dynamics of LP-FT for classification models on the basis of the neural tangent kernel (NTK) theory. Our analysis decomposes the NTK matrix into two components, highlighting the importance of the linear head norm alongside the prediction accuracy at the start of the FT stage. We also observe a significant increase in the linear head norm during LP, stemming from training with the cross-entropy (CE) loss, which effectively minimizes feature changes. Furthermore, we find that this increased norm can adversely affect model calibration, a challenge that can be addressed by temperature scaling. Additionally, we extend our analysis with the NTK to the low-rank adaptation (LoRA) method and validate its effectiveness. Our experiments with a Transformer-based model on natural language processing tasks across multiple benchmarks confirm our theoretical analysis and demonstrate the effectiveness of LP-FT in fine-tuning language models. Code is available at https://github.com/tom4649/lp-ft_ntk.","sentences":["The two-stage fine-tuning (FT) method, linear probing then fine-tuning (LP-FT), consistently outperforms linear probing (LP) and FT alone in terms of accuracy for both in-distribution (ID) and out-of-distribution (OOD) data.","This success is largely attributed to the preservation of pre-trained features, achieved through a near-optimal linear head obtained during LP.","However, despite the widespread use of large language models, the exploration of complex architectures such as Transformers remains limited.","In this paper, we analyze the training dynamics of LP-FT for classification models on the basis of the neural tangent kernel (NTK) theory.","Our analysis decomposes the NTK matrix into two components, highlighting the importance of the linear head norm alongside the prediction accuracy at the start of the FT stage.","We also observe a significant increase in the linear head norm during LP, stemming from training with the cross-entropy (CE) loss, which effectively minimizes feature changes.","Furthermore, we find that this increased norm can adversely affect model calibration, a challenge that can be addressed by temperature scaling.","Additionally, we extend our analysis with the NTK to the low-rank adaptation (LoRA) method and validate its effectiveness.","Our experiments with a Transformer-based model on natural language processing tasks across multiple benchmarks confirm our theoretical analysis and demonstrate the effectiveness of LP-FT in fine-tuning language models.","Code is available at https://github.com/tom4649/lp-ft_ntk."],"url":"http://arxiv.org/abs/2405.16747v1","category":"cs.LG"}
{"created":"2024-05-27 01:13:01","title":"PP-SAM: Perturbed Prompts for Robust Adaptation of Segment Anything Model for Polyp Segmentation","abstract":"The Segment Anything Model (SAM), originally designed for general-purpose segmentation tasks, has been used recently for polyp segmentation. Nonetheless, fine-tuning SAM with data from new imaging centers or clinics poses significant challenges. This is because this necessitates the creation of an expensive and time-intensive annotated dataset, along with the potential for variability in user prompts during inference. To address these issues, we propose a robust fine-tuning technique, PP-SAM, that allows SAM to adapt to the polyp segmentation task with limited images. To this end, we utilize variable perturbed bounding box prompts (BBP) to enrich the learning context and enhance the model's robustness to BBP perturbations during inference. Rigorous experiments on polyp segmentation benchmarks reveal that our variable BBP perturbation significantly improves model resilience. Notably, on Kvasir, 1-shot fine-tuning boosts the DICE score by 20% and 37% with 50 and 100-pixel BBP perturbations during inference, respectively. Moreover, our experiments show that 1-shot, 5-shot, and 10-shot PP-SAM with 50-pixel perturbations during inference outperform a recent state-of-the-art (SOTA) polyp segmentation method by 26%, 7%, and 5% DICE scores, respectively. Our results motivate the broader applicability of our PP-SAM for other medical imaging tasks with limited samples. Our implementation is available at https://github.com/SLDGroup/PP-SAM.","sentences":["The Segment Anything Model (SAM), originally designed for general-purpose segmentation tasks, has been used recently for polyp segmentation.","Nonetheless, fine-tuning SAM with data from new imaging centers or clinics poses significant challenges.","This is because this necessitates the creation of an expensive and time-intensive annotated dataset, along with the potential for variability in user prompts during inference.","To address these issues, we propose a robust fine-tuning technique, PP-SAM, that allows SAM to adapt to the polyp segmentation task with limited images.","To this end, we utilize variable perturbed bounding box prompts (BBP) to enrich the learning context and enhance the model's robustness to BBP perturbations during inference.","Rigorous experiments on polyp segmentation benchmarks reveal that our variable BBP perturbation significantly improves model resilience.","Notably, on Kvasir, 1-shot fine-tuning boosts the DICE score by 20% and 37% with 50 and 100-pixel BBP perturbations during inference, respectively.","Moreover, our experiments show that 1-shot, 5-shot, and 10-shot PP-SAM with 50-pixel perturbations during inference outperform a recent state-of-the-art (SOTA) polyp segmentation method by 26%, 7%, and 5% DICE scores, respectively.","Our results motivate the broader applicability of our PP-SAM for other medical imaging tasks with limited samples.","Our implementation is available at https://github.com/SLDGroup/PP-SAM."],"url":"http://arxiv.org/abs/2405.16740v1","category":"cs.CV"}
{"created":"2024-05-27 00:12:51","title":"Pretraining with Random Noise for Fast and Robust Learning without Weight Transport","abstract":"The brain prepares for learning even before interacting with the environment, by refining and optimizing its structures through spontaneous neural activity that resembles random noise. However, the mechanism of such a process has yet to be thoroughly understood, and it is unclear whether this process can benefit the algorithm of machine learning. Here, we study this issue using a neural network with a feedback alignment algorithm, demonstrating that pretraining neural networks with random noise increases the learning efficiency as well as generalization abilities without weight transport. First, we found that random noise training modifies forward weights to match backward synaptic feedback, which is necessary for teaching errors by feedback alignment. As a result, a network with pre-aligned weights learns notably faster than a network without random noise training, even reaching a convergence speed comparable to that of a backpropagation algorithm. Sequential training with both random noise and data brings weights closer to synaptic feedback than training solely with data, enabling more precise credit assignment and faster learning. We also found that each readout probability approaches the chance level and that the effective dimensionality of weights decreases in a network pretrained with random noise. This pre-regularization allows the network to learn simple solutions of a low rank, reducing the generalization loss during subsequent training. This also enables the network robustly to generalize a novel, out-of-distribution dataset. Lastly, we confirmed that random noise pretraining reduces the amount of meta-loss, enhancing the network ability to adapt to various tasks. Overall, our results suggest that random noise training with feedback alignment offers a straightforward yet effective method of pretraining that facilitates quick and reliable learning without weight transport.","sentences":["The brain prepares for learning even before interacting with the environment, by refining and optimizing its structures through spontaneous neural activity that resembles random noise.","However, the mechanism of such a process has yet to be thoroughly understood, and it is unclear whether this process can benefit the algorithm of machine learning.","Here, we study this issue using a neural network with a feedback alignment algorithm, demonstrating that pretraining neural networks with random noise increases the learning efficiency as well as generalization abilities without weight transport.","First, we found that random noise training modifies forward weights to match backward synaptic feedback, which is necessary for teaching errors by feedback alignment.","As a result, a network with pre-aligned weights learns notably faster than a network without random noise training, even reaching a convergence speed comparable to that of a backpropagation algorithm.","Sequential training with both random noise and data brings weights closer to synaptic feedback than training solely with data, enabling more precise credit assignment and faster learning.","We also found that each readout probability approaches the chance level and that the effective dimensionality of weights decreases in a network pretrained with random noise.","This pre-regularization allows the network to learn simple solutions of a low rank, reducing the generalization loss during subsequent training.","This also enables the network robustly to generalize a novel, out-of-distribution dataset.","Lastly, we confirmed that random noise pretraining reduces the amount of meta-loss, enhancing the network ability to adapt to various tasks.","Overall, our results suggest that random noise training with feedback alignment offers a straightforward yet effective method of pretraining that facilitates quick and reliable learning without weight transport."],"url":"http://arxiv.org/abs/2405.16731v1","category":"cs.LG"}
{"created":"2024-05-26 23:14:37","title":"Amortized Active Causal Induction with Deep Reinforcement Learning","abstract":"We present Causal Amortized Active Structure Learning (CAASL), an active intervention design policy that can select interventions that are adaptive, real-time and that does not require access to the likelihood. This policy, an amortized network based on the transformer, is trained with reinforcement learning on a simulator of the design environment, and a reward function that measures how close the true causal graph is to a causal graph posterior inferred from the gathered data. On synthetic data and a single-cell gene expression simulator, we demonstrate empirically that the data acquired through our policy results in a better estimate of the underlying causal graph than alternative strategies. Our design policy successfully achieves amortized intervention design on the distribution of the training environment while also generalizing well to distribution shifts in test-time design environments. Further, our policy also demonstrates excellent zero-shot generalization to design environments with dimensionality higher than that during training, and to intervention types that it has not been trained on.","sentences":["We present Causal Amortized Active Structure Learning (CAASL), an active intervention design policy that can select interventions that are adaptive, real-time and that does not require access to the likelihood.","This policy, an amortized network based on the transformer, is trained with reinforcement learning on a simulator of the design environment, and a reward function that measures how close the true causal graph is to a causal graph posterior inferred from the gathered data.","On synthetic data and a single-cell gene expression simulator, we demonstrate empirically that the data acquired through our policy results in a better estimate of the underlying causal graph than alternative strategies.","Our design policy successfully achieves amortized intervention design on the distribution of the training environment while also generalizing well to distribution shifts in test-time design environments.","Further, our policy also demonstrates excellent zero-shot generalization to design environments with dimensionality higher than that during training, and to intervention types that it has not been trained on."],"url":"http://arxiv.org/abs/2405.16718v1","category":"cs.LG"}
{"created":"2024-05-26 22:50:03","title":"Adaptive Incentive Design with Learning Agents","abstract":"How can the system operator learn an incentive mechanism that achieves social optimality based on limited information about the agents' behavior, who are dynamically updating their strategies? To answer this question, we propose an \\emph{adaptive} incentive mechanism. This mechanism updates the incentives of agents based on the feedback of each agent's externality, evaluated as the difference between the player's marginal cost and society's marginal cost at each time step. The proposed mechanism updates the incentives on a slower timescale compared to the agents' learning dynamics, resulting in a two-timescale coupled dynamical system. Notably, this mechanism is agnostic to the specific learning dynamics used by agents to update their strategies. We show that any fixed point of this adaptive incentive mechanism corresponds to the optimal incentive mechanism, ensuring that the Nash equilibrium coincides with the socially optimal strategy. Additionally, we provide sufficient conditions that guarantee the convergence of the adaptive incentive mechanism to a fixed point. Our results apply to both atomic and non-atomic games. To demonstrate the effectiveness of our proposed mechanism, we verify the convergence conditions in two practically relevant games: atomic networked quadratic aggregative games and non-atomic network routing games.","sentences":["How can the system operator learn an incentive mechanism that achieves social optimality based on limited information about the agents' behavior, who are dynamically updating their strategies?","To answer this question, we propose an \\emph{adaptive} incentive mechanism.","This mechanism updates the incentives of agents based on the feedback of each agent's externality, evaluated as the difference between the player's marginal cost and society's marginal cost at each time step.","The proposed mechanism updates the incentives on a slower timescale compared to the agents' learning dynamics, resulting in a two-timescale coupled dynamical system.","Notably, this mechanism is agnostic to the specific learning dynamics used by agents to update their strategies.","We show that any fixed point of this adaptive incentive mechanism corresponds to the optimal incentive mechanism, ensuring that the Nash equilibrium coincides with the socially optimal strategy.","Additionally, we provide sufficient conditions that guarantee the convergence of the adaptive incentive mechanism to a fixed point.","Our results apply to both atomic and non-atomic games.","To demonstrate the effectiveness of our proposed mechanism, we verify the convergence conditions in two practically relevant games: atomic networked quadratic aggregative games and non-atomic network routing games."],"url":"http://arxiv.org/abs/2405.16716v1","category":"cs.GT"}
{"created":"2024-05-26 22:08:58","title":"REX: Designing User-centered Repair and Explanations to Address Robot Failures","abstract":"Robots in real-world environments continuously engage with multiple users and encounter changes that lead to unexpected conflicts in fulfilling user requests. Recent technical advancements (e.g., large-language models (LLMs), program synthesis) offer various methods for automatically generating repair plans that address such conflicts. In this work, we understand how automated repair and explanations can be designed to improve user experience with robot failures through two user studies. In our first, online study ($n=162$), users expressed increased trust, satisfaction, and utility with the robot performing automated repair and explanations. However, we also identified risk factors -- safety, privacy, and complexity -- that require adaptive repair strategies. The second, in-person study ($n=24$) elucidated distinct repair and explanation strategies depending on the level of risk severity and type. Using a design-based approach, we explore automated repair with explanations as a solution for robots to handle conflicts and failures, complemented by adaptive strategies for risk factors. Finally, we discuss the implications of incorporating such strategies into robot designs to achieve seamless operation among changing user needs and environments.","sentences":["Robots in real-world environments continuously engage with multiple users and encounter changes that lead to unexpected conflicts in fulfilling user requests.","Recent technical advancements (e.g., large-language models (LLMs), program synthesis) offer various methods for automatically generating repair plans that address such conflicts.","In this work, we understand how automated repair and explanations can be designed to improve user experience with robot failures through two user studies.","In our first, online study ($n=162$), users expressed increased trust, satisfaction, and utility with the robot performing automated repair and explanations.","However, we also identified risk factors -- safety, privacy, and complexity -- that require adaptive repair strategies.","The second, in-person study ($n=24$) elucidated distinct repair and explanation strategies depending on the level of risk severity and type.","Using a design-based approach, we explore automated repair with explanations as a solution for robots to handle conflicts and failures, complemented by adaptive strategies for risk factors.","Finally, we discuss the implications of incorporating such strategies into robot designs to achieve seamless operation among changing user needs and environments."],"url":"http://arxiv.org/abs/2405.16710v1","category":"cs.RO"}
{"created":"2024-05-26 20:41:36","title":"Reconstructing the Charlie Parker Omnibook using an audio-to-score automatic transcription pipeline","abstract":"The Charlie Parker Omnibook is a cornerstone of jazz music education, described by pianist Ethan Iverson as \"the most important jazz education text ever published\". In this work we propose a new transcription pipeline and explore the extent to which state of the art music technology is able to reconstruct these scores directly from the audio without human intervention. Our pipeline includes: a newly trained source separation model for saxophone, a new MIDI transcription model for solo saxophone and an adaptation of an existing MIDI-to-score method for monophonic instruments.   To assess this pipeline we also provide an enhanced dataset of Charlie Parker transcriptions as score-audio pairs with accurate MIDI alignments and downbeat annotations. This represents a challenging new benchmark for automatic audio-to-score transcription that we hope will advance research into areas beyond transcribing audio-to-MIDI alone.   Together, these form another step towards producing scores that musicians can use directly, without the need for onerous corrections or revisions. To facilitate future research, all model checkpoints and data are made available to download along with code for the transcription pipeline. Improvements in our modular pipeline could one day make the automatic transcription of complex jazz solos a routine possibility, thereby enriching the resources available for music education and preservation.","sentences":["The Charlie Parker Omnibook is a cornerstone of jazz music education, described by pianist Ethan Iverson as \"the most important jazz education text ever published\".","In this work we propose a new transcription pipeline and explore the extent to which state of the art music technology is able to reconstruct these scores directly from the audio without human intervention.","Our pipeline includes: a newly trained source separation model for saxophone, a new MIDI transcription model for solo saxophone and an adaptation of an existing MIDI-to-score method for monophonic instruments.   ","To assess this pipeline we also provide an enhanced dataset of Charlie Parker transcriptions as score-audio pairs with accurate MIDI alignments and downbeat annotations.","This represents a challenging new benchmark for automatic audio-to-score transcription that we hope will advance research into areas beyond transcribing audio-to-MIDI alone.   ","Together, these form another step towards producing scores that musicians can use directly, without the need for onerous corrections or revisions.","To facilitate future research, all model checkpoints and data are made available to download along with code for the transcription pipeline.","Improvements in our modular pipeline could one day make the automatic transcription of complex jazz solos a routine possibility, thereby enriching the resources available for music education and preservation."],"url":"http://arxiv.org/abs/2405.16687v1","category":"cs.SD"}
{"created":"2024-05-26 19:44:42","title":"iDIGIT4L. Nuevos ecosistemas de digitalizaci\u00f3n y aprendizaje hombre-m\u00e1quina para sistemas de fabricaci\u00f3n industrial heredados","abstract":"The digitization of the productive ecosystem related to human-machine interaction has become a priority for small and medium-sized enterprises. Particularly to face the challenges of Industry 4.0 and advanced digital skills in the workplace. From the research point of view, digitization opens a global scenario for the generation of opportunities for learning in existing manufacturing systems. Mainly, traditional environments must deal with competitive pressures to incorporate new technologies and adapt the skills of workers. In this paper is presented the iDIGIT4L project, which was envisaged to research and develop a digitization ecosystem where people and systems interact in order to transform industrial processes in an intelligent and predictive way. The project contributes with a three-tier human-machine learning methodology that provides augmented and bi-directional interaction in a traditional manufacturing scenario. It is based on the implementation of a non-intrusively integrated digital twin, characterizing an old industrial milling machine for learning through knowledge models supported by the experience of skilled workers. As a result, it has been possible to simultaneously update the functionalities of the industrial system and the digital skills of the workers, becoming an integral part of the digital twin.","sentences":["The digitization of the productive ecosystem related to human-machine interaction has become a priority for small and medium-sized enterprises.","Particularly to face the challenges of Industry 4.0 and advanced digital skills in the workplace.","From the research point of view, digitization opens a global scenario for the generation of opportunities for learning in existing manufacturing systems.","Mainly, traditional environments must deal with competitive pressures to incorporate new technologies and adapt the skills of workers.","In this paper is presented the iDIGIT4L project, which was envisaged to research and develop a digitization ecosystem where people and systems interact in order to transform industrial processes in an intelligent and predictive way.","The project contributes with a three-tier human-machine learning methodology that provides augmented and bi-directional interaction in a traditional manufacturing scenario.","It is based on the implementation of a non-intrusively integrated digital twin, characterizing an old industrial milling machine for learning through knowledge models supported by the experience of skilled workers.","As a result, it has been possible to simultaneously update the functionalities of the industrial system and the digital skills of the workers, becoming an integral part of the digital twin."],"url":"http://arxiv.org/abs/2405.16676v1","category":"cs.HC"}
{"created":"2024-05-26 19:25:08","title":"Mixture of Experts Using Tensor Products","abstract":"In multi-task learning, the conventional approach involves training a model on multiple tasks simultaneously. However, the training signals from different tasks can interfere with one another, potentially leading to \\textit{negative transfer}. To mitigate this, we investigate if modular language models can facilitate positive transfer and systematic generalization. Specifically, we propose a novel modular language model (\\texttt{TensorPoly}), that balances parameter efficiency with nuanced routing methods. For \\textit{modules}, we reparameterize Low-Rank Adaptation (\\texttt{LoRA}) by employing an entangled tensor through the use of tensor product operations and name the resulting approach \\texttt{TLoRA}. For \\textit{routing function}, we tailor two innovative routing functions according to the granularity: \\texttt{TensorPoly-I} which directs to each rank within the entangled tensor while \\texttt{TensorPoly-II} offers a finer-grained routing approach targeting each order of the entangled tensor. The experimental results from the multi-task T0-benchmark demonstrate that: 1) all modular LMs surpass the corresponding dense approaches, highlighting the potential of modular language models to mitigate negative inference in multi-task learning and deliver superior outcomes. 2) \\texttt{TensorPoly-I} achieves higher parameter efficiency in adaptation and outperforms other modular LMs, which shows the potential of our approach in multi-task transfer learning.","sentences":["In multi-task learning, the conventional approach involves training a model on multiple tasks simultaneously.","However, the training signals from different tasks can interfere with one another, potentially leading to \\textit{negative transfer}.","To mitigate this, we investigate if modular language models can facilitate positive transfer and systematic generalization.","Specifically, we propose a novel modular language model (\\texttt{TensorPoly}), that balances parameter efficiency with nuanced routing methods.","For \\textit{modules}, we reparameterize Low-Rank Adaptation (\\texttt{LoRA}) by employing an entangled tensor through the use of tensor product operations and name the resulting approach \\texttt{TLoRA}.","For \\textit{routing function}, we tailor two innovative routing functions according to the granularity: \\texttt{TensorPoly-I} which directs to each rank within the entangled tensor while \\texttt{TensorPoly-II} offers a finer-grained routing approach targeting each order of the entangled tensor.","The experimental results from the multi-task T0-benchmark demonstrate that: 1) all modular LMs surpass the corresponding dense approaches, highlighting the potential of modular language models to mitigate negative inference in multi-task learning and deliver superior outcomes.","2) \\texttt{TensorPoly-I} achieves higher parameter efficiency in adaptation and outperforms other modular LMs, which shows the potential of our approach in multi-task transfer learning."],"url":"http://arxiv.org/abs/2405.16671v1","category":"cs.LG"}
{"created":"2024-05-26 19:20:26","title":"Low-resourced Languages and Online Knowledge Repositories: A Need-Finding Study","abstract":"Online Knowledge Repositories (OKRs) like Wikipedia offer communities a way to share and preserve information about themselves and their ways of living. However, for communities with low-resourced languages -- including most African communities -- the quality and volume of content available are often inadequate. One reason for this lack of adequate content could be that many OKRs embody Western ways of knowledge preservation and sharing, requiring many low-resourced language communities to adapt to new interactions. To understand the challenges faced by low-resourced language contributors on the popular OKR Wikipedia, we conducted (1) a thematic analysis of Wikipedia forum discussions and (2) a contextual inquiry study with 14 novice contributors. We focused on three Ethiopian languages: Afan Oromo, Amharic, and Tigrinya. Our analysis revealed several recurring themes; for example, contributors struggle to find resources to corroborate their articles in low-resourced languages, and language technology support, like translation systems and spellcheck, result in several errors that waste contributors' time. We hope our study will support designers in making online knowledge repositories accessible to low-resourced language speakers.","sentences":["Online Knowledge Repositories (OKRs) like Wikipedia offer communities a way to share and preserve information about themselves and their ways of living.","However, for communities with low-resourced languages -- including most African communities -- the quality and volume of content available are often inadequate.","One reason for this lack of adequate content could be that many OKRs embody Western ways of knowledge preservation and sharing, requiring many low-resourced language communities to adapt to new interactions.","To understand the challenges faced by low-resourced language contributors on the popular OKR Wikipedia, we conducted (1) a thematic analysis of Wikipedia forum discussions and (2) a contextual inquiry study with 14 novice contributors.","We focused on three Ethiopian languages: Afan Oromo, Amharic, and Tigrinya.","Our analysis revealed several recurring themes; for example, contributors struggle to find resources to corroborate their articles in low-resourced languages, and language technology support, like translation systems and spellcheck, result in several errors that waste contributors' time.","We hope our study will support designers in making online knowledge repositories accessible to low-resourced language speakers."],"url":"http://arxiv.org/abs/2405.16669v1","category":"cs.HC"}
{"created":"2024-05-26 18:10:49","title":"A two-speed actuator for robotics with fast seamless gear shifting","abstract":"This paper present a novel dual-speed actuator adapted to robotics. In many applications, robots have to bear large loads while moving slowly and also have to move quickly through the air with almost no load. This lead to conflicting requirements for their actuators. Multiple gear ratios address this issue by allowing an effective use of power over a wide range of torque-speed load conditions. Furthermore, very different gear ratios also lead to drastic changes of the intrinsic impedance, enabling a non-back-drivable mode for stiff position control and a back-drivable mode for force control. The proposed actuator consists of two electric motors coupled to a differential; one has a large gear ratio while the other is almost direct-drive and equipped with a brake. During the high-force mode the brake is locked, only one motor is used, and the actuator behaves like a regular highly-geared servo-motor. During the high-speed mode the brake is open, both motor are used at the same time, and the actuator behaves like a direct drive motor. A dynamic model is developed and novel controllers are proposed for synergic use of both motors. The redundancy of motors is exploited for maintaining full control of the output during mode transitions, allowing for fast and seamless switching even when interacting with unknown environments. Results are demonstrated with a proof-of-concept linear actuator.","sentences":["This paper present a novel dual-speed actuator adapted to robotics.","In many applications, robots have to bear large loads while moving slowly and also have to move quickly through the air with almost no load.","This lead to conflicting requirements for their actuators.","Multiple gear ratios address this issue by allowing an effective use of power over a wide range of torque-speed load conditions.","Furthermore, very different gear ratios also lead to drastic changes of the intrinsic impedance, enabling a non-back-drivable mode for stiff position control and a back-drivable mode for force control.","The proposed actuator consists of two electric motors coupled to a differential; one has a large gear ratio while the other is almost direct-drive and equipped with a brake.","During the high-force mode the brake is locked, only one motor is used, and the actuator behaves like a regular highly-geared servo-motor.","During the high-speed mode the brake is open, both motor are used at the same time, and the actuator behaves like a direct drive motor.","A dynamic model is developed and novel controllers are proposed for synergic use of both motors.","The redundancy of motors is exploited for maintaining full control of the output during mode transitions, allowing for fast and seamless switching even when interacting with unknown environments.","Results are demonstrated with a proof-of-concept linear actuator."],"url":"http://arxiv.org/abs/2405.16652v1","category":"cs.RO"}
{"created":"2024-05-26 17:38:44","title":"Pick up the PACE: A Parameter-Free Optimizer for Lifelong Reinforcement Learning","abstract":"A key challenge in lifelong reinforcement learning (RL) is the loss of plasticity, where previous learning progress hinders an agent's adaptation to new tasks. While regularization and resetting can help, they require precise hyperparameter selection at the outset and environment-dependent adjustments. Building on the principled theory of online convex optimization, we present a parameter-free optimizer for lifelong RL, called PACE, which requires no tuning or prior knowledge about the distribution shifts. Extensive experiments on Procgen, Atari, and Gym Control environments show that PACE works surprisingly well$\\unicode{x2013}$mitigating loss of plasticity and rapidly adapting to challenging distribution shifts$\\unicode{x2013}$despite the underlying optimization problem being nonconvex and nonstationary.","sentences":["A key challenge in lifelong reinforcement learning (RL) is the loss of plasticity, where previous learning progress hinders an agent's adaptation to new tasks.","While regularization and resetting can help, they require precise hyperparameter selection at the outset and environment-dependent adjustments.","Building on the principled theory of online convex optimization, we present a parameter-free optimizer for lifelong RL, called PACE, which requires no tuning or prior knowledge about the distribution shifts.","Extensive experiments on Procgen, Atari, and Gym Control environments show that PACE works surprisingly well$\\unicode{x2013}$mitigating loss of plasticity and rapidly adapting to challenging distribution shifts$\\unicode{x2013}$despite the underlying optimization problem being nonconvex and nonstationary."],"url":"http://arxiv.org/abs/2405.16642v1","category":"cs.LG"}
{"created":"2024-05-26 17:31:21","title":"A Survey of Multimodal Large Language Model from A Data-centric Perspective","abstract":"Human beings perceive the world through diverse senses such as sight, smell, hearing, and touch. Similarly, multimodal large language models (MLLMs) enhance the capabilities of traditional large language models by integrating and processing data from multiple modalities including text, vision, audio, video, and 3D environments. Data plays a pivotal role in the development and refinement of these models. In this survey, we comprehensively review the literature on MLLMs from a data-centric perspective. Specifically, we explore methods for preparing multimodal data during the pretraining and adaptation phases of MLLMs. Additionally, we analyze the evaluation methods for datasets and review benchmarks for evaluating MLLMs. Our survey also outlines potential future research directions. This work aims to provide researchers with a detailed understanding of the data-driven aspects of MLLMs, fostering further exploration and innovation in this field.","sentences":["Human beings perceive the world through diverse senses such as sight, smell, hearing, and touch.","Similarly, multimodal large language models (MLLMs) enhance the capabilities of traditional large language models by integrating and processing data from multiple modalities including text, vision, audio, video, and 3D environments.","Data plays a pivotal role in the development and refinement of these models.","In this survey, we comprehensively review the literature on MLLMs from a data-centric perspective.","Specifically, we explore methods for preparing multimodal data during the pretraining and adaptation phases of MLLMs.","Additionally, we analyze the evaluation methods for datasets and review benchmarks for evaluating MLLMs.","Our survey also outlines potential future research directions.","This work aims to provide researchers with a detailed understanding of the data-driven aspects of MLLMs, fostering further exploration and innovation in this field."],"url":"http://arxiv.org/abs/2405.16640v1","category":"cs.AI"}
{"created":"2024-05-26 16:30:43","title":"Error Performance Analysis of UAV-Mounted RIS for NOMA Systems with Practical Constraints","abstract":"Uncrewed aerial vehicles (UAVs) have attracted recent attention for sixth-generation (6G) networks due to their low cost and flexible deployment. In order to maximize the ever-increasing data rates, spectral efficiency, and wider coverage, technologies such as reconfigurable intelligent surface (RIS) and non-orthogonal multiple access (NOMA) are adapted with UAVs (UAV-RIS NOMA). However, the error performance of UAV-RIS NOMA has not been considered, yet. In this letter, we investigate the error probability of UAV-RIS NOMA systems. We also consider the practical constraints of hardware impairments (HWI) at the transceivers, inter-cell interference (ICI), and imperfect successive interference cancellation (SIC). The analytical derivations are validated by Monte-Carlo simulations. Our results demonstrate that our proposed system achieves higher performance gain (more than 5 dB with increasing the number of RIS elements) with less error probability compared to UAVs without RIS. Moreover, it is found that the HWI, ICI, and imperfect SIC have shown a negative impact on the system performance.","sentences":["Uncrewed aerial vehicles (UAVs) have attracted recent attention for sixth-generation (6G) networks due to their low cost and flexible deployment.","In order to maximize the ever-increasing data rates, spectral efficiency, and wider coverage, technologies such as reconfigurable intelligent surface (RIS) and non-orthogonal multiple access (NOMA) are adapted with UAVs (UAV-RIS NOMA).","However, the error performance of UAV-RIS NOMA has not been considered, yet.","In this letter, we investigate the error probability of UAV-RIS NOMA systems.","We also consider the practical constraints of hardware impairments (HWI) at the transceivers, inter-cell interference (ICI), and imperfect successive interference cancellation (SIC).","The analytical derivations are validated by Monte-Carlo simulations.","Our results demonstrate that our proposed system achieves higher performance gain (more than 5 dB with increasing the number of RIS elements) with less error probability compared to UAVs without RIS.","Moreover, it is found that the HWI, ICI, and imperfect SIC have shown a negative impact on the system performance."],"url":"http://arxiv.org/abs/2405.16620v1","category":"eess.SP"}
{"created":"2024-05-26 15:28:42","title":"A CMDP-within-online framework for Meta-Safe Reinforcement Learning","abstract":"Meta-reinforcement learning has widely been used as a learning-to-learn framework to solve unseen tasks with limited experience. However, the aspect of constraint violations has not been adequately addressed in the existing works, making their application restricted in real-world settings. In this paper, we study the problem of meta-safe reinforcement learning (Meta-SRL) through the CMDP-within-online framework to establish the first provable guarantees in this important setting. We obtain task-averaged regret bounds for the reward maximization (optimality gap) and constraint violations using gradient-based meta-learning and show that the task-averaged optimality gap and constraint satisfaction improve with task-similarity in a static environment or task-relatedness in a dynamic environment. Several technical challenges arise when making this framework practical. To this end, we propose a meta-algorithm that performs inexact online learning on the upper bounds of within-task optimality gap and constraint violations estimated by off-policy stationary distribution corrections. Furthermore, we enable the learning rates to be adapted for every task and extend our approach to settings with a competing dynamically changing oracle. Finally, experiments are conducted to demonstrate the effectiveness of our approach.","sentences":["Meta-reinforcement learning has widely been used as a learning-to-learn framework to solve unseen tasks with limited experience.","However, the aspect of constraint violations has not been adequately addressed in the existing works, making their application restricted in real-world settings.","In this paper, we study the problem of meta-safe reinforcement learning (Meta-SRL) through the CMDP-within-online framework to establish the first provable guarantees in this important setting.","We obtain task-averaged regret bounds for the reward maximization (optimality gap) and constraint violations using gradient-based meta-learning and show that the task-averaged optimality gap and constraint satisfaction improve with task-similarity in a static environment or task-relatedness in a dynamic environment.","Several technical challenges arise when making this framework practical.","To this end, we propose a meta-algorithm that performs inexact online learning on the upper bounds of within-task optimality gap and constraint violations estimated by off-policy stationary distribution corrections.","Furthermore, we enable the learning rates to be adapted for every task and extend our approach to settings with a competing dynamically changing oracle.","Finally, experiments are conducted to demonstrate the effectiveness of our approach."],"url":"http://arxiv.org/abs/2405.16601v1","category":"cs.LG"}
{"created":"2024-05-26 15:25:26","title":"Image-Text-Image Knowledge Transferring for Lifelong Person Re-Identification with Hybrid Clothing States","abstract":"With the continuous expansion of intelligent surveillance networks, lifelong person re-identification (LReID) has received widespread attention, pursuing the need of self-evolution across different domains. However, existing LReID studies accumulate knowledge with the assumption that people would not change their clothes. In this paper, we propose a more practical task, namely lifelong person re-identification with hybrid clothing states (LReID-Hybrid), which takes a series of cloth-changing and cloth-consistent domains into account during lifelong learning. To tackle the challenges of knowledge granularity mismatch and knowledge presentation mismatch that occurred in LReID-Hybrid, we take advantage of the consistency and generalization of the text space, and propose a novel framework, dubbed $Teata$, to effectively align, transfer and accumulate knowledge in an \"image-text-image\" closed loop. Concretely, to achieve effective knowledge transfer, we design a Structured Semantic Prompt (SSP) learning to decompose the text prompt into several structured pairs to distill knowledge from the image space with a unified granularity of text description. Then, we introduce a Knowledge Adaptation and Projection strategy (KAP), which tunes text knowledge via a slow-paced learner to adapt to different tasks without catastrophic forgetting. Extensive experiments demonstrate the superiority of our proposed $Teata$ for LReID-Hybrid as well as on conventional LReID benchmarks over advanced methods.","sentences":["With the continuous expansion of intelligent surveillance networks, lifelong person re-identification (LReID) has received widespread attention, pursuing the need of self-evolution across different domains.","However, existing LReID studies accumulate knowledge with the assumption that people would not change their clothes.","In this paper, we propose a more practical task, namely lifelong person re-identification with hybrid clothing states (LReID-Hybrid), which takes a series of cloth-changing and cloth-consistent domains into account during lifelong learning.","To tackle the challenges of knowledge granularity mismatch and knowledge presentation mismatch that occurred in LReID-Hybrid, we take advantage of the consistency and generalization of the text space, and propose a novel framework, dubbed $Teata$, to effectively align, transfer and accumulate knowledge in an \"image-text-image\" closed loop.","Concretely, to achieve effective knowledge transfer, we design a Structured Semantic Prompt (SSP) learning to decompose the text prompt into several structured pairs to distill knowledge from the image space with a unified granularity of text description.","Then, we introduce a Knowledge Adaptation and Projection strategy (KAP), which tunes text knowledge via a slow-paced learner to adapt to different tasks without catastrophic forgetting.","Extensive experiments demonstrate the superiority of our proposed $Teata$ for LReID-Hybrid as well as on conventional LReID benchmarks over advanced methods."],"url":"http://arxiv.org/abs/2405.16600v1","category":"cs.CV"}
{"created":"2024-05-26 15:14:54","title":"Protect-Your-IP: Scalable Source-Tracing and Attribution against Personalized Generation","abstract":"With the advent of personalized generation models, users can more readily create images resembling existing content, heightening the risk of violating portrait rights and intellectual property (IP). Traditional post-hoc detection and source-tracing methods for AI-generated content (AIGC) employ proactive watermark approaches; however, these are less effective against personalized generation models. Moreover, attribution techniques for AIGC rely on passive detection but often struggle to differentiate AIGC from authentic images, presenting a substantial challenge. Integrating these two processes into a cohesive framework not only meets the practical demands for protection and forensics but also improves the effectiveness of attribution tasks. Inspired by this insight, we propose a unified approach for image copyright source-tracing and attribution, introducing an innovative watermarking-attribution method that blends proactive and passive strategies. We embed copyright watermarks into protected images and train a watermark decoder to retrieve copyright information from the outputs of personalized models, using this watermark as an initial step for confirming if an image is AIGC-generated. To pinpoint specific generation techniques, we utilize powerful visual backbone networks for classification. Additionally, we implement an incremental learning strategy to adeptly attribute new personalized models without losing prior knowledge, thereby enhancing the model's adaptability to novel generation methods. We have conducted experiments using various celebrity portrait series sourced online, and the results affirm the efficacy of our method in source-tracing and attribution tasks, as well as its robustness against knowledge forgetting.","sentences":["With the advent of personalized generation models, users can more readily create images resembling existing content, heightening the risk of violating portrait rights and intellectual property (IP).","Traditional post-hoc detection and source-tracing methods for AI-generated content (AIGC) employ proactive watermark approaches; however, these are less effective against personalized generation models.","Moreover, attribution techniques for AIGC rely on passive detection but often struggle to differentiate AIGC from authentic images, presenting a substantial challenge.","Integrating these two processes into a cohesive framework not only meets the practical demands for protection and forensics but also improves the effectiveness of attribution tasks.","Inspired by this insight, we propose a unified approach for image copyright source-tracing and attribution, introducing an innovative watermarking-attribution method that blends proactive and passive strategies.","We embed copyright watermarks into protected images and train a watermark decoder to retrieve copyright information from the outputs of personalized models, using this watermark as an initial step for confirming if an image is AIGC-generated.","To pinpoint specific generation techniques, we utilize powerful visual backbone networks for classification.","Additionally, we implement an incremental learning strategy to adeptly attribute new personalized models without losing prior knowledge, thereby enhancing the model's adaptability to novel generation methods.","We have conducted experiments using various celebrity portrait series sourced online, and the results affirm the efficacy of our method in source-tracing and attribution tasks, as well as its robustness against knowledge forgetting."],"url":"http://arxiv.org/abs/2405.16596v1","category":"cs.CV"}
{"created":"2024-05-28 16:59:20","title":"A comparison of mixed precision iterative refinement approaches for least-squares problems","abstract":"Various approaches to iterative refinement (IR) for least-squares problems have been proposed in the literature and it may not be clear which approach is suitable for a given problem. We consider three approaches to IR for least-squares problems when two precisions are used and review their theoretical guarantees, known shortcomings and when the method can be expected to recognize that the correct solution has been found, and extend uniform precision analysis for an IR approach based on the semi-normal equations to the two-precision case. We focus on the situation where it is desired to refine the solution to the working precision level. It is shown that the IR methods exhibit different sensitivities to the conditioning of the problem and the size of the least-squares residual, which should be taken into account when choosing the IR approach. We also discuss a new approach that is based on solving multiple least-squares problems.","sentences":["Various approaches to iterative refinement (IR) for least-squares problems have been proposed in the literature and it may not be clear which approach is suitable for a given problem.","We consider three approaches to IR for least-squares problems when two precisions are used and review their theoretical guarantees, known shortcomings and when the method can be expected to recognize that the correct solution has been found, and extend uniform precision analysis for an IR approach based on the semi-normal equations to the two-precision case.","We focus on the situation where it is desired to refine the solution to the working precision level.","It is shown that the IR methods exhibit different sensitivities to the conditioning of the problem and the size of the least-squares residual, which should be taken into account when choosing the IR approach.","We also discuss a new approach that is based on solving multiple least-squares problems."],"url":"http://arxiv.org/abs/2405.18363v1","category":"math.NA"}
{"created":"2024-05-28 16:50:42","title":"Evolutionary Algorithms for Optimizing Emergency Exit Placement in Indoor Environments","abstract":"The problem of finding the optimal placement of emergency exits in an indoor environment to facilitate the rapid and orderly evacuation of crowds is addressed in this work. A cellular-automaton model is used to simulate the behavior of pedestrians in such scenarios, taking into account factors such as the environment, the pedestrians themselves, and the interactions among them. A metric is proposed to determine how successful or satisfactory an evacuation was. Subsequently, two metaheuristic algorithms, namely an iterated greedy heuristic and an evolutionary algorithm (EA) are proposed to solve the optimization problem. A comparative analysis shows that the proposed EA is able to find effective solutions for different scenarios, and that an island-based version of it outperforms the other two algorithms in terms of solution quality.","sentences":["The problem of finding the optimal placement of emergency exits in an indoor environment to facilitate the rapid and orderly evacuation of crowds is addressed in this work.","A cellular-automaton model is used to simulate the behavior of pedestrians in such scenarios, taking into account factors such as the environment, the pedestrians themselves, and the interactions among them.","A metric is proposed to determine how successful or satisfactory an evacuation was.","Subsequently, two metaheuristic algorithms, namely an iterated greedy heuristic and an evolutionary algorithm (EA) are proposed to solve the optimization problem.","A comparative analysis shows that the proposed EA is able to find effective solutions for different scenarios, and that an island-based version of it outperforms the other two algorithms in terms of solution quality."],"url":"http://arxiv.org/abs/2405.18352v1","category":"cs.NE"}
{"created":"2024-05-28 16:24:43","title":"Spatial-temporal analysis of neural desynchronization in sleep-like states reveals critical dynamics","abstract":"Sleep is characterized by non-rapid eye movement (nREM) sleep, originating from widespread neuronal synchrony, and REM sleep, with neuronal desynchronization akin to waking behavior. While these were thought to be global brain states, recent research suggests otherwise. Using time-frequency analysis of mesoscopic voltage-sensitive dye recordings of mice in a urethane-anesthetized model of sleep, we find transient neural desynchronization occurring heterogeneously across the cortex within a background of synchronized neural activity, in a manner reminiscent of a critical spreading process and indicative of an \"edge-of-synchronization phase\" transition.","sentences":["Sleep is characterized by non-rapid eye movement (nREM) sleep, originating from widespread neuronal synchrony, and REM sleep, with neuronal desynchronization akin to waking behavior.","While these were thought to be global brain states, recent research suggests otherwise.","Using time-frequency analysis of mesoscopic voltage-sensitive dye recordings of mice in a urethane-anesthetized model of sleep, we find transient neural desynchronization occurring heterogeneously across the cortex within a background of synchronized neural activity, in a manner reminiscent of a critical spreading process and indicative of an \"edge-of-synchronization phase\" transition."],"url":"http://arxiv.org/abs/2405.18329v1","category":"q-bio.NC"}
{"created":"2024-05-28 15:36:46","title":"Minimal hypersurfaces in $\\mathbb{S}^{4}(1)$ by doubling the equatorial $\\mathbb{S}^{3}$","abstract":"For each large enough $m\\in\\mathbb{N}$ we construct by PDE gluing methods a closed embedded minimal hypersurface ${\\breve{M}_m}$ doubling the equatorial three-sphere $\\mathbb{S}_{eq}^3$ in $\\mathbb{S}^4(1)$, with ${\\breve{M}_m}$ containing $m^2$ bridges modelled after the three-dimensional catenoid and centered at the points of a square $m\\times m$ lattice $L$ contained in the Clifford torus $\\mathbb{T}^2\\subset \\mathbb{S}_{eq}^3$. The construction respects the symmetries of the lattice $L$ as a subset of $\\mathbb{S}^4(1)$ and is based on the Linearized Doubling (LD) methodology which was first introduced in the construction of minimal surface doublings of $\\mathbb{S}_{eq}^2$ in $\\mathbb{S}^3(1)$.","sentences":["For each large enough $m\\in\\mathbb{N}$ we construct by PDE gluing methods a closed embedded minimal hypersurface ${\\breve{M}_m}$ doubling the equatorial three-sphere $\\mathbb{S}_{eq}^3$ in $\\mathbb{S}^4(1)$, with ${\\breve{M}_m}$ containing $m^2$ bridges modelled after the three-dimensional catenoid and centered at the points of a square $m\\times m$ lattice $L$ contained in the Clifford torus $\\mathbb{T}^2\\subset \\mathbb{S}_{eq}^3$. The construction respects the symmetries of the lattice $L$ as a subset of $\\mathbb{S}^4(1)$ and is based on the Linearized Doubling (LD) methodology which was first introduced in the construction of minimal surface doublings of $\\mathbb{S}_{eq}^2$ in $\\mathbb{S}^3(1)$."],"url":"http://arxiv.org/abs/2405.18283v1","category":"math.DG"}
{"created":"2024-05-28 14:51:32","title":"Compiling with Arrays","abstract":"Linear algebra computations are foundational for neural networks and machine learning, often handled through arrays. While many functional programming languages feature lists and recursion, arrays in linear algebra demand constant-time access and bulk operations. To bridge this gap, some languages represent arrays as (eager) functions instead of lists. In this paper, we connect this idea to a formal logical foundation by interpreting functions as the usual negative types from polarized type theory, and arrays as the corresponding dual positive version of the function type. Positive types are defined to have a single elimination form whose computational interpretation is pattern matching. Just like (positive) product types bind two variables during pattern matching, (positive) array types bind variables with multiplicity during pattern matching. We follow a similar approach for Booleans by introducing conditionally-defined variables.   The positive formulation for the array type enables us to combine typed partial evaluation and common subexpression elimination into an elegant algorithm whose result enjoys a property we call maximal fission, which we argue can be beneficial for further optimizations. For this purpose, we present the novel intermediate representation indexed administrative normal form (AiNF), which relies on the formal logical foundation of the positive formulation for the array type to facilitate maximal loop fission and subsequent optimizations. AiNF is normal with regard to commuting conversion for both let-bindings and for-loops, leading to flat and maximally fissioned terms. We mechanize the translation and normalization from a simple surface language to AiNF, establishing that the process terminates, preserves types, and produces maximally fissioned terms.","sentences":["Linear algebra computations are foundational for neural networks and machine learning, often handled through arrays.","While many functional programming languages feature lists and recursion, arrays in linear algebra demand constant-time access and bulk operations.","To bridge this gap, some languages represent arrays as (eager) functions instead of lists.","In this paper, we connect this idea to a formal logical foundation by interpreting functions as the usual negative types from polarized type theory, and arrays as the corresponding dual positive version of the function type.","Positive types are defined to have a single elimination form whose computational interpretation is pattern matching.","Just like (positive) product types bind two variables during pattern matching, (positive) array types bind variables with multiplicity during pattern matching.","We follow a similar approach for Booleans by introducing conditionally-defined variables.   ","The positive formulation for the array type enables us to combine typed partial evaluation and common subexpression elimination into an elegant algorithm whose result enjoys a property we call maximal fission, which we argue can be beneficial for further optimizations.","For this purpose, we present the novel intermediate representation indexed administrative normal form (AiNF), which relies on the formal logical foundation of the positive formulation for the array type to facilitate maximal loop fission and subsequent optimizations.","AiNF is normal with regard to commuting conversion for both let-bindings and for-loops, leading to flat and maximally fissioned terms.","We mechanize the translation and normalization from a simple surface language to AiNF, establishing that the process terminates, preserves types, and produces maximally fissioned terms."],"url":"http://arxiv.org/abs/2405.18242v1","category":"cs.PL"}
{"created":"2024-05-28 14:30:36","title":"A path towards constraining the evolution of the interstellar medium and outflows in the Milky Way using APOGEE","abstract":"In recent years, the study of the Milky Way has significantly advanced due to extensive spectroscopic surveys of its stars, complemented by astroseismic and astrometric data. However, it remains disjoint from recent advancements in understanding the physics of the Galactic interstellar medium (ISM). This paper introduces a new model for the chemical evolution of the Milky Way that can be constrained on stellar data, because it combines a state-of-the-art ISM model with a Milky Way stellar disc model. Utilizing a dataset of red clump stars from APOGEE, known for their precise ages and metallicities, we concentrate on the last 6 billion years -- a period marked by Milky Way's secular evolution. We examine the oxygen abundance in the low-$\\alpha$ disc stars relative to their ages and birth radii, validating or constraining critical ISM parameters that remain largely unexplored in extragalactic observations. The models that successfully reproduce the radius -- metallicity distribution and the age -- metallicity distribution of stars without violating existing ISM observations indicate a need for modest differential oxygen enrichment in Galactic outflows, meaning that the oxygen abundance of outflows is higher than the local ISM abundance, irrespective of outflow mass loading. The models also suggest somewhat elevated ISM gas velocity dispersion levels over the past 6 billion years compared to galaxies of similar mass. The extra turbulence necessary could result from energy from gas accretion onto the Galaxy, supernovae clustering in the ISM, or increased star formation efficiency per freefall time. This work provides a novel approach to constraining the Galactic ISM and outflows, leveraging the detailed insights available from contemporary Milky Way surveys.","sentences":["In recent years, the study of the Milky Way has significantly advanced due to extensive spectroscopic surveys of its stars, complemented by astroseismic and astrometric data.","However, it remains disjoint from recent advancements in understanding the physics of the Galactic interstellar medium (ISM).","This paper introduces a new model for the chemical evolution of the Milky Way that can be constrained on stellar data, because it combines a state-of-the-art ISM model with a Milky Way stellar disc model.","Utilizing a dataset of red clump stars from APOGEE, known for their precise ages and metallicities, we concentrate on the last 6 billion years -- a period marked by Milky Way's secular evolution.","We examine the oxygen abundance in the low-$\\alpha$ disc stars relative to their ages and birth radii, validating or constraining critical ISM parameters that remain largely unexplored in extragalactic observations.","The models that successfully reproduce the radius -- metallicity distribution and the age -- metallicity distribution of stars without violating existing ISM observations indicate a need for modest differential oxygen enrichment in Galactic outflows, meaning that the oxygen abundance of outflows is higher than the local ISM abundance, irrespective of outflow mass loading.","The models also suggest somewhat elevated ISM gas velocity dispersion levels over the past 6 billion years compared to galaxies of similar mass.","The extra turbulence necessary could result from energy from gas accretion onto the Galaxy, supernovae clustering in the ISM, or increased star formation efficiency per freefall time.","This work provides a novel approach to constraining the Galactic ISM and outflows, leveraging the detailed insights available from contemporary Milky Way surveys."],"url":"http://arxiv.org/abs/2405.18223v1","category":"astro-ph.GA"}
{"created":"2024-05-28 14:04:09","title":"Delving into Differentially Private Transformer","abstract":"Deep learning with differential privacy (DP) has garnered significant attention over the past years, leading to the development of numerous methods aimed at enhancing model accuracy and training efficiency. This paper delves into the problem of training Transformer models with differential privacy. Our treatment is modular: the logic is to `reduce' the problem of training DP Transformer to the more basic problem of training DP vanilla neural nets. The latter is better understood and amenable to many model-agnostic methods. Such `reduction' is done by first identifying the hardness unique to DP Transformer training: the attention distraction phenomenon and a lack of compatibility with existing techniques for efficient gradient clipping. To deal with these two issues, we propose the Re-Attention Mechanism and Phantom Clipping, respectively. We believe that our work not only casts new light on training DP Transformers but also promotes a modular treatment to advance research in the field of differentially private deep learning.","sentences":["Deep learning with differential privacy (DP) has garnered significant attention over the past years, leading to the development of numerous methods aimed at enhancing model accuracy and training efficiency.","This paper delves into the problem of training Transformer models with differential privacy.","Our treatment is modular: the logic is to `reduce' the problem of training DP Transformer to the more basic problem of training DP vanilla neural nets.","The latter is better understood and amenable to many model-agnostic methods.","Such `reduction' is done by first identifying the hardness unique to DP Transformer training: the attention distraction phenomenon and a lack of compatibility with existing techniques for efficient gradient clipping.","To deal with these two issues, we propose the Re-Attention Mechanism and Phantom Clipping, respectively.","We believe that our work not only casts new light on training DP Transformers but also promotes a modular treatment to advance research in the field of differentially private deep learning."],"url":"http://arxiv.org/abs/2405.18194v1","category":"cs.LG"}
{"created":"2024-05-28 13:53:58","title":"Revisiting the strain-induced softening behaviour in hydrogels","abstract":"Usually, the strain-induced softening behaviour observed in the differential modulus $K(T,\\gamma)$ of hydrogels has been attributed to the breakage of internal structures of the network, such as the cross-links that bind together the polymer chains. Here we consider a stress-strain relationship that we have recently derived from a coarse-grained model to demonstrate that no rupture of the network is needed for rubber-like gels to present such behaviour. In particular, we show that, in some cases, the decreasing of $K(T,\\gamma)$ as a function of the strain $\\gamma$ is closely related to the energy-related contribution to the elastic modulus that has been experimentally observed, e.g., for tetra-PEG hydrogels. Thus, our results suggest that, instead of the breakage of structures, the softening behaviour can be also related to the effective interaction between the chains in the network and their neighbouring solvent molecules. Comparison to experimental data determined for several hydrogels is included to illustrate that behaviour and to validate our approach.","sentences":["Usually, the strain-induced softening behaviour observed in the differential modulus $K(T,\\gamma)$ of hydrogels has been attributed to the breakage of internal structures of the network, such as the cross-links that bind together the polymer chains.","Here we consider a stress-strain relationship that we have recently derived from a coarse-grained model to demonstrate that no rupture of the network is needed for rubber-like gels to present such behaviour.","In particular, we show that, in some cases, the decreasing of $K(T,\\gamma)$ as a function of the strain $\\gamma$ is closely related to the energy-related contribution to the elastic modulus that has been experimentally observed, e.g., for tetra-PEG hydrogels.","Thus, our results suggest that, instead of the breakage of structures, the softening behaviour can be also related to the effective interaction between the chains in the network and their neighbouring solvent molecules.","Comparison to experimental data determined for several hydrogels is included to illustrate that behaviour and to validate our approach."],"url":"http://arxiv.org/abs/2405.18185v1","category":"cond-mat.soft"}
{"created":"2024-05-28 13:44:34","title":"Feynman Integral Reductions by Intersection Theory with Orthogonal Bases and Closed Formulae","abstract":"We present a prescription for choosing orthogonal bases of differential $n$-forms belonging to quadratic twisted period integrals, with respect to the intersection number inner product. To evaluate these inner products, we additionally propose a new closed formula for intersection numbers beyond $\\mathrm{d} \\log$ forms. These findings allow us to systematically construct orthonormal bases between twisted period integrals of this type. In the context of Feynman integrals, this represents all diagrams at one-loop.","sentences":["We present a prescription for choosing orthogonal bases of differential $n$-forms belonging to quadratic twisted period integrals, with respect to the intersection number inner product.","To evaluate these inner products, we additionally propose a new closed formula for intersection numbers beyond $\\mathrm{d} \\log$ forms.","These findings allow us to systematically construct orthonormal bases between twisted period integrals of this type.","In the context of Feynman integrals, this represents all diagrams at one-loop."],"url":"http://arxiv.org/abs/2405.18178v1","category":"hep-th"}
{"created":"2024-05-28 13:22:34","title":"Finsler $p$-Laplace equation with a potential: Maz'ya-type characterization and attainments of the Hardy constant","abstract":"We study positive properties of the quasilinear elliptic equation   $$-\\mathrm{div}\\mathcal{A}(x,\\nabla u)+V|u|^{p-2}u=0\\quad (1<p<\\infty)\\qquad \\mbox{ in } \\Omega,$$ where the function $\\mathcal{A}(x,\\xi)$ is induced by a family of norms on $\\mathbb{R}^{n}$ ($n\\geq 2$) parameterized by points in the domain $\\Omega\\subseteq\\mathbb{R}^{n}$, and $V$ belongs to a certain local Morrey space. We first establish two-sided estimates for Bregman distances of $|\\xi|^{p}_{s,a}$ ($1<s<\\infty$), where $a=(a_{1},a_{2},\\ldots,a_{n})$ and $a_{1},a_{2},\\ldots,a_{n}$ are certain functions with positive local lower and upper bounds in $\\Omega$. These estimates lead to a Maz'ya-type characterization for Hardy-weights of the corresponding functionals. Then we prove three types of sufficient conditions for the attainment of the Hardy constant in a certain space $\\widetilde{W}^{1,p}_{0}(\\Omega)$.","sentences":["We study positive properties of the quasilinear elliptic equation   $$-\\mathrm{div}\\mathcal{A}(x,\\nabla u)+V|u|^{p-2}u=0\\quad (1<p<\\infty)\\qquad \\mbox{ in } \\Omega,$$ where the function $\\mathcal{A}(x,\\xi)$ is induced by a family of norms on $\\mathbb{R}^{n}$ ($n\\geq 2$) parameterized by points in the domain $\\Omega\\subseteq\\mathbb{R}^{n}$, and $V$ belongs to a certain local Morrey space.","We first establish two-sided estimates for Bregman distances of $|\\xi|^{p}_{s,a}$ ($1<s<\\infty$), where $a=(a_{1},a_{2},\\ldots,a_{n})$ and $a_{1},a_{2},\\ldots,a_{n}$ are certain functions with positive local lower and upper bounds in $\\Omega$. These estimates lead to a Maz'ya-type characterization for Hardy-weights of the corresponding functionals.","Then we prove three types of sufficient conditions for the attainment of the Hardy constant in a certain space $\\widetilde{W}^{1,p}_{0}(\\Omega)$."],"url":"http://arxiv.org/abs/2405.18159v1","category":"math.AP"}
{"created":"2024-05-28 12:47:49","title":"A Grid-Free Fluid Solver based on Gaussian Spatial Representation","abstract":"We present a grid-free fluid solver featuring a novel Gaussian representation. Drawing inspiration from the expressive capabilities of 3D Gaussian Splatting in multi-view image reconstruction, we model the continuous flow velocity as a weighted sum of multiple Gaussian functions. Leveraging this representation, we derive differential operators for the field and implement a time-dependent PDE solver using the traditional operator splitting method. Compared to implicit neural representations as another continuous spatial representation with increasing attention, our method with flexible 3D Gaussians presents enhanced accuracy on vorticity preservation. Moreover, we apply physics-driven strategies to accelerate the optimization-based time integration of Gaussian functions. This temporal evolution surpasses previous work based on implicit neural representation with reduced computational time and memory. Although not surpassing the quality of state-of-the-art Eulerian methods in fluid simulation, experiments and ablation studies indicate the potential of our memory-efficient representation. With enriched spatial information, our method exhibits a distinctive perspective combining the advantages of Eulerian and Lagrangian approaches.","sentences":["We present a grid-free fluid solver featuring a novel Gaussian representation.","Drawing inspiration from the expressive capabilities of 3D Gaussian Splatting in multi-view image reconstruction, we model the continuous flow velocity as a weighted sum of multiple Gaussian functions.","Leveraging this representation, we derive differential operators for the field and implement a time-dependent PDE solver using the traditional operator splitting method.","Compared to implicit neural representations as another continuous spatial representation with increasing attention, our method with flexible 3D Gaussians presents enhanced accuracy on vorticity preservation.","Moreover, we apply physics-driven strategies to accelerate the optimization-based time integration of Gaussian functions.","This temporal evolution surpasses previous work based on implicit neural representation with reduced computational time and memory.","Although not surpassing the quality of state-of-the-art Eulerian methods in fluid simulation, experiments and ablation studies indicate the potential of our memory-efficient representation.","With enriched spatial information, our method exhibits a distinctive perspective combining the advantages of Eulerian and Lagrangian approaches."],"url":"http://arxiv.org/abs/2405.18133v1","category":"cs.GR"}
{"created":"2024-05-28 12:39:24","title":"Graph Coarsening with Message-Passing Guarantees","abstract":"Graph coarsening aims to reduce the size of a large graph while preserving some of its key properties, which has been used in many applications to reduce computational load and memory footprint. For instance, in graph machine learning, training Graph Neural Networks (GNNs) on coarsened graphs leads to drastic savings in time and memory. However, GNNs rely on the Message-Passing (MP) paradigm, and classical spectral preservation guarantees for graph coarsening do not directly lead to theoretical guarantees when performing naive message-passing on the coarsened graph. In this work, we propose a new message-passing operation specific to coarsened graphs, which exhibit theoretical guarantees on the preservation of the propagated signal. Interestingly, and in a sharp departure from previous proposals, this operation on coarsened graphs is oriented, even when the original graph is undirected. We conduct node classification tasks on synthetic and real data and observe improved results compared to performing naive message-passing on the coarsened graph.","sentences":["Graph coarsening aims to reduce the size of a large graph while preserving some of its key properties, which has been used in many applications to reduce computational load and memory footprint.","For instance, in graph machine learning, training Graph Neural Networks (GNNs) on coarsened graphs leads to drastic savings in time and memory.","However, GNNs rely on the Message-Passing (MP) paradigm, and classical spectral preservation guarantees for graph coarsening do not directly lead to theoretical guarantees when performing naive message-passing on the coarsened graph.","In this work, we propose a new message-passing operation specific to coarsened graphs, which exhibit theoretical guarantees on the preservation of the propagated signal.","Interestingly, and in a sharp departure from previous proposals, this operation on coarsened graphs is oriented, even when the original graph is undirected.","We conduct node classification tasks on synthetic and real data and observe improved results compared to performing naive message-passing on the coarsened graph."],"url":"http://arxiv.org/abs/2405.18127v1","category":"cs.LG"}
{"created":"2024-05-28 12:31:23","title":"Dual-Path Multi-Scale Transformer for High-Quality Image Deraining","abstract":"Despite the superiority of convolutional neural networks (CNNs) and Transformers in single-image rain removal, current multi-scale models still face significant challenges due to their reliance on single-scale feature pyramid patterns. In this paper, we propose an effective rain removal method, the dual-path multi-scale Transformer (DPMformer) for high-quality image reconstruction by leveraging rich multi-scale information. This method consists of a backbone path and two branch paths from two different multi-scale approaches. Specifically, one path adopts the coarse-to-fine strategy, progressively downsampling the image to 1/2 and 1/4 scales, which helps capture fine-scale potential rain information fusion. Simultaneously, we employ the multi-patch stacked model (non-overlapping blocks of size 2 and 4) to enrich the feature information of the deep network in the other path. To learn a richer blend of features, the backbone path fully utilizes the multi-scale information to achieve high-quality rain removal image reconstruction. Extensive experiments on benchmark datasets demonstrate that our method achieves promising performance compared to other state-of-the-art methods.","sentences":["Despite the superiority of convolutional neural networks (CNNs) and Transformers in single-image rain removal, current multi-scale models still face significant challenges due to their reliance on single-scale feature pyramid patterns.","In this paper, we propose an effective rain removal method, the dual-path multi-scale Transformer (DPMformer) for high-quality image reconstruction by leveraging rich multi-scale information.","This method consists of a backbone path and two branch paths from two different multi-scale approaches.","Specifically, one path adopts the coarse-to-fine strategy, progressively downsampling the image to 1/2 and 1/4 scales, which helps capture fine-scale potential rain information fusion.","Simultaneously, we employ the multi-patch stacked model (non-overlapping blocks of size 2 and 4) to enrich the feature information of the deep network in the other path.","To learn a richer blend of features, the backbone path fully utilizes the multi-scale information to achieve high-quality rain removal image reconstruction.","Extensive experiments on benchmark datasets demonstrate that our method achieves promising performance compared to other state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.18124v1","category":"cs.CV"}
{"created":"2024-05-28 12:30:12","title":"Projective structures and Hodge theory","abstract":"Every compact Riemann surface $X$ admits a natural projective structure $p_u$ as a consequence of the uniformization theorem. In this work we describe the construction of another natural projective structure on $X$, namely the Hodge projective structure $p_h$, related to the second fundamental form of the period map. We then describe how projective structures correspond to $(1,1)$-differential forms on the moduli space of projective curves and, from this correspondence, we deduce that $p_u$ and $p_h$ are not the same structure.","sentences":["Every compact Riemann surface $X$ admits a natural projective structure $p_u$ as a consequence of the uniformization theorem.","In this work we describe the construction of another natural projective structure on $X$, namely the Hodge projective structure $p_h$, related to the second fundamental form of the period map.","We then describe how projective structures correspond to $(1,1)$-differential forms on the moduli space of projective curves and, from this correspondence, we deduce that $p_u$ and $p_h$ are not the same structure."],"url":"http://arxiv.org/abs/2405.18122v1","category":"math.AG"}
{"created":"2024-05-28 12:16:56","title":"Differential polarizability at 1064 nm of the strontium intercombination transition","abstract":"We measure the scalar, vector and tensor components of the differential dynamic polarizability of the strontium intercombination transition at 1064 nm. We compare the experimental values with the theoretical prediction based on the most recently published spectroscopic data, and find a very good agreement. We also identify a close-to-circular `magic' polarization where the differential polarizability strictly vanishes, and precisely determine its ellipticity. Our work opens new perspectives for laser cooling optically trapped strontium atoms, and provides a new benchmark for atomic models in the near infrared spectral range.","sentences":["We measure the scalar, vector and tensor components of the differential dynamic polarizability of the strontium intercombination transition at 1064 nm.","We compare the experimental values with the theoretical prediction based on the most recently published spectroscopic data, and find a very good agreement.","We also identify a close-to-circular `magic' polarization where the differential polarizability strictly vanishes, and precisely determine its ellipticity.","Our work opens new perspectives for laser cooling optically trapped strontium atoms, and provides a new benchmark for atomic models in the near infrared spectral range."],"url":"http://arxiv.org/abs/2405.18109v1","category":"physics.atom-ph"}
{"created":"2024-05-28 12:05:20","title":"A Pontryagin Perspective on Reinforcement Learning","abstract":"Reinforcement learning has traditionally focused on learning state-dependent policies to solve optimal control problems in a closed-loop fashion. In this work, we introduce the paradigm of open-loop reinforcement learning where a fixed action sequence is learned instead. We present three new algorithms: one robust model-based method and two sample-efficient model-free methods. Rather than basing our algorithms on Bellman's equation from dynamic programming, our work builds on Pontryagin's principle from the theory of open-loop optimal control. We provide convergence guarantees and evaluate all methods empirically on a pendulum swing-up task, as well as on two high-dimensional MuJoCo tasks, demonstrating remarkable performance compared to existing baselines.","sentences":["Reinforcement learning has traditionally focused on learning state-dependent policies to solve optimal control problems in a closed-loop fashion.","In this work, we introduce the paradigm of open-loop reinforcement learning where a fixed action sequence is learned instead.","We present three new algorithms: one robust model-based method and two sample-efficient model-free methods.","Rather than basing our algorithms on Bellman's equation from dynamic programming, our work builds on Pontryagin's principle from the theory of open-loop optimal control.","We provide convergence guarantees and evaluate all methods empirically on a pendulum swing-up task, as well as on two high-dimensional MuJoCo tasks, demonstrating remarkable performance compared to existing baselines."],"url":"http://arxiv.org/abs/2405.18100v1","category":"cs.LG"}
{"created":"2024-05-28 12:00:16","title":"A space-time variational formulation for the many-body electronic Schr{\u00f6}dinger evolution equation","abstract":"We prove in this paper that the solution of the time-dependent Schr{\\\"o}dinger equation can be expressed as the solution of a global space-time quadratic minimization problem that is amenable to Galerkin time-space discretization schemes, using an appropriate least-square formulation. The present analysis can be applied to the electronic many-body time-dependent Schr{\\\"o}dinger equation with an arbitrary number of electrons and interaction potentials with Coulomb singularities. We motivate the interest of the present approach with two goals: first, the design of Galerkin space-time discretization methods; second, the definition of dynamical low-rank approximations following a variational principle different from the classical Dirac-Frenkel principle, and for which it is possible to prove the global-in-time existence of solutions.","sentences":["We prove in this paper that the solution of the time-dependent Schr{\\\"o}dinger equation can be expressed as the solution of a global space-time quadratic minimization problem that is amenable to Galerkin time-space discretization schemes, using an appropriate least-square formulation.","The present analysis can be applied to the electronic many-body time-dependent Schr{\\\"o}dinger equation with an arbitrary number of electrons and interaction potentials with Coulomb singularities.","We motivate the interest of the present approach with two goals: first, the design of Galerkin space-time discretization methods; second, the definition of dynamical low-rank approximations following a variational principle different from the classical Dirac-Frenkel principle, and for which it is possible to prove the global-in-time existence of solutions."],"url":"http://arxiv.org/abs/2405.18094v1","category":"math.NA"}
{"created":"2024-05-28 11:42:56","title":"Full Field Inversion of the Attenuated Wave Equation: Theory and Numerical Inversion","abstract":"Standard photoacoustic tomography (PAT) provides data that consist of time-dependent signals governed by the wave equation, which are measured on an observation surface. In contrast, the measured data from the recently invented full-field PAT is the Radon transform of the solution of the wave equation on a spatial domain at a single instant in time. While reconstruction using classical PAT data has been extensively studied, not much is known about the full-field PAT problem. In this paper, we study full-field photoacoustic tomography with spatially variable sound speed and spatially variable damping. In particular, we prove the uniqueness and stability of the associated single-time full-field wave inversion problem and develop algorithms for its numerical inversion using iterative and variational regularization methods. Numerical simulations are presented for both full-angle and limited-angle data cases","sentences":["Standard photoacoustic tomography (PAT) provides data that consist of time-dependent signals governed by the wave equation, which are measured on an observation surface.","In contrast, the measured data from the recently invented full-field PAT is the Radon transform of the solution of the wave equation on a spatial domain at a single instant in time.","While reconstruction using classical PAT data has been extensively studied, not much is known about the full-field PAT problem.","In this paper, we study full-field photoacoustic tomography with spatially variable sound speed and spatially variable damping.","In particular, we prove the uniqueness and stability of the associated single-time full-field wave inversion problem and develop algorithms for its numerical inversion using iterative and variational regularization methods.","Numerical simulations are presented for both full-angle and limited-angle data cases"],"url":"http://arxiv.org/abs/2405.18082v1","category":"math.NA"}
{"created":"2024-05-28 10:38:27","title":"Convergence rates of particle approximation of forward-backward splitting algorithm for granular medium equations","abstract":"We study the spatially homogeneous granular medium equation \\[\\partial_t\\mu=\\rm{div}(\\mu\\nabla V)+\\rm{div}(\\mu(\\nabla W \\ast \\mu))+\\Delta\\mu\\,,\\] within a large and natural class of the confinement potentials $V$ and interaction potentials $W$. The considered problem do not need to assume that $\\nabla V$ or $\\nabla W$ are globally Lipschitz. With the aim of providing particle approximation of solutions, we design efficient forward-backward splitting algorithms. Sharp convergence rates in terms of the Wasserstein distance are provided.","sentences":["We study the spatially homogeneous granular medium equation \\[\\partial_t\\mu=\\rm{div}(\\mu\\nabla V)+\\rm{div}(\\mu(\\nabla W \\ast \\mu))+\\Delta\\mu\\,,\\] within a large and natural class of the confinement potentials $V$ and interaction potentials $W$.","The considered problem do not need to assume that $\\nabla V$ or $\\nabla W$ are globally Lipschitz.","With the aim of providing particle approximation of solutions, we design efficient forward-backward splitting algorithms.","Sharp convergence rates in terms of the Wasserstein distance are provided."],"url":"http://arxiv.org/abs/2405.18034v1","category":"math.NA"}
{"created":"2024-05-28 08:41:44","title":"Global $L^p$ estimate for some kind of Kolmogorov-Fokker-Planck Equations in nondivergence form","abstract":"In this paper, we mainly investigate a class of Kolmogorov-Fokker-Planck operator with 4 different scalings in nondivergence form. And we assume the coefficients $a^{ij}$ are only measurable in $t$ and satisfy the vanishing mean oscillation in space variables. We establish a global priori estimates of $\\nabla_x^u$, $\\dy u$ and $\\dz u$ in $L^p$ space which extend the work of Dong and Yastrzhembskiy \\cite{ref49} where they focus on the 3 different scalings KFP operator. Moreover we establish a kind of Poincare inequality for homogeneous equations.","sentences":["In this paper, we mainly investigate a class of Kolmogorov-Fokker-Planck operator with 4 different scalings in nondivergence form.","And we assume the coefficients $a^{ij}$ are only measurable in $t$ and satisfy the vanishing mean oscillation in space variables.","We establish a global priori estimates of $\\nabla_x^u$, $\\dy u$ and $\\dz u$ in $L^p$ space which extend the work of Dong and Yastrzhembskiy \\cite{ref49} where they focus on the 3 different scalings KFP operator.","Moreover we establish a kind of Poincare inequality for homogeneous equations."],"url":"http://arxiv.org/abs/2405.17961v1","category":"math.AP"}
{"created":"2024-05-28 07:50:00","title":"Towards Unified Robustness Against Both Backdoor and Adversarial Attacks","abstract":"Deep Neural Networks (DNNs) are known to be vulnerable to both backdoor and adversarial attacks. In the literature, these two types of attacks are commonly treated as distinct robustness problems and solved separately, since they belong to training-time and inference-time attacks respectively. However, this paper revealed that there is an intriguing connection between them: (1) planting a backdoor into a model will significantly affect the model's adversarial examples; (2) for an infected model, its adversarial examples have similar features as the triggered images. Based on these observations, a novel Progressive Unified Defense (PUD) algorithm is proposed to defend against backdoor and adversarial attacks simultaneously. Specifically, our PUD has a progressive model purification scheme to jointly erase backdoors and enhance the model's adversarial robustness. At the early stage, the adversarial examples of infected models are utilized to erase backdoors. With the backdoor gradually erased, our model purification can naturally turn into a stage to boost the model's robustness against adversarial attacks. Besides, our PUD algorithm can effectively identify poisoned images, which allows the initial extra dataset not to be completely clean. Extensive experimental results show that, our discovered connection between backdoor and adversarial attacks is ubiquitous, no matter what type of backdoor attack. The proposed PUD outperforms the state-of-the-art backdoor defense, including the model repairing-based and data filtering-based methods. Besides, it also has the ability to compete with the most advanced adversarial defense methods.","sentences":["Deep Neural Networks (DNNs) are known to be vulnerable to both backdoor and adversarial attacks.","In the literature, these two types of attacks are commonly treated as distinct robustness problems and solved separately, since they belong to training-time and inference-time attacks respectively.","However, this paper revealed that there is an intriguing connection between them: (1) planting a backdoor into a model will significantly affect the model's adversarial examples; (2) for an infected model, its adversarial examples have similar features as the triggered images.","Based on these observations, a novel Progressive Unified Defense (PUD) algorithm is proposed to defend against backdoor and adversarial attacks simultaneously.","Specifically, our PUD has a progressive model purification scheme to jointly erase backdoors and enhance the model's adversarial robustness.","At the early stage, the adversarial examples of infected models are utilized to erase backdoors.","With the backdoor gradually erased, our model purification can naturally turn into a stage to boost the model's robustness against adversarial attacks.","Besides, our PUD algorithm can effectively identify poisoned images, which allows the initial extra dataset not to be completely clean.","Extensive experimental results show that, our discovered connection between backdoor and adversarial attacks is ubiquitous, no matter what type of backdoor attack.","The proposed PUD outperforms the state-of-the-art backdoor defense, including the model repairing-based and data filtering-based methods.","Besides, it also has the ability to compete with the most advanced adversarial defense methods."],"url":"http://arxiv.org/abs/2405.17929v1","category":"cs.CV"}
{"created":"2024-05-28 07:29:02","title":"Deep learning inference of the neutron star equation of state","abstract":"We present a pipeline to infer the equation of state of neutron stars from observations based on deep neural networks. In particular, using the standard (deterministic), as well as Bayesian (probabilistic) deep networks, we explore how one can infer the interior speed of sound of the star given a set of mock observations of total stellar mass, stellar radius and tidal deformability. We discuss in detail the construction of our simulated dataset of stellar observables starting from the solution of the gravitational equations, as well as the relevant architectures for the deep networks, along with their performance and accuracy. We further explain how our pipeline is capable to detect a possible QCD phase transition in the stellar core. Our results show that deep networks offer a promising tool towards solving the inverse problem of neutron stars, and the accurate inference of their interior from future stellar observations.","sentences":["We present a pipeline to infer the equation of state of neutron stars from observations based on deep neural networks.","In particular, using the standard (deterministic), as well as Bayesian (probabilistic) deep networks, we explore how one can infer the interior speed of sound of the star given a set of mock observations of total stellar mass, stellar radius and tidal deformability.","We discuss in detail the construction of our simulated dataset of stellar observables starting from the solution of the gravitational equations, as well as the relevant architectures for the deep networks, along with their performance and accuracy.","We further explain how our pipeline is capable to detect a possible QCD phase transition in the stellar core.","Our results show that deep networks offer a promising tool towards solving the inverse problem of neutron stars, and the accurate inference of their interior from future stellar observations."],"url":"http://arxiv.org/abs/2405.17908v1","category":"astro-ph.HE"}
{"created":"2024-05-28 07:18:45","title":"$C^2M^3$: Cycle-Consistent Multi-Model Merging","abstract":"In this paper, we present a novel data-free method for merging neural networks in weight space. Differently from most existing works, our method optimizes for the permutations of network neurons globally across all layers. This allows us to enforce cycle consistency of the permutations when merging $N \\geq 3$ models, allowing circular compositions of permutations to be computed without accumulating error along the path. We qualitatively and quantitatively motivate the need for such a constraint, showing its benefits when merging sets of models in scenarios spanning varying architectures and datasets. We finally show that, when coupled with activation renormalization, our approach yields the best results in the task.","sentences":["In this paper, we present a novel data-free method for merging neural networks in weight space.","Differently from most existing works, our method optimizes for the permutations of network neurons globally across all layers.","This allows us to enforce cycle consistency of the permutations when merging $N \\geq 3$ models, allowing circular compositions of permutations to be computed without accumulating error along the path.","We qualitatively and quantitatively motivate the need for such a constraint, showing its benefits when merging sets of models in scenarios spanning varying architectures and datasets.","We finally show that, when coupled with activation renormalization, our approach yields the best results in the task."],"url":"http://arxiv.org/abs/2405.17897v1","category":"cs.LG"}
{"created":"2024-05-28 06:14:38","title":"A Deep Neural Network Approach to Fare Evasion","abstract":"Fare evasion is a problem for public transport companies, with LSTM models this issue can help companies get an analytical insight into where this issue occurs the most, to prevent capital loss. In addition to the financial burden this problem causes, having more inspectors is not enough to alleviate the problem. The purpose of this study is to find a different way to predict fare evasion in the public transport sector. Through the use of keypoint extractions of passengers in video footage, an LSTM model is trained on those keypoints to help predict the actions of passengers between payments and evasions. The results were promising when it came to predicting the actions of passengers on real-time footage. Thus a sophisticated approach can help to decrease the fare evasion problem. A ReID model can be used alongside the LSTM model for better accuracy, as there is always the chance that a person might only pay for the fare at a later stage. With both models, it is possible for public transport companies to start narrowing down where the root of their fare evasion problems emerges.","sentences":["Fare evasion is a problem for public transport companies, with LSTM models this issue can help companies get an analytical insight into where this issue occurs the most, to prevent capital loss.","In addition to the financial burden this problem causes, having more inspectors is not enough to alleviate the problem.","The purpose of this study is to find a different way to predict fare evasion in the public transport sector.","Through the use of keypoint extractions of passengers in video footage, an LSTM model is trained on those keypoints to help predict the actions of passengers between payments and evasions.","The results were promising when it came to predicting the actions of passengers on real-time footage.","Thus a sophisticated approach can help to decrease the fare evasion problem.","A ReID model can be used alongside the LSTM model for better accuracy, as there is always the chance that a person might only pay for the fare at a later stage.","With both models, it is possible for public transport companies to start narrowing down where the root of their fare evasion problems emerges."],"url":"http://arxiv.org/abs/2405.17855v1","category":"cs.CV"}
{"created":"2024-05-28 05:20:01","title":"An Innovative Networks in Federated Learning","abstract":"This paper presents the development and application of Wavelet Kolmogorov-Arnold Networks (Wav-KAN) in federated learning. We implemented Wav-KAN \\cite{wav-kan} in the clients. Indeed, we have considered both continuous wavelet transform (CWT) and also discrete wavelet transform (DWT) to enable multiresolution capabaility which helps in heteregeneous data distribution across clients. Extensive experiments were conducted on different datasets, demonstrating Wav-KAN's superior performance in terms of interpretability, computational speed, training and test accuracy. Our federated learning algorithm integrates wavelet-based activation functions, parameterized by weight, scale, and translation, to enhance local and global model performance. Results show significant improvements in computational efficiency, robustness, and accuracy, highlighting the effectiveness of wavelet selection in scalable neural network design.","sentences":["This paper presents the development and application of Wavelet Kolmogorov-Arnold Networks (Wav-KAN) in federated learning.","We implemented Wav-KAN \\cite{wav-kan} in the clients.","Indeed, we have considered both continuous wavelet transform (CWT) and also discrete wavelet transform (DWT) to enable multiresolution capabaility which helps in heteregeneous data distribution across clients.","Extensive experiments were conducted on different datasets, demonstrating Wav-KAN's superior performance in terms of interpretability, computational speed, training and test accuracy.","Our federated learning algorithm integrates wavelet-based activation functions, parameterized by weight, scale, and translation, to enhance local and global model performance.","Results show significant improvements in computational efficiency, robustness, and accuracy, highlighting the effectiveness of wavelet selection in scalable neural network design."],"url":"http://arxiv.org/abs/2405.17836v1","category":"eess.SP"}
{"created":"2024-05-28 04:29:23","title":"Hyperspectral and multispectral image fusion with arbitrary resolution through self-supervised representations","abstract":"The fusion of a low-resolution hyperspectral image (LR-HSI) with a high-resolution multispectral image (HR-MSI) has emerged as an effective technique for achieving HSI super-resolution (SR). Previous studies have mainly concentrated on estimating the posterior distribution of the latent high-resolution hyperspectral image (HR-HSI), leveraging an appropriate image prior and likelihood computed from the discrepancy between the latent HSI and observed images. Low rankness stands out for preserving latent HSI characteristics through matrix factorization among the various priors. However, this method only enhances resolution within the dimensions of the two modalities. To overcome this limitation, we propose a novel continuous low-rank factorization (CLoRF) by integrating two neural representations into the matrix factorization, capturing spatial and spectral information, respectively. This approach enables us to harness both the low rankness from the matrix factorization and the continuity from neural representation in a self-supervised manner. Theoretically, we prove the low-rank property and Lipschitz continuity in the proposed continuous low-rank factorization. Experimentally, our method significantly surpasses existing techniques and achieves user-desired resolutions without the need for neural network retraining.","sentences":["The fusion of a low-resolution hyperspectral image (LR-HSI) with a high-resolution multispectral image (HR-MSI) has emerged as an effective technique for achieving HSI super-resolution (SR).","Previous studies have mainly concentrated on estimating the posterior distribution of the latent high-resolution hyperspectral image (HR-HSI), leveraging an appropriate image prior and likelihood computed from the discrepancy between the latent HSI and observed images.","Low rankness stands out for preserving latent HSI characteristics through matrix factorization among the various priors.","However, this method only enhances resolution within the dimensions of the two modalities.","To overcome this limitation, we propose a novel continuous low-rank factorization (CLoRF) by integrating two neural representations into the matrix factorization, capturing spatial and spectral information, respectively.","This approach enables us to harness both the low rankness from the matrix factorization and the continuity from neural representation in a self-supervised manner.","Theoretically, we prove the low-rank property and Lipschitz continuity in the proposed continuous low-rank factorization.","Experimentally, our method significantly surpasses existing techniques and achieves user-desired resolutions without the need for neural network retraining."],"url":"http://arxiv.org/abs/2405.17818v1","category":"cs.CV"}
{"created":"2024-05-28 03:38:30","title":"Classical and quantum thermodynamics in a non-equilibrium regime: Application to Stirling engine","abstract":"We develop a thermodynamic theory in the non-equilibrium regime by extending a theory based on the dimensionless (DL) minimum work principle previously developed for a thermodynamic system-bath model [S.Koyanagi and Y.Tanimura,J.Chem.Phys.160,(2024)]. Our results are described by non-equilibrium thermodynamic potentials expressed in time-derivative form in terms of extensive and intensive variables. This is made possible through the incorporation of waste heat, which is equivalent to the loss work consumed by the bath, into the definitions of thermodynamic potentials in a non-equilibrium regime. These potentials can be evaluated from the DL non-equilibrium-to-equilibrium minimum work principle, which is derived from the principle of DL minimum work and is equivalent to the second law of thermodynamics. We thus obtain the non-equilibrium Massieu-Planck potentials as entropic potentials and the non-equilibrium Helmholtz-Gibbs potentials as free energies. Our results are verified numerically by simulating a Stirling engine consisting of two isothermal and two thermostatic processes using the quantum Fokker-Planck equations and the classical Kramers equation derived from the thermodynamic system-bath model. We then show that any thermodynamic process can be analyzed using a non-equilibrium work diagram analogous to the equilibrium one for given time-dependent intensive variables. The results can be used to develop efficient heat machines in non-equilibrium regimes.","sentences":["We develop a thermodynamic theory in the non-equilibrium regime by extending a theory based on the dimensionless (DL) minimum work principle previously developed for a thermodynamic system-bath model [S.Koyanagi and Y.Tanimura,J.Chem.Phys.160,(2024)].","Our results are described by non-equilibrium thermodynamic potentials expressed in time-derivative form in terms of extensive and intensive variables.","This is made possible through the incorporation of waste heat, which is equivalent to the loss work consumed by the bath, into the definitions of thermodynamic potentials in a non-equilibrium regime.","These potentials can be evaluated from the DL non-equilibrium-to-equilibrium minimum work principle, which is derived from the principle of DL minimum work and is equivalent to the second law of thermodynamics.","We thus obtain the non-equilibrium Massieu-Planck potentials as entropic potentials and the non-equilibrium Helmholtz-Gibbs potentials as free energies.","Our results are verified numerically by simulating a Stirling engine consisting of two isothermal and two thermostatic processes using the quantum Fokker-Planck equations and the classical Kramers equation derived from the thermodynamic system-bath model.","We then show that any thermodynamic process can be analyzed using a non-equilibrium work diagram analogous to the equilibrium one for given time-dependent intensive variables.","The results can be used to develop efficient heat machines in non-equilibrium regimes."],"url":"http://arxiv.org/abs/2405.17791v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-28 03:12:33","title":"The Binary Quantized Neural Network for Dense Prediction via Specially Designed Upsampling and Attention","abstract":"Deep learning-based information processing consumes long time and requires huge computing resources, especially for dense prediction tasks which require an output for each pixel, like semantic segmentation and salient object detection. There are mainly two challenges for quantization of dense prediction tasks. Firstly, directly applying the upsampling operation that dense prediction tasks require is extremely crude and causes unacceptable accuracy reduction. Secondly, the complex structure of dense prediction networks means it is difficult to maintain a fast speed as well as a high accuracy when performing quantization. In this paper, we propose an effective upsampling method and an efficient attention computation strategy to transfer the success of the binary neural networks (BNN) from single prediction tasks to dense prediction tasks. Firstly, we design a simple and robust multi-branch parallel upsampling structure to achieve the high accuracy. Then we further optimize the attention method which plays an important role in segmentation but has huge computation complexity. Our attention method can reduce the computational complexity by a factor of one hundred times but retain the original effect. Experiments on Cityscapes, KITTI road, and ECSSD fully show the effectiveness of our work.","sentences":["Deep learning-based information processing consumes long time and requires huge computing resources, especially for dense prediction tasks which require an output for each pixel, like semantic segmentation and salient object detection.","There are mainly two challenges for quantization of dense prediction tasks.","Firstly, directly applying the upsampling operation that dense prediction tasks require is extremely crude and causes unacceptable accuracy reduction.","Secondly, the complex structure of dense prediction networks means it is difficult to maintain a fast speed as well as a high accuracy when performing quantization.","In this paper, we propose an effective upsampling method and an efficient attention computation strategy to transfer the success of the binary neural networks (BNN) from single prediction tasks to dense prediction tasks.","Firstly, we design a simple and robust multi-branch parallel upsampling structure to achieve the high accuracy.","Then we further optimize the attention method which plays an important role in segmentation but has huge computation complexity.","Our attention method can reduce the computational complexity by a factor of one hundred times but retain the original effect.","Experiments on Cityscapes, KITTI road, and ECSSD fully show the effectiveness of our work."],"url":"http://arxiv.org/abs/2405.17776v1","category":"cs.LG"}
{"created":"2024-05-28 02:56:45","title":"Non-uniform dependence on initial data for the generalized Camassa-Holm equation in $C^1$","abstract":"It is shown in \\cite[Adv. Differ. Equ(2017)]{HT} that the Cauchy problem for the generalized Camassa-Holm equation is well-posed in $C^1$ and the data-to-solution map is H\\\"{o}lder continuous from $C^\\alpha$ to $\\mathcal{C}([0,T];C^\\alpha)$ with $\\alpha\\in[0,1)$. In this paper, we further show that the data-to-solution map of the generalized Camassa-Holm equation is not uniformly continuous on the initial data in $C^1$. In particular, our result also can be a complement of previous work on the classical Camassa-Holm equation in \\cite[Geom. Funct. Anal(2002)]{G02}.","sentences":["It is shown in \\cite[Adv. Differ.","Equ(2017)]{HT} that the Cauchy problem for the generalized Camassa-Holm equation is well-posed in $C^1$ and the data-to-solution map is H\\\"{o}lder continuous from $C^\\alpha$ to $\\mathcal{C}([0,T];C^\\alpha)$ with $\\alpha\\in[0,1)$. In this paper, we further show that the data-to-solution map of the generalized Camassa-Holm equation is not uniformly continuous on the initial data in $C^1$. In particular, our result also can be a complement of previous work on the classical Camassa-Holm equation in \\cite[Geom.","Funct. Anal(2002)]{G02}."],"url":"http://arxiv.org/abs/2405.17771v1","category":"math.AP"}
{"created":"2024-05-28 02:47:53","title":"Revisiting the Message Passing in Heterophilous Graph Neural Networks","abstract":"Graph Neural Networks (GNNs) have demonstrated strong performance in graph mining tasks due to their message-passing mechanism, which is aligned with the homophily assumption that adjacent nodes exhibit similar behaviors. However, in many real-world graphs, connected nodes may display contrasting behaviors, termed as heterophilous patterns, which has attracted increased interest in heterophilous GNNs (HTGNNs). Although the message-passing mechanism seems unsuitable for heterophilous graphs due to the propagation of class-irrelevant information, it is still widely used in many existing HTGNNs and consistently achieves notable success. This raises the question: why does message passing remain effective on heterophilous graphs? To answer this question, in this paper, we revisit the message-passing mechanisms in heterophilous graph neural networks and reformulate them into a unified heterophilious message-passing (HTMP) mechanism. Based on HTMP and empirical analysis, we reveal that the success of message passing in existing HTGNNs is attributed to implicitly enhancing the compatibility matrix among classes. Moreover, we argue that the full potential of the compatibility matrix is not completely achieved due to the existence of incomplete and noisy semantic neighborhoods in real-world heterophilous graphs. To bridge this gap, we introduce a new approach named CMGNN, which operates within the HTMP mechanism to explicitly leverage and improve the compatibility matrix. A thorough evaluation involving 10 benchmark datasets and comparative analysis against 13 well-established baselines highlights the superior performance of the HTMP mechanism and CMGNN method.","sentences":["Graph Neural Networks (GNNs) have demonstrated strong performance in graph mining tasks due to their message-passing mechanism, which is aligned with the homophily assumption that adjacent nodes exhibit similar behaviors.","However, in many real-world graphs, connected nodes may display contrasting behaviors, termed as heterophilous patterns, which has attracted increased interest in heterophilous GNNs (HTGNNs).","Although the message-passing mechanism seems unsuitable for heterophilous graphs due to the propagation of class-irrelevant information, it is still widely used in many existing HTGNNs and consistently achieves notable success.","This raises the question: why does message passing remain effective on heterophilous graphs?","To answer this question, in this paper, we revisit the message-passing mechanisms in heterophilous graph neural networks and reformulate them into a unified heterophilious message-passing (HTMP) mechanism.","Based on HTMP and empirical analysis, we reveal that the success of message passing in existing HTGNNs is attributed to implicitly enhancing the compatibility matrix among classes.","Moreover, we argue that the full potential of the compatibility matrix is not completely achieved due to the existence of incomplete and noisy semantic neighborhoods in real-world heterophilous graphs.","To bridge this gap, we introduce a new approach named CMGNN, which operates within the HTMP mechanism to explicitly leverage and improve the compatibility matrix.","A thorough evaluation involving 10 benchmark datasets and comparative analysis against 13 well-established baselines highlights the superior performance of the HTMP mechanism and CMGNN method."],"url":"http://arxiv.org/abs/2405.17768v1","category":"cs.LG"}
{"created":"2024-05-28 02:18:56","title":"NASPrecision: Neural Architecture Search-Driven Multi-Stage Learning for Surface Roughness Prediction in Ultra-Precision Machining","abstract":"Accurate surface roughness prediction is critical for ensuring high product quality, especially in areas like manufacturing and aerospace, where the smallest imperfections can compromise performance or safety. However, this is challenging due to complex, non-linear interactions among variables, which is further exacerbated with limited and imbalanced datasets. Existing methods using traditional machine learning algorithms require extensive domain knowledge for feature engineering and substantial human intervention for model selection. To address these issues, we propose NASPrecision, a Neural Architecture Search (NAS)-Driven Multi-Stage Learning Framework. This innovative approach autonomously identifies the most suitable features and models for various surface roughness prediction tasks and significantly enhances the performance by multi-stage learning. Our framework operates in three stages: 1) architecture search stage, employing NAS to automatically identify the most effective model architecture; 2) initial training stage, where we train the neural network for initial predictions; 3) refinement stage, where a subsequent model is appended to refine and capture subtle variations overlooked by the initial training stage. In light of limited and imbalanced datasets, we adopt a generative data augmentation technique to balance and generate new data by learning the underlying data distribution. We conducted experiments on three distinct real-world datasets linked to different machining techniques. Results show improvements in Mean Absolute Percentage Error (MAPE), Root Mean Square Error (RMSE), and Standard Deviation (STD) by 18%, 31%, and 22%, respectively. This establishes it as a robust and general solution for precise surface roughness prediction, potentially boosting production efficiency and product quality in key industries while minimizing domain expertise and human intervention.","sentences":["Accurate surface roughness prediction is critical for ensuring high product quality, especially in areas like manufacturing and aerospace, where the smallest imperfections can compromise performance or safety.","However, this is challenging due to complex, non-linear interactions among variables, which is further exacerbated with limited and imbalanced datasets.","Existing methods using traditional machine learning algorithms require extensive domain knowledge for feature engineering and substantial human intervention for model selection.","To address these issues, we propose NASPrecision, a Neural Architecture Search (NAS)-Driven Multi-Stage Learning Framework.","This innovative approach autonomously identifies the most suitable features and models for various surface roughness prediction tasks and significantly enhances the performance by multi-stage learning.","Our framework operates in three stages: 1) architecture search stage, employing NAS to automatically identify the most effective model architecture; 2) initial training stage, where we train the neural network for initial predictions; 3) refinement stage, where a subsequent model is appended to refine and capture subtle variations overlooked by the initial training stage.","In light of limited and imbalanced datasets, we adopt a generative data augmentation technique to balance and generate new data by learning the underlying data distribution.","We conducted experiments on three distinct real-world datasets linked to different machining techniques.","Results show improvements in Mean Absolute Percentage Error (MAPE), Root Mean Square Error (RMSE), and Standard Deviation (STD) by 18%, 31%, and 22%, respectively.","This establishes it as a robust and general solution for precise surface roughness prediction, potentially boosting production efficiency and product quality in key industries while minimizing domain expertise and human intervention."],"url":"http://arxiv.org/abs/2405.17757v1","category":"cs.CE"}
{"created":"2024-05-28 01:56:25","title":"Shaping the distribution of neural responses with interneurons in a recurrent circuit model","abstract":"Efficient coding theory posits that sensory circuits transform natural signals into neural representations that maximize information transmission subject to resource constraints. Local interneurons are thought to play an important role in these transformations, shaping patterns of circuit activity to facilitate and direct information flow. However, the relationship between these coordinated, nonlinear, circuit-level transformations and the properties of interneurons (e.g., connectivity, activation functions, response dynamics) remains unknown. Here, we propose a normative computational model that establishes such a relationship. Our model is derived from an optimal transport objective that conceptualizes the circuit's input-response function as transforming the inputs to achieve a target response distribution. The circuit, which is comprised of primary neurons that are recurrently connected to a set of local interneurons, continuously optimizes this objective by dynamically adjusting both the synaptic connections between neurons as well as the interneuron activation functions. In an application motivated by redundancy reduction theory, we demonstrate that when the inputs are natural image statistics and the target distribution is a spherical Gaussian, the circuit learns a nonlinear transformation that significantly reduces statistical dependencies in neural responses. Overall, our results provide a framework in which the distribution of circuit responses is systematically and nonlinearly controlled by adjustment of interneuron connectivity and activation functions.","sentences":["Efficient coding theory posits that sensory circuits transform natural signals into neural representations that maximize information transmission subject to resource constraints.","Local interneurons are thought to play an important role in these transformations, shaping patterns of circuit activity to facilitate and direct information flow.","However, the relationship between these coordinated, nonlinear, circuit-level transformations and the properties of interneurons (e.g., connectivity, activation functions, response dynamics) remains unknown.","Here, we propose a normative computational model that establishes such a relationship.","Our model is derived from an optimal transport objective that conceptualizes the circuit's input-response function as transforming the inputs to achieve a target response distribution.","The circuit, which is comprised of primary neurons that are recurrently connected to a set of local interneurons, continuously optimizes this objective by dynamically adjusting both the synaptic connections between neurons as well as the interneuron activation functions.","In an application motivated by redundancy reduction theory, we demonstrate that when the inputs are natural image statistics and the target distribution is a spherical Gaussian, the circuit learns a nonlinear transformation that significantly reduces statistical dependencies in neural responses.","Overall, our results provide a framework in which the distribution of circuit responses is systematically and nonlinearly controlled by adjustment of interneuron connectivity and activation functions."],"url":"http://arxiv.org/abs/2405.17745v1","category":"q-bio.NC"}
{"created":"2024-05-28 00:10:01","title":"Inversion of the exponential X-ray transform of symmetric 2-tensors","abstract":"A unique inversion of the exponential X-ray transform of some class of symmetric 2-tensor field in a two dimensional strictly convex set is considered. The approach to inversion is based on the Cauchy problem for a Beltrami-like equation associated with $A$-analytic maps.","sentences":["A unique inversion of the exponential X-ray transform of some class of symmetric 2-tensor field in a two dimensional strictly convex set is considered.","The approach to inversion is based on the Cauchy problem for a Beltrami-like equation associated with $A$-analytic maps."],"url":"http://arxiv.org/abs/2405.17714v1","category":"math.AP"}
{"created":"2024-05-27 23:04:37","title":"P4: Towards private, personalized, and Peer-to-Peer learning","abstract":"Personalized learning is a proposed approach to address the problem of data heterogeneity in collaborative machine learning. In a decentralized setting, the two main challenges of personalization are client clustering and data privacy. In this paper, we address these challenges by developing P4 (Personalized Private Peer-to-Peer) a method that ensures that each client receives a personalized model while maintaining differential privacy guarantee of each client's local dataset during and after the training. Our approach includes the design of a lightweight algorithm to identify similar clients and group them in a private, peer-to-peer (P2P) manner. Once grouped, we develop differentially-private knowledge distillation for clients to co-train with minimal impact on accuracy. We evaluate our proposed method on three benchmark datasets (FEMNIST or Federated EMNIST, CIFAR-10 and CIFAR-100) and two different neural network architectures (Linear and CNN-based networks) across a range of privacy parameters. The results demonstrate the potential of P4, as it outperforms the state-of-the-art of differential private P2P by up to 40 percent in terms of accuracy. We also show the practicality of P4 by implementing it on resource constrained devices, and validating that it has minimal overhead, e.g., about 7 seconds to run collaborative training between two clients.","sentences":["Personalized learning is a proposed approach to address the problem of data heterogeneity in collaborative machine learning.","In a decentralized setting, the two main challenges of personalization are client clustering and data privacy.","In this paper, we address these challenges by developing P4 (Personalized Private Peer-to-Peer) a method that ensures that each client receives a personalized model while maintaining differential privacy guarantee of each client's local dataset during and after the training.","Our approach includes the design of a lightweight algorithm to identify similar clients and group them in a private, peer-to-peer (P2P) manner.","Once grouped, we develop differentially-private knowledge distillation for clients to co-train with minimal impact on accuracy.","We evaluate our proposed method on three benchmark datasets (FEMNIST or Federated EMNIST, CIFAR-10 and CIFAR-100) and two different neural network architectures (Linear and CNN-based networks) across a range of privacy parameters.","The results demonstrate the potential of P4, as it outperforms the state-of-the-art of differential private P2P by up to 40 percent in terms of accuracy.","We also show the practicality of P4 by implementing it on resource constrained devices, and validating that it has minimal overhead, e.g., about 7 seconds to run collaborative training between two clients."],"url":"http://arxiv.org/abs/2405.17697v1","category":"cs.LG"}
{"created":"2024-05-27 21:25:01","title":"Nonlinear steepest descent on a torus: A case study of the Landau-Lifshitz equation","abstract":"We obtain rigorous large time asymptotics for the Landau-Lifshitz equation in the soliton free case by extending the nonlinear steepest descent method to genus 1 surfaces. The methods presented in this paper pave the way to rigorous analysis of other integrable equations on the torus and enables asymptotic analysis on different regimes of the Landau-Lifshitz equation.","sentences":["We obtain rigorous large time asymptotics for the Landau-Lifshitz equation in the soliton free case by extending the nonlinear steepest descent method to genus 1 surfaces.","The methods presented in this paper pave the way to rigorous analysis of other integrable equations on the torus and enables asymptotic analysis on different regimes of the Landau-Lifshitz equation."],"url":"http://arxiv.org/abs/2405.17662v1","category":"math.AP"}
{"created":"2024-05-27 20:53:22","title":"InversionView: A General-Purpose Method for Reading Information from Neural Activations","abstract":"The inner workings of neural networks can be better understood if we can fully decipher the information encoded in neural activations. In this paper, we argue that this information is embodied by the subset of inputs that give rise to similar activations. Computing such subsets is nontrivial as the input space is exponentially large. We propose InversionView, which allows us to practically inspect this subset by sampling from a trained decoder model conditioned on activations. This helps uncover the information content of activation vectors, and facilitates understanding of the algorithms implemented by transformer models. We present three case studies where we investigate models ranging from small transformers to GPT-2. In these studies, we demonstrate the characteristics of our method, show the distinctive advantages it offers, and provide causally verified circuits.","sentences":["The inner workings of neural networks can be better understood if we can fully decipher the information encoded in neural activations.","In this paper, we argue that this information is embodied by the subset of inputs that give rise to similar activations.","Computing such subsets is nontrivial as the input space is exponentially large.","We propose InversionView, which allows us to practically inspect this subset by sampling from a trained decoder model conditioned on activations.","This helps uncover the information content of activation vectors, and facilitates understanding of the algorithms implemented by transformer models.","We present three case studies where we investigate models ranging from small transformers to GPT-2.","In these studies, we demonstrate the characteristics of our method, show the distinctive advantages it offers, and provide causally verified circuits."],"url":"http://arxiv.org/abs/2405.17653v1","category":"cs.LG"}
{"created":"2024-05-27 20:43:20","title":"The investigation of singular integro-differential equations relating to adhesive contact problems of the theory of viscoelasticity","abstract":"The exact and approximate solutions of singular integro-differential equations relating to the problems of interaction of an elastic thin finite or infinite non-homogeneous patch with a plate are considered, provided that the materials of plate and patch possess the creep property. Using the method of orthogonal polynomials the problem is reduced to the infinite system of Volterra integral equations, and using the method of integral transformations this problem is reduced to the different boundary value problems of the theory of analytic functions. An asymptotic analysis is also performed.","sentences":["The exact and approximate solutions of singular integro-differential equations relating to the problems of interaction of an elastic thin finite or infinite non-homogeneous patch with a plate are considered, provided that the materials of plate and patch possess the creep property.","Using the method of orthogonal polynomials the problem is reduced to the infinite system of Volterra integral equations, and using the method of integral transformations this problem is reduced to the different boundary value problems of the theory of analytic functions.","An asymptotic analysis is also performed."],"url":"http://arxiv.org/abs/2405.17647v1","category":"math-ph"}
{"created":"2024-05-27 19:27:56","title":"Abelianization of Lie algebroids and Lie groupoids","abstract":"We investigate the abelianization of a Lie algebroid and provide a necessary and sufficient condition for its existence. We also study the abelianization of groupoids and provide sufficient conditions for its existence in the smooth category and a necessary and sufficient condition for its existence in the diffeological category.","sentences":["We investigate the abelianization of a Lie algebroid and provide a necessary and sufficient condition for its existence.","We also study the abelianization of groupoids and provide sufficient conditions for its existence in the smooth category and a necessary and sufficient condition for its existence in the diffeological category."],"url":"http://arxiv.org/abs/2405.17617v1","category":"math.DG"}
{"created":"2024-05-27 19:01:05","title":"GDSW preconditioners for composite Discontinuous Galerkin discretizations of multicompartment reaction-diffusion problems","abstract":"The aim of the present work is to design, analyze theoretically, and test numerically, a generalized Dryja-Smith-Widlund (GDSW) preconditioner for composite Discontinuous Galerkin discretizations of multicompartment parabolic reaction-diffusion equations, where the solution can exhibit natural discontinuities across the domain. We prove that the resulting preconditioned operator for the solution of the discrete system arising at each time step converges with a scalable and quasi-optimal upper bound for the condition number. The GDSW preconditioner is then applied to the EMI (Extracellular - Membrane - Intracellular) reaction-diffusion system, recently proposed to model microscopically the spatiotemporal evolution of cardiac bioelectrical potentials. Numerical tests validate the scalability and quasi-optimality of the EMI-GDSW preconditioner, and investigate its robustness with respect to the time step size as well as jumps in the diffusion coefficients.","sentences":["The aim of the present work is to design, analyze theoretically, and test numerically, a generalized Dryja-Smith-Widlund (GDSW) preconditioner for composite Discontinuous Galerkin discretizations of multicompartment parabolic reaction-diffusion equations, where the solution can exhibit natural discontinuities across the domain.","We prove that the resulting preconditioned operator for the solution of the discrete system arising at each time step converges with a scalable and quasi-optimal upper bound for the condition number.","The GDSW preconditioner is then applied to the EMI (Extracellular - Membrane - Intracellular) reaction-diffusion system, recently proposed to model microscopically the spatiotemporal evolution of cardiac bioelectrical potentials.","Numerical tests validate the scalability and quasi-optimality of the EMI-GDSW preconditioner, and investigate its robustness with respect to the time step size as well as jumps in the diffusion coefficients."],"url":"http://arxiv.org/abs/2405.17601v1","category":"math.NA"}
{"created":"2024-05-27 18:57:56","title":"Seismic Image Denoising With A Physics-Constrained Deep Image Prior","abstract":"Seismic images often contain both coherent and random artifacts which complicate their interpretation. To mitigate these artifacts, we introduce a novel unsupervised deep-learning method based on Deep Image Prior (DIP) which uses convolutional neural networks. Our approach optimizes the network weights to refine the migration velocity model, rather than the seismic image, effectively isolating meaningful image features from noise and artifacts. We apply this method to synthetic and real seismic data, demonstrating significant improvements over standard DIP techniques with minimal computational overhead.","sentences":["Seismic images often contain both coherent and random artifacts which complicate their interpretation.","To mitigate these artifacts, we introduce a novel unsupervised deep-learning method based on Deep Image Prior (DIP) which uses convolutional neural networks.","Our approach optimizes the network weights to refine the migration velocity model, rather than the seismic image, effectively isolating meaningful image features from noise and artifacts.","We apply this method to synthetic and real seismic data, demonstrating significant improvements over standard DIP techniques with minimal computational overhead."],"url":"http://arxiv.org/abs/2405.17597v1","category":"physics.geo-ph"}
{"created":"2024-05-27 18:55:52","title":"Element-Free Probability Distributions and Random Partitions","abstract":"An \"element-free\" probability distribution is what remains of a probability distribution after we forget the elements to which the probabilities were assigned. These objects naturally arise in Bayesian statistics, in situations where elements are used as labels and their specific identity is not important.   This paper develops the structural theory of element-free distributions, using multisets and category theory. We give operations for moving between element-free and ordinary distributions, and we show that these operations commute with multinomial sampling. We then exploit this theory to prove two representation theorems. These theorems show that element-free distributions provide a natural representation for key random structures in Bayesian nonparametric clustering: exchangeable random partitions, and random distributions parametrized by a base measure.","sentences":["An \"element-free\" probability distribution is what remains of a probability distribution after we forget the elements to which the probabilities were assigned.","These objects naturally arise in Bayesian statistics, in situations where elements are used as labels and their specific identity is not important.   ","This paper develops the structural theory of element-free distributions, using multisets and category theory.","We give operations for moving between element-free and ordinary distributions, and we show that these operations commute with multinomial sampling.","We then exploit this theory to prove two representation theorems.","These theorems show that element-free distributions provide a natural representation for key random structures in Bayesian nonparametric clustering: exchangeable random partitions, and random distributions parametrized by a base measure."],"url":"http://arxiv.org/abs/2405.17595v1","category":"cs.LO"}
{"created":"2024-05-27 18:48:43","title":"Individualized Dynamic Mediation Analysis Using Latent Factor Models","abstract":"Mediation analysis plays a crucial role in causal inference as it can investigate the pathways through which treatment influences outcome. Most existing mediation analysis assumes that mediation effects are static and homogeneous within populations. However, mediation effects usually change over time and exhibit significant heterogeneity in many real-world applications. Additionally, the presence of unobserved confounding variables imposes a significant challenge to inferring both causal effect and mediation effect. To address these issues, we propose an individualized dynamic mediation analysis method. Our approach can identify the significant mediators of the population level while capturing the time-varying and heterogeneous mediation effects via latent factor modeling on coefficients of structural equation models. Another advantage of our method is that we can infer individualized mediation effects in the presence of unmeasured time-varying confounders. We provide estimation consistency for our proposed causal estimand and selection consistency for significant mediators. Extensive simulation studies and an application to a DNA methylation study demonstrate the effectiveness and advantages of our method.","sentences":["Mediation analysis plays a crucial role in causal inference as it can investigate the pathways through which treatment influences outcome.","Most existing mediation analysis assumes that mediation effects are static and homogeneous within populations.","However, mediation effects usually change over time and exhibit significant heterogeneity in many real-world applications.","Additionally, the presence of unobserved confounding variables imposes a significant challenge to inferring both causal effect and mediation effect.","To address these issues, we propose an individualized dynamic mediation analysis method.","Our approach can identify the significant mediators of the population level while capturing the time-varying and heterogeneous mediation effects via latent factor modeling on coefficients of structural equation models.","Another advantage of our method is that we can infer individualized mediation effects in the presence of unmeasured time-varying confounders.","We provide estimation consistency for our proposed causal estimand and selection consistency for significant mediators.","Extensive simulation studies and an application to a DNA methylation study demonstrate the effectiveness and advantages of our method."],"url":"http://arxiv.org/abs/2405.17591v1","category":"stat.ME"}
{"created":"2024-05-27 18:38:58","title":"Schottky-Invariant $p$-Adic Diffusion Operators","abstract":"A parametrised diffusion operator on the regular domain $\\Omega$ of a $p$-adic Schottky group is constructed. It is defined as an integral operator on the complex-valued functions on $\\Omega$ which are invariant under the Schottky group $\\Gamma$, where integration is against the measure defined by an invariant regular differential 1-form $\\omega$. It is proven that the space of Schottky invariant $L^2$-functions on $\\Omega$ outside the zeros of $\\omega$ has an orthonormal basis consiting of $\\Gamma$-invariant extensions of Kozyrev wavelets which are eigenfunctions of the operator. The eigenvalues are calculated, and it is shown that the heat equation for this operator provides a unique solution for its Cauchy problem with Schottky-invariant continuous initial conditions supportes outside the zero set of $\\omega$, and gives rise to a strong Markov process on the corresponding orbit space for the Schottky group whose paths are c\\`adl\\`ag.","sentences":["A parametrised diffusion operator on the regular domain $\\Omega$ of a $p$-adic Schottky group is constructed.","It is defined as an integral operator on the complex-valued functions on $\\Omega$ which are invariant under the Schottky group $\\Gamma$, where integration is against the measure defined by an invariant regular differential 1-form $\\omega$. It is proven that the space of Schottky invariant $L^2$-functions on $\\Omega$ outside the zeros of $\\omega$ has an orthonormal basis consiting of $\\Gamma$-invariant extensions of Kozyrev wavelets which are eigenfunctions of the operator.","The eigenvalues are calculated, and it is shown that the heat equation for this operator provides a unique solution for its Cauchy problem with Schottky-invariant continuous initial conditions supportes outside the zero set of $\\omega$, and gives rise to a strong Markov process on the corresponding orbit space for the Schottky group whose paths are c\\`adl\\`ag."],"url":"http://arxiv.org/abs/2405.17586v1","category":"math.AG"}
{"created":"2024-05-27 18:33:37","title":"Understanding Forgetting in Continual Learning with Linear Regression","abstract":"Continual learning, focused on sequentially learning multiple tasks, has gained significant attention recently. Despite the tremendous progress made in the past, the theoretical understanding, especially factors contributing to catastrophic forgetting, remains relatively unexplored. In this paper, we provide a general theoretical analysis of forgetting in the linear regression model via Stochastic Gradient Descent (SGD) applicable to both underparameterized and overparameterized regimes. Our theoretical framework reveals some interesting insights into the intricate relationship between task sequence and algorithmic parameters, an aspect not fully captured in previous studies due to their restrictive assumptions. Specifically, we demonstrate that, given a sufficiently large data size, the arrangement of tasks in a sequence, where tasks with larger eigenvalues in their population data covariance matrices are trained later, tends to result in increased forgetting. Additionally, our findings highlight that an appropriate choice of step size will help mitigate forgetting in both underparameterized and overparameterized settings. To validate our theoretical analysis, we conducted simulation experiments on both linear regression models and Deep Neural Networks (DNNs). Results from these simulations substantiate our theoretical findings.","sentences":["Continual learning, focused on sequentially learning multiple tasks, has gained significant attention recently.","Despite the tremendous progress made in the past, the theoretical understanding, especially factors contributing to catastrophic forgetting, remains relatively unexplored.","In this paper, we provide a general theoretical analysis of forgetting in the linear regression model via Stochastic Gradient Descent (SGD) applicable to both underparameterized and overparameterized regimes.","Our theoretical framework reveals some interesting insights into the intricate relationship between task sequence and algorithmic parameters, an aspect not fully captured in previous studies due to their restrictive assumptions.","Specifically, we demonstrate that, given a sufficiently large data size, the arrangement of tasks in a sequence, where tasks with larger eigenvalues in their population data covariance matrices are trained later, tends to result in increased forgetting.","Additionally, our findings highlight that an appropriate choice of step size will help mitigate forgetting in both underparameterized and overparameterized settings.","To validate our theoretical analysis, we conducted simulation experiments on both linear regression models and Deep Neural Networks (DNNs).","Results from these simulations substantiate our theoretical findings."],"url":"http://arxiv.org/abs/2405.17583v1","category":"cs.LG"}
{"created":"2024-05-27 18:20:24","title":"Local structure theory of Einstein manifolds with boundary","abstract":"We study local structure of the moduli space of compact Einstein metrics with respect to the boundary conformal metric and mean curvature. In dimension three, we confirm M. Anderson's conjecture in a strong sense, showing that the map from Einstein metrics to such boundary data is generically a local diffeomorphism. In dimensions greater than three, we obtain similar results for Ricci flat metrics and negative Einstein metrics under new non-degenerate boundary conditions.","sentences":["We study local structure of the moduli space of compact Einstein metrics with respect to the boundary conformal metric and mean curvature.","In dimension three, we confirm M. Anderson's conjecture in a strong sense, showing that the map from Einstein metrics to such boundary data is generically a local diffeomorphism.","In dimensions greater than three, we obtain similar results for Ricci flat metrics and negative Einstein metrics under new non-degenerate boundary conditions."],"url":"http://arxiv.org/abs/2405.17577v1","category":"math.DG"}
{"created":"2024-05-27 18:15:40","title":"Interpretable Prognostics with Concept Bottleneck Models","abstract":"Deep learning approaches have recently been extensively explored for the prognostics of industrial assets. However, they still suffer from a lack of interpretability, which hinders their adoption in safety-critical applications. To improve their trustworthiness, explainable AI (XAI) techniques have been applied in prognostics, primarily to quantify the importance of input variables for predicting the remaining useful life (RUL) using post-hoc attribution methods. In this work, we propose the application of Concept Bottleneck Models (CBMs), a family of inherently interpretable neural network architectures based on concept explanations, to the task of RUL prediction. Unlike attribution methods, which explain decisions in terms of low-level input features, concepts represent high-level information that is easily understandable by users. Moreover, once verified in actual applications, CBMs enable domain experts to intervene on the concept activations at test-time. We propose using the different degradation modes of an asset as intermediate concepts. Our case studies on the New Commercial Modular AeroPropulsion System Simulation (N-CMAPSS) aircraft engine dataset for RUL prediction demonstrate that the performance of CBMs can be on par or superior to black-box models, while being more interpretable, even when the available labeled concepts are limited. Code available at \\href{https://github.com/EPFL-IMOS/concept-prognostics/}{\\url{github.com/EPFL-IMOS/concept-prognostics/}}.","sentences":["Deep learning approaches have recently been extensively explored for the prognostics of industrial assets.","However, they still suffer from a lack of interpretability, which hinders their adoption in safety-critical applications.","To improve their trustworthiness, explainable AI (XAI) techniques have been applied in prognostics, primarily to quantify the importance of input variables for predicting the remaining useful life (RUL) using post-hoc attribution methods.","In this work, we propose the application of Concept Bottleneck Models (CBMs), a family of inherently interpretable neural network architectures based on concept explanations, to the task of RUL prediction.","Unlike attribution methods, which explain decisions in terms of low-level input features, concepts represent high-level information that is easily understandable by users.","Moreover, once verified in actual applications, CBMs enable domain experts to intervene on the concept activations at test-time.","We propose using the different degradation modes of an asset as intermediate concepts.","Our case studies on the New Commercial Modular AeroPropulsion System Simulation (N-CMAPSS) aircraft engine dataset for RUL prediction demonstrate that the performance of CBMs can be on par or superior to black-box models, while being more interpretable, even when the available labeled concepts are limited.","Code available at \\href{https://github.com/EPFL-IMOS/concept-prognostics/}{\\url{github.com/EPFL-IMOS/concept-prognostics/}}."],"url":"http://arxiv.org/abs/2405.17575v1","category":"cs.LG"}
{"created":"2024-05-27 18:10:55","title":"Bluesky: Network Topology, Polarisation, and Algorithmic Curation","abstract":"Bluesky is a nascent ``Twitter-like'' and decentralized social media network with novel features and unprecedented data access. This paper provides a characterization of the network, studying the political leaning, polarization, network structure, and algorithmic curation mechanisms of five million users. The dataset spans from the website's first release in February of 2023. Users of the new social media site are predominantly left-center leaning and share little to no links associated with questionable sources. In contrast to the homogeneous political stance, we find significant issues-based divergence by studying opinions related to the Israel-Palestine conflict. Two clear homophilic clusters emerge: Pro-Palestinian voices make up the plurality of messages related to the conflict and the proportion has increased with a lessening of interest. We investigate multiple layers of the multi-scale Bluesky network based on replies, likes, reposts, and follows, highlighting differences and similarities between the layers. We differentiate between persistent and non-persistent interactions and measure metrics of network topology over time. All networks are heavy-tailed, clustered, and connected by short paths. We showcase all feeds - algorithmic content recommenders - created for and by users. A large number of custom feeds have been created but their uptake by users is limited. Multiple popular feeds aim to provide similar feeds that are neither topical nor chronological. We conclude by claiming that Bluesky - for all its novel features - is very similar in terms of its network structure to existing and larger social media sites and provides unprecedented research opportunities for social scientists, network scientists, and political scientists alike.","sentences":["Bluesky is a nascent ``Twitter-like'' and decentralized social media network with novel features and unprecedented data access.","This paper provides a characterization of the network, studying the political leaning, polarization, network structure, and algorithmic curation mechanisms of five million users.","The dataset spans from the website's first release in February of 2023.","Users of the new social media site are predominantly left-center leaning and share little to no links associated with questionable sources.","In contrast to the homogeneous political stance, we find significant issues-based divergence by studying opinions related to the Israel-Palestine conflict.","Two clear homophilic clusters emerge: Pro-Palestinian voices make up the plurality of messages related to the conflict and the proportion has increased with a lessening of interest.","We investigate multiple layers of the multi-scale Bluesky network based on replies, likes, reposts, and follows, highlighting differences and similarities between the layers.","We differentiate between persistent and non-persistent interactions and measure metrics of network topology over time.","All networks are heavy-tailed, clustered, and connected by short paths.","We showcase all feeds - algorithmic content recommenders - created for and by users.","A large number of custom feeds have been created but their uptake by users is limited.","Multiple popular feeds aim to provide similar feeds that are neither topical nor chronological.","We conclude by claiming that Bluesky - for all its novel features - is very similar in terms of its network structure to existing and larger social media sites and provides unprecedented research opportunities for social scientists, network scientists, and political scientists alike."],"url":"http://arxiv.org/abs/2405.17571v1","category":"cs.SI"}
{"created":"2024-05-27 18:03:37","title":"ExtremeMETA: High-speed Lightweight Image Segmentation Model by Remodeling Multi-channel Metamaterial Imagers","abstract":"Deep neural networks (DNNs) have heavily relied on traditional computational units like CPUs and GPUs. However, this conventional approach brings significant computational burdens, latency issues, and high power consumption, limiting their effectiveness. This has sparked the need for lightweight networks like ExtremeC3Net. On the other hand, there have been notable advancements in optical computational units, particularly with metamaterials, offering the exciting prospect of energy-efficient neural networks operating at the speed of light. Yet, the digital design of metamaterial neural networks (MNNs) faces challenges such as precision, noise, and bandwidth, limiting their application to intuitive tasks and low-resolution images. In this paper, we propose a large kernel lightweight segmentation model, ExtremeMETA. Based on the ExtremeC3Net, the ExtremeMETA maximizes the ability of the first convolution layer by exploring a larger convolution kernel and multiple processing paths. With the proposed large kernel convolution model, we extend the optic neural network application boundary to the segmentation task. To further lighten the computation burden of the digital processing part, a set of model compression methods is applied to improve model efficiency in the inference stage. The experimental results on three publicly available datasets demonstrate that the optimized efficient design improved segmentation performance from 92.45 to 95.97 on mIoU while reducing computational FLOPs from 461.07 MMacs to 166.03 MMacs. The proposed the large kernel lightweight model ExtremeMETA showcases the hybrid design's ability on complex tasks.","sentences":["Deep neural networks (DNNs) have heavily relied on traditional computational units like CPUs and GPUs.","However, this conventional approach brings significant computational burdens, latency issues, and high power consumption, limiting their effectiveness.","This has sparked the need for lightweight networks like ExtremeC3Net.","On the other hand, there have been notable advancements in optical computational units, particularly with metamaterials, offering the exciting prospect of energy-efficient neural networks operating at the speed of light.","Yet, the digital design of metamaterial neural networks (MNNs) faces challenges such as precision, noise, and bandwidth, limiting their application to intuitive tasks and low-resolution images.","In this paper, we propose a large kernel lightweight segmentation model, ExtremeMETA.","Based on the ExtremeC3Net, the ExtremeMETA maximizes the ability of the first convolution layer by exploring a larger convolution kernel and multiple processing paths.","With the proposed large kernel convolution model, we extend the optic neural network application boundary to the segmentation task.","To further lighten the computation burden of the digital processing part, a set of model compression methods is applied to improve model efficiency in the inference stage.","The experimental results on three publicly available datasets demonstrate that the optimized efficient design improved segmentation performance from 92.45 to 95.97 on mIoU while reducing computational FLOPs from 461.07 MMacs to 166.03 MMacs.","The proposed the large kernel lightweight model ExtremeMETA showcases the hybrid design's ability on complex tasks."],"url":"http://arxiv.org/abs/2405.17568v1","category":"cs.CV"}
{"created":"2024-05-27 18:00:49","title":"A deep-learning algorithm to disentangle self-interacting dark matter and AGN feedback models","abstract":"Different models of dark matter can alter the distribution of mass in galaxy clusters in a variety of ways. However, so can uncertain astrophysical feedback mechanisms. Here we present a Machine Learning method that ''learns'' how the impact of dark matter self-interactions differs from that of astrophysical feedback in order to break this degeneracy and make inferences on dark matter. We train a Convolutional Neural Network on images of galaxy clusters from hydro-dynamic simulations. In the idealised case our algorithm is 80% accurate at identifying if a galaxy cluster harbours collisionless dark matter, dark matter with ${\\sigma}_{\\rm DM}/m = 0.1$cm$^2/$g or with ${\\sigma}_{DM}/m = 1$cm$^2$/g. Whilst we find adding X-ray emissivity maps does not improve the performance in differentiating collisional dark matter, it does improve the ability to disentangle different models of astrophysical feedback. We include noise to resemble data expected from Euclid and Chandra and find our model has a statistical error of < 0.01cm$^2$/g and that our algorithm is insensitive to shape measurement bias and photometric redshift errors. This method represents a new way to analyse data from upcoming telescopes that is an order of magnitude more precise and many orders faster, enabling us to explore the dark matter parameter space like never before.","sentences":["Different models of dark matter can alter the distribution of mass in galaxy clusters in a variety of ways.","However, so can uncertain astrophysical feedback mechanisms.","Here we present a Machine Learning method that ''learns'' how the impact of dark matter self-interactions differs from that of astrophysical feedback in order to break this degeneracy and make inferences on dark matter.","We train a Convolutional Neural Network on images of galaxy clusters from hydro-dynamic simulations.","In the idealised case our algorithm is 80% accurate at identifying if a galaxy cluster harbours collisionless dark matter, dark matter with ${\\sigma}_{\\rm DM}/m = 0.1$cm$^2/$g or with ${\\sigma}_{DM}/m = 1$cm$^2$/g.","Whilst we find adding X-ray emissivity maps does not improve the performance in differentiating collisional dark matter, it does improve the ability to disentangle different models of astrophysical feedback.","We include noise to resemble data expected from Euclid and Chandra and find our model has a statistical error of < 0.01cm$^2$/g and that our algorithm is insensitive to shape measurement bias and photometric redshift errors.","This method represents a new way to analyse data from upcoming telescopes that is an order of magnitude more precise and many orders faster, enabling us to explore the dark matter parameter space like never before."],"url":"http://arxiv.org/abs/2405.17566v1","category":"astro-ph.CO"}
{"created":"2024-05-27 18:00:03","title":"Probabilistic Verification of Neural Networks using Branch and Bound","abstract":"Probabilistic verification of neural networks is concerned with formally analysing the output distribution of a neural network under a probability distribution of the inputs. Examples of probabilistic verification include verifying the demographic parity fairness notion or quantifying the safety of a neural network. We present a new algorithm for the probabilistic verification of neural networks based on an algorithm for computing and iteratively refining lower and upper bounds on probabilities over the outputs of a neural network. By applying state-of-the-art bound propagation and branch and bound techniques from non-probabilistic neural network verification, our algorithm significantly outpaces existing probabilistic verification algorithms, reducing solving times for various benchmarks from the literature from tens of minutes to tens of seconds. Furthermore, our algorithm compares favourably even to dedicated algorithms for restricted subsets of probabilistic verification. We complement our empirical evaluation with a theoretical analysis, proving that our algorithm is sound and, under mildly restrictive conditions, also complete when using a suitable set of heuristics.","sentences":["Probabilistic verification of neural networks is concerned with formally analysing the output distribution of a neural network under a probability distribution of the inputs.","Examples of probabilistic verification include verifying the demographic parity fairness notion or quantifying the safety of a neural network.","We present a new algorithm for the probabilistic verification of neural networks based on an algorithm for computing and iteratively refining lower and upper bounds on probabilities over the outputs of a neural network.","By applying state-of-the-art bound propagation and branch and bound techniques from non-probabilistic neural network verification, our algorithm significantly outpaces existing probabilistic verification algorithms, reducing solving times for various benchmarks from the literature from tens of minutes to tens of seconds.","Furthermore, our algorithm compares favourably even to dedicated algorithms for restricted subsets of probabilistic verification.","We complement our empirical evaluation with a theoretical analysis, proving that our algorithm is sound and, under mildly restrictive conditions, also complete when using a suitable set of heuristics."],"url":"http://arxiv.org/abs/2405.17556v1","category":"cs.LG"}
{"created":"2024-05-27 18:00:00","title":"Bayesian RG Flow in Neural Network Field Theories","abstract":"The Neural Network Field Theory correspondence (NNFT) is a mapping from neural network (NN) architectures into the space of statistical field theories (SFTs). The Bayesian renormalization group (BRG) is an information-theoretic coarse graining scheme that generalizes the principles of the Exact Renormalization Group (ERG) to arbitrarily parameterized probability distributions, including those of NNs. In BRG, coarse graining is performed in parameter space with respect to an information-theoretic distinguishability scale set by the Fisher information metric. In this paper, we unify NNFT and BRG to form a powerful new framework for exploring the space of NNs and SFTs, which we coin BRG-NNFT. With BRG-NNFT, NN training dynamics can be interpreted as inducing a flow in the space of SFTs from the information-theoretic `IR' $\\rightarrow$ `UV'. Conversely, applying an information-shell coarse graining to the trained network's parameters induces a flow in the space of SFTs from the information-theoretic `UV' $\\rightarrow$ `IR'. When the information-theoretic cutoff scale coincides with a standard momentum scale, BRG is equivalent to ERG. We demonstrate the BRG-NNFT correspondence on two analytically tractable examples. First, we construct BRG flows for trained, infinite-width NNs, of arbitrary depth, with generic activation functions. As a special case, we then restrict to architectures with a single infinitely-wide layer, scalar outputs, and generalized cos-net activations. In this case, we show that BRG coarse-graining corresponds exactly to the momentum-shell ERG flow of a free scalar SFT. Our analytic results are corroborated by a numerical experiment in which an ensemble of asymptotically wide NNs are trained and subsequently renormalized using an information-shell BRG scheme.","sentences":["The Neural Network Field Theory correspondence (NNFT) is a mapping from neural network (NN) architectures into the space of statistical field theories (SFTs).","The Bayesian renormalization group (BRG) is an information-theoretic coarse graining scheme that generalizes the principles of the Exact Renormalization Group (ERG) to arbitrarily parameterized probability distributions, including those of NNs.","In BRG, coarse graining is performed in parameter space with respect to an information-theoretic distinguishability scale set by the Fisher information metric.","In this paper, we unify NNFT and BRG to form a powerful new framework for exploring the space of NNs and SFTs, which we coin BRG-NNFT.","With BRG-NNFT, NN training dynamics can be interpreted as inducing a flow in the space of SFTs from the information-theoretic `IR' $\\rightarrow$ `UV'.","Conversely, applying an information-shell coarse graining to the trained network's parameters induces a flow in the space of SFTs from the information-theoretic `UV' $\\rightarrow$ `IR'.","When the information-theoretic cutoff scale coincides with a standard momentum scale, BRG is equivalent to ERG.","We demonstrate the BRG-NNFT correspondence on two analytically tractable examples.","First, we construct BRG flows for trained, infinite-width NNs, of arbitrary depth, with generic activation functions.","As a special case, we then restrict to architectures with a single infinitely-wide layer, scalar outputs, and generalized cos-net activations.","In this case, we show that BRG coarse-graining corresponds exactly to the momentum-shell ERG flow of a free scalar SFT.","Our analytic results are corroborated by a numerical experiment in which an ensemble of asymptotically wide NNs are trained and subsequently renormalized using an information-shell BRG scheme."],"url":"http://arxiv.org/abs/2405.17538v1","category":"hep-th"}
{"created":"2024-05-27 18:00:00","title":"Approximately-symmetric neural networks for quantum spin liquids","abstract":"We propose and analyze a family of approximately-symmetric neural networks for quantum spin liquid problems. These tailored architectures are parameter-efficient, scalable, and significantly out-perform existing symmetry-unaware neural network architectures. Utilizing the mixed-field toric code model, we demonstrate that our approach is competitive with the state-of-the-art tensor network and quantum Monte Carlo methods. Moreover, at the largest system sizes (N=480), our method allows us to explore Hamiltonians with sign problems beyond the reach of both quantum Monte Carlo and finite-size matrix-product states. The network comprises an exactly symmetric block following a non-symmetric block, which we argue learns a transformation of the ground state analogous to quasiadiabatic continuation. Our work paves the way toward investigating quantum spin liquid problems within interpretable neural network architectures","sentences":["We propose and analyze a family of approximately-symmetric neural networks for quantum spin liquid problems.","These tailored architectures are parameter-efficient, scalable, and significantly out-perform existing symmetry-unaware neural network architectures.","Utilizing the mixed-field toric code model, we demonstrate that our approach is competitive with the state-of-the-art tensor network and quantum Monte Carlo methods.","Moreover, at the largest system sizes (N=480), our method allows us to explore Hamiltonians with sign problems beyond the reach of both quantum Monte Carlo and finite-size matrix-product states.","The network comprises an exactly symmetric block following a non-symmetric block, which we argue learns a transformation of the ground state analogous to quasiadiabatic continuation.","Our work paves the way toward investigating quantum spin liquid problems within interpretable neural network architectures"],"url":"http://arxiv.org/abs/2405.17541v1","category":"quant-ph"}
{"created":"2024-05-27 17:59:35","title":"From Neurons to Neutrons: A Case Study in Interpretability","abstract":"Mechanistic Interpretability (MI) promises a path toward fully understanding how neural networks make their predictions. Prior work demonstrates that even when trained to perform simple arithmetic, models can implement a variety of algorithms (sometimes concurrently) depending on initialization and hyperparameters. Does this mean neuron-level interpretability techniques have limited applicability? We argue that high-dimensional neural networks can learn low-dimensional representations of their training data that are useful beyond simply making good predictions. Such representations can be understood through the mechanistic interpretability lens and provide insights that are surprisingly faithful to human-derived domain knowledge. This indicates that such approaches to interpretability can be useful for deriving a new understanding of a problem from models trained to solve it. As a case study, we extract nuclear physics concepts by studying models trained to reproduce nuclear data.","sentences":["Mechanistic Interpretability (MI) promises a path toward fully understanding how neural networks make their predictions.","Prior work demonstrates that even when trained to perform simple arithmetic, models can implement a variety of algorithms (sometimes concurrently) depending on initialization and hyperparameters.","Does this mean neuron-level interpretability techniques have limited applicability?","We argue that high-dimensional neural networks can learn low-dimensional representations of their training data that are useful beyond simply making good predictions.","Such representations can be understood through the mechanistic interpretability lens and provide insights that are surprisingly faithful to human-derived domain knowledge.","This indicates that such approaches to interpretability can be useful for deriving a new understanding of a problem from models trained to solve it.","As a case study, we extract nuclear physics concepts by studying models trained to reproduce nuclear data."],"url":"http://arxiv.org/abs/2405.17425v1","category":"cs.LG"}
{"created":"2024-05-27 17:59:07","title":"MoSca: Dynamic Gaussian Fusion from Casual Videos via 4D Motion Scaffolds","abstract":"We introduce 4D Motion Scaffolds (MoSca), a neural information processing system designed to reconstruct and synthesize novel views of dynamic scenes from monocular videos captured casually in the wild. To address such a challenging and ill-posed inverse problem, we leverage prior knowledge from foundational vision models, lift the video data to a novel Motion Scaffold (MoSca) representation, which compactly and smoothly encodes the underlying motions / deformations. The scene geometry and appearance are then disentangled from the deformation field, and are encoded by globally fusing the Gaussians anchored onto the MoSca and optimized via Gaussian Splatting. Additionally, camera poses can be seamlessly initialized and refined during the dynamic rendering process, without the need for other pose estimation tools. Experiments demonstrate state-of-the-art performance on dynamic rendering benchmarks.","sentences":["We introduce 4D Motion Scaffolds (MoSca), a neural information processing system designed to reconstruct and synthesize novel views of dynamic scenes from monocular videos captured casually in the wild.","To address such a challenging and ill-posed inverse problem, we leverage prior knowledge from foundational vision models, lift the video data to a novel Motion Scaffold (MoSca) representation, which compactly and smoothly encodes the underlying motions / deformations.","The scene geometry and appearance are then disentangled from the deformation field, and are encoded by globally fusing the Gaussians anchored onto the MoSca and optimized via Gaussian Splatting.","Additionally, camera poses can be seamlessly initialized and refined during the dynamic rendering process, without the need for other pose estimation tools.","Experiments demonstrate state-of-the-art performance on dynamic rendering benchmarks."],"url":"http://arxiv.org/abs/2405.17421v1","category":"cs.CV"}
{"created":"2024-05-27 17:59:04","title":"Survival of the Fittest Representation: A Case Study with Modular Addition","abstract":"When a neural network can learn multiple distinct algorithms to solve a task, how does it \"choose\" between them during training? To approach this question, we take inspiration from ecology: when multiple species coexist, they eventually reach an equilibrium where some survive while others die out. Analogously, we suggest that a neural network at initialization contains many solutions (representations and algorithms), which compete with each other under pressure from resource constraints, with the \"fittest\" ultimately prevailing. To investigate this Survival of the Fittest hypothesis, we conduct a case study on neural networks performing modular addition, and find that these networks' multiple circular representations at different Fourier frequencies undergo such competitive dynamics, with only a few circles surviving at the end. We find that the frequencies with high initial signals and gradients, the \"fittest,\" are more likely to survive. By increasing the embedding dimension, we also observe more surviving frequencies. Inspired by the Lotka-Volterra equations describing the dynamics between species, we find that the dynamics of the circles can be nicely characterized by a set of linear differential equations. Our results with modular addition show that it is possible to decompose complicated representations into simpler components, along with their basic interactions, to offer insight on the training dynamics of representations.","sentences":["When a neural network can learn multiple distinct algorithms to solve a task, how does it \"choose\" between them during training?","To approach this question, we take inspiration from ecology: when multiple species coexist, they eventually reach an equilibrium where some survive while others die out.","Analogously, we suggest that a neural network at initialization contains many solutions (representations and algorithms), which compete with each other under pressure from resource constraints, with the \"fittest\" ultimately prevailing.","To investigate this Survival of the Fittest hypothesis, we conduct a case study on neural networks performing modular addition, and find that these networks' multiple circular representations at different Fourier frequencies undergo such competitive dynamics, with only a few circles surviving at the end.","We find that the frequencies with high initial signals and gradients, the \"fittest,\" are more likely to survive.","By increasing the embedding dimension, we also observe more surviving frequencies.","Inspired by the Lotka-Volterra equations describing the dynamics between species, we find that the dynamics of the circles can be nicely characterized by a set of linear differential equations.","Our results with modular addition show that it is possible to decompose complicated representations into simpler components, along with their basic interactions, to offer insight on the training dynamics of representations."],"url":"http://arxiv.org/abs/2405.17420v1","category":"cs.LG"}
{"created":"2024-05-27 17:58:18","title":"Splitting aspects of holomorphic distributions with locally free tangent sheaf","abstract":"In this work, we mainly deal with a two-dimensional singular holomorphic distribution $\\mathcal{D}$ defined on $M$, in the two situations $M=\\mathbb{P}^n$ or $M=(\\mathbb{C}^n,0)$, tangent to a one-dimensional foliation $\\mathcal{G}$ on $M$, and whose tangent sheaf $T_{\\mathcal{D}}$ is locally free. We provide sufficient conditions on $\\mathcal{G}$ so that there is another one-dimensional foliation $\\mathcal{H}$ on $M$ tangent to $\\mathcal{D}$, such that their respective tangent sheaves satisfy the splitting relation $T_{\\mathcal{D}}=T_{\\mathcal{G}} \\oplus T_{\\mathcal{H}}$. As an application, we show that if $\\mathcal{F}$ is a codimension one holomorphic foliation on $\\mathbb{P}^3$ with locally free tangent sheaf and tangent to a nontrivial holomorphic vector field on $\\mathbb{P}^3$, then $T_{\\mathcal{F}}$ splits. Some results on division of holomorphic differential forms by tangent vector fields are also obtained.","sentences":["In this work, we mainly deal with a two-dimensional singular holomorphic distribution $\\mathcal{D}$ defined on $M$, in the two situations $M=\\mathbb{P}^n$ or $M=(\\mathbb{C}^n,0)$, tangent to a one-dimensional foliation $\\mathcal{G}$ on $M$, and whose tangent sheaf $T_{\\mathcal{D}}$ is locally free.","We provide sufficient conditions on $\\mathcal{G}$ so that there is another one-dimensional foliation $\\mathcal{H}$ on $M$ tangent to $\\mathcal{D}$, such that their respective tangent sheaves satisfy the splitting relation $T_{\\mathcal{D}}=T_{\\mathcal{G}} \\oplus T_{\\mathcal{H}}$. As an application, we show that if $\\mathcal{F}$ is a codimension one holomorphic foliation on $\\mathbb{P}^3$ with locally free tangent sheaf and tangent to a nontrivial holomorphic vector field on $\\mathbb{P}^3$, then $T_{\\mathcal{F}}$ splits.","Some results on division of holomorphic differential forms by tangent vector fields are also obtained."],"url":"http://arxiv.org/abs/2405.17415v1","category":"math.CV"}
{"created":"2024-05-27 17:55:05","title":"Deep Learning Calabi-Yau four folds with hybrid and recurrent neural network architectures","abstract":"In this work, we report the results of applying deep learning based on hybrid convolutional-recurrent and purely recurrent neural network architectures to the dataset of almost one million complete intersection Calabi-Yau four-folds (CICY4) to machine-learn their four Hodge numbers $h^{1,1}, h^{2,1}, h^{3,1}, h^{2,2}$. In particular, we explored and experimented with twelve different neural network models, nine of which are convolutional-recurrent (CNN-RNN) hybrids with the RNN unit being either GRU (Gated Recurrent Unit) or Long Short Term Memory (LSTM). The remaining four models are purely recurrent neural networks based on LSTM. In terms of the $h^{1,1}, h^{2,1}, h^{3,1}, h^{2,2}$ prediction accuracies, at 72% training ratio, our best performing individual model is CNN-LSTM-400, a hybrid CNN-LSTM with the LSTM hidden size of 400, which obtained 99.74%, 98.07%, 95.19%, 81.01%, our second best performing individual model is LSTM-448, an LSTM-based model with the hidden size of 448, which obtained 99.74%, 97.51%, 94.24%, and 78.63%. These results were improved by forming ensembles of the top two, three or even four models. Our best ensemble, consisting of the top three models, achieved the accuracies of 99.80%, 98.40%, 95.80%, 83.02%. At 80% training ratio, the top two performing models LSTM-448 and LSTM-424 are both LSTM-based with the hidden sizes of 448 and 424. Compared with the 72% training ratio, there is a significant improvement of accuracies, which reached 99.85%, 98.66%, 96.26%, 84.77% for the best individual model and 99.88%, 98.91%, 96.96%, 86.78% for the best ensemble.","sentences":["In this work, we report the results of applying deep learning based on hybrid convolutional-recurrent and purely recurrent neural network architectures to the dataset of almost one million complete intersection Calabi-Yau four-folds (CICY4) to machine-learn their four Hodge numbers $h^{1,1}, h^{2,1}, h^{3,1}, h^{2,2}$. In particular, we explored and experimented with twelve different neural network models, nine of which are convolutional-recurrent (CNN-RNN) hybrids with the RNN unit being either GRU (Gated Recurrent Unit) or Long Short Term Memory (LSTM).","The remaining four models are purely recurrent neural networks based on LSTM.","In terms of the $h^{1,1}, h^{2,1}, h^{3,1}, h^{2,2}$ prediction accuracies, at 72% training ratio, our best performing individual model is CNN-LSTM-400, a hybrid CNN-LSTM with the LSTM hidden size of 400, which obtained 99.74%, 98.07%, 95.19%, 81.01%, our second best performing individual model is LSTM-448, an LSTM-based model with the hidden size of 448, which obtained 99.74%, 97.51%, 94.24%, and 78.63%.","These results were improved by forming ensembles of the top two, three or even four models.","Our best ensemble, consisting of the top three models, achieved the accuracies of 99.80%, 98.40%, 95.80%, 83.02%.","At 80% training ratio, the top two performing models LSTM-448 and LSTM-424 are both LSTM-based with the hidden sizes of 448 and 424.","Compared with the 72% training ratio, there is a significant improvement of accuracies, which reached 99.85%, 98.66%, 96.26%, 84.77% for the best individual model and 99.88%, 98.91%, 96.96%, 86.78% for the best ensemble."],"url":"http://arxiv.org/abs/2405.17406v1","category":"hep-th"}
{"created":"2024-05-27 17:55:01","title":"Calibrated Dataset Condensation for Faster Hyperparameter Search","abstract":"Dataset condensation can be used to reduce the computational cost of training multiple models on a large dataset by condensing the training dataset into a small synthetic set. State-of-the-art approaches rely on matching the model gradients between the real and synthetic data. However, there is no theoretical guarantee of the generalizability of the condensed data: data condensation often generalizes poorly across hyperparameters/architectures in practice. This paper considers a different condensation objective specifically geared toward hyperparameter search. We aim to generate a synthetic validation dataset so that the validation-performance rankings of the models, with different hyperparameters, on the condensed and original datasets are comparable. We propose a novel hyperparameter-calibrated dataset condensation (HCDC) algorithm, which obtains the synthetic validation dataset by matching the hyperparameter gradients computed via implicit differentiation and efficient inverse Hessian approximation. Experiments demonstrate that the proposed framework effectively maintains the validation-performance rankings of models and speeds up hyperparameter/architecture search for tasks on both images and graphs.","sentences":["Dataset condensation can be used to reduce the computational cost of training multiple models on a large dataset by condensing the training dataset into a small synthetic set.","State-of-the-art approaches rely on matching the model gradients between the real and synthetic data.","However, there is no theoretical guarantee of the generalizability of the condensed data: data condensation often generalizes poorly across hyperparameters/architectures in practice.","This paper considers a different condensation objective specifically geared toward hyperparameter search.","We aim to generate a synthetic validation dataset so that the validation-performance rankings of the models, with different hyperparameters, on the condensed and original datasets are comparable.","We propose a novel hyperparameter-calibrated dataset condensation (HCDC) algorithm, which obtains the synthetic validation dataset by matching the hyperparameter gradients computed via implicit differentiation and efficient inverse Hessian approximation.","Experiments demonstrate that the proposed framework effectively maintains the validation-performance rankings of models and speeds up hyperparameter/architecture search for tasks on both images and graphs."],"url":"http://arxiv.org/abs/2405.17535v1","category":"cs.LG"}
{"created":"2024-05-27 17:52:12","title":"Spectral Greedy Coresets for Graph Neural Networks","abstract":"The ubiquity of large-scale graphs in node-classification tasks significantly hinders the real-world applications of Graph Neural Networks (GNNs). Node sampling, graph coarsening, and dataset condensation are effective strategies for enhancing data efficiency. However, owing to the interdependence of graph nodes, coreset selection, which selects subsets of the data examples, has not been successfully applied to speed up GNN training on large graphs, warranting special treatment. This paper studies graph coresets for GNNs and avoids the interdependence issue by selecting ego-graphs (i.e., neighborhood subgraphs around a node) based on their spectral embeddings. We decompose the coreset selection problem for GNNs into two phases: a coarse selection of widely spread ego graphs and a refined selection to diversify their topologies. We design a greedy algorithm that approximately optimizes both objectives. Our spectral greedy graph coreset (SGGC) scales to graphs with millions of nodes, obviates the need for model pre-training, and applies to low-homophily graphs. Extensive experiments on ten datasets demonstrate that SGGC outperforms other coreset methods by a wide margin, generalizes well across GNN architectures, and is much faster than graph condensation.","sentences":["The ubiquity of large-scale graphs in node-classification tasks significantly hinders the real-world applications of Graph Neural Networks (GNNs).","Node sampling, graph coarsening, and dataset condensation are effective strategies for enhancing data efficiency.","However, owing to the interdependence of graph nodes, coreset selection, which selects subsets of the data examples, has not been successfully applied to speed up GNN training on large graphs, warranting special treatment.","This paper studies graph coresets for GNNs and avoids the interdependence issue by selecting ego-graphs (i.e., neighborhood subgraphs around a node) based on their spectral embeddings.","We decompose the coreset selection problem for GNNs into two phases: a coarse selection of widely spread ego graphs and a refined selection to diversify their topologies.","We design a greedy algorithm that approximately optimizes both objectives.","Our spectral greedy graph coreset (SGGC) scales to graphs with millions of nodes, obviates the need for model pre-training, and applies to low-homophily graphs.","Extensive experiments on ten datasets demonstrate that SGGC outperforms other coreset methods by a wide margin, generalizes well across GNN architectures, and is much faster than graph condensation."],"url":"http://arxiv.org/abs/2405.17404v1","category":"cs.LG"}
{"created":"2024-05-27 17:48:32","title":"CrEIMBO: Cross Ensemble Interactions in Multi-view Brain Observations","abstract":"Modern recordings of neural activity provide diverse observations of neurons across brain areas, behavioral conditions, and subjects -- thus presenting an exciting opportunity to reveal the fundamentals of brain-wide dynamics underlying cognitive function. Current methods, however, often fail to fully harness the richness of such data as they either provide an uninterpretable representation (e.g., via \"black box\" deep networks) or over-simplify the model (e.g., assume stationary dynamics or analyze each session independently). Here, instead of regarding asynchronous recordings that lack alignment in neural identity or brain areas as a limitation, we exploit these diverse views of the same brain system to learn a unified model of brain dynamics. We assume that brain observations stem from the joint activity of a set of functional neural ensembles (groups of co-active neurons) that are similar in functionality across recordings, and propose to discover the ensemble and their non-stationary dynamical interactions in a new model we term CrEIMBO (Cross-Ensemble Interactions in Multi-view Brain Observations). CrEIMBO identifies the composition of the per-session neural ensembles through graph-driven dictionary learning and models the ensemble dynamics as a latent sparse time-varying decomposition of global sub-circuits, thereby capturing non-stationary dynamics. CrEIMBO identifies multiple co-active sub-circuits while maintaining representation interpretability due to sharing sub-circuits across sessions. CrEIMBO distinguishes session-specific from global (session-invariant) computations by exploring when distinct sub-circuits are active. We demonstrate CrEIMBO's ability to recover ground truth components in synthetic data and uncover meaningful brain dynamics, capturing cross-subject and inter- and intra-area variability, in high-density electrode recordings of humans performing a memory task.","sentences":["Modern recordings of neural activity provide diverse observations of neurons across brain areas, behavioral conditions, and subjects -- thus presenting an exciting opportunity to reveal the fundamentals of brain-wide dynamics underlying cognitive function.","Current methods, however, often fail to fully harness the richness of such data as they either provide an uninterpretable representation (e.g., via \"black box\" deep networks) or over-simplify the model (e.g., assume stationary dynamics or analyze each session independently).","Here, instead of regarding asynchronous recordings that lack alignment in neural identity or brain areas as a limitation, we exploit these diverse views of the same brain system to learn a unified model of brain dynamics.","We assume that brain observations stem from the joint activity of a set of functional neural ensembles (groups of co-active neurons) that are similar in functionality across recordings, and propose to discover the ensemble and their non-stationary dynamical interactions in a new model we term CrEIMBO (Cross-Ensemble Interactions in Multi-view Brain Observations).","CrEIMBO identifies the composition of the per-session neural ensembles through graph-driven dictionary learning and models the ensemble dynamics as a latent sparse time-varying decomposition of global sub-circuits, thereby capturing non-stationary dynamics.","CrEIMBO identifies multiple co-active sub-circuits while maintaining representation interpretability due to sharing sub-circuits across sessions.","CrEIMBO distinguishes session-specific from global (session-invariant) computations by exploring when distinct sub-circuits are active.","We demonstrate CrEIMBO's ability to recover ground truth components in synthetic data and uncover meaningful brain dynamics, capturing cross-subject and inter- and intra-area variability, in high-density electrode recordings of humans performing a memory task."],"url":"http://arxiv.org/abs/2405.17395v1","category":"q-bio.NC"}
{"created":"2024-05-27 17:45:21","title":"Global existence, fast signal diffusion limit, and $L^\\infty$-in-time convergence rates in a competitive chemotaxis system","abstract":"We study a chemotaxis system that includes two competitive prey and one predator species in a two-dimensional domain, where the movement of prey (resp. predators) is driven by chemicals secreted by predators (resp. prey), called mutually repulsive (resp. mutually attractive) chemotaxis effect. The kinetics for all species are chosen according to the competitive Lotka-Volterra equations for prey and to a Holling type functional response for the predator. Under the biologically relevant scenario that the chemicals diffuse much faster than the individual diffusion of all species and a suitable re-scaling, equations for chemical concentrations are parabolic with slow evolution of coefficient $0<\\varepsilon\\ll 1$. In the first main result, we show the global existence of a unique classical solution to the system for each $\\varepsilon$. Secondly, we study rigorously the so-called fast signal diffusion limit, passing from the system including parabolic equations with the slow evolution to the one with all elliptic equations for chemical concentrations, i.e. the limit as $\\varepsilon \\to 0$. This explains why elliptic equations can be proposed for chemical concentration instead of parabolic ones with slow evolution. Thirdly, the $L^\\infty$-in-time convergence rates for the fast signal diffusion limit are estimated, where the effect of the initial layer is carefully treated. Finally, the differences between the systems with and without the slow evolution, and between the systems with one or two preys are discussed due to numerical simulations.","sentences":["We study a chemotaxis system that includes two competitive prey and one predator species in a two-dimensional domain, where the movement of prey (resp.","predators) is driven by chemicals secreted by predators (resp.","prey), called mutually repulsive (resp.","mutually attractive) chemotaxis effect.","The kinetics for all species are chosen according to the competitive Lotka-Volterra equations for prey and to a Holling type functional response for the predator.","Under the biologically relevant scenario that the chemicals diffuse much faster than the individual diffusion of all species and a suitable re-scaling, equations for chemical concentrations are parabolic with slow evolution of coefficient $0<\\varepsilon\\ll 1$.","In the first main result, we show the global existence of a unique classical solution to the system for each $\\varepsilon$. Secondly, we study rigorously the so-called fast signal diffusion limit, passing from the system including parabolic equations with the slow evolution to the one with all elliptic equations for chemical concentrations, i.e. the limit as $\\varepsilon \\to 0$.","This explains why elliptic equations can be proposed for chemical concentration instead of parabolic ones with slow evolution.","Thirdly, the $L^\\infty$-in-time convergence rates for the fast signal diffusion limit are estimated, where the effect of the initial layer is carefully treated.","Finally, the differences between the systems with and without the slow evolution, and between the systems with one or two preys are discussed due to numerical simulations."],"url":"http://arxiv.org/abs/2405.17392v2","category":"math.AP"}
{"created":"2024-05-27 17:44:33","title":"Dataset-learning duality and emergent criticality","abstract":"In artificial neural networks, the activation dynamics of non-trainable variables is strongly coupled to the learning dynamics of trainable variables. During the activation pass, the boundary neurons (e.g., input neurons) are mapped to the bulk neurons (e.g., hidden neurons), and during the learning pass, both bulk and boundary neurons are mapped to changes in trainable variables (e.g., weights and biases). For example, in feed-forward neural networks, forward propagation is the activation pass and backward propagation is the learning pass. We show that a composition of the two maps establishes a duality map between a subspace of non-trainable boundary variables (e.g., dataset) and a tangent subspace of trainable variables (i.e., learning). In general, the dataset-learning duality is a complex non-linear map between high-dimensional spaces, but in a learning equilibrium, the problem can be linearized and reduced to many weakly coupled one-dimensional problems. We use the duality to study the emergence of criticality, or the power-law distributions of fluctuations of the trainable variables. In particular, we show that criticality can emerge in the learning system even from the dataset in a non-critical state, and that the power-law distribution can be modified by changing either the activation function or the loss function.","sentences":["In artificial neural networks, the activation dynamics of non-trainable variables is strongly coupled to the learning dynamics of trainable variables.","During the activation pass, the boundary neurons (e.g., input neurons) are mapped to the bulk neurons (e.g., hidden neurons), and during the learning pass, both bulk and boundary neurons are mapped to changes in trainable variables (e.g., weights and biases).","For example, in feed-forward neural networks, forward propagation is the activation pass and backward propagation is the learning pass.","We show that a composition of the two maps establishes a duality map between a subspace of non-trainable boundary variables (e.g., dataset) and a tangent subspace of trainable variables (i.e., learning).","In general, the dataset-learning duality is a complex non-linear map between high-dimensional spaces, but in a learning equilibrium, the problem can be linearized and reduced to many weakly coupled one-dimensional problems.","We use the duality to study the emergence of criticality, or the power-law distributions of fluctuations of the trainable variables.","In particular, we show that criticality can emerge in the learning system even from the dataset in a non-critical state, and that the power-law distribution can be modified by changing either the activation function or the loss function."],"url":"http://arxiv.org/abs/2405.17391v1","category":"cs.LG"}
{"created":"2024-05-27 17:33:03","title":"How Does Perfect Fitting Affect Representation Learning? On the Training Dynamics of Representations in Deep Neural Networks","abstract":"In this paper, we elucidate how representations in deep neural networks (DNNs) evolve during training. We focus on overparameterized learning settings where the training continues much after the trained DNN starts to perfectly fit its training data. We examine the evolution of learned representations along the entire training process, including its perfect fitting regime, and with respect to the epoch-wise double descent phenomenon. We explore the representational similarity of DNN layers, each layer with respect to its own representations throughout the training process. For this, we use two similarity metrics: (1) The centered kernel alignment (CKA) similarity; (2) Similarity of decision regions of linear classifier probes that we train for the DNN layers. Our extensive experiments discover training dynamics patterns that can emerge in layers depending on the relative layer-depth, DNN width, and architecture. We show that representations at the deeper layers evolve much more in the training when an epoch-wise double descent occurs. For Vision Transformer, we show that the perfect fitting threshold creates a transition in the evolution of representations across all the encoder blocks.","sentences":["In this paper, we elucidate how representations in deep neural networks (DNNs) evolve during training.","We focus on overparameterized learning settings where the training continues much after the trained DNN starts to perfectly fit its training data.","We examine the evolution of learned representations along the entire training process, including its perfect fitting regime, and with respect to the epoch-wise double descent phenomenon.","We explore the representational similarity of DNN layers, each layer with respect to its own representations throughout the training process.","For this, we use two similarity metrics: (1) The centered kernel alignment (CKA) similarity; (2) Similarity of decision regions of linear classifier probes that we train for the DNN layers.","Our extensive experiments discover training dynamics patterns that can emerge in layers depending on the relative layer-depth, DNN width, and architecture.","We show that representations at the deeper layers evolve much more in the training when an epoch-wise double descent occurs.","For Vision Transformer, we show that the perfect fitting threshold creates a transition in the evolution of representations across all the encoder blocks."],"url":"http://arxiv.org/abs/2405.17377v1","category":"cs.LG"}
{"created":"2024-05-27 17:24:11","title":"Predict joint angle of body parts based on sequence pattern recognition","abstract":"The way organs are positioned and moved in the workplace can cause pain and physical harm. Therefore, ergonomists use ergonomic risk assessments based on visual observation of the workplace, or review pictures and videos taken in the workplace. Sometimes the workers in the photos are not in perfect condition. Some parts of the workers' bodies may not be in the camera's field of view, could be obscured by objects, or by self-occlusion, this is the main problem in 2D human posture recognition. It is difficult to predict the position of body parts when they are not visible in the image, and geometric mathematical methods are not entirely suitable for this purpose. Therefore, we created a dataset with artificial images of a 3D human model, specifically for painful postures, and real human photos from different viewpoints. Each image we captured was based on a predefined joint angle for each 3D model or human model. We created various images, including images where some body parts are not visible. Nevertheless, the joint angle is estimated beforehand, so we could study the case by converting the input images into the sequence of joint connections between predefined body parts and extracting the desired joint angle with a convolutional neural network. In the end, we obtained root mean square error (RMSE) of 12.89 and mean absolute error (MAE) of 4.7 on the test dataset.","sentences":["The way organs are positioned and moved in the workplace can cause pain and physical harm.","Therefore, ergonomists use ergonomic risk assessments based on visual observation of the workplace, or review pictures and videos taken in the workplace.","Sometimes the workers in the photos are not in perfect condition.","Some parts of the workers' bodies may not be in the camera's field of view, could be obscured by objects, or by self-occlusion, this is the main problem in 2D human posture recognition.","It is difficult to predict the position of body parts when they are not visible in the image, and geometric mathematical methods are not entirely suitable for this purpose.","Therefore, we created a dataset with artificial images of a 3D human model, specifically for painful postures, and real human photos from different viewpoints.","Each image we captured was based on a predefined joint angle for each 3D model or human model.","We created various images, including images where some body parts are not visible.","Nevertheless, the joint angle is estimated beforehand, so we could study the case by converting the input images into the sequence of joint connections between predefined body parts and extracting the desired joint angle with a convolutional neural network.","In the end, we obtained root mean square error (RMSE) of 12.89 and mean absolute error (MAE) of 4.7 on the test dataset."],"url":"http://arxiv.org/abs/2405.17369v1","category":"cs.CV"}
{"created":"2024-05-27 17:21:04","title":"Finite Fractal Dimension of uniform attractors for non-autonomous dynamical systems with infinite dimensional symbol space","abstract":"The aim of this paper is to find an upper bound for the box-counting dimension of uniform attractors for non-autonomous dynamical systems. Contrary to the results in literature, we do not ask the symbol space to have finite box-counting dimension. Instead, we ask a condition on the semi-continuity of pullback attractors of the system as time goes to infinity. This semi-continuity can be achieved if we suppose the existence of finite-dimensional exponential uniform attractors for the limit symbols. After showing these new results, we apply them to study the box-counting dimension of the uniform attractor for a reaction-diffusion equation, and we find a specific forcing term such that the symbol space has infinite box-counting dimension but the uniform attractor has finite box-counting dimension anyway.","sentences":["The aim of this paper is to find an upper bound for the box-counting dimension of uniform attractors for non-autonomous dynamical systems.","Contrary to the results in literature, we do not ask the symbol space to have finite box-counting dimension.","Instead, we ask a condition on the semi-continuity of pullback attractors of the system as time goes to infinity.","This semi-continuity can be achieved if we suppose the existence of finite-dimensional exponential uniform attractors for the limit symbols.","After showing these new results, we apply them to study the box-counting dimension of the uniform attractor for a reaction-diffusion equation, and we find a specific forcing term such that the symbol space has infinite box-counting dimension but the uniform attractor has finite box-counting dimension anyway."],"url":"http://arxiv.org/abs/2405.17367v1","category":"math.DS"}
{"created":"2024-05-27 17:14:45","title":"Speech Loudness in Broadcasting and Streaming","abstract":"The introduction and regulation of loudness in broadcasting and streaming brought clear benefits to the audience, e.g., a level of uniformity across programs and channels. Yet, speech loudness is frequently reported as being too low in certain passages, which can hinder the full understanding and enjoyment of movies and TV programs. This paper proposes expanding the set of loudness-based measures typically used in the industry. We focus on speech loudness, and we show that, when clean speech is not available, Deep Neural Networks (DNNs) can be used to isolate the speech signal and so to accurately estimate speech loudness, providing a more precise estimate compared to speech-gated loudness. Moreover, we define critical passages, i.e., passages in which speech is likely to be hard to understand. Critical passages are defined based on the local Speech Loudness Deviation (SLD) and the local Speech-to-Background Loudness Difference (SBLD), as SLD and SBLD significantly contribute to intelligibility and listening effort. In contrast to other more comprehensive measures of intelligibility and listening effort, SLD and SBLD can be straightforwardly measured, are intuitive, and, most importantly, can be easily controlled by adjusting the speech level in the mix or by enabling personalization at the user's end. Finally, examples are provided that show how the detection of critical passages can support the evaluation and control of the speech signal during and after content production.","sentences":["The introduction and regulation of loudness in broadcasting and streaming brought clear benefits to the audience, e.g., a level of uniformity across programs and channels.","Yet, speech loudness is frequently reported as being too low in certain passages, which can hinder the full understanding and enjoyment of movies and TV programs.","This paper proposes expanding the set of loudness-based measures typically used in the industry.","We focus on speech loudness, and we show that, when clean speech is not available, Deep Neural Networks (DNNs) can be used to isolate the speech signal and so to accurately estimate speech loudness, providing a more precise estimate compared to speech-gated loudness.","Moreover, we define critical passages, i.e., passages in which speech is likely to be hard to understand.","Critical passages are defined based on the local Speech Loudness Deviation (SLD) and the local Speech-to-Background Loudness Difference (SBLD), as SLD and SBLD significantly contribute to intelligibility and listening effort.","In contrast to other more comprehensive measures of intelligibility and listening effort, SLD and SBLD can be straightforwardly measured, are intuitive, and, most importantly, can be easily controlled by adjusting the speech level in the mix or by enabling personalization at the user's end.","Finally, examples are provided that show how the detection of critical passages can support the evaluation and control of the speech signal during and after content production."],"url":"http://arxiv.org/abs/2405.17364v1","category":"eess.AS"}
{"created":"2024-05-27 17:10:40","title":"Controllable Deformations in Compressible Isotropic Implicit Elasticity","abstract":"For a given material, \\emph{controllable deformations} are those deformations that can be maintained in the absence of body forces and by applying only boundary tractions. For a given class of materials, \\emph{universal deformations} are those deformations that are controllable for any material within the class. In this paper, we characterize the universal deformations in compressible isotropic implicit elasticity defined by solids whose constitutive equations, in terms of the Cauchy stress $\\boldsymbol{\\sigma}$ and the left Cauchy-Green strain $\\mathbf{b}$, have the implicit form $\\boldsymbol{\\mathsf{f}}(\\boldsymbol{\\sigma},\\mathbf{b})=\\mathbf{0}$. We prove that universal deformations are homogeneous. However, an important observation is that, unlike Cauchy (and Green) elasticity, not every homogeneous deformation is permissible for a given implicit-elastic solid. In other words, the set of universal deformations is material-dependent, yet it remains a subset of homogeneous deformations.","sentences":["For a given material, \\emph{controllable deformations} are those deformations that can be maintained in the absence of body forces and by applying only boundary tractions.","For a given class of materials, \\emph{universal deformations} are those deformations that are controllable for any material within the class.","In this paper, we characterize the universal deformations in compressible isotropic implicit elasticity defined by solids whose constitutive equations, in terms of the Cauchy stress $\\boldsymbol{\\sigma}$ and the left Cauchy-Green strain $\\mathbf{b}$, have the implicit form $\\boldsymbol{\\mathsf{f}}(\\boldsymbol{\\sigma},\\mathbf{b})=\\mathbf{0}$. We prove that universal deformations are homogeneous.","However, an important observation is that, unlike Cauchy (and Green) elasticity, not every homogeneous deformation is permissible for a given implicit-elastic solid.","In other words, the set of universal deformations is material-dependent, yet it remains a subset of homogeneous deformations."],"url":"http://arxiv.org/abs/2405.17362v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-27 17:10:04","title":"A One-Layer Decoder-Only Transformer is a Two-Layer RNN: With an Application to Certified Robustness","abstract":"This paper reveals a key insight that a one-layer decoder-only Transformer is equivalent to a two-layer Recurrent Neural Network (RNN). Building on this insight, we propose ARC-Tran, a novel approach for verifying the robustness of decoder-only Transformers against arbitrary perturbation spaces. Compared to ARC-Tran, current robustness verification techniques are limited either to specific and length-preserving perturbations like word substitutions or to recursive models like LSTMs. ARC-Tran addresses these limitations by meticulously managing position encoding to prevent mismatches and by utilizing our key insight to achieve precise and scalable verification. Our evaluation shows that ARC-Tran (1) trains models more robust to arbitrary perturbation spaces than those produced by existing techniques and (2) shows high certification accuracy of the resulting models.","sentences":["This paper reveals a key insight that a one-layer decoder-only Transformer is equivalent to a two-layer Recurrent Neural Network (RNN).","Building on this insight, we propose ARC-Tran, a novel approach for verifying the robustness of decoder-only Transformers against arbitrary perturbation spaces.","Compared to ARC-Tran, current robustness verification techniques are limited either to specific and length-preserving perturbations like word substitutions or to recursive models like LSTMs.","ARC-Tran addresses these limitations by meticulously managing position encoding to prevent mismatches and by utilizing our key insight to achieve precise and scalable verification.","Our evaluation shows that ARC-Tran (1) trains models more robust to arbitrary perturbation spaces than those produced by existing techniques and (2) shows high certification accuracy of the resulting models."],"url":"http://arxiv.org/abs/2405.17361v1","category":"cs.CL"}
{"created":"2024-05-27 16:54:49","title":"DOF-GS: Adjustable Depth-of-Field 3D Gaussian Splatting for Refocusing,Defocus Rendering and Blur Removal","abstract":"3D Gaussian Splatting-based techniques have recently advanced 3D scene reconstruction and novel view synthesis, achieving high-quality real-time rendering. However, these approaches are inherently limited by the underlying pinhole camera assumption in modeling the images and hence only work for All-in-Focus (AiF) sharp image inputs. This severely affects their applicability in real-world scenarios where images often exhibit defocus blur due to the limited depth-of-field (DOF) of imaging devices. Additionally, existing 3D Gaussian Splatting (3DGS) methods also do not support rendering of DOF effects.   To address these challenges, we introduce DOF-GS that allows for rendering adjustable DOF effects, removing defocus blur as well as refocusing of 3D scenes, all from multi-view images degraded by defocus blur. To this end, we re-imagine the traditional Gaussian Splatting pipeline by employing a finite aperture camera model coupled with explicit, differentiable defocus rendering guided by the Circle-of-Confusion (CoC). The proposed framework provides for dynamic adjustment of DOF effects by changing the aperture and focal distance of the underlying camera model on-demand. It also enables rendering varying DOF effects of 3D scenes post-optimization, and generating AiF images from defocused training images. Furthermore, we devise a joint optimization strategy to further enhance details in the reconstructed scenes by jointly optimizing rendered defocused and AiF images. Our experimental results indicate that DOF-GS produces high-quality sharp all-in-focus renderings conditioned on inputs compromised by defocus blur, with the training process incurring only a modest increase in GPU memory consumption. We further demonstrate the applications of the proposed method for adjustable defocus rendering and refocusing of the 3D scene from input images degraded by defocus blur.","sentences":["3D Gaussian Splatting-based techniques have recently advanced 3D scene reconstruction and novel view synthesis, achieving high-quality real-time rendering.","However, these approaches are inherently limited by the underlying pinhole camera assumption in modeling the images and hence only work for All-in-Focus (AiF) sharp image inputs.","This severely affects their applicability in real-world scenarios where images often exhibit defocus blur due to the limited depth-of-field (DOF) of imaging devices.","Additionally, existing 3D Gaussian Splatting (3DGS) methods also do not support rendering of DOF effects.   ","To address these challenges, we introduce DOF-GS that allows for rendering adjustable DOF effects, removing defocus blur as well as refocusing of 3D scenes, all from multi-view images degraded by defocus blur.","To this end, we re-imagine the traditional Gaussian Splatting pipeline by employing a finite aperture camera model coupled with explicit, differentiable defocus rendering guided by the Circle-of-Confusion (CoC).","The proposed framework provides for dynamic adjustment of DOF effects by changing the aperture and focal distance of the underlying camera model on-demand.","It also enables rendering varying DOF effects of 3D scenes post-optimization, and generating AiF images from defocused training images.","Furthermore, we devise a joint optimization strategy to further enhance details in the reconstructed scenes by jointly optimizing rendered defocused and AiF images.","Our experimental results indicate that DOF-GS produces high-quality sharp all-in-focus renderings conditioned on inputs compromised by defocus blur, with the training process incurring only a modest increase in GPU memory consumption.","We further demonstrate the applications of the proposed method for adjustable defocus rendering and refocusing of the 3D scene from input images degraded by defocus blur."],"url":"http://arxiv.org/abs/2405.17351v1","category":"cs.CV"}
{"created":"2024-05-27 16:54:15","title":"Metric structural human connectomes: localization and multifractality of eigenmodes","abstract":"In this study, we explore the fundamental principles behind the architecture of the human brain's structural connectome, from the perspective of spectral analysis of Laplacian and adjacency matrices. Building on the idea that the brain strikes a balance between efficient information processing and minimizing wiring costs, we aim to understand the impact of the metric properties of the connectome and how they relate to the existence of an inherent scale. We demonstrate that a simple generative model, combining nonlinear preferential attachment with an exponential penalty for spatial distance between nodes, can effectively reproduce several key characteristics of the human connectome, including spectral density, edge length distribution, eigenmode localization and local clustering properties. We also delve into the finer spectral properties of the human structural connectomes by evaluating the inverse participation ratios ($\\text{IPR}_q$) across various parts of the spectrum. Our analysis reveals that the level statistics in the soft cluster region of the Laplacian spectrum deviate from a purely Poisson distribution due to interactions between clusters. Additionally, we identified scar-like localized modes with large IPR values in the continuum spectrum. We identify multiple fractal eigenmodes distributed across different parts of the spectrum, evaluate their fractal dimensions and find a power-law relationship in the return probability, which is a hallmark of critical behavior. We discuss the conjectures that a brain operates in the Griffiths or multifractal phases.","sentences":["In this study, we explore the fundamental principles behind the architecture of the human brain's structural connectome, from the perspective of spectral analysis of Laplacian and adjacency matrices.","Building on the idea that the brain strikes a balance between efficient information processing and minimizing wiring costs, we aim to understand the impact of the metric properties of the connectome and how they relate to the existence of an inherent scale.","We demonstrate that a simple generative model, combining nonlinear preferential attachment with an exponential penalty for spatial distance between nodes, can effectively reproduce several key characteristics of the human connectome, including spectral density, edge length distribution, eigenmode localization and local clustering properties.","We also delve into the finer spectral properties of the human structural connectomes by evaluating the inverse participation ratios ($\\text{IPR}_q$) across various parts of the spectrum.","Our analysis reveals that the level statistics in the soft cluster region of the Laplacian spectrum deviate from a purely Poisson distribution due to interactions between clusters.","Additionally, we identified scar-like localized modes with large IPR values in the continuum spectrum.","We identify multiple fractal eigenmodes distributed across different parts of the spectrum, evaluate their fractal dimensions and find a power-law relationship in the return probability, which is a hallmark of critical behavior.","We discuss the conjectures that a brain operates in the Griffiths or multifractal phases."],"url":"http://arxiv.org/abs/2405.17349v1","category":"q-bio.NC"}
{"created":"2024-05-27 16:32:33","title":"Moduli spaces in positive geometry","abstract":"These are lecture notes for five lectures given at MPI Leipzig in May 2024. We study the moduli space M_{0,n} of n distinct points on P^1 as a positive geometry and a binary geometry. We develop mathematical formalism to study Cachazo-He-Yuan's scattering equations and the associated scalar and Yang-Mills amplitudes. We discuss open superstring amplitudes and relations to tropical geometry.","sentences":["These are lecture notes for five lectures given at MPI Leipzig in May 2024.","We study the moduli space M_{0,n} of n distinct points on P^1 as a positive geometry and a binary geometry.","We develop mathematical formalism to study Cachazo-He-Yuan's scattering equations and the associated scalar and Yang-Mills amplitudes.","We discuss open superstring amplitudes and relations to tropical geometry."],"url":"http://arxiv.org/abs/2405.17332v1","category":"math.AG"}
{"created":"2024-05-27 16:30:58","title":"Refraction FWI of a circular shot OBN acquisition in the Brazilian pre-salt region","abstract":"We develop a workflow based on full-waveform inversion (FWI) to estimate P-wave velocities in a deepwater Brazilian pre-salt field using the recently introduced circular shot ocean bottom node (OBN) acquisition geometry. Such a geometry comprises a source vessel sailing in large radius concentric circular trajectories and seismic signals are recorded by OBN arrays. The circular shot OBN survey provides mostly refracted waves separately from reflected waves, so the FWI process is mainly driven by diving waves. We introduce a new FWI workflow to analyze non-preprocessed OBN refraction data, which includes automated steps such as data selection solving an Eikonal equation, estimation of a source signature that accounts for ghost and bubble effects, and gradient preconditioning using a non-stationary filter and seismic illumination. We consider two objective functions based on the $L^1$ and $L^2$ norms. The FWI results demonstrated that using our proposed workflow with the $L^1$ norm objective function and the circular OBN survey can lead to an improvement in pre-salt velocity models. Furthermore, using these improved models we construct reverse-time migration (RTM) images of the conventional OBN dataset, showing significant improvements in the salt stratification, the base of salt, and the lateral resolution of the pre-salt area. The Brazilian pre-salt case study demonstrated that the circular shot OBN acquisition maximizes the illumination of deep reservoirs through the ultra-long offset and full-azimuth coverage that prioritizes the recording of diving waves.","sentences":["We develop a workflow based on full-waveform inversion (FWI) to estimate P-wave velocities in a deepwater Brazilian pre-salt field using the recently introduced circular shot ocean bottom node (OBN) acquisition geometry.","Such a geometry comprises a source vessel sailing in large radius concentric circular trajectories and seismic signals are recorded by OBN arrays.","The circular shot OBN survey provides mostly refracted waves separately from reflected waves, so the FWI process is mainly driven by diving waves.","We introduce a new FWI workflow to analyze non-preprocessed OBN refraction data, which includes automated steps such as data selection solving an Eikonal equation, estimation of a source signature that accounts for ghost and bubble effects, and gradient preconditioning using a non-stationary filter and seismic illumination.","We consider two objective functions based on the $L^1$ and $L^2$ norms.","The FWI results demonstrated that using our proposed workflow with the $L^1$ norm objective function and the circular OBN survey can lead to an improvement in pre-salt velocity models.","Furthermore, using these improved models we construct reverse-time migration (RTM) images of the conventional OBN dataset, showing significant improvements in the salt stratification, the base of salt, and the lateral resolution of the pre-salt area.","The Brazilian pre-salt case study demonstrated that the circular shot OBN acquisition maximizes the illumination of deep reservoirs through the ultra-long offset and full-azimuth coverage that prioritizes the recording of diving waves."],"url":"http://arxiv.org/abs/2405.17330v1","category":"physics.geo-ph"}
{"created":"2024-05-27 16:30:11","title":"Clip Body and Tail Separately: High Probability Guarantees for DPSGD with Heavy Tails","abstract":"Differentially Private Stochastic Gradient Descent (DPSGD) is widely utilized to preserve training data privacy in deep learning, which first clips the gradients to a predefined norm and then injects calibrated noise into the training procedure. Existing DPSGD works typically assume the gradients follow sub-Gaussian distributions and design various clipping mechanisms to optimize training performance. However, recent studies have shown that the gradients in deep learning exhibit a heavy-tail phenomenon, that is, the tails of the gradient have infinite variance, which may lead to excessive clipping loss to the gradients with existing DPSGD mechanisms. To address this problem, we propose a novel approach, Discriminative Clipping~(DC)-DPSGD, with two key designs. First, we introduce a subspace identification technique to distinguish between body and tail gradients. Second, we present a discriminative clipping mechanism that applies different clipping thresholds for body and tail gradients to reduce the clipping loss. Under the non-convex condition, \\ourtech{} reduces the empirical gradient norm from {${\\mathbb{O}\\left(\\log^{\\max(0,\\theta-1)}(T/\\delta)\\log^{2\\theta}(\\sqrt{T})\\right)}$} to {${\\mathbb{O}\\left(\\log(\\sqrt{T})\\right)}$} with heavy-tailed index $\\theta\\geq 1/2$, iterations $T$, and arbitrary probability $\\delta$. Extensive experiments on four real-world datasets demonstrate that our approach outperforms three baselines by up to 9.72\\% in terms of accuracy.","sentences":["Differentially Private Stochastic Gradient Descent (DPSGD) is widely utilized to preserve training data privacy in deep learning, which first clips the gradients to a predefined norm and then injects calibrated noise into the training procedure.","Existing DPSGD works typically assume the gradients follow sub-Gaussian distributions and design various clipping mechanisms to optimize training performance.","However, recent studies have shown that the gradients in deep learning exhibit a heavy-tail phenomenon, that is, the tails of the gradient have infinite variance, which may lead to excessive clipping loss to the gradients with existing DPSGD mechanisms.","To address this problem, we propose a novel approach, Discriminative Clipping~(DC)-DPSGD, with two key designs.","First, we introduce a subspace identification technique to distinguish between body and tail gradients.","Second, we present a discriminative clipping mechanism that applies different clipping thresholds for body and tail gradients to reduce the clipping loss.","Under the non-convex condition, \\ourtech{} reduces the empirical gradient norm from {${\\mathbb{O}\\left(\\log^{\\max(0,\\theta-1)}(T/\\delta)\\log^{2\\theta}(\\sqrt{T})\\right)}$} to {${\\mathbb{O}\\left(\\log(\\sqrt{T})\\right)}$} with heavy-tailed index $\\theta\\geq 1/2$, iterations $T$, and arbitrary probability $\\delta$. Extensive experiments on four real-world datasets demonstrate that our approach outperforms three baselines by up to 9.72\\% in terms of accuracy."],"url":"http://arxiv.org/abs/2405.17529v1","category":"cs.LG"}
{"created":"2024-05-27 16:27:57","title":"Magnetic properties of diluted hexaferrites","abstract":"We revisit the magnetic properties of the hexagonal ferrite PbFe$_{12-x}$Ga$_x$O$_{19}$. Recent experiments have reported puzzling dependencies of the ordering temperature and the saturation magnetization on the Ga concentration $x$. To explain these observations, we perform large-scale Monte Carlo simulations, focusing on the effects of an unequal distribution of the Ga impurities over the five distinct Fe sublattices. Ab-initio density-functional calculations predict that the Ga ions preferably occupy the $12k$ sublattice and (to a lesser extent) the $2a$ sublattice. We incorporate this insight into a nonuniform model of the Ga distribution. Monte Carlo simulations using this model lead to an excellent agreement between the theoretical and experimental values of the ordering temperature and saturation magnetization, indicating that the unequal distribution of the Ga impurities is the main reason for the unusual magnetic properties of PbFe$_{12-x}$Ga$_x$O$_{19}$. We also compute the temperature and concentration dependencies of the sublattice magnetizations, and we study the character of the zero-temperature transition that takes place when the ordering temperature is tuned to zero.","sentences":["We revisit the magnetic properties of the hexagonal ferrite PbFe$_{12-x}$Ga$_x$O$_{19}$.","Recent experiments have reported puzzling dependencies of the ordering temperature and the saturation magnetization on the Ga concentration $x$. To explain these observations, we perform large-scale Monte Carlo simulations, focusing on the effects of an unequal distribution of the Ga impurities over the five distinct Fe sublattices.","Ab-initio density-functional calculations predict that the Ga ions preferably occupy the $12k$ sublattice and (to a lesser extent) the $2a$ sublattice.","We incorporate this insight into a nonuniform model of the Ga distribution.","Monte Carlo simulations using this model lead to an excellent agreement between the theoretical and experimental values of the ordering temperature and saturation magnetization, indicating that the unequal distribution of the Ga impurities is the main reason for the unusual magnetic properties of PbFe$_{12-x}$Ga$_x$O$_{19}$. We also compute the temperature and concentration dependencies of the sublattice magnetizations, and we study the character of the zero-temperature transition that takes place when the ordering temperature is tuned to zero."],"url":"http://arxiv.org/abs/2405.17328v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-27 16:26:19","title":"Exploring hidden priors when interpreting gravitational wave and electromagnetic probes of the nuclear equation of state","abstract":"The wide range of sub- and super-nuclear densities achieved in neutron stars makes them ideal probes of dense nuclear behavior in the form of the nuclear equation of state (EoS). Studying neutron stars both in isolation and in highly dynamic events, many recent observations, most famously the gravitational wave and electromagnetic signals associated with the BNS merger GW170817/AT2017gfo, have provided suggestive insight into these highest nuclear densities. Measurements of galactic neutron star masses and radii from NICER and other radio and X-ray measurements provide critical complementary perspectives, bounding other features of the EoS. Though nominally congruent, in this paper we highlight many underappreciated \"hidden\" priors embedded in joint analysis of these many messengers, and their systematic impact on joint EoS inference. In this work, we perform a careful step-by-step Bayesian inference using a simple low-dimensional parametric EoS model, incrementally adding information from galactic pulsars, gravitational wave sources, and more speculative constraints involving kilonovae and the neutron star maximum mass. At each stage, we carefully discuss the marginal likelihood, explaining how hidden priors impact conclusions. Conversely, we also quantify how much information these measurements have, arguing that many if not most have minimal impact relative to the explicit and hidden prior assumptions. Specifically, we find two features dominate our inference: on the one hand, the choice of EoS parameterization and particularly hyperprior, and on the other the astrophysical priors associated with interpreting events, particularly the multimessenger source GW170817. In an appendix, we outline a simple semianalytic projection suitable for assessing the measurability of the EoS with ensembles of present and future detections.","sentences":["The wide range of sub- and super-nuclear densities achieved in neutron stars makes them ideal probes of dense nuclear behavior in the form of the nuclear equation of state (EoS).","Studying neutron stars both in isolation and in highly dynamic events, many recent observations, most famously the gravitational wave and electromagnetic signals associated with the BNS merger GW170817/AT2017gfo, have provided suggestive insight into these highest nuclear densities.","Measurements of galactic neutron star masses and radii from NICER and other radio and X-ray measurements provide critical complementary perspectives, bounding other features of the EoS. Though nominally congruent, in this paper we highlight many underappreciated \"hidden\" priors embedded in joint analysis of these many messengers, and their systematic impact on joint EoS inference.","In this work, we perform a careful step-by-step Bayesian inference using a simple low-dimensional parametric EoS model, incrementally adding information from galactic pulsars, gravitational wave sources, and more speculative constraints involving kilonovae and the neutron star maximum mass.","At each stage, we carefully discuss the marginal likelihood, explaining how hidden priors impact conclusions.","Conversely, we also quantify how much information these measurements have, arguing that many if not most have minimal impact relative to the explicit and hidden prior assumptions.","Specifically, we find two features dominate our inference: on the one hand, the choice of EoS parameterization and particularly hyperprior, and on the other the astrophysical priors associated with interpreting events, particularly the multimessenger source GW170817.","In an appendix, we outline a simple semianalytic projection suitable for assessing the measurability of the EoS with ensembles of present and future detections."],"url":"http://arxiv.org/abs/2405.17326v1","category":"astro-ph.HE"}
{"created":"2024-05-27 16:21:41","title":"Evaluation of computational and energy performance in matrix multiplication algorithms on CPU and GPU using MKL, cuBLAS and SYCL","abstract":"Matrix multiplication is fundamental in the backpropagation algorithm used to train deep neural network models. Libraries like Intel's MKL or NVIDIA's cuBLAS implemented new and optimized matrix multiplication techniques that increase performance and reduce computational costs. These techniques can also be implemented in CUDA and SYCL and functions with AVX2 and AVX512 instructions, which have lower performance but better precision. The study compares execution times and power consumption using PAPI and PERF and compares accuracy for different matrix sizes. Comparisons were made on architectures such as third and fourth-generation Intel CPUs and NVIDIA V100 and A100 GPUs. The MKL library showed the best performance with a slight loss of precision, while OpenMP and SYCL on the CPU implementation showed the best accuracy but a loss of performance. On the other hand, the results on GPU showed that cuBLAS with tensor cores had the best performance; however, it had a cost in accuracy. The cuBLAS library without these specialized cores shows minimal performance loss and much higher accuracy. The data obtained on different architectures showed that the CPU could achieve performance close to that obtained on the GPU with increased power consumption. These results are conditional on certain hardware specifications, such as the number of cores, clock frequency, processor generation for the CPU, and the speed and bandwidth of the PCI bus and device architecture (compute capability) for the GPU.","sentences":["Matrix multiplication is fundamental in the backpropagation algorithm used to train deep neural network models.","Libraries like Intel's MKL or NVIDIA's cuBLAS implemented new and optimized matrix multiplication techniques that increase performance and reduce computational costs.","These techniques can also be implemented in CUDA and SYCL and functions with AVX2 and AVX512 instructions, which have lower performance but better precision.","The study compares execution times and power consumption using PAPI and PERF and compares accuracy for different matrix sizes.","Comparisons were made on architectures such as third and fourth-generation Intel CPUs and NVIDIA V100 and A100 GPUs.","The MKL library showed the best performance with a slight loss of precision, while OpenMP and SYCL on the CPU implementation showed the best accuracy but a loss of performance.","On the other hand, the results on GPU showed that cuBLAS with tensor cores had the best performance; however, it had a cost in accuracy.","The cuBLAS library without these specialized cores shows minimal performance loss and much higher accuracy.","The data obtained on different architectures showed that the CPU could achieve performance close to that obtained on the GPU with increased power consumption.","These results are conditional on certain hardware specifications, such as the number of cores, clock frequency, processor generation for the CPU, and the speed and bandwidth of the PCI bus and device architecture (compute capability) for the GPU."],"url":"http://arxiv.org/abs/2405.17322v1","category":"cs.DC"}
{"created":"2024-05-27 16:20:50","title":"An explicit formula of the parameter dependence of de partial derivatives of the Green's functions related to arbitrary two-point boundary conditions","abstract":"In this paper we obtain an explicit formula of the parameter dependence of the partial derivatives of the Green's functions related to two-point boundary conditions. Such expression follows as an integral of both kernels times the difference of the corresponding parameters of each Green's function. As a direct consequence, we deduce a simpler proof of the monotony of the constant sign of the partial derivative of a Green's function with respect to a real parameter. As a consequence, we improve the results obtained in \\cite{C1}, where the monotone dependence was proved for the constant sign Green's function (not for any ot its partial derivatives) and under weaker assumptions on the Green's function. The arguments are valid for any other types of Ordinary Differential Equations coupled to Nonlocal Conditions. Moreover, analogous ideas could be developed for Partial and Fractional Differential Equations.","sentences":["In this paper we obtain an explicit formula of the parameter dependence of the partial derivatives of the Green's functions related to two-point boundary conditions.","Such expression follows as an integral of both kernels times the difference of the corresponding parameters of each Green's function.","As a direct consequence, we deduce a simpler proof of the monotony of the constant sign of the partial derivative of a Green's function with respect to a real parameter.","As a consequence, we improve the results obtained in \\cite{C1}, where the monotone dependence was proved for the constant sign Green's function (not for any ot its partial derivatives) and under weaker assumptions on the Green's function.","The arguments are valid for any other types of Ordinary Differential Equations coupled to Nonlocal Conditions.","Moreover, analogous ideas could be developed for Partial and Fractional Differential Equations."],"url":"http://arxiv.org/abs/2405.17320v1","category":"math.CA"}
{"created":"2024-05-28 17:57:12","title":"3D StreetUnveiler with Semantic-Aware 2DGS","abstract":"Unveiling an empty street from crowded observations captured by in-car cameras is crucial for autonomous driving. However, removing all temporary static objects, such as stopped vehicles and standing pedestrians, presents a significant challenge. Unlike object-centric 3D inpainting, which relies on thorough observation in a small scene, street scenes involve long trajectories that differ from previous 3D inpainting tasks. The camera-centric moving environment of captured videos further complicates the task due to the limited degree and time duration of object observation. To address these obstacles, we introduce StreetUnveiler to reconstruct an empty street. StreetUnveiler learns a 3D representation of the empty street from crowded observations. Our representation is based on the hard-label semantic 2D Gaussian Splatting (2DGS) for its scalability and ability to identify Gaussians to be removed. We inpaint rendered image after removing unwanted Gaussians to provide pseudo-labels and subsequently re-optimize the 2DGS. Given its temporal continuous movement, we divide the empty street scene into observed, partial-observed, and unobserved regions, which we propose to locate through a rendered alpha map. This decomposition helps us to minimize the regions that need to be inpainted. To enhance the temporal consistency of the inpainting, we introduce a novel time-reversal framework to inpaint frames in reverse order and use later frames as references for earlier frames to fully utilize the long-trajectory observations. Our experiments conducted on the street scene dataset successfully reconstructed a 3D representation of the empty street. The mesh representation of the empty street can be extracted for further applications. Project page and more visualizations can be found at: https://streetunveiler.github.io","sentences":["Unveiling an empty street from crowded observations captured by in-car cameras is crucial for autonomous driving.","However, removing all temporary static objects, such as stopped vehicles and standing pedestrians, presents a significant challenge.","Unlike object-centric 3D inpainting, which relies on thorough observation in a small scene, street scenes involve long trajectories that differ from previous 3D inpainting tasks.","The camera-centric moving environment of captured videos further complicates the task due to the limited degree and time duration of object observation.","To address these obstacles, we introduce StreetUnveiler to reconstruct an empty street.","StreetUnveiler learns a 3D representation of the empty street from crowded observations.","Our representation is based on the hard-label semantic 2D Gaussian Splatting (2DGS) for its scalability and ability to identify Gaussians to be removed.","We inpaint rendered image after removing unwanted Gaussians to provide pseudo-labels and subsequently re-optimize the 2DGS.","Given its temporal continuous movement, we divide the empty street scene into observed, partial-observed, and unobserved regions, which we propose to locate through a rendered alpha map.","This decomposition helps us to minimize the regions that need to be inpainted.","To enhance the temporal consistency of the inpainting, we introduce a novel time-reversal framework to inpaint frames in reverse order and use later frames as references for earlier frames to fully utilize the long-trajectory observations.","Our experiments conducted on the street scene dataset successfully reconstructed a 3D representation of the empty street.","The mesh representation of the empty street can be extracted for further applications.","Project page and more visualizations can be found at: https://streetunveiler.github.io"],"url":"http://arxiv.org/abs/2405.18416v1","category":"cs.CV"}
{"created":"2024-05-28 17:36:11","title":"What can machine learning help with microstructure-informed materials modeling and design?","abstract":"Machine learning techniques have been widely employed as effective tools in addressing various engineering challenges in recent years, particularly for the challenging task of microstructure-informed materials modeling. This work provides a comprehensive review of the current machine learning-assisted and data-driven advancements in this field, including microstructure characterization and reconstruction, multiscale simulation, correlations among process, microstructure, and properties, as well as microstructure optimization and inverse design. It outlines the achievements of existing research through best practices and suggests potential avenues for future investigations. Moreover, it prepares the readers with educative instructions of basic knowledge and an overview on machine learning, microstructure descriptors and machine learning-assisted material modeling, lowering the interdisciplinary hurdles. It should help to stimulate and attract more research attention to the rapidly growing field of machine learning-based modeling and design of microstructured materials.","sentences":["Machine learning techniques have been widely employed as effective tools in addressing various engineering challenges in recent years, particularly for the challenging task of microstructure-informed materials modeling.","This work provides a comprehensive review of the current machine learning-assisted and data-driven advancements in this field, including microstructure characterization and reconstruction, multiscale simulation, correlations among process, microstructure, and properties, as well as microstructure optimization and inverse design.","It outlines the achievements of existing research through best practices and suggests potential avenues for future investigations.","Moreover, it prepares the readers with educative instructions of basic knowledge and an overview on machine learning, microstructure descriptors and machine learning-assisted material modeling, lowering the interdisciplinary hurdles.","It should help to stimulate and attract more research attention to the rapidly growing field of machine learning-based modeling and design of microstructured materials."],"url":"http://arxiv.org/abs/2405.18396v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-28 17:22:15","title":"A Note on the Prediction-Powered Bootstrap","abstract":"We introduce PPBoot: a bootstrap-based method for prediction-powered inference. PPBoot is applicable to arbitrary estimation problems and is very simple to implement, essentially only requiring one application of the bootstrap. Through a series of examples, we demonstrate that PPBoot often performs nearly identically to (and sometimes better than) the earlier PPI(++) method based on asymptotic normality$\\unicode{x2013}$when the latter is applicable$\\unicode{x2013}$without requiring any asymptotic characterizations. Given its versatility, PPBoot could simplify and expand the scope of application of prediction-powered inference to problems where central limit theorems are hard to prove.","sentences":["We introduce PPBoot: a bootstrap-based method for prediction-powered inference.","PPBoot is applicable to arbitrary estimation problems and is very simple to implement, essentially only requiring one application of the bootstrap.","Through a series of examples, we demonstrate that PPBoot often performs nearly identically to (and sometimes better than) the earlier PPI(++) method based on asymptotic normality$\\unicode{x2013}$when the latter is applicable$\\unicode{x2013}$without requiring any asymptotic characterizations.","Given its versatility, PPBoot could simplify and expand the scope of application of prediction-powered inference to problems where central limit theorems are hard to prove."],"url":"http://arxiv.org/abs/2405.18379v1","category":"stat.ML"}
{"created":"2024-05-28 15:24:35","title":"Signal-Plus-Noise Decomposition of Nonlinear Spiked Random Matrix Models","abstract":"In this paper, we study a nonlinear spiked random matrix model where a nonlinear function is applied element-wise to a noise matrix perturbed by a rank-one signal. We establish a signal-plus-noise decomposition for this model and identify precise phase transitions in the structure of the signal components at critical thresholds of signal strength. To demonstrate the applicability of this decomposition, we then utilize it to study new phenomena in the problems of signed signal recovery in nonlinear models and community detection in transformed stochastic block models. Finally, we validate our results through a series of numerical simulations.","sentences":["In this paper, we study a nonlinear spiked random matrix model where a nonlinear function is applied element-wise to a noise matrix perturbed by a rank-one signal.","We establish a signal-plus-noise decomposition for this model and identify precise phase transitions in the structure of the signal components at critical thresholds of signal strength.","To demonstrate the applicability of this decomposition, we then utilize it to study new phenomena in the problems of signed signal recovery in nonlinear models and community detection in transformed stochastic block models.","Finally, we validate our results through a series of numerical simulations."],"url":"http://arxiv.org/abs/2405.18274v1","category":"math.ST"}
{"created":"2024-05-28 15:04:17","title":"Truthful Dataset Valuation by Pointwise Mutual Information","abstract":"A common way to evaluate a dataset in ML involves training a model on this dataset and assessing the model's performance on a test set. However, this approach has two issues: (1) it may incentivize undesirable data manipulation in data marketplaces, as the self-interested data providers seek to modify the dataset to maximize their evaluation scores; (2) it may select datasets that overfit to potentially small test sets. We propose a new data valuation method that provably guarantees the following: data providers always maximize their expected score by truthfully reporting their observed data. Any manipulation of the data, including but not limited to data duplication, adding random data, data removal, or re-weighting data from different groups, cannot increase their expected score. Our method, following the paradigm of proper scoring rules, measures the pointwise mutual information (PMI) of the test dataset and the evaluated dataset. However, computing the PMI of two datasets is challenging. We introduce a novel PMI measuring method that greatly improves tractability within Bayesian machine learning contexts. This is accomplished through a new characterization of PMI that relies solely on the posterior probabilities of the model parameter at an arbitrarily selected value. Finally, we support our theoretical results with simulations and further test the effectiveness of our data valuation method in identifying the top datasets among multiple data providers. Interestingly, our method outperforms the standard approach of selecting datasets based on the trained model's test performance, suggesting that our truthful valuation score can also be more robust to overfitting.","sentences":["A common way to evaluate a dataset in ML involves training a model on this dataset and assessing the model's performance on a test set.","However, this approach has two issues: (1) it may incentivize undesirable data manipulation in data marketplaces, as the self-interested data providers seek to modify the dataset to maximize their evaluation scores; (2) it may select datasets that overfit to potentially small test sets.","We propose a new data valuation method that provably guarantees the following: data providers always maximize their expected score by truthfully reporting their observed data.","Any manipulation of the data, including but not limited to data duplication, adding random data, data removal, or re-weighting data from different groups, cannot increase their expected score.","Our method, following the paradigm of proper scoring rules, measures the pointwise mutual information (PMI) of the test dataset and the evaluated dataset.","However, computing the PMI of two datasets is challenging.","We introduce a novel PMI measuring method that greatly improves tractability within Bayesian machine learning contexts.","This is accomplished through a new characterization of PMI that relies solely on the posterior probabilities of the model parameter at an arbitrarily selected value.","Finally, we support our theoretical results with simulations and further test the effectiveness of our data valuation method in identifying the top datasets among multiple data providers.","Interestingly, our method outperforms the standard approach of selecting datasets based on the trained model's test performance, suggesting that our truthful valuation score can also be more robust to overfitting."],"url":"http://arxiv.org/abs/2405.18253v1","category":"cs.LG"}
{"created":"2024-05-28 13:12:54","title":"Tree Coloring: Random Order and Predictions","abstract":"Coloring is a notoriously hard problem, and even more so in the online setting, where each arriving vertex has to be colored immediately and irrevocably. Already on trees, which are trivially two-colorable, it is impossible to achieve anything better than a logarithmic competitive ratio.   We show how to undercut this bound by a double-logarithmic factor in the slightly relaxed online model where the vertices arrive in random order. We then also analyze algorithms with predictions, showing how well we can color trees with machine-learned advice of varying reliability. We further extend our analysis to all two-colorable graphs and provide matching lower bounds in both cases. Finally, we demonstrate how the two mentioned approaches, both of which diminish the often unjustified pessimism of the classical online model, can be combined to yield even better results.","sentences":["Coloring is a notoriously hard problem, and even more so in the online setting, where each arriving vertex has to be colored immediately and irrevocably.","Already on trees, which are trivially two-colorable, it is impossible to achieve anything better than a logarithmic competitive ratio.   ","We show how to undercut this bound by a double-logarithmic factor in the slightly relaxed online model where the vertices arrive in random order.","We then also analyze algorithms with predictions, showing how well we can color trees with machine-learned advice of varying reliability.","We further extend our analysis to all two-colorable graphs and provide matching lower bounds in both cases.","Finally, we demonstrate how the two mentioned approaches, both of which diminish the often unjustified pessimism of the classical online model, can be combined to yield even better results."],"url":"http://arxiv.org/abs/2405.18151v1","category":"cs.DS"}
{"created":"2024-05-28 11:59:44","title":"Pipette: Automatic Fine-grained Large Language Model Training Configurator for Real-World Clusters","abstract":"Training large language models (LLMs) is known to be challenging because of the huge computational and memory capacity requirements. To address these issues, it is common to use a cluster of GPUs with 3D parallelism, which splits a model along the data batch, pipeline stage, and intra-layer tensor dimensions. However, the use of 3D parallelism produces the additional challenge of finding the optimal number of ways on each dimension and mapping the split models onto the GPUs. Several previous studies have attempted to automatically find the optimal configuration, but many of these lacked several important aspects. For instance, the heterogeneous nature of the interconnect speeds is often ignored. While the peak bandwidths for the interconnects are usually made equal, the actual attained bandwidth varies per link in real-world clusters. Combined with the critical path modeling that does not properly consider the communication, they easily fall into sub-optimal configurations. In addition, they often fail to consider the memory requirement per GPU, often recommending solutions that could not be executed. To address these challenges, we propose Pipette, which is an automatic fine-grained LLM training configurator for real-world clusters. By devising better performance models along with the memory estimator and fine-grained individual GPU assignment, Pipette achieves faster configurations that satisfy the memory constraints. We evaluated Pipette on large clusters to show that it provides a significant speedup over the prior art. The implementation of Pipette is available at https://github.com/yimjinkyu1/date2024_pipette.","sentences":["Training large language models (LLMs) is known to be challenging because of the huge computational and memory capacity requirements.","To address these issues, it is common to use a cluster of GPUs with 3D parallelism, which splits a model along the data batch, pipeline stage, and intra-layer tensor dimensions.","However, the use of 3D parallelism produces the additional challenge of finding the optimal number of ways on each dimension and mapping the split models onto the GPUs.","Several previous studies have attempted to automatically find the optimal configuration, but many of these lacked several important aspects.","For instance, the heterogeneous nature of the interconnect speeds is often ignored.","While the peak bandwidths for the interconnects are usually made equal, the actual attained bandwidth varies per link in real-world clusters.","Combined with the critical path modeling that does not properly consider the communication, they easily fall into sub-optimal configurations.","In addition, they often fail to consider the memory requirement per GPU, often recommending solutions that could not be executed.","To address these challenges, we propose Pipette, which is an automatic fine-grained LLM training configurator for real-world clusters.","By devising better performance models along with the memory estimator and fine-grained individual GPU assignment, Pipette achieves faster configurations that satisfy the memory constraints.","We evaluated Pipette on large clusters to show that it provides a significant speedup over the prior art.","The implementation of Pipette is available at https://github.com/yimjinkyu1/date2024_pipette."],"url":"http://arxiv.org/abs/2405.18093v1","category":"cs.DC"}
{"created":"2024-05-28 11:45:30","title":"Guidance and Control Networks with Periodic Activation Functions","abstract":"Inspired by the versatility of sinusoidal representation networks (SIRENs), we present a modified Guidance & Control Networks (G&CNETs) variant using periodic activation functions in the hidden layers. We demonstrate that the resulting G&CNETs train faster and achieve a lower overall training error on three different control scenarios on which G&CNETs have been tested previously. A preliminary analysis is presented in an attempt to explain the superior performance of the SIREN architecture for the particular types of tasks that G&CNETs excel on.","sentences":["Inspired by the versatility of sinusoidal representation networks (SIRENs), we present a modified Guidance & Control Networks (G&CNETs) variant using periodic activation functions in the hidden layers.","We demonstrate that the resulting G&CNETs train faster and achieve a lower overall training error on three different control scenarios on which G&CNETs have been tested previously.","A preliminary analysis is presented in an attempt to explain the superior performance of the SIREN architecture for the particular types of tasks that G&CNETs excel on."],"url":"http://arxiv.org/abs/2405.18084v1","category":"cs.LG"}
{"created":"2024-05-28 10:07:50","title":"The association between environmental variables and short-term mortality: evidence from Europe","abstract":"Using fine-grained, publicly available data, this paper studies the association between environmental factors, i.e., variables capturing weather and air pollution characteristics, and weekly mortality rates in small geographical regions in Europe. Hereto, we develop a mortality modelling framework where a baseline captures a region-specific, seasonal historical trend observed within the weekly mortality rates. Using a machine learning algorithm, we then explain deviations from this baseline using anomalies and extreme indices constructed from the environmental data. We illustrate our proposed modelling framework through a case study on more than 550 NUTS 3 regions (Nomenclature of Territorial Units for Statistics, level 3) located in 20 different European countries. Through interpretation tools, we unravel insights into which environmental features are most important when estimating excess or deficit mortality with respect to the baseline and explore how these features interact. Moreover, we investigate harvesting effects of the environmental features through our constructed weekly mortality modelling framework. Our findings show that temperature-related features exert the most significant influence in explaining deviations in mortality from the baseline. Furthermore, we find that environmental features prove particularly beneficial in southern regions for explaining elevated levels of mortality over short time periods.","sentences":["Using fine-grained, publicly available data, this paper studies the association between environmental factors, i.e., variables capturing weather and air pollution characteristics, and weekly mortality rates in small geographical regions in Europe.","Hereto, we develop a mortality modelling framework where a baseline captures a region-specific, seasonal historical trend observed within the weekly mortality rates.","Using a machine learning algorithm, we then explain deviations from this baseline using anomalies and extreme indices constructed from the environmental data.","We illustrate our proposed modelling framework through a case study on more than 550 NUTS 3 regions (Nomenclature of Territorial Units for Statistics, level 3) located in 20 different European countries.","Through interpretation tools, we unravel insights into which environmental features are most important when estimating excess or deficit mortality with respect to the baseline and explore how these features interact.","Moreover, we investigate harvesting effects of the environmental features through our constructed weekly mortality modelling framework.","Our findings show that temperature-related features exert the most significant influence in explaining deviations in mortality from the baseline.","Furthermore, we find that environmental features prove particularly beneficial in southern regions for explaining elevated levels of mortality over short time periods."],"url":"http://arxiv.org/abs/2405.18020v1","category":"stat.AP"}
{"created":"2024-05-28 09:53:47","title":"Flow-Assisted Motion Learning Network for Weakly-Supervised Group Activity Recognition","abstract":"Weakly-Supervised Group Activity Recognition (WSGAR) aims to understand the activity performed together by a group of individuals with the video-level label and without actor-level labels. We propose Flow-Assisted Motion Learning Network (Flaming-Net) for WSGAR, which consists of the motion-aware actor encoder to extract actor features and the two-pathways relation module to infer the interaction among actors and their activity. Flaming-Net leverages an additional optical flow modality in the training stage to enhance its motion awareness when finding locally active actors. The first pathway of the relation module, the actor-centric path, initially captures the temporal dynamics of individual actors and then constructs inter-actor relationships. In parallel, the group-centric path starts by building spatial connections between actors within the same timeframe and then captures simultaneous spatio-temporal dynamics among them. We demonstrate that Flaming-Net achieves new state-of-the-art WSGAR results on two benchmarks, including a 2.8%p higher MPCA score on the NBA dataset. Importantly, we use the optical flow modality only for training and not for inference.","sentences":["Weakly-Supervised Group Activity Recognition (WSGAR) aims to understand the activity performed together by a group of individuals with the video-level label and without actor-level labels.","We propose Flow-Assisted Motion Learning Network (Flaming-Net) for WSGAR, which consists of the motion-aware actor encoder to extract actor features and the two-pathways relation module to infer the interaction among actors and their activity.","Flaming-Net leverages an additional optical flow modality in the training stage to enhance its motion awareness when finding locally active actors.","The first pathway of the relation module, the actor-centric path, initially captures the temporal dynamics of individual actors and then constructs inter-actor relationships.","In parallel, the group-centric path starts by building spatial connections between actors within the same timeframe and then captures simultaneous spatio-temporal dynamics among them.","We demonstrate that Flaming-Net achieves new state-of-the-art WSGAR results on two benchmarks, including a 2.8%p higher MPCA score on the NBA dataset.","Importantly, we use the optical flow modality only for training and not for inference."],"url":"http://arxiv.org/abs/2405.18012v1","category":"cs.CV"}
{"created":"2024-05-28 09:50:46","title":"Exploring Context Window of Large Language Models via Decomposed Positional Vectors","abstract":"Transformer-based large language models (LLMs) typically have a limited context window, resulting in significant performance degradation when processing text beyond the length of the context window. Extensive studies have been proposed to extend the context window and achieve length extrapolation of LLMs, but there is still a lack of in-depth interpretation of these approaches. In this study, we explore the positional information within and beyond the context window for deciphering the underlying mechanism of LLMs. By using a mean-based decomposition method, we disentangle positional vectors from hidden states of LLMs and analyze their formation and effect on attention. Furthermore, when texts exceed the context window, we analyze the change of positional vectors in two settings, i.e., direct extrapolation and context window extension. Based on our findings, we design two training-free context window extension methods, positional vector replacement and attention window extension. Experimental results show that our methods can effectively extend the context window length.","sentences":["Transformer-based large language models (LLMs) typically have a limited context window, resulting in significant performance degradation when processing text beyond the length of the context window.","Extensive studies have been proposed to extend the context window and achieve length extrapolation of LLMs, but there is still a lack of in-depth interpretation of these approaches.","In this study, we explore the positional information within and beyond the context window for deciphering the underlying mechanism of LLMs.","By using a mean-based decomposition method, we disentangle positional vectors from hidden states of LLMs and analyze their formation and effect on attention.","Furthermore, when texts exceed the context window, we analyze the change of positional vectors in two settings, i.e., direct extrapolation and context window extension.","Based on our findings, we design two training-free context window extension methods, positional vector replacement and attention window extension.","Experimental results show that our methods can effectively extend the context window length."],"url":"http://arxiv.org/abs/2405.18009v1","category":"cs.CL"}
{"created":"2024-05-28 08:50:14","title":"AttenCraft: Attention-guided Disentanglement of Multiple Concepts for Text-to-Image Customization","abstract":"With the unprecedented performance being achieved by text-to-image (T2I) diffusion models, T2I customization further empowers users to tailor the diffusion model to new concepts absent in the pre-training dataset, termed subject-driven generation. Moreover, extracting several new concepts from a single image enables the model to learn multiple concepts, and simultaneously decreases the difficulties of training data preparation, urging the disentanglement of multiple concepts to be a new challenge. However, existing models for disentanglement commonly require pre-determined masks or retain background elements. To this end, we propose an attention-guided method, AttenCraft, for multiple concept disentanglement. In particular, our method leverages self-attention and cross-attention maps to create accurate masks for each concept within a single initialization step, omitting any required mask preparation by humans or other models. The created masks are then applied to guide the cross-attention activation of each target concept during training and achieve concept disentanglement. Additionally, we introduce Uniform sampling and Reweighted sampling schemes to alleviate the non-synchronicity of feature acquisition from different concepts, and improve generation quality. Our method outperforms baseline models in terms of image-alignment, and behaves comparably on text-alignment. Finally, we showcase the applicability of AttenCraft to more complicated settings, such as an input image containing three concepts. The project is available at https://github.com/junjie-shentu/AttenCraft.","sentences":["With the unprecedented performance being achieved by text-to-image (T2I) diffusion models, T2I customization further empowers users to tailor the diffusion model to new concepts absent in the pre-training dataset, termed subject-driven generation.","Moreover, extracting several new concepts from a single image enables the model to learn multiple concepts, and simultaneously decreases the difficulties of training data preparation, urging the disentanglement of multiple concepts to be a new challenge.","However, existing models for disentanglement commonly require pre-determined masks or retain background elements.","To this end, we propose an attention-guided method, AttenCraft, for multiple concept disentanglement.","In particular, our method leverages self-attention and cross-attention maps to create accurate masks for each concept within a single initialization step, omitting any required mask preparation by humans or other models.","The created masks are then applied to guide the cross-attention activation of each target concept during training and achieve concept disentanglement.","Additionally, we introduce Uniform sampling and Reweighted sampling schemes to alleviate the non-synchronicity of feature acquisition from different concepts, and improve generation quality.","Our method outperforms baseline models in terms of image-alignment, and behaves comparably on text-alignment.","Finally, we showcase the applicability of AttenCraft to more complicated settings, such as an input image containing three concepts.","The project is available at https://github.com/junjie-shentu/AttenCraft."],"url":"http://arxiv.org/abs/2405.17965v1","category":"cs.CV"}
{"created":"2024-05-28 08:48:08","title":"Transformer and Hybrid Deep Learning Based Models for Machine-Generated Text Detection","abstract":"This paper describes the approach of the UniBuc - NLP team in tackling the SemEval 2024 Task 8: Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text Detection. We explored transformer-based and hybrid deep learning architectures. For subtask B, our transformer-based model achieved a strong \\textbf{second-place} out of $77$ teams with an accuracy of \\textbf{86.95\\%}, demonstrating the architecture's suitability for this task. However, our models showed overfitting in subtask A which could potentially be fixed with less fine-tunning and increasing maximum sequence length. For subtask C (token-level classification), our hybrid model overfit during training, hindering its ability to detect transitions between human and machine-generated text.","sentences":["This paper describes the approach of the UniBuc - NLP team in tackling the SemEval 2024 Task 8: Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text Detection.","We explored transformer-based and hybrid deep learning architectures.","For subtask B, our transformer-based model achieved a strong \\textbf{second-place} out of $77$ teams with an accuracy of \\textbf{86.95\\%}, demonstrating the architecture's suitability for this task.","However, our models showed overfitting in subtask A which could potentially be fixed with less fine-tunning and increasing maximum sequence length.","For subtask C (token-level classification), our hybrid model overfit during training, hindering its ability to detect transitions between human and machine-generated text."],"url":"http://arxiv.org/abs/2405.17964v1","category":"cs.CL"}
{"created":"2024-05-28 08:02:42","title":"RC-Mixup: A Data Augmentation Strategy against Noisy Data for Regression Tasks","abstract":"We study the problem of robust data augmentation for regression tasks in the presence of noisy data. Data augmentation is essential for generalizing deep learning models, but most of the techniques like the popular Mixup are primarily designed for classification tasks on image data. Recently, there are also Mixup techniques that are specialized to regression tasks like C-Mixup. In comparison to Mixup, which takes linear interpolations of pairs of samples, C-Mixup is more selective in which samples to mix based on their label distances for better regression performance. However, C-Mixup does not distinguish noisy versus clean samples, which can be problematic when mixing and lead to suboptimal model performance. At the same time, robust training has been heavily studied where the goal is to train accurate models against noisy data through multiple rounds of model training. We thus propose our data augmentation strategy RC-Mixup, which tightly integrates C-Mixup with multi-round robust training methods for a synergistic effect. In particular, C-Mixup improves robust training in identifying clean data, while robust training provides cleaner data to C-Mixup for it to perform better. A key advantage of RC-Mixup is that it is data-centric where the robust model training algorithm itself does not need to be modified, but can simply benefit from data mixing. We show in our experiments that RC-Mixup significantly outperforms C-Mixup and robust training baselines on noisy data benchmarks and can be integrated with various robust training methods.","sentences":["We study the problem of robust data augmentation for regression tasks in the presence of noisy data.","Data augmentation is essential for generalizing deep learning models, but most of the techniques like the popular Mixup are primarily designed for classification tasks on image data.","Recently, there are also Mixup techniques that are specialized to regression tasks like C-Mixup.","In comparison to Mixup, which takes linear interpolations of pairs of samples, C-Mixup is more selective in which samples to mix based on their label distances for better regression performance.","However, C-Mixup does not distinguish noisy versus clean samples, which can be problematic when mixing and lead to suboptimal model performance.","At the same time, robust training has been heavily studied where the goal is to train accurate models against noisy data through multiple rounds of model training.","We thus propose our data augmentation strategy RC-Mixup, which tightly integrates C-Mixup with multi-round robust training methods for a synergistic effect.","In particular, C-Mixup improves robust training in identifying clean data, while robust training provides cleaner data to C-Mixup for it to perform better.","A key advantage of RC-Mixup is that it is data-centric where the robust model training algorithm itself does not need to be modified, but can simply benefit from data mixing.","We show in our experiments that RC-Mixup significantly outperforms C-Mixup and robust training baselines on noisy data benchmarks and can be integrated with various robust training methods."],"url":"http://arxiv.org/abs/2405.17938v1","category":"cs.LG"}
{"created":"2024-05-28 07:53:40","title":"Online Merging Optimizers for Boosting Rewards and Mitigating Tax in Alignment","abstract":"Effectively aligning Large Language Models (LLMs) with human-centric values while preventing the degradation of abilities acquired through Pre-training and Supervised Fine-tuning (SFT) poses a central challenge in Reinforcement Learning from Human Feedback (RLHF). In this paper, we first discover that interpolating RLHF and SFT model parameters can adjust the trade-off between human preference and basic capabilities, thereby reducing the alignment tax at the cost of alignment reward. Inspired by this, we propose integrating the RL policy and SFT models at each optimization step in RLHF to continuously regulate the training direction, introducing the Online Merging Optimizer. Specifically, we merge gradients with the parameter differences between SFT and pretrained models, effectively steering the gradient towards maximizing rewards in the direction of SFT optimization. We demonstrate that our optimizer works well with different LLM families, such as Qwen and LLaMA, across various model sizes ranging from 1.8B to 8B, various RLHF algorithms like DPO and KTO, and existing model merging methods. It significantly enhances alignment reward while mitigating alignment tax, achieving higher overall performance across 14 benchmarks.","sentences":["Effectively aligning Large Language Models (LLMs) with human-centric values while preventing the degradation of abilities acquired through Pre-training and Supervised Fine-tuning (SFT) poses a central challenge in Reinforcement Learning from Human Feedback (RLHF).","In this paper, we first discover that interpolating RLHF and SFT model parameters can adjust the trade-off between human preference and basic capabilities, thereby reducing the alignment tax at the cost of alignment reward.","Inspired by this, we propose integrating the RL policy and SFT models at each optimization step in RLHF to continuously regulate the training direction, introducing the Online Merging Optimizer.","Specifically, we merge gradients with the parameter differences between SFT and pretrained models, effectively steering the gradient towards maximizing rewards in the direction of SFT optimization.","We demonstrate that our optimizer works well with different LLM families, such as Qwen and LLaMA, across various model sizes ranging from 1.8B to 8B, various RLHF algorithms like DPO and KTO, and existing model merging methods.","It significantly enhances alignment reward while mitigating alignment tax, achieving higher overall performance across 14 benchmarks."],"url":"http://arxiv.org/abs/2405.17931v1","category":"cs.CL"}
{"created":"2024-05-28 07:42:42","title":"Fisher's Legacy of Directional Statistics, and Beyond to Statistics on Manifolds","abstract":"It will not be an exaggeration to say that R A Fisher is the Albert Einstein of Statistics. He pioneered almost all the main branches of statistics, but it is not as well known that he opened the area of Directional Statistics with his 1953 paper introducing a distribution on the sphere which is now known as the Fisher distribution. He stressed that for spherical data one should take into account that the data is on a manifold. We will describe this Fisher distribution and reanalyse his geological data. We also comment on the two goals he set himself in that paper, and how he reinvented the von Mises distribution on the circle. Since then, many extensions of this distribution have appeared bearing Fisher's name such as the von Mises Fisher distribution and the matrix Fisher distribution. In fact, the subject of Directional Statistics has grown tremendously in the last two decades with new applications emerging in Life Sciences, Image Analysis, Machine Learning and so on. We give a recent new method of constructing the Fisher type distribution which has been motivated by some problems in Machine Learning. The subject related to his distribution has evolved since then more broadly as Statistics on Manifolds which also includes the new field of Shape Analysis. We end with a historical note pointing out some correspondence between D'Arcy Thompson and R A Fisher related to Shape Analysis.","sentences":["It will not be an exaggeration to say that R A Fisher is the Albert Einstein of Statistics.","He pioneered almost all the main branches of statistics, but it is not as well known that he opened the area of Directional Statistics with his 1953 paper introducing a distribution on the sphere which is now known as the Fisher distribution.","He stressed that for spherical data one should take into account that the data is on a manifold.","We will describe this Fisher distribution and reanalyse his geological data.","We also comment on the two goals he set himself in that paper, and how he reinvented the von Mises distribution on the circle.","Since then, many extensions of this distribution have appeared bearing Fisher's name such as the von Mises Fisher distribution and the matrix Fisher distribution.","In fact, the subject of Directional Statistics has grown tremendously in the last two decades with new applications emerging in Life Sciences, Image Analysis, Machine Learning and so on.","We give a recent new method of constructing the Fisher type distribution which has been motivated by some problems in Machine Learning.","The subject related to his distribution has evolved since then more broadly as Statistics on Manifolds which also includes the new field of Shape Analysis.","We end with a historical note pointing out some correspondence between D'Arcy Thompson and R A Fisher related to Shape Analysis."],"url":"http://arxiv.org/abs/2405.17919v1","category":"stat.ME"}
{"created":"2024-05-28 07:11:30","title":"Improving Discrete Diffusion Models via Structured Preferential Generation","abstract":"In the domains of image and audio, diffusion models have shown impressive performance. However, their application to discrete data types, such as language, has often been suboptimal compared to autoregressive generative models. This paper tackles the challenge of improving discrete diffusion models by introducing a structured forward process that leverages the inherent information hierarchy in discrete categories, such as words in text. Our approach biases the generative process to produce certain categories before others, resulting in a notable improvement in log-likelihood scores on the text8 dataset. This work paves the way for more advances in discrete diffusion models with potentially significant enhancements in performance.","sentences":["In the domains of image and audio, diffusion models have shown impressive performance.","However, their application to discrete data types, such as language, has often been suboptimal compared to autoregressive generative models.","This paper tackles the challenge of improving discrete diffusion models by introducing a structured forward process that leverages the inherent information hierarchy in discrete categories, such as words in text.","Our approach biases the generative process to produce certain categories before others, resulting in a notable improvement in log-likelihood scores on the text8 dataset.","This work paves the way for more advances in discrete diffusion models with potentially significant enhancements in performance."],"url":"http://arxiv.org/abs/2405.17889v1","category":"cs.LG"}
{"created":"2024-05-28 06:52:19","title":"Decentralized Directed Collaboration for Personalized Federated Learning","abstract":"Personalized Federated Learning (PFL) is proposed to find the greatest personalized models for each client. To avoid the central failure and communication bottleneck in the server-based FL, we concentrate on the Decentralized Personalized Federated Learning (DPFL) that performs distributed model training in a Peer-to-Peer (P2P) manner. Most personalized works in DPFL are based on undirected and symmetric topologies, however, the data, computation and communication resources heterogeneity result in large variances in the personalized models, which lead the undirected aggregation to suboptimal personalized performance and unguaranteed convergence. To address these issues, we propose a directed collaboration DPFL framework by incorporating stochastic gradient push and partial model personalized, called \\textbf{D}ecentralized \\textbf{Fed}erated \\textbf{P}artial \\textbf{G}radient \\textbf{P}ush (\\textbf{DFedPGP}). It personalizes the linear classifier in the modern deep model to customize the local solution and learns a consensus representation in a fully decentralized manner. Clients only share gradients with a subset of neighbors based on the directed and asymmetric topologies, which guarantees flexible choices for resource efficiency and better convergence. Theoretically, we show that the proposed DFedPGP achieves a superior convergence rate of $\\mathcal{O}(\\frac{1}{\\sqrt{T}})$ in the general non-convex setting, and prove the tighter connectivity among clients will speed up the convergence. The proposed method achieves state-of-the-art (SOTA) accuracy in both data and computation heterogeneity scenarios, demonstrating the efficiency of the directed collaboration and partial gradient push.","sentences":["Personalized Federated Learning (PFL) is proposed to find the greatest personalized models for each client.","To avoid the central failure and communication bottleneck in the server-based FL, we concentrate on the Decentralized Personalized Federated Learning (DPFL) that performs distributed model training in a Peer-to-Peer (P2P) manner.","Most personalized works in DPFL are based on undirected and symmetric topologies, however, the data, computation and communication resources heterogeneity result in large variances in the personalized models, which lead the undirected aggregation to suboptimal personalized performance and unguaranteed convergence.","To address these issues, we propose a directed collaboration DPFL framework by incorporating stochastic gradient push and partial model personalized, called \\textbf{D}ecentralized \\textbf{Fed}erated \\textbf{P}artial \\textbf{G}radient \\textbf{P}ush (\\textbf{DFedPGP}).","It personalizes the linear classifier in the modern deep model to customize the local solution and learns a consensus representation in a fully decentralized manner.","Clients only share gradients with a subset of neighbors based on the directed and asymmetric topologies, which guarantees flexible choices for resource efficiency and better convergence.","Theoretically, we show that the proposed DFedPGP achieves a superior convergence rate of $\\mathcal{O}(\\frac{1}{\\sqrt{T}})$ in the general non-convex setting, and prove the tighter connectivity among clients will speed up the convergence.","The proposed method achieves state-of-the-art (SOTA) accuracy in both data and computation heterogeneity scenarios, demonstrating the efficiency of the directed collaboration and partial gradient push."],"url":"http://arxiv.org/abs/2405.17876v1","category":"cs.LG"}
{"created":"2024-05-28 05:33:13","title":"Benchmark Underestimates the Readiness of Multi-lingual Dialogue Agents","abstract":"Creating multilingual task-oriented dialogue (TOD) agents is challenging due to the high cost of training data acquisition. Following the research trend of improving training data efficiency, we show for the first time, that in-context learning is sufficient to tackle multilingual TOD.   To handle the challenging dialogue state tracking (DST) subtask, we break it down to simpler steps that are more compatible with in-context learning where only a handful of few-shot examples are used. We test our approach on the multilingual TOD dataset X-RiSAWOZ, which has 12 domains in Chinese, English, French, Korean, Hindi, and code-mixed Hindi-English. Our turn-by-turn DST accuracy on the 6 languages range from 55.6% to 80.3%, seemingly worse than the SOTA results from fine-tuned models that achieve from 60.7% to 82.8%; our BLEU scores in the response generation (RG) subtask are also significantly lower than SOTA.   However, after manual evaluation of the validation set, we find that by correcting gold label errors and improving dataset annotation schema, GPT-4 with our prompts can achieve (1) 89.6%-96.8% accuracy in DST, and (2) more than 99% correct response generation across different languages. This leads us to conclude that current automatic metrics heavily underestimate the effectiveness of in-context learning.","sentences":["Creating multilingual task-oriented dialogue (TOD) agents is challenging due to the high cost of training data acquisition.","Following the research trend of improving training data efficiency, we show for the first time, that in-context learning is sufficient to tackle multilingual TOD.   ","To handle the challenging dialogue state tracking (DST) subtask, we break it down to simpler steps that are more compatible with in-context learning where only a handful of few-shot examples are used.","We test our approach on the multilingual TOD dataset X-RiSAWOZ, which has 12 domains in Chinese, English, French, Korean, Hindi, and code-mixed Hindi-English.","Our turn-by-turn DST accuracy on the 6 languages range from 55.6% to 80.3%, seemingly worse than the SOTA results from fine-tuned models that achieve from 60.7% to 82.8%; our BLEU scores in the response generation (RG) subtask are also significantly lower than SOTA.   ","However, after manual evaluation of the validation set, we find that by correcting gold label errors and improving dataset annotation schema, GPT-4 with our prompts can achieve (1) 89.6%-96.8% accuracy in DST, and (2) more than 99% correct response generation across different languages.","This leads us to conclude that current automatic metrics heavily underestimate the effectiveness of in-context learning."],"url":"http://arxiv.org/abs/2405.17840v1","category":"cs.CL"}
{"created":"2024-05-28 17:59:42","title":"Feasibility of Privacy-Preserving Entity Resolution on Confidential Healthcare Datasets Using Homomorphic Encryption","abstract":"Patient datasets contain confidential information which is protected by laws and regulations such as HIPAA and GDPR. Ensuring comprehensive patient information necessitates privacy-preserving entity resolution (PPER), which identifies identical patient entities across multiple databases from different healthcare organizations while maintaining data privacy. Existing methods often lack cryptographic security or are computationally impractical for real-world datasets. We introduce a PPER pipeline based on AMPPERE, a secure abstract computation model utilizing cryptographic tools like homomorphic encryption. Our tailored approach incorporates extensive parallelization techniques and optimal parameters specifically for patient datasets. Experimental results demonstrate the proposed method's effectiveness in terms of accuracy and efficiency compared to various baselines.","sentences":["Patient datasets contain confidential information which is protected by laws and regulations such as HIPAA and GDPR.","Ensuring comprehensive patient information necessitates privacy-preserving entity resolution (PPER), which identifies identical patient entities across multiple databases from different healthcare organizations while maintaining data privacy.","Existing methods often lack cryptographic security or are computationally impractical for real-world datasets.","We introduce a PPER pipeline based on AMPPERE, a secure abstract computation model utilizing cryptographic tools like homomorphic encryption.","Our tailored approach incorporates extensive parallelization techniques and optimal parameters specifically for patient datasets.","Experimental results demonstrate the proposed method's effectiveness in terms of accuracy and efficiency compared to various baselines."],"url":"http://arxiv.org/abs/2405.18430v1","category":"cs.CE"}
{"created":"2024-05-28 17:44:48","title":"Batch VUV4 Characterization for the SBC-LAr10 scintillating bubble chamber","abstract":"The Scintillating Bubble Chamber (SBC) collaboration purchased 32 Hamamatsu VUV4 silicon photomultipliers (SiPMs) for use in SBC-LAr10, a bubble chamber containing 10~kg of liquid argon. The VUV4 SiPMs, or Quads, underwent a characterization at two temperatures which measured the breakdown voltage ($V_{\\text{BD}}$), the SiPM gain ($g_{\\text{SiPM}}$), the rate of change of $g_{\\text{SiPM}}$ with respect to voltage ($m$), the dark count rate (DCR), and the probability of a correlated avalanche (P$_{\\text{CA}}$) as well as the temperature coefficients of these parameters. A Peltier-based chilled vacuum chamber was developed at Queen's University to cool down the Quads to $233.15\\pm0.2$~K and $255.15\\pm0.2$~K with average stability of $\\pm20$~mK. A mostly assumption-free analysis was derived to estimate $V_{\\text{BD}}$ to tens of mV precision and DCR close to Poissonian error. The temperature dependence of $V_{\\text{BD}}$ was found to be $56\\pm2$~mV~K$^{-1}$, and $m$ on average across all Quads was found to be $(459\\pm3(\\rm{stat.})\\pm23(\\rm{sys.}))\\times 10^{3}~e^-$~PE$^{-1}$~V$^{-1}$. The average DCR temperature coefficient was estimated to be $0.099\\pm0.008$~K$^{-1}$ corresponding to a reduction factor of 7 for every 20~K drop in temperature. The average temperature dependence of P$_{\\text{CA}}$ was estimated to be $4000\\pm1000$~ppm~K$^{-1}$. P$_{\\text{CA}}$ estimated from the average across all SiPMs is a better estimator than the P$_{\\text{CA}}$ calculated from individual SiPMs, whereas all of the other parameters, the opposite is true. All the estimated parameters were measured to the precision required for SBC-LAr10, and the Quads will be used in conditions to optimize the signal-to-noise ratio.","sentences":["The Scintillating Bubble Chamber (SBC) collaboration purchased 32 Hamamatsu VUV4 silicon photomultipliers (SiPMs) for use in SBC-LAr10, a bubble chamber containing 10~kg of liquid argon.","The VUV4 SiPMs, or Quads, underwent a characterization at two temperatures which measured the breakdown voltage ($V_{\\text{BD}}$), the SiPM gain ($g_{\\text{SiPM}}$), the rate of change of $g_{\\text{SiPM}}$ with respect to voltage ($m$), the dark count rate (DCR), and the probability of a correlated avalanche (P$_{\\text{CA}}$) as well as the temperature coefficients of these parameters.","A Peltier-based chilled vacuum chamber was developed at Queen's University to cool down the Quads to $233.15\\pm0.2$~K and $255.15\\pm0.2$~K with average stability of $\\pm20$~mK. A mostly assumption-free analysis was derived to estimate $V_{\\text{BD}}$ to tens of mV precision and DCR close to Poissonian error.","The temperature dependence of $V_{\\text{BD}}$ was found to be $56\\pm2$~mV~K$^{-1}$, and $m$ on average across all Quads was found to be $(459\\pm3(\\rm{stat.})\\pm23(\\rm{sys.}))\\times 10^{3}~e^-$~PE$^{-1}$~V$^{-1}$. The average DCR temperature coefficient was estimated to be $0.099\\pm0.008$~K$^{-1}$ corresponding to a reduction factor of 7 for every 20~K drop in temperature.","The average temperature dependence of P$_{\\text{CA}}$ was estimated to be $4000\\pm1000$~ppm~K$^{-1}$. P$_{\\text{CA}}$ estimated from the average across all SiPMs is a better estimator than the P$_{\\text{CA}}$ calculated from individual SiPMs, whereas all of the other parameters, the opposite is true.","All the estimated parameters were measured to the precision required for SBC-LAr10, and the Quads will be used in conditions to optimize the signal-to-noise ratio."],"url":"http://arxiv.org/abs/2405.18403v1","category":"physics.ins-det"}
{"created":"2024-05-28 16:15:26","title":"Optimal Design in Repeated Testing for Count Data","abstract":"In this paper, we develop optimal designs for growth curve models with count data based on the Rasch Poisson-Gamma counts (RPGCM) model. This model is often used in educational and psychological testing when test results yield count data. In the RPGCM, the test scores are determined by respondents ability and item difficulty. Locally D-optimal designs are derived for maximum quasi-likelihood estimation to efficiently estimate the mean abilities of the respondents over time. Using the log link, both unstructured, linear and nonlinear growth curves of log mean abilities are taken into account. Finally, the sensitivity of the derived optimal designs due to an imprecise choice of parameter values is analyzed using D-efficiency.","sentences":["In this paper, we develop optimal designs for growth curve models with count data based on the Rasch Poisson-Gamma counts (RPGCM) model.","This model is often used in educational and psychological testing when test results yield count data.","In the RPGCM, the test scores are determined by respondents ability and item difficulty.","Locally D-optimal designs are derived for maximum quasi-likelihood estimation to efficiently estimate the mean abilities of the respondents over time.","Using the log link, both unstructured, linear and nonlinear growth curves of log mean abilities are taken into account.","Finally, the sensitivity of the derived optimal designs due to an imprecise choice of parameter values is analyzed using D-efficiency."],"url":"http://arxiv.org/abs/2405.18323v1","category":"stat.ME"}
{"created":"2024-05-28 11:29:29","title":"Carbon-Aware Computing in a Network of Data Centers: A Hierarchical Game-Theoretic Approach","abstract":"Over the past decade, the continuous surge in cloud computing demand has intensified data center workloads, leading to significant carbon emissions and driving the need for improving their efficiency and sustainability. This paper focuses on the optimal allocation problem of batch compute loads with temporal and spatial flexibility across a global network of data centers. We propose a bilevel game-theoretic solution approach that captures the inherent hierarchical relationship between supervisory control objectives, such as carbon reduction and peak shaving, and operational objectives, such as priority-aware scheduling. Numerical simulations with real carbon intensity data demonstrate that the proposed approach successfully reduces carbon emissions while simultaneously ensuring operational reliability and priority-aware scheduling.","sentences":["Over the past decade, the continuous surge in cloud computing demand has intensified data center workloads, leading to significant carbon emissions and driving the need for improving their efficiency and sustainability.","This paper focuses on the optimal allocation problem of batch compute loads with temporal and spatial flexibility across a global network of data centers.","We propose a bilevel game-theoretic solution approach that captures the inherent hierarchical relationship between supervisory control objectives, such as carbon reduction and peak shaving, and operational objectives, such as priority-aware scheduling.","Numerical simulations with real carbon intensity data demonstrate that the proposed approach successfully reduces carbon emissions while simultaneously ensuring operational reliability and priority-aware scheduling."],"url":"http://arxiv.org/abs/2405.18070v1","category":"cs.GT"}
{"created":"2024-05-28 09:39:29","title":"Network-Aware Reliability Modeling and Optimization for Microservice Placement","abstract":"Optimizing microservice placement to enhance the reliability of services is crucial for improving the service level of microservice architecture-based mobile networks and Internet of Things (IoT) networks. Despite extensive research on service reliability, the impact of network load and routing on service reliability remains understudied, leading to suboptimal models and unsatisfactory performance. To address this issue, we propose a novel network-aware service reliability model that effectively captures the correlation between network state changes and reliability. Based on this model, we formulate the microservice placement problem as an integer nonlinear programming problem, aiming to maximize service reliability. Subsequently, a service reliability-aware placement (SRP) algorithm is proposed to solve the problem efficiently. To reduce bandwidth consumption, we further discuss the microservice placement problem with the shared backup path mechanism and propose a placement algorithm based on the SRP algorithm using shared path reliability calculation, known as the SRP-S algorithm. Extensive simulations demonstrate that the SRP algorithm reduces service failures by up to 29% compared to the benchmark algorithms. By introducing the shared backup path mechanism, the SRP-S algorithm reduces bandwidth consumption by up to 62% compared to the SRP algorithm with the fully protected path mechanism. It also reduces service failures by up to 21% compared to the SRP algorithm with the shared backup mechanism.","sentences":["Optimizing microservice placement to enhance the reliability of services is crucial for improving the service level of microservice architecture-based mobile networks and Internet of Things (IoT) networks.","Despite extensive research on service reliability, the impact of network load and routing on service reliability remains understudied, leading to suboptimal models and unsatisfactory performance.","To address this issue, we propose a novel network-aware service reliability model that effectively captures the correlation between network state changes and reliability.","Based on this model, we formulate the microservice placement problem as an integer nonlinear programming problem, aiming to maximize service reliability.","Subsequently, a service reliability-aware placement (SRP) algorithm is proposed to solve the problem efficiently.","To reduce bandwidth consumption, we further discuss the microservice placement problem with the shared backup path mechanism and propose a placement algorithm based on the SRP algorithm using shared path reliability calculation, known as the SRP-S algorithm.","Extensive simulations demonstrate that the SRP algorithm reduces service failures by up to 29% compared to the benchmark algorithms.","By introducing the shared backup path mechanism, the SRP-S algorithm reduces bandwidth consumption by up to 62% compared to the SRP algorithm with the fully protected path mechanism.","It also reduces service failures by up to 21% compared to the SRP algorithm with the shared backup mechanism."],"url":"http://arxiv.org/abs/2405.18001v1","category":"cs.NI"}
{"created":"2024-05-28 07:36:56","title":"Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models","abstract":"Long-context modeling capabilities are important for large language models (LLMs) in various applications. However, directly training LLMs with long context windows is insufficient to enhance this capability since some training samples do not exhibit strong semantic dependencies across long contexts. In this study, we propose a data mining framework \\textbf{ProLong} that can assign each training sample with a long dependency score, which can be used to rank and filter samples that are more advantageous for enhancing long-context modeling abilities in LLM training. Specifically, we first use delta perplexity scores to measure the \\textit{Dependency Strength} between text segments in a given document. Then we refine this metric based on the \\textit{Dependency Distance} of these segments to incorporate spatial relationships across long-contexts. Final results are calibrated with a \\textit{Dependency Specificity} metric to prevent trivial dependencies introduced by repetitive patterns. Moreover, a random sampling approach is proposed to optimize the computational efficiency of ProLong. Comprehensive experiments on multiple benchmarks indicate that ProLong effectively identifies documents that carry long dependencies and LLMs trained on these documents exhibit significantly enhanced long-context modeling capabilities.","sentences":["Long-context modeling capabilities are important for large language models (LLMs) in various applications.","However, directly training LLMs with long context windows is insufficient to enhance this capability since some training samples do not exhibit strong semantic dependencies across long contexts.","In this study, we propose a data mining framework \\textbf{ProLong} that can assign each training sample with a long dependency score, which can be used to rank and filter samples that are more advantageous for enhancing long-context modeling abilities in LLM training.","Specifically, we first use delta perplexity scores to measure the \\textit{Dependency Strength} between text segments in a given document.","Then we refine this metric based on the \\textit{Dependency Distance} of these segments to incorporate spatial relationships across long-contexts.","Final results are calibrated with a \\textit{Dependency Specificity} metric to prevent trivial dependencies introduced by repetitive patterns.","Moreover, a random sampling approach is proposed to optimize the computational efficiency of ProLong.","Comprehensive experiments on multiple benchmarks indicate that ProLong effectively identifies documents that carry long dependencies and LLMs trained on these documents exhibit significantly enhanced long-context modeling capabilities."],"url":"http://arxiv.org/abs/2405.17915v1","category":"cs.CL"}
{"created":"2024-05-28 04:38:40","title":"An optimal chromatic bound for ($P_2+P_3$, gem)-free graphs","abstract":"Given a graph $G$, the parameters $\\chi(G)$ and $\\omega(G)$ respectively denote the chromatic number and the clique number of $G$. A function $f : \\mathbb{N} \\rightarrow \\mathbb{N}$ such that $f(1) = 1$ and $f(x) \\geq x$, for all $x \\in \\mathbb{N}$ is called a $\\chi$-binding function for the given class of graphs $\\cal{G}$ if every $G \\in \\cal{G}$ satisfies $\\chi(G) \\leq f(\\omega(G))$, and the \\emph{smallest $\\chi$-binding function} $f^*$ for $\\cal{G}$ is defined as $f^*(x) := \\max\\{\\chi(G)\\mid G\\in {\\cal G} \\mbox{ and } \\omega(G)=x\\}$. In general, the problem of obtaining the smallest $\\chi$-binding function for the given class of graphs seems to be extremely hard, and only a few classes of graphs are studied in this direction. In this paper, we study the class of ($P_2+ P_3$, gem)-free graphs, and prove that the function $\\phi:\\mathbb{N}\\rightarrow \\mathbb{N}$ defined by $\\phi(1)=1$, $\\phi(2)=4$, $\\phi(3)=6$ and $\\phi(x)=\\left\\lceil\\frac{1}{4}(5x-1)\\right\\rceil$, for $x\\geq 4$ is the smallest $\\chi$-binding function for the class of ($P_2+ P_3$, gem)-free graphs.","sentences":["Given a graph $G$, the parameters $\\chi(G)$ and $\\omega(G)$ respectively denote the chromatic number and the clique number of $G$. A function $f : \\mathbb{N} \\rightarrow \\mathbb{N}$ such that $f(1)","= 1$ and $f(x)","\\geq x$, for all $x \\in \\mathbb{N}$ is called a $\\chi$-binding function for the given class of graphs $\\cal{G}$ if every $G \\in \\cal{G}$ satisfies $\\chi(G) \\leq f(\\omega(G))$, and the \\emph{smallest $\\chi$-binding function} $f^*$ for $\\cal{G}$ is defined as $f^*(x) :","= \\max\\{\\chi(G)\\mid G\\in {\\cal G} \\mbox{ and } \\omega(G)=x\\}$. In general, the problem of obtaining the smallest $\\chi$-binding function for the given class of graphs seems to be extremely hard, and only a few classes of graphs are studied in this direction.","In this paper, we study the class of ($P_2+ P_3$, gem)-free graphs, and prove that the function $\\phi:\\mathbb{N}\\rightarrow \\mathbb{N}$ defined by $\\phi(1)=1$, $\\phi(2)=4$, $\\phi(3)=6$ and $\\phi(x)=\\left\\lceil\\frac{1}{4}(5x-1)\\right\\rceil$, for $x\\geq 4$ is the smallest $\\chi$-binding function for the class of ($P_2+ P_3$, gem)-free graphs."],"url":"http://arxiv.org/abs/2405.17819v1","category":"math.CO"}
{"created":"2024-05-28 04:08:36","title":"Entry-Wise Eigenvector Analysis and Improved Rates for Topic Modeling on Short Documents","abstract":"Topic modeling is a widely utilized tool in text analysis. We investigate the optimal rate for estimating a topic model. Specifically, we consider a scenario with $n$ documents, a vocabulary of size $p$, and document lengths at the order $N$. When $N\\geq c\\cdot p$, referred to as the long-document case, the optimal rate is established in the literature at $\\sqrt{p/(Nn)}$. However, when $N=o(p)$, referred to as the short-document case, the optimal rate remains unknown. In this paper, we first provide new entry-wise large-deviation bounds for the empirical singular vectors of a topic model. We then apply these bounds to improve the error rate of a spectral algorithm, Topic-SCORE. Finally, by comparing the improved error rate with the minimax lower bound, we conclude that the optimal rate is still $\\sqrt{p/(Nn)}$ in the short-document case.","sentences":["Topic modeling is a widely utilized tool in text analysis.","We investigate the optimal rate for estimating a topic model.","Specifically, we consider a scenario with $n$ documents, a vocabulary of size $p$, and document lengths at the order $N$. When $N\\geq c\\cdot p$, referred to as the long-document case, the optimal rate is established in the literature at $\\sqrt{p/(Nn)}$. However, when $N=o(p)$, referred to as the short-document case, the optimal rate remains unknown.","In this paper, we first provide new entry-wise large-deviation bounds for the empirical singular vectors of a topic model.","We then apply these bounds to improve the error rate of a spectral algorithm, Topic-SCORE.","Finally, by comparing the improved error rate with the minimax lower bound, we conclude that the optimal rate is still $\\sqrt{p/(Nn)}$ in the short-document case."],"url":"http://arxiv.org/abs/2405.17806v1","category":"math.ST"}
{"created":"2024-05-28 03:35:41","title":"On the Downlink Average {Energy }Efficiency of Non-Stationary XL-MIMO","abstract":"Extra large-scale multiple-input multiple-output (XL-MIMO) is a key technology for future wireless communication systems. This paper considers the effects of visibility region (VR) at the base station (BS) in a non-stationary multi-user XL-MIMO scenario, where only partial antennas can receive users' signal. In time division duplexing (TDD) mode, we first estimate the VR at the BS by detecting the energy of the received signal during uplink training phase. The probabilities of two detection errors are derived and the uplink channel on the detected VR is estimated. In downlink data transmission, to avoid cumbersome Monte-Carlo trials, we derive a deterministic approximate expression for ergodic {average energy efficiency (EE)} with the regularized zero-forcing (RZF) precoding. In frequency division duplexing (FDD) mode, the VR is estimated in uplink training and then the channel information of detected VR is acquired from the feedback channel. In downlink data transmission, the approximation of ergodic average {EE} is also derived with the RZF precoding. Invoking approximate results, we propose an alternate optimization algorithm to design the detection threshold and the pilot length in both TDD and FDD modes. The numerical results reveal the impacts of VR estimation error on ergodic average {EE} and demonstrate the effectiveness of our proposed algorithm.","sentences":["Extra large-scale multiple-input multiple-output (XL-MIMO) is a key technology for future wireless communication systems.","This paper considers the effects of visibility region (VR) at the base station (BS) in a non-stationary multi-user XL-MIMO scenario, where only partial antennas can receive users' signal.","In time division duplexing (TDD) mode, we first estimate the VR at the BS by detecting the energy of the received signal during uplink training phase.","The probabilities of two detection errors are derived and the uplink channel on the detected VR is estimated.","In downlink data transmission, to avoid cumbersome Monte-Carlo trials, we derive a deterministic approximate expression for ergodic {average energy efficiency (EE)} with the regularized zero-forcing (RZF) precoding.","In frequency division duplexing (FDD) mode, the VR is estimated in uplink training and then the channel information of detected VR is acquired from the feedback channel.","In downlink data transmission, the approximation of ergodic average {EE} is also derived with the RZF precoding.","Invoking approximate results, we propose an alternate optimization algorithm to design the detection threshold and the pilot length in both TDD and FDD modes.","The numerical results reveal the impacts of VR estimation error on ergodic average {EE} and demonstrate the effectiveness of our proposed algorithm."],"url":"http://arxiv.org/abs/2405.17789v1","category":"cs.IT"}
{"created":"2024-05-28 02:30:04","title":"Capturing dynamics and thermodynamics of a three-level quantum heat engine via programmable quantum circuits","abstract":"This research employs the Kraus representation and Sz.-Nagy dilation theorem to model a three-level quantum heat on quantum circuits, investigating its dynamic evolution and thermodynamic performance. The feasibility of the dynamic model is validated by tracking the changes of population. On the basis of reinforcement learning algorithm, the optimal cycle of the quantum heat engine for maximal average power is proposed and verified by the thermodynamic model. The stability of quantum circuit simulations is scrutinized through a comparative analysis of theoretical and simulated results, predicated on an orthogonal test. These results affirm the practicality of simulating quantum heat engines on quantum circuits, offering potential for substantially curtailing the experimental expenses associated with the construction of such engines.","sentences":["This research employs the Kraus representation and Sz.-Nagy dilation theorem to model a three-level quantum heat on quantum circuits, investigating its dynamic evolution and thermodynamic performance.","The feasibility of the dynamic model is validated by tracking the changes of population.","On the basis of reinforcement learning algorithm, the optimal cycle of the quantum heat engine for maximal average power is proposed and verified by the thermodynamic model.","The stability of quantum circuit simulations is scrutinized through a comparative analysis of theoretical and simulated results, predicated on an orthogonal test.","These results affirm the practicality of simulating quantum heat engines on quantum circuits, offering potential for substantially curtailing the experimental expenses associated with the construction of such engines."],"url":"http://arxiv.org/abs/2405.17763v1","category":"quant-ph"}
{"created":"2024-05-28 02:27:53","title":"Double Variance Reduction: A Smoothing Trick for Composite Optimization Problems without First-Order Gradient","abstract":"Variance reduction techniques are designed to decrease the sampling variance, thereby accelerating convergence rates of first-order (FO) and zeroth-order (ZO) optimization methods. However, in composite optimization problems, ZO methods encounter an additional variance called the coordinate-wise variance, which stems from the random gradient estimation. To reduce this variance, prior works require estimating all partial derivatives, essentially approximating FO information. This approach demands O(d) function evaluations (d is the dimension size), which incurs substantial computational costs and is prohibitive in high-dimensional scenarios. This paper proposes the Zeroth-order Proximal Double Variance Reduction (ZPDVR) method, which utilizes the averaging trick to reduce both sampling and coordinate-wise variances. Compared to prior methods, ZPDVR relies solely on random gradient estimates, calls the stochastic zeroth-order oracle (SZO) in expectation $\\mathcal{O}(1)$ times per iteration, and achieves the optimal $\\mathcal{O}(d(n + \\kappa)\\log (\\frac{1}{\\epsilon}))$ SZO query complexity in the strongly convex and smooth setting, where $\\kappa$ represents the condition number and $\\epsilon$ is the desired accuracy. Empirical results validate ZPDVR's linear convergence and demonstrate its superior performance over other related methods.","sentences":["Variance reduction techniques are designed to decrease the sampling variance, thereby accelerating convergence rates of first-order (FO) and zeroth-order (ZO) optimization methods.","However, in composite optimization problems, ZO methods encounter an additional variance called the coordinate-wise variance, which stems from the random gradient estimation.","To reduce this variance, prior works require estimating all partial derivatives, essentially approximating FO information.","This approach demands O(d) function evaluations (d is the dimension size), which incurs substantial computational costs and is prohibitive in high-dimensional scenarios.","This paper proposes the Zeroth-order Proximal Double Variance Reduction (ZPDVR) method, which utilizes the averaging trick to reduce both sampling and coordinate-wise variances.","Compared to prior methods, ZPDVR relies solely on random gradient estimates, calls the stochastic zeroth-order oracle (SZO) in expectation $\\mathcal{O}(1)$ times per iteration, and achieves the optimal $\\mathcal{O}(d(n + \\kappa)\\log (\\frac{1}{\\epsilon}))$ SZO query complexity in the strongly convex and smooth setting, where $\\kappa$ represents the condition number and $\\epsilon$ is the desired accuracy.","Empirical results validate ZPDVR's linear convergence and demonstrate its superior performance over other related methods."],"url":"http://arxiv.org/abs/2405.17761v1","category":"cs.LG"}
{"created":"2024-05-28 01:44:29","title":"Phonon Number Measurement Using Optimal Composite Pulses","abstract":"Measuring the phonon number of the laser-cooled ions is an indispensable step in evaluating whether an ion is in ground state. At present, commonly used methods in the experiments are red-to-blue sideband ratios and adiabatic evolution red-sideband methods. We theoretically propose a method using composite pulses which does not need a fit of state evolution and can directly measure the population of the selected Fock state. It can measure higher Fock state population more directly comparing with the adiabatic evolution red-sideband method. We use quantum optimal control method to improve the fidelity of unitary operation of the composite pulses. With quantum optimal control technology, we can discuss the situation where the laser strength is strong, and many approximations will not be necessary, where the gate fidelity can be further improved. Then we give a method to modify the measurement result for a higher accuracy which has a good performance, and we give an example to illustrate its application on high Fock state measurement.","sentences":["Measuring the phonon number of the laser-cooled ions is an indispensable step in evaluating whether an ion is in ground state.","At present, commonly used methods in the experiments are red-to-blue sideband ratios and adiabatic evolution red-sideband methods.","We theoretically propose a method using composite pulses which does not need a fit of state evolution and can directly measure the population of the selected Fock state.","It can measure higher Fock state population more directly comparing with the adiabatic evolution red-sideband method.","We use quantum optimal control method to improve the fidelity of unitary operation of the composite pulses.","With quantum optimal control technology, we can discuss the situation where the laser strength is strong, and many approximations will not be necessary, where the gate fidelity can be further improved.","Then we give a method to modify the measurement result for a higher accuracy which has a good performance, and we give an example to illustrate its application on high Fock state measurement."],"url":"http://arxiv.org/abs/2405.17736v1","category":"quant-ph"}
{"created":"2024-05-28 01:36:38","title":"State Feedback as a Strategy for Control and Analysis of COVID-19","abstract":"This paper presents a study on a compartmental epidemic model for COVID-19, examining the stability of its equilibrium points upon the introduction of vaccination as a strategy to mitigate the spread of the disease. Initially, the SIQR (Susceptible-Infectious-Quarantine-Recovered) mathematical model and its technical aspects are introduced. Subsequently, vaccination is incorporated as a control measure within the model scope. Equilibrium points and the basic reproductive number are determined, followed by an analysis of their stability. Furthermore, controllability characteristics and Optimal Control strategies for the system are investigated, supplemented by numerical simulations.","sentences":["This paper presents a study on a compartmental epidemic model for COVID-19, examining the stability of its equilibrium points upon the introduction of vaccination as a strategy to mitigate the spread of the disease.","Initially, the SIQR (Susceptible-Infectious-Quarantine-Recovered) mathematical model and its technical aspects are introduced.","Subsequently, vaccination is incorporated as a control measure within the model scope.","Equilibrium points and the basic reproductive number are determined, followed by an analysis of their stability.","Furthermore, controllability characteristics and Optimal Control strategies for the system are investigated, supplemented by numerical simulations."],"url":"http://arxiv.org/abs/2405.17735v1","category":"math.NA"}
{"created":"2024-05-28 00:55:37","title":"Optimal stability of Hardy-Littlewood-Sobolev and Sobolev inequalities of arbitrary orders with dimension-dependent constants","abstract":"Dolbeault-Esteban-Figalli-Frank-Loss [19] and Chen-Lu-Tang [17] established the optimal asymptotic lower bound for stability of the first-order Sobolev inequality and fractional Sobolev inequality of order $s$ for $0<s<1$ respectively. However, it left the problem of the optimal lower bound for stability of high-order Sobolev inequality and high-order fractional Sobolev inequality unsolved. The purpose of this paper is to solve this problem.   The main difficulty lies in establishing the optimal asymptotic behavior for the local stability of the Sobolev inequality for all $0<s<n/2$. The proof of the local stability when $0<s\\leq 1$ relies on ``cuttings\" at various heights and this helps to split the $L^2$ integral of first order or fractional order derivative of order $0<s<1$. However, this approach does not seem to work for $1<s<n/2$. In order to overcome this difficulty, we directly establish the local stability for the HLS inequality with the optimal asymptotic lower bounds.   To achieve our goal, we develop a new strategy based on the $H^{-s}-$decomposition instead of $L^{\\frac{2n}{n+2s}}-$decomposition to obtain the local stability of the HLS inequality with $L^{\\frac{2n}{n+2s}}-$distance. This kind of ``new local stability\" also brings more difficulties to using the rearrangement flow to deduce the global stability from local stability because of the non-uniqueness of $\\|r\\|_{\\frac{2n}{n+2s}}$ and non-continuity of $\\|r\\|_{\\frac{2n}{n+2s}}$ norm for the rearrangement flow. We establish the norm comparison theorem for $\\|r\\|_{\\frac{2n}{n+2s}}$ and \"new continuity\" theorem for the rearrangement flow to overcome this difficulty (see Lemma 3.1, Lemma 3.3 and Lemma 3.5).","sentences":["Dolbeault-Esteban-Figalli-Frank-Loss [19] and Chen-Lu-Tang","[17] established the optimal asymptotic lower bound for stability of the first-order Sobolev inequality and fractional Sobolev inequality of order $s$ for $0<s<1$ respectively.","However, it left the problem of the optimal lower bound for stability of high-order Sobolev inequality and high-order fractional Sobolev inequality unsolved.","The purpose of this paper is to solve this problem.   ","The main difficulty lies in establishing the optimal asymptotic behavior for the local stability of the Sobolev inequality for all $0<s<n/2$.","The proof of the local stability when $0<s\\leq 1$ relies on ``cuttings\" at various heights and this helps to split the $L^2$ integral of first order or fractional order derivative of order $0<s<1$. However, this approach does not seem to work for $1<s<n/2$.","In order to overcome this difficulty, we directly establish the local stability for the HLS inequality with the optimal asymptotic lower bounds.   ","To achieve our goal, we develop a new strategy based on the $H^{-s}-$decomposition instead of $L^{\\frac{2n}{n+2s}}-$decomposition to obtain the local stability of the HLS inequality with $L^{\\frac{2n}{n+2s}}-$distance.","This kind of ``new local stability\" also brings more difficulties to using the rearrangement flow to deduce the global stability from local stability because of the non-uniqueness of $\\|r\\|_{\\frac{2n}{n+2s}}$ and non-continuity of $\\|r\\|_{\\frac{2n}{n+2s}}$ norm for the rearrangement flow.","We establish the norm comparison theorem for $\\|r\\|_{\\frac{2n}{n+2s}}$ and \"new continuity\" theorem for the rearrangement flow to overcome this difficulty (see Lemma 3.1, Lemma 3.3 and Lemma 3.5)."],"url":"http://arxiv.org/abs/2405.17727v1","category":"math.AP"}
{"created":"2024-05-28 00:40:43","title":"TableDC: Deep Clustering for Tabular Data","abstract":"Deep clustering (DC), a fusion of deep representation learning and clustering, has recently demonstrated positive results in data science, particularly text processing and computer vision. However, joint optimization of feature learning and data distribution in the multi-dimensional space is domain-specific, so existing DC methods struggle to generalize to other application domains (such as data integration and cleaning). In data management tasks, where high-density embeddings and overlapping clusters dominate, a data management-specific DC algorithm should be able to interact better with the data properties for supporting data cleaning and integration tasks. This paper presents a deep clustering algorithm for tabular data (TableDC) that reflects the properties of data management applications, particularly schema inference, entity resolution, and domain discovery. To address overlapping clusters, TableDC integrates Mahalanobis distance, which considers variance and correlation within the data, offering a similarity method suitable for tables, rows, or columns in high-dimensional latent spaces. TableDC provides flexibility for the final clustering assignment and shows higher tolerance to outliers through its heavy-tailed Cauchy distribution as the similarity kernel. The proposed similarity measure is particularly beneficial where the embeddings of raw data are densely packed and exhibit high degrees of overlap. Data cleaning tasks may involve a large number of clusters, which affects the scalability of existing DC methods. TableDC's self-supervised module efficiently learns data embeddings with a large number of clusters compared to existing benchmarks, which scale in quadratic time. We evaluated TableDC with several existing DC, Standard Clustering (SC), and state-of-the-art bespoke methods over benchmark datasets. TableDC consistently outperforms existing DC, SC, and bespoke methods.","sentences":["Deep clustering (DC), a fusion of deep representation learning and clustering, has recently demonstrated positive results in data science, particularly text processing and computer vision.","However, joint optimization of feature learning and data distribution in the multi-dimensional space is domain-specific, so existing DC methods struggle to generalize to other application domains (such as data integration and cleaning).","In data management tasks, where high-density embeddings and overlapping clusters dominate, a data management-specific DC algorithm should be able to interact better with the data properties for supporting data cleaning and integration tasks.","This paper presents a deep clustering algorithm for tabular data (TableDC) that reflects the properties of data management applications, particularly schema inference, entity resolution, and domain discovery.","To address overlapping clusters, TableDC integrates Mahalanobis distance, which considers variance and correlation within the data, offering a similarity method suitable for tables, rows, or columns in high-dimensional latent spaces.","TableDC provides flexibility for the final clustering assignment and shows higher tolerance to outliers through its heavy-tailed Cauchy distribution as the similarity kernel.","The proposed similarity measure is particularly beneficial where the embeddings of raw data are densely packed and exhibit high degrees of overlap.","Data cleaning tasks may involve a large number of clusters, which affects the scalability of existing DC methods.","TableDC's self-supervised module efficiently learns data embeddings with a large number of clusters compared to existing benchmarks, which scale in quadratic time.","We evaluated TableDC with several existing DC, Standard Clustering (SC), and state-of-the-art bespoke methods over benchmark datasets.","TableDC consistently outperforms existing DC, SC, and bespoke methods."],"url":"http://arxiv.org/abs/2405.17723v1","category":"cs.DB"}
{"created":"2024-05-28 00:03:41","title":"RealityEffects: Augmenting 3D Volumetric Videos with Object-Centric Annotation and Dynamic Visual Effects","abstract":"This paper introduces RealityEffects, a desktop authoring interface designed for editing and augmenting 3D volumetric videos with object-centric annotations and visual effects. RealityEffects enhances volumetric capture by introducing a novel method for augmenting captured physical motion with embedded, responsive visual effects, referred to as object-centric augmentation. In RealityEffects, users can interactively attach various visual effects to physical objects within the captured 3D scene, enabling these effects to dynamically move and animate in sync with the corresponding physical motion and body movements. The primary contribution of this paper is the development of a taxonomy for such object-centric augmentations, which includes annotated labels, highlighted objects, ghost effects, and trajectory visualization. This taxonomy is informed by an analysis of 120 edited videos featuring object-centric visual effects. The findings from our user study confirm that our direct manipulation techniques lower the barriers to editing and annotating volumetric captures, thereby enhancing interactive and engaging viewing experiences of 3D volumetric videos.","sentences":["This paper introduces RealityEffects, a desktop authoring interface designed for editing and augmenting 3D volumetric videos with object-centric annotations and visual effects.","RealityEffects enhances volumetric capture by introducing a novel method for augmenting captured physical motion with embedded, responsive visual effects, referred to as object-centric augmentation.","In RealityEffects, users can interactively attach various visual effects to physical objects within the captured 3D scene, enabling these effects to dynamically move and animate in sync with the corresponding physical motion and body movements.","The primary contribution of this paper is the development of a taxonomy for such object-centric augmentations, which includes annotated labels, highlighted objects, ghost effects, and trajectory visualization.","This taxonomy is informed by an analysis of 120 edited videos featuring object-centric visual effects.","The findings from our user study confirm that our direct manipulation techniques lower the barriers to editing and annotating volumetric captures, thereby enhancing interactive and engaging viewing experiences of 3D volumetric videos."],"url":"http://arxiv.org/abs/2405.17711v1","category":"cs.HC"}
{"created":"2024-05-27 23:17:24","title":"Compression and In-Situ Query Processing for Fine-Grained Array Lineage","abstract":"Tracking data lineage is important for data integrity, reproducibility, and debugging data science workflows. However, fine-grained lineage (i.e., at a cell level) is challenging to store, even for the smallest datasets. This paper introduces DSLog, a storage system that efficiently stores, indexes, and queries array data lineage, agnostic to capture methodology. A main contribution is our new compression algorithm, named ProvRC, that compresses captured lineage relationships. Using ProvRC for lineage compression result in a significant storage reduction over functions with simple spatial regularity, beating alternative columnar-store baselines by up to 2000x}. We also show that ProvRC facilitates in-situ query processing that allows forward and backward lineage queries without decompression - in the optimal case, surpassing baselines by 20x in query latency on random numpy pipelines.","sentences":["Tracking data lineage is important for data integrity, reproducibility, and debugging data science workflows.","However, fine-grained lineage (i.e., at a cell level) is challenging to store, even for the smallest datasets.","This paper introduces DSLog, a storage system that efficiently stores, indexes, and queries array data lineage, agnostic to capture methodology.","A main contribution is our new compression algorithm, named ProvRC, that compresses captured lineage relationships.","Using ProvRC for lineage compression result in a significant storage reduction over functions with simple spatial regularity, beating alternative columnar-store baselines by up to 2000x}.","We also show that ProvRC facilitates in-situ query processing that allows forward and backward lineage queries without decompression - in the optimal case, surpassing baselines by 20x in query latency on random numpy pipelines."],"url":"http://arxiv.org/abs/2405.17701v1","category":"cs.DB"}
{"created":"2024-05-27 23:02:24","title":"Bias Detection Via Signaling","abstract":"We introduce and study the problem of detecting whether an agent is updating their prior beliefs given new evidence in an optimal way that is Bayesian, or whether they are biased towards their own prior. In our model, biased agents form posterior beliefs that are a convex combination of their prior and the Bayesian posterior, where the more biased an agent is, the closer their posterior is to the prior. Since we often cannot observe the agent's beliefs directly, we take an approach inspired by information design. Specifically, we measure an agent's bias by designing a signaling scheme and observing the actions they take in response to different signals, assuming that they are maximizing their own expected utility; our goal is to detect bias with a minimum number of signals. Our main results include a characterization of scenarios where a single signal suffices and a computationally efficient algorithm to compute optimal signaling schemes.","sentences":["We introduce and study the problem of detecting whether an agent is updating their prior beliefs given new evidence in an optimal way that is Bayesian, or whether they are biased towards their own prior.","In our model, biased agents form posterior beliefs that are a convex combination of their prior and the Bayesian posterior, where the more biased an agent is, the closer their posterior is to the prior.","Since we often cannot observe the agent's beliefs directly, we take an approach inspired by information design.","Specifically, we measure an agent's bias by designing a signaling scheme and observing the actions they take in response to different signals, assuming that they are maximizing their own expected utility; our goal is to detect bias with a minimum number of signals.","Our main results include a characterization of scenarios where a single signal suffices and a computationally efficient algorithm to compute optimal signaling schemes."],"url":"http://arxiv.org/abs/2405.17694v1","category":"cs.GT"}
{"created":"2024-05-27 22:52:49","title":"Fully Subexponential Time Approximation Scheme for Product Partition","abstract":"In this paper we study the Product Partition Problem (PPP), i.e. we are given a set of $n$ natural numbers represented on $m$ bits each and we are asked if a subset exists such that the product of the numbers in the subset equals the product of the numbers not in the subset. Our approach is to obtain the integer factorization of each number. This is the subexponential step. We then form a matrix with the exponents of the primes and propose a novel procedure which modifies the given numbers in such a way that their integer factorization contains sufficient primes to facilitate the search for the solution to the partition problem, while maintaining a similar product. We show that the required time and memory to run the proposed algorithm is subexponential.","sentences":["In this paper we study the Product Partition Problem (PPP), i.e. we are given a set of $n$ natural numbers represented on $m$ bits each and we are asked if a subset exists such that the product of the numbers in the subset equals the product of the numbers not in the subset.","Our approach is to obtain the integer factorization of each number.","This is the subexponential step.","We then form a matrix with the exponents of the primes and propose a novel procedure which modifies the given numbers in such a way that their integer factorization contains sufficient primes to facilitate the search for the solution to the partition problem, while maintaining a similar product.","We show that the required time and memory to run the proposed algorithm is subexponential."],"url":"http://arxiv.org/abs/2405.17692v1","category":"cs.DS"}
{"created":"2024-05-27 22:41:41","title":"Multi-qubit Lattice Surgery Scheduling","abstract":"Fault-tolerant quantum computation using two-dimensional topological quantum error correcting codes can benefit from multi-qubit long-range operations. By using simple commutation rules, a quantum circuit can be transpiled into a sequence of solely non-Clifford multi-qubit gates. Prior work on fault-tolerant compilation avoids optimal scheduling of such gates since they reduce the parallelizability of the circuit. We observe that the reduced parallelization potential is outweighed by the significant reduction in the number of gates. We therefore devise a method for scheduling multi-qubit lattice surgery using an earliest-available-first policy, solving the associated forest packing problem using a representation of the multi-qubit gates as Steiner trees. Our extensive testing on random and application-inspired circuits demonstrates the method's scalability and performance. We show that the transpilation significantly reduces the circuit length on the set of circuits tested, and that the resulting circuit of multi-qubit gates has a further reduction in the expected circuit execution time compared to serial execution.","sentences":["Fault-tolerant quantum computation using two-dimensional topological quantum error correcting codes can benefit from multi-qubit long-range operations.","By using simple commutation rules, a quantum circuit can be transpiled into a sequence of solely non-Clifford multi-qubit gates.","Prior work on fault-tolerant compilation avoids optimal scheduling of such gates since they reduce the parallelizability of the circuit.","We observe that the reduced parallelization potential is outweighed by the significant reduction in the number of gates.","We therefore devise a method for scheduling multi-qubit lattice surgery using an earliest-available-first policy, solving the associated forest packing problem using a representation of the multi-qubit gates as Steiner trees.","Our extensive testing on random and application-inspired circuits demonstrates the method's scalability and performance.","We show that the transpilation significantly reduces the circuit length on the set of circuits tested, and that the resulting circuit of multi-qubit gates has a further reduction in the expected circuit execution time compared to serial execution."],"url":"http://arxiv.org/abs/2405.17688v1","category":"quant-ph"}
{"created":"2024-05-27 21:47:41","title":"Towards the use of multiple ROIs for radiomics-based survival modelling: finding a strategy of aggregating lesions","abstract":"The main objective of this work is to explore the possibility of incorporating radiomic information from multiple lesions into survival models. We hypothesise that when more lesions are present, their inclusion can improve model performance, and we aim to find an optimal strategy for using multiple distinct regions in modelling.   The idea of using multiple regions of interest (ROIs) to extract radiomic features for predictive models has been implemented in many recent works. However, in almost all studies, analogous regions were segmented according to particular criteria for all patients -- for example, the primary tumour and peritumoral area, or subregions of the primary tumour. They can be included in a model in a straightforward way as additional features. A more interesting scenario occurs when multiple distinct ROIs are present, such as multiple lesions in a regionally disseminated cancer. Since the number of such regions may differ between patients, their inclusion in a model is non-trivial and requires additional processing steps.   We proposed several methods of handling multiple ROIs representing either ROI or risk aggregation strategy, compared them to a published one, and evaluated their performance in different classes of survival models in a Monte Carlo Cross-Validation scheme. We demonstrated the effectiveness of the methods using a cohort of 115 non-small cell lung cancer patients, for whom we predicted the metastasis risk based on features extracted from PET images in original resolution or interpolated to CT image resolution. For both feature sets, incorporating all available lesions, as opposed to a singular ROI representing the primary tumour, allowed for considerable improvement of predictive ability regardless of the model.","sentences":["The main objective of this work is to explore the possibility of incorporating radiomic information from multiple lesions into survival models.","We hypothesise that when more lesions are present, their inclusion can improve model performance, and we aim to find an optimal strategy for using multiple distinct regions in modelling.   ","The idea of using multiple regions of interest (ROIs) to extract radiomic features for predictive models has been implemented in many recent works.","However, in almost all studies, analogous regions were segmented according to particular criteria for all patients -- for example, the primary tumour and peritumoral area, or subregions of the primary tumour.","They can be included in a model in a straightforward way as additional features.","A more interesting scenario occurs when multiple distinct ROIs are present, such as multiple lesions in a regionally disseminated cancer.","Since the number of such regions may differ between patients, their inclusion in a model is non-trivial and requires additional processing steps.   ","We proposed several methods of handling multiple ROIs representing either ROI or risk aggregation strategy, compared them to a published one, and evaluated their performance in different classes of survival models in a Monte Carlo Cross-Validation scheme.","We demonstrated the effectiveness of the methods using a cohort of 115 non-small cell lung cancer patients, for whom we predicted the metastasis risk based on features extracted from PET images in original resolution or interpolated to CT image resolution.","For both feature sets, incorporating all available lesions, as opposed to a singular ROI representing the primary tumour, allowed for considerable improvement of predictive ability regardless of the model."],"url":"http://arxiv.org/abs/2405.17668v1","category":"stat.AP"}
{"created":"2024-05-27 20:48:33","title":"An Analysis of Performance Bottlenecks in MRI Pre-Processing","abstract":"Magnetic Resonance Image (MRI) pre-processing is a critical step for neuroimaging analysis. However, the computational cost of MRI pre-processing pipelines is a major bottleneck for large cohort studies and some clinical applications. While High-Performance Computing (HPC) and, more recently, Deep Learning have been adopted to accelerate the computations, these techniques require costly hardware and are not accessible to all researchers. Therefore, it is important to understand the performance bottlenecks of MRI pre-processing pipelines to improve their performance. Using Intel VTune profiler, we characterized the bottlenecks of several commonly used MRI-preprocessing pipelines from the ANTs, FSL, and FreeSurfer toolboxes. We found that few functions contributed to most of the CPU time, and that linear interpolation was the largest contributor. Data access was also a substantial bottleneck. We identified a bug in the ITK library that impacts the performance of ANTs pipeline in single-precision and a potential issue with the OpenMP scaling in FreeSurfer recon-all. Our results provide a reference for future efforts to optimize MRI pre-processing pipelines.","sentences":["Magnetic Resonance Image (MRI) pre-processing is a critical step for neuroimaging analysis.","However, the computational cost of MRI pre-processing pipelines is a major bottleneck for large cohort studies and some clinical applications.","While High-Performance Computing (HPC) and, more recently, Deep Learning have been adopted to accelerate the computations, these techniques require costly hardware and are not accessible to all researchers.","Therefore, it is important to understand the performance bottlenecks of MRI pre-processing pipelines to improve their performance.","Using Intel VTune profiler, we characterized the bottlenecks of several commonly used MRI-preprocessing pipelines from the ANTs, FSL, and FreeSurfer toolboxes.","We found that few functions contributed to most of the CPU time, and that linear interpolation was the largest contributor.","Data access was also a substantial bottleneck.","We identified a bug in the ITK library that impacts the performance of ANTs pipeline in single-precision and a potential issue with the OpenMP scaling in FreeSurfer recon-all.","Our results provide a reference for future efforts to optimize MRI pre-processing pipelines."],"url":"http://arxiv.org/abs/2405.17650v1","category":"cs.PF"}
{"created":"2024-05-27 19:49:18","title":"Salutary Labeling with Zero Human Annotation","abstract":"Active learning strategically selects informative unlabeled data points and queries their ground truth labels for model training. The prevailing assumption underlying this machine learning paradigm is that acquiring these ground truth labels will optimally enhance model performance. However, this assumption may not always hold true or maximize learning capacity, particularly considering the costly labor annotations required for ground truth labels. In contrast to traditional ground truth labeling, this paper proposes salutary labeling, which automatically assigns the most beneficial labels to the most informative samples without human annotation. Specifically, we utilize the influence function, a tool for estimating sample influence, to select newly added samples and assign their salutary labels by choosing the category that maximizes their positive influence. This process eliminates the need for human annotation. Extensive experiments conducted on nine benchmark datasets demonstrate the superior performance of our salutary labeling approach over traditional active learning strategies. Additionally, we provide several in-depth explorations and practical applications of large language model (LLM) fine-tuning.","sentences":["Active learning strategically selects informative unlabeled data points and queries their ground truth labels for model training.","The prevailing assumption underlying this machine learning paradigm is that acquiring these ground truth labels will optimally enhance model performance.","However, this assumption may not always hold true or maximize learning capacity, particularly considering the costly labor annotations required for ground truth labels.","In contrast to traditional ground truth labeling, this paper proposes salutary labeling, which automatically assigns the most beneficial labels to the most informative samples without human annotation.","Specifically, we utilize the influence function, a tool for estimating sample influence, to select newly added samples and assign their salutary labels by choosing the category that maximizes their positive influence.","This process eliminates the need for human annotation.","Extensive experiments conducted on nine benchmark datasets demonstrate the superior performance of our salutary labeling approach over traditional active learning strategies.","Additionally, we provide several in-depth explorations and practical applications of large language model (LLM) fine-tuning."],"url":"http://arxiv.org/abs/2405.17627v1","category":"cs.LG"}
{"created":"2024-05-27 19:25:55","title":"Design of a Rectangular Linear Microstrip Patch Antenna Array for 5G Communication","abstract":"This paper presents the design and characterization of a rectangular microstrip patch antenna array optimized for operation within the Ku-band frequency range. The antenna array is impedance-matched to 50 Ohms and utilizes a microstrip line feeding mechanism for excitation. The design maintains compact dimensions, with the overall antenna occupying an area of 29.5x7 mm. The antenna structure is modelled on an R03003 substrate material, featuring a dielectric constant of 3, a low-loss tangent of 0.0009, and a thickness of 1.574 mm. The substrate is backed by a conducting ground plane, and the array consists of six radiating patch elements positioned on top. Evaluation of the designed antenna array reveals a resonant frequency of 18GHz, with a -10 dB impedance bandwidth extending over 700MHz. The antenna demonstrates a high gain of 7.51dBi, making it well-suited for applications in 5G and future communication systems. Its compact form factor, cost-effectiveness, and broad impedance and radiation coverage further underscore its potential in these domains.","sentences":["This paper presents the design and characterization of a rectangular microstrip patch antenna array optimized for operation within the Ku-band frequency range.","The antenna array is impedance-matched to 50 Ohms and utilizes a microstrip line feeding mechanism for excitation.","The design maintains compact dimensions, with the overall antenna occupying an area of 29.5x7 mm.","The antenna structure is modelled on an R03003 substrate material, featuring a dielectric constant of 3, a low-loss tangent of 0.0009, and a thickness of 1.574 mm.","The substrate is backed by a conducting ground plane, and the array consists of six radiating patch elements positioned on top.","Evaluation of the designed antenna array reveals a resonant frequency of 18GHz, with a -10 dB impedance bandwidth extending over 700MHz.","The antenna demonstrates a high gain of 7.51dBi, making it well-suited for applications in 5G and future communication systems.","Its compact form factor, cost-effectiveness, and broad impedance and radiation coverage further underscore its potential in these domains."],"url":"http://arxiv.org/abs/2405.17616v1","category":"eess.SY"}
{"created":"2024-05-27 19:22:41","title":"A Framework for Multi-modal Learning: Jointly Modeling Inter- & Intra-Modality Dependencies","abstract":"Supervised multi-modal learning involves mapping multiple modalities to a target label. Previous studies in this field have concentrated on capturing in isolation either the inter-modality dependencies (the relationships between different modalities and the label) or the intra-modality dependencies (the relationships within a single modality and the label). We argue that these conventional approaches that rely solely on either inter- or intra-modality dependencies may not be optimal in general. We view the multi-modal learning problem from the lens of generative models where we consider the target as a source of multiple modalities and the interaction between them. Towards that end, we propose inter- & intra-modality modeling (I2M2) framework, which captures and integrates both the inter- and intra-modality dependencies, leading to more accurate predictions. We evaluate our approach using real-world healthcare and vision-and-language datasets with state-of-the-art models, demonstrating superior performance over traditional methods focusing only on one type of modality dependency.","sentences":["Supervised multi-modal learning involves mapping multiple modalities to a target label.","Previous studies in this field have concentrated on capturing in isolation either the inter-modality dependencies (the relationships between different modalities and the label) or the intra-modality dependencies (the relationships within a single modality and the label).","We argue that these conventional approaches that rely solely on either inter- or intra-modality dependencies may not be optimal in general.","We view the multi-modal learning problem from the lens of generative models where we consider the target as a source of multiple modalities and the interaction between them.","Towards that end, we propose inter- & intra-modality modeling (I2M2) framework, which captures and integrates both the inter- and intra-modality dependencies, leading to more accurate predictions.","We evaluate our approach using real-world healthcare and vision-and-language datasets with state-of-the-art models, demonstrating superior performance over traditional methods focusing only on one type of modality dependency."],"url":"http://arxiv.org/abs/2405.17613v1","category":"cs.CV"}
{"created":"2024-05-27 19:20:22","title":"A note on the error analysis of data-driven closure models for large eddy simulations of turbulence","abstract":"In this work, we provide a mathematical formulation for error propagation in flow trajectory prediction using data-driven turbulence closure modeling. Under the assumption that the predicted state of a large eddy simulation prediction must be close to that of a subsampled direct numerical simulation, we retrieve an upper bound for the prediction error when utilizing a data-driven closure model. We also demonstrate that this error is significantly affected by the time step size and the Jacobian which play a role in amplifying the initial one-step error made by using the closure. Our analysis also shows that the error propagates exponentially with rollout time and the upper bound of the system Jacobian which is itself influenced by the Jacobian of the closure formulation. These findings could enable the development of new regularization techniques for ML models based on the identified error-bound terms, improving their robustness and reducing error propagation.","sentences":["In this work, we provide a mathematical formulation for error propagation in flow trajectory prediction using data-driven turbulence closure modeling.","Under the assumption that the predicted state of a large eddy simulation prediction must be close to that of a subsampled direct numerical simulation, we retrieve an upper bound for the prediction error when utilizing a data-driven closure model.","We also demonstrate that this error is significantly affected by the time step size and the Jacobian which play a role in amplifying the initial one-step error made by using the closure.","Our analysis also shows that the error propagates exponentially with rollout time and the upper bound of the system Jacobian which is itself influenced by the Jacobian of the closure formulation.","These findings could enable the development of new regularization techniques for ML models based on the identified error-bound terms, improving their robustness and reducing error propagation."],"url":"http://arxiv.org/abs/2405.17612v1","category":"physics.flu-dyn"}
{"created":"2024-05-27 19:12:42","title":"A Patient-Specific Framework for Autonomous Spinal Fixation via a Steerable Drilling Robot","abstract":"In this paper, with the goal of enhancing the minimally invasive spinal fixation procedure in osteoporotic patients, we propose a first-of-its-kind image-guided robotic framework for performing an autonomous and patient-specific procedure using a unique concentric tube steerable drilling robot (CT-SDR). Particularly, leveraging a CT-SDR, we introduce the concept of J-shape drilling based on a pre-operative trajectory planned in CT scan of a patient followed by appropriate calibration, registration, and navigation steps to safely execute this trajectory in real-time using our unique robotic setup. To thoroughly evaluate the performance of our framework, we performed several experiments on two different vertebral phantoms designed based on CT scan of real patients.","sentences":["In this paper, with the goal of enhancing the minimally invasive spinal fixation procedure in osteoporotic patients, we propose a first-of-its-kind image-guided robotic framework for performing an autonomous and patient-specific procedure using a unique concentric tube steerable drilling robot (CT-SDR).","Particularly, leveraging a CT-SDR, we introduce the concept of J-shape drilling based on a pre-operative trajectory planned in CT scan of a patient followed by appropriate calibration, registration, and navigation steps to safely execute this trajectory in real-time using our unique robotic setup.","To thoroughly evaluate the performance of our framework, we performed several experiments on two different vertebral phantoms designed based on CT scan of real patients."],"url":"http://arxiv.org/abs/2405.17606v1","category":"cs.RO"}
{"created":"2024-05-27 19:02:24","title":"Towards Biomechanical Evaluation of a Transformative Additively Manufactured Flexible Pedicle Screw for Robotic Spinal Fixation","abstract":"Vital for spinal fracture treatment, pedicle screw fixation is the gold standard for spinal fixation procedures. Nevertheless, due to the screw pullout and loosening issues, this surgery often fails to be effective for patients suffering from osteoporosis (i.e., having low bone mineral density). These failures can be attributed to the rigidity of existing drilling instruments and pedicle screws forcing clinicians to place these implants into the osteoporotic regions of the vertebral body. To address this critical issue, we have developed a steerable drilling robotic system and evaluated its performance in drilling various J- and U-shape trajectories. Complementary to this robotic system, in this paper, we propose design, additive manufacturing, and biomechanical evaluation of a transformative flexible pedicle screw (FPS) that can be placed in pre-drilled straight and curved trajectories. To evaluate the performance of the proposed flexible implant, we designed and fabricated two different types of FPSs using the direct metal laser sintering (DMLS) process. Utilizing our unique experimental setup and ASTM standards, we then performed various pullout experiments on these FPSs to evaluate and analyze their biomechanical performance implanted in straight trajectories.","sentences":["Vital for spinal fracture treatment, pedicle screw fixation is the gold standard for spinal fixation procedures.","Nevertheless, due to the screw pullout and loosening issues, this surgery often fails to be effective for patients suffering from osteoporosis (i.e., having low bone mineral density).","These failures can be attributed to the rigidity of existing drilling instruments and pedicle screws forcing clinicians to place these implants into the osteoporotic regions of the vertebral body.","To address this critical issue, we have developed a steerable drilling robotic system and evaluated its performance in drilling various J- and U-shape trajectories.","Complementary to this robotic system, in this paper, we propose design, additive manufacturing, and biomechanical evaluation of a transformative flexible pedicle screw (FPS) that can be placed in pre-drilled straight and curved trajectories.","To evaluate the performance of the proposed flexible implant, we designed and fabricated two different types of FPSs using the direct metal laser sintering (DMLS) process.","Utilizing our unique experimental setup and ASTM standards, we then performed various pullout experiments on these FPSs to evaluate and analyze their biomechanical performance implanted in straight trajectories."],"url":"http://arxiv.org/abs/2405.17603v1","category":"cs.RO"}
{"created":"2024-05-27 19:01:03","title":"Spatial Spinal Fixation: A Transformative Approach Using a Unique Robot-Assisted Steerable Drilling System and Flexible Pedicle Screw","abstract":"Spinal fixation procedures are currently limited by the rigidity of the existing instruments and pedicle screws leading to fixation failures and rigid pedicle screw pull out. Leveraging our recently developed Concentric Tube Steerable Drilling Robot (CT-SDR) in integration with a robotic manipulator, to address the aforementioned issue, here we introduce the transformative concept of Spatial Spinal Fixation (SSF) using a unique Flexible Pedicle Screw (FPS). The proposed SSF procedure enables planar and out-of-plane placement of the FPS throughout the full volume of the vertebral body. In other words, not only does our fixation system provide the option of drilling in-plane and out-of-plane trajectories, it also enables implanting the FPS inside linear (represented by an I-shape) and/or non-linear (represented by J-shape) trajectories. To thoroughly evaluate the functionality of our proposed robotic system and the SSF procedure, we have performed various experiments by drilling different I-J and J-J drilling trajectory pairs into our custom-designed L3 vertebral phantoms and analyzed the accuracy of the procedure using various metrics.","sentences":["Spinal fixation procedures are currently limited by the rigidity of the existing instruments and pedicle screws leading to fixation failures and rigid pedicle screw pull out.","Leveraging our recently developed Concentric Tube Steerable Drilling Robot (CT-SDR) in integration with a robotic manipulator, to address the aforementioned issue, here we introduce the transformative concept of Spatial Spinal Fixation (SSF) using a unique Flexible Pedicle Screw (FPS).","The proposed SSF procedure enables planar and out-of-plane placement of the FPS throughout the full volume of the vertebral body.","In other words, not only does our fixation system provide the option of drilling in-plane and out-of-plane trajectories, it also enables implanting the FPS inside linear (represented by an I-shape) and/or non-linear (represented by J-shape) trajectories.","To thoroughly evaluate the functionality of our proposed robotic system and the SSF procedure, we have performed various experiments by drilling different I-J and J-J drilling trajectory pairs into our custom-designed L3 vertebral phantoms and analyzed the accuracy of the procedure using various metrics."],"url":"http://arxiv.org/abs/2405.17600v1","category":"cs.RO"}
{"created":"2024-05-27 18:55:09","title":"Towards Achieving Cooperation Compliance of Human Drivers in Mixed Traffic","abstract":"We consider a mixed-traffic environment in transportation systems, where Connected and Automated Vehicles (CAVs) coexist with potentially non-cooperative Human-Driven Vehicles (HDVs). We develop a cooperation compliance control framework to incentivize HDVs to align their behavior with socially optimal objectives using a ``refundable toll'' scheme so as to achieve a desired compliance probability for all non-compliant HDVs through a feedback control mechanism combining global with local (individual) components. We apply this scheme to the lane-changing problem, where a ``Social Planner'' provides references to the HDVs, measures their state errors, and induces cooperation compliance for safe lane-changing through a refundable toll approach. Simulation results are included to show the effectiveness of our cooperation compliance controller in terms of improved compliance and lane-changing maneuver safety and efficiency when non-cooperative HDVs are present.","sentences":["We consider a mixed-traffic environment in transportation systems, where Connected and Automated Vehicles (CAVs) coexist with potentially non-cooperative Human-Driven Vehicles (HDVs).","We develop a cooperation compliance control framework to incentivize HDVs to align their behavior with socially optimal objectives using a ``refundable toll'' scheme so as to achieve a desired compliance probability for all non-compliant HDVs through a feedback control mechanism combining global with local (individual) components.","We apply this scheme to the lane-changing problem, where a ``Social Planner'' provides references to the HDVs, measures their state errors, and induces cooperation compliance for safe lane-changing through a refundable toll approach.","Simulation results are included to show the effectiveness of our cooperation compliance controller in terms of improved compliance and lane-changing maneuver safety and efficiency when non-cooperative HDVs are present."],"url":"http://arxiv.org/abs/2405.17594v1","category":"eess.SY"}
{"created":"2024-05-27 18:19:09","title":"Container pre-marshalling problem minimizing CV@R under uncertainty of ship arrival times","abstract":"This paper is concerned with the container pre-marshalling problem, which involves relocating containers in the storage area so that they can be efficiently loaded onto ships without reshuffles. In reality, however, ship arrival times are affected by various external factors, which can cause the order of container retrieval to be different from the initial plan. To represent such uncertainty, we generate multiple scenarios from a multivariate probability distribution of ship arrival times. We derive a mixed-integer linear optimization model to find an optimal container layout such that the conditional value-at-risk is minimized for the number of misplaced containers responsible for reshuffles. Moreover, we devise an exact algorithm based on the cutting-plane method to handle large-scale problems. Numerical experiments using synthetic datasets demonstrate that our method can produce high-quality container layouts compared with the conventional robust optimization model. Additionally, our algorithm can speed up the computation of solving large-scale problems.","sentences":["This paper is concerned with the container pre-marshalling problem, which involves relocating containers in the storage area so that they can be efficiently loaded onto ships without reshuffles.","In reality, however, ship arrival times are affected by various external factors, which can cause the order of container retrieval to be different from the initial plan.","To represent such uncertainty, we generate multiple scenarios from a multivariate probability distribution of ship arrival times.","We derive a mixed-integer linear optimization model to find an optimal container layout such that the conditional value-at-risk is minimized for the number of misplaced containers responsible for reshuffles.","Moreover, we devise an exact algorithm based on the cutting-plane method to handle large-scale problems.","Numerical experiments using synthetic datasets demonstrate that our method can produce high-quality container layouts compared with the conventional robust optimization model.","Additionally, our algorithm can speed up the computation of solving large-scale problems."],"url":"http://arxiv.org/abs/2405.17576v1","category":"math.OC"}
{"created":"2024-05-27 18:00:00","title":"The MQT Handbook: A Summary of Design Automation Tools and Software for Quantum Computing","abstract":"Quantum computers are becoming a reality and numerous quantum computing applications with a near-term perspective (e.g., for finance, chemistry, machine learning, and optimization) and with a long-term perspective (e.g., for cryptography or unstructured search) are currently being investigated. However, designing and realizing potential applications for these devices in a scalable fashion requires automated, efficient, and user-friendly software tools that cater to the needs of end users, engineers, and physicists at every level of the entire quantum software stack. Many of the problems to be tackled in that regard are similar to design problems from the classical realm for which sophisticated design automation tools have been developed in the previous decades.   The Munich Quantum Toolkit (MQT) is a collection of software tools for quantum computing developed by the Chair for Design Automation at the Technical University of Munich which explicitly utilizes this design automation expertise. Our overarching objective is to provide solutions for design tasks across the entire quantum software stack. This entails high-level support for end users in realizing their applications, efficient methods for the classical simulation, compilation, and verification of quantum circuits, tools for quantum error correction, support for physical design, and more. These methods are supported by corresponding data structures (such as decision diagrams) and core methods (such as SAT encodings/solvers). All of the developed tools are available as open-source implementations and are hosted on https://github.com/cda-tum.","sentences":["Quantum computers are becoming a reality and numerous quantum computing applications with a near-term perspective (e.g., for finance, chemistry, machine learning, and optimization) and with a long-term perspective (e.g., for cryptography or unstructured search) are currently being investigated.","However, designing and realizing potential applications for these devices in a scalable fashion requires automated, efficient, and user-friendly software tools that cater to the needs of end users, engineers, and physicists at every level of the entire quantum software stack.","Many of the problems to be tackled in that regard are similar to design problems from the classical realm for which sophisticated design automation tools have been developed in the previous decades.   ","The Munich Quantum Toolkit (MQT) is a collection of software tools for quantum computing developed by the Chair for Design Automation at the Technical University of Munich which explicitly utilizes this design automation expertise.","Our overarching objective is to provide solutions for design tasks across the entire quantum software stack.","This entails high-level support for end users in realizing their applications, efficient methods for the classical simulation, compilation, and verification of quantum circuits, tools for quantum error correction, support for physical design, and more.","These methods are supported by corresponding data structures (such as decision diagrams) and core methods (such as SAT encodings/solvers).","All of the developed tools are available as open-source implementations and are hosted on https://github.com/cda-tum."],"url":"http://arxiv.org/abs/2405.17543v1","category":"quant-ph"}
{"created":"2024-05-28 17:59:00","title":"Afterglow Linear Polarization Signatures from Shallow GRB Jets: Implications for Energetic GRBs","abstract":"Gamma-ray bursts (GRBs) are powered by ultra-relativistic jets. The launching sites of these jets are surrounded by dense media, which the jets must cross before they can accelerate and release the high energy emission. Interaction with the medium leads to the formation of a mildly relativistic sheath around the jet resulting in an angular structures in the jet's asymptotic Lorentz factor and energy per solid angle, which modifies the afterglow emission. We build a semi-analytical tool to analyze the afterglow light curve and polarization signatures of jets observed from a wide range of viewing angles, and focus on ones with slowly declining energy profiles known as shallow jets. We find overall lower polarization compared to the classical top-hat jet model. We provide an analytical expression for the peak polarization degree as a function of the energy profile power-law index, magnetic field configuration and viewing angle, and show that it occurs near the light curve break time for all viewers. When applying our tool to GRB 221009A, suspected to originate from a shallow jet, we find that the suggested jet structures for this event agree with the upper limits placed on the afterglow polarization in the optical and X-ray bands. We also find that at early times the polarization levels may be significantly higher, allowing for a potential distinction between different jet structure models and possibly constraining the magnetization in both forward and reverse shocks at that stage.","sentences":["Gamma-ray bursts (GRBs) are powered by ultra-relativistic jets.","The launching sites of these jets are surrounded by dense media, which the jets must cross before they can accelerate and release the high energy emission.","Interaction with the medium leads to the formation of a mildly relativistic sheath around the jet resulting in an angular structures in the jet's asymptotic Lorentz factor and energy per solid angle, which modifies the afterglow emission.","We build a semi-analytical tool to analyze the afterglow light curve and polarization signatures of jets observed from a wide range of viewing angles, and focus on ones with slowly declining energy profiles known as shallow jets.","We find overall lower polarization compared to the classical top-hat jet model.","We provide an analytical expression for the peak polarization degree as a function of the energy profile power-law index, magnetic field configuration and viewing angle, and show that it occurs near the light curve break time for all viewers.","When applying our tool to GRB 221009A, suspected to originate from a shallow jet, we find that the suggested jet structures for this event agree with the upper limits placed on the afterglow polarization in the optical and X-ray bands.","We also find that at early times the polarization levels may be significantly higher, allowing for a potential distinction between different jet structure models and possibly constraining the magnetization in both forward and reverse shocks at that stage."],"url":"http://arxiv.org/abs/2405.18423v1","category":"astro-ph.HE"}
{"created":"2024-05-28 17:04:23","title":"Neutron Data Evaluation of 243Am","abstract":"The diverse measured data base of n+243Am was evaluated using a statistical theory and genera-lized least squares codes. Consistent description of total, capture and fission measured data provides an important constraint for the inelastic scattering cross section. Important constraints for the measured capture cross section in the 0.15-300 keV energy range come from the average radiative and neutron S0 and S1 strength functions. The evaluated inelastic cross sections of available evaluations are in severe disagreement, predicted change of the inelastic cross section shape at En ~1.5 MeV is attributed to the sharp increase of the level density of the residual odd-even nuclide 241Am due to the onset of three-quasi-particle excitations. The influence of exclusive (n, xnf) pre-fission neutrons on prompt fission neutron spectra (PFNS) and (n, xn) spectra is modelled. Contributions of emissive/non-emissive fission and exclusive spectra of (n, xnf) reactions are defined by a consistent description of the 241Am(n, F), 241Am(n, 2n). Data file is at https://www-nds.iaea.org/minskact/data/original/za095243","sentences":["The diverse measured data base of n+243Am was evaluated using a statistical theory and genera-lized least squares codes.","Consistent description of total, capture and fission measured data provides an important constraint for the inelastic scattering cross section.","Important constraints for the measured capture cross section in the 0.15-300 keV energy range come from the average radiative and neutron S0 and S1 strength functions.","The evaluated inelastic cross sections of available evaluations are in severe disagreement, predicted change of the inelastic cross section shape at En ~1.5 MeV is attributed to the sharp increase of the level density of the residual odd-even nuclide 241Am due to the onset of three-quasi-particle excitations.","The influence of exclusive (n, xnf) pre-fission neutrons on prompt fission neutron spectra (PFNS) and (n, xn) spectra is modelled.","Contributions of emissive/non-emissive fission and exclusive spectra of (n, xnf) reactions are defined by a consistent description of the 241Am(n, F), 241Am(n, 2n).","Data file is at https://www-nds.iaea.org/minskact/data/original/za095243"],"url":"http://arxiv.org/abs/2405.18366v1","category":"nucl-th"}
{"created":"2024-05-28 16:54:45","title":"Evaluating radiation impact on transmon qubits in above and underground facilities","abstract":"Superconducting qubits can be sensitive to abrupt energy deposits caused by cosmic rays and ambient radioactivity. Previous studies have focused on understanding possible correlated effects over time and distance due to cosmic rays. In this study, for the first time, we directly compare the response of a transmon qubit measured initially at the Fermilab SQMS above-ground facilities and then at the deep underground Gran Sasso Laboratory (INFN-LNGS, Italy). We observe same average qubit lifetime T$_1$ of roughly 80 microseconds at above and underground facilities. We then apply a fast decay detection protocol and investigate the time structure, sensitivity and relative rates of triggered events due to radiation versus intrinsic noise, comparing above and underground performance of several high-coherence qubits. Using gamma sources of variable activity we calibrate the response of the qubit to different levels of radiation in an environment with minimal background radiation. Results indicate that qubits respond to a strong gamma source and it is possible to detect particle impacts. However, when comparing above and underground results, we do not observe a difference in radiation induced-like events for these sapphire and niobium-based transmon qubits. We conclude that the majority of these events are not radiation related and to be attributed to other noise sources which by far dominate single qubit errors in modern transmon qubits.","sentences":["Superconducting qubits can be sensitive to abrupt energy deposits caused by cosmic rays and ambient radioactivity.","Previous studies have focused on understanding possible correlated effects over time and distance due to cosmic rays.","In this study, for the first time, we directly compare the response of a transmon qubit measured initially at the Fermilab SQMS above-ground facilities and then at the deep underground Gran Sasso Laboratory (INFN-LNGS, Italy).","We observe same average qubit lifetime T$_1$ of roughly 80 microseconds at above and underground facilities.","We then apply a fast decay detection protocol and investigate the time structure, sensitivity and relative rates of triggered events due to radiation versus intrinsic noise, comparing above and underground performance of several high-coherence qubits.","Using gamma sources of variable activity we calibrate the response of the qubit to different levels of radiation in an environment with minimal background radiation.","Results indicate that qubits respond to a strong gamma source and it is possible to detect particle impacts.","However, when comparing above and underground results, we do not observe a difference in radiation induced-like events for these sapphire and niobium-based transmon qubits.","We conclude that the majority of these events are not radiation related and to be attributed to other noise sources which by far dominate single qubit errors in modern transmon qubits."],"url":"http://arxiv.org/abs/2405.18355v1","category":"quant-ph"}
{"created":"2024-05-28 16:26:41","title":"Revisiting the decoupling limit of the Georgi-Machacek model with a scalar singlet","abstract":"We study the connection between collider and dark matter phenomenology in the singlet extension of the Georgi-Machacek model. In this framework, the singlet scalar serves as a suitable thermal dark matter (DM) candidate. Our focus lies on the region $v_{\\chi}<1$ GeV, where $v_{\\chi}$ is the common vacuum expectation value of the neutral components of the scalar triplets of the model. Setting bounds on the model parameters from theoretical, electroweak precision and LHC experimental constraints, we find that the BSM Higgs sector is highly constrained. Allowed values for the masses of the custodial fiveplets, triplets and singlet are restricted to the range $140~ {\\rm GeV }< M_{H_5} < 350~ {\\rm GeV }$, $150~ {\\rm GeV }< M_{H_3} < 270 ~{\\rm GeV }$ and $145~ {\\rm GeV }< M_{H} < 300~ {\\rm GeV }$. The extended scalar sector provides new channels for DM annihilation into BSM scalars that allow to satisfy the observed relic density constraint while being consistent with direct DM detection limits. The allowed region of the parameter space of the model can be explored in the upcoming DM detection experiments, both direct and indirect. In particular, the possible high values of BR$(H^0_5\\to\\gamma\\gamma)$ can lead to an indirect DM signal within the reach of CTA. The same feature also provides the possibility of exploring the model at the High-Luminosity run of the LHC. In a simple cut-based analysis, we find that a signal of about $4\\sigma$ significance can be achieved in final states with at least two photons for one of our benchmark points.","sentences":["We study the connection between collider and dark matter phenomenology in the singlet extension of the Georgi-Machacek model.","In this framework, the singlet scalar serves as a suitable thermal dark matter (DM) candidate.","Our focus lies on the region $v_{\\chi}<1$ GeV, where $v_{\\chi}$ is the common vacuum expectation value of the neutral components of the scalar triplets of the model.","Setting bounds on the model parameters from theoretical, electroweak precision and LHC experimental constraints, we find that the BSM Higgs sector is highly constrained.","Allowed values for the masses of the custodial fiveplets, triplets and singlet are restricted to the range","$140~ {\\rm GeV }< M_{H_5} < 350~ {\\rm GeV }$, $150~ {\\rm GeV }< M_{H_3} < 270 ~{\\rm GeV }$ and $145~ {\\rm GeV }< M_{H} < 300~ {\\rm GeV }$.","The extended scalar sector provides new channels for DM annihilation into BSM scalars that allow to satisfy the observed relic density constraint while being consistent with direct DM detection limits.","The allowed region of the parameter space of the model can be explored in the upcoming DM detection experiments, both direct and indirect.","In particular, the possible high values of BR$(H^0_5\\to\\gamma\\gamma)$ can lead to an indirect DM signal within the reach of CTA.","The same feature also provides the possibility of exploring the model at the High-Luminosity run of the LHC.","In a simple cut-based analysis, we find that a signal of about $4\\sigma$ significance can be achieved in final states with at least two photons for one of our benchmark points."],"url":"http://arxiv.org/abs/2405.18332v1","category":"hep-ph"}
{"created":"2024-05-28 16:11:10","title":"Full Silicon Pillar-based 1D Optomechanical cavities","abstract":"Nanomechanical resonators can serve as ultrasensitive, miniaturized force probes. While vertical structures like nanopillars are ideal for this purpose, transducing their motion is challenging. Pillar-based photonic crystals (PhCs) offer a potential solution by integrating optical transduction within the pillars. However, achieving high-quality PhCs is hindered by inefficient vertical light confinement. Here, we present a full-silicon 1D photonic crystal cavity based on nanopillars as a new platform with great potential for applications in force sensing and biosensing areas. Its unit cell consists of a silicon pillar with larger diameter at its top portion than at the bottom, which allows vertical light confinement and an energy bandgap in the near infrared range for transverse-magnetic (TM) polarization. We experimentally demonstrate optical cavities with Q-factors exceeding 1e3 constructed by inserting a defect within a periodic arrangement of this type of pillars. Given the fact that that each nanopillar naturally behaves as a nanomechanical cantilever, the fabricated geometries are excellent optomechanical (OM) photonic crystal cavities in which the mechanical motion of each nanopillar composing the cavity can be optically transduced. These novel geometries display enhanced mechanical properties, cost-effectiveness, integration possibilities, and scalability, and opens and new path in front of the widely used suspended Si beam OM cavities made on silicon-on-insulator.","sentences":["Nanomechanical resonators can serve as ultrasensitive, miniaturized force probes.","While vertical structures like nanopillars are ideal for this purpose, transducing their motion is challenging.","Pillar-based photonic crystals (PhCs) offer a potential solution by integrating optical transduction within the pillars.","However, achieving high-quality PhCs is hindered by inefficient vertical light confinement.","Here, we present a full-silicon 1D photonic crystal cavity based on nanopillars as a new platform with great potential for applications in force sensing and biosensing areas.","Its unit cell consists of a silicon pillar with larger diameter at its top portion than at the bottom, which allows vertical light confinement and an energy bandgap in the near infrared range for transverse-magnetic (TM) polarization.","We experimentally demonstrate optical cavities with Q-factors exceeding 1e3 constructed by inserting a defect within a periodic arrangement of this type of pillars.","Given the fact that that each nanopillar naturally behaves as a nanomechanical cantilever, the fabricated geometries are excellent optomechanical (OM) photonic crystal cavities in which the mechanical motion of each nanopillar composing the cavity can be optically transduced.","These novel geometries display enhanced mechanical properties, cost-effectiveness, integration possibilities, and scalability, and opens and new path in front of the widely used suspended Si beam OM cavities made on silicon-on-insulator."],"url":"http://arxiv.org/abs/2405.18319v1","category":"physics.optics"}
{"created":"2024-05-28 15:38:42","title":"Full and approximated NLO predictions for like-sign W-boson scattering at the LHC","abstract":"We report on a recent calculation of next-to-leading-order (NLO) QCD and electroweak corrections to like-sign W-boson scattering at the Large Hadron Collider, including all partonic channels and W-boson decays in the process $pp \\to e^+ \\nu_e \\mu^+ \\nu_\\mu jj + X$. The calculation is implemented in the Monte Carlo integrator Bonsay and comprises the full tower of NLO contributions of the orders $\\alpha_s^3\\alpha^4$, $\\alpha_s^2\\alpha^5$, $\\alpha_s\\alpha^6$, and $\\alpha^7$. Our numerical results confirm and extend previous results, in particular the occurrence of large purely electroweak corrections of the order of $\\sim-12\\%$ for integrated cross sections, which get even larger in distributions. We construct a \"VBS approximation\" for the NLO prediction based on partonic channels and gauge-invariant (sub)matrix elements potentially containing the vector-boson scattering (VBS) subprocess and on resonance expansions of the Wdecays. The VBS approximation reproduces the full NLO predictions within $\\sim1.5\\%$ in the most important regions of phase space. Moreover, we discuss results from different versions of \"effective vector-boson approximations\" at leading order, based on the collinear emission of W bosons of incoming (anti)quarks. However, owing to the only mild collinear enhancement and the design of VBS analysis cuts, the quality of this approximation turns out to be only qualitative at the LHC.","sentences":["We report on a recent calculation of next-to-leading-order (NLO) QCD and electroweak corrections to like-sign W-boson scattering at the Large Hadron Collider, including all partonic channels and W-boson decays in the process $pp \\to e^+ \\nu_e \\mu^+ \\nu_\\mu jj","+ X$.","The calculation is implemented in the Monte Carlo integrator Bonsay and comprises the full tower of NLO contributions of the orders $\\alpha_s^3\\alpha^4$, $\\alpha_s^2\\alpha^5$, $\\alpha_s\\alpha^6$, and $\\alpha^7$. Our numerical results confirm and extend previous results, in particular the occurrence of large purely electroweak corrections of the order of $\\sim-12\\%$ for integrated cross sections, which get even larger in distributions.","We construct a \"VBS approximation\" for the NLO prediction based on partonic channels and gauge-invariant (sub)matrix elements potentially containing the vector-boson scattering (VBS) subprocess and on resonance expansions of the Wdecays.","The VBS approximation reproduces the full NLO predictions within $\\sim1.5\\%$ in the most important regions of phase space.","Moreover, we discuss results from different versions of \"effective vector-boson approximations\" at leading order, based on the collinear emission of W bosons of incoming (anti)quarks.","However, owing to the only mild collinear enhancement and the design of VBS analysis cuts, the quality of this approximation turns out to be only qualitative at the LHC."],"url":"http://arxiv.org/abs/2405.18286v1","category":"hep-ph"}
{"created":"2024-05-28 15:16:21","title":"Error-Free and Current-Driven Synthetic Antiferromagnetic Domain Wall Memory Enabled by Channel Meandering","abstract":"We propose a new type of multi-bit and energy-efficient magnetic memory based on current-driven, field-free, and highly controlled domain wall motion. A meandering domain wall channel with precisely interspersed pinning regions provides the multi-bit capability of a magnetic tunnel junction. The magnetic free layer of the memory device has perpendicular magnetic anisotropy and interfacial Dzyaloshinskii-Moriya interaction, so that spin-orbit torques induce efficient domain wall motion. Using micromagnetic simulations, we find two pinning mechanisms that lead to different cell designs: two-way switching and four-way switching. The memory cell design choices and the physics behind these pinning mechanisms are discussed in detail. Furthermore, we show that switching reliability and speed may be significantly improved by replacing the ferromagnetic free layer with a synthetic antiferromagnetic layer. Switching behavior and material choices will be discussed for the two implementations.","sentences":["We propose a new type of multi-bit and energy-efficient magnetic memory based on current-driven, field-free, and highly controlled domain wall motion.","A meandering domain wall channel with precisely interspersed pinning regions provides the multi-bit capability of a magnetic tunnel junction.","The magnetic free layer of the memory device has perpendicular magnetic anisotropy and interfacial Dzyaloshinskii-Moriya interaction, so that spin-orbit torques induce efficient domain wall motion.","Using micromagnetic simulations, we find two pinning mechanisms that lead to different cell designs: two-way switching and four-way switching.","The memory cell design choices and the physics behind these pinning mechanisms are discussed in detail.","Furthermore, we show that switching reliability and speed may be significantly improved by replacing the ferromagnetic free layer with a synthetic antiferromagnetic layer.","Switching behavior and material choices will be discussed for the two implementations."],"url":"http://arxiv.org/abs/2405.18261v1","category":"cs.ET"}
{"created":"2024-05-28 15:08:24","title":"Electrical Control Grain Dimensionality with Multilevel Magnetic Anisotropy","abstract":"In alignment with the increasing demand for larger storage capacity and longer data retention, electrical control of magnetic anisotropy has been a research focus in the realm of spintronics. Typically, magnetic anisotropy is determined by grain dimensionality, which is set during the fabrication of magnetic thin films. Despite the intrinsic correlation between magnetic anisotropy and grain dimensionality, there is a lack of experimental evidence for electrically controlling grain dimensionality, thereby impeding the efficiency of magnetic anisotropy modulation. Here, we demonstrate an electric field control of grain dimensionality and prove it as the active mechanism for tuning interfacial magnetism. The reduction in grain dimensionality is associated with a transition from ferromagnetic to superparamagnetic behavior. We achieve a non-volatile and reversible modulation of the coercivity in both the ferromagnetic and superparamagnetic regimes. Subsequent electrical and elemental analysis confirms the variation in grain dimensionality upon the application of gate voltages, revealing a transition from a multidomain to a single-domain state accompanied by a reduction in grain dimensionality. Furthermore, we exploit the influence of grain dimensionality on domain wall motion, extending its applicability to multilevel magnetic memory and synaptic devices. Our results provide a strategy for tuning interfacial magnetism through grain size engineering for advancements in high-performance spintronics.","sentences":["In alignment with the increasing demand for larger storage capacity and longer data retention, electrical control of magnetic anisotropy has been a research focus in the realm of spintronics.","Typically, magnetic anisotropy is determined by grain dimensionality, which is set during the fabrication of magnetic thin films.","Despite the intrinsic correlation between magnetic anisotropy and grain dimensionality, there is a lack of experimental evidence for electrically controlling grain dimensionality, thereby impeding the efficiency of magnetic anisotropy modulation.","Here, we demonstrate an electric field control of grain dimensionality and prove it as the active mechanism for tuning interfacial magnetism.","The reduction in grain dimensionality is associated with a transition from ferromagnetic to superparamagnetic behavior.","We achieve a non-volatile and reversible modulation of the coercivity in both the ferromagnetic and superparamagnetic regimes.","Subsequent electrical and elemental analysis confirms the variation in grain dimensionality upon the application of gate voltages, revealing a transition from a multidomain to a single-domain state accompanied by a reduction in grain dimensionality.","Furthermore, we exploit the influence of grain dimensionality on domain wall motion, extending its applicability to multilevel magnetic memory and synaptic devices.","Our results provide a strategy for tuning interfacial magnetism through grain size engineering for advancements in high-performance spintronics."],"url":"http://arxiv.org/abs/2405.18256v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-28 15:06:09","title":"Model Dependent Analysis of D_((s))^+ arrows \u03b7^((')) l^+ \u03bd_l Decays in Beyond Standard Model","abstract":"Motivated by the recent experimental results of branching fractions for D_((s))^+ arrows {\\eta}^((')) l^+ {\\nu}_l decays, which deviate from their SM predictions, we have investigated these decays in W' model and scalar leptoquark model to find possible signatures of new physics (NP) in semileptonic charm decays induced by c arrow(s,d)l {nu}_l transitions. Using recent experimental results of branching fractions for semileptonic D meson decays, new coupling parameters are predicted for the above NP models. Branching fraction, forward-backward asymmetry and lepton polarization asymmetry are studied taking the predicted NP coupling parameters. Results of branching fractions in scalar leptoquark model are found very close to the experimental results and exist around the range 1{sigma} deviation. We have presented a comparative study of the NP models to check their sensitivity on these decays. We anticipate that further research on these decays will significantly support our findings.","sentences":["Motivated by the recent experimental results of branching fractions for D_((s))^+ arrows {\\eta}^(('))","l^+ {\\nu}_l decays, which deviate from their SM predictions, we have investigated these decays in W' model and scalar leptoquark model to find possible signatures of new physics (NP) in semileptonic charm decays induced by c arrow(s,d)l {nu}_l transitions.","Using recent experimental results of branching fractions for semileptonic D meson decays, new coupling parameters are predicted for the above NP models.","Branching fraction, forward-backward asymmetry and lepton polarization asymmetry are studied taking the predicted NP coupling parameters.","Results of branching fractions in scalar leptoquark model are found very close to the experimental results and exist around the range 1{sigma} deviation.","We have presented a comparative study of the NP models to check their sensitivity on these decays.","We anticipate that further research on these decays will significantly support our findings."],"url":"http://arxiv.org/abs/2405.18254v1","category":"hep-ph"}
{"created":"2024-05-28 15:01:05","title":"Spontaneous flows in active smectics with dislocations","abstract":"We construct a hydrodynamic theory of active smectics A in two-dimensional space, including the creation/annihilation and motility of dislocations with Burgers' number $\\pm1$. We derive analytical criteria on the set of parameters that lead to flows. We show that the motility of dislocations can lead to flow transitions with distinct features from the previously reported active Helfrich--Hurault shear instability with, notably, a first-order transition in the velocity from quiescence to turbulence.","sentences":["We construct a hydrodynamic theory of active smectics A in two-dimensional space, including the creation/annihilation and motility of dislocations with Burgers' number $\\pm1$. We derive analytical criteria on the set of parameters that lead to flows.","We show that the motility of dislocations can lead to flow transitions with distinct features from the previously reported active Helfrich--Hurault shear instability with, notably, a first-order transition in the velocity from quiescence to turbulence."],"url":"http://arxiv.org/abs/2405.18250v1","category":"cond-mat.soft"}
{"created":"2024-05-28 14:59:21","title":"Strange quark stars: the role of excluded volume effects","abstract":"We study cold strange quark stars employing an enhanced version of the quark-mass density-dependent model which incorporates excluded volume effects to address non-perturbative QCD repulsive interactions. We provide a comparative analysis of our mass formula parametrization with previous models from the literature. We identify the regions within the parameter space where three-flavor quark matter is more stable than the most tightly bound atomic nucleus (stability window). Specifically, we show that excluded volume effects do not change the Gibbs free energy per baryon at zero pressure, rendering the stability window unaffected. The curves of pressure versus energy density exhibit various shapes -- convex upward, concave downward, or nearly linear -- depending on the mass parametrization. This behavior results in different patterns of increase, decrease, or constancy in the speed of sound as a function of baryon number density. We analyze the mass-radius relationship of strange quark stars, revealing a significant increase in maximum gravitational mass and a shift in the curves towards larger radii as the excluded volume effect intensifies. Excluded volume effects render our models compatible with all modern astrophysical constraints, including the properties of the recently observed low-mass compact object HESSJ1731.","sentences":["We study cold strange quark stars employing an enhanced version of the quark-mass density-dependent model which incorporates excluded volume effects to address non-perturbative QCD repulsive interactions.","We provide a comparative analysis of our mass formula parametrization with previous models from the literature.","We identify the regions within the parameter space where three-flavor quark matter is more stable than the most tightly bound atomic nucleus (stability window).","Specifically, we show that excluded volume effects do not change the Gibbs free energy per baryon at zero pressure, rendering the stability window unaffected.","The curves of pressure versus energy density exhibit various shapes -- convex upward, concave downward, or nearly linear -- depending on the mass parametrization.","This behavior results in different patterns of increase, decrease, or constancy in the speed of sound as a function of baryon number density.","We analyze the mass-radius relationship of strange quark stars, revealing a significant increase in maximum gravitational mass and a shift in the curves towards larger radii as the excluded volume effect intensifies.","Excluded volume effects render our models compatible with all modern astrophysical constraints, including the properties of the recently observed low-mass compact object HESSJ1731."],"url":"http://arxiv.org/abs/2405.18249v1","category":"nucl-th"}
{"created":"2024-05-28 14:53:29","title":"Tunable magnetism of Boron Imidazolate-based Metal-Organic Frameworks","abstract":"Magnetic metal-organic frameworks (MMOFs), where magnetic metal nodes are connected into a crystal structure by organic linkers, have a potential to host exotic magnetic states. We present a study of bulk magnetic properties of four metal-organic frameworks with boron imidazolate linkers, Zn-BIF, Cu-BIF, Co-BIF, and Ni-BIF, displaying a variety of lattice structures and nontrivial magnetic behaviors. While non-magnetic Zn-BIF provides an offset of magnetic response, magnetic susceptibility measurements of the other three magnetic materials demonstrate the presence of weak magnetic interactions in these MOFs, which differ between materials by sign and size. Cu-MOF, where magnetic nodes are connected into octahedral cages, shows simple paramagnetic behavior. Triangular lattice Co-MOF shows antiferromagnetic interactions on the order of 1 K, and a spin-crossover-like effect in magnetic susceptibility due to thermal depopulation of excited crystal electric field levels. Magnetic properties of Ni-BIF suggest sizable ferromagnetic interactions. Using DC/AC susceptibility and variable-field DC magnetization, we detect behavior that resembles that of superparamagnetic single-magnetic-domain nanoparticles, but with particle sizes on the scale of the unit cell, and discuss possible microscopic origins of this behavior. This work demonstrates the variety of magnetic properties that are possible with a single organic ligand, and establishes the low energy scale of magnetic interactions through superexchange in boron imidazolate frameworks.","sentences":["Magnetic metal-organic frameworks (MMOFs), where magnetic metal nodes are connected into a crystal structure by organic linkers, have a potential to host exotic magnetic states.","We present a study of bulk magnetic properties of four metal-organic frameworks with boron imidazolate linkers, Zn-BIF, Cu-BIF, Co-BIF, and Ni-BIF, displaying a variety of lattice structures and nontrivial magnetic behaviors.","While non-magnetic Zn-BIF provides an offset of magnetic response, magnetic susceptibility measurements of the other three magnetic materials demonstrate the presence of weak magnetic interactions in these MOFs, which differ between materials by sign and size.","Cu-MOF, where magnetic nodes are connected into octahedral cages, shows simple paramagnetic behavior.","Triangular lattice Co-MOF shows antiferromagnetic interactions on the order of 1 K, and a spin-crossover-like effect in magnetic susceptibility due to thermal depopulation of excited crystal electric field levels.","Magnetic properties of Ni-BIF suggest sizable ferromagnetic interactions.","Using DC/AC susceptibility and variable-field DC magnetization, we detect behavior that resembles that of superparamagnetic single-magnetic-domain nanoparticles, but with particle sizes on the scale of the unit cell, and discuss possible microscopic origins of this behavior.","This work demonstrates the variety of magnetic properties that are possible with a single organic ligand, and establishes the low energy scale of magnetic interactions through superexchange in boron imidazolate frameworks."],"url":"http://arxiv.org/abs/2405.18244v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-28 14:37:37","title":"FAST Discovery of Eight Isolated Millisecond Pulsars in NGC 6517","abstract":"We present the discovery of 8 isolated millisecond pulsars in Globular Cluster (GC) NGC 6517 using the Five-Hundred-meter Aperture Spherical radio Telescope (FAST). The spin periods of those pulsars (namely PSR J1801-0857K to R, or, NGC 6517K to R) are all shorter than 10 ms. With these discoveries, NGC 6517 is currently the GC with the most known pulsars in the FAST sky. The largest difference in dispersion measure of the pulsars in NGC 6517 is 11.2 cm$^{-3}$ pc, the second among all GCs. The fraction of isolated pulsars in this GC (16 of 17, 94$\\%$) is consistent with previous studies indicating an overabundance of isolated pulsars in the densest GCs, especially in those undergoing cluster core collapse. Considering the FAST GC pulsar discoveries, we modeled the GC pulsar population using the empirical Bayesian method described by Turk and Lorimer with the recent counts. Using this approach, we find that the expected number of potential pulsars in GCs seems to be correlated with the central escape velocity, hence, the GCs Liller 1, NGC 6441, M54 (NGC 6715), and $\\omega$-Cen (NGC 5139) are expected to host the largest numbers of pulsars.","sentences":["We present the discovery of 8 isolated millisecond pulsars in Globular Cluster (GC) NGC 6517 using the Five-Hundred-meter Aperture Spherical radio Telescope (FAST).","The spin periods of those pulsars (namely PSR J1801-0857K to R, or, NGC 6517K to R) are all shorter than 10 ms.","With these discoveries, NGC 6517 is currently the GC with the most known pulsars in the FAST sky.","The largest difference in dispersion measure of the pulsars in NGC 6517 is 11.2 cm$^{-3}$ pc, the second among all GCs.","The fraction of isolated pulsars in this GC (16 of 17, 94$\\%$) is consistent with previous studies indicating an overabundance of isolated pulsars in the densest GCs, especially in those undergoing cluster core collapse.","Considering the FAST GC pulsar discoveries, we modeled the GC pulsar population using the empirical Bayesian method described by Turk and Lorimer with the recent counts.","Using this approach, we find that the expected number of potential pulsars in GCs seems to be correlated with the central escape velocity, hence, the GCs Liller 1, NGC 6441, M54 (NGC 6715), and $\\omega$-Cen (NGC 5139) are expected to host the largest numbers of pulsars."],"url":"http://arxiv.org/abs/2405.18228v1","category":"astro-ph.HE"}
{"created":"2024-05-28 14:37:22","title":"Theoretical bounds on dark Higgs mass in a self-interacting dark matter model with $U(1)'$","abstract":"Motivated by the null results of current dark matter searches and the small-scale problems, we study a dark sector charged by a spontaneous broken gauge $U(1)'$. To explore the parameter space of this model, in addition to the consideration of the small-scale data, we also consider the theoretical bounds on the dark Higgs mass, with the upper bound coming from the tree-level perturbative unitarity and the lower bound from the one-loop Linde-Weinberg bound. We deeply examine the dependence of the Linde-Weinberg bound on gauge choice and energy scale. Combining the theoretical and observational constraints, we obtain the following ranges for the parameter space: the dark matter mass is 10-500 GeV, the mediator (dark photon) mass is 0.5-5 MeV, the dark Higgs mass is 0.05-50 MeV, the dark fine-structure constant is 0.001-0.5. We conclude that the dark Higgs in this model cannot be ignored in the phenomenological study of the dark sector.","sentences":["Motivated by the null results of current dark matter searches and the small-scale problems, we study a dark sector charged by a spontaneous broken gauge $U(1)'$. To explore the parameter space of this model, in addition to the consideration of the small-scale data, we also consider the theoretical bounds on the dark Higgs mass, with the upper bound coming from the tree-level perturbative unitarity and the lower bound from the one-loop Linde-Weinberg bound.","We deeply examine the dependence of the Linde-Weinberg bound on gauge choice and energy scale.","Combining the theoretical and observational constraints, we obtain the following ranges for the parameter space: the dark matter mass is 10-500 GeV, the mediator (dark photon) mass is 0.5-5 MeV, the dark Higgs mass is 0.05-50 MeV, the dark fine-structure constant is 0.001-0.5.","We conclude that the dark Higgs in this model cannot be ignored in the phenomenological study of the dark sector."],"url":"http://arxiv.org/abs/2405.18226v1","category":"hep-ph"}
{"created":"2024-05-28 14:18:49","title":"Constraining Axion-Gluon Coupling in Mono-hadron Processes","abstract":"The axion-gluon coupling can be constrained directly through hard exclusive processes at the LHC. Specifically, we study the associated production of a long-lived axion with a $\\rho^0$ meson in ultra-peripheral $AA$ collisions and in $pp$ collisions. With the axion escaped from the detector, the final state is characterized by a mono-hadron signature. The main background in our analysis originates from the $\\rho^0+\\pi^0$ process, where the photons from the $\\pi^0$ decay are undetected due to limited detector performance. Our analysis yields an exclusion limit of the axion-gluon coupling that is comparable to the limit obtained from the mono-jet process at the LHC.","sentences":["The axion-gluon coupling can be constrained directly through hard exclusive processes at the LHC.","Specifically, we study the associated production of a long-lived axion with a $\\rho^0$ meson in ultra-peripheral $AA$ collisions and in $pp$ collisions.","With the axion escaped from the detector, the final state is characterized by a mono-hadron signature.","The main background in our analysis originates from the $\\rho^0+\\pi^0$ process, where the photons from the $\\pi^0$ decay are undetected due to limited detector performance.","Our analysis yields an exclusion limit of the axion-gluon coupling that is comparable to the limit obtained from the mono-jet process at the LHC."],"url":"http://arxiv.org/abs/2405.18215v1","category":"hep-ph"}
{"created":"2024-05-28 14:15:43","title":"Observation and manipulation of charge carrier distribution at the SiO$_2$/Si interface","abstract":"Using low-energy muons, we map the charge carrier concentration as a function of depth and electric field across the \\SiOSi interface up to a depth of \\SI{100}{\\nano\\meter} in Si-based MOS capacitors. The results show that the formation of the anisotropic bond-centered muonium \\MuBCz state in Si serves as a direct measure of the local changes in electronic structures. Different band-bending conditions could be distinguished, and the extension of the depletion width was directly extracted using the localized stopping and probing depth of the muons. Furthermore, electron build-up on the Si side of the \\SiOO/Si interface, caused by the mirror charge induced by the fixed positive charge in the oxide and the image force effect, was observed. Our work represents a significant extension of the application of the muon spin rotation technique ($\\mu$SR) and lays the foundation for further research on direct observation of charge carrier density manipulation at technologically important semiconductor device interfaces.","sentences":["Using low-energy muons, we map the charge carrier concentration as a function of depth and electric field across the \\SiOSi interface up to a depth of \\SI{100}{\\nano\\meter} in Si-based MOS capacitors.","The results show that the formation of the anisotropic bond-centered muonium \\MuBCz state in Si serves as a direct measure of the local changes in electronic structures.","Different band-bending conditions could be distinguished, and the extension of the depletion width was directly extracted using the localized stopping and probing depth of the muons.","Furthermore, electron build-up on the Si side of the \\SiOO/Si interface, caused by the mirror charge induced by the fixed positive charge in the oxide and the image force effect, was observed.","Our work represents a significant extension of the application of the muon spin rotation technique ($\\mu$SR) and lays the foundation for further research on direct observation of charge carrier density manipulation at technologically important semiconductor device interfaces."],"url":"http://arxiv.org/abs/2405.18211v1","category":"cond-mat.other"}
{"created":"2024-05-28 14:03:32","title":"Dalitz-plot analysis of B+- --> K+-K+K- decays","abstract":"We study B+- -- > K+-K+K- decays using the QCD factorization model with final state interactions between K+ and K- mesons taken into account. The parameters of the model are fitted to the data of the BABAR and LHCb collaborations. We describe the K Kbar effective mass distributions and examine the CP-violating asymmetry effects in the full range of the Dalitz plot.","sentences":["We study B+- -- > K+-K+K- decays using the QCD factorization model with final state interactions between K+ and K- mesons taken into account.","The parameters of the model are fitted to the data of the BABAR and LHCb collaborations.","We describe the K Kbar effective mass distributions and examine the CP-violating asymmetry effects in the full range of the Dalitz plot."],"url":"http://arxiv.org/abs/2405.18192v1","category":"hep-ph"}
{"created":"2024-05-28 14:03:31","title":"Instantons in $\u03c6^4$ Theories: Transseries, Virial Theorems and Numerical Aspects","abstract":"We discuss numerical aspects of instantons in two- and three-dimensional $\\phi^4$ theories with an internal $O(N)$ symmetry group, the so-called $N$-vector model. Combining asymptotic transseries expansions for large argument with convergence acceleration techniques, we obtain high-precision values for certain integrals of the instanton that naturally occur in loop corrections around instanton configurations. Knowledge of these numerical properties are necessary in order to evaluate corrections to the large-order factorial growth of perturbation theory in $\\phi^4$ theories. The results contribute to the understanding of the mathematical structures underlying the instanton configurations.","sentences":["We discuss numerical aspects of instantons in two- and three-dimensional $\\phi^4$ theories with an internal $O(N)$ symmetry group, the so-called $N$-vector model.","Combining asymptotic transseries expansions for large argument with convergence acceleration techniques, we obtain high-precision values for certain integrals of the instanton that naturally occur in loop corrections around instanton configurations.","Knowledge of these numerical properties are necessary in order to evaluate corrections to the large-order factorial growth of perturbation theory in $\\phi^4$ theories.","The results contribute to the understanding of the mathematical structures underlying the instanton configurations."],"url":"http://arxiv.org/abs/2405.18191v1","category":"hep-th"}
{"created":"2024-05-28 13:44:21","title":"On the resistance regular graphs","abstract":"For a connected graph $G$, its resistance matrix is denoted by $R(G)$. If all the row(column) sums of $R(G)$ are equal, then $G$ is said to be resistance regular. In $[13]$, J. Zhou et al. posed the question regarding the existence of a non-regular resistance regular graph. In this article, we establish that all resistance regular graphs are regular, thus conclusively answering Zhou's question by showing that no non-regular resistance regular graph exists. Also, we compute the resistance energies of some resistance regular graphs. Furthermore, we determine various bounds for the resistance energy and resistance spectral radius of $G.$","sentences":["For a connected graph $G$, its resistance matrix is denoted by $R(G)$. If all the row(column) sums of $R(G)$ are equal, then $G$ is said to be resistance regular.","In $[13]$, J. Zhou et al. posed the question regarding the existence of a non-regular resistance regular graph.","In this article, we establish that all resistance regular graphs are regular, thus conclusively answering Zhou's question by showing that no non-regular resistance regular graph exists.","Also, we compute the resistance energies of some resistance regular graphs.","Furthermore, we determine various bounds for the resistance energy and resistance spectral radius of $G.$"],"url":"http://arxiv.org/abs/2405.18177v1","category":"math.CO"}
{"created":"2024-05-28 13:01:19","title":"One-form symmetries and the 3d $\\mathcal{N}=2$ $A$-model: Topologically twisted indices for any $G$","abstract":"We study three-dimensional $\\mathcal{N}=2$ supersymmetric Chern-Simons-matter gauge theories with a one-form symmetry in the $A$-model formalism on $\\Sigma_g\\times S^1$. We explicitly compute expectation values of topological line operators that implement the one-form symmetry. This allows us to compute the topologically twisted index on the closed Riemann surface $\\Sigma_g$ for any real compact gauge group $G$. All computations are carried out in the effective $A$-model on $\\Sigma_g$, whose ground states are the so-called Bethe vacua. We discuss how the 3d one-form symmetry acts on the Bethe vacua, and how its 't Hooft anomaly constrains the vacuum structure. In the special case of the $SU(N)_K$ $\\mathcal{N}=2$ Chern-Simons theory, we obtain results for the $(SU(N)/\\mathbb{Z}_r)^{\\theta}_K$ $\\mathcal{N}=2$ Chern-Simons theories, for all non-anomalous $\\mathbb{Z}_r \\subseteq \\mathbb{Z}_N$ subgroups of the center of the gauge group, and with the associated $\\mathbb{Z}_r$ $\\theta$-angle turned on, reproducing and extending various results in the literature. In particular, we find an interesting mixed 't Hooft anomaly between gravity and the $\\mathbb{Z}_r$ one-form symmetry of the $SU(N)_K$ theory (for $N$ even, $\\frac{N}{r}$ odd and $\\frac{K}{r}$ even). This plays a key role in our derivation of the Witten index, which we explicitly compute for any $N$, $K$ and $r$ in terms of refinements of Jordan's totient function. Our results lead to precise conjectures about integrality of indices, which appear to have a strong number-theoretic flavour. Note: this paper directly builds upon unpublished notes by Brian Willett from 2020.","sentences":["We study three-dimensional $\\mathcal{N}=2$ supersymmetric Chern-Simons-matter gauge theories with a one-form symmetry in the $A$-model formalism on $\\Sigma_g\\times S^1$.","We explicitly compute expectation values of topological line operators that implement the one-form symmetry.","This allows us to compute the topologically twisted index on the closed Riemann surface $\\Sigma_g$ for any real compact gauge group $G$. All computations are carried out in the effective $A$-model on $\\Sigma_g$, whose ground states are the so-called Bethe vacua.","We discuss how the 3d one-form symmetry acts on the Bethe vacua, and how its 't Hooft anomaly constrains the vacuum structure.","In the special case of the $SU(N)_K$ $\\mathcal{N}=2$ Chern-Simons theory, we obtain results for the $(SU(N)/\\mathbb{Z}_r)^{\\theta}_K$ $\\mathcal{N}=2$ Chern-Simons theories, for all non-anomalous $\\mathbb{Z}_r \\subseteq \\mathbb{Z}_N$ subgroups of the center of the gauge group, and with the associated $\\mathbb{Z}_r$ $\\theta$-angle turned on, reproducing and extending various results in the literature.","In particular, we find an interesting mixed 't Hooft anomaly between gravity and the $\\mathbb{Z}_r$ one-form symmetry of the $SU(N)_K$ theory (for $N$ even, $\\frac{N}{r}$ odd and $\\frac{K}{r}$ even).","This plays a key role in our derivation of the Witten index, which we explicitly compute for any $N$, $K$ and $r$ in terms of refinements of Jordan's totient function.","Our results lead to precise conjectures about integrality of indices, which appear to have a strong number-theoretic flavour.","Note: this paper directly builds upon unpublished notes by Brian Willett from 2020."],"url":"http://arxiv.org/abs/2405.18141v1","category":"hep-th"}
{"created":"2024-05-28 12:50:58","title":"Treatment of QED corrections in jet production in deep inelastic scattering at ZEUS","abstract":"A new measurement of inclusive jet production in deep inelastic scattering was recently published by the ZEUS Collaboration. This contribution presents a detailed discussion of the treatment of higher-order QED effects in this measurement. A comprehensive treatment of these effects is crucial for a more direct comparison between ever more precise measurements and theoretical calculations. The present analysis is the only measurement of jet production in deep inelastic scattering that can be compared to full NNLO QCD + NLO electroweak predictions.","sentences":["A new measurement of inclusive jet production in deep inelastic scattering was recently published by the ZEUS Collaboration.","This contribution presents a detailed discussion of the treatment of higher-order QED effects in this measurement.","A comprehensive treatment of these effects is crucial for a more direct comparison between ever more precise measurements and theoretical calculations.","The present analysis is the only measurement of jet production in deep inelastic scattering that can be compared to full NNLO QCD + NLO electroweak predictions."],"url":"http://arxiv.org/abs/2405.18136v1","category":"hep-ex"}
{"created":"2024-05-28 12:44:06","title":"SMART: spectral energy distributions Markov chain analysis with radiative transfer models","abstract":"In this paper we present the publicly available open-source spectral energy distribution (SED) fitting code SMART (Spectral energy distributions Markov chain Analysis with Radiative Transfer models). Implementing a Bayesian Markov chain Monte Carlo (MCMC) method, SMART fits the ultraviolet to millimetre SEDs of galaxies exclusively with radiative transfer models that currently constitute four types of pre-computed libraries, which describe the starburst, active galactic nucleus (AGN) torus, host galaxy and polar dust components. An important novelty of SMART is that, although it fits SEDs exclusively with radiative transfer models, it takes comparable time to popular energy balance methods to run. Here we describe the key features of SMART and test it by fitting the multi-wavelength SEDs of the 42 local ultraluminous infrared galaxies (ULIRGs) that constitute the HERschel Ultraluminous Infrared Galaxy Survey (HERUS) sample. The Spitzer spectroscopy data of the HERUS ULIRGs are included in the fitting at a spectral resolution, which is matched to that of the radiative transfer models. We also present other results that highlight the performance and versatility of SMART. SMART promises to be a useful tool for studying galaxy evolution in the JWST era. SMART is developed in PYTHON and is available at https://github.com/ch-var/SMART.git.","sentences":["In this paper we present the publicly available open-source spectral energy distribution (SED) fitting code SMART (Spectral energy distributions Markov chain Analysis with Radiative Transfer models).","Implementing a Bayesian Markov chain Monte Carlo (MCMC) method, SMART fits the ultraviolet to millimetre SEDs of galaxies exclusively with radiative transfer models that currently constitute four types of pre-computed libraries, which describe the starburst, active galactic nucleus (AGN) torus, host galaxy and polar dust components.","An important novelty of SMART is that, although it fits SEDs exclusively with radiative transfer models, it takes comparable time to popular energy balance methods to run.","Here we describe the key features of SMART and test it by fitting the multi-wavelength SEDs of the 42 local ultraluminous infrared galaxies (ULIRGs) that constitute the HERschel Ultraluminous Infrared Galaxy Survey (HERUS) sample.","The Spitzer spectroscopy data of the HERUS ULIRGs are included in the fitting at a spectral resolution, which is matched to that of the radiative transfer models.","We also present other results that highlight the performance and versatility of SMART.","SMART promises to be a useful tool for studying galaxy evolution in the JWST era.","SMART is developed in PYTHON and is available at https://github.com/ch-var/SMART.git."],"url":"http://arxiv.org/abs/2405.18130v1","category":"astro-ph.GA"}
{"created":"2024-05-28 12:09:10","title":"The commuting graphs of certain cyclic-by-abelian groups","abstract":"Let $G$ be a finite, non-abelian group of the form $G = A N$, where $A \\leq G$ is abelian, and $N \\trianglelefteq G$ is cyclic. We prove that the commuting graph $\\Gamma(G)$ of $G$ is either a connected graph of diameter at most four, or the disjoint union of several complete graphs. These results apply to all finite metacyclic groups, and groups of square-free order in particular.","sentences":["Let $G$ be a finite, non-abelian group of the form $G =","A N$, where $A \\leq G$ is abelian, and $N \\trianglelefteq G$ is cyclic.","We prove that the commuting graph $\\Gamma(G)$ of $G$ is either a connected graph of diameter at most four, or the disjoint union of several complete graphs.","These results apply to all finite metacyclic groups, and groups of square-free order in particular."],"url":"http://arxiv.org/abs/2405.18103v1","category":"math.GR"}
{"created":"2024-05-28 12:08:37","title":"Anomalous dimensions for hard exclusive processes","abstract":"We give an overview of recent developments in the computation of the anomalous dimension matrix of composite operators in non-forward kinematics. The elements of this matrix determine the scale dependence of non-perturbative parton distributions, such as GPDs, and hence constitute important input for phenomenological studies of exclusive processes like deeply-virtual Compton scattering. Particular emphasis will be put on a recently developed method that exploits consistency relations for the anomalous dimension matrix which follow from the renormalization structure of the operators.","sentences":["We give an overview of recent developments in the computation of the anomalous dimension matrix of composite operators in non-forward kinematics.","The elements of this matrix determine the scale dependence of non-perturbative parton distributions, such as GPDs, and hence constitute important input for phenomenological studies of exclusive processes like deeply-virtual Compton scattering.","Particular emphasis will be put on a recently developed method that exploits consistency relations for the anomalous dimension matrix which follow from the renormalization structure of the operators."],"url":"http://arxiv.org/abs/2405.18101v1","category":"hep-ph"}
{"created":"2024-05-28 11:47:28","title":"Antiferromagnetic order of topological orbital moments in atomic-scale skyrmion lattices","abstract":"Topological orbital moments can arise in non-coplanar spin structures even in the absence of spin-orbit coupling and a net topological orbital magnetization occurs for the triple-Q state and for isolated skyrmions. For atomic-scale skyrmion lattices, a significant effect can also be expected, however, no studies have been reported yet. Here, we observe via spin-polarized scanning tunneling microscopy a non-coplanar atomic-scale spin structure with a nearly square magnetic unit cell for a pseudomorphic Fe monolayer on three atomic Ir layers on the Re(0001) surface. Employing density functional theory (DFT) calculations we consider different skyrmionic lattices to find the magnetic ground state. By mapping the DFT total energies to an atomistic spin model we demonstrate that these spin textures are stabilized by the interplay of the Dzyaloshinskii-Moriya and four-spin interactions. We evaluate the emerging phenomena of the different non-coplanar magnetic states and find significant local topological orbital moments oriented perpendicular to the surface, which order in an antiferromagnetic fashion.","sentences":["Topological orbital moments can arise in non-coplanar spin structures even in the absence of spin-orbit coupling and a net topological orbital magnetization occurs for the triple-Q state and for isolated skyrmions.","For atomic-scale skyrmion lattices, a significant effect can also be expected, however, no studies have been reported yet.","Here, we observe via spin-polarized scanning tunneling microscopy a non-coplanar atomic-scale spin structure with a nearly square magnetic unit cell for a pseudomorphic Fe monolayer on three atomic Ir layers on the Re(0001) surface.","Employing density functional theory (DFT) calculations we consider different skyrmionic lattices to find the magnetic ground state.","By mapping the DFT total energies to an atomistic spin model we demonstrate that these spin textures are stabilized by the interplay of the Dzyaloshinskii-Moriya and four-spin interactions.","We evaluate the emerging phenomena of the different non-coplanar magnetic states and find significant local topological orbital moments oriented perpendicular to the surface, which order in an antiferromagnetic fashion."],"url":"http://arxiv.org/abs/2405.18088v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-28 11:46:22","title":"A$^3$COSMOS: Measuring the cosmic dust-attenuated star formation rate density at $4 < z < 5$","abstract":"[Abridged] In recent years, conflicting results have provided an uncertain view of the dust-attenuated properties of $z>4$ star-forming galaxies (SFGs). To solve this, we used the deepest data publicly available in COSMOS to build a mass-complete ($>10^{9.5}\\,M_{\\odot}$) sample of SFGs at $4<z<5$ and measured their dust-attenuated properties by stacking all archival ALMA band 6 and 7 observations available. Combining this information with their rest-frame ultraviolet emission from the COSMOS2020 catalog, we constrained the IRX ($\\equiv L_{\\rm IR}/L_{\\rm UV}$)--$\\beta_{\\rm UV}$, IRX--$M_\\ast$, and SFR--$M_\\ast$ relations at $z\\sim4.5$. Finally, using these relations and the stellar mass function of SFGs at $z\\sim4.5$, we inferred the unattenuated and dust-attenuated SFRD at this epoch. SFGs at $z\\sim4.5$ follow an IRX--$\\beta_{\\rm UV}$ relation that is consistent with that of local starbursts, while they follow a steeper IRX--$M_\\ast$ relation than observed locally. The grain properties of dust in these SFGs seems thus similar to those in local starbursts but its mass and geometry result in lower attenuation in low-mass SFGs. SFGs at $z\\sim4.5$ lie on a linear SFR--$M_\\ast$ relation, whose normalization varies by 0.3 dex, when we exclude or include from our stacks the ALMA primary targets. The cosmic SFRD$(>M_\\ast)$ converges at $M_\\ast<10^{9}\\,M_\\odot$ and is dominated by SFGs with $M_\\ast\\sim10^{9.5-10.5}\\,M_\\odot$. The fraction of the cosmic SFRD that is attenuated by dust, ${\\rm SFRD}_{\\rm IR}(>M_\\ast)/ {\\rm SFRD}(>M_\\ast)$, is $90\\pm4\\%$ for $M_\\ast\\,=\\,10^{10}\\,M_\\odot$, $68\\pm10\\%$ for $M_\\ast=10^{8.9}\\,M_\\odot$ (i.e., $0.03\\times M^\\star$; $M^\\star$ being the characteristic stellar mass of SFGs) and this value converges to $60\\pm10\\%$ for $M_\\ast=10^{8}\\,M_\\odot$. Even at this early epoch, the fraction of the cosmic SFRD that is attenuated by dust remains thus significant.","sentences":["[Abridged] In recent years, conflicting results have provided an uncertain view of the dust-attenuated properties of $z>4$ star-forming galaxies (SFGs).","To solve this, we used the deepest data publicly available in COSMOS to build a mass-complete ($>10^{9.5}\\,M_{\\odot}$) sample of SFGs at $4<z<5$ and measured their dust-attenuated properties by stacking all archival ALMA band 6 and 7 observations available.","Combining this information with their rest-frame ultraviolet emission from the COSMOS2020 catalog, we constrained the IRX ($\\equiv L_{\\rm IR}/L_{\\rm UV}$)--$\\beta_{\\rm UV}$, IRX--$M_\\ast$, and SFR--$M_\\ast$ relations at $z\\sim4.5$. Finally, using these relations and the stellar mass function of SFGs at $z\\sim4.5$, we inferred the unattenuated and dust-attenuated SFRD at this epoch.","SFGs at $z\\sim4.5$ follow an IRX--$\\beta_{\\rm UV}$ relation that is consistent with that of local starbursts, while they follow a steeper IRX--$M_\\ast$ relation than observed locally.","The grain properties of dust in these SFGs seems thus similar to those in local starbursts but its mass and geometry result in lower attenuation in low-mass SFGs.","SFGs at $z\\sim4.5$ lie on a linear SFR--$M_\\ast$ relation, whose normalization varies by 0.3 dex, when we exclude or include from our stacks the ALMA primary targets.","The cosmic SFRD$(>M_\\ast)$ converges at $M_\\ast<10^{9}\\,M_\\odot$ and is dominated by SFGs with $M_\\ast\\sim10^{9.5-10.5}\\,M_\\odot$. The fraction of the cosmic SFRD that is attenuated by dust, ${\\rm SFRD}_{\\rm IR}(>M_\\ast)/ {\\rm SFRD}(>M_\\ast)$, is $90\\pm4\\%$ for $M_\\ast\\,=\\,10^{10}\\,M_\\odot$, $68\\pm10\\%$ for $M_\\ast=10^{8.9}\\,M_\\odot$ (i.e., $0.03\\times M^\\star$; $M^\\star$ being the characteristic stellar mass of SFGs) and this value converges to $60\\pm10\\%$ for $M_\\ast=10^{8}\\,M_\\odot$. Even at this early epoch, the fraction of the cosmic SFRD that is attenuated by dust remains thus significant."],"url":"http://arxiv.org/abs/2405.18086v1","category":"astro-ph.GA"}
{"created":"2024-05-28 11:09:37","title":"Forward $J/\u03c8+ J/\u03c8$ and $J/\u03c8+ \u03c8^\\prime$ production with High Energy Factorization","abstract":"We calculate the cross sections of associated $J/\\psi + \\psi^\\prime$ and $J/\\psi + J/\\psi$ production in $pp$ collisions at $\\sqrt s = 13$ TeV in the forward kinematic region. The High Energy Factorization ($k_T$-factorization) framework supplemented with the Catani-Ciafaloni-Fiorani-Marchesini evolution of gluon densities in a proton is applied. We demonstrate that latest data on $J/\\psi + J/\\psi$ production and first experimental data on $J/\\psi + \\psi^\\prime$ events taken very recently by the LHCb Collaboration can be described well by the color singlet terms and contributions from the double parton scattering (DPS) with the standard choice for $\\sigma_{\\rm eff}$ parameter. The relative production rate $\\sigma(J/\\psi + \\psi^\\prime)/\\sigma(J/\\psi + J/\\psi)$ is found to be sensitive to the DPS terms as well as to feeddown contributions.","sentences":["We calculate the cross sections of associated $J/\\psi + \\psi^\\prime$ and $J/\\psi + J/\\psi$ production in $pp$ collisions at $\\sqrt s = 13$ TeV in the forward kinematic region.","The High Energy Factorization ($k_T$-factorization) framework supplemented with the Catani-Ciafaloni-Fiorani-Marchesini evolution of gluon densities in a proton is applied.","We demonstrate that latest data on $J/\\psi + J/\\psi$ production and first experimental data on $J/\\psi + \\psi^\\prime$ events taken very recently by the LHCb Collaboration can be described well by the color singlet terms and contributions from the double parton scattering (DPS) with the standard choice for $\\sigma_{\\rm eff}$ parameter.","The relative production rate $\\sigma(J/\\psi + \\psi^\\prime)/\\sigma(J/\\psi + J/\\psi)$ is found to be sensitive to the DPS terms as well as to feeddown contributions."],"url":"http://arxiv.org/abs/2405.18054v1","category":"hep-ph"}
{"created":"2024-05-28 09:48:29","title":"Persistence Diagram Estimation : Beyond Plug-in Approaches","abstract":"Persistent homology is a tool from Topological Data Analysis (TDA) used to summarize the topology underlying data. It can be conveniently represented through persistence diagrams. Observing a noisy signal, common strategies to infer its persistence diagram involve plug-in estimators, and convergence properties are then derived from sup-norm stability. This dependence on the sup-norm convergence of the preliminary estimator is restrictive, as it essentially imposes to consider regular classes of signals. Departing from these approaches, we design an estimator based on image persistence. In the context of the Gaussian white noise model, and for large classes of piecewise-H\\\"older signals, we prove that the proposed estimator is consistent and achieves minimax rates. Notably, these rates coincide with the well known minimax rates for H\\\"older continuous signals.","sentences":["Persistent homology is a tool from Topological Data Analysis (TDA) used to summarize the topology underlying data.","It can be conveniently represented through persistence diagrams.","Observing a noisy signal, common strategies to infer its persistence diagram involve plug-in estimators, and convergence properties are then derived from sup-norm stability.","This dependence on the sup-norm convergence of the preliminary estimator is restrictive, as it essentially imposes to consider regular classes of signals.","Departing from these approaches, we design an estimator based on image persistence.","In the context of the Gaussian white noise model, and for large classes of piecewise-H\\\"older signals, we prove that the proposed estimator is consistent and achieves minimax rates.","Notably, these rates coincide with the well known minimax rates for H\\\"older continuous signals."],"url":"http://arxiv.org/abs/2405.18005v1","category":"math.ST"}
{"created":"2024-05-28 09:18:28","title":"Diagnostics of magnetohydrodynamic modes in the ISM through synchrotron polarization statistics","abstract":"One of the biggest challenges in understanding Magnetohydrodynamic (MHD) turbulence is identifying the plasma mode components from observational data. Previous studies on synchrotron polarization from the interstellar medium (ISM) suggest that the dominant MHD modes can be identified via statistics of Stokes parameters, which would be crucial for studying various ISM processes such as the scattering and acceleration of cosmic rays, star formation, dynamo. In this paper, we present a numerical study of the Synchrotron Polarization Analysis (SPA) method through systematic investigation of the statistical properties of the Stokes parameters. We derive the theoretical basis for our method from the fundamental statistics of MHD turbulence, recognizing that the projection of the MHD modes allows us to identify the modes dominating the energy fraction from synchrotron observations. Based on the discovery, we revise the SPA method using synthetic synchrotron polarization observations obtained from 3D ideal MHD simulations with a wide range of plasma parameters and driving mechanisms, and present a modified recipe for mode identification. We propose a classification criterion based on a new SPA+ fitting procedure, which allows us to distinguish between Alfv\\'en mode and compressible/slow mode dominated turbulence. We further propose a new method to identify fast modes by analyzing the asymmetry of the SPA+ signature and establish a new asymmetry parameter to detect the presence of fast mode turbulence. Additionally, we confirm through numerical tests that the identification of the compressible and fast modes is not affected by Faraday rotation in both the emitting plasma and the foreground.","sentences":["One of the biggest challenges in understanding Magnetohydrodynamic (MHD) turbulence is identifying the plasma mode components from observational data.","Previous studies on synchrotron polarization from the interstellar medium (ISM) suggest that the dominant MHD modes can be identified via statistics of Stokes parameters, which would be crucial for studying various ISM processes such as the scattering and acceleration of cosmic rays, star formation, dynamo.","In this paper, we present a numerical study of the Synchrotron Polarization Analysis (SPA) method through systematic investigation of the statistical properties of the Stokes parameters.","We derive the theoretical basis for our method from the fundamental statistics of MHD turbulence, recognizing that the projection of the MHD modes allows us to identify the modes dominating the energy fraction from synchrotron observations.","Based on the discovery, we revise the SPA method using synthetic synchrotron polarization observations obtained from 3D ideal MHD simulations with a wide range of plasma parameters and driving mechanisms, and present a modified recipe for mode identification.","We propose a classification criterion based on a new SPA+ fitting procedure, which allows us to distinguish between Alfv\\'en mode and compressible/slow mode dominated turbulence.","We further propose a new method to identify fast modes by analyzing the asymmetry of the SPA+ signature and establish a new asymmetry parameter to detect the presence of fast mode turbulence.","Additionally, we confirm through numerical tests that the identification of the compressible and fast modes is not affected by Faraday rotation in both the emitting plasma and the foreground."],"url":"http://arxiv.org/abs/2405.17985v1","category":"astro-ph.GA"}
{"created":"2024-05-28 08:57:52","title":"A Qualitative Analysis Framework for mHealth Privacy Practices","abstract":"Mobile Health (mHealth) applications have become a crucial part of health monitoring and management. However, the proliferation of these applications has also raised concerns over the privacy and security of Personally Identifiable Information and Protected Health Information. Addressing these concerns, this paper introduces a novel framework for the qualitative evaluation of privacy practices in mHealth apps, particularly focusing on the handling and transmission of sensitive user data. Our investigation encompasses an analysis of 152 leading mHealth apps on the Android platform, leveraging the proposed framework to provide a multifaceted view of their data processing activities. Despite stringent regulations like the General Data Protection Regulation in the European Union and the Health Insurance Portability and Accountability Act in the United States, our findings indicate persistent issues with negligence and misuse of sensitive user information. We uncover significant instances of health information leakage to third-party trackers and a widespread neglect of privacy-by-design and transparency principles. Our research underscores the critical need for stricter enforcement of data protection laws and sets a foundation for future efforts aimed at enhancing user privacy within the mHealth ecosystem.","sentences":["Mobile Health (mHealth) applications have become a crucial part of health monitoring and management.","However, the proliferation of these applications has also raised concerns over the privacy and security of Personally Identifiable Information and Protected Health Information.","Addressing these concerns, this paper introduces a novel framework for the qualitative evaluation of privacy practices in mHealth apps, particularly focusing on the handling and transmission of sensitive user data.","Our investigation encompasses an analysis of 152 leading mHealth apps on the Android platform, leveraging the proposed framework to provide a multifaceted view of their data processing activities.","Despite stringent regulations like the General Data Protection Regulation in the European Union and the Health Insurance Portability and Accountability Act in the United States, our findings indicate persistent issues with negligence and misuse of sensitive user information.","We uncover significant instances of health information leakage to third-party trackers and a widespread neglect of privacy-by-design and transparency principles.","Our research underscores the critical need for stricter enforcement of data protection laws and sets a foundation for future efforts aimed at enhancing user privacy within the mHealth ecosystem."],"url":"http://arxiv.org/abs/2405.17971v1","category":"cs.CY"}
{"created":"2024-05-28 08:57:35","title":"Elucidating nanostructural organisation and photonic properties of butterfly wing scales using hyperspectral microscopy","abstract":"Biophotonic nanostructures in butterfly wing scales remain fascinating examples of biological functional materials, with intriguing open questions in regards to formation and evolutionary function. One particularly interesting butterfly species, Erora opisena (Lycaenidae: Theclinae), develops wing scales that contain three-dimensional photonic crystals that closely resemble a single gyroid geometry. Unlike most other gyroid forming butterflies, E. opisena develops discrete gyroid crystallites with a pronounced size gradient hinting at a developmental sequence frozen in time. Here, we use a hyperspectral (wavelength-resolved) microscopy technique to investigate the ultrastructural organisation of these gyroid crystallites in dry, adult wing scales. We show that reflectance corresponds to crystallite size, where larger crystallites reflect green wavelengths more intensely; this relationship could be used to infer size from the optical signal. We further successfully resolve the red-shifted reflectance signal from wing scales immersed in refractive index oils with varying refractive index, including values similar to water or cytosol. Such photonic crystals with lower refractive index contrast may be similar to the hypothesized nanostructural forms in the developing butterfly scales. The ability to resolve these fainter signals hints at the potential of this facile light microscopy method for in vivo analysis of nanostructure formation in developing butterflies.","sentences":["Biophotonic nanostructures in butterfly wing scales remain fascinating examples of biological functional materials, with intriguing open questions in regards to formation and evolutionary function.","One particularly interesting butterfly species, Erora opisena (Lycaenidae: Theclinae), develops wing scales that contain three-dimensional photonic crystals that closely resemble a single gyroid geometry.","Unlike most other gyroid forming butterflies, E. opisena develops discrete gyroid crystallites with a pronounced size gradient hinting at a developmental sequence frozen in time.","Here, we use a hyperspectral (wavelength-resolved) microscopy technique to investigate the ultrastructural organisation of these gyroid crystallites in dry, adult wing scales.","We show that reflectance corresponds to crystallite size, where larger crystallites reflect green wavelengths more intensely; this relationship could be used to infer size from the optical signal.","We further successfully resolve the red-shifted reflectance signal from wing scales immersed in refractive index oils with varying refractive index, including values similar to water or cytosol.","Such photonic crystals with lower refractive index contrast may be similar to the hypothesized nanostructural forms in the developing butterfly scales.","The ability to resolve these fainter signals hints at the potential of this facile light microscopy method for in vivo analysis of nanostructure formation in developing butterflies."],"url":"http://arxiv.org/abs/2405.17970v1","category":"physics.optics"}
{"created":"2024-05-28 08:53:32","title":"Charge Amplification in Low Pressure CF4:SF6:He Mixtures with a Multi-Mesh ThGEM for Directional Dark Matter Searches","abstract":"The CYGNO collaboration is developing next generation directional Dark Matter (DM) detection experiments, using gaseous Time Projection Chambers (TPCs), as a robust method for identifying Weakly Interacting Massive Particles (WIMPs) below the Neutrino Fog. SF6 is potentially ideal for this since it provides a high fluorine content, enhancing sensitivity to spin-dependent interactions and, as a Negative Ion Drift (NID) gas, reduces charge diffusion leading to improved positional resolution. CF4, although not a NID gas, has also been identified as a favourable gas target as it provides a scintillation signal which can be used for a complimentary light/charge readout approach. These gases can operate at low pressures to elongate Nuclear Recoil (NR) tracks and facilitate directional measurements. In principle, He could be added to low pressure SF6/CF4 without significant detriment to the length of 16S, 12C, and 19F recoils. This would improve the target mass, sensitivity to lower WIMP masses, and offer the possibility of atmospheric operation; potentially reducing the cost of a containment vessel. In this article, we present gas gain and energy resolution measurements, taken with a Multi-Mesh Thick Gaseous Electron Multiplier (MMThGEM), in low pressure SF6 and CF4:SF6 mixtures following the addition of He. We find that the CF4:SF6:He mixtures tested were able to produce gas gains on the order of 10^4 up to a total pressure of 100 Torr. These results demonstrate an order of magnitude improvement in charge amplification in NID gas mixtures with a He component.","sentences":["The CYGNO collaboration is developing next generation directional Dark Matter (DM) detection experiments, using gaseous Time Projection Chambers (TPCs), as a robust method for identifying Weakly Interacting Massive Particles (WIMPs) below the Neutrino Fog. SF6 is potentially ideal for this since it provides a high fluorine content, enhancing sensitivity to spin-dependent interactions and, as a Negative Ion Drift (NID) gas, reduces charge diffusion leading to improved positional resolution.","CF4, although not a NID gas, has also been identified as a favourable gas target as it provides a scintillation signal which can be used for a complimentary light/charge readout approach.","These gases can operate at low pressures to elongate Nuclear Recoil (NR) tracks and facilitate directional measurements.","In principle, He could be added to low pressure SF6/CF4 without significant detriment to the length of 16S, 12C, and 19F recoils.","This would improve the target mass, sensitivity to lower WIMP masses, and offer the possibility of atmospheric operation; potentially reducing the cost of a containment vessel.","In this article, we present gas gain and energy resolution measurements, taken with a Multi-Mesh Thick Gaseous Electron Multiplier (MMThGEM), in low pressure SF6 and CF4:SF6 mixtures following the addition of He.","We find that the CF4:SF6:He mixtures tested were able to produce gas gains on the order of 10","^4 up to a total pressure of 100 Torr",".","These results demonstrate an order of magnitude improvement in charge amplification in NID gas mixtures with a He component."],"url":"http://arxiv.org/abs/2405.17967v1","category":"physics.ins-det"}
{"created":"2024-05-28 08:19:33","title":"Defining Root-$T\\overline{T}$","abstract":"We give a tentative definition of the recently introduced Root-$T\\bar{T}$ operator in a generic, two dimensional quantum conformal field theory with continuous spectrum of scaling weights. The definition assumes certain factorization properties and uses Schwinger parametrization to introduce the square root. The properties of the operator thus defined is investigated by explicit computation of variations o two- and three-point correlation functions.","sentences":["We give a tentative definition of the recently introduced Root-$T\\bar{T}$ operator in a generic, two dimensional quantum conformal field theory with continuous spectrum of scaling weights.","The definition assumes certain factorization properties and uses Schwinger parametrization to introduce the square root.","The properties of the operator thus defined is investigated by explicit computation of variations o two- and three-point correlation functions."],"url":"http://arxiv.org/abs/2405.17945v1","category":"hep-th"}
{"created":"2024-05-28 08:02:22","title":"Data-driven background model for the CUORE experiment","abstract":"We present the model we developed to reconstruct the CUORE radioactive background based on the analysis of an experimental exposure of 1038.4 kg yr. The data reconstruction relies on a simultaneous Bayesian fit applied to energy spectra over a broad energy range. The high granularity of the CUORE detector, together with the large exposure and extended stable operations, allow for an in-depth exploration of both spatial and time dependence of backgrounds. We achieve high sensitivity to both bulk and surface activities of the materials of the setup, detecting levels as low as 10 nBq kg$^{-1}$ and 0.1 nBq cm$^{-2}$, respectively. We compare the contamination levels we extract from the background model with prior radio-assay data, which informs future background risk mitigation strategies. The results of this background model play a crucial role in constructing the background budget for the CUPID experiment as it will exploit the same CUORE infrastructure.","sentences":["We present the model we developed to reconstruct the CUORE radioactive background based on the analysis of an experimental exposure of 1038.4 kg yr.","The data reconstruction relies on a simultaneous Bayesian fit applied to energy spectra over a broad energy range.","The high granularity of the CUORE detector, together with the large exposure and extended stable operations, allow for an in-depth exploration of both spatial and time dependence of backgrounds.","We achieve high sensitivity to both bulk and surface activities of the materials of the setup, detecting levels as low as 10 nBq kg$^{-1}$ and 0.1 nBq cm$^{-2}$, respectively.","We compare the contamination levels we extract from the background model with prior radio-assay data, which informs future background risk mitigation strategies.","The results of this background model play a crucial role in constructing the background budget for the CUPID experiment as it will exploit the same CUORE infrastructure."],"url":"http://arxiv.org/abs/2405.17937v1","category":"nucl-ex"}
{"created":"2024-05-28 07:37:44","title":"Boosting General Trimap-free Matting in the Real-World Image","abstract":"Image matting aims to obtain an alpha matte that separates foreground objects from the background accurately. Recently, trimap-free matting has been well studied because it requires only the original image without any extra input. Such methods usually extract a rough foreground by itself to take place trimap as further guidance. However, the definition of 'foreground' lacks a unified standard and thus ambiguities arise. Besides, the extracted foreground is sometimes incomplete due to inadequate network design. Most importantly, there is not a large-scale real-world matting dataset, and current trimap-free methods trained with synthetic images suffer from large domain shift problems in practice. In this paper, we define the salient object as foreground, which is consistent with human cognition and annotations of the current matting dataset. Meanwhile, data and technologies in salient object detection can be transferred to matting in a breeze. To obtain a more accurate and complete alpha matte, we propose a network called \\textbf{M}ulti-\\textbf{F}eature fusion-based \\textbf{C}oarse-to-fine Network \\textbf{(MFC-Net)}, which fully integrates multiple features for an accurate and complete alpha matte. Furthermore, we introduce image harmony in data composition to bridge the gap between synthetic and real images. More importantly, we establish the largest general matting dataset \\textbf{(Real-19k)} in the real world to date. Experiments show that our method is significantly effective on both synthetic and real-world images, and the performance in the real-world dataset is far better than existing matting-free methods. Our code and data will be released soon.","sentences":["Image matting aims to obtain an alpha matte that separates foreground objects from the background accurately.","Recently, trimap-free matting has been well studied because it requires only the original image without any extra input.","Such methods usually extract a rough foreground by itself to take place trimap as further guidance.","However, the definition of 'foreground' lacks a unified standard and thus ambiguities arise.","Besides, the extracted foreground is sometimes incomplete due to inadequate network design.","Most importantly, there is not a large-scale real-world matting dataset, and current trimap-free methods trained with synthetic images suffer from large domain shift problems in practice.","In this paper, we define the salient object as foreground, which is consistent with human cognition and annotations of the current matting dataset.","Meanwhile, data and technologies in salient object detection can be transferred to matting in a breeze.","To obtain a more accurate and complete alpha matte, we propose a network called \\textbf{M}ulti-\\textbf{F}eature fusion-based \\textbf{C}oarse-to-fine Network \\textbf{(MFC-Net)}, which fully integrates multiple features for an accurate and complete alpha matte.","Furthermore, we introduce image harmony in data composition to bridge the gap between synthetic and real images.","More importantly, we establish the largest general matting dataset \\textbf{(Real-19k)} in the real world to date.","Experiments show that our method is significantly effective on both synthetic and real-world images, and the performance in the real-world dataset is far better than existing matting-free methods.","Our code and data will be released soon."],"url":"http://arxiv.org/abs/2405.17916v1","category":"cs.CV"}
{"created":"2024-05-28 07:08:46","title":"Constraining vector dark matter and dark photon with degenerate mass in a hidden local SU(2) model","abstract":"We discuss degenerate vector dark matter and dark photon that are induced from hidden $SU(2)_H$ gauge sector where it is spontaneously broken by vacuum expectation value of $SU(2)_H$ doublet. Kinetic mixing between $SU(2)_H$ and $U(1)_Y$ gauge fields can be generated by introducing dimension six operator realizing dark photon interactions. In estimating relic density we focus on the process in which dark matter annihilates into dark photons, and search for the region of dark matter mass and gauge coupling realizing observed relic density. We then discuss constraints from dark photon physics, thermalization of dark sector and direct detection of dark matter. It is then found that constraints from direct detection experiments give us the strongest upper limits on the dark photon interactions.","sentences":["We discuss degenerate vector dark matter and dark photon that are induced from hidden $SU(2)_H$ gauge sector where it is spontaneously broken by vacuum expectation value of $SU(2)_H$ doublet.","Kinetic mixing between $SU(2)_H$ and $U(1)_Y$ gauge fields can be generated by introducing dimension six operator realizing dark photon interactions.","In estimating relic density we focus on the process in which dark matter annihilates into dark photons, and search for the region of dark matter mass and gauge coupling realizing observed relic density.","We then discuss constraints from dark photon physics, thermalization of dark sector and direct detection of dark matter.","It is then found that constraints from direct detection experiments give us the strongest upper limits on the dark photon interactions."],"url":"http://arxiv.org/abs/2405.17885v1","category":"hep-ph"}
{"created":"2024-05-28 06:29:02","title":"Towards Video Codec Performance Evaluation: A Rate-Energy-Distortion Perspective","abstract":"The Bj{\\o}ntegaard Delta rate (BD-rate) objectively assesses the coding efficiency of video codecs using the rate-distortion (R-D) performance but overlooks encoding energy, which is crucial in practical applications, especially for those on handheld devices. Although R-D analysis can be extended to incorporate encoding energy as energy-distortion (E-D), it fails to integrate all three parameters seamlessly. This work proposes a novel approach to address this limitation by introducing a 3D representation of rate, encoding energy, and distortion through surface fitting. In addition, we evaluate various surface fitting techniques based on their accuracy and investigate the proposed 3D representation and its projections. The overlapping areas in projections help in encoder selection and recommend avoiding the slow presets of the older encoders (x264, x265), as the recent encoders (x265, VVenC) offer higher quality for the same bitrate-energy performance and provide a lower rate for the same energy-distortion performance.","sentences":["The Bj{\\o}ntegaard Delta rate (BD-rate) objectively assesses the coding efficiency of video codecs using the rate-distortion (R-D) performance but overlooks encoding energy, which is crucial in practical applications, especially for those on handheld devices.","Although R-D analysis can be extended to incorporate encoding energy as energy-distortion (E-D), it fails to integrate all three parameters seamlessly.","This work proposes a novel approach to address this limitation by introducing a 3D representation of rate, encoding energy, and distortion through surface fitting.","In addition, we evaluate various surface fitting techniques based on their accuracy and investigate the proposed 3D representation and its projections.","The overlapping areas in projections help in encoder selection and recommend avoiding the slow presets of the older encoders (x264, x265), as the recent encoders (x265, VVenC) offer higher quality for the same bitrate-energy performance and provide a lower rate for the same energy-distortion performance."],"url":"http://arxiv.org/abs/2405.17866v1","category":"eess.IV"}
{"created":"2024-05-28 06:24:42","title":"Ferromagnetic ferroelectricity due to the Kugel-Khomskii mechanism of the orbital ordering assisted by atomic Hund's second rule effects","abstract":"The exchange interactions in insulators depend on the orbital state of magnetic ions, obeying certain phenomenological principles, known as Goodenough-Kanamori-Anderson rules. Particularly, the ferro order of alike orbitals tends to stabilize antiferromagnetic interactions, while the antiferro order of unlike orbitals favors ferromagnetic interactions. The Kugel-Khomskii theory provides a universal view on such coupling between spin and orbital degrees of freedom, based on the superexchange processes: namely, for a given magnetic order, the occupied orbitals tend to arrange in a way to further minimize the exchange energy. Then, if two magnetic sites are connected by the spatial inversion, the antiferro orbital order should lead to the ferromagnetic coupling and break the inversion symmetry. This constitutes the basic idea of our work, which opens a new route for designing ferromagnetic ferroelectrics - the rare but fundamentally and practically important multiferroic materials. After illustrating the basic idea on toy-model examples, we propose that such behavior can be indeed realized in the van der Waals ferromagnet VI$_3$, employing for this analysis the realistic model derived from first-principles calculations for magnetic $3d$ bands. We argue that the intraatomic Coulomb interactions responsible for Hund's second rule, acting against the crystal field, tend to restore the orbital degeneracy of the ionic $d^{2}$ state in VI$_3$ and, thus, provide a necessary flexibility for activating the Kugel-Khomskii mechanism of the orbital ordering. In the honeycomb lattice, this orbital ordering breaks the inversion symmetry, stabilizing the ferromagnetic-ferroelectric ground state. The symmetry breaking leads to the canting of magnetization, which can be further controlled by the magnetic field, producing a huge change of electric polarization.","sentences":["The exchange interactions in insulators depend on the orbital state of magnetic ions, obeying certain phenomenological principles, known as Goodenough-Kanamori-Anderson rules.","Particularly, the ferro order of alike orbitals tends to stabilize antiferromagnetic interactions, while the antiferro order of unlike orbitals favors ferromagnetic interactions.","The Kugel-Khomskii theory provides a universal view on such coupling between spin and orbital degrees of freedom, based on the superexchange processes: namely, for a given magnetic order, the occupied orbitals tend to arrange in a way to further minimize the exchange energy.","Then, if two magnetic sites are connected by the spatial inversion, the antiferro orbital order should lead to the ferromagnetic coupling and break the inversion symmetry.","This constitutes the basic idea of our work, which opens a new route for designing ferromagnetic ferroelectrics - the rare but fundamentally and practically important multiferroic materials.","After illustrating the basic idea on toy-model examples, we propose that such behavior can be indeed realized in the van der Waals ferromagnet VI$_3$, employing for this analysis the realistic model derived from first-principles calculations for magnetic $3d$ bands.","We argue that the intraatomic Coulomb interactions responsible for Hund's second rule, acting against the crystal field, tend to restore the orbital degeneracy of the ionic $d^{2}$ state in VI$_3$ and, thus, provide a necessary flexibility for activating the Kugel-Khomskii mechanism of the orbital ordering.","In the honeycomb lattice, this orbital ordering breaks the inversion symmetry, stabilizing the ferromagnetic-ferroelectric ground state.","The symmetry breaking leads to the canting of magnetization, which can be further controlled by the magnetic field, producing a huge change of electric polarization."],"url":"http://arxiv.org/abs/2405.17864v1","category":"cond-mat.str-el"}
{"created":"2024-05-28 06:18:15","title":"Strong limits on keV-scale galactic sterile neutrino dark matter with stray light from NuSTAR after 11 years of operation","abstract":"Using tremendous photon statistics gained with the stray light aperture of the NuSTAR telescope over 11 years of operation, we set strong limits on the emission of close to monochromatic photons from the radiative decays of putative dark matter sterile neutrinos in the Milky Way. In the energy range of 3-20 keV covered by the NuSTAR, the obtained limits reach the edge of theoretical predictions of realistic models leaving only a small room left to explore.","sentences":["Using tremendous photon statistics gained with the stray light aperture of the NuSTAR telescope over 11 years of operation, we set strong limits on the emission of close to monochromatic photons from the radiative decays of putative dark matter sterile neutrinos in the Milky Way.","In the energy range of 3-20 keV covered by the NuSTAR, the obtained limits reach the edge of theoretical predictions of realistic models leaving only a small room left to explore."],"url":"http://arxiv.org/abs/2405.17861v1","category":"hep-ph"}
{"created":"2024-05-28 06:17:52","title":"Prediction of Energy Resolution in the JUNO Experiment","abstract":"This paper presents the energy resolution study in the JUNO experiment, incorporating the latest knowledge acquired during the detector construction phase. The determination of neutrino mass ordering in JUNO requires an exceptional energy resolution better than 3\\% at 1 MeV. To achieve this ambitious goal, significant efforts have been undertaken in the design and production of the key components of the JUNO detector. Various factors affecting the detection of inverse beta decay signals have an impact on the energy resolution, extending beyond the statistical fluctuations of the detected number of photons, such as the properties of liquid scintillator, performance of photomultiplier tubes, and the energy reconstruction algorithm. To account for these effects, a full JUNO simulation and reconstruction approach is employed. This enables the modeling of all relevant effects and the evaluation of associated inputs to accurately estimate the energy resolution. The study reveals an energy resolution of 2.95\\% at 1 MeV. Furthermore, the study assesses the contribution of major effects to the overall energy resolution budget. This analysis serves as a reference for interpreting future measurements of energy resolution during JUNO data taking. Moreover, it provides a guideline in comprehending the energy resolution characteristics of liquid scintillator-based detectors.","sentences":["This paper presents the energy resolution study in the JUNO experiment, incorporating the latest knowledge acquired during the detector construction phase.","The determination of neutrino mass ordering in JUNO requires an exceptional energy resolution better than 3\\% at 1 MeV.","To achieve this ambitious goal, significant efforts have been undertaken in the design and production of the key components of the JUNO detector.","Various factors affecting the detection of inverse beta decay signals have an impact on the energy resolution, extending beyond the statistical fluctuations of the detected number of photons, such as the properties of liquid scintillator, performance of photomultiplier tubes, and the energy reconstruction algorithm.","To account for these effects, a full JUNO simulation and reconstruction approach is employed.","This enables the modeling of all relevant effects and the evaluation of associated inputs to accurately estimate the energy resolution.","The study reveals an energy resolution of 2.95\\% at 1 MeV.","Furthermore, the study assesses the contribution of major effects to the overall energy resolution budget.","This analysis serves as a reference for interpreting future measurements of energy resolution during JUNO data taking.","Moreover, it provides a guideline in comprehending the energy resolution characteristics of liquid scintillator-based detectors."],"url":"http://arxiv.org/abs/2405.17860v1","category":"hep-ex"}
{"created":"2024-05-28 06:07:22","title":"Advances in laser-plasma interactions using intense vortex laser beams","abstract":"Low-intensity light beams carrying Orbital Angular Momentum (OAM), commonly known as vortex beams, have garnered significant attention due to promising applications in areas ranging from optical trapping to communication. In recent years, there has been a surge in global research exploring the potential of high-intensity vortex laser beams and specifically their interactions with plasmas. This paper provides a comprehensive review of recent advances in this area. Compared to conventional laser beams, intense vortex beams exhibit unique properties such as twisted phase fronts, OAM delivery, hollow intensity distribution, and spatially isolated longitudinal fields. These distinct characteristics give rise to a multitude of rich phenomena, profoundly influencing laser-plasma interactions and offering diverse applications. The paper also discusses future prospects and identifies promising general research areas involving vortex beams. These areas include low-divergence particle acceleration, instability suppression, high-energy photon delivery with OAM, and the generation of strong magnetic fields. With growing scientific interest and application potential, the study of intense vortex lasers is poised for rapid development in the coming years.","sentences":["Low-intensity light beams carrying Orbital Angular Momentum (OAM), commonly known as vortex beams, have garnered significant attention due to promising applications in areas ranging from optical trapping to communication.","In recent years, there has been a surge in global research exploring the potential of high-intensity vortex laser beams and specifically their interactions with plasmas.","This paper provides a comprehensive review of recent advances in this area.","Compared to conventional laser beams, intense vortex beams exhibit unique properties such as twisted phase fronts, OAM delivery, hollow intensity distribution, and spatially isolated longitudinal fields.","These distinct characteristics give rise to a multitude of rich phenomena, profoundly influencing laser-plasma interactions and offering diverse applications.","The paper also discusses future prospects and identifies promising general research areas involving vortex beams.","These areas include low-divergence particle acceleration, instability suppression, high-energy photon delivery with OAM, and the generation of strong magnetic fields.","With growing scientific interest and application potential, the study of intense vortex lasers is poised for rapid development in the coming years."],"url":"http://arxiv.org/abs/2405.17852v1","category":"physics.plasm-ph"}
{"created":"2024-05-28 06:05:00","title":"Shedding Light on Dark Sectors with Gravitational Waves","abstract":"The nature of dark matter remains one of the greatest unsolved mysteries in elementary particle physics. It might well be that the dark matter particle belongs to a dark sector completely secluded or extremely weakly coupled to the visible sector. We demonstrate that gravitational waves arising from first order phase transitions in the early Universe can be used to look for signatures of such dark sector models connected to neutron physics. This introduces a new connection between gravitational wave physics and nuclear physics experiments. Focusing on two particular extensions of the Standard Model with dark U(1) and SU(2) gauge groups constructed to address the neutron lifetime puzzle, we show how those signatures can be searched for in future gravitational wave and astrometry experiments.","sentences":["The nature of dark matter remains one of the greatest unsolved mysteries in elementary particle physics.","It might well be that the dark matter particle belongs to a dark sector completely secluded or extremely weakly coupled to the visible sector.","We demonstrate that gravitational waves arising from first order phase transitions in the early Universe can be used to look for signatures of such dark sector models connected to neutron physics.","This introduces a new connection between gravitational wave physics and nuclear physics experiments.","Focusing on two particular extensions of the Standard Model with dark U(1) and SU(2) gauge groups constructed to address the neutron lifetime puzzle, we show how those signatures can be searched for in future gravitational wave and astrometry experiments."],"url":"http://arxiv.org/abs/2405.17851v1","category":"hep-ph"}
{"created":"2024-05-28 05:04:04","title":"Agarose Derived Carbon Based Nanocomposite for Hydrogen Storage at Near-Ambient Conditions","abstract":"Nanocomposites comprising of high surface area adsorption materials and nanosized transition metals have emerged as a promising strategy for hydrogen storage application due to their inherent ability to store atomic and molecular forms of hydrogen by invoking mechanisms like physisorption and spillover mechanism or Kubas interaction. The potential use of these materials for both transport and stationary applications depends on reaching the ultimate storage capacity and scalability. In addition to achieving good hydrogen storage capacity, it is also vital to explore novel and efficient synthesis routes to control the microstructure. Herein, a direct and simple thermal decomposition technique is reported to synthesize carbon-based nanocomposites, where nickel nanoparticles are dispersed in a porous carbon matrix. The structure, morphology, composition and nature of bonding in the samples were investigated using transmission electron microscopy, scanning electron microscopy, energy dispersive spectroscopy, X-ray diffraction and Raman spectroscopy. Sorption-desorption isotherms were used to study the hydrogen storage capacity of the nanocomposites at a moderate H2 pressure of 20 bar. Among the various nanocomposites examined, the best obtained storage capacity was 0.73 wt.% (against 0.11 wt.% for pure carbon sample) at 298 K with reversible cyclability. It is shown that the uniform dispersion of catalytic nanoparticles along with a high surface area carbon matrix helps in the enhancement of hydrogen storage capacity by a factor of 6.5 times over pure carbon.","sentences":["Nanocomposites comprising of high surface area adsorption materials and nanosized transition metals have emerged as a promising strategy for hydrogen storage application due to their inherent ability to store atomic and molecular forms of hydrogen by invoking mechanisms like physisorption and spillover mechanism or Kubas interaction.","The potential use of these materials for both transport and stationary applications depends on reaching the ultimate storage capacity and scalability.","In addition to achieving good hydrogen storage capacity, it is also vital to explore novel and efficient synthesis routes to control the microstructure.","Herein, a direct and simple thermal decomposition technique is reported to synthesize carbon-based nanocomposites, where nickel nanoparticles are dispersed in a porous carbon matrix.","The structure, morphology, composition and nature of bonding in the samples were investigated using transmission electron microscopy, scanning electron microscopy, energy dispersive spectroscopy, X-ray diffraction and Raman spectroscopy.","Sorption-desorption isotherms were used to study the hydrogen storage capacity of the nanocomposites at a moderate H2 pressure of 20 bar.","Among the various nanocomposites examined, the best obtained storage capacity was 0.73 wt.% (against 0.11 wt.% for pure carbon sample) at 298 K with reversible cyclability.","It is shown that the uniform dispersion of catalytic nanoparticles along with a high surface area carbon matrix helps in the enhancement of hydrogen storage capacity by a factor of 6.5 times over pure carbon."],"url":"http://arxiv.org/abs/2405.17831v1","category":"cond-mat.mtrl-sci"}
