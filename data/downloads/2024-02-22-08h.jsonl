{"created":"2024-02-21 18:56:03","title":"D-Flow: Differentiating through Flows for Controlled Generation","abstract":"Taming the generation outcome of state of the art Diffusion and Flow-Matching (FM) models without having to re-train a task-specific model unlocks a powerful tool for solving inverse problems, conditional generation, and controlled generation in general. In this work we introduce D-Flow, a simple framework for controlling the generation process by differentiating through the flow, optimizing for the source (noise) point. We motivate this framework by our key observation stating that for Diffusion/FM models trained with Gaussian probability paths, differentiating through the generation process projects gradient on the data manifold, implicitly injecting the prior into the optimization process. We validate our framework on linear and non-linear controlled generation problems including: image and audio inverse problems and conditional molecule generation reaching state of the art performance across all.","sentences":["Taming the generation outcome of state of the art Diffusion and Flow-Matching (FM) models without having to re-train a task-specific model unlocks a powerful tool for solving inverse problems, conditional generation, and controlled generation in general.","In this work we introduce D-Flow, a simple framework for controlling the generation process by differentiating through the flow, optimizing for the source (noise) point.","We motivate this framework by our key observation stating that for Diffusion/FM models trained with Gaussian probability paths, differentiating through the generation process projects gradient on the data manifold, implicitly injecting the prior into the optimization process.","We validate our framework on linear and non-linear controlled generation problems including: image and audio inverse problems and conditional molecule generation reaching state of the art performance across all."],"url":"http://arxiv.org/abs/2402.14017v1","category":"cs.LG"}
{"created":"2024-02-21 18:54:37","title":"Corrective Machine Unlearning","abstract":"Machine Learning models increasingly face data integrity challenges due to the use of large-scale training datasets drawn from the internet. We study what model developers can do if they detect that some data was manipulated or incorrect. Such manipulated data can cause adverse effects like vulnerability to backdoored samples, systematic biases, and in general, reduced accuracy on certain input domains. Often, all manipulated training samples are not known, and only a small, representative subset of the affected data is flagged.   We formalize \"Corrective Machine Unlearning\" as the problem of mitigating the impact of data affected by unknown manipulations on a trained model, possibly knowing only a subset of impacted samples. We demonstrate that the problem of corrective unlearning has significantly different requirements from traditional privacy-oriented unlearning. We find most existing unlearning methods, including the gold-standard retraining-from-scratch, require most of the manipulated data to be identified for effective corrective unlearning. However, one approach, SSD, achieves limited success in unlearning adverse effects with just a small portion of the manipulated samples, showing the tractability of this setting. We hope our work spurs research towards developing better methods for corrective unlearning and offers practitioners a new strategy to handle data integrity challenges arising from web-scale training.","sentences":["Machine Learning models increasingly face data integrity challenges due to the use of large-scale training datasets drawn from the internet.","We study what model developers can do if they detect that some data was manipulated or incorrect.","Such manipulated data can cause adverse effects like vulnerability to backdoored samples, systematic biases, and in general, reduced accuracy on certain input domains.","Often, all manipulated training samples are not known, and only a small, representative subset of the affected data is flagged.   ","We formalize \"Corrective Machine Unlearning\" as the problem of mitigating the impact of data affected by unknown manipulations on a trained model, possibly knowing only a subset of impacted samples.","We demonstrate that the problem of corrective unlearning has significantly different requirements from traditional privacy-oriented unlearning.","We find most existing unlearning methods, including the gold-standard retraining-from-scratch, require most of the manipulated data to be identified for effective corrective unlearning.","However, one approach, SSD, achieves limited success in unlearning adverse effects with just a small portion of the manipulated samples, showing the tractability of this setting.","We hope our work spurs research towards developing better methods for corrective unlearning and offers practitioners a new strategy to handle data integrity challenges arising from web-scale training."],"url":"http://arxiv.org/abs/2402.14015v1","category":"cs.LG"}
{"created":"2024-02-21 18:52:45","title":"JWST MIRI Imager Observations of Supernova SN 1987A","abstract":"There exist very few mid-infrared (IR) observations of supernovae (SNe) in general. Therefore, SN 1987A, the closest visible SN in 400 years, gives us the opportunity to explore the mid-IR properties of SNe, the dust in their ejecta and surrounding medium, and to witness the birth of a SN remnant (SNR). The James Webb Space Telescope (JWST), with its high spatial resolution and extreme sensitivity, gives a new view on these issues. We report on the first imaging observations obtained with the Mid-InfraRed Instrument (MIRI). We build temperature maps and discuss the morphology of the nascent SNR. Our results show that the temperatures in the equatorial ring (ER) are quite non-uniform. This could be due to dust destruction in some parts of the ring, as had been assumed in some previous works. We show that the IR emission extends beyond the ER, illustrating the fact that the shock wave has now passed through this ring to affect the circumstellar medium on a larger scale. Finally, while sub-mm Atacama Large Millimeter Array (ALMA) observations have hinted at the location of the compact remnant of SN 1987A, we note that our MIRI data have found no such evidence.","sentences":["There exist very few mid-infrared (IR) observations of supernovae (SNe) in general.","Therefore, SN 1987A, the closest visible SN in 400 years, gives us the opportunity to explore the mid-IR properties of SNe, the dust in their ejecta and surrounding medium, and to witness the birth of a SN remnant (SNR).","The James Webb Space Telescope (JWST), with its high spatial resolution and extreme sensitivity, gives a new view on these issues.","We report on the first imaging observations obtained with the Mid-InfraRed Instrument (MIRI).","We build temperature maps and discuss the morphology of the nascent SNR.","Our results show that the temperatures in the equatorial ring (ER) are quite non-uniform.","This could be due to dust destruction in some parts of the ring, as had been assumed in some previous works.","We show that the IR emission extends beyond the ER, illustrating the fact that the shock wave has now passed through this ring to affect the circumstellar medium on a larger scale.","Finally, while sub-mm Atacama Large Millimeter Array (ALMA) observations have hinted at the location of the compact remnant of SN 1987A, we note that our MIRI data have found no such evidence."],"url":"http://arxiv.org/abs/2402.14014v1","category":"astro-ph.SR"}
{"created":"2024-02-21 18:51:02","title":"Spectral mod p Satake isomorphism for GL_n","abstract":"Let $K/\\mathbb{Q}_p$ be a finite extension with residue field $k$. By a work of Emerton--Gee, irreducible components inside the reduced special fiber of the moduli stack of rank $n$ \\'etale $(\\varphi,\\Gamma)$-modules are labeled by Serre weights of $\\mathrm{GL}_n(k)$. Let $\\sigma$ be a non-Steinberg Serre weight and $\\mathcal{C}_\\sigma$ be the corresponding irreducible component. Motivated by the categorical $p$-adic local Langlands program, we construct a natural injective map $\\mathcal{O}(\\mathcal{C}_\\sigma) \\hookrightarrow \\mathcal{H}(\\sigma)$ from the ring of global functions on $\\mathcal{C}_\\sigma$ to the Hecke algebra of $\\sigma$ compatible with the mod $p$ Satake isomorphism by Herzig and Henniart--Vign\\'eras in a suitable sense. For sufficiently generic $\\sigma$, we prove that it is an isomorphism. As an application, we obtain a natural stratification of the irreducible component whose strata are equipped with a parabolic structure. Our main input is a construction of a morphism from an integral Hecke algebra of a generic tame type to the ring of global functions on a tamely potentially crystalline Emerton--Gee stack.","sentences":["Let $K/\\mathbb{Q}_p$ be a finite extension with residue field $k$. By a work of Emerton--Gee, irreducible components inside the reduced special fiber of the moduli stack of rank $n$ \\'etale $(\\varphi,\\Gamma)$-modules are labeled by Serre weights of $\\mathrm{GL}_n(k)$. Let $\\sigma$ be a non-Steinberg Serre weight and $\\mathcal{C}_\\sigma$ be the corresponding irreducible component.","Motivated by the categorical $p$-adic local Langlands program, we construct a natural injective map $\\mathcal{O}(\\mathcal{C}_\\sigma) \\hookrightarrow \\mathcal{H}(\\sigma)$ from the ring of global functions on $\\mathcal{C}_\\sigma$ to the Hecke algebra of $\\sigma$ compatible with the mod $p$ Satake isomorphism by Herzig and Henniart--Vign\\'eras in a suitable sense.","For sufficiently generic $\\sigma$, we prove that it is an isomorphism.","As an application, we obtain a natural stratification of the irreducible component whose strata are equipped with a parabolic structure.","Our main input is a construction of a morphism from an integral Hecke algebra of a generic tame type to the ring of global functions on a tamely potentially crystalline Emerton--Gee stack."],"url":"http://arxiv.org/abs/2402.14011v1","category":"math.NT"}
{"created":"2024-02-21 18:50:12","title":"Geometry-Informed Neural Networks","abstract":"We introduce the concept of geometry-informed neural networks (GINNs), which encompass (i) learning under geometric constraints, (ii) neural fields as a suitable representation, and (iii) generating diverse solutions to under-determined systems often encountered in geometric tasks. Notably, the GINN formulation does not require training data, and as such can be considered generative modeling driven purely by constraints. We add an explicit diversity loss to mitigate mode collapse. We consider several constraints, in particular, the connectedness of components which we convert to a differentiable loss through Morse theory. Experimentally, we demonstrate the efficacy of the GINN learning paradigm across a range of two and three-dimensional scenarios with increasing levels of complexity.","sentences":["We introduce the concept of geometry-informed neural networks (GINNs), which encompass (i) learning under geometric constraints, (ii) neural fields as a suitable representation, and (iii) generating diverse solutions to under-determined systems often encountered in geometric tasks.","Notably, the GINN formulation does not require training data, and as such can be considered generative modeling driven purely by constraints.","We add an explicit diversity loss to mitigate mode collapse.","We consider several constraints, in particular, the connectedness of components which we convert to a differentiable loss through Morse theory.","Experimentally, we demonstrate the efficacy of the GINN learning paradigm across a range of two and three-dimensional scenarios with increasing levels of complexity."],"url":"http://arxiv.org/abs/2402.14009v1","category":"cs.LG"}
{"created":"2024-02-21 18:49:26","title":"OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems","abstract":"Recent advancements have seen Large Language Models (LLMs) and Large Multimodal Models (LMMs) surpassing general human capabilities in various tasks, approaching the proficiency level of human experts across multiple domains. With traditional benchmarks becoming less challenging for these models, new rigorous challenges are essential to gauge their advanced abilities. In this work, we present OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring 8,952 problems from Olympiad-level mathematics and physics competitions, including the Chinese college entrance exam. Each problem is detailed with expert-level annotations for step-by-step reasoning. Evaluating top-tier models on OlympiadBench, we implement a comprehensive assessment methodology to accurately evaluate model responses. Notably, the best-performing model, GPT-4V, attains an average score of 17.23% on OlympiadBench, with a mere 11.28% in physics, highlighting the benchmark rigor and the intricacy of physical reasoning. Our analysis orienting GPT-4V points out prevalent issues with hallucinations, knowledge omissions, and logical fallacies. We hope that our challenging benchmark can serve as a valuable resource for helping future AGI research endeavors.","sentences":["Recent advancements have seen Large Language Models (LLMs) and Large Multimodal Models (LMMs) surpassing general human capabilities in various tasks, approaching the proficiency level of human experts across multiple domains.","With traditional benchmarks becoming less challenging for these models, new rigorous challenges are essential to gauge their advanced abilities.","In this work, we present OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring 8,952 problems from Olympiad-level mathematics and physics competitions, including the Chinese college entrance exam.","Each problem is detailed with expert-level annotations for step-by-step reasoning.","Evaluating top-tier models on OlympiadBench, we implement a comprehensive assessment methodology to accurately evaluate model responses.","Notably, the best-performing model, GPT-4V, attains an average score of 17.23% on OlympiadBench, with a mere 11.28% in physics, highlighting the benchmark rigor and the intricacy of physical reasoning.","Our analysis orienting GPT-4V points out prevalent issues with hallucinations, knowledge omissions, and logical fallacies.","We hope that our challenging benchmark can serve as a valuable resource for helping future AGI research endeavors."],"url":"http://arxiv.org/abs/2402.14008v1","category":"cs.CL"}
{"created":"2024-02-21 18:48:38","title":"Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models","abstract":"Text watermarking technology aims to tag and identify content produced by large language models (LLMs) to prevent misuse. In this study, we introduce the concept of ''cross-lingual consistency'' in text watermarking, which assesses the ability of text watermarks to maintain their effectiveness after being translated into other languages. Preliminary empirical results from two LLMs and three watermarking methods reveal that current text watermarking technologies lack consistency when texts are translated into various languages. Based on this observation, we propose a Cross-lingual Watermark Removal Attack (CWRA) to bypass watermarking by first obtaining a response from an LLM in a pivot language, which is then translated into the target language. CWRA can effectively remove watermarks by reducing the Area Under the Curve (AUC) from 0.95 to 0.67 without performance loss. Furthermore, we analyze two key factors that contribute to the cross-lingual consistency in text watermarking and propose a defense method that increases the AUC from 0.67 to 0.88 under CWRA.","sentences":["Text watermarking technology aims to tag and identify content produced by large language models (LLMs) to prevent misuse.","In this study, we introduce the concept of ''cross-lingual consistency'' in text watermarking, which assesses the ability of text watermarks to maintain their effectiveness after being translated into other languages.","Preliminary empirical results from two LLMs and three watermarking methods reveal that current text watermarking technologies lack consistency when texts are translated into various languages.","Based on this observation, we propose a Cross-lingual Watermark Removal Attack (CWRA) to bypass watermarking by first obtaining a response from an LLM in a pivot language, which is then translated into the target language.","CWRA can effectively remove watermarks by reducing the Area Under the Curve (AUC) from 0.95 to 0.67 without performance loss.","Furthermore, we analyze two key factors that contribute to the cross-lingual consistency in text watermarking and propose a defense method that increases the AUC from 0.67 to 0.88 under CWRA."],"url":"http://arxiv.org/abs/2402.14007v1","category":"cs.CL"}
{"created":"2024-02-21 18:42:47","title":"On a theorem of B. Keller on Yoneda algebras of simple modules","abstract":"A theorem of Keller states that the Yoneda algebra of the simple modules over a finite-dimensional algebra is generated in cohomological degrees $0$ and $1$ as a minimal $A_\\infty$-algebra. We provide a proof of an extension of Keller's theorem to abelian length categories by reducing the problem to a particular class of Nakayama algebras, where the claim can be shown by direct computation.","sentences":["A theorem of Keller states that the Yoneda algebra of the simple modules over a finite-dimensional algebra is generated in cohomological degrees $0$ and $1$ as a minimal $A_\\infty$-algebra.","We provide a proof of an extension of Keller's theorem to abelian length categories by reducing the problem to a particular class of Nakayama algebras, where the claim can be shown by direct computation."],"url":"http://arxiv.org/abs/2402.14004v1","category":"math.RT"}
{"created":"2024-02-21 18:40:24","title":"Hallucinations or Attention Misdirection? The Path to Strategic Value Extraction in Business Using Large Language Models","abstract":"Large Language Models with transformer architecture have revolutionized the domain of text generation, setting unprecedented benchmarks. Despite their impressive capabilities, LLMs have been criticized for generating outcomes that deviate from factual accuracy or display logical inconsistencies, phenomena commonly referred to as hallucinations. This term, however, has often been misapplied to any results deviating from the instructor's expectations, which this paper defines as attention misdirection rather than true hallucinations. Understanding the distinction between hallucinations and attention misdirection becomes increasingly relevant in business contexts, where the ramifications of such errors can significantly impact the value extraction from these inherently pre-trained models. This paper highlights the best practices of the PGI, Persona, Grouping, and Intelligence, method, a strategic framework that achieved a remarkable error rate of only 3,15 percent across 4,000 responses generated by GPT in response to a real business challenge. It emphasizes that by equipping experimentation with knowledge, businesses can unlock opportunities for innovation through the use of these natively pre-trained models. This reinforces the notion that strategic application grounded in a skilled team can maximize the benefits of emergent technologies such as the LLMs.","sentences":["Large Language Models with transformer architecture have revolutionized the domain of text generation, setting unprecedented benchmarks.","Despite their impressive capabilities, LLMs have been criticized for generating outcomes that deviate from factual accuracy or display logical inconsistencies, phenomena commonly referred to as hallucinations.","This term, however, has often been misapplied to any results deviating from the instructor's expectations, which this paper defines as attention misdirection rather than true hallucinations.","Understanding the distinction between hallucinations and attention misdirection becomes increasingly relevant in business contexts, where the ramifications of such errors can significantly impact the value extraction from these inherently pre-trained models.","This paper highlights the best practices of the PGI, Persona, Grouping, and Intelligence, method, a strategic framework that achieved a remarkable error rate of only 3,15 percent across 4,000 responses generated by GPT in response to a real business challenge.","It emphasizes that by equipping experimentation with knowledge, businesses can unlock opportunities for innovation through the use of these natively pre-trained models.","This reinforces the notion that strategic application grounded in a skilled team can maximize the benefits of emergent technologies such as the LLMs."],"url":"http://arxiv.org/abs/2402.14002v1","category":"cs.CL"}
{"created":"2024-02-21 18:36:26","title":"Real-time 3D-aware Portrait Editing from a Single Image","abstract":"This work presents 3DPE, a practical tool that can efficiently edit a face image following given prompts, like reference images or text descriptions, in the 3D-aware manner. To this end, a lightweight module is distilled from a 3D portrait generator and a text-to-image model, which provide prior knowledge of face geometry and open-vocabulary editing capability, respectively. Such a design brings two compelling advantages over existing approaches. First, our system achieves real-time editing with a feedforward network (i.e., ~0.04s per image), over 100x faster than the second competitor. Second, thanks to the powerful priors, our module could focus on the learning of editing-related variations, such that it manages to handle various types of editing simultaneously in the training phase and further supports fast adaptation to user-specified novel types of editing during inference (e.g., with ~5min fine-tuning per case). The code, the model, and the interface will be made publicly available to facilitate future research.","sentences":["This work presents 3DPE, a practical tool that can efficiently edit a face image following given prompts, like reference images or text descriptions, in the 3D-aware manner.","To this end, a lightweight module is distilled from a 3D portrait generator and a text-to-image model, which provide prior knowledge of face geometry and open-vocabulary editing capability, respectively.","Such a design brings two compelling advantages over existing approaches.","First, our system achieves real-time editing with a feedforward network (i.e., ~0.04s per image), over 100x faster than the second competitor.","Second, thanks to the powerful priors, our module could focus on the learning of editing-related variations, such that it manages to handle various types of editing simultaneously in the training phase and further supports fast adaptation to user-specified novel types of editing during inference (e.g., with ~5min fine-tuning per case).","The code, the model, and the interface will be made publicly available to facilitate future research."],"url":"http://arxiv.org/abs/2402.14000v1","category":"cs.CV"}
{"created":"2024-02-21 18:31:24","title":"The distribution of $\\gcd(n,\u03c6(n))$","abstract":"Let $\\phi(n)$denote Euler's phi function. We study the distribution of the numbers $gcd(n,\\phi(n))$ and their divisors. Our results generalize previous results of Erd\\\"{o}s and Pollack.","sentences":["Let $\\phi(n)$denote Euler's phi function.","We study the distribution of the numbers $gcd(n,\\phi(n))$ and their divisors.","Our results generalize previous results of Erd\\\"{o}s and Pollack."],"url":"http://arxiv.org/abs/2402.13997v1","category":"math.NT"}
{"created":"2024-02-21 18:28:27","title":"Binary dynamics to second post-Newtonian order in scalar-tensor and Einstein-scalar-Gauss-Bonnet gravity from effective field theory","abstract":"Using effective field theory methods, we compute in detail the Lagrangian for the conservative dynamics of compact binary systems, for spinless constituents and in the gravitationally bound case, in massless scalar-tensor (ST) and Einstein-scalar-Gauss-Bonnet gravity (EsGB) to the second post-Newtonian (2PN) order. We employ the Kaluza-Klein parameterization of the metric, and demonstrate that, also in this case, a significant reduction in the number of diagrams is achieved. Still, to the 2PN accuracy, additional 39 diagrams involving scalar interactions must be added to the pure general relativistic result. Diagrams involving the Gauss-Bonnet term at the formal 1PN order are computed and shown to scale as a 3PN correction when observational constraints are taken into account. We also investigate at which order the additional $\\mathcal{O}(G^3)$ topologies contribute. Finally, the results derived in this paper provide an essential step toward the correct derivation of the conservative dynamics at the 3PN level in ST and EsGB theories.","sentences":["Using effective field theory methods, we compute in detail the Lagrangian for the conservative dynamics of compact binary systems, for spinless constituents and in the gravitationally bound case, in massless scalar-tensor (ST) and Einstein-scalar-Gauss-Bonnet gravity (EsGB) to the second post-Newtonian (2PN) order.","We employ the Kaluza-Klein parameterization of the metric, and demonstrate that, also in this case, a significant reduction in the number of diagrams is achieved.","Still, to the 2PN accuracy, additional 39 diagrams involving scalar interactions must be added to the pure general relativistic result.","Diagrams involving the Gauss-Bonnet term at the formal 1PN order are computed and shown to scale as a 3PN correction when observational constraints are taken into account.","We also investigate at which order the additional $\\mathcal{O}(G^3)$ topologies contribute.","Finally, the results derived in this paper provide an essential step toward the correct derivation of the conservative dynamics at the 3PN level in ST and EsGB theories."],"url":"http://arxiv.org/abs/2402.13996v1","category":"gr-qc"}
{"created":"2024-02-21 18:26:25","title":"Clifford circuits over non-cyclic abelian groups","abstract":"We present a discussion of the generalized Clifford group over non-cyclic finite abelian groups. These Clifford groups appear naturally in the theory of topological error correction and abelian anyon models. We demonstrate a generalized Gottesman-Knill theorem, stating that every Clifford circuit can be efficiently classically simulated. We additionally provide circuits for a universal quantum computing scheme based on local two-qudit Clifford gates and magic states.","sentences":["We present a discussion of the generalized Clifford group over non-cyclic finite abelian groups.","These Clifford groups appear naturally in the theory of topological error correction and abelian anyon models.","We demonstrate a generalized Gottesman-Knill theorem, stating that every Clifford circuit can be efficiently classically simulated.","We additionally provide circuits for a universal quantum computing scheme based on local two-qudit Clifford gates and magic states."],"url":"http://arxiv.org/abs/2402.13994v1","category":"quant-ph"}
{"created":"2024-02-21 18:21:21","title":"A General Theory of Static Response for Markov Jump Processes","abstract":"We consider Markov jump processes on a graph described by a rate matrix that depends on various control parameters. We derive explicit expressions for the static responses of edge currents and steady-state probabilities. We show that they are constrained by the graph topology (i.e. the incidence matrix) by deriving response relations (i.e. linear constraints linking the different responses) and topology-dependent bounds. For unicyclic networks, all scaled current sensitivities are between zero and one and must sum to one. Applying these results to stochastic thermodynamics, we derive explicit expressions for the static response of fundamental currents (which carry the full dissipation) to fundamental thermodynamic forces (which drive the system away from equilibrium).","sentences":["We consider Markov jump processes on a graph described by a rate matrix that depends on various control parameters.","We derive explicit expressions for the static responses of edge currents and steady-state probabilities.","We show that they are constrained by the graph topology (i.e. the incidence matrix) by deriving response relations (i.e. linear constraints linking the different responses) and topology-dependent bounds.","For unicyclic networks, all scaled current sensitivities are between zero and one and must sum to one.","Applying these results to stochastic thermodynamics, we derive explicit expressions for the static response of fundamental currents (which carry the full dissipation) to fundamental thermodynamic forces (which drive the system away from equilibrium)."],"url":"http://arxiv.org/abs/2402.13990v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-21 18:12:07","title":"Stability-Aware Training of Neural Network Interatomic Potentials with Differentiable Boltzmann Estimators","abstract":"Neural network interatomic potentials (NNIPs) are an attractive alternative to ab-initio methods for molecular dynamics (MD) simulations. However, they can produce unstable simulations which sample unphysical states, limiting their usefulness for modeling phenomena occurring over longer timescales. To address these challenges, we present Stability-Aware Boltzmann Estimator (StABlE) Training, a multi-modal training procedure which combines conventional supervised training from quantum-mechanical energies and forces with reference system observables, to produce stable and accurate NNIPs. StABlE Training iteratively runs MD simulations to seek out unstable regions, and corrects the instabilities via supervision with a reference observable. The training procedure is enabled by the Boltzmann Estimator, which allows efficient computation of gradients required to train neural networks to system observables, and can detect both global and local instabilities. We demonstrate our methodology across organic molecules, tetrapeptides, and condensed phase systems, along with using three modern NNIP architectures. In all three cases, StABlE-trained models achieve significant improvements in simulation stability and recovery of structural and dynamic observables. In some cases, StABlE-trained models outperform conventional models trained on datasets 50 times larger. As a general framework applicable across NNIP architectures and systems, StABlE Training is a powerful tool for training stable and accurate NNIPs, particularly in the absence of large reference datasets.","sentences":["Neural network interatomic potentials (NNIPs) are an attractive alternative to ab-initio methods for molecular dynamics (MD) simulations.","However, they can produce unstable simulations which sample unphysical states, limiting their usefulness for modeling phenomena occurring over longer timescales.","To address these challenges, we present Stability-Aware Boltzmann Estimator (StABlE) Training, a multi-modal training procedure which combines conventional supervised training from quantum-mechanical energies and forces with reference system observables, to produce stable and accurate NNIPs.","StABlE Training iteratively runs MD simulations to seek out unstable regions, and corrects the instabilities via supervision with a reference observable.","The training procedure is enabled by the Boltzmann Estimator, which allows efficient computation of gradients required to train neural networks to system observables, and can detect both global and local instabilities.","We demonstrate our methodology across organic molecules, tetrapeptides, and condensed phase systems, along with using three modern NNIP architectures.","In all three cases, StABlE-trained models achieve significant improvements in simulation stability and recovery of structural and dynamic observables.","In some cases, StABlE-trained models outperform conventional models trained on datasets 50 times larger.","As a general framework applicable across NNIP architectures and systems, StABlE Training is a powerful tool for training stable and accurate NNIPs, particularly in the absence of large reference datasets."],"url":"http://arxiv.org/abs/2402.13984v1","category":"cs.LG"}
{"created":"2024-02-21 18:09:57","title":"Specht property for the graded identities of the pair $(M_2(D), sl_2(D))$","abstract":"Let $D$ be a Noetherian infinite integral domain, denote by $M_2(D)$ and by $sl_2(D)$ the $2\\times 2$ matrix algebra and the Lie algebra of the traceless matrices in $M_2(D)$, respectively. In this paper we study the natural grading by the cyclic group $\\mathbb{Z}_2$ of order 2 on $M_2(D)$ and on $sl_2(D)$. We describe a finite basis of the graded polynomial identities for the pair $(M_2(D), sl_2(D))$. Moreover we prove that the ideal of the graded identities for this pair satisfies the Specht property, that is every ideal of graded identities of pairs (associative algebra, Lie algebra), satisfying the graded identities for $(M_2(D), sl_2(D))$, is finitely generated. The polynomial identities for $M_2(D)$ are known if $D$ is any field of characteristic different from 2. The identities for the Lie algebra $sl_2(D)$ are known when $D$ is an infinite field. The identities for the pair we consider were first described by Razmyslov when $D$ is a field of characteristic 0, and afterwards by the second author when $D$ is an infinite field. The graded identities for the pair $(M_2(D), gl_2(D))$ were also described, by Krasilnikov and the second author.   In order to obtain these results we use certain graded analogues of the generic matrices, and also techniques developed by G. Higman concerning partially well ordered sets.","sentences":["Let $D$ be a Noetherian infinite integral domain, denote by $M_2(D)$ and by $sl_2(D)$ the $2\\times 2$ matrix algebra and the Lie algebra of the traceless matrices in $M_2(D)$, respectively.","In this paper we study the natural grading by the cyclic group $\\mathbb{Z}_2$ of order 2 on $M_2(D)$ and on $sl_2(D)$. We describe a finite basis of the graded polynomial identities for the pair $(M_2(D), sl_2(D))$.","Moreover we prove that the ideal of the graded identities for this pair satisfies the Specht property, that is every ideal of graded identities of pairs (associative algebra, Lie algebra), satisfying the graded identities for $(M_2(D), sl_2(D))$, is finitely generated.","The polynomial identities for $M_2(D)$ are known if $D$ is any field of characteristic different from 2.","The identities for the Lie algebra $sl_2(D)$ are known when $D$ is an infinite field.","The identities for the pair we consider were first described by Razmyslov when $D$ is a field of characteristic 0, and afterwards by the second author when $D$ is an infinite field.","The graded identities for the pair $(M_2(D), gl_2(D))$ were also described, by Krasilnikov and the second author.   ","In order to obtain these results we use certain graded analogues of the generic matrices, and also techniques developed by G. Higman concerning partially well ordered sets."],"url":"http://arxiv.org/abs/2402.13982v1","category":"math.RA"}
{"created":"2024-02-21 18:09:37","title":"Plug-and-play analytical paradigm for the scattering of plane waves by \"layer-cake\" periodic systems","abstract":"We investigate the scattering of scalar plane waves in two dimensions by a heterogeneous layer that is periodic in the direction parallel to its boundary. On describing the layer as a union of periodic laminae, we develop a solution of the scattering problem by merging the concept of propagator matrices and that of Bloch eigenstates featured by the unit cell of each lamina. The featured Bloch eigenstates are obtained by solving the quadratic eigenvalue problem (QEVP) that seeks a complex-valued wavenumber normal to the layer boundary given (i) the excitation frequency, and (ii) real-valued wavenumber parallel to the boundary -- that is preserved throughout the system. Spectral analysis of the QEVP reveals sufficient conditions for discreteness of the eigenvalue spectrum and the fact that all eigenvalues come in complex-conjugate pairs. By deploying the factorization afforded by the propagator matrix approach, we demonstrate that the contribution of individual eigenvalues (and so eigenmodes) to the solution diminishes exponentially with absolute value of their imaginary part, which then forms a rational basis for truncation of the factorized Bloch-wave solution. The proposed methodology caters for the optimal design of rainbow traps, energy harvesters, and metasurfaces, whose potency to manipulate waves is decided not only by the individual dispersion characteristics of the component laminae, but also by ordering and generally fitting of the latter into a composite layer. Using the factorized Bloch approach, evaluation of trial configurations -- as generated by the permutation and window translation/stretching of the component laminae -- can be accelerated by decades.","sentences":["We investigate the scattering of scalar plane waves in two dimensions by a heterogeneous layer that is periodic in the direction parallel to its boundary.","On describing the layer as a union of periodic laminae, we develop a solution of the scattering problem by merging the concept of propagator matrices and that of Bloch eigenstates featured by the unit cell of each lamina.","The featured Bloch eigenstates are obtained by solving the quadratic eigenvalue problem (QEVP) that seeks a complex-valued wavenumber normal to the layer boundary given (i) the excitation frequency, and (ii) real-valued wavenumber parallel to the boundary -- that is preserved throughout the system.","Spectral analysis of the QEVP reveals sufficient conditions for discreteness of the eigenvalue spectrum and the fact that all eigenvalues come in complex-conjugate pairs.","By deploying the factorization afforded by the propagator matrix approach, we demonstrate that the contribution of individual eigenvalues (and so eigenmodes) to the solution diminishes exponentially with absolute value of their imaginary part, which then forms a rational basis for truncation of the factorized Bloch-wave solution.","The proposed methodology caters for the optimal design of rainbow traps, energy harvesters, and metasurfaces, whose potency to manipulate waves is decided not only by the individual dispersion characteristics of the component laminae, but also by ordering and generally fitting of the latter into a composite layer.","Using the factorized Bloch approach, evaluation of trial configurations -- as generated by the permutation and window translation/stretching of the component laminae -- can be accelerated by decades."],"url":"http://arxiv.org/abs/2402.13981v1","category":"math-ph"}
{"created":"2024-02-21 18:09:04","title":"The Importance of Architecture Choice in Deep Learning for Climate Applications","abstract":"Machine Learning has become a pervasive tool in climate science applications. However, current models fail to address nonstationarity induced by anthropogenic alterations in greenhouse emissions and do not routinely quantify the uncertainty of proposed projections. In this paper, we model the Atlantic Meridional Overturning Circulation (AMOC) which is of major importance to climate in Europe and the US East Coast by transporting warm water to these regions, and has the potential for abrupt collapse. We can generate arbitrarily extreme climate scenarios through arbitrary time scales which we then predict using neural networks. Our analysis shows that the AMOC is predictable using neural networks under a diverse set of climate scenarios. Further experiments reveal that MLPs and Deep Ensembles can learn the physics of the AMOC instead of imitating its progression through autocorrelation. With quantified uncertainty, an intriguing pattern of \"spikes\" before critical points of collapse in the AMOC casts doubt on previous analyses that predicted an AMOC collapse within this century. Our results show that Bayesian Neural Networks perform poorly compared to more dense architectures and care should be taken when applying neural networks to nonstationary scenarios such as climate projections. Further, our results highlight that big NN models might have difficulty in modeling global Earth System dynamics accurately and be successfully applied in nonstationary climate scenarios due to the physics being challenging for neural networks to capture.","sentences":["Machine Learning has become a pervasive tool in climate science applications.","However, current models fail to address nonstationarity induced by anthropogenic alterations in greenhouse emissions and do not routinely quantify the uncertainty of proposed projections.","In this paper, we model the Atlantic Meridional Overturning Circulation (AMOC) which is of major importance to climate in Europe and the US East Coast by transporting warm water to these regions, and has the potential for abrupt collapse.","We can generate arbitrarily extreme climate scenarios through arbitrary time scales which we then predict using neural networks.","Our analysis shows that the AMOC is predictable using neural networks under a diverse set of climate scenarios.","Further experiments reveal that MLPs and Deep Ensembles can learn the physics of the AMOC instead of imitating its progression through autocorrelation.","With quantified uncertainty, an intriguing pattern of \"spikes\" before critical points of collapse in the AMOC casts doubt on previous analyses that predicted an AMOC collapse within this century.","Our results show that Bayesian Neural Networks perform poorly compared to more dense architectures and care should be taken when applying neural networks to nonstationary scenarios such as climate projections.","Further, our results highlight that big NN models might have difficulty in modeling global Earth System dynamics accurately and be successfully applied in nonstationary climate scenarios due to the physics being challenging for neural networks to capture."],"url":"http://arxiv.org/abs/2402.13979v1","category":"cs.LG"}
{"created":"2024-02-21 18:07:58","title":"Web bases in degree two from hourglass plabic graphs","abstract":"Webs give a diagrammatic calculus for spaces of $U_q(\\mathfrak{sl}_r)$-tensor invariants, but intrinsic characterizations of web bases are only known in certain cases. Recently, we introduced hourglass plabic graphs to give the first such $U_q(\\mathfrak{sl}_4)$-web bases. Separately, Fraser introduced a web basis for Pl\\\"{u}cker degree two representations of arbitrary $U_q(\\mathfrak{sl}_r)$. Here, we show that Fraser's basis agrees with that predicted by the hourglass plabic graph framework and give an intrinsic characterization of the resulting webs. A further compelling feature with many applications is that our bases exhibit rotation-invariance. Together with the results of our earlier paper, this implies that hourglass plabic graphs give a uniform description of all known rotation-invariant $U_q(\\mathfrak{sl}_r)$-web bases. Moreover, this provides a single combinatorial model simultaneously generalizing the Tamari lattice, the alternating sign matrix lattice, and the lattice of plane partitions. As a part of our argument, we develop properties of square faces in arbitrary hourglass plabic graphs, a key step in our program towards general $U_q(\\mathfrak{sl}_r)$-web bases.","sentences":["Webs give a diagrammatic calculus for spaces of $U_q(\\mathfrak{sl}_r)$-tensor invariants, but intrinsic characterizations of web bases are only known in certain cases.","Recently, we introduced hourglass plabic graphs to give the first such $U_q(\\mathfrak{sl}_4)$-web bases.","Separately, Fraser introduced a web basis for Pl\\\"{u}cker degree two representations of arbitrary $U_q(\\mathfrak{sl}_r)$. Here, we show that Fraser's basis agrees with that predicted by the hourglass plabic graph framework and give an intrinsic characterization of the resulting webs.","A further compelling feature with many applications is that our bases exhibit rotation-invariance.","Together with the results of our earlier paper, this implies that hourglass plabic graphs give a uniform description of all known rotation-invariant $U_q(\\mathfrak{sl}_r)$-web bases.","Moreover, this provides a single combinatorial model simultaneously generalizing the Tamari lattice, the alternating sign matrix lattice, and the lattice of plane partitions.","As a part of our argument, we develop properties of square faces in arbitrary hourglass plabic graphs, a key step in our program towards general $U_q(\\mathfrak{sl}_r)$-web bases."],"url":"http://arxiv.org/abs/2402.13978v1","category":"math.CO"}
{"created":"2024-02-21 18:07:12","title":"Relative entropy and modulated free energy without confinement via self-similar transformation","abstract":"This note extends the modulated entropy and free energy methods for proving mean-field limits/propagation of chaos to the whole space without any confining potential, in contrast to previous work limited to the torus or requiring confinement in the whole space, for all log/Riesz flows. Our novel idea is a scale transformation, sometimes called self-similar coordinates in the PDE literature, which converts the problem to one with a quadratic confining potential, up to a time-dependent renormalization of the interaction potential. In these self-similar coordinates, one can then establish a Gr\\\"onwall relation for the relative entropy or modulated free energy, conditional on bounds for the Hessian of the mean-field log density. This generalizes recent work of Feng-Wang arXiv:2310.05156, which extended the Jabin-Wang relative entropy method to the whole space for the viscous vortex model. Moreover, in contrast to previous work, our approach allows to obtain uniform-in-time propagation of chaos and even polynomial-in-time generation of chaos in the whole space without confinement, provided one has suitable decay estimates for the mean-field log density. The desired regularity bounds and decay estimates are the subject of a companion paper.","sentences":["This note extends the modulated entropy and free energy methods for proving mean-field limits/propagation of chaos to the whole space without any confining potential, in contrast to previous work limited to the torus or requiring confinement in the whole space, for all log/Riesz flows.","Our novel idea is a scale transformation, sometimes called self-similar coordinates in the PDE literature, which converts the problem to one with a quadratic confining potential, up to a time-dependent renormalization of the interaction potential.","In these self-similar coordinates, one can then establish a Gr\\\"onwall relation for the relative entropy or modulated free energy, conditional on bounds for the Hessian of the mean-field log density.","This generalizes recent work of Feng-Wang arXiv:2310.05156, which extended the Jabin-Wang relative entropy method to the whole space for the viscous vortex model.","Moreover, in contrast to previous work, our approach allows to obtain uniform-in-time propagation of chaos and even polynomial-in-time generation of chaos in the whole space without confinement, provided one has suitable decay estimates for the mean-field log density.","The desired regularity bounds and decay estimates are the subject of a companion paper."],"url":"http://arxiv.org/abs/2402.13977v1","category":"math.AP"}
{"created":"2024-02-21 18:03:38","title":"Non-Markovian maximal couplings and a vertical reflection principle on a class of sub-Riemannian manifolds","abstract":"We develop non-Markovian, non-co-adapted couplings for sub-Riemannian Brownian motions in various sub-Riemannian manifolds, namely the three-dimensional Heisenberg group, higher-dimensional non-isotropic Heisenberg groups, SL(2,R) and its universal cover, and SU(2,C). Our primary focus is on the situation when the processes start from two points on the same vertical fiber, since in general co-adapted couplings cannot give the sharp rate for the coupling time in this case. Then for general points, we use this vertical coupling as the second stage of a two-stage coupling. Non-Markovian couplings in this context were first introduced by Banerjee-Gordina-Mariano, for the three-dimensional Heisenberg group, and were more recently extended by B\\'en\\'efice to SL(2,R) and SU(2,C), using a detailed consideration of the Brownian bridge. In contrast, our couplings are based on global isometries of the space, giving couplings that are maximal, as well as making the construction relatively simple and uniform across different manifolds. Moreover, this construction gives a coupling time that equals a hitting time for the vertical component of one of the Brownian motions, and this vertical component satisfies a reflection principle, which is useful in explicitly bounding the tail probability of the coupling time. We estimate the coupling time in these various situations and give applications to inequalities for the heat semigroup.","sentences":["We develop non-Markovian, non-co-adapted couplings for sub-Riemannian Brownian motions in various sub-Riemannian manifolds, namely the three-dimensional Heisenberg group, higher-dimensional non-isotropic Heisenberg groups, SL(2,R) and its universal cover, and SU(2,C).","Our primary focus is on the situation when the processes start from two points on the same vertical fiber, since in general co-adapted couplings cannot give the sharp rate for the coupling time in this case.","Then for general points, we use this vertical coupling as the second stage of a two-stage coupling.","Non-Markovian couplings in this context were first introduced by Banerjee-Gordina-Mariano, for the three-dimensional Heisenberg group, and were more recently extended by B\\'en\\'efice to SL(2,R) and SU(2,C), using a detailed consideration of the Brownian bridge.","In contrast, our couplings are based on global isometries of the space, giving couplings that are maximal, as well as making the construction relatively simple and uniform across different manifolds.","Moreover, this construction gives a coupling time that equals a hitting time for the vertical component of one of the Brownian motions, and this vertical component satisfies a reflection principle, which is useful in explicitly bounding the tail probability of the coupling time.","We estimate the coupling time in these various situations and give applications to inequalities for the heat semigroup."],"url":"http://arxiv.org/abs/2402.13976v1","category":"math.PR"}
{"created":"2024-02-21 17:57:53","title":"Two-dimensional Lorentz-violating Casimir effect","abstract":"In this study, we consider the four-dimensional Maxwell electrodynamics extended with CPT-even Myers-Pospelov Lorentz-violating dimension-six operators to investigate the associated two-dimensional properties in the context of quantum vacuum fluctuation effects, namely, the Casimir effect. Upon projecting out the 4D theory down to a 2D theory we obtain analogs of these operators leading to a modified dispersion relation in a Lorentz invariance violation (LIV) scalar model equivalent to the electromagnetic theory. By making use of the modified dispersion relation, we derive exact analytic expressions for the Casimir energy and force induced by imposing Dirichlet boundary conditions on the scalar field. In the regime where the LIV parameter becomes very small, we recover known results for the Casimir energy and force plus correction terms due to the LIV.","sentences":["In this study, we consider the four-dimensional Maxwell electrodynamics extended with CPT-even Myers-Pospelov Lorentz-violating dimension-six operators to investigate the associated two-dimensional properties in the context of quantum vacuum fluctuation effects, namely, the Casimir effect.","Upon projecting out the 4D theory down to a 2D theory we obtain analogs of these operators leading to a modified dispersion relation in a Lorentz invariance violation (LIV) scalar model equivalent to the electromagnetic theory.","By making use of the modified dispersion relation, we derive exact analytic expressions for the Casimir energy and force induced by imposing Dirichlet boundary conditions on the scalar field.","In the regime where the LIV parameter becomes very small, we recover known results for the Casimir energy and force plus correction terms due to the LIV."],"url":"http://arxiv.org/abs/2402.13972v1","category":"hep-th"}
{"created":"2024-02-21 17:56:51","title":"Birational geometry of Calabi-Yau pairs $(\\mathbb{P}^3, D)$ of coregularity 2","abstract":"This paper aims to study the birational geometry of log Calabi-Yau pairs$(\\mathbb{P}^3, D)$ of coregularity 2, where in this case $D$ is an irreducible normal quartic surface with canonical singularities. We completely classify which toric weighted blowups of a point will initiate a volume preserving Sarkisov link starting with this pair. Depending on the type of singularity, our results point out that some of these weights do not work generically for a general member of the corresponding coarse moduli space of quartics.","sentences":["This paper aims to study the birational geometry of log Calabi-Yau pairs$(\\mathbb{P}^3, D)$ of coregularity 2, where in this case $D$ is an irreducible normal quartic surface with canonical singularities.","We completely classify which toric weighted blowups of a point will initiate a volume preserving Sarkisov link starting with this pair.","Depending on the type of singularity, our results point out that some of these weights do not work generically for a general member of the corresponding coarse moduli space of quartics."],"url":"http://arxiv.org/abs/2402.13970v1","category":"math.AG"}
{"created":"2024-02-21 17:55:44","title":"On the decomposition group of a nonsingular plane cubic by a log Calabi-Yau geometrical perspective","abstract":"This paper aims to study the decomposition group of a nonsingular plane cubic under the light of the log Calabi-Yau geometry. Using this approach we prove that an appropriate algorithm of the Sarkisov Program in dimension 2 applied to an element of this group is automatically volume preserving. From this, we deduce some properties of the (volume preserving) Sarkisov factorization of its elements. We also negatively answer a question posed by Blanc, Pan and Vust asking whether the canonical complex of a nonsingular plane cubic is split. Within a similar context em dimension 3, we exhibit in detail an interesting counterexample for a possible generalization of a theorem by Pan in which there exists a Sarkisov factorization obtained algorithmically that is not volume preserving.","sentences":["This paper aims to study the decomposition group of a nonsingular plane cubic under the light of the log Calabi-Yau geometry.","Using this approach we prove that an appropriate algorithm of the Sarkisov Program in dimension 2 applied to an element of this group is automatically volume preserving.","From this, we deduce some properties of the (volume preserving)","Sarkisov factorization of its elements.","We also negatively answer a question posed by Blanc, Pan and Vust asking whether the canonical complex of a nonsingular plane cubic is split.","Within a similar context em dimension 3, we exhibit in detail an interesting counterexample for a possible generalization of a theorem by Pan in which there exists a Sarkisov factorization obtained algorithmically that is not volume preserving."],"url":"http://arxiv.org/abs/2402.13968v1","category":"math.AG"}
{"created":"2024-02-21 17:47:20","title":"Towards Building Multilingual Language Model for Medicine","abstract":"In this paper, we aim to develop an open-source, multilingual language model for medicine, that the benefits a wider, linguistically diverse audience from different regions. In general, we present the contribution from the following aspects: first, for multilingual medical-specific adaptation, we construct a new multilingual medical corpus, that contains approximately 25.5B tokens encompassing 6 main languages, termed as MMedC, that enables auto-regressive training for existing general LLMs. second, to monitor the development of multilingual LLMs in medicine, we propose a new multilingual medical multi-choice question-answering benchmark with rationale, termed as MMedBench; third, we have assessed a number of popular, opensource large language models (LLMs) on our benchmark, along with those further auto-regressive trained on MMedC, as a result, our final model, termed as MMedLM 2, with only 7B parameters, achieves superior performance compared to all other open-source models, even rivaling GPT-4 on MMedBench. We will make the resources publicly available, including code, model weights, and datasets.","sentences":["In this paper, we aim to develop an open-source, multilingual language model for medicine, that the benefits a wider, linguistically diverse audience from different regions.","In general, we present the contribution from the following aspects: first, for multilingual medical-specific adaptation, we construct a new multilingual medical corpus, that contains approximately 25.5B tokens encompassing 6 main languages, termed as MMedC, that enables auto-regressive training for existing general LLMs.","second, to monitor the development of multilingual LLMs in medicine, we propose a new multilingual medical multi-choice question-answering benchmark with rationale, termed as MMedBench; third, we have assessed a number of popular, opensource large language models (LLMs) on our benchmark, along with those further auto-regressive trained on MMedC, as a result, our final model, termed as MMedLM 2, with only 7B parameters, achieves superior performance compared to all other open-source models, even rivaling GPT-4 on MMedBench.","We will make the resources publicly available, including code, model weights, and datasets."],"url":"http://arxiv.org/abs/2402.13963v1","category":"cs.CL"}
{"created":"2024-02-21 17:40:51","title":"Improving threshold for fault-tolerant color code quantum computing by flagged weight optimization","abstract":"Color codes are promising quantum error correction (QEC) codes because they have an advantage over surface codes in that all Clifford gates can be implemented transversally. However, thresholds of color codes under circuit-level noise are relatively low mainly because measurements of their high-weight stabilizer generators cause an increase in a circuit depth, and thus, substantial errors are introduced. This makes color codes not the best candidate. Here, we propose a method to suppress the impact of such errors by optimizing weights of decoders using flag qubits and reducing the circuit depth using cat states. We set the weights based on conditional error probabilities conditioned on the measurement outcomes of flag qubits. In numerical simulations, we improve the threshold of the (4.8.8) color code under the circuit-level noise from 0.14% to around 0.27%, which is calculated by using an integer programming decoder. Furthermore, in the (6.6.6) color code, we achieved a circuit-level threshold of around 0.36%, which is almost the same value as the highest value in the previous studies employing the same noise model. In both cases, the achieved logical error rates at low physical error rates are almost one order of magnitude lower than a conventional method that uses a single ancilla qubit for each stabilizer measurement. This method can also be applied to other weight-based decoders, making the color codes more promising for the candidate of experimental implementation of QEC. Furthermore, one can utilize this approach to improve a threshold of wider classes of QEC codes, such as high-rate quantum low-density parity check codes.","sentences":["Color codes are promising quantum error correction (QEC) codes because they have an advantage over surface codes in that all Clifford gates can be implemented transversally.","However, thresholds of color codes under circuit-level noise are relatively low mainly because measurements of their high-weight stabilizer generators cause an increase in a circuit depth, and thus, substantial errors are introduced.","This makes color codes not the best candidate.","Here, we propose a method to suppress the impact of such errors by optimizing weights of decoders using flag qubits and reducing the circuit depth using cat states.","We set the weights based on conditional error probabilities conditioned on the measurement outcomes of flag qubits.","In numerical simulations, we improve the threshold of the (4.8.8) color code under the circuit-level noise from 0.14% to around 0.27%, which is calculated by using an integer programming decoder.","Furthermore, in the (6.6.6) color code, we achieved a circuit-level threshold of around 0.36%, which is almost the same value as the highest value in the previous studies employing the same noise model.","In both cases, the achieved logical error rates at low physical error rates are almost one order of magnitude lower than a conventional method that uses a single ancilla qubit for each stabilizer measurement.","This method can also be applied to other weight-based decoders, making the color codes more promising for the candidate of experimental implementation of QEC.","Furthermore, one can utilize this approach to improve a threshold of wider classes of QEC codes, such as high-rate quantum low-density parity check codes."],"url":"http://arxiv.org/abs/2402.13958v1","category":"quant-ph"}
{"created":"2024-02-21 17:36:07","title":"Can You Learn Semantics Through Next-Word Prediction? The Case of Entailment","abstract":"Do LMs infer the semantics of text from co-occurrence patterns in their training data? Merrill et al. (2022) argue that, in theory, probabilities predicted by an optimal LM encode semantic information about entailment relations, but it is unclear whether neural LMs trained on corpora learn entailment in this way because of strong idealizing assumptions made by Merrill et al. In this work, we investigate whether their theory can be used to decode entailment judgments from neural LMs. We find that a test similar to theirs can decode entailment relations between natural sentences, well above random chance, though not perfectly, across many datasets and LMs. This suggests LMs implicitly model aspects of semantics to predict semantic effects on sentence co-occurrence patterns. However, we find the test that predicts entailment in practice works in the opposite direction to the theoretical test. We thus revisit the assumptions underlying the original test, finding its derivation did not adequately account for redundancy in human-written text. We argue that correctly accounting for redundancy related to explanations might derive the observed flipped test and, more generally, improve linguistic theories of human speakers.","sentences":["Do LMs infer the semantics of text from co-occurrence patterns in their training data?","Merrill et al. (2022) argue that, in theory, probabilities predicted by an optimal LM encode semantic information about entailment relations, but it is unclear whether neural LMs trained on corpora learn entailment in this way because of strong idealizing assumptions made by Merrill et al.","In this work, we investigate whether their theory can be used to decode entailment judgments from neural LMs.","We find that a test similar to theirs can decode entailment relations between natural sentences, well above random chance, though not perfectly, across many datasets and LMs.","This suggests LMs implicitly model aspects of semantics to predict semantic effects on sentence co-occurrence patterns.","However, we find the test that predicts entailment in practice works in the opposite direction to the theoretical test.","We thus revisit the assumptions underlying the original test, finding its derivation did not adequately account for redundancy in human-written text.","We argue that correctly accounting for redundancy related to explanations might derive the observed flipped test and, more generally, improve linguistic theories of human speakers."],"url":"http://arxiv.org/abs/2402.13956v1","category":"cs.CL"}
{"created":"2024-02-21 17:35:51","title":"BEE-NET: A deep neural network to identify in-the-wild Bodily Expression of Emotions","abstract":"In this study, we investigate how environmental factors, specifically the scenes and objects involved, can affect the expression of emotions through body language. To this end, we introduce a novel multi-stream deep convolutional neural network named BEE-NET. We also propose a new late fusion strategy that incorporates meta-information on places and objects as prior knowledge in the learning process. Our proposed probabilistic pooling model leverages this information to generate a joint probability distribution of both available and anticipated non-available contextual information in latent space. Importantly, our fusion strategy is differentiable, allowing for end-to-end training and capturing of hidden associations among data points without requiring further post-processing or regularisation. To evaluate our deep model, we use the Body Language Database (BoLD), which is currently the largest available database for the Automatic Identification of the in-the-wild Bodily Expression of Emotions (AIBEE). Our experimental results demonstrate that our proposed approach surpasses the current state-of-the-art in AIBEE by a margin of 2.07%, achieving an Emotional Recognition Score of 66.33%.","sentences":["In this study, we investigate how environmental factors, specifically the scenes and objects involved, can affect the expression of emotions through body language.","To this end, we introduce a novel multi-stream deep convolutional neural network named BEE-NET.","We also propose a new late fusion strategy that incorporates meta-information on places and objects as prior knowledge in the learning process.","Our proposed probabilistic pooling model leverages this information to generate a joint probability distribution of both available and anticipated non-available contextual information in latent space.","Importantly, our fusion strategy is differentiable, allowing for end-to-end training and capturing of hidden associations among data points without requiring further post-processing or regularisation.","To evaluate our deep model, we use the Body Language Database (BoLD), which is currently the largest available database for the Automatic Identification of the in-the-wild Bodily Expression of Emotions (AIBEE).","Our experimental results demonstrate that our proposed approach surpasses the current state-of-the-art in AIBEE by a margin of 2.07%, achieving an Emotional Recognition Score of 66.33%."],"url":"http://arxiv.org/abs/2402.13955v1","category":"cs.CV"}
{"created":"2024-02-21 17:32:00","title":"On Courant and Pleijel theorems for sub-Riemannian Laplacians","abstract":"We are interested in the number of nodal domains of eigenfunctions of sub-Laplacians on sub-Riemannian manifolds. Specifically, we investigate the validity of Pleijel's theorem, which states that, as soon as the dimension is strictly larger than 1, the number of nodal domains of an eigenfunction corresponding to the k-th eigenvalue is strictly (and uniformly, in a certain sense) smaller than k for large k.   In the first part of this paper we reduce this question from the case of general sub-Riemannian manifolds to that of nilpotent groups.   In the second part, we analyze in detail the case where the nilpotent group is a Heisenberg group times a Euclidean space. Along the way we improve known bounds on the optimal constants in the Faber-Krahn and isoperimetric inequalities on these groups.","sentences":["We are interested in the number of nodal domains of eigenfunctions of sub-Laplacians on sub-Riemannian manifolds.","Specifically, we investigate the validity of Pleijel's theorem, which states that, as soon as the dimension is strictly larger than 1, the number of nodal domains of an eigenfunction corresponding to the k-th eigenvalue is strictly (and uniformly, in a certain sense) smaller than k for large k.   ","In the first part of this paper we reduce this question from the case of general sub-Riemannian manifolds to that of nilpotent groups.   ","In the second part, we analyze in detail the case where the nilpotent group is a Heisenberg group times a Euclidean space.","Along the way we improve known bounds on the optimal constants in the Faber-Krahn and isoperimetric inequalities on these groups."],"url":"http://arxiv.org/abs/2402.13953v1","category":"math.SP"}
{"created":"2024-02-21 17:29:05","title":"A Post-Newtonian Analysis of Regularized 4D-EGB Theory: Complete Set of PPN Parameters and Observational Constraints","abstract":"We performed a post-Newtonian analysis of the regularized four-dimensional Einstein-Gauss-Bonnet gravitational theory (4D-EGB). The resulting metric differs from the classical parametrized post-Newtonian (PPN) formalism in that a new gravitational potential arises from the integration of the approximate field equations. We also investigated the conserved quantities and equations of motion for massive bodies and light rays to a certain degree. By computing the predicted periastron advance rate in a binary system, we obtained an observational constraint that is stronger than those of previous analyses. Although the usual 10 PPN parameters can still be derived within the PPN framework, an extra parameter is needed to account for the full post-Newtonian tests.","sentences":["We performed a post-Newtonian analysis of the regularized four-dimensional Einstein-Gauss-Bonnet gravitational theory (4D-EGB).","The resulting metric differs from the classical parametrized post-Newtonian (PPN) formalism in that a new gravitational potential arises from the integration of the approximate field equations.","We also investigated the conserved quantities and equations of motion for massive bodies and light rays to a certain degree.","By computing the predicted periastron advance rate in a binary system, we obtained an observational constraint that is stronger than those of previous analyses.","Although the usual 10 PPN parameters can still be derived within the PPN framework, an extra parameter is needed to account for the full post-Newtonian tests."],"url":"http://arxiv.org/abs/2402.13951v1","category":"gr-qc"}
{"created":"2024-02-21 17:23:59","title":"Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning","abstract":"Large language models (LLMs) have been shown to perform better when asked to reason step-by-step before answering a question. However, it is unclear to what degree the model's final answer is faithful to the stated reasoning steps. In this paper, we perform a causal mediation analysis on twelve LLMs to examine how intermediate reasoning steps generated by the LLM influence the final outcome and find that LLMs do not reliably use their intermediate reasoning steps when generating an answer. To address this issue, we introduce FRODO, a framework to tailor small-sized LMs to generate correct reasoning steps and robustly reason over these steps. FRODO consists of an inference module that learns to generate correct reasoning steps using an implicit causal reward function and a reasoning module that learns to faithfully reason over these intermediate inferences using a counterfactual and causal preference objective. Our experiments show that FRODO significantly outperforms four competitive baselines. Furthermore, FRODO improves the robustness and generalization ability of the reasoning LM, yielding higher performance on out-of-distribution test sets. Finally, we find that FRODO's rationales are more faithful to its final answer predictions than standard supervised fine-tuning.","sentences":["Large language models (LLMs) have been shown to perform better when asked to reason step-by-step before answering a question.","However, it is unclear to what degree the model's final answer is faithful to the stated reasoning steps.","In this paper, we perform a causal mediation analysis on twelve LLMs to examine how intermediate reasoning steps generated by the LLM influence the final outcome and find that LLMs do not reliably use their intermediate reasoning steps when generating an answer.","To address this issue, we introduce FRODO, a framework to tailor small-sized LMs to generate correct reasoning steps and robustly reason over these steps.","FRODO consists of an inference module that learns to generate correct reasoning steps using an implicit causal reward function and a reasoning module that learns to faithfully reason over these intermediate inferences using a counterfactual and causal preference objective.","Our experiments show that FRODO significantly outperforms four competitive baselines.","Furthermore, FRODO improves the robustness and generalization ability of the reasoning LM, yielding higher performance on out-of-distribution test sets.","Finally, we find that FRODO's rationales are more faithful to its final answer predictions than standard supervised fine-tuning."],"url":"http://arxiv.org/abs/2402.13950v1","category":"cs.CL"}
{"created":"2024-02-21 17:23:43","title":"Generating Realistic Arm Movements in Reinforcement Learning: A Quantitative Comparison of Reward Terms and Task Requirements","abstract":"The mimicking of human-like arm movement characteristics involves the consideration of three factors during control policy synthesis: (a) chosen task requirements, (b) inclusion of noise during movement execution and (c) chosen optimality principles. Previous studies showed that when considering these factors (a-c) individually, it is possible to synthesize arm movements that either kinematically match the experimental data or reproduce the stereotypical triphasic muscle activation pattern. However, to date no quantitative comparison has been made on how realistic the arm movement generated by each factor is; as well as whether a partial or total combination of all factors results in arm movements with human-like kinematic characteristics and a triphasic muscle pattern. To investigate this, we used reinforcement learning to learn a control policy for a musculoskeletal arm model, aiming to discern which combination of factors (a-c) results in realistic arm movements according to four frequently reported stereotypical characteristics. Our findings indicate that incorporating velocity and acceleration requirements into the reaching task, employing reward terms that encourage minimization of mechanical work, hand jerk, and control effort, along with the inclusion of noise during movement, leads to the emergence of realistic human arm movements in reinforcement learning. We expect that the gained insights will help in the future to better predict desired arm movements and corrective forces in wearable assistive devices.","sentences":["The mimicking of human-like arm movement characteristics involves the consideration of three factors during control policy synthesis: (a) chosen task requirements, (b) inclusion of noise during movement execution and (c) chosen optimality principles.","Previous studies showed that when considering these factors (a-c) individually, it is possible to synthesize arm movements that either kinematically match the experimental data or reproduce the stereotypical triphasic muscle activation pattern.","However, to date no quantitative comparison has been made on how realistic the arm movement generated by each factor is; as well as whether a partial or total combination of all factors results in arm movements with human-like kinematic characteristics and a triphasic muscle pattern.","To investigate this, we used reinforcement learning to learn a control policy for a musculoskeletal arm model, aiming to discern which combination of factors (a-c) results in realistic arm movements according to four frequently reported stereotypical characteristics.","Our findings indicate that incorporating velocity and acceleration requirements into the reaching task, employing reward terms that encourage minimization of mechanical work, hand jerk, and control effort, along with the inclusion of noise during movement, leads to the emergence of realistic human arm movements in reinforcement learning.","We expect that the gained insights will help in the future to better predict desired arm movements and corrective forces in wearable assistive devices."],"url":"http://arxiv.org/abs/2402.13949v1","category":"cs.RO"}
{"created":"2024-02-21 17:21:25","title":"Circular dichroism in high-harmonic generation in achiral nanostructures under vortex beam irradiation","abstract":"In this study, we investigate the nonlinear optical phenomena emerging from the interaction of vortex beams with achiral nanoparticles, leading to the observation of nonlinear circular dichroism in the high-harmonic generation. Despite the achiral symmetry of the nanoparticles, the interplay between the vector properties of the light, the symmetry of the nanoparticles, and the symmetry of the crystalline lattice of the nanoparticle material leads to circular dichroism in the nonlinear regime. We derive a formula that describes the conditions for the {appearance} of circular dichroism across all possible scenarios, taking into account all the symmetries. We also show that the absolute value of the incident beam's orbital angular momentum does not play a major role. We believe that this work provides important insights that can help in improvement the design process of chiral sensors, making them more versatile and effective.","sentences":["In this study, we investigate the nonlinear optical phenomena emerging from the interaction of vortex beams with achiral nanoparticles, leading to the observation of nonlinear circular dichroism in the high-harmonic generation.","Despite the achiral symmetry of the nanoparticles, the interplay between the vector properties of the light, the symmetry of the nanoparticles, and the symmetry of the crystalline lattice of the nanoparticle material leads to circular dichroism in the nonlinear regime.","We derive a formula that describes the conditions for the {appearance} of circular dichroism across all possible scenarios, taking into account all the symmetries.","We also show that the absolute value of the incident beam's orbital angular momentum does not play a major role.","We believe that this work provides important insights that can help in improvement the design process of chiral sensors, making them more versatile and effective."],"url":"http://arxiv.org/abs/2402.13947v1","category":"physics.optics"}
{"created":"2024-02-21 17:18:25","title":"AttackGNN: Red-Teaming GNNs in Hardware Security Using Reinforcement Learning","abstract":"Machine learning has shown great promise in addressing several critical hardware security problems. In particular, researchers have developed novel graph neural network (GNN)-based techniques for detecting intellectual property (IP) piracy, detecting hardware Trojans (HTs), and reverse engineering circuits, to name a few. These techniques have demonstrated outstanding accuracy and have received much attention in the community. However, since these techniques are used for security applications, it is imperative to evaluate them thoroughly and ensure they are robust and do not compromise the security of integrated circuits.   In this work, we propose AttackGNN, the first red-team attack on GNN-based techniques in hardware security. To this end, we devise a novel reinforcement learning (RL) agent that generates adversarial examples, i.e., circuits, against the GNN-based techniques. We overcome three challenges related to effectiveness, scalability, and generality to devise a potent RL agent. We target five GNN-based techniques for four crucial classes of problems in hardware security: IP piracy, detecting/localizing HTs, reverse engineering, and hardware obfuscation. Through our approach, we craft circuits that fool all GNNs considered in this work. For instance, to evade IP piracy detection, we generate adversarial pirated circuits that fool the GNN-based defense into classifying our crafted circuits as not pirated. For attacking HT localization GNN, our attack generates HT-infested circuits that fool the defense on all tested circuits. We obtain a similar 100% success rate against GNNs for all classes of problems.","sentences":["Machine learning has shown great promise in addressing several critical hardware security problems.","In particular, researchers have developed novel graph neural network (GNN)-based techniques for detecting intellectual property (IP) piracy, detecting hardware Trojans (HTs), and reverse engineering circuits, to name a few.","These techniques have demonstrated outstanding accuracy and have received much attention in the community.","However, since these techniques are used for security applications, it is imperative to evaluate them thoroughly and ensure they are robust and do not compromise the security of integrated circuits.   ","In this work, we propose AttackGNN, the first red-team attack on GNN-based techniques in hardware security.","To this end, we devise a novel reinforcement learning (RL) agent that generates adversarial examples, i.e., circuits, against the GNN-based techniques.","We overcome three challenges related to effectiveness, scalability, and generality to devise a potent RL agent.","We target five GNN-based techniques for four crucial classes of problems in hardware security: IP piracy, detecting/localizing HTs, reverse engineering, and hardware obfuscation.","Through our approach, we craft circuits that fool all GNNs considered in this work.","For instance, to evade IP piracy detection, we generate adversarial pirated circuits that fool the GNN-based defense into classifying our crafted circuits as not pirated.","For attacking HT localization GNN, our attack generates HT-infested circuits that fool the defense on all tested circuits.","We obtain a similar 100% success rate against GNNs for all classes of problems."],"url":"http://arxiv.org/abs/2402.13946v1","category":"cs.LG"}
{"created":"2024-02-21 17:15:47","title":"Probabilistic Neural Networks (PNNs) for Modeling Aleatoric Uncertainty in Scientific Machine Learning","abstract":"This paper investigates the use of probabilistic neural networks (PNNs) to model aleatoric uncertainty, which refers to the inherent variability in the input-output relationships of a system, often characterized by unequal variance or heteroscedasticity. Unlike traditional neural networks that produce deterministic outputs, PNNs generate probability distributions for the target variable, allowing the determination of both predicted means and intervals in regression scenarios. Contributions of this paper include the development of a probabilistic distance metric to optimize PNN architecture, and the deployment of PNNs in controlled data sets as well as a practical material science case involving fiber-reinforced composites. The findings confirm that PNNs effectively model aleatoric uncertainty, proving to be more appropriate than the commonly employed Gaussian process regression for this purpose. Specifically, in a real-world scientific machine learning context, PNNs yield remarkably accurate output mean estimates with R-squared scores approaching 0.97, and their predicted intervals exhibit a high correlation coefficient of nearly 0.80, closely matching observed data intervals. Hence, this research contributes to the ongoing exploration of leveraging the sophisticated representational capacity of neural networks to delineate complex input-output relationships in scientific problems.","sentences":["This paper investigates the use of probabilistic neural networks (PNNs) to model aleatoric uncertainty, which refers to the inherent variability in the input-output relationships of a system, often characterized by unequal variance or heteroscedasticity.","Unlike traditional neural networks that produce deterministic outputs, PNNs generate probability distributions for the target variable, allowing the determination of both predicted means and intervals in regression scenarios.","Contributions of this paper include the development of a probabilistic distance metric to optimize PNN architecture, and the deployment of PNNs in controlled data sets as well as a practical material science case involving fiber-reinforced composites.","The findings confirm that PNNs effectively model aleatoric uncertainty, proving to be more appropriate than the commonly employed Gaussian process regression for this purpose.","Specifically, in a real-world scientific machine learning context, PNNs yield remarkably accurate output mean estimates with R-squared scores approaching 0.97, and their predicted intervals exhibit a high correlation coefficient of nearly 0.80, closely matching observed data intervals.","Hence, this research contributes to the ongoing exploration of leveraging the sophisticated representational capacity of neural networks to delineate complex input-output relationships in scientific problems."],"url":"http://arxiv.org/abs/2402.13945v1","category":"stat.ML"}
{"created":"2024-02-21 17:13:43","title":"Self-Avoiding Walks on Cayley Graphs Through the Lens of Symbolic Dynamics","abstract":"We study dynamical and computational properties of the set of bi-infinite self-avoiding walks on Cayley graphs, as well as ways to compute, approximate and bound their connective constant. To do this, we introduce the skeleton $X_{G,S}$ of a finitely generated group $G$ relative to a generating set $S$, which is a one-dimensional subshift made of configurations on $S$ that avoid all words that reduce to the identity. We provide a characterization of groups which have SFT skeletons and sofic skeletons: first, there exists a finite generating set $S$ such that $X_{G,S}$ is a subshift of finite type if and only if $G$ is a plain group; second, there exists $S$ such that $X_{G,S}$ is sofic if and only if $G$ is a plain group, $\\mathbb{Z}\\times\\mathbb{Z}/2\\mathbb{Z}$ or $\\mathcal{D}_{\\infty}\\times\\mathbb{Z}/2\\mathbb{Z}$. We also characterize finitely generated torsion groups as groups whose skeletons are aperiodic.   For connective constants, using graph height functions and bridges, we show that Cayley graphs of finitely generated torsion groups do not admit graph height functions, and that for groups that admit transitive graph height functions, the connective constant is equal to the growth rate of periodic points of the skeleton. Using a counting argument due to Rosenfeld, we give bounds on the connective constant of infinite free Burnside groups. Finally, we take a brief look at the set of bi-infinite geodesics and introduce an analog of the connective constant for the geodesic growth.","sentences":["We study dynamical and computational properties of the set of bi-infinite self-avoiding walks on Cayley graphs, as well as ways to compute, approximate and bound their connective constant.","To do this, we introduce the skeleton $X_{G,S}$ of a finitely generated group $G$ relative to a generating set $S$, which is a one-dimensional subshift made of configurations on $S$ that avoid all words that reduce to the identity.","We provide a characterization of groups which have SFT skeletons and sofic skeletons: first, there exists a finite generating set $S$ such that $X_{G,S}$ is a subshift of finite type if and only if $G$ is a plain group; second, there exists $S$ such that $X_{G,S}$ is sofic if and only if $G$ is a plain group, $\\mathbb{Z}\\times\\mathbb{Z}/2\\mathbb{Z}$ or $\\mathcal{D}_{\\infty}\\times\\mathbb{Z}/2\\mathbb{Z}$. We also characterize finitely generated torsion groups as groups whose skeletons are aperiodic.   ","For connective constants, using graph height functions and bridges, we show that Cayley graphs of finitely generated torsion groups do not admit graph height functions, and that for groups that admit transitive graph height functions, the connective constant is equal to the growth rate of periodic points of the skeleton.","Using a counting argument due to Rosenfeld, we give bounds on the connective constant of infinite free Burnside groups.","Finally, we take a brief look at the set of bi-infinite geodesics and introduce an analog of the connective constant for the geodesic growth."],"url":"http://arxiv.org/abs/2402.13944v1","category":"math.CO"}
{"created":"2024-02-21 17:08:00","title":"Microstructured large-area photoconductive terahertz emitters driven at high average power","abstract":"Emitters based on photoconductive materials excited by ultrafast lasers are well established and popular devices for THz generation. However, so far, these emitters, both photoconductive antennas and large area emitters, were mostly explored using driving lasers with moderate average powers (either fiber lasers with up to hundreds of milliwatts or Ti:Sapphire systems up to few watts). In this paper, we explore the use of high power, MHz repetition rate Ytterbium (Yb) based oscillator for THz emission using a microstructured large area photoconductive emitter, consist of semi insulating GaAs with a 10 by 10 mm2 active area. As a driving source, we use a frequency doubled home built high average power ultrafast Yb oscillator, delivering 22 W of average power, 115 fs pulses with 91 MHz repetition rate at a central wavelength of 516 nm. When applying 9 W of average power (after an optical chopper with a duty cycle of 50 percent) on the structure without optimized heatsinking, we obtain 65 uW THz average power, 4 THz bandwidth; furthermore, we safely apply up to 18 W of power on the structure without observing damage. We investigate the impact of excitation power, bias voltage, optical fluence, and their interplay on the emitter performance and explore in detail the sources of thermal load originating from electrical and optical power. Optical power is found to have a more critical impact on LAE saturation than electrical power, thus optimized heatsinking will allow us to improve the conversion efficiency in the near future towards much higher emitter power. This work paves the way towards achieving hundreds of MHz or even GHz repetition rates, high power THz sources based on photoconductive emitters, that are of great interest for example for future THz imaging applications.","sentences":["Emitters based on photoconductive materials excited by ultrafast lasers are well established and popular devices for THz generation.","However, so far, these emitters, both photoconductive antennas and large area emitters, were mostly explored using driving lasers with moderate average powers (either fiber lasers with up to hundreds of milliwatts or Ti:Sapphire systems up to few watts).","In this paper, we explore the use of high power, MHz repetition rate Ytterbium (Yb) based oscillator for THz emission using a microstructured large area photoconductive emitter, consist of semi insulating GaAs with a 10 by 10 mm2 active area.","As a driving source, we use a frequency doubled home built high average power ultrafast Yb oscillator, delivering 22 W of average power, 115 fs pulses with 91 MHz repetition rate at a central wavelength of 516 nm.","When applying 9 W of average power (after an optical chopper with a duty cycle of 50 percent) on the structure without optimized heatsinking, we obtain 65 uW THz average power, 4 THz bandwidth; furthermore, we safely apply up to 18 W of power on the structure without observing damage.","We investigate the impact of excitation power, bias voltage, optical fluence, and their interplay on the emitter performance and explore in detail the sources of thermal load originating from electrical and optical power.","Optical power is found to have a more critical impact on LAE saturation than electrical power, thus optimized heatsinking will allow us to improve the conversion efficiency in the near future towards much higher emitter power.","This work paves the way towards achieving hundreds of MHz or even GHz repetition rates, high power THz sources based on photoconductive emitters, that are of great interest for example for future THz imaging applications."],"url":"http://arxiv.org/abs/2402.13940v1","category":"physics.optics"}
{"created":"2024-02-21 17:07:09","title":"What is the focus of XAI in UI design? Prioritizing UI design principles for enhancing XAI user experience","abstract":"With the widespread application of artificial intelligence(AI), the explainable AI (XAI) field has undergone a notable resurgence. In this background, the importance of user experience in XAI has become increasingly prominent. Simultaneously, the user interface (UI) serves as a crucial link between XAI and users. However, despite the existence of UI design principles for XAI, there is a lack of prioritization based on their significance. This will lead practitioners to have a vague understanding of different design principles, making it difficult to allocate design space reasonably and emphasize design focal points. This paper aims to prioritize four design principles, providing clear guidance for UI design in XAI. Initially, we conducted a lightweight summary to derive five user experience standards for non-expert users in XAI. Subsequently, we developed four corresponding webpage prototypes for the four design principles. Nineteen participants then interacted with these prototypes, providing ratings based on five user experience standards, and We calculated the weights of the design principles. Our findings indicate that, for non-expert users, \"sensitivity\" is the optimal UI design principle (weight = 0.3296), followed by \"flexibility\" (weight = 0.3014). Finally, we engage in further discussion and summarization of our research results, and present future works and limitations.","sentences":["With the widespread application of artificial intelligence(AI), the explainable AI (XAI) field has undergone a notable resurgence.","In this background, the importance of user experience in XAI has become increasingly prominent.","Simultaneously, the user interface (UI) serves as a crucial link between XAI and users.","However, despite the existence of UI design principles for XAI, there is a lack of prioritization based on their significance.","This will lead practitioners to have a vague understanding of different design principles, making it difficult to allocate design space reasonably and emphasize design focal points.","This paper aims to prioritize four design principles, providing clear guidance for UI design in XAI.","Initially, we conducted a lightweight summary to derive five user experience standards for non-expert users in XAI.","Subsequently, we developed four corresponding webpage prototypes for the four design principles.","Nineteen participants then interacted with these prototypes, providing ratings based on five user experience standards, and We calculated the weights of the design principles.","Our findings indicate that, for non-expert users, \"sensitivity\" is the optimal UI design principle (weight = 0.3296), followed by \"flexibility\" (weight = 0.3014).","Finally, we engage in further discussion and summarization of our research results, and present future works and limitations."],"url":"http://arxiv.org/abs/2402.13939v1","category":"cs.HC"}
{"created":"2024-02-21 17:07:04","title":"A $(5/3+\u03b5)$-Approximation for Tricolored Non-crossing Euclidean TSP","abstract":"In the Tricolored Euclidean Traveling Salesperson problem, we are given~$k=3$ sets of points in the plane and are looking for disjoint tours, each covering one of the sets. Arora (1998) famously gave a PTAS based on ``patching'' for the case $k=1$ and, recently, Dross et al.~(2023) generalized this result to~$k=2$. Our contribution is a $(5/3+\\epsilon)$-approximation algorithm for~$k=3$ that further generalizes Arora's approach. It is believed that patching is generally no longer possible for more than two tours. We circumvent this issue by either applying a conditional patching scheme for three tours or using an alternative approach based on a weighted solution for $k=2$.","sentences":["In the Tricolored Euclidean Traveling Salesperson problem, we are given~$k=3$ sets of points in the plane and are looking for disjoint tours, each covering one of the sets.","Arora (1998) famously gave a PTAS based on ``patching'' for the case $k=1$ and, recently, Dross et al.~(2023) generalized this result to~$k=2$. Our contribution is a $(5/3+\\epsilon)$-approximation algorithm for~$k=3$ that further generalizes Arora's approach.","It is believed that patching is generally no longer possible for more than two tours.","We circumvent this issue by either applying a conditional patching scheme for three tours or using an alternative approach based on a weighted solution for $k=2$."],"url":"http://arxiv.org/abs/2402.13938v1","category":"cs.DS"}
{"created":"2024-02-21 17:05:06","title":"Distinctive Image Captioning: Leveraging Ground Truth Captions in CLIP Guided Reinforcement Learning","abstract":"Training image captioning models using teacher forcing results in very generic samples, whereas more distinctive captions can be very useful in retrieval applications or to produce alternative texts describing images for accessibility. Reinforcement Learning (RL) allows to use cross-modal retrieval similarity score between the generated caption and the input image as reward to guide the training, leading to more distinctive captions. Recent studies show that pre-trained cross-modal retrieval models can be used to provide this reward, completely eliminating the need for reference captions. However, we argue in this paper that Ground Truth (GT) captions can still be useful in this RL framework. We propose a new image captioning model training strategy that makes use of GT captions in different ways. Firstly, they can be used to train a simple MLP discriminator that serves as a regularization to prevent reward hacking and ensures the fluency of generated captions, resulting in a textual GAN setup extended for multimodal inputs. Secondly, they can serve as additional trajectories in the RL strategy, resulting in a teacher forcing loss weighted by the similarity of the GT to the image. This objective acts as an additional learning signal grounded to the distribution of the GT captions. Thirdly, they can serve as strong baselines when added to the pool of captions used to compute the proposed contrastive reward to reduce the variance of gradient estimate. Experiments on MS-COCO demonstrate the interest of the proposed training strategy to produce highly distinctive captions while maintaining high writing quality.","sentences":["Training image captioning models using teacher forcing results in very generic samples, whereas more distinctive captions can be very useful in retrieval applications or to produce alternative texts describing images for accessibility.","Reinforcement Learning (RL) allows to use cross-modal retrieval similarity score between the generated caption and the input image as reward to guide the training, leading to more distinctive captions.","Recent studies show that pre-trained cross-modal retrieval models can be used to provide this reward, completely eliminating the need for reference captions.","However, we argue in this paper that Ground Truth (GT) captions can still be useful in this RL framework.","We propose a new image captioning model training strategy that makes use of GT captions in different ways.","Firstly, they can be used to train a simple MLP discriminator that serves as a regularization to prevent reward hacking and ensures the fluency of generated captions, resulting in a textual GAN setup extended for multimodal inputs.","Secondly, they can serve as additional trajectories in the RL strategy, resulting in a teacher forcing loss weighted by the similarity of the GT to the image.","This objective acts as an additional learning signal grounded to the distribution of the GT captions.","Thirdly, they can serve as strong baselines when added to the pool of captions used to compute the proposed contrastive reward to reduce the variance of gradient estimate.","Experiments on MS-COCO demonstrate the interest of the proposed training strategy to produce highly distinctive captions while maintaining high writing quality."],"url":"http://arxiv.org/abs/2402.13936v1","category":"cs.CL"}
{"created":"2024-02-21 17:00:56","title":"Do Efficient Transformers Really Save Computation?","abstract":"As transformer-based language models are trained on increasingly large datasets and with vast numbers of parameters, finding more efficient alternatives to the standard Transformer has become very valuable. While many efficient Transformers and Transformer alternatives have been proposed, none provide theoretical guarantees that they are a suitable replacement for the standard Transformer. This makes it challenging to identify when to use a specific model and what directions to prioritize for further investigation. In this paper, we aim to understand the capabilities and limitations of efficient Transformers, specifically the Sparse Transformer and the Linear Transformer. We focus on their reasoning capability as exhibited by Chain-of-Thought (CoT) prompts and follow previous works to model them as Dynamic Programming (DP) problems. Our results show that while these models are expressive enough to solve general DP tasks, contrary to expectations, they require a model size that scales with the problem size. Nonetheless, we identify a class of DP problems for which these models can be more efficient than the standard Transformer. We confirm our theoretical results through experiments on representative DP tasks, adding to the understanding of efficient Transformers' practical strengths and weaknesses.","sentences":["As transformer-based language models are trained on increasingly large datasets and with vast numbers of parameters, finding more efficient alternatives to the standard Transformer has become very valuable.","While many efficient Transformers and Transformer alternatives have been proposed, none provide theoretical guarantees that they are a suitable replacement for the standard Transformer.","This makes it challenging to identify when to use a specific model and what directions to prioritize for further investigation.","In this paper, we aim to understand the capabilities and limitations of efficient Transformers, specifically the Sparse Transformer and the Linear Transformer.","We focus on their reasoning capability as exhibited by Chain-of-Thought (CoT) prompts and follow previous works to model them as Dynamic Programming (DP) problems.","Our results show that while these models are expressive enough to solve general DP tasks, contrary to expectations, they require a model size that scales with the problem size.","Nonetheless, we identify a class of DP problems for which these models can be more efficient than the standard Transformer.","We confirm our theoretical results through experiments on representative DP tasks, adding to the understanding of efficient Transformers' practical strengths and weaknesses."],"url":"http://arxiv.org/abs/2402.13934v1","category":"cs.LG"}
{"created":"2024-02-21 16:59:53","title":"Tumor segmentation on whole slide images: training or prompting?","abstract":"Tumor segmentation stands as a pivotal task in cancer diagnosis. Given the immense dimensions of whole slide images (WSI) in histology, deep learning approaches for WSI classification mainly operate at patch-wise or superpixel-wise level. However, these solutions often struggle to capture global WSI information and cannot directly generate the binary mask. Downsampling the WSI and performing semantic segmentation is another possible approach. While this method offers computational efficiency, it necessitates a large amount of annotated data since resolution reduction may lead to information loss. Visual prompting is a novel paradigm that allows the model to perform new tasks by making subtle modifications to the input space, rather than adapting the model itself. Such approach has demonstrated promising results on many computer vision tasks. In this paper, we show the efficacy of visual prompting in the context of tumor segmentation for three distinct organs. In comparison to classical methods trained for this specific task, our findings reveal that, with appropriate prompt examples, visual prompting can achieve comparable or better performance without extensive fine-tuning.","sentences":["Tumor segmentation stands as a pivotal task in cancer diagnosis.","Given the immense dimensions of whole slide images (WSI) in histology, deep learning approaches for WSI classification mainly operate at patch-wise or superpixel-wise level.","However, these solutions often struggle to capture global WSI information and cannot directly generate the binary mask.","Downsampling the WSI and performing semantic segmentation is another possible approach.","While this method offers computational efficiency, it necessitates a large amount of annotated data since resolution reduction may lead to information loss.","Visual prompting is a novel paradigm that allows the model to perform new tasks by making subtle modifications to the input space, rather than adapting the model itself.","Such approach has demonstrated promising results on many computer vision tasks.","In this paper, we show the efficacy of visual prompting in the context of tumor segmentation for three distinct organs.","In comparison to classical methods trained for this specific task, our findings reveal that, with appropriate prompt examples, visual prompting can achieve comparable or better performance without extensive fine-tuning."],"url":"http://arxiv.org/abs/2402.13932v1","category":"cs.CV"}
{"created":"2024-02-21 16:51:05","title":"SDXL-Lightning: Progressive Adversarial Diffusion Distillation","abstract":"We propose a diffusion distillation method that achieves new state-of-the-art in one-step/few-step 1024px text-to-image generation based on SDXL. Our method combines progressive and adversarial distillation to achieve a balance between quality and mode coverage. In this paper, we discuss the theoretical analysis, discriminator design, model formulation, and training techniques. We open-source our distilled SDXL-Lightning models both as LoRA and full UNet weights.","sentences":["We propose a diffusion distillation method that achieves new state-of-the-art in one-step/few-step 1024px text-to-image generation based on SDXL.","Our method combines progressive and adversarial distillation to achieve a balance between quality and mode coverage.","In this paper, we discuss the theoretical analysis, discriminator design, model formulation, and training techniques.","We open-source our distilled SDXL-Lightning models both as LoRA and full UNet weights."],"url":"http://arxiv.org/abs/2402.13929v1","category":"cs.CV"}
{"created":"2024-02-21 16:48:38","title":"Supporting the next generation lithography roadmap using partial state-feedback reduced-order switching predictive models","abstract":"To support the ever-increasing performance requirements of lithography systems in terms of throughput and accuracy, in this paper, we introduce a design framework for partial state-feedback reduced-order switching predictive models. By combining measurements and predictions, this method aims to: 1) improve overall system performance by reducing the placement errors in a die within and across the full-wafer and 2) eliminate redundant measurements by using predictions to improve system throughput. We primarily focus on well-known trade-off introduced by measurement time, which can correct errors at a cost of throughput, noise and not being robust to nonlinearities. The proposed predictive model consists of a reduced-order model with a switching logic that acts a scheduler to deal with uncertain operating conditions. The utilization of linear predictive models as a basis for the control design appeals to the ease and cost of implementation therewith enhancing the applicability. For the add-on part, the scheduler logic is adapted based on expected operating conditions of the system while guaranteeing global uniform ultimate bounded asymptotic stability. Lastly, to deal with measurement layouts, the predictor combines the measurement into model using partial state-feedback. Effectiveness of the proposed strategy is demonstrated in practice on a high-precision industrial scanner.","sentences":["To support the ever-increasing performance requirements of lithography systems in terms of throughput and accuracy, in this paper, we introduce a design framework for partial state-feedback reduced-order switching predictive models.","By combining measurements and predictions, this method aims to: 1) improve overall system performance by reducing the placement errors in a die within and across the full-wafer and 2) eliminate redundant measurements by using predictions to improve system throughput.","We primarily focus on well-known trade-off introduced by measurement time, which can correct errors at a cost of throughput, noise and not being robust to nonlinearities.","The proposed predictive model consists of a reduced-order model with a switching logic that acts a scheduler to deal with uncertain operating conditions.","The utilization of linear predictive models as a basis for the control design appeals to the ease and cost of implementation therewith enhancing the applicability.","For the add-on part, the scheduler logic is adapted based on expected operating conditions of the system while guaranteeing global uniform ultimate bounded asymptotic stability.","Lastly, to deal with measurement layouts, the predictor combines the measurement into model using partial state-feedback.","Effectiveness of the proposed strategy is demonstrated in practice on a high-precision industrial scanner."],"url":"http://arxiv.org/abs/2402.13928v1","category":"math.OC"}
{"created":"2024-02-21 16:48:07","title":"The Delusional Hedge Algorithm as a Model of Human Learning from Diverse Opinions","abstract":"Whereas cognitive models of learning often assume direct experience with both the features of an event and with a true label or outcome, much of everyday learning arises from hearing the opinions of others, without direct access to either the experience or the ground truth outcome. We consider how people can learn which opinions to trust in such scenarios by extending the hedge algorithm: a classic solution for learning from diverse information sources. We first introduce a semi-supervised variant we call the delusional hedge capable of learning from both supervised and unsupervised experiences. In two experiments, we examine the alignment between human judgments and predictions from the standard hedge, the delusional hedge, and a heuristic baseline model. Results indicate that humans effectively incorporate both labeled and unlabeled information in a manner consistent with the delusional hedge algorithm -- suggesting that human learners not only gauge the accuracy of information sources but also their consistency with other reliable sources. The findings advance our understanding of human learning from diverse opinions, with implications for the development of algorithms that better capture how people learn to weigh conflicting information sources.","sentences":["Whereas cognitive models of learning often assume direct experience with both the features of an event and with a true label or outcome, much of everyday learning arises from hearing the opinions of others, without direct access to either the experience or the ground truth outcome.","We consider how people can learn which opinions to trust in such scenarios by extending the hedge algorithm: a classic solution for learning from diverse information sources.","We first introduce a semi-supervised variant we call the delusional hedge capable of learning from both supervised and unsupervised experiences.","In two experiments, we examine the alignment between human judgments and predictions from the standard hedge, the delusional hedge, and a heuristic baseline model.","Results indicate that humans effectively incorporate both labeled and unlabeled information in a manner consistent with the delusional hedge algorithm -- suggesting that human learners not only gauge the accuracy of information sources but also their consistency with other reliable sources.","The findings advance our understanding of human learning from diverse opinions, with implications for the development of algorithms that better capture how people learn to weigh conflicting information sources."],"url":"http://arxiv.org/abs/2402.13927v1","category":"cs.AI"}
{"created":"2024-02-21 16:46:36","title":"Large Language Models are Vulnerable to Bait-and-Switch Attacks for Generating Harmful Content","abstract":"The risks derived from large language models (LLMs) generating deceptive and damaging content have been the subject of considerable research, but even safe generations can lead to problematic downstream impacts. In our study, we shift the focus to how even safe text coming from LLMs can be easily turned into potentially dangerous content through Bait-and-Switch attacks. In such attacks, the user first prompts LLMs with safe questions and then employs a simple find-and-replace post-hoc technique to manipulate the outputs into harmful narratives. The alarming efficacy of this approach in generating toxic content highlights a significant challenge in developing reliable safety guardrails for LLMs. In particular, we stress that focusing on the safety of the verbatim LLM outputs is insufficient and that we also need to consider post-hoc transformations.","sentences":["The risks derived from large language models (LLMs) generating deceptive and damaging content have been the subject of considerable research, but even safe generations can lead to problematic downstream impacts.","In our study, we shift the focus to how even safe text coming from LLMs can be easily turned into potentially dangerous content through Bait-and-Switch attacks.","In such attacks, the user first prompts LLMs with safe questions and then employs a simple find-and-replace post-hoc technique to manipulate the outputs into harmful narratives.","The alarming efficacy of this approach in generating toxic content highlights a significant challenge in developing reliable safety guardrails for LLMs.","In particular, we stress that focusing on the safety of the verbatim LLM outputs is insufficient and that we also need to consider post-hoc transformations."],"url":"http://arxiv.org/abs/2402.13926v1","category":"cs.CL"}
{"created":"2024-02-21 16:41:26","title":"Exploring Primordial Black Holes and Gravitational Waves with R-Symmetric GUT Higgs Inflation","abstract":"This study investigates the realization of R-symmetric Higgs inflation within the framework of no-scale-like supergravity, aiming to elucidate the formation of primordial black holes and observable gravitational waves within a class of GUT models. We explore the possibility of an ultra-slow-roll phase in a hybrid inflation framework, where the GUT Higgs field primarily takes on the role of the inflaton. The amplification of the scalar power spectrum gives rise to scalar-induced gravitational waves and the generation of primordial black holes. The predicted stochastic gravitational wave background falls within the sensitivity range of existing and upcoming gravitational wave detectors, while primordial black holes hold the potential to explain the abundance of dark matter. Furthermore, we highlight the significance of the leading-order nonrenormalizable term in the superpotential of achieving inflationary observables consistent with the latest experimental data. Additionally, the predicted range of the tensor-to-scalar ratio, a key measure of primordial gravitational waves, lies within the observational window of future experiments searching for B-mode polarization patterns in cosmic microwave background data.","sentences":["This study investigates the realization of R-symmetric Higgs inflation within the framework of no-scale-like supergravity, aiming to elucidate the formation of primordial black holes and observable gravitational waves within a class of GUT models.","We explore the possibility of an ultra-slow-roll phase in a hybrid inflation framework, where the GUT Higgs field primarily takes on the role of the inflaton.","The amplification of the scalar power spectrum gives rise to scalar-induced gravitational waves and the generation of primordial black holes.","The predicted stochastic gravitational wave background falls within the sensitivity range of existing and upcoming gravitational wave detectors, while primordial black holes hold the potential to explain the abundance of dark matter.","Furthermore, we highlight the significance of the leading-order nonrenormalizable term in the superpotential of achieving inflationary observables consistent with the latest experimental data.","Additionally, the predicted range of the tensor-to-scalar ratio, a key measure of primordial gravitational waves, lies within the observational window of future experiments searching for B-mode polarization patterns in cosmic microwave background data."],"url":"http://arxiv.org/abs/2402.13924v1","category":"astro-ph.CO"}
{"created":"2024-02-21 16:40:13","title":"Robust recovery for stochastic block models, simplified and generalized","abstract":"We study the problem of $\\textit{robust community recovery}$: efficiently recovering communities in sparse stochastic block models in the presence of adversarial corruptions. In the absence of adversarial corruptions, there are efficient algorithms when the $\\textit{signal-to-noise ratio}$ exceeds the $\\textit{Kesten--Stigum (KS) threshold}$, widely believed to be the computational threshold for this problem. The question we study is: does the computational threshold for robust community recovery also lie at the KS threshold? We answer this question affirmatively, providing an algorithm for robust community recovery for arbitrary stochastic block models on any constant number of communities, generalizing the work of Ding, d'Orsi, Nasser & Steurer on an efficient algorithm above the KS threshold in the case of $2$-community block models.   There are three main ingredients to our work:   (i) The Bethe Hessian of the graph is defined as $H_G(t) \\triangleq (D_G-I)t^2 - A_Gt + I$ where $D_G$ is the diagonal matrix of degrees and $A_G$ is the adjacency matrix. Empirical work suggested that the Bethe Hessian for the stochastic block model has outlier eigenvectors corresponding to the communities right above the Kesten-Stigum threshold. We formally confirm the existence of outlier eigenvalues for the Bethe Hessian, by explicitly constructing outlier eigenvectors from the community vectors.   (ii) We develop an algorithm for a variant of robust PCA on sparse matrices. Specifically, an algorithm to partially recover top eigenspaces from adversarially corrupted sparse matrices under mild delocalization constraints.   (iii) A rounding algorithm to turn vector assignments of vertices into a community assignment, inspired by the algorithm of Charikar \\& Wirth \\cite{CW04} for $2$XOR.","sentences":["We study the problem of $\\textit{robust community recovery}$: efficiently recovering communities in sparse stochastic block models in the presence of adversarial corruptions.","In the absence of adversarial corruptions, there are efficient algorithms when the $\\textit{signal-to-noise ratio}$ exceeds the $\\textit{Kesten--Stigum (KS) threshold}$, widely believed to be the computational threshold for this problem.","The question we study is: does the computational threshold for robust community recovery also lie at the KS threshold?","We answer this question affirmatively, providing an algorithm for robust community recovery for arbitrary stochastic block models on any constant number of communities, generalizing the work of Ding, d'Orsi, Nasser & Steurer on an efficient algorithm above the KS threshold in the case of $2$-community block models.   ","There are three main ingredients to our work:   (i) The Bethe Hessian of the graph is defined as $H_G(t)","\\triangleq (D_G-I)t^2 - A_Gt","+ I$ where $D_G$ is the diagonal matrix of degrees and $A_G$ is the adjacency matrix.","Empirical work suggested that the Bethe Hessian for the stochastic block model has outlier eigenvectors corresponding to the communities right above the Kesten-Stigum threshold.","We formally confirm the existence of outlier eigenvalues for the Bethe Hessian, by explicitly constructing outlier eigenvectors from the community vectors.   ","(ii) We develop an algorithm for a variant of robust PCA on sparse matrices.","Specifically, an algorithm to partially recover top eigenspaces from adversarially corrupted sparse matrices under mild delocalization constraints.   ","(iii) A rounding algorithm to turn vector assignments of vertices into a community assignment, inspired by the algorithm of Charikar \\& Wirth \\cite{CW04} for $2$XOR."],"url":"http://arxiv.org/abs/2402.13921v1","category":"cs.DS"}
{"created":"2024-02-21 16:33:22","title":"SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization","abstract":"Large Language Models (LLMs) such as GPT and Llama have demonstrated significant achievements in summarization tasks but struggle with factual inaccuracies, a critical issue in clinical NLP applications where errors could lead to serious consequences. To counter the high costs and limited availability of expert-annotated data for factual alignment, this study introduces an innovative pipeline that utilizes GPT-3.5 and GPT-4 to generate high-quality feedback aimed at enhancing factual consistency in clinical note summarization. Our research primarily focuses on edit feedback, mirroring the practical scenario in which medical professionals refine AI system outputs without the need for additional annotations. Despite GPT's proven expertise in various clinical NLP tasks, such as the Medical Licensing Examination, there is scant research on its capacity to deliver expert-level edit feedback for improving weaker LMs or LLMs generation quality. This work leverages GPT's advanced capabilities in clinical NLP to offer expert-level edit feedback. Through the use of two distinct alignment algorithms (DPO and SALT) based on GPT edit feedback, our goal is to reduce hallucinations and align closely with medical facts, endeavoring to narrow the divide between AI-generated content and factual accuracy. This highlights the substantial potential of GPT edits in enhancing the alignment of clinical factuality.","sentences":["Large Language Models (LLMs) such as GPT and Llama have demonstrated significant achievements in summarization tasks but struggle with factual inaccuracies, a critical issue in clinical NLP applications where errors could lead to serious consequences.","To counter the high costs and limited availability of expert-annotated data for factual alignment, this study introduces an innovative pipeline that utilizes GPT-3.5 and GPT-4 to generate high-quality feedback aimed at enhancing factual consistency in clinical note summarization.","Our research primarily focuses on edit feedback, mirroring the practical scenario in which medical professionals refine AI system outputs without the need for additional annotations.","Despite GPT's proven expertise in various clinical NLP tasks, such as the Medical Licensing Examination, there is scant research on its capacity to deliver expert-level edit feedback for improving weaker LMs or LLMs generation quality.","This work leverages GPT's advanced capabilities in clinical NLP to offer expert-level edit feedback.","Through the use of two distinct alignment algorithms (DPO and SALT) based on GPT edit feedback, our goal is to reduce hallucinations and align closely with medical facts, endeavoring to narrow the divide between AI-generated content and factual accuracy.","This highlights the substantial potential of GPT edits in enhancing the alignment of clinical factuality."],"url":"http://arxiv.org/abs/2402.13919v1","category":"cs.CL"}
{"created":"2024-02-21 16:32:38","title":"What Linguistic Features and Languages are Important in LLM Translation?","abstract":"Large Language Models (LLMs) demonstrate strong capability across multiple tasks, including machine translation. Our study focuses on evaluating Llama2's machine translation capabilities and exploring how translation depends on languages in its training data. Our experiments show that the 7B Llama2 model yields above 10 BLEU score for all languages it has seen, but not always for languages it has not seen. Most gains for those unseen languages are observed the most with the model scale compared to using chat versions or adding shot count. Furthermore, our linguistic distance analysis reveals that syntactic similarity is not always the primary linguistic factor in determining translation quality. Interestingly, we discovered that under specific circumstances, some languages, despite having significantly less training data than English, exhibit strong correlations comparable to English. Our discoveries here give new perspectives for the current landscape of LLMs, raising the possibility that LLMs centered around languages other than English may offer a more effective foundation for a multilingual model.","sentences":["Large Language Models (LLMs) demonstrate strong capability across multiple tasks, including machine translation.","Our study focuses on evaluating Llama2's machine translation capabilities and exploring how translation depends on languages in its training data.","Our experiments show that the 7B Llama2 model yields above 10 BLEU score for all languages it has seen, but not always for languages it has not seen.","Most gains for those unseen languages are observed the most with the model scale compared to using chat versions or adding shot count.","Furthermore, our linguistic distance analysis reveals that syntactic similarity is not always the primary linguistic factor in determining translation quality.","Interestingly, we discovered that under specific circumstances, some languages, despite having significantly less training data than English, exhibit strong correlations comparable to English.","Our discoveries here give new perspectives for the current landscape of LLMs, raising the possibility that LLMs centered around languages other than English may offer a more effective foundation for a multilingual model."],"url":"http://arxiv.org/abs/2402.13917v1","category":"cs.CL"}
{"created":"2024-02-21 16:31:07","title":"A Combined Learning and Optimization Framework to Transfer Human Whole-body Loco-manipulation Skills to Mobile Manipulators","abstract":"Humans' ability to smoothly switch between locomotion and manipulation is a remarkable feature of sensorimotor coordination. Leaning and replication of such human-like strategies can lead to the development of more sophisticated robots capable of performing complex whole-body tasks in real-world environments. To this end, this paper proposes a combined learning and optimization framework for transferring human's loco-manipulation soft-switching skills to mobile manipulators. The methodology departs from data collection of human demonstrations for a locomotion-integrated manipulation task through a vision system. Next, the wrist and pelvis motions are mapped to mobile manipulators' End-Effector (EE) and mobile base. A kernelized movement primitive algorithm learns the wrist and pelvis trajectories and generalizes to new desired points according to task requirements. Next, the reference trajectories are sent to a hierarchical quadratic programming controller, where the EE and the mobile base reference trajectories are provided as the first and second priority tasks, generating the feasible and optimal joint level commands. A locomotion-integrated pick-and-place task is executed to validate the proposed approach. After a human demonstrates the task, a mobile manipulator executes the task with the same and new settings, grasping a bottle at non-zero velocity. The results showed that the proposed approach successfully transfers the human loco-manipulation skills to mobile manipulators, even with different geometry.","sentences":["Humans' ability to smoothly switch between locomotion and manipulation is a remarkable feature of sensorimotor coordination.","Leaning and replication of such human-like strategies can lead to the development of more sophisticated robots capable of performing complex whole-body tasks in real-world environments.","To this end, this paper proposes a combined learning and optimization framework for transferring human's loco-manipulation soft-switching skills to mobile manipulators.","The methodology departs from data collection of human demonstrations for a locomotion-integrated manipulation task through a vision system.","Next, the wrist and pelvis motions are mapped to mobile manipulators' End-Effector (EE) and mobile base.","A kernelized movement primitive algorithm learns the wrist and pelvis trajectories and generalizes to new desired points according to task requirements.","Next, the reference trajectories are sent to a hierarchical quadratic programming controller, where the EE and the mobile base reference trajectories are provided as the first and second priority tasks, generating the feasible and optimal joint level commands.","A locomotion-integrated pick-and-place task is executed to validate the proposed approach.","After a human demonstrates the task, a mobile manipulator executes the task with the same and new settings, grasping a bottle at non-zero velocity.","The results showed that the proposed approach successfully transfers the human loco-manipulation skills to mobile manipulators, even with different geometry."],"url":"http://arxiv.org/abs/2402.13915v1","category":"cs.RO"}
{"created":"2024-02-21 16:30:24","title":"Explain to Question not to Justify","abstract":"Explainable Artificial Intelligence (XAI) is a young but very promising field of research. Unfortunately, the progress in this field is currently slowed down by divergent and incompatible goals. In this paper, we separate various threads tangled within the area of XAI into two complementary cultures of human/value-oriented explanations (BLUE XAI) and model/validation-oriented explanations (RED XAI). We also argue that the area of RED XAI is currently under-explored and hides great opportunities and potential for important research necessary to ensure the safety of AI systems. We conclude this paper by presenting promising challenges in this area.","sentences":["Explainable Artificial Intelligence (XAI) is a young but very promising field of research.","Unfortunately, the progress in this field is currently slowed down by divergent and incompatible goals.","In this paper, we separate various threads tangled within the area of XAI into two complementary cultures of human/value-oriented explanations (BLUE XAI) and model/validation-oriented explanations (RED XAI).","We also argue that the area of RED XAI is currently under-explored and hides great opportunities and potential for important research necessary to ensure the safety of AI systems.","We conclude this paper by presenting promising challenges in this area."],"url":"http://arxiv.org/abs/2402.13914v1","category":"cs.AI"}
{"created":"2024-02-21 16:28:32","title":"An Automated Chemical Exploration of NGC 6334I at 340 au Resolution","abstract":"Much of the information gleaned from observations of star-forming regions comes from the analysis of their molecular emission spectra, particularly in the radio regime. The time-consuming nature of fitting synthetic spectra to observations interactively for such line-rich sources, however, often results in such analysis being limited to data extracted from a single-dish observation or a handful of pixels from an interferometric observation. Yet, star-forming regions display a wide variety of physical conditions that are difficult, if not impossible, to accurately characterize with such a limited number of spectra. We have developed an automated fitting routine that visits every pixel in the field of view of an ALMA data cube and determines the best-fit physical parameters, including excitation temperature and column densities, for a given list of molecules. In this proof-of-concept work, we provide an overview of the fitting routine and apply it to 0\".26, 1.1 km s$^{-1}$ resolution ALMA observations of two sites of massive star-formation in NGC 6334I. Parameters were found for 21 distinct molecules by generating synthetic spectra across 7.48 GHz of spectral bandwidth between 280 and 351 GHz. Spatial images of the derived parameters for each of the > 8000 pixels are presented with special attention paid to the C$_2$H$_4$O$_2$ isomers and their relative variations. We highlight the greater scientific utility of the column density and velocity images of individual molecules compared to traditional moment maps of single transitions.","sentences":["Much of the information gleaned from observations of star-forming regions comes from the analysis of their molecular emission spectra, particularly in the radio regime.","The time-consuming nature of fitting synthetic spectra to observations interactively for such line-rich sources, however, often results in such analysis being limited to data extracted from a single-dish observation or a handful of pixels from an interferometric observation.","Yet, star-forming regions display a wide variety of physical conditions that are difficult, if not impossible, to accurately characterize with such a limited number of spectra.","We have developed an automated fitting routine that visits every pixel in the field of view of an ALMA data cube and determines the best-fit physical parameters, including excitation temperature and column densities, for a given list of molecules.","In this proof-of-concept work, we provide an overview of the fitting routine and apply it to 0\".26, 1.1 km s$^{-1}$ resolution ALMA observations of two sites of massive star-formation in NGC 6334I. Parameters were found for 21 distinct molecules by generating synthetic spectra across 7.48 GHz of spectral bandwidth between 280 and 351 GHz.","Spatial images of the derived parameters for each of the > 8000 pixels are presented with special attention paid to the C$_2$H$_4$O$_2$ isomers and their relative variations.","We highlight the greater scientific utility of the column density and velocity images of individual molecules compared to traditional moment maps of single transitions."],"url":"http://arxiv.org/abs/2402.13913v1","category":"astro-ph.GA"}
{"created":"2024-02-21 16:24:16","title":"Duality between the quantum inverted harmonic oscillator and inverse square potentials","abstract":"In this paper we show how the quantum mechanics of the inverted harmonic oscillator can be mapped to the quantum mechanics of a particle in a super-critical inverse square potential. We demonstrate this by relating both of these systems to the Berry-Keating system with hamiltonian $H=(xp+px)/2$. It has long been appreciated that the quantum mechanics of the inverse square potential has an ambiguity in choosing a boundary condition near the origin and we show how this ambiguity is mapped to the inverted harmonic oscillator system. Imposing a boundary condition requires specifying a distance scale where it is applied and changes to this scale come with a renormalization group (RG) evolution of the boundary condition that ensures observables do not directly depend on the scale (which is arbitrary). Physical scales instead emerge as RG invariants of this evolution. The RG flow for the inverse square potential is known to follow limit cycles describing the discrete breaking of classical scale invariance in a simple example of a quantum anomaly, and we find that limit cycles also occur for the inverted harmonic oscillator. However, unlike the inverse square potential where the continuous scaling symmetry is explicit, in the case of the inverted harmonic oscillator it is hidden and occurs because the hamiltonian is part of a larger su(1,1) spectrum generating algebra. Our map does not require the boundary condition to be self-adjoint, as can be appropriate for systems that involve the absorption or emission of particles.","sentences":["In this paper we show how the quantum mechanics of the inverted harmonic oscillator can be mapped to the quantum mechanics of a particle in a super-critical inverse square potential.","We demonstrate this by relating both of these systems to the Berry-Keating system with hamiltonian $H=(xp+px)/2$. It has long been appreciated that the quantum mechanics of the inverse square potential has an ambiguity in choosing a boundary condition near the origin and we show how this ambiguity is mapped to the inverted harmonic oscillator system.","Imposing a boundary condition requires specifying a distance scale where it is applied and changes to this scale come with a renormalization group (RG) evolution of the boundary condition that ensures observables do not directly depend on the scale (which is arbitrary).","Physical scales instead emerge as RG invariants of this evolution.","The RG flow for the inverse square potential is known to follow limit cycles describing the discrete breaking of classical scale invariance in a simple example of a quantum anomaly, and we find that limit cycles also occur for the inverted harmonic oscillator.","However, unlike the inverse square potential where the continuous scaling symmetry is explicit, in the case of the inverted harmonic oscillator it is hidden and occurs because the hamiltonian is part of a larger su(1,1) spectrum generating algebra.","Our map does not require the boundary condition to be self-adjoint, as can be appropriate for systems that involve the absorption or emission of particles."],"url":"http://arxiv.org/abs/2402.13909v1","category":"quant-ph"}
{"created":"2024-02-21 16:23:37","title":"Peridynamic micromechanics of composites: a review","abstract":"We consider a static peridynamic (proposed by Silling, see J. Mech. Phys. Solids 2000; 48:175--209) composite materials (CMs) of both the random and periodic structures. In the framework of the second background of micromechanics (also called computational analytical micromechanics, CAM), one proved that local micromechanics (LM) and peridynamic micromechanics (PM) are formally similar to each other for CM of both random and periodic structures. It allows straightforward generalization of LM methods to their PM counterparts. It turns out that a plurality of micromechanics phenomena [e.g. statistically homogeneous and inhomogeneous media, inhomogeneous loading (inhomogeneous body force is included), nonlinear and nonlocal constitutive laws of phases, and coupled physical phenomena] can be analyzed by one universal tool (called CAM), which is sufficiently flexible and based on physically clear hypotheses that can be modified and improved if necessary (up to abandonment of these hypotheses) in the framework of a unique scheme for analyses of a wide class of mentioned problems. The schemes of these approaches are considered in the current paper.","sentences":["We consider a static peridynamic (proposed by Silling, see J. Mech.","Phys.","Solids 2000; 48:175--209) composite materials (CMs) of both the random and periodic structures.","In the framework of the second background of micromechanics (also called computational analytical micromechanics, CAM), one proved that local micromechanics (LM) and peridynamic micromechanics (PM) are formally similar to each other for CM of both random and periodic structures.","It allows straightforward generalization of LM methods to their PM counterparts.","It turns out that a plurality of micromechanics phenomena [e.g. statistically homogeneous and inhomogeneous media, inhomogeneous loading (inhomogeneous body force is included), nonlinear and nonlocal constitutive laws of phases, and coupled physical phenomena] can be analyzed by one universal tool (called CAM), which is sufficiently flexible and based on physically clear hypotheses that can be modified and improved if necessary (up to abandonment of these hypotheses) in the framework of a unique scheme for analyses of a wide class of mentioned problems.","The schemes of these approaches are considered in the current paper."],"url":"http://arxiv.org/abs/2402.13908v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-21 16:16:50","title":"Herbrand's Theorem in Refutation Schemata","abstract":"An inductive proof can be represented as a proof schema, i.e. as a parameterized sequence of proofs defined in a primitive recursive way. A corresponding cut-elimination method, called schematic CERES, can be used to analyze these proofs, and to extract their (schematic) Herbrand sequents, even though Herbrand's theorem in general does not hold for proofs with induction inferences. This work focuses on the most crucial part of the schematic cut-elimination method, which is to construct a refutation of a schematic formula that represents the cut-structure of the original proof schema. We develop a new framework for schematic substitutions and define a unification algorithm for resolution schemata. Moreover, we show that this new formalism allows the extraction of a structure from the refutation schema, called a Herbrand schema, which represents its Herbrand sequent.","sentences":["An inductive proof can be represented as a proof schema, i.e. as a parameterized sequence of proofs defined in a primitive recursive way.","A corresponding cut-elimination method, called schematic CERES, can be used to analyze these proofs, and to extract their (schematic) Herbrand sequents, even though Herbrand's theorem in general does not hold for proofs with induction inferences.","This work focuses on the most crucial part of the schematic cut-elimination method, which is to construct a refutation of a schematic formula that represents the cut-structure of the original proof schema.","We develop a new framework for schematic substitutions and define a unification algorithm for resolution schemata.","Moreover, we show that this new formalism allows the extraction of a structure from the refutation schema, called a Herbrand schema, which represents its Herbrand sequent."],"url":"http://arxiv.org/abs/2402.13905v1","category":"math.LO"}
{"created":"2024-02-21 16:15:20","title":"Calibrating Large Language Models with Sample Consistency","abstract":"Accurately gauging the confidence level of Large Language Models' (LLMs) predictions is pivotal for their reliable application. However, LLMs are often uncalibrated inherently and elude conventional calibration techniques due to their proprietary nature and massive scale. In this work, we explore the potential of deriving confidence from the distribution of multiple randomly sampled model generations, via three measures of consistency. We perform an extensive evaluation across various open and closed-source models on nine reasoning datasets. Results show that consistency-based calibration methods outperform existing post-hoc approaches. Meanwhile, we find that factors such as intermediate explanations, model scaling, and larger sample sizes enhance calibration, while instruction-tuning makes calibration more difficult. Moreover, confidence scores obtained from consistency have the potential to enhance model performance. Finally, we offer practical guidance on choosing suitable consistency metrics for calibration, tailored to the characteristics of various LMs.","sentences":["Accurately gauging the confidence level of Large Language Models' (LLMs) predictions is pivotal for their reliable application.","However, LLMs are often uncalibrated inherently and elude conventional calibration techniques due to their proprietary nature and massive scale.","In this work, we explore the potential of deriving confidence from the distribution of multiple randomly sampled model generations, via three measures of consistency.","We perform an extensive evaluation across various open and closed-source models on nine reasoning datasets.","Results show that consistency-based calibration methods outperform existing post-hoc approaches.","Meanwhile, we find that factors such as intermediate explanations, model scaling, and larger sample sizes enhance calibration, while instruction-tuning makes calibration more difficult.","Moreover, confidence scores obtained from consistency have the potential to enhance model performance.","Finally, we offer practical guidance on choosing suitable consistency metrics for calibration, tailored to the characteristics of various LMs."],"url":"http://arxiv.org/abs/2402.13904v1","category":"cs.CL"}
{"created":"2024-02-21 16:13:49","title":"Dealing with unbounded gradients in stochastic saddle-point optimization","abstract":"We study the performance of stochastic first-order methods for finding saddle points of convex-concave functions. A notorious challenge faced by such methods is that the gradients can grow arbitrarily large during optimization, which may result in instability and divergence. In this paper, we propose a simple and effective regularization technique that stabilizes the iterates and yields meaningful performance guarantees even if the domain and the gradient noise scales linearly with the size of the iterates (and is thus potentially unbounded). Besides providing a set of general results, we also apply our algorithm to a specific problem in reinforcement learning, where it leads to performance guarantees for finding near-optimal policies in an average-reward MDP without prior knowledge of the bias span.","sentences":["We study the performance of stochastic first-order methods for finding saddle points of convex-concave functions.","A notorious challenge faced by such methods is that the gradients can grow arbitrarily large during optimization, which may result in instability and divergence.","In this paper, we propose a simple and effective regularization technique that stabilizes the iterates and yields meaningful performance guarantees even if the domain and the gradient noise scales linearly with the size of the iterates (and is thus potentially unbounded).","Besides providing a set of general results, we also apply our algorithm to a specific problem in reinforcement learning, where it leads to performance guarantees for finding near-optimal policies in an average-reward MDP without prior knowledge of the bias span."],"url":"http://arxiv.org/abs/2402.13903v1","category":"cs.LG"}
{"created":"2024-02-21 16:11:47","title":"Non-asymptotic Convergence of Discrete-time Diffusion Models: New Approach and Improved Rate","abstract":"The denoising diffusion model emerges recently as a powerful generative technique that converts noise into data. Theoretical convergence guarantee has been mainly studied for continuous-time diffusion models, and has been obtained for discrete-time diffusion models only for distributions with bounded support in the literature. In this paper, we establish the convergence guarantee for substantially larger classes of distributions under discrete-time diffusion models and further improve the convergence rate for distributions with bounded support. In particular, we first establish the convergence rates for both smooth and general (possibly non-smooth) distributions having finite second moment. We then specialize our results to a number of interesting classes of distributions with explicit parameter dependencies, including distributions with Lipschitz scores, Gaussian mixture distributions, and distributions with bounded support. We further propose a novel accelerated sampler and show that it improves the convergence rates of the corresponding regular sampler by orders of magnitude with respect to all system parameters. For distributions with bounded support, our result improves the dimensional dependence of the previous convergence rate by orders of magnitude. Our study features a novel analysis technique that constructs tilting factor representation of the convergence error and exploits Tweedie's formula for handling Taylor expansion power terms.","sentences":["The denoising diffusion model emerges recently as a powerful generative technique that converts noise into data.","Theoretical convergence guarantee has been mainly studied for continuous-time diffusion models, and has been obtained for discrete-time diffusion models only for distributions with bounded support in the literature.","In this paper, we establish the convergence guarantee for substantially larger classes of distributions under discrete-time diffusion models and further improve the convergence rate for distributions with bounded support.","In particular, we first establish the convergence rates for both smooth and general (possibly non-smooth) distributions having finite second moment.","We then specialize our results to a number of interesting classes of distributions with explicit parameter dependencies, including distributions with Lipschitz scores, Gaussian mixture distributions, and distributions with bounded support.","We further propose a novel accelerated sampler and show that it improves the convergence rates of the corresponding regular sampler by orders of magnitude with respect to all system parameters.","For distributions with bounded support, our result improves the dimensional dependence of the previous convergence rate by orders of magnitude.","Our study features a novel analysis technique that constructs tilting factor representation of the convergence error and exploits Tweedie's formula for handling Taylor expansion power terms."],"url":"http://arxiv.org/abs/2402.13901v1","category":"cs.LG"}
{"created":"2024-02-21 16:11:29","title":"Rapid Conformational Analysis of Semi-Flexible Liquid Crystals","abstract":"We present an approach for rapid conformational analysis of semi-flexible liquid crystals. We use a simple graphical user interface (GUI) tool that leverages rules-based methods for efficient generation of bend-angle distributions, offering a significant improvement over traditional single-conformer analysis. Our methods demonstrated proficiency in approximating molecular shapes comparable to those obtained from molecular dynamics (MD) simulations, albeit with notable deviations in the under sampling of hairpin conformations and oversampling of extended configurations. Re-evaluation of existing data revealed an apparent weak correlation between NTB transition temperatures and bend angles, underscoring the complexity of molecular shapes beyond mere geometry. Furthermore, we integrated this conformational analysis into a pipeline of algorithmic molecular design, utilizing a fragment-based genetic algorithm to generate novel cyanobiphenyl-containing materials. This integration opens new avenues for the exploration of liquid crystalline materials, particularly in systems where systematic conformer searches are impractical, such as large oligomeric systems. Our findings highlight the potential and growing importance of computational approaches in accelerating the design and synthesis of next-generation liquid crystalline materials.","sentences":["We present an approach for rapid conformational analysis of semi-flexible liquid crystals.","We use a simple graphical user interface (GUI) tool that leverages rules-based methods for efficient generation of bend-angle distributions, offering a significant improvement over traditional single-conformer analysis.","Our methods demonstrated proficiency in approximating molecular shapes comparable to those obtained from molecular dynamics (MD) simulations, albeit with notable deviations in the under sampling of hairpin conformations and oversampling of extended configurations.","Re-evaluation of existing data revealed an apparent weak correlation between NTB transition temperatures and bend angles, underscoring the complexity of molecular shapes beyond mere geometry.","Furthermore, we integrated this conformational analysis into a pipeline of algorithmic molecular design, utilizing a fragment-based genetic algorithm to generate novel cyanobiphenyl-containing materials.","This integration opens new avenues for the exploration of liquid crystalline materials, particularly in systems where systematic conformer searches are impractical, such as large oligomeric systems.","Our findings highlight the potential and growing importance of computational approaches in accelerating the design and synthesis of next-generation liquid crystalline materials."],"url":"http://arxiv.org/abs/2402.13900v1","category":"cond-mat.soft"}
{"created":"2024-02-21 16:10:38","title":"On the limitations of non-geometric fluxes to realize dS vacua","abstract":"In this paper, we perform a systematic and analytical exploration of de Sitter conditions in type IIA compactifications with (non-)geometric fluxes along with the standard NS-NS and RR $p$-form fluxes. Exploiting the fact that the F-term scalar potential can be written as a bilinear form, we start by studying the most generic case. We find four conditions that the scalar fields and fluxes must satisfy to achieve de Sitter vacua. Particularizing to different configurations, we recover and extend previous results in the literature. We then impose an Ansatz in which the F-terms are proportional to the respective K\\\"ahler derivatives. In this set-up we are able to derive additional constraints and to classify the possible dS no-go scenarios in terms of eight axionic fluxes. Individually considering that any of these fluxes can be vanishing or non-vanishing leads to a total of 256 flux configurations. We find that 227 of these 256 possibilities result in a dS no-go scenario. The remaining 29 flux configurations, a priori, do not lead to dS no-go cases and would deserve further investigation.","sentences":["In this paper, we perform a systematic and analytical exploration of de Sitter conditions in type IIA compactifications with (non-)geometric fluxes along with the standard NS-NS and RR","$p$-form fluxes.","Exploiting the fact that the F-term scalar potential can be written as a bilinear form, we start by studying the most generic case.","We find four conditions that the scalar fields and fluxes must satisfy to achieve de Sitter vacua.","Particularizing to different configurations, we recover and extend previous results in the literature.","We then impose an Ansatz in which the F-terms are proportional to the respective K\\\"ahler derivatives.","In this set-up we are able to derive additional constraints and to classify the possible dS no-go scenarios in terms of eight axionic fluxes.","Individually considering that any of these fluxes can be vanishing or non-vanishing leads to a total of 256 flux configurations.","We find that 227 of these 256 possibilities result in a dS no-go scenario.","The remaining 29 flux configurations, a priori, do not lead to dS no-go cases and would deserve further investigation."],"url":"http://arxiv.org/abs/2402.13899v1","category":"hep-th"}
{"created":"2024-02-21 16:09:25","title":"Science Checker Reloaded: A Bidirectional Paradigm for Transparency and Logical Reasoning","abstract":"Information retrieval is a rapidly evolving field. However it still faces significant limitations in the scientific and industrial vast amounts of information, such as semantic divergence and vocabulary gaps in sparse retrieval, low precision and lack of interpretability in semantic search, or hallucination and outdated information in generative models. In this paper, we introduce a two-block approach to tackle these hurdles for long documents. The first block enhances language understanding in sparse retrieval by query expansion to retrieve relevant documents. The second block deepens the result by providing comprehensive and informative answers to the complex question using only the information spread in the long document, enabling bidirectional engagement. At various stages of the pipeline, intermediate results are presented to users to facilitate understanding of the system's reasoning. We believe this bidirectional approach brings significant advancements in terms of transparency, logical thinking, and comprehensive understanding in the field of scientific information retrieval.","sentences":["Information retrieval is a rapidly evolving field.","However it still faces significant limitations in the scientific and industrial vast amounts of information, such as semantic divergence and vocabulary gaps in sparse retrieval, low precision and lack of interpretability in semantic search, or hallucination and outdated information in generative models.","In this paper, we introduce a two-block approach to tackle these hurdles for long documents.","The first block enhances language understanding in sparse retrieval by query expansion to retrieve relevant documents.","The second block deepens the result by providing comprehensive and informative answers to the complex question using only the information spread in the long document, enabling bidirectional engagement.","At various stages of the pipeline, intermediate results are presented to users to facilitate understanding of the system's reasoning.","We believe this bidirectional approach brings significant advancements in terms of transparency, logical thinking, and comprehensive understanding in the field of scientific information retrieval."],"url":"http://arxiv.org/abs/2402.13897v1","category":"cs.IR"}
{"created":"2024-02-21 16:05:49","title":"Grover's oracle for the Shortest Vector Problem and its application in hybrid classical-quantum solvers","abstract":"Finding the shortest vector in a lattice is a problem that is believed to be hard both for classical and quantum computers. Many major post-quantum secure cryptosystems base their security on the hardness of the Shortest Vector Problem (SVP). Finding the best classical, quantum or hybrid classical-quantum algorithms for SVP is necessary to select cryptosystem parameters that offer sufficient level of security. Grover's search quantum algorithm provides a generic quadratic speed-up, given access to an oracle implementing some function which describes when a solution is found. In this paper we provide concrete implementation of such an oracle for the SVP. We define the circuit, and evaluate costs in terms of number of qubits, number of gates, depth and T-quantum cost. We then analyze how to combine Grover's quantum search for small SVP instances with state-of-the-art classical solvers that use well known algorithms, such as the BKZ, where the former is used as a subroutine. This could enable solving larger instances of SVP with higher probability than classical state-of-the-art records, but still very far from posing any threat to cryptosystems being considered for standardization. Depending on the technology available, there is a spectrum of trade-offs in creating this combination.","sentences":["Finding the shortest vector in a lattice is a problem that is believed to be hard both for classical and quantum computers.","Many major post-quantum secure cryptosystems base their security on the hardness of the Shortest Vector Problem (SVP).","Finding the best classical, quantum or hybrid classical-quantum algorithms for SVP is necessary to select cryptosystem parameters that offer sufficient level of security.","Grover's search quantum algorithm provides a generic quadratic speed-up, given access to an oracle implementing some function which describes when a solution is found.","In this paper we provide concrete implementation of such an oracle for the SVP.","We define the circuit, and evaluate costs in terms of number of qubits, number of gates, depth and T-quantum cost.","We then analyze how to combine Grover's quantum search for small SVP instances with state-of-the-art classical solvers that use well known algorithms, such as the BKZ, where the former is used as a subroutine.","This could enable solving larger instances of SVP with higher probability than classical state-of-the-art records, but still very far from posing any threat to cryptosystems being considered for standardization.","Depending on the technology available, there is a spectrum of trade-offs in creating this combination."],"url":"http://arxiv.org/abs/2402.13895v1","category":"quant-ph"}
{"created":"2024-02-21 16:05:33","title":"Quantum particle localization observables on Cauchy surfaces of Minkowski spacetime and their causal properties","abstract":"We introduce and study a general notion of spatial localization on spacelike smooth Cauchy surfaces of quantum systems in Minkowski spacetime. The notion is constructed in terms of a coherent family of normalized POVMs, one for each said Cauchy surface. We prove that a family of POVMs of this type automatically satisfies a causality condition which generalizes Castrigiano's one and implies it when restricting to flat spacelike Cauchy surfaces. As a consequence no conflict with Hegerfeldt's theorem arises. We furthermore prove that such families of POVMs do exist for massive Klein-Gordon particles, since some of them are extensions of already known spatial localization observables. These are construted out of positive definite kernels or are defined in terms of the stress-energy tensor operator. Some further features of these structures are investigated, in particular, the relation with the triple of Newton-Wigner selfadjoint operators and a modified form of Heisenberg inequality in the rest $3$-spaces of Minkowski reference frames","sentences":["We introduce and study a general notion of spatial localization on spacelike smooth Cauchy surfaces of quantum systems in Minkowski spacetime.","The notion is constructed in terms of a coherent family of normalized POVMs, one for each said Cauchy surface.","We prove that a family of POVMs of this type automatically satisfies a causality condition which generalizes Castrigiano's one and implies it when restricting to flat spacelike Cauchy surfaces.","As a consequence no conflict with Hegerfeldt's theorem arises.","We furthermore prove that such families of POVMs do exist for massive Klein-Gordon particles, since some of them are extensions of already known spatial localization observables.","These are construted out of positive definite kernels or are defined in terms of the stress-energy tensor operator.","Some further features of these structures are investigated, in particular, the relation with the triple of Newton-Wigner selfadjoint operators and a modified form of Heisenberg inequality in the rest $3$-spaces of Minkowski reference frames"],"url":"http://arxiv.org/abs/2402.13894v1","category":"math-ph"}
{"created":"2024-02-21 16:00:42","title":"Bispectral duality and separation of variables from surface defect transition","abstract":"We study two types of surface observables $-$ the $\\mathbf{Q}$-observables and the $\\mathbf{H}$-observables $-$ of the 4d $\\mathcal{N}=2$ $A_1$-quiver $U(N)$ gauge theory obtained by coupling a 2d $\\mathcal{N}=(2,2)$ gauged linear sigma model. We demonstrate that the transition between the two surface defects manifests as a Fourier transformation between the surface observables. Utilizing the results from our previous works, which establish that the $\\mathbf{Q}$-observables and the $\\mathbf{H}$-observables give rise, respectively, to the $Q$-operators on the evaluation module over the Yangian $Y(\\mathfrak{gl}(2))$ and the Hecke operators on the twisted $\\widehat{\\mathfrak{sl}}(N)$-coinvariants, we derive an exact duality between the spectral problems of the $\\mathfrak{gl}(2)$ XXX spin chain with $N$ sites and the $\\mathfrak{sl}(N)$ Gaudin model with 4 sites, both of which are defined on bi-infinite modules. Moreover, we present a dual description of the monodromy surface defect as coupling a 2d $\\mathcal{N}=(2,2)$ gauged linear sigma model. Employing this dual perspective, we demonstrate how the monodromy surface defect undergoes a transition to multiple $\\mathbf{Q}$-observables or $\\mathbf{H}$-observables, implemented through integral transformations between their surface observables. These transformations provide, respectively, $\\hbar$-deformation and a higher-rank generalization of the KZ/BPZ correspondence. In the limit $\\varepsilon_2\\to 0$, they give rise to the quantum separation of variables for the $\\mathfrak{gl}(2)$ XXX spin chain and the $\\mathfrak{sl}(N)$ Gaudin model, respectively.","sentences":["We study two types of surface observables $-$ the $\\mathbf{Q}$-observables and the $\\mathbf{H}$-observables $-$ of the 4d $\\mathcal{N}=2$ $A_1$-quiver $U(N)$ gauge theory obtained by coupling a 2d $\\mathcal{N}=(2,2)$ gauged linear sigma model.","We demonstrate that the transition between the two surface defects manifests as a Fourier transformation between the surface observables.","Utilizing the results from our previous works, which establish that the $\\mathbf{Q}$-observables and the $\\mathbf{H}$-observables give rise, respectively, to the $Q$-operators on the evaluation module over the Yangian $Y(\\mathfrak{gl}(2))$ and the Hecke operators on the twisted $\\widehat{\\mathfrak{sl}}(N)$-coinvariants, we derive an exact duality between the spectral problems of the $\\mathfrak{gl}(2)$ XXX spin chain with $N$ sites and the $\\mathfrak{sl}(N)$ Gaudin model with 4 sites, both of which are defined on bi-infinite modules.","Moreover, we present a dual description of the monodromy surface defect as coupling a 2d $\\mathcal{N}=(2,2)$ gauged linear sigma model.","Employing this dual perspective, we demonstrate how the monodromy surface defect undergoes a transition to multiple $\\mathbf{Q}$-observables or $\\mathbf{H}$-observables, implemented through integral transformations between their surface observables.","These transformations provide, respectively, $\\hbar$-deformation and a higher-rank generalization of the KZ/BPZ correspondence.","In the limit $\\varepsilon_2\\to 0$, they give rise to the quantum separation of variables for the $\\mathfrak{gl}(2)$ XXX spin chain and the $\\mathfrak{sl}(N)$ Gaudin model, respectively."],"url":"http://arxiv.org/abs/2402.13889v1","category":"hep-th"}
{"created":"2024-02-21 15:58:37","title":"Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models","abstract":"Large Language Models (LLMs) have demonstrated remarkable capabilities across various applications, fundamentally reshaping the landscape of natural language processing (NLP) research. However, recent evaluation frameworks often rely on the output probabilities of LLMs for predictions, primarily due to computational constraints, diverging from real-world LLM usage scenarios. While widely employed, the efficacy of these probability-based evaluation strategies remains an open research question. This study aims to scrutinize the validity of such probability-based evaluation methods within the context of using LLMs for Multiple Choice Questions (MCQs), highlighting their inherent limitations. Our empirical investigation reveals that the prevalent probability-based evaluation method inadequately aligns with generation-based prediction. Furthermore, current evaluation frameworks typically assess LLMs through predictive tasks based on output probabilities rather than directly generating responses, owing to computational limitations. We illustrate that these probability-based approaches do not effectively correspond with generative predictions. The outcomes of our study can enhance the understanding of LLM evaluation methodologies and provide insights for future research in this domain.","sentences":["Large Language Models (LLMs) have demonstrated remarkable capabilities across various applications, fundamentally reshaping the landscape of natural language processing (NLP) research.","However, recent evaluation frameworks often rely on the output probabilities of LLMs for predictions, primarily due to computational constraints, diverging from real-world LLM usage scenarios.","While widely employed, the efficacy of these probability-based evaluation strategies remains an open research question.","This study aims to scrutinize the validity of such probability-based evaluation methods within the context of using LLMs for Multiple Choice Questions (MCQs), highlighting their inherent limitations.","Our empirical investigation reveals that the prevalent probability-based evaluation method inadequately aligns with generation-based prediction.","Furthermore, current evaluation frameworks typically assess LLMs through predictive tasks based on output probabilities rather than directly generating responses, owing to computational limitations.","We illustrate that these probability-based approaches do not effectively correspond with generative predictions.","The outcomes of our study can enhance the understanding of LLM evaluation methodologies and provide insights for future research in this domain."],"url":"http://arxiv.org/abs/2402.13887v1","category":"cs.CL"}
{"created":"2024-02-21 15:50:56","title":"Isospin precession in non-Abelian Aharonov-Bohm scattering","abstract":"The concept of pseudoclassical isospin is illustrated by the non-Abelian Aharonov-Bohm effect proposed by Wu and Yang in 1975. The spatial motion is free however the isospin precesses when the enclosed magnetic flux and the incoming particle's isosopin are not parallel. The non-Abelian phase factor $\\mathfrak{F}$ of Wu and Yang acts on the isospin as an S-matrix. The scattering becomes side-independent when the enclosed flux is quantized, ${\\Phi}_N=N\\Phi_0$ with $N$ an integer. The gauge group $SU(2)$ is an internal symmetry and generates conserved charges only when the flux is quantized, which then splits into two series: for $N=2k$ $SU(2)$ acts trivially but for $N=1+2k$ the implementation is twisted. The orbital and the internal angular momenta are separately conserved. The double rotational symmetry is broken to $SO(2)\\times SO(2)$ when $N$ odd. For unquantized flux there are no internal symmetries, the charge is not conserved and protons can be turned into neutrons.","sentences":["The concept of pseudoclassical isospin is illustrated by the non-Abelian Aharonov-Bohm effect proposed by Wu and Yang in 1975.","The spatial motion is free however the isospin precesses when the enclosed magnetic flux and the incoming particle's isosopin are not parallel.","The non-Abelian phase factor $\\mathfrak{F}$ of Wu and Yang acts on the isospin as an S-matrix.","The scattering becomes side-independent when the enclosed flux is quantized, ${\\Phi}_N=N\\Phi_0$ with $N$ an integer.","The gauge group $SU(2)$ is an internal symmetry and generates conserved charges only when the flux is quantized, which then splits into two series: for $N=2k$ $SU(2)$ acts trivially but for $N=1+2k$ the implementation is twisted.","The orbital and the internal angular momenta are separately conserved.","The double rotational symmetry is broken to $SO(2)\\times SO(2)$ when $N$ odd.","For unquantized flux there are no internal symmetries, the charge is not conserved and protons can be turned into neutrons."],"url":"http://arxiv.org/abs/2402.13883v1","category":"hep-th"}
{"created":"2024-02-21 15:49:33","title":"Partial-transpose-guided entanglement classes and minimum noise filtering in many-body Gaussian quantum systems","abstract":"The reduction and distortion of quantum correlations in the presence of classical noise leads to varied levels of inefficiency in the availability of entanglement as a resource for quantum information processing protocols. While generically minimizing required entanglement for mixed quantum states remains challenging, a class of many-body Gaussian quantum states ($\\mathcal{N}$IC) is here identified that exhibits two-mode bipartite entanglement structure, resembling that of pure states, for which the logarithmic negativity entanglement measure remains invariant upon inclusion of the classical correlations and optimal entanglement resources can be clearly quantified. This subclass is found to be embedded within a broader class of many-body Gaussian states ($\\mathcal{N}$-SOL) that retain two-mode entanglement structure for detection processes. These two entanglement classes are relevant in theoretical and experimental applications from the scalar field vacuum to the local axial motional modes of trapped ion chains. Utilizing the subspace that heralds inseparability in response to partial transposition, a minimum noise filtering process is designed to be necessary, sufficient, and computable for determining membership in these classes of entanglement structure. Application of this process to spacelike regions of the free scalar field vacuum is found to improve resource upper bounds, providing new understanding of the entanglement required for the quantum simulation of quantum fields as observed by arrays of local detectors.","sentences":["The reduction and distortion of quantum correlations in the presence of classical noise leads to varied levels of inefficiency in the availability of entanglement as a resource for quantum information processing protocols.","While generically minimizing required entanglement for mixed quantum states remains challenging, a class of many-body Gaussian quantum states ($\\mathcal{N}$IC) is here identified that exhibits two-mode bipartite entanglement structure, resembling that of pure states, for which the logarithmic negativity entanglement measure remains invariant upon inclusion of the classical correlations and optimal entanglement resources can be clearly quantified.","This subclass is found to be embedded within a broader class of many-body Gaussian states ($\\mathcal{N}$-SOL) that retain two-mode entanglement structure for detection processes.","These two entanglement classes are relevant in theoretical and experimental applications from the scalar field vacuum to the local axial motional modes of trapped ion chains.","Utilizing the subspace that heralds inseparability in response to partial transposition, a minimum noise filtering process is designed to be necessary, sufficient, and computable for determining membership in these classes of entanglement structure.","Application of this process to spacelike regions of the free scalar field vacuum is found to improve resource upper bounds, providing new understanding of the entanglement required for the quantum simulation of quantum fields as observed by arrays of local detectors."],"url":"http://arxiv.org/abs/2402.13881v1","category":"quant-ph"}
{"created":"2024-02-21 15:42:13","title":"Infinite (continuous) spin particle in constant curvature space","abstract":"We present a new particle model that generalizes for curved space-time an infinite spin particle in flat space. The model is described by commuting Weyl spinor additional coordinates. It is proved that this model is consistent only in an external gravitational field corresponding to constant curvature spaces. A full set of first-class constraints in de Sitter and anti-de Sitter spaces is obtained.","sentences":["We present a new particle model that generalizes for curved space-time an infinite spin particle in flat space.","The model is described by commuting Weyl spinor additional coordinates.","It is proved that this model is consistent only in an external gravitational field corresponding to constant curvature spaces.","A full set of first-class constraints in de Sitter and anti-de Sitter spaces is obtained."],"url":"http://arxiv.org/abs/2402.13879v1","category":"hep-th"}
{"created":"2024-02-21 15:39:31","title":"Reference Energies for Valence Ionizations and Satellite Transitions","abstract":"Upon ionization of an atom or a molecule, another electron (or more) can be simultaneously excited. These concurrently generated states are called ``satellites'' (or shake-up transitions) as they appear in ionization spectra as higher-energy peaks with weaker intensity and larger width than the main peaks associated with single-particle ionizations. Satellites, which correspond to electronically excited states of the cationic species, are notoriously challenging to model using conventional single-reference methods due to their high excitation degree compared to the neutral reference state. This work reports 40 satellite transition energies and 58 valence ionization potentials of full configuration interaction (FCI) quality computed in small molecular systems. Following the protocol developed for the QUEST database [V\\'eril, M.; et al. Wiley Interdiscip. Rev.: Comput. Mol.Sci. 2021, 11, e1517, https://doi.org/10.1002/wcms.1517], these reference energies are computed using the configuration interaction using a perturbative selection made iteratively (CIPSI) method. In addition, the accuracy of the well-known coupled-cluster (CC) hierarchy (CC2, CCSD, CC3, CCSDT, CC4, and CCSDTQ) is gauged against these new accurate references. The performances of various approximations based on many-body Green's functions ($GW$, GF2, and $T$-matrix) for ionization potentials are also analyzed. Their limitations in correctly modeling satellite transitions are discussed.","sentences":["Upon ionization of an atom or a molecule, another electron (or more) can be simultaneously excited.","These concurrently generated states are called ``satellites'' (or shake-up transitions) as they appear in ionization spectra as higher-energy peaks with weaker intensity and larger width than the main peaks associated with single-particle ionizations.","Satellites, which correspond to electronically excited states of the cationic species, are notoriously challenging to model using conventional single-reference methods due to their high excitation degree compared to the neutral reference state.","This work reports 40 satellite transition energies and 58 valence ionization potentials of full configuration interaction (FCI) quality computed in small molecular systems.","Following the protocol developed for the QUEST database [V\\'eril, M.; et al. Wiley Interdiscip.","Rev.:","Comput.","Mol.Sci. 2021, 11, e1517, https://doi.org/10.1002/wcms.1517], these reference energies are computed using the configuration interaction using a perturbative selection made iteratively (CIPSI) method.","In addition, the accuracy of the well-known coupled-cluster (CC) hierarchy (CC2, CCSD, CC3, CCSDT, CC4, and CCSDTQ) is gauged against these new accurate references.","The performances of various approximations based on many-body Green's functions ($GW$, GF2, and $T$-matrix) for ionization potentials are also analyzed.","Their limitations in correctly modeling satellite transitions are discussed."],"url":"http://arxiv.org/abs/2402.13877v1","category":"physics.chem-ph"}
{"created":"2024-02-21 15:23:21","title":"Generative Probabilistic Time Series Forecasting and Applications in Grid Operations","abstract":"Generative probabilistic forecasting produces future time series samples according to the conditional probability distribution given past time series observations. Such techniques are essential in risk-based decision-making and planning under uncertainty with broad applications in grid operations, including electricity price forecasting, risk-based economic dispatch, and stochastic optimizations. Inspired by Wiener and Kallianpur's innovation representation, we propose a weak innovation autoencoder architecture and a learning algorithm to extract independent and identically distributed innovation sequences from nonparametric stationary time series. We show that the weak innovation sequence is Bayesian sufficient, which makes the proposed weak innovation autoencoder a canonical architecture for generative probabilistic forecasting. The proposed technique is applied to forecasting highly volatile real-time electricity prices, demonstrating superior performance across multiple forecasting measures over leading probabilistic and point forecasting techniques.","sentences":["Generative probabilistic forecasting produces future time series samples according to the conditional probability distribution given past time series observations.","Such techniques are essential in risk-based decision-making and planning under uncertainty with broad applications in grid operations, including electricity price forecasting, risk-based economic dispatch, and stochastic optimizations.","Inspired by Wiener and Kallianpur's innovation representation, we propose a weak innovation autoencoder architecture and a learning algorithm to extract independent and identically distributed innovation sequences from nonparametric stationary time series.","We show that the weak innovation sequence is Bayesian sufficient, which makes the proposed weak innovation autoencoder a canonical architecture for generative probabilistic forecasting.","The proposed technique is applied to forecasting highly volatile real-time electricity prices, demonstrating superior performance across multiple forecasting measures over leading probabilistic and point forecasting techniques."],"url":"http://arxiv.org/abs/2402.13870v1","category":"cs.LG"}
{"created":"2024-02-21 15:23:21","title":"An Explainable Transformer-based Model for Phishing Email Detection: A Large Language Model Approach","abstract":"Phishing email is a serious cyber threat that tries to deceive users by sending false emails with the intention of stealing confidential information or causing financial harm. Attackers, often posing as trustworthy entities, exploit technological advancements and sophistication to make detection and prevention of phishing more challenging. Despite extensive academic research, phishing detection remains an ongoing and formidable challenge in the cybersecurity landscape. Large Language Models (LLMs) and Masked Language Models (MLMs) possess immense potential to offer innovative solutions to address long-standing challenges. In this research paper, we present an optimized, fine-tuned transformer-based DistilBERT model designed for the detection of phishing emails. In the detection process, we work with a phishing email dataset and utilize the preprocessing techniques to clean and solve the imbalance class issues. Through our experiments, we found that our model effectively achieves high accuracy, demonstrating its capability to perform well. Finally, we demonstrate our fine-tuned model using Explainable-AI (XAI) techniques such as Local Interpretable Model-Agnostic Explanations (LIME) and Transformer Interpret to explain how our model makes predictions in the context of text classification for phishing emails.","sentences":["Phishing email is a serious cyber threat that tries to deceive users by sending false emails with the intention of stealing confidential information or causing financial harm.","Attackers, often posing as trustworthy entities, exploit technological advancements and sophistication to make detection and prevention of phishing more challenging.","Despite extensive academic research, phishing detection remains an ongoing and formidable challenge in the cybersecurity landscape.","Large Language Models (LLMs) and Masked Language Models (MLMs) possess immense potential to offer innovative solutions to address long-standing challenges.","In this research paper, we present an optimized, fine-tuned transformer-based DistilBERT model designed for the detection of phishing emails.","In the detection process, we work with a phishing email dataset and utilize the preprocessing techniques to clean and solve the imbalance class issues.","Through our experiments, we found that our model effectively achieves high accuracy, demonstrating its capability to perform well.","Finally, we demonstrate our fine-tuned model using Explainable-AI (XAI) techniques such as Local Interpretable Model-Agnostic Explanations (LIME) and Transformer Interpret to explain how our model makes predictions in the context of text classification for phishing emails."],"url":"http://arxiv.org/abs/2402.13871v1","category":"cs.LG"}
{"created":"2024-02-21 15:20:58","title":"A Uniformly Random Solution to Algorithmic Redistricting","abstract":"The process of drawing electoral district boundaries is known as political redistricting. Within this context, gerrymandering is the practice of drawing these boundaries such that they unfairly favor a particular political party, often leading to unequal representation and skewed electoral outcomes. One of the few ways to detect gerrymandering is by algorithmically sampling redistricting plans. Previous methods mainly focus on sampling from some neighborhood of ``realistic' districting plans, rather than a uniform sample of the entire space. We present a deterministic subexponential time algorithm to uniformly sample from the space of all possible $ k $-partitions of a bounded degree planar graph, and with this construct a sample of the entire space of redistricting plans. We also give a way to restrict this sample space to plans that match certain compactness and population constraints at the cost of added complexity. The algorithm runs in $ 2^{O(\\sqrt{n}\\log n)} $ time, although we only give a heuristic implementation. Our method generalizes an algorithm to count self-avoiding walks on a square to count paths that split general planar graphs into $ k $ regions, and uses this to sample from the space of all $ k $-partitions of a planar graph.","sentences":["The process of drawing electoral district boundaries is known as political redistricting.","Within this context, gerrymandering is the practice of drawing these boundaries such that they unfairly favor a particular political party, often leading to unequal representation and skewed electoral outcomes.","One of the few ways to detect gerrymandering is by algorithmically sampling redistricting plans.","Previous methods mainly focus on sampling from some neighborhood of ``realistic' districting plans, rather than a uniform sample of the entire space.","We present a deterministic subexponential time algorithm to uniformly sample from the space of all possible $ k $-partitions of a bounded degree planar graph, and with this construct a sample of the entire space of redistricting plans.","We also give a way to restrict this sample space to plans that match certain compactness and population constraints at the cost of added complexity.","The algorithm runs in $ 2^{O(\\sqrt{n}\\log n)} $ time, although we only give a heuristic implementation.","Our method generalizes an algorithm to count self-avoiding walks on a square to count paths that split general planar graphs into $ k $ regions, and uses this to sample from the space of all $ k $-partitions of a planar graph."],"url":"http://arxiv.org/abs/2402.13868v1","category":"cs.DS"}
{"created":"2024-02-21 15:19:09","title":"RFI-DRUnet: Restoring dynamic spectra corrupted by radio frequency interference -- Application to pulsar observations","abstract":"Radio frequency interference (RFI) have been an enduring concern in radio astronomy, particularly for the observations of pulsars which require high timing precision and data sensitivity. In most works of the literature, RFI mitigation has been formulated as a detection task that consists of localizing possible RFI in dynamic spectra. This strategy inevitably leads to a potential loss of information since parts of the signal identified as possibly RFI-corrupted are generally not considered in the subsequent data processing pipeline. Conversely, this work proposes to tackle RFI mitigation as a joint detection and restoration that allows parts of the dynamic spectrum affected by RFI to be not only identified but also recovered. The proposed supervised method relies on a deep convolutional network whose architecture inherits the performance reached by a recent yet popular image-denoising network. To train this network, a whole simulation framework is built to generate large data sets according to physics-inspired and statistical models of the pulsar signals and of the RFI. The relevance of the proposed approach is quantitatively assessed by conducting extensive experiments. In particular, the results show that the restored dynamic spectra are sufficiently reliable to estimate pulsar times-of-arrivals with an accuracy close to the one that would be obtained from RFI-free signals.","sentences":["Radio frequency interference (RFI) have been an enduring concern in radio astronomy, particularly for the observations of pulsars which require high timing precision and data sensitivity.","In most works of the literature, RFI mitigation has been formulated as a detection task that consists of localizing possible RFI in dynamic spectra.","This strategy inevitably leads to a potential loss of information since parts of the signal identified as possibly RFI-corrupted are generally not considered in the subsequent data processing pipeline.","Conversely, this work proposes to tackle RFI mitigation as a joint detection and restoration that allows parts of the dynamic spectrum affected by RFI to be not only identified but also recovered.","The proposed supervised method relies on a deep convolutional network whose architecture inherits the performance reached by a recent yet popular image-denoising network.","To train this network, a whole simulation framework is built to generate large data sets according to physics-inspired and statistical models of the pulsar signals and of the RFI.","The relevance of the proposed approach is quantitatively assessed by conducting extensive experiments.","In particular, the results show that the restored dynamic spectra are sufficiently reliable to estimate pulsar times-of-arrivals with an accuracy close to the one that would be obtained from RFI-free signals."],"url":"http://arxiv.org/abs/2402.13867v1","category":"astro-ph.IM"}
{"created":"2024-02-21 15:14:20","title":"Kuaiji: the First Chinese Accounting Large Language Model","abstract":"Large Language Models (LLMs) like ChatGPT and GPT-4 have demonstrated impressive proficiency in comprehending and generating natural language. However, they encounter difficulties when tasked with adapting to specialized domains such as accounting. To address this challenge, we introduce Kuaiji, a tailored Accounting Large Language Model. Kuaiji is meticulously fine-tuned using the Baichuan framework, which encompasses continuous pre-training and supervised fine-tuning processes. Supported by CAtAcctQA, a dataset containing large genuine accountant-client dialogues, Kuaiji exhibits exceptional accuracy and response speed. Our contributions encompass the creation of the first Chinese accounting dataset, the establishment of Kuaiji as a leading open-source Chinese accounting LLM, and the validation of its efficacy through real-world accounting scenarios.","sentences":["Large Language Models (LLMs) like ChatGPT and GPT-4 have demonstrated impressive proficiency in comprehending and generating natural language.","However, they encounter difficulties when tasked with adapting to specialized domains such as accounting.","To address this challenge, we introduce Kuaiji, a tailored Accounting Large Language Model.","Kuaiji is meticulously fine-tuned using the Baichuan framework, which encompasses continuous pre-training and supervised fine-tuning processes.","Supported by CAtAcctQA, a dataset containing large genuine accountant-client dialogues, Kuaiji exhibits exceptional accuracy and response speed.","Our contributions encompass the creation of the first Chinese accounting dataset, the establishment of Kuaiji as a leading open-source Chinese accounting LLM, and the validation of its efficacy through real-world accounting scenarios."],"url":"http://arxiv.org/abs/2402.13866v1","category":"cs.CL"}
{"created":"2024-02-21 15:12:40","title":"How to fault-tolerantly realize any quantum circuit with local operations","abstract":"We show how to realize a general quantum circuit involving gates between arbitrary pairs of qubits by means of geometrically local quantum operations and efficient classical computation. We prove that circuit-level local stochastic noise modeling an imperfect implementation of our derived schemes is equivalent to local stochastic noise in the original circuit. Our constructions incur a constant-factor increase in the quantum circuit depth and a polynomial overhead in the number of qubits: To execute an arbitrary quantum circuit on $n$ qubits, we give a 3D quantum fault-tolerance architecture involving $O(n^{3/2} \\log^3 n)$ qubits, and a quasi-2D architecture using $O(n^2 \\log^3 n)$ qubits. Applied to recent fault-tolerance constructions, this gives a fault-tolerance threshold theorem for universal quantum computations with local operations, a polynomial qubit overhead and a quasi-polylogarithmic depth overhead. More generally, our transformation dispenses with the need for considering the locality of operations when designing schemes for fault-tolerant quantum information processing.","sentences":["We show how to realize a general quantum circuit involving gates between arbitrary pairs of qubits by means of geometrically local quantum operations and efficient classical computation.","We prove that circuit-level local stochastic noise modeling an imperfect implementation of our derived schemes is equivalent to local stochastic noise in the original circuit.","Our constructions incur a constant-factor increase in the quantum circuit depth and a polynomial overhead in the number of qubits: To execute an arbitrary quantum circuit on $n$ qubits, we give a 3D quantum fault-tolerance architecture involving $O(n^{3/2} \\log^3 n)$ qubits, and a quasi-2D architecture using $O(n^2 \\log^3 n)$ qubits.","Applied to recent fault-tolerance constructions, this gives a fault-tolerance threshold theorem for universal quantum computations with local operations, a polynomial qubit overhead and a quasi-polylogarithmic depth overhead.","More generally, our transformation dispenses with the need for considering the locality of operations when designing schemes for fault-tolerant quantum information processing."],"url":"http://arxiv.org/abs/2402.13863v1","category":"quant-ph"}
{"created":"2024-02-21 15:10:20","title":"Improving Efficiency of Iso-Surface Extraction on Implicit Neural Representations Using Uncertainty Propagation","abstract":"Implicit Neural representations (INRs) are widely used for scientific data reduction and visualization by modeling the function that maps a spatial location to a data value. Without any prior knowledge about the spatial distribution of values, we are forced to sample densely from INRs to perform visualization tasks like iso-surface extraction which can be very computationally expensive. Recently, range analysis has shown promising results in improving the efficiency of geometric queries, such as ray casting and hierarchical mesh extraction, on INRs for 3D geometries by using arithmetic rules to bound the output range of the network within a spatial region. However, the analysis bounds are often too conservative for complex scientific data. In this paper, we present an improved technique for range analysis by revisiting the arithmetic rules and analyzing the probability distribution of the network output within a spatial region. We model this distribution efficiently as a Gaussian distribution by applying the central limit theorem. Excluding low probability values, we are able to tighten the output bounds, resulting in a more accurate estimation of the value range, and hence more accurate identification of iso-surface cells and more efficient iso-surface extraction on INRs. Our approach demonstrates superior performance in terms of the iso-surface extraction time on four datasets compared to the original range analysis method and can also be generalized to other geometric query tasks.","sentences":["Implicit Neural representations (INRs) are widely used for scientific data reduction and visualization by modeling the function that maps a spatial location to a data value.","Without any prior knowledge about the spatial distribution of values, we are forced to sample densely from INRs to perform visualization tasks like iso-surface extraction which can be very computationally expensive.","Recently, range analysis has shown promising results in improving the efficiency of geometric queries, such as ray casting and hierarchical mesh extraction, on INRs for 3D geometries by using arithmetic rules to bound the output range of the network within a spatial region.","However, the analysis bounds are often too conservative for complex scientific data.","In this paper, we present an improved technique for range analysis by revisiting the arithmetic rules and analyzing the probability distribution of the network output within a spatial region.","We model this distribution efficiently as a Gaussian distribution by applying the central limit theorem.","Excluding low probability values, we are able to tighten the output bounds, resulting in a more accurate estimation of the value range, and hence more accurate identification of iso-surface cells and more efficient iso-surface extraction on INRs.","Our approach demonstrates superior performance in terms of the iso-surface extraction time on four datasets compared to the original range analysis method and can also be generalized to other geometric query tasks."],"url":"http://arxiv.org/abs/2402.13861v1","category":"cs.GR"}
{"created":"2024-02-21 15:10:03","title":"Friedmann's Universe Controlled by Gauss-Bonnet Modified Gravity","abstract":"The accepted idea that the expansion of the universe is accelerating needs, for compatibility to general relativity, the introduction of some unusual forms of matter. However, several authors have proposed that instead of making weird hypothesis on some yet unobservable species of matter, one should follow the original idea of the first Einstein's paper on cosmology and consider that in the cosmic scene one has to modify the equations that controls the gravitational metric. This possibility led us to re-examine the evolution of the topological invariant containing two duals in a dynamical universe, the so called Gauss-Bonnet topological invariant. The particular interest on this invariant is due to the fact that in a homogeneous and isotropic universe this invariant drives the cosmic acceleration. In a decelerating scenario and as a necessary previous condition of an ulterior acceleration this invariant must have an extremum identified to its maximum value. We will examine the conditions for this to occur, and a description of the universe with epochs of accelerated and decelerated expansion.","sentences":["The accepted idea that the expansion of the universe is accelerating needs, for compatibility to general relativity, the introduction of some unusual forms of matter.","However, several authors have proposed that instead of making weird hypothesis on some yet unobservable species of matter, one should follow the original idea of the first Einstein's paper on cosmology and consider that in the cosmic scene one has to modify the equations that controls the gravitational metric.","This possibility led us to re-examine the evolution of the topological invariant containing two duals in a dynamical universe, the so called Gauss-Bonnet topological invariant.","The particular interest on this invariant is due to the fact that in a homogeneous and isotropic universe this invariant drives the cosmic acceleration.","In a decelerating scenario and as a necessary previous condition of an ulterior acceleration this invariant must have an extremum identified to its maximum value.","We will examine the conditions for this to occur, and a description of the universe with epochs of accelerated and decelerated expansion."],"url":"http://arxiv.org/abs/2402.13860v1","category":"gr-qc"}
{"created":"2024-02-21 14:59:46","title":"RealDex: Towards Human-like Grasping for Robotic Dexterous Hand","abstract":"In this paper, we introduce RealDex, a pioneering dataset capturing authentic dexterous hand grasping motions infused with human behavioral patterns, enriched by multi-view and multimodal visual data. Utilizing a teleoperation system, we seamlessly synchronize human-robot hand poses in real time. This collection of human-like motions is crucial for training dexterous hands to mimic human movements more naturally and precisely. RealDex holds immense promise in advancing humanoid robot for automated perception, cognition, and manipulation in real-world scenarios. Moreover, we introduce a cutting-edge dexterous grasping motion generation framework, which aligns with human experience and enhances real-world applicability through effectively utilizing Multimodal Large Language Models. Extensive experiments have demonstrated the superior performance of our method on RealDex and other open datasets. The complete dataset and code will be made available upon the publication of this work.","sentences":["In this paper, we introduce RealDex, a pioneering dataset capturing authentic dexterous hand grasping motions infused with human behavioral patterns, enriched by multi-view and multimodal visual data.","Utilizing a teleoperation system, we seamlessly synchronize human-robot hand poses in real time.","This collection of human-like motions is crucial for training dexterous hands to mimic human movements more naturally and precisely.","RealDex holds immense promise in advancing humanoid robot for automated perception, cognition, and manipulation in real-world scenarios.","Moreover, we introduce a cutting-edge dexterous grasping motion generation framework, which aligns with human experience and enhances real-world applicability through effectively utilizing Multimodal Large Language Models.","Extensive experiments have demonstrated the superior performance of our method on RealDex and other open datasets.","The complete dataset and code will be made available upon the publication of this work."],"url":"http://arxiv.org/abs/2402.13853v1","category":"cs.RO"}
{"created":"2024-02-21 14:56:36","title":"Neural Control System for Continuous Glucose Monitoring and Maintenance","abstract":"Precise glucose level management is pivotal for individuals with diabetes, averting severe complications. In this work, we introduce a novel neural control system for continuous glucose monitoring and maintenance, utilizing differential predictive control. Our system, guided by a sophisticated neural policy and differentiable modeling, dynamically adjusts insulin delivery in real-time, enhancing glucose optimization. This end-to-end approach maximizes efficiency, ensuring personalized care and improved health outcomes, as affirmed by empirical findings.","sentences":["Precise glucose level management is pivotal for individuals with diabetes, averting severe complications.","In this work, we introduce a novel neural control system for continuous glucose monitoring and maintenance, utilizing differential predictive control.","Our system, guided by a sophisticated neural policy and differentiable modeling, dynamically adjusts insulin delivery in real-time, enhancing glucose optimization.","This end-to-end approach maximizes efficiency, ensuring personalized care and improved health outcomes, as affirmed by empirical findings."],"url":"http://arxiv.org/abs/2402.13852v1","category":"cs.LG"}
{"created":"2024-02-21 14:54:30","title":"VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models","abstract":"Autoregressive Visual Language Models (VLMs) showcase impressive few-shot learning capabilities in a multimodal context. Recently, multimodal instruction tuning has been proposed to further enhance instruction-following abilities. However, we uncover the potential threat posed by backdoor attacks on autoregressive VLMs during instruction tuning. Adversaries can implant a backdoor by injecting poisoned samples with triggers embedded in instructions or images, enabling malicious manipulation of the victim model's predictions with predefined triggers. Nevertheless, the frozen visual encoder in autoregressive VLMs imposes constraints on the learning of conventional image triggers. Additionally, adversaries may encounter restrictions in accessing the parameters and architectures of the victim model. To address these challenges, we propose a multimodal instruction backdoor attack, namely VL-Trojan. Our approach facilitates image trigger learning through an isolating and clustering strategy and enhance black-box-attack efficacy via an iterative character-level text trigger generation method. Our attack successfully induces target outputs during inference, significantly surpassing baselines (+62.52\\%) in ASR. Moreover, it demonstrates robustness across various model scales and few-shot in-context reasoning scenarios.","sentences":["Autoregressive Visual Language Models (VLMs) showcase impressive few-shot learning capabilities in a multimodal context.","Recently, multimodal instruction tuning has been proposed to further enhance instruction-following abilities.","However, we uncover the potential threat posed by backdoor attacks on autoregressive VLMs during instruction tuning.","Adversaries can implant a backdoor by injecting poisoned samples with triggers embedded in instructions or images, enabling malicious manipulation of the victim model's predictions with predefined triggers.","Nevertheless, the frozen visual encoder in autoregressive VLMs imposes constraints on the learning of conventional image triggers.","Additionally, adversaries may encounter restrictions in accessing the parameters and architectures of the victim model.","To address these challenges, we propose a multimodal instruction backdoor attack, namely VL-Trojan.","Our approach facilitates image trigger learning through an isolating and clustering strategy and enhance black-box-attack efficacy via an iterative character-level text trigger generation method.","Our attack successfully induces target outputs during inference, significantly surpassing baselines (+62.52\\%) in ASR.","Moreover, it demonstrates robustness across various model scales and few-shot in-context reasoning scenarios."],"url":"http://arxiv.org/abs/2402.13851v1","category":"cs.CV"}
{"created":"2024-02-21 14:52:01","title":"Conformal and nonminimal couplings in fractional cosmology","abstract":"Fractional differential calculus is a mathematical tool that has found applications in studying social and physical behaviours considered \"anomalous\". It is often used when traditional integer derivatives models fail to represent cases where the power law is observed accurately. Fractional calculus must reflect non-local, frequency- and history-dependent properties of power-law phenomena. This tool has various important applications, such as fractional mass conservation, electrochemical analysis, groundwater flow problems, and fractional spatiotemporal diffusion equations. It can also be used in cosmology to explain late-time cosmic acceleration without the need for dark energy. We review some models using fractional differential equations. We assume the Einstein-Hilbert action based on a fractional derivative action and add a scalar field $\\phi$ to create a non-minimal interaction theory with the coupling $\\xi R \\phi^2 $ between gravity and the scalar field, where $\\xi$ is the interaction constant. By employing various mathematical approaches, we can offer precise schemes to find analytical and numerical approximations of the solutions. Moreover, we comprehensively study the modified cosmological equations and analyze the solution space using the theory of dynamical systems and asymptotic expansion methods. This enables us to provide a qualitative description of cosmologies with a scalar field based on fractional calculus formalism.","sentences":["Fractional differential calculus is a mathematical tool that has found applications in studying social and physical behaviours considered \"anomalous\".","It is often used when traditional integer derivatives models fail to represent cases where the power law is observed accurately.","Fractional calculus must reflect non-local, frequency- and history-dependent properties of power-law phenomena.","This tool has various important applications, such as fractional mass conservation, electrochemical analysis, groundwater flow problems, and fractional spatiotemporal diffusion equations.","It can also be used in cosmology to explain late-time cosmic acceleration without the need for dark energy.","We review some models using fractional differential equations.","We assume the Einstein-Hilbert action based on a fractional derivative action and add a scalar field $\\phi$ to create a non-minimal interaction theory with the coupling $\\xi R \\phi^2 $ between gravity and the scalar field, where $\\xi$ is the interaction constant.","By employing various mathematical approaches, we can offer precise schemes to find analytical and numerical approximations of the solutions.","Moreover, we comprehensively study the modified cosmological equations and analyze the solution space using the theory of dynamical systems and asymptotic expansion methods.","This enables us to provide a qualitative description of cosmologies with a scalar field based on fractional calculus formalism."],"url":"http://arxiv.org/abs/2402.13850v1","category":"gr-qc"}
{"created":"2024-02-21 14:50:24","title":"Zero-BEV: Zero-shot Projection of Any First-Person Modality to BEV Maps","abstract":"Bird's-eye view (BEV) maps are an important geometrically structured representation widely used in robotics, in particular self-driving vehicles and terrestrial robots. Existing algorithms either require depth information for the geometric projection, which is not always reliably available, or are trained end-to-end in a fully supervised way to map visual first-person observations to BEV representation, and are therefore restricted to the output modality they have been trained for. In contrast, we propose a new model capable of performing zero-shot projections of any modality available in a first person view to the corresponding BEV map. This is achieved by disentangling the geometric inverse perspective projection from the modality transformation, eg. RGB to occupancy. The method is general and we showcase experiments projecting to BEV three different modalities: semantic segmentation, motion vectors and object bounding boxes detected in first person. We experimentally show that the model outperforms competing methods, in particular the widely used baseline resorting to monocular depth estimation.","sentences":["Bird's-eye view (BEV) maps are an important geometrically structured representation widely used in robotics, in particular self-driving vehicles and terrestrial robots.","Existing algorithms either require depth information for the geometric projection, which is not always reliably available, or are trained end-to-end in a fully supervised way to map visual first-person observations to BEV representation, and are therefore restricted to the output modality they have been trained for.","In contrast, we propose a new model capable of performing zero-shot projections of any modality available in a first person view to the corresponding BEV map.","This is achieved by disentangling the geometric inverse perspective projection from the modality transformation, eg.","RGB to occupancy.","The method is general and we showcase experiments projecting to BEV three different modalities: semantic segmentation, motion vectors and object bounding boxes detected in first person.","We experimentally show that the model outperforms competing methods, in particular the widely used baseline resorting to monocular depth estimation."],"url":"http://arxiv.org/abs/2402.13848v1","category":"cs.CV"}
{"created":"2024-02-21 14:44:00","title":"Large Language Models are Advanced Anonymizers","abstract":"Recent work in privacy research on large language models has shown that they achieve near human-level performance at inferring personal data from real-world online texts. With consistently increasing model capabilities, existing text anonymization methods are currently lacking behind regulatory requirements and adversarial threats. This raises the question of how individuals can effectively protect their personal data in sharing online texts. In this work, we take two steps to answer this question: We first present a new setting for evaluating anonymizations in the face of adversarial LLMs inferences, allowing for a natural measurement of anonymization performance while remedying some of the shortcomings of previous metrics. We then present our LLM-based adversarial anonymization framework leveraging the strong inferential capabilities of LLMs to inform our anonymization procedure. In our experimental evaluation, we show on real-world and synthetic online texts how adversarial anonymization outperforms current industry-grade anonymizers both in terms of the resulting utility and privacy.","sentences":["Recent work in privacy research on large language models has shown that they achieve near human-level performance at inferring personal data from real-world online texts.","With consistently increasing model capabilities, existing text anonymization methods are currently lacking behind regulatory requirements and adversarial threats.","This raises the question of how individuals can effectively protect their personal data in sharing online texts.","In this work, we take two steps to answer this question: We first present a new setting for evaluating anonymizations in the face of adversarial LLMs inferences, allowing for a natural measurement of anonymization performance while remedying some of the shortcomings of previous metrics.","We then present our LLM-based adversarial anonymization framework leveraging the strong inferential capabilities of LLMs to inform our anonymization procedure.","In our experimental evaluation, we show on real-world and synthetic online texts how adversarial anonymization outperforms current industry-grade anonymizers both in terms of the resulting utility and privacy."],"url":"http://arxiv.org/abs/2402.13846v1","category":"cs.CR"}
{"created":"2024-02-21 14:42:32","title":"State-dependent stiffness enhances wave propagation along elastic filaments","abstract":"We study an elastic filament beating in a viscous fluid with curvature-dependent bending stiffness. Our numerical and experimental investigations reveal that such differential stiffness can sustain planar bending waves far along flexible filaments. In particular, basal actuation is a viable, parsimonious mechanism for generating high-amplitude planar bending waves, in stark contrast to the uniform-stiffness case. Further, the resulting beat patterns closely resemble the power-and-recovery strokes of propulsive biological filaments, suggesting applications in robotic and engineered systems.","sentences":["We study an elastic filament beating in a viscous fluid with curvature-dependent bending stiffness.","Our numerical and experimental investigations reveal that such differential stiffness can sustain planar bending waves far along flexible filaments.","In particular, basal actuation is a viable, parsimonious mechanism for generating high-amplitude planar bending waves, in stark contrast to the uniform-stiffness case.","Further, the resulting beat patterns closely resemble the power-and-recovery strokes of propulsive biological filaments, suggesting applications in robotic and engineered systems."],"url":"http://arxiv.org/abs/2402.13844v1","category":"cond-mat.soft"}
{"created":"2024-02-21 14:38:02","title":"LLM4SBR: A Lightweight and Effective Framework for Integrating Large Language Models in Session-based Recommendation","abstract":"Traditional session-based recommendation (SBR) utilizes session behavior sequences from anonymous users for recommendation. Although this strategy is highly efficient, it sacrifices the inherent semantic information of the items, making it difficult for the model to understand the true intent of the session and resulting in a lack of interpretability in the recommended results. Recently, large language models (LLMs) have flourished across various domains, offering a glimpse of hope in addressing the aforementioned challenges. Inspired by the impact of LLMs, research exploring the integration of LLMs with the Recommender system (RS) has surged like mushrooms after rain. However, constrained by high time and space costs, as well as the brief and anonymous nature of session data, the first LLM recommendation framework suitable for industrial deployment has yet to emerge in the field of SBR. To address the aforementioned challenges, we have proposed the LLM Integration Framework for SBR (LLM4SBR). Serving as a lightweight and plug-and-play framework, LLM4SBR adopts a two-step strategy. Firstly, we transform session data into a bimodal form of text and behavior. In the first step, leveraging the inferential capabilities of LLMs, we conduct inference on session text data from different perspectives and design the component for auxiliary enhancement. In the second step, the SBR model is trained on behavior data, aligning and averaging two modal session representations from different perspectives. Finally, we fuse session representations from different perspectives and modalities as the ultimate session representation for recommendation. We conducted experiments on two real-world datasets, and the results demonstrate that LLM4SBR significantly improves the performance of traditional SBR models and is highly lightweight and efficient, making it suitable for industrial deployment.","sentences":["Traditional session-based recommendation (SBR) utilizes session behavior sequences from anonymous users for recommendation.","Although this strategy is highly efficient, it sacrifices the inherent semantic information of the items, making it difficult for the model to understand the true intent of the session and resulting in a lack of interpretability in the recommended results.","Recently, large language models (LLMs) have flourished across various domains, offering a glimpse of hope in addressing the aforementioned challenges.","Inspired by the impact of LLMs, research exploring the integration of LLMs with the Recommender system (RS) has surged like mushrooms after rain.","However, constrained by high time and space costs, as well as the brief and anonymous nature of session data, the first LLM recommendation framework suitable for industrial deployment has yet to emerge in the field of SBR.","To address the aforementioned challenges, we have proposed the LLM Integration Framework for SBR (LLM4SBR).","Serving as a lightweight and plug-and-play framework, LLM4SBR adopts a two-step strategy.","Firstly, we transform session data into a bimodal form of text and behavior.","In the first step, leveraging the inferential capabilities of LLMs, we conduct inference on session text data from different perspectives and design the component for auxiliary enhancement.","In the second step, the SBR model is trained on behavior data, aligning and averaging two modal session representations from different perspectives.","Finally, we fuse session representations from different perspectives and modalities as the ultimate session representation for recommendation.","We conducted experiments on two real-world datasets, and the results demonstrate that LLM4SBR significantly improves the performance of traditional SBR models and is highly lightweight and efficient, making it suitable for industrial deployment."],"url":"http://arxiv.org/abs/2402.13840v1","category":"cs.IR"}
{"created":"2024-02-21 14:35:53","title":"Emergent spin and clock variable in Bianchi type-I quantum cosmology","abstract":"We consider the Bianchi type-I model of the universe in the Wheeler-DeWitt quantization scheme with the matter degree of freedom represented by a scalar field. As a consequence, the quantum mechanical equation of the universe is obtained in the minisuperspace consisting of the Misner variables and the scalar field. Employing Dirac factorization, we find that the volume parameter makes a suitable choice for the clock variable whereas the matter clock leads to various inconsistencies. We further find that the minisuperspace orbital angular momentum operator does not commute with the Hamiltonian in the Dirac-type equation. We therefore find the missing part of the angular momentum whereby the total angular momentum commutes with the Hamiltonian. We interpret this missing part as the spin of the quantum universe during its early stage of evolution. The emergence of the three-component spin vector is owing to the presence of anisotropy in the Bianchi type-I model of the universe which is absent in the quantization of isotropic models.","sentences":["We consider the Bianchi type-I model of the universe in the Wheeler-DeWitt quantization scheme with the matter degree of freedom represented by a scalar field.","As a consequence, the quantum mechanical equation of the universe is obtained in the minisuperspace consisting of the Misner variables and the scalar field.","Employing Dirac factorization, we find that the volume parameter makes a suitable choice for the clock variable whereas the matter clock leads to various inconsistencies.","We further find that the minisuperspace orbital angular momentum operator does not commute with the Hamiltonian in the Dirac-type equation.","We therefore find the missing part of the angular momentum whereby the total angular momentum commutes with the Hamiltonian.","We interpret this missing part as the spin of the quantum universe during its early stage of evolution.","The emergence of the three-component spin vector is owing to the presence of anisotropy in the Bianchi type-I model of the universe which is absent in the quantization of isotropic models."],"url":"http://arxiv.org/abs/2402.13839v1","category":"gr-qc"}
{"created":"2024-02-21 14:25:30","title":"Loop equations for generalised eigenvalue models","abstract":"We derive the loop equation for the 1-matrix model with generic difference-type measure for eigenvalues and develop a recursive algebraic framework for solving it to an arbitrary order in the coupling constant in and beyond the planar approximation. The planar limit is solved exactly for a one-parametric family of models and in the general case at strong coupling. The Wilson loop in the N=2* super-Yang-Mills theory and the Hoppe model are used to illustrate our methods.","sentences":["We derive the loop equation for the 1-matrix model with generic difference-type measure for eigenvalues and develop a recursive algebraic framework for solving it to an arbitrary order in the coupling constant in and beyond the planar approximation.","The planar limit is solved exactly for a one-parametric family of models and in the general case at strong coupling.","The Wilson loop in the N=2* super-Yang-Mills theory and the Hoppe model are used to illustrate our methods."],"url":"http://arxiv.org/abs/2402.13835v1","category":"hep-th"}
{"created":"2024-02-21 14:23:13","title":"Gorenstein D-branes of type B","abstract":"Let (S; n) be a commutative noetherian local ring and w in n be non-zerodivisor. We propose a natural definition of the category of Gorenstein pairs of w, by replacing projectives with finitely generated Gorenstein projective S-modules in the category of pairs. It is shown that the category of Gorenstein pairs of w is a Frobeius category, and in particular, its projective objects are the same as the category of pairs. The stable category of Gorenstein pairs, which admits a natural structure of a triangulated category, is called Gorenstein D-branes of type B. It is proved that the singularity category of the factor ring R = S=(w) as well as the category of D-branes of type B, can be realized as triangulated subcategories of Gorenstein D-branes of type B. In studying the category of Gorenstein D-branes of type B, the submodule category of Gorenstein projective S-modules has been used appropriately.","sentences":["Let (S; n) be a commutative noetherian local ring and w in n be non-zerodivisor.","We propose a natural definition of the category of Gorenstein pairs of w, by replacing projectives with finitely generated Gorenstein projective S-modules in the category of pairs.","It is shown that the category of Gorenstein pairs of w is a Frobeius category, and in particular, its projective objects are the same as the category of pairs.","The stable category of Gorenstein pairs, which admits a natural structure of a triangulated category, is called Gorenstein D-branes of type B.","It is proved that the singularity category of the factor ring R = S=(w) as well as the category of D-branes of type B, can be realized as triangulated subcategories of Gorenstein D-branes of type B.","In studying the category of Gorenstein D-branes of type B, the submodule category of Gorenstein projective S-modules has been used appropriately."],"url":"http://arxiv.org/abs/2402.13833v1","category":"math.RT"}
{"created":"2024-02-21 14:17:45","title":"Origami: (un)folding the abstraction of recursion schemes for program synthesis","abstract":"Program synthesis with Genetic Programming searches for a correct program that satisfies the input specification, which is usually provided as input-output examples. One particular challenge is how to effectively handle loops and recursion avoiding programs that never terminate. A helpful abstraction that can alleviate this problem is the employment of Recursion Schemes that generalize the combination of data production and consumption. Recursion Schemes are very powerful as they allow the construction of programs that can summarize data, create sequences, and perform advanced calculations. The main advantage of writing a program using Recursion Schemes is that the programs are composed of well defined templates with only a few parts that need to be synthesized. In this paper we make an initial study of the benefits of using program synthesis with fold and unfold templates, and outline some preliminary experimental results. To highlight the advantages and disadvantages of this approach, we manually solved the entire GPSB benchmark using recursion schemes, highlighting the parts that should be evolved compared to alternative implementations. We noticed that, once the choice of which recursion scheme is made, the synthesis process can be simplified as each of the missing parts of the template are reduced to simpler functions, which are further constrained by their own input and output types.","sentences":["Program synthesis with Genetic Programming searches for a correct program that satisfies the input specification, which is usually provided as input-output examples.","One particular challenge is how to effectively handle loops and recursion avoiding programs that never terminate.","A helpful abstraction that can alleviate this problem is the employment of Recursion Schemes that generalize the combination of data production and consumption.","Recursion Schemes are very powerful as they allow the construction of programs that can summarize data, create sequences, and perform advanced calculations.","The main advantage of writing a program using Recursion Schemes is that the programs are composed of well defined templates with only a few parts that need to be synthesized.","In this paper we make an initial study of the benefits of using program synthesis with fold and unfold templates, and outline some preliminary experimental results.","To highlight the advantages and disadvantages of this approach, we manually solved the entire GPSB benchmark using recursion schemes, highlighting the parts that should be evolved compared to alternative implementations.","We noticed that, once the choice of which recursion scheme is made, the synthesis process can be simplified as each of the missing parts of the template are reduced to simpler functions, which are further constrained by their own input and output types."],"url":"http://arxiv.org/abs/2402.13828v1","category":"cs.NE"}
{"created":"2024-02-21 14:08:53","title":"Graph with any rational density and no rich subsets of linear size","abstract":"A well-known application of the dependent random choice asserts that any $n$-vertex graph $G$ with positive edge density contains a `rich' vertex subset $U$ of size $n^{1-o(1)}$ such that every pair of vertices in $U$ has at least $n^{1-o(1)}$ common neighbors. In 2003, using a beautiful construction on hypercube, Kostochka and Sudakov showed that this is tight: one cannot remove the $o(1)$ terms even if the edge density of $G$ is $1/2$. In this paper, we generalize their result from pairs to tuples. To be precise, we show that given every pair of positive integers $p<q$, there is an $n$-vertex graph $G$ for all sufficiently large $n$ with edge density $p/q$ such that any vertex subset $U$ of size $\\Omega(n)$ contains $q$ vertices, any $p$ of which have $o(n)$ common neighbors. The edge density $p/q$ is best possible. Our construction uses isoperimetry and concentration of measure on high dimensional complex spheres.","sentences":["A well-known application of the dependent random choice asserts that any $n$-vertex graph $G$ with positive edge density contains a `rich' vertex subset $U$ of size $n^{1-o(1)}$ such that every pair of vertices in $U$ has at least $n^{1-o(1)}$ common neighbors.","In 2003, using a beautiful construction on hypercube, Kostochka and Sudakov showed that this is tight: one cannot remove the $o(1)$ terms even if the edge density of $G$ is $1/2$. In this paper, we generalize their result from pairs to tuples.","To be precise, we show that given every pair of positive integers $p<q$, there is an $n$-vertex graph $G$ for all sufficiently large $n$ with edge density $p/q$ such that any vertex subset $U$ of size $\\Omega(n)$ contains $q$ vertices, any $p$ of which have $o(n)$ common neighbors.","The edge density $p/q$ is best possible.","Our construction uses isoperimetry and concentration of measure on high dimensional complex spheres."],"url":"http://arxiv.org/abs/2402.13825v1","category":"math.CO"}
{"created":"2024-02-21 14:04:04","title":"Multi-Agent Contract Design beyond Binary Actions","abstract":"We study hidden-action principal-agent problems with multiple agents. Unlike previous work, we consider a general setting in which each agent has an arbitrary number of actions, and the joint action induces outcomes according to an arbitrary distribution. We study two classes of mechanisms: a class of deterministic mechanisms that is the natural extension of single-agent contracts, in which the agents play a Nash equilibrium of the game induced by the contract, and a class of randomized mechanisms that is inspired by single-agent randomized contracts and correlated equilibria.","sentences":["We study hidden-action principal-agent problems with multiple agents.","Unlike previous work, we consider a general setting in which each agent has an arbitrary number of actions, and the joint action induces outcomes according to an arbitrary distribution.","We study two classes of mechanisms: a class of deterministic mechanisms that is the natural extension of single-agent contracts, in which the agents play a Nash equilibrium of the game induced by the contract, and a class of randomized mechanisms that is inspired by single-agent randomized contracts and correlated equilibria."],"url":"http://arxiv.org/abs/2402.13824v1","category":"cs.GT"}
{"created":"2024-02-21 13:59:47","title":"Performance Improvement Bounds for Lipschitz Configurable Markov Decision Processes","abstract":"Configurable Markov Decision Processes (Conf-MDPs) have recently been introduced as an extension of the traditional Markov Decision Processes (MDPs) to model the real-world scenarios in which there is the possibility to intervene in the environment in order to configure some of its parameters. In this paper, we focus on a particular subclass of Conf-MDP that satisfies regularity conditions, namely Lipschitz continuity. We start by providing a bound on the Wasserstein distance between $\\gamma$-discounted stationary distributions induced by changing policy and configuration. This result generalizes the already existing bounds both for Conf-MDPs and traditional MDPs. Then, we derive a novel performance improvement lower bound.","sentences":["Configurable Markov Decision Processes (Conf-MDPs) have recently been introduced as an extension of the traditional Markov Decision Processes (MDPs) to model the real-world scenarios in which there is the possibility to intervene in the environment in order to configure some of its parameters.","In this paper, we focus on a particular subclass of Conf-MDP that satisfies regularity conditions, namely Lipschitz continuity.","We start by providing a bound on the Wasserstein distance between $\\gamma$-discounted stationary distributions induced by changing policy and configuration.","This result generalizes the already existing bounds both for Conf-MDPs and traditional MDPs.","Then, we derive a novel performance improvement lower bound."],"url":"http://arxiv.org/abs/2402.13821v1","category":"cs.LG"}
{"created":"2024-02-21 13:59:21","title":"FLD: Fourier Latent Dynamics for Structured Motion Representation and Learning","abstract":"Motion trajectories offer reliable references for physics-based motion learning but suffer from sparsity, particularly in regions that lack sufficient data coverage. To address this challenge, we introduce a self-supervised, structured representation and generation method that extracts spatial-temporal relationships in periodic or quasi-periodic motions. The motion dynamics in a continuously parameterized latent space enable our method to enhance the interpolation and generalization capabilities of motion learning algorithms. The motion learning controller, informed by the motion parameterization, operates online tracking of a wide range of motions, including targets unseen during training. With a fallback mechanism, the controller dynamically adapts its tracking strategy and automatically resorts to safe action execution when a potentially risky target is proposed. By leveraging the identified spatial-temporal structure, our work opens new possibilities for future advancements in general motion representation and learning algorithms.","sentences":["Motion trajectories offer reliable references for physics-based motion learning but suffer from sparsity, particularly in regions that lack sufficient data coverage.","To address this challenge, we introduce a self-supervised, structured representation and generation method that extracts spatial-temporal relationships in periodic or quasi-periodic motions.","The motion dynamics in a continuously parameterized latent space enable our method to enhance the interpolation and generalization capabilities of motion learning algorithms.","The motion learning controller, informed by the motion parameterization, operates online tracking of a wide range of motions, including targets unseen during training.","With a fallback mechanism, the controller dynamically adapts its tracking strategy and automatically resorts to safe action execution when a potentially risky target is proposed.","By leveraging the identified spatial-temporal structure, our work opens new possibilities for future advancements in general motion representation and learning algorithms."],"url":"http://arxiv.org/abs/2402.13820v1","category":"cs.LG"}
{"created":"2024-02-21 13:57:36","title":"Beyond Hate Speech: NLP's Challenges and Opportunities in Uncovering Dehumanizing Language","abstract":"Dehumanization, characterized as a subtle yet harmful manifestation of hate speech, involves denying individuals of their human qualities and often results in violence against marginalized groups. Despite significant progress in Natural Language Processing across various domains, its application in detecting dehumanizing language is limited, largely due to the scarcity of publicly available annotated data for this domain. This paper evaluates the performance of cutting-edge NLP models, including GPT-4, GPT-3.5, and LLAMA-2, in identifying dehumanizing language. Our findings reveal that while these models demonstrate potential, achieving a 70\\% accuracy rate in distinguishing dehumanizing language from broader hate speech, they also display biases. They are over-sensitive in classifying other forms of hate speech as dehumanization for a specific subset of target groups, while more frequently failing to identify clear cases of dehumanization for other target groups. Moreover, leveraging one of the best-performing models, we automatically annotated a larger dataset for training more accessible models. However, our findings indicate that these models currently do not meet the high-quality data generation threshold necessary for this task.","sentences":["Dehumanization, characterized as a subtle yet harmful manifestation of hate speech, involves denying individuals of their human qualities and often results in violence against marginalized groups.","Despite significant progress in Natural Language Processing across various domains, its application in detecting dehumanizing language is limited, largely due to the scarcity of publicly available annotated data for this domain.","This paper evaluates the performance of cutting-edge NLP models, including GPT-4, GPT-3.5, and LLAMA-2, in identifying dehumanizing language.","Our findings reveal that while these models demonstrate potential, achieving a 70\\% accuracy rate in distinguishing dehumanizing language from broader hate speech, they also display biases.","They are over-sensitive in classifying other forms of hate speech as dehumanization for a specific subset of target groups, while more frequently failing to identify clear cases of dehumanization for other target groups.","Moreover, leveraging one of the best-performing models, we automatically annotated a larger dataset for training more accessible models.","However, our findings indicate that these models currently do not meet the high-quality data generation threshold necessary for this task."],"url":"http://arxiv.org/abs/2402.13818v1","category":"cs.CL"}
{"created":"2024-02-21 13:55:48","title":"A unified framework of non-local parametric methods for image denoising","abstract":"We propose a unified view of non-local methods for single-image denoising, for which BM3D is the most popular representative, that operate by gathering noisy patches together according to their similarities in order to process them collaboratively. Our general estimation framework is based on the minimization of the quadratic risk, which is approximated in two steps, and adapts to photon and electronic noises. Relying on unbiased risk estimation (URE) for the first step and on ``internal adaptation'', a concept borrowed from deep learning theory, for the second, we show that our approach enables to reinterpret and reconcile previous state-of-the-art non-local methods. Within this framework, we propose a novel denoiser called NL-Ridge that exploits linear combinations of patches. While conceptually simpler, we show that NL-Ridge can outperform well-established state-of-the-art single-image denoisers.","sentences":["We propose a unified view of non-local methods for single-image denoising, for which BM3D is the most popular representative, that operate by gathering noisy patches together according to their similarities in order to process them collaboratively.","Our general estimation framework is based on the minimization of the quadratic risk, which is approximated in two steps, and adapts to photon and electronic noises.","Relying on unbiased risk estimation (URE) for the first step and on ``internal adaptation'', a concept borrowed from deep learning theory, for the second, we show that our approach enables to reinterpret and reconcile previous state-of-the-art non-local methods.","Within this framework, we propose a novel denoiser called NL-Ridge that exploits linear combinations of patches.","While conceptually simpler, we show that NL-Ridge can outperform well-established state-of-the-art single-image denoisers."],"url":"http://arxiv.org/abs/2402.13816v1","category":"cs.CV"}
{"created":"2024-02-21 13:46:25","title":"NeuralDiffuser: Controllable fMRI Reconstruction with Primary Visual Feature Guided Diffusion","abstract":"Reconstructing visual stimuli from functional Magnetic Resonance Imaging (fMRI) based on Latent Diffusion Models (LDM) provides a fine-grained retrieval of the brain. A challenge persists in reconstructing a cohesive alignment of details (such as structure, background, texture, color, etc.). Moreover, LDMs would generate different image results even under the same conditions. For these, we first uncover the neuroscientific perspective of LDM-based methods that is top-down creation based on pre-trained knowledge from massive images but lack of detail-driven bottom-up perception resulting in unfaithful details. We propose NeuralDiffuser which introduces primary visual feature guidance to provide detail cues in the form of gradients, extending the bottom-up process for LDM-based methods to achieve faithful semantics and details. We also developed a novel guidance strategy to ensure the consistency of repeated reconstructions rather than a variety of results. We obtain the state-of-the-art performance of NeuralDiffuser on the Natural Senses Dataset (NSD), which offers more faithful details and consistent results.","sentences":["Reconstructing visual stimuli from functional Magnetic Resonance Imaging (fMRI) based on Latent Diffusion Models (LDM) provides a fine-grained retrieval of the brain.","A challenge persists in reconstructing a cohesive alignment of details (such as structure, background, texture, color, etc.).","Moreover, LDMs would generate different image results even under the same conditions.","For these, we first uncover the neuroscientific perspective of LDM-based methods that is top-down creation based on pre-trained knowledge from massive images but lack of detail-driven bottom-up perception resulting in unfaithful details.","We propose NeuralDiffuser which introduces primary visual feature guidance to provide detail cues in the form of gradients, extending the bottom-up process for LDM-based methods to achieve faithful semantics and details.","We also developed a novel guidance strategy to ensure the consistency of repeated reconstructions rather than a variety of results.","We obtain the state-of-the-art performance of NeuralDiffuser on the Natural Senses Dataset (NSD), which offers more faithful details and consistent results."],"url":"http://arxiv.org/abs/2402.13809v1","category":"cs.NE"}
{"created":"2024-02-21 13:44:59","title":"Offshoring Emissions through Used Vehicle Exports","abstract":"Policies to reduce transport emissions often overlook the international flow of used vehicles. We quantify the rate at which used vehicles generated CO2 and pollution for all used vehicles exported from Great Britain; a globally leading used vehicle exporter across 2005-2021. Destined for low-middle-income countries, exported vehicles fail roadworthiness standards and, even under extremely optimistic functioning as new assumptions, generate at least 13-53 percent more emissions than scrapped or on-road vehicles.","sentences":["Policies to reduce transport emissions often overlook the international flow of used vehicles.","We quantify the rate at which used vehicles generated CO2 and pollution for all used vehicles exported from Great Britain; a globally leading used vehicle exporter across 2005-2021.","Destined for low-middle-income countries, exported vehicles fail roadworthiness standards and, even under extremely optimistic functioning as new assumptions, generate at least 13-53 percent more emissions than scrapped or on-road vehicles."],"url":"http://arxiv.org/abs/2402.13807v1","category":"econ.GN"}
{"created":"2024-02-21 13:37:43","title":"Reconfigurable Intelligent Surfaces for THz: Hardware Impairments and Switching Technologies","abstract":"The demand for unprecedented performance in the upcoming 6G wireless networks is fomenting the research on THz communications empowered by Reconfigurable Inteligent Surfaces (RISs). A wide range of use cases have been proposed, most of them, assuming high-level RIS models that overlook some of the hardware impairments that this technology faces. The expectation is that the emergent reconfigurable THz technologies will eventually overcome its current limitations. This disassociation from the hardware may mask nonphysical assumptions, perceived as hardware limitations. In this paper, a top-down approach bounded by physical constraints is presented, distilling from system-level specifications, hardware requirements, and upper bounds for the RIS-aided system performance. We consider D-band indoor and outdoor scenarios where a more realistic assessment of the state-of-the-art solution can be made. The goal is to highlight the intricacies of the design procedure based on sound assumptions for the RIS performance. For a given signal range and angular coverage, we quantify the required RIS size, number of switching elements, and maximum achievable bandwidth and capacity.","sentences":["The demand for unprecedented performance in the upcoming 6G wireless networks is fomenting the research on THz communications empowered by Reconfigurable Inteligent Surfaces (RISs).","A wide range of use cases have been proposed, most of them, assuming high-level RIS models that overlook some of the hardware impairments that this technology faces.","The expectation is that the emergent reconfigurable THz technologies will eventually overcome its current limitations.","This disassociation from the hardware may mask nonphysical assumptions, perceived as hardware limitations.","In this paper, a top-down approach bounded by physical constraints is presented, distilling from system-level specifications, hardware requirements, and upper bounds for the RIS-aided system performance.","We consider D-band indoor and outdoor scenarios where a more realistic assessment of the state-of-the-art solution can be made.","The goal is to highlight the intricacies of the design procedure based on sound assumptions for the RIS performance.","For a given signal range and angular coverage, we quantify the required RIS size, number of switching elements, and maximum achievable bandwidth and capacity."],"url":"http://arxiv.org/abs/2402.13804v1","category":"cs.IT"}
{"created":"2024-02-21 13:34:31","title":"Collapse of inelastic hard spheres in dimension $d \\geq 2$","abstract":"We investigate the collapse of three inelastic particles in dimension $d \\geq 2$. We obtain general results of convergence and asymptotics concerning the variables of the dynamical system describing a collapsing system of particles. We prove a complete classification of the singularities when a collapse of three particles takes place, obtaining only two possible orders of collisions between the particles. In the first case we recover that the particles arrange in a nearly-linear chain, already studied by Zhou and Kadanoff, and in the second case we obtain that the particles arrange in a triangle, and we show that, after sufficiently many collisions, the particles collide according to a unique order of collisions, which is periodic. Finally, we construct an initial configuration leading to a nearly-linear collapse, stable under perturbations, and such that the angle between the particles at the time of collapse can be chosen a priori, with an arbitrary precision.","sentences":["We investigate the collapse of three inelastic particles in dimension $d \\geq 2$.","We obtain general results of convergence and asymptotics concerning the variables of the dynamical system describing a collapsing system of particles.","We prove a complete classification of the singularities when a collapse of three particles takes place, obtaining only two possible orders of collisions between the particles.","In the first case we recover that the particles arrange in a nearly-linear chain, already studied by Zhou and Kadanoff, and in the second case we obtain that the particles arrange in a triangle, and we show that, after sufficiently many collisions, the particles collide according to a unique order of collisions, which is periodic.","Finally, we construct an initial configuration leading to a nearly-linear collapse, stable under perturbations, and such that the angle between the particles at the time of collapse can be chosen a priori, with an arbitrary precision."],"url":"http://arxiv.org/abs/2402.13803v1","category":"math-ph"}
{"created":"2024-02-21 13:30:45","title":"Resilience of Hund's rule in the Chemical Space of Small Organic Molecules","abstract":"We embark on a quest to identify small molecules in the chemical space that can potentially violate Hund's rule. Utilizing twelve TDDFT approximations and the ADC(2) many-body method, we report the energies of S$_1$ and T$_1$ excited states of 12,880 closed-shell organic molecules within the bigQM7$\\omega$ dataset with up to 7 CONF atoms. In this comprehensive dataset, none of the molecules in their minimum energy geometry, exhibit a negative S$_1$-T$_1$ energy gap at the ADC($2$) level while several molecules display S$_1$-T$_1$ gap of $<0.1$ eV. The spin-component-scaled double-hybrid method SCS-PBE-QIDH demonstrates the best agreement with ADC(2). Yet, at this level, a few molecules with a strained $sp^3$-N center turn out as false-positives with the S$_1$ state lower in energy than T$_1$. We investigate a prototypical cage molecule with an energy gap $<-0.2$ eV, which a closer examination revealed as yet another false positive. We conclude that in the chemical space of small closed-shell organic molecules, it is possible to identify geometric and electronic structural features giving rise to S$_1$-T$_1$ degeneracy, but there is no evidence of a negative gap. The data generated in this study is shared as a module, facilitating seamless molecular discovery through data mining.","sentences":["We embark on a quest to identify small molecules in the chemical space that can potentially violate Hund's rule.","Utilizing twelve TDDFT approximations and the ADC(2)","many-body method, we report the energies of S$_1$ and T$_1$ excited states of 12,880 closed-shell organic molecules within the bigQM7$\\omega$ dataset with up to 7 CONF atoms.","In this comprehensive dataset, none of the molecules in their minimum energy geometry, exhibit a negative S$_1$-T$_1$ energy gap at the ADC($2$) level while several molecules display S$_1$-T$_1$ gap of $<0.1$ eV. The spin-component-scaled double-hybrid method SCS-PBE-QIDH demonstrates the best agreement with ADC(2).","Yet, at this level, a few molecules with a strained $sp^3$-N center turn out as false-positives with the S$_1$ state lower in energy than T$_1$. We investigate a prototypical cage molecule with an energy gap $<-0.2$ eV, which a closer examination revealed as yet another false positive.","We conclude that in the chemical space of small closed-shell organic molecules, it is possible to identify geometric and electronic structural features giving rise to S$_1$-T$_1$ degeneracy, but there is no evidence of a negative gap.","The data generated in this study is shared as a module, facilitating seamless molecular discovery through data mining."],"url":"http://arxiv.org/abs/2402.13801v1","category":"physics.chem-ph"}
{"created":"2024-02-21 13:27:05","title":"Radiating particle in the vicinity of the weakly charged Schwarzschild black hole","abstract":"It is well known that supermassive black holes in the centers of galaxies are capable of accelerating charged particles to very high energies. In many cases, the particle acceleration by black holes occurs electromagnetically through an electric field induced by the source. In such scenarios, the accelerated particles radiate electromagnetic waves, leading to the appearance of the backreaction force, which can considerably change the dynamics, especially, if the particles are relativistic. The effect of the radiation reaction force due to accelerating electric field of the central body in curved spacetime has not been considered previously. We study the dynamics of radiating charged particles in the field of the Schwarzschild black hole in the presence of an electric field associated with a small central charge of negligible gravitational influence. We start from the flat spacetime description, solving the Lorentz-Dirac equation reduced to the Landau-Lifshitz form. In curved spacetime, we use the DeWitt-Brehme equation and discuss the effect of the self-force, also known as the tail term, within the given approach. We also study the pure effect of the self-force to calculate the radiative deceleration of radially moving charged particles. In the case of bounded orbits, we find that the radiation reaction force can stabilize and circularize the orbits of oscillating charged particles by suppressing the oscillations or causing the particles to spiral down into the black hole depending on the sign of the electrostatic interaction. In all cases, we calculate the energy losses and exact trajectories of charged particles for different values and signs of electric charge.","sentences":["It is well known that supermassive black holes in the centers of galaxies are capable of accelerating charged particles to very high energies.","In many cases, the particle acceleration by black holes occurs electromagnetically through an electric field induced by the source.","In such scenarios, the accelerated particles radiate electromagnetic waves, leading to the appearance of the backreaction force, which can considerably change the dynamics, especially, if the particles are relativistic.","The effect of the radiation reaction force due to accelerating electric field of the central body in curved spacetime has not been considered previously.","We study the dynamics of radiating charged particles in the field of the Schwarzschild black hole in the presence of an electric field associated with a small central charge of negligible gravitational influence.","We start from the flat spacetime description, solving the Lorentz-Dirac equation reduced to the Landau-Lifshitz form.","In curved spacetime, we use the DeWitt-Brehme equation and discuss the effect of the self-force, also known as the tail term, within the given approach.","We also study the pure effect of the self-force to calculate the radiative deceleration of radially moving charged particles.","In the case of bounded orbits, we find that the radiation reaction force can stabilize and circularize the orbits of oscillating charged particles by suppressing the oscillations or causing the particles to spiral down into the black hole depending on the sign of the electrostatic interaction.","In all cases, we calculate the energy losses and exact trajectories of charged particles for different values and signs of electric charge."],"url":"http://arxiv.org/abs/2402.13797v1","category":"gr-qc"}
{"created":"2024-02-21 13:24:14","title":"Revisiting Convergence of AdaGrad with Relaxed Assumptions","abstract":"In this study, we revisit the convergence of AdaGrad with momentum (covering AdaGrad as a special case) on non-convex smooth optimization problems. We consider a general noise model where the noise magnitude is controlled by the function value gap together with the gradient magnitude. This model encompasses a broad range of noises including bounded noise, sub-Gaussian noise, affine variance noise and the expected smoothness, and it has been shown to be more realistic in many practical applications. Our analysis yields a probabilistic convergence rate which, under the general noise, could reach at (\\tilde{\\mathcal{O}}(1/\\sqrt{T})). This rate does not rely on prior knowledge of problem-parameters and could accelerate to (\\tilde{\\mathcal{O}}(1/T)) where (T) denotes the total number iterations, when the noise parameters related to the function value gap and noise level are sufficiently small. The convergence rate thus matches the lower rate for stochastic first-order methods over non-convex smooth landscape up to logarithm terms [Arjevani et al., 2023]. We further derive a convergence bound for AdaGrad with mometum, considering the generalized smoothness where the local smoothness is controlled by a first-order function of the gradient norm.","sentences":["In this study, we revisit the convergence of AdaGrad with momentum (covering AdaGrad as a special case) on non-convex smooth optimization problems.","We consider a general noise model where the noise magnitude is controlled by the function value gap together with the gradient magnitude.","This model encompasses a broad range of noises including bounded noise, sub-Gaussian noise, affine variance noise and the expected smoothness, and it has been shown to be more realistic in many practical applications.","Our analysis yields a probabilistic convergence rate which, under the general noise, could reach at (\\tilde{\\mathcal{O}}(1/\\sqrt{T})).","This rate does not rely on prior knowledge of problem-parameters and could accelerate to (\\tilde{\\mathcal{O}}(1/T)) where (T) denotes the total number iterations, when the noise parameters related to the function value gap and noise level are sufficiently small.","The convergence rate thus matches the lower rate for stochastic first-order methods over non-convex smooth landscape up to logarithm terms [Arjevani et al., 2023].","We further derive a convergence bound for AdaGrad with mometum, considering the generalized smoothness where the local smoothness is controlled by a first-order function of the gradient norm."],"url":"http://arxiv.org/abs/2402.13794v1","category":"math.OC"}
{"created":"2024-02-21 13:18:14","title":"The seasonality of air ticket prices before and after the pandemic","abstract":"This study investigates price seasonality in the Brazilian air transport industry, emphasizing the impact of the COVID-19 pandemic on domestic airline pricing strategies. Given potential shifts in demand patterns following the global health crisis, this study explores possible long-term structural changes in the seasonality of Brazilian airfare. We analyze an open dataset of domestic city pairs from 2013 to 2023, employing an econometric model developed using Stata software. Our findings indicate alterations in seasonal patterns and long-term trends in the post-pandemic era. These changes underscore potential shifts in the composition of leisure and business travelers, along with the cost pressures faced by airlines.","sentences":["This study investigates price seasonality in the Brazilian air transport industry, emphasizing the impact of the COVID-19 pandemic on domestic airline pricing strategies.","Given potential shifts in demand patterns following the global health crisis, this study explores possible long-term structural changes in the seasonality of Brazilian airfare.","We analyze an open dataset of domestic city pairs from 2013 to 2023, employing an econometric model developed using Stata software.","Our findings indicate alterations in seasonal patterns and long-term trends in the post-pandemic era.","These changes underscore potential shifts in the composition of leisure and business travelers, along with the cost pressures faced by airlines."],"url":"http://arxiv.org/abs/2402.13789v1","category":"econ.GN"}
{"created":"2024-02-21 13:16:11","title":"A Unifying Theory for Runge--Kutta-like Time Integrators: Convergence and Stability","abstract":"The work deals with two major topics concerning the numerical analysis of Runge-Kutta-like (RK-like) methods, namely their stability and order of convergence. RK-like methods differ from additive RK methods in that their coefficients are allowed to depend on the solution and the step size. As a result of this, we also refer to them as non-standard additive RK (NSARK) methods. The first major part of this thesis is dedicated to providing a tool for deriving order conditions for NSARK methods. The proposed approach may yield implicit order conditions, which can be rewritten in explicit form using the NB-series of the stages. The obtained explicit order conditions can be further reduced using Gr\\\"obner bases computations. With the presented approach, it was possible for the first time to obtain conditions for the construction of 3rd and 4th order GeCo as well as 4th order MPRK schemes. Moreover, a new fourth order MPRK method is constructed using our theory and the order of convergence is validated numerically. The second major part is concerned with the stability of nonlinear time integrators preserving at least one linear invariant. We discuss how the given approach generalizes the notion of A-stability. We can prove that investigating the Jacobian of the generating map is sufficient to understand the stability of the nonlinear method in a neighborhood of the steady state. This approach allows for the first time the investigation of several modified Patankar. In the case of MPRK schemes, we compute a general stability function in a way that can be easily adapted to the case of PDRS. Finally, the approach from the theory of dynamical systems is used to derive a necessary condition for avoiding unrealistic oscillations of the numerical approximation.","sentences":["The work deals with two major topics concerning the numerical analysis of Runge-Kutta-like (RK-like) methods, namely their stability and order of convergence.","RK-like methods differ from additive RK methods in that their coefficients are allowed to depend on the solution and the step size.","As a result of this, we also refer to them as non-standard additive RK (NSARK) methods.","The first major part of this thesis is dedicated to providing a tool for deriving order conditions for NSARK methods.","The proposed approach may yield implicit order conditions, which can be rewritten in explicit form using the NB-series of the stages.","The obtained explicit order conditions can be further reduced using Gr\\\"obner bases computations.","With the presented approach, it was possible for the first time to obtain conditions for the construction of 3rd and 4th order GeCo as well as 4th order MPRK schemes.","Moreover, a new fourth order MPRK method is constructed using our theory and the order of convergence is validated numerically.","The second major part is concerned with the stability of nonlinear time integrators preserving at least one linear invariant.","We discuss how the given approach generalizes the notion of A-stability.","We can prove that investigating the Jacobian of the generating map is sufficient to understand the stability of the nonlinear method in a neighborhood of the steady state.","This approach allows for the first time the investigation of several modified Patankar.","In the case of MPRK schemes, we compute a general stability function in a way that can be easily adapted to the case of PDRS.","Finally, the approach from the theory of dynamical systems is used to derive a necessary condition for avoiding unrealistic oscillations of the numerical approximation."],"url":"http://arxiv.org/abs/2402.13788v1","category":"math.NA"}
{"created":"2024-02-21 13:10:58","title":"Synthesis of Hierarchical Controllers Based on Deep Reinforcement Learning Policies","abstract":"We propose a novel approach to the problem of controller design for environments modeled as Markov decision processes (MDPs). Specifically, we consider a hierarchical MDP a graph with each vertex populated by an MDP called a \"room\". We first apply deep reinforcement learning (DRL) to obtain low-level policies for each room, scaling to large rooms of unknown structure. We then apply reactive synthesis to obtain a high-level planner that chooses which low-level policy to execute in each room. The central challenge in synthesizing the planner is the need for modeling rooms. We address this challenge by developing a DRL procedure to train concise \"latent\" policies together with PAC guarantees on their performance. Unlike previous approaches, ours circumvents a model distillation step. Our approach combats sparse rewards in DRL and enables reusability of low-level policies. We demonstrate feasibility in a case study involving agent navigation amid moving obstacles.","sentences":["We propose a novel approach to the problem of controller design for environments modeled as Markov decision processes (MDPs).","Specifically, we consider a hierarchical MDP a graph with each vertex populated by an MDP called a \"room\".","We first apply deep reinforcement learning (DRL) to obtain low-level policies for each room, scaling to large rooms of unknown structure.","We then apply reactive synthesis to obtain a high-level planner that chooses which low-level policy to execute in each room.","The central challenge in synthesizing the planner is the need for modeling rooms.","We address this challenge by developing a DRL procedure to train concise \"latent\" policies together with PAC guarantees on their performance.","Unlike previous approaches, ours circumvents a model distillation step.","Our approach combats sparse rewards in DRL and enables reusability of low-level policies.","We demonstrate feasibility in a case study involving agent navigation amid moving obstacles."],"url":"http://arxiv.org/abs/2402.13785v1","category":"cs.AI"}
{"created":"2024-02-21 13:09:46","title":"Theoretical analysis and predictions for the double electron capture of $^{124}$Xe","abstract":"We provide a complete theoretical description of the two-neutrino electron capture in $^{124}$Xe, improving both the nuclear and the atomic structure calculations. We improve the general formalism through the use of the Taylor expansion method, leading to higher order terms in the decay rate of the process. The nuclear part is treated with pn-QRPA and interacting shell model (ISM) methods. The nuclear matrix elements (NMEs) are calculated with the pn-QRPA method with spin restoration by fixing the input parameters so that the experimental decay rate is reproduced, resulting in values significantly lower than in previous calculations. The validity of the pn-QRPA NMEs is tested by showing their values to be comparable with the ones for double-beta decay with emission of two electrons of $^{128,130}$Te, which have similar pairing features. Within the ISM, we reproduce the total experimental half-life within a factor of two and predict the capture fraction to the KK channel of about 74\\%. We also predict the capture fractions to other decay channels and show that for the cumulative decay to the $\\rm{KL_{1}}$-$\\rm{KO_{1}}$ channels, a capture fraction of about 24\\% could be observed experimentally. On the atomic side, calculations are improved by accounting for the Pauli blocking of the decay of innermost nucleon states and by considering all $s$-wave electrons available for capture, expanding beyond the K and L$_1$ orbitals considered in previous studies. We also provide improved atomic relaxation energies of the final atomic states of $^{124}$Te, which may be used as input for background modeling in liquid Xenon experiments.","sentences":["We provide a complete theoretical description of the two-neutrino electron capture in $^{124}$Xe, improving both the nuclear and the atomic structure calculations.","We improve the general formalism through the use of the Taylor expansion method, leading to higher order terms in the decay rate of the process.","The nuclear part is treated with pn-QRPA and interacting shell model (ISM) methods.","The nuclear matrix elements (NMEs) are calculated with the pn-QRPA method with spin restoration by fixing the input parameters so that the experimental decay rate is reproduced, resulting in values significantly lower than in previous calculations.","The validity of the pn-QRPA NMEs is tested by showing their values to be comparable with the ones for double-beta decay with emission of two electrons of $^{128,130}$Te, which have similar pairing features.","Within the ISM, we reproduce the total experimental half-life within a factor of two and predict the capture fraction to the KK channel of about 74\\%.","We also predict the capture fractions to other decay channels and show that for the cumulative decay to the $\\rm{KL_{1}}$-$\\rm{KO_{1}}$ channels, a capture fraction of about 24\\% could be observed experimentally.","On the atomic side, calculations are improved by accounting for the Pauli blocking of the decay of innermost nucleon states and by considering all $s$-wave electrons available for capture, expanding beyond the K and L$_1$ orbitals considered in previous studies.","We also provide improved atomic relaxation energies of the final atomic states of $^{124}$Te, which may be used as input for background modeling in liquid Xenon experiments."],"url":"http://arxiv.org/abs/2402.13784v1","category":"nucl-th"}
{"created":"2024-02-21 13:06:52","title":"Semirings for Probabilistic and Neuro-Symbolic Logic Programming","abstract":"The field of probabilistic logic programming (PLP) focuses on integrating probabilistic models into programming languages based on logic. Over the past 30 years, numerous languages and frameworks have been developed for modeling, inference and learning in probabilistic logic programs. While originally PLP focused on discrete probability, more recent approaches have incorporated continuous distributions as well as neural networks, effectively yielding neural-symbolic methods. We provide a unified algebraic perspective on PLP, showing that many if not most of the extensions of PLP can be cast within a common algebraic logic programming framework, in which facts are labeled with elements of a semiring and disjunction and conjunction are replaced by addition and multiplication. This does not only hold for the PLP variations itself but also for the underlying execution mechanism that is based on (algebraic) model counting.","sentences":["The field of probabilistic logic programming (PLP) focuses on integrating probabilistic models into programming languages based on logic.","Over the past 30 years, numerous languages and frameworks have been developed for modeling, inference and learning in probabilistic logic programs.","While originally PLP focused on discrete probability, more recent approaches have incorporated continuous distributions as well as neural networks, effectively yielding neural-symbolic methods.","We provide a unified algebraic perspective on PLP, showing that many if not most of the extensions of PLP can be cast within a common algebraic logic programming framework, in which facts are labeled with elements of a semiring and disjunction and conjunction are replaced by addition and multiplication.","This does not only hold for the PLP variations itself but also for the underlying execution mechanism that is based on (algebraic) model counting."],"url":"http://arxiv.org/abs/2402.13782v1","category":"cs.AI"}
{"created":"2024-02-21 12:58:40","title":"Contextual Molecule Representation Learning from Chemical Reaction Knowledge","abstract":"In recent years, self-supervised learning has emerged as a powerful tool to harness abundant unlabelled data for representation learning and has been broadly adopted in diverse areas. However, when applied to molecular representation learning (MRL), prevailing techniques such as masked sub-unit reconstruction often fall short, due to the high degree of freedom in the possible combinations of atoms within molecules, which brings insurmountable complexity to the masking-reconstruction paradigm. To tackle this challenge, we introduce REMO, a self-supervised learning framework that takes advantage of well-defined atom-combination rules in common chemistry. Specifically, REMO pre-trains graph/Transformer encoders on 1.7 million known chemical reactions in the literature. We propose two pre-training objectives: Masked Reaction Centre Reconstruction (MRCR) and Reaction Centre Identification (RCI). REMO offers a novel solution to MRL by exploiting the underlying shared patterns in chemical reactions as \\textit{context} for pre-training, which effectively infers meaningful representations of common chemistry knowledge. Such contextual representations can then be utilized to support diverse downstream molecular tasks with minimum finetuning, such as affinity prediction and drug-drug interaction prediction. Extensive experimental results on MoleculeACE, ACNet, drug-drug interaction (DDI), and reaction type classification show that across all tested downstream tasks, REMO outperforms the standard baseline of single-molecule masked modeling used in current MRL. Remarkably, REMO is the pioneering deep learning model surpassing fingerprint-based methods in activity cliff benchmarks.","sentences":["In recent years, self-supervised learning has emerged as a powerful tool to harness abundant unlabelled data for representation learning and has been broadly adopted in diverse areas.","However, when applied to molecular representation learning (MRL), prevailing techniques such as masked sub-unit reconstruction often fall short, due to the high degree of freedom in the possible combinations of atoms within molecules, which brings insurmountable complexity to the masking-reconstruction paradigm.","To tackle this challenge, we introduce REMO, a self-supervised learning framework that takes advantage of well-defined atom-combination rules in common chemistry.","Specifically, REMO pre-trains graph/Transformer encoders on 1.7 million known chemical reactions in the literature.","We propose two pre-training objectives: Masked Reaction Centre Reconstruction (MRCR) and Reaction Centre Identification (RCI).","REMO offers a novel solution to MRL by exploiting the underlying shared patterns in chemical reactions as \\textit{context} for pre-training, which effectively infers meaningful representations of common chemistry knowledge.","Such contextual representations can then be utilized to support diverse downstream molecular tasks with minimum finetuning, such as affinity prediction and drug-drug interaction prediction.","Extensive experimental results on MoleculeACE, ACNet, drug-drug interaction (DDI), and reaction type classification show that across all tested downstream tasks, REMO outperforms the standard baseline of single-molecule masked modeling used in current MRL.","Remarkably, REMO is the pioneering deep learning model surpassing fingerprint-based methods in activity cliff benchmarks."],"url":"http://arxiv.org/abs/2402.13779v1","category":"cs.LG"}
{"created":"2024-02-21 12:57:43","title":"Weakly supervised localisation of prostate cancer using reinforcement learning for bi-parametric MR images","abstract":"In this paper we propose a reinforcement learning based weakly supervised system for localisation. We train a controller function to localise regions of interest within an image by introducing a novel reward definition that utilises non-binarised classification probability, generated by a pre-trained binary classifier which classifies object presence in images or image crops. The object-presence classifier may then inform the controller of its localisation quality by quantifying the likelihood of the image containing an object. Such an approach allows us to minimize any potential labelling or human bias propagated via human labelling for fully supervised localisation. We evaluate our proposed approach for a task of cancerous lesion localisation on a large dataset of real clinical bi-parametric MR images of the prostate. Comparisons to the commonly used multiple-instance learning weakly supervised localisation and to a fully supervised baseline show that our proposed method outperforms the multi-instance learning and performs comparably to fully-supervised learning, using only image-level classification labels for training.","sentences":["In this paper we propose a reinforcement learning based weakly supervised system for localisation.","We train a controller function to localise regions of interest within an image by introducing a novel reward definition that utilises non-binarised classification probability, generated by a pre-trained binary classifier which classifies object presence in images or image crops.","The object-presence classifier may then inform the controller of its localisation quality by quantifying the likelihood of the image containing an object.","Such an approach allows us to minimize any potential labelling or human bias propagated via human labelling for fully supervised localisation.","We evaluate our proposed approach for a task of cancerous lesion localisation on a large dataset of real clinical bi-parametric MR images of the prostate.","Comparisons to the commonly used multiple-instance learning weakly supervised localisation and to a fully supervised baseline show that our proposed method outperforms the multi-instance learning and performs comparably to fully-supervised learning, using only image-level classification labels for training."],"url":"http://arxiv.org/abs/2402.13778v1","category":"cs.CV"}
{"created":"2024-02-21 12:54:48","title":"Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions","abstract":"Deep generative models (DGMs) have demonstrated great success across various domains, particularly in generating texts, images, and videos using models trained from offline data. Similarly, data-driven decision-making and robotic control also necessitate learning a generator function from the offline data to serve as the strategy or policy. In this case, applying deep generative models in offline policy learning exhibits great potential, and numerous studies have explored in this direction. However, this field still lacks a comprehensive review and so developments of different branches are relatively independent. Thus, we provide the first systematic review on the applications of deep generative models for offline policy learning. In particular, we cover five mainstream deep generative models, including Variational Auto-Encoders, Generative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion Models, and their applications in both offline reinforcement learning (offline RL) and imitation learning (IL). Offline RL and IL are two main branches of offline policy learning and are widely-adopted techniques for sequential decision-making. Specifically, for each type of DGM-based offline policy learning, we distill its fundamental scheme, categorize related works based on the usage of the DGM, and sort out the development process of algorithms in that field. Subsequent to the main content, we provide in-depth discussions on deep generative models and offline policy learning as a summary, based on which we present our perspectives on future research directions. This work offers a hands-on reference for the research progress in deep generative models for offline policy learning, and aims to inspire improved DGM-based offline RL or IL algorithms.","sentences":["Deep generative models (DGMs) have demonstrated great success across various domains, particularly in generating texts, images, and videos using models trained from offline data.","Similarly, data-driven decision-making and robotic control also necessitate learning a generator function from the offline data to serve as the strategy or policy.","In this case, applying deep generative models in offline policy learning exhibits great potential, and numerous studies have explored in this direction.","However, this field still lacks a comprehensive review and so developments of different branches are relatively independent.","Thus, we provide the first systematic review on the applications of deep generative models for offline policy learning.","In particular, we cover five mainstream deep generative models, including Variational Auto-Encoders, Generative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion Models, and their applications in both offline reinforcement learning (offline RL) and imitation learning (IL).","Offline RL and IL are two main branches of offline policy learning and are widely-adopted techniques for sequential decision-making.","Specifically, for each type of DGM-based offline policy learning, we distill its fundamental scheme, categorize related works based on the usage of the DGM, and sort out the development process of algorithms in that field.","Subsequent to the main content, we provide in-depth discussions on deep generative models and offline policy learning as a summary, based on which we present our perspectives on future research directions.","This work offers a hands-on reference for the research progress in deep generative models for offline policy learning, and aims to inspire improved DGM-based offline RL or IL algorithms."],"url":"http://arxiv.org/abs/2402.13777v1","category":"cs.LG"}
{"created":"2024-02-21 12:54:40","title":"Cas-DiffCom: Cascaded diffusion model for infant longitudinal super-resolution 3D medical image completion","abstract":"Early infancy is a rapid and dynamic neurodevelopmental period for behavior and neurocognition. Longitudinal magnetic resonance imaging (MRI) is an effective tool to investigate such a crucial stage by capturing the developmental trajectories of the brain structures. However, longitudinal MRI acquisition always meets a serious data-missing problem due to participant dropout and failed scans, making longitudinal infant brain atlas construction and developmental trajectory delineation quite challenging. Thanks to the development of an AI-based generative model, neuroimage completion has become a powerful technique to retain as much available data as possible. However, current image completion methods usually suffer from inconsistency within each individual subject in the time dimension, compromising the overall quality. To solve this problem, our paper proposed a two-stage cascaded diffusion model, Cas-DiffCom, for dense and longitudinal 3D infant brain MRI completion and super-resolution. We applied our proposed method to the Baby Connectome Project (BCP) dataset. The experiment results validate that Cas-DiffCom achieves both individual consistency and high fidelity in longitudinal infant brain image completion. We further applied the generated infant brain images to two downstream tasks, brain tissue segmentation and developmental trajectory delineation, to declare its task-oriented potential in the neuroscience field.","sentences":["Early infancy is a rapid and dynamic neurodevelopmental period for behavior and neurocognition.","Longitudinal magnetic resonance imaging (MRI) is an effective tool to investigate such a crucial stage by capturing the developmental trajectories of the brain structures.","However, longitudinal MRI acquisition always meets a serious data-missing problem due to participant dropout and failed scans, making longitudinal infant brain atlas construction and developmental trajectory delineation quite challenging.","Thanks to the development of an AI-based generative model, neuroimage completion has become a powerful technique to retain as much available data as possible.","However, current image completion methods usually suffer from inconsistency within each individual subject in the time dimension, compromising the overall quality.","To solve this problem, our paper proposed a two-stage cascaded diffusion model, Cas-DiffCom, for dense and longitudinal 3D infant brain MRI completion and super-resolution.","We applied our proposed method to the Baby Connectome Project (BCP) dataset.","The experiment results validate that Cas-DiffCom achieves both individual consistency and high fidelity in longitudinal infant brain image completion.","We further applied the generated infant brain images to two downstream tasks, brain tissue segmentation and developmental trajectory delineation, to declare its task-oriented potential in the neuroscience field."],"url":"http://arxiv.org/abs/2402.13776v1","category":"eess.IV"}
{"created":"2024-02-21 12:52:31","title":"Top cell attachment for a Poincare Duality complex","abstract":"Let M be a simply-connected closed Poincare Duality complex of dimension n. Then M is obtained by attaching a cell of highest dimension to its (n-1)-skeleton M'. Conditions are given for when the skeletal inclusion i:M' --> M has the property that the based loops on i has a right homotopy inverse. This is an integral version of the rational statement that such a right homotopy inverse always exists provided the rational cohomology of M is not generated by a single element. New methods are developed in order to do the integral case. These lead to p-local versions and recover the full rational statement. Families for which the integral statement holds include moment-angle manifolds and quasi-toric manifolds.","sentences":["Let M be a simply-connected closed Poincare Duality complex of dimension n.","Then M is obtained by attaching a cell of highest dimension to its (n-1)-skeleton M'.","Conditions are given for when the skeletal inclusion","i:M' --> M has the property that the based loops on i has a right homotopy inverse.","This is an integral version of the rational statement that such a right homotopy inverse always exists provided the rational cohomology of M is not generated by a single element.","New methods are developed in order to do the integral case.","These lead to p-local versions and recover the full rational statement.","Families for which the integral statement holds include moment-angle manifolds and quasi-toric manifolds."],"url":"http://arxiv.org/abs/2402.13775v1","category":"math.AT"}
{"created":"2024-02-21 12:50:44","title":"Spatial-Domain Wireless Jamming with Reconfigurable Intelligent Surfaces","abstract":"Today, we rely heavily on the constant availability of wireless communication systems. As a result, wireless jamming continues to prevail as an imminent threat: Attackers can create deliberate radio interference to overshadow desired signals, leading to denial of service. Although the broadcast nature of radio signal propagation makes such an attack possible in the first place, it likewise poses a challenge for the attacker, preventing precise targeting of single devices. In particular, the jamming signal will likely not only reach the victim receiver but also other neighboring devices. In this work, we introduce spatial control of wireless jamming signals, granting a new degree of freedom to leverage for jamming attacks. Our novel strategy employs an environment-adaptive reconfigurable intelligent surface (RIS), exploiting multipath signal propagation to spatially focus jamming signals on particular victim devices. We investigate this effect through extensive experimentation and show that our approach can disable the wireless communication of a victim device while leaving neighbouring devices unaffected. In particular, we demonstrate complete denial-of-service of a Wi-Fi device while a second device located at a distance as close as 5 mm remains unaffected, sustaining wireless communication at a data rate of 60 Mbit/s. We also show that the attacker can change the attack target on-the-fly, dynamically selecting the device to be jammed.","sentences":["Today, we rely heavily on the constant availability of wireless communication systems.","As a result, wireless jamming continues to prevail as an imminent threat: Attackers can create deliberate radio interference to overshadow desired signals, leading to denial of service.","Although the broadcast nature of radio signal propagation makes such an attack possible in the first place, it likewise poses a challenge for the attacker, preventing precise targeting of single devices.","In particular, the jamming signal will likely not only reach the victim receiver but also other neighboring devices.","In this work, we introduce spatial control of wireless jamming signals, granting a new degree of freedom to leverage for jamming attacks.","Our novel strategy employs an environment-adaptive reconfigurable intelligent surface (RIS), exploiting multipath signal propagation to spatially focus jamming signals on particular victim devices.","We investigate this effect through extensive experimentation and show that our approach can disable the wireless communication of a victim device while leaving neighbouring devices unaffected.","In particular, we demonstrate complete denial-of-service of a Wi-Fi device while a second device located at a distance as close as 5 mm remains unaffected, sustaining wireless communication at a data rate of 60 Mbit/s. We also show that the attacker can change the attack target on-the-fly, dynamically selecting the device to be jammed."],"url":"http://arxiv.org/abs/2402.13773v1","category":"cs.CR"}
{"created":"2024-02-21 12:48:45","title":"Mask-up: Investigating Biases in Face Re-identification for Masked Faces","abstract":"AI based Face Recognition Systems (FRSs) are now widely distributed and deployed as MLaaS solutions all over the world, moreso since the COVID-19 pandemic for tasks ranging from validating individuals' faces while buying SIM cards to surveillance of citizens. Extensive biases have been reported against marginalized groups in these systems and have led to highly discriminatory outcomes. The post-pandemic world has normalized wearing face masks but FRSs have not kept up with the changing times. As a result, these systems are susceptible to mask based face occlusion. In this study, we audit four commercial and nine open-source FRSs for the task of face re-identification between different varieties of masked and unmasked images across five benchmark datasets (total 14,722 images). These simulate a realistic validation/surveillance task as deployed in all major countries around the world. Three of the commercial and five of the open-source FRSs are highly inaccurate; they further perpetuate biases against non-White individuals, with the lowest accuracy being 0%. A survey for the same task with 85 human participants also results in a low accuracy of 40%. Thus a human-in-the-loop moderation in the pipeline does not alleviate the concerns, as has been frequently hypothesized in literature. Our large-scale study shows that developers, lawmakers and users of such services need to rethink the design principles behind FRSs, especially for the task of face re-identification, taking cognizance of observed biases.","sentences":["AI based Face Recognition Systems (FRSs) are now widely distributed and deployed as MLaaS solutions all over the world, moreso since the COVID-19 pandemic for tasks ranging from validating individuals' faces while buying SIM cards to surveillance of citizens.","Extensive biases have been reported against marginalized groups in these systems and have led to highly discriminatory outcomes.","The post-pandemic world has normalized wearing face masks but FRSs have not kept up with the changing times.","As a result, these systems are susceptible to mask based face occlusion.","In this study, we audit four commercial and nine open-source FRSs for the task of face re-identification between different varieties of masked and unmasked images across five benchmark datasets (total 14,722 images).","These simulate a realistic validation/surveillance task as deployed in all major countries around the world.","Three of the commercial and five of the open-source FRSs are highly inaccurate; they further perpetuate biases against non-White individuals, with the lowest accuracy being 0%.","A survey for the same task with 85 human participants also results in a low accuracy of 40%.","Thus a human-in-the-loop moderation in the pipeline does not alleviate the concerns, as has been frequently hypothesized in literature.","Our large-scale study shows that developers, lawmakers and users of such services need to rethink the design principles behind FRSs, especially for the task of face re-identification, taking cognizance of observed biases."],"url":"http://arxiv.org/abs/2402.13771v1","category":"cs.CV"}
{"created":"2024-02-21 12:44:21","title":"General Debiasing for Graph-based Collaborative Filtering via Adversarial Graph Dropout","abstract":"Graph neural networks (GNNs) have shown impressive performance in recommender systems, particularly in collaborative filtering (CF). The key lies in aggregating neighborhood information on a user-item interaction graph to enhance user/item representations. However, we have discovered that this aggregation mechanism comes with a drawback, which amplifies biases present in the interaction graph. For instance, a user's interactions with items can be driven by both unbiased true interest and various biased factors like item popularity or exposure. However, the current aggregation approach combines all information, both biased and unbiased, leading to biased representation learning. Consequently, graph-based recommenders can learn distorted views of users/items, hindering the modeling of their true preferences and generalizations. To address this issue, we introduce a novel framework called Adversarial Graph Dropout (AdvDrop). It differentiates between unbiased and biased interactions, enabling unbiased representation learning. For each user/item, AdvDrop employs adversarial learning to split the neighborhood into two views: one with bias-mitigated interactions and the other with bias-aware interactions. After view-specific aggregation, AdvDrop ensures that the bias-mitigated and bias-aware representations remain invariant, shielding them from the influence of bias. We validate AdvDrop's effectiveness on five public datasets that cover both general and specific biases, demonstrating significant improvements. Furthermore, our method exhibits meaningful separation of subgraphs and achieves unbiased representations for graph-based CF models, as revealed by in-depth analysis. Our code is publicly available at https://github.com/Arthurma71/AdvDrop.","sentences":["Graph neural networks (GNNs) have shown impressive performance in recommender systems, particularly in collaborative filtering (CF).","The key lies in aggregating neighborhood information on a user-item interaction graph to enhance user/item representations.","However, we have discovered that this aggregation mechanism comes with a drawback, which amplifies biases present in the interaction graph.","For instance, a user's interactions with items can be driven by both unbiased true interest and various biased factors like item popularity or exposure.","However, the current aggregation approach combines all information, both biased and unbiased, leading to biased representation learning.","Consequently, graph-based recommenders can learn distorted views of users/items, hindering the modeling of their true preferences and generalizations.","To address this issue, we introduce a novel framework called Adversarial Graph Dropout (AdvDrop).","It differentiates between unbiased and biased interactions, enabling unbiased representation learning.","For each user/item, AdvDrop employs adversarial learning to split the neighborhood into two views: one with bias-mitigated interactions and the other with bias-aware interactions.","After view-specific aggregation, AdvDrop ensures that the bias-mitigated and bias-aware representations remain invariant, shielding them from the influence of bias.","We validate AdvDrop's effectiveness on five public datasets that cover both general and specific biases, demonstrating significant improvements.","Furthermore, our method exhibits meaningful separation of subgraphs and achieves unbiased representations for graph-based CF models, as revealed by in-depth analysis.","Our code is publicly available at https://github.com/Arthurma71/AdvDrop."],"url":"http://arxiv.org/abs/2402.13769v1","category":"cs.IR"}
{"created":"2024-02-21 12:41:57","title":"Generalized Red-Blue Circular Annulus Cover Problem","abstract":"We study the Generalized Red-Blue Annulus Cover problem for two sets of points, red ($R$) and blue ($B$), where each point $p \\in R\\cup B$ is associated with a positive penalty ${\\cal P}(p)$. The red points have non-covering penalties, and the blue points have covering penalties. The objective is to compute a circular annulus ${\\cal A}$ such that the value of the function ${\\cal P}({R}^{out})$ + ${\\cal P}({ B}^{in})$ is minimum, where ${R}^{out} \\subseteq {R}$ is the set of red points not covered by ${\\cal A}$ and ${B}^{in} \\subseteq {B}$ is the set of blue points covered by $\\cal A$. We also study another version of this problem, where all the red points in $R$ and the minimum number of points in $B$ are covered by the circular annulus in two dimensions. We design polynomial-time algorithms for all such circular annulus problems.","sentences":["We study the Generalized Red-Blue Annulus Cover problem for two sets of points, red ($R$) and blue ($B$), where each point $p \\in R\\cup B$ is associated with a positive penalty ${\\cal P}(p)$.","The red points have non-covering penalties, and the blue points have covering penalties.","The objective is to compute a circular annulus ${\\cal A}$ such that the value of the function ${\\cal P}({R}^{out})$ + ${\\cal P}({ B}^{in})$ is minimum, where ${R}^{out} \\subseteq {R}$ is the set of red points not covered by ${\\cal A}$ and ${B}^{in} \\subseteq {B}$ is the set of blue points covered by $\\cal A$.","We also study another version of this problem, where all the red points in $R$ and the minimum number of points in $B$ are covered by the circular annulus in two dimensions.","We design polynomial-time algorithms for all such circular annulus problems."],"url":"http://arxiv.org/abs/2402.13767v1","category":"cs.CG"}
{"created":"2024-02-21 12:39:20","title":"Accuracy-Preserving Calibration via Statistical Modeling on Probability Simplex","abstract":"Classification models based on deep neural networks (DNNs) must be calibrated to measure the reliability of predictions. Some recent calibration methods have employed a probabilistic model on the probability simplex. However, these calibration methods cannot preserve the accuracy of pre-trained models, even those with a high classification accuracy. We propose an accuracy-preserving calibration method using the Concrete distribution as the probabilistic model on the probability simplex. We theoretically prove that a DNN model trained on cross-entropy loss has optimality as the parameter of the Concrete distribution. We also propose an efficient method that synthetically generates samples for training probabilistic models on the probability simplex. We demonstrate that the proposed method can outperform previous methods in accuracy-preserving calibration tasks using benchmarks.","sentences":["Classification models based on deep neural networks (DNNs) must be calibrated to measure the reliability of predictions.","Some recent calibration methods have employed a probabilistic model on the probability simplex.","However, these calibration methods cannot preserve the accuracy of pre-trained models, even those with a high classification accuracy.","We propose an accuracy-preserving calibration method using the Concrete distribution as the probabilistic model on the probability simplex.","We theoretically prove that a DNN model trained on cross-entropy loss has optimality as the parameter of the Concrete distribution.","We also propose an efficient method that synthetically generates samples for training probabilistic models on the probability simplex.","We demonstrate that the proposed method can outperform previous methods in accuracy-preserving calibration tasks using benchmarks."],"url":"http://arxiv.org/abs/2402.13765v1","category":"cs.LG"}
{"created":"2024-02-21 12:38:59","title":"CriticBench: Evaluating Large Language Models as Critic","abstract":"Critique ability are crucial in the scalable oversight and self-improvement of Large Language Models (LLMs). While many recent studies explore the critique ability of LLMs to judge and refine flaws in generations, how to comprehensively and reliably measure the critique abilities of LLMs is under-explored. This paper introduces \\shortname, a novel benchmark designed to comprehensively and reliably evaluate four key critique ability dimensions of LLMs: feedback, comparison, refinement and meta-feedback. \\shortname~encompasses nine diverse tasks, each assessing the LLMs' ability to critique responses at varying levels of quality granularity. Our extensive evaluations of open-source and closed-source LLMs reveal intriguing relationships between the critique ability and tasks, response qualities, and model scales. Datasets, resources and evaluation toolkit for \\shortname~will be publicly released at \\url{https://github.com/gmftbyGMFTBY/CriticBench}.","sentences":["Critique ability are crucial in the scalable oversight and self-improvement of Large Language Models (LLMs).","While many recent studies explore the critique ability of LLMs to judge and refine flaws in generations, how to comprehensively and reliably measure the critique abilities of LLMs is under-explored.","This paper introduces \\shortname, a novel benchmark designed to comprehensively and reliably evaluate four key critique ability dimensions of LLMs: feedback, comparison, refinement and meta-feedback.","\\shortname~encompasses nine diverse tasks, each assessing the LLMs' ability to critique responses at varying levels of quality granularity.","Our extensive evaluations of open-source and closed-source LLMs reveal intriguing relationships between the critique ability and tasks, response qualities, and model scales.","Datasets, resources and evaluation toolkit for \\shortname~will be publicly released at \\url{https://github.com/gmftbyGMFTBY/CriticBench}."],"url":"http://arxiv.org/abs/2402.13764v1","category":"cs.CL"}
{"created":"2024-02-21 12:37:44","title":"Sharpening the dark matter signature in gravitational waveforms II: Numerical simulations with the NbodyIMRI code","abstract":"Future gravitational wave observatories can probe dark matter by detecting the dephasing in the waveform of binary black hole mergers induced by dark matter overdensities. Such a detection hinges on the accurate modelling of the dynamical friction, induced by dark matter on the secondary compact object in intermediate and extreme mass ratio inspirals. In this paper, we introduce NbodyIMRI, a new publicly available code designed for simulating binary systems within cold dark matter `spikes'. Leveraging higher particle counts and finer timesteps, we validate the applicability of the standard dynamical friction formalism and provide an accurate determination of the maximum impact parameter of particles which can effectively scatter with a compact object, across various mass ratios. We also show that in addition to feedback due to dynamical friction, the dark matter also evolves through a `stirring' effect driven by the time-dependent potential of the binary. We introduce a simple semi-analytical scheme to account for this effect and demonstrate that including stirring tends to slow the rate of dark matter depletion and therefore enhances the impact of dark matter on the dynamics of the binary.","sentences":["Future gravitational wave observatories can probe dark matter by detecting the dephasing in the waveform of binary black hole mergers induced by dark matter overdensities.","Such a detection hinges on the accurate modelling of the dynamical friction, induced by dark matter on the secondary compact object in intermediate and extreme mass ratio inspirals.","In this paper, we introduce NbodyIMRI, a new publicly available code designed for simulating binary systems within cold dark matter `spikes'.","Leveraging higher particle counts and finer timesteps, we validate the applicability of the standard dynamical friction formalism and provide an accurate determination of the maximum impact parameter of particles which can effectively scatter with a compact object, across various mass ratios.","We also show that in addition to feedback due to dynamical friction, the dark matter also evolves through a `stirring' effect driven by the time-dependent potential of the binary.","We introduce a simple semi-analytical scheme to account for this effect and demonstrate that including stirring tends to slow the rate of dark matter depletion and therefore enhances the impact of dark matter on the dynamics of the binary."],"url":"http://arxiv.org/abs/2402.13762v1","category":"gr-qc"}
{"created":"2024-02-21 12:37:41","title":"Leaving No Matter Unturned -- Analysing existing LHC measurements and events with jets and missing transverse energy measured by the ATLAS Experiment insearch of Dark Matter","abstract":"Various astrophysical observations point towards an as-of-yet unexplained, mainly gravitationally interacting type of matter. If this matter, called Dark Matter, is an elementary particle, it could be produced in particle collisions at the Large Hadron Collider. Given its weak interaction with ordinary matter, however, it would not be directly observable with the general-purpose detectors at the Large Hadron Collider. Its production would therefore manifest as events in which detector-visible objects recoil against the detector-invisible Dark Matter, giving rise to missing transverse energy. This thesis focuses on final states in which these visible objects are jets.   A measurement of the final state of large missing transverse energy and at least one jet in 139 fb$^{-1}$ of proton-proton collisions at 13 TeV recorded with the ATLAS detector at the Large Hadron Collider is performed in this thesis. Good agreement between measured data and Standard-Model prediction is found in a statistical fit, corresponding to a reduced chi-square of 1.37. The measurement is corrected for detector effects to facilitate later reinterpretation. Measurements prepared in such a way can, for example, be exploited by the CONTUR toolkit to set constraints on new theories. Both, the results of the measurement and the CONTUR toolkit making use of existing measurements at the Large Hadron Collider, are employed to set exclusion limits on a model able to explain Dark Matter, the two-Higgs-doublet model with a pseudoscalar mediator to Dark Matter. At $\\tan\\beta=1$, masses of the pseudoscalar $A$ up to 425 GeV and larger than 1600 GeV are excluded at 95 % confidence level. At $m_H\\equiv m_A\\equiv m_{H^\\pm}=$ 600 GeV, masses of the pseudoscalar $a$ up to 550 GeV and values of $\\tan\\beta$ up to 1.5 as well as larger than 20 are excluded at 95 % confidence level.","sentences":["Various astrophysical observations point towards an as-of-yet unexplained, mainly gravitationally interacting type of matter.","If this matter, called Dark Matter, is an elementary particle, it could be produced in particle collisions at the Large Hadron Collider.","Given its weak interaction with ordinary matter, however, it would not be directly observable with the general-purpose detectors at the Large Hadron Collider.","Its production would therefore manifest as events in which detector-visible objects recoil against the detector-invisible Dark Matter, giving rise to missing transverse energy.","This thesis focuses on final states in which these visible objects are jets.   ","A measurement of the final state of large missing transverse energy and at least one jet in 139 fb$^{-1}$ of proton-proton collisions at 13 TeV recorded with the ATLAS detector at the Large Hadron Collider is performed in this thesis.","Good agreement between measured data and Standard-Model prediction is found in a statistical fit, corresponding to a reduced chi-square of 1.37.","The measurement is corrected for detector effects to facilitate later reinterpretation.","Measurements prepared in such a way can, for example, be exploited by the CONTUR toolkit to set constraints on new theories.","Both, the results of the measurement and the CONTUR toolkit making use of existing measurements at the Large Hadron Collider, are employed to set exclusion limits on a model able to explain Dark Matter, the two-Higgs-doublet model with a pseudoscalar mediator to Dark Matter.","At $\\tan\\beta=1$, masses of the pseudoscalar $A$ up to 425 GeV and larger than 1600 GeV are excluded at 95 % confidence level.","At $m_H\\equiv m_A\\equiv m_{H^\\pm}=$ 600 GeV, masses of the pseudoscalar $a$ up to 550 GeV and values of $\\tan\\beta$ up to 1.5 as well as larger than 20 are excluded at 95 % confidence level."],"url":"http://arxiv.org/abs/2402.13760v1","category":"hep-ex"}
{"created":"2024-02-21 12:35:19","title":"Factual Consistency Evaluation of Summarisation in the Era of Large Language Models","abstract":"Factual inconsistency with source documents in automatically generated summaries can lead to misinformation or pose risks. Existing factual consistency(FC) metrics are constrained by their performance, efficiency, and explainability. Recent advances in Large language models (LLMs) have demonstrated remarkable potential in text evaluation but their effectiveness in assessing FC in summarisation remains underexplored. Prior research has mostly focused on proprietary LLMs, leaving essential factors that affect their assessment capabilities unexplored. Additionally, current FC evaluation benchmarks are restricted to news articles, casting doubt on the generality of the FC methods tested on them. In this paper, we first address the gap by introducing TreatFact a dataset of LLM-generated summaries of clinical texts, annotated for FC by domain experts. Moreover, we benchmark 11 LLMs for FC evaluation across news and clinical domains and analyse the impact of model size, prompts, pre-training and fine-tuning data. Our findings reveal that despite proprietary models prevailing on the task, open-source LLMs lag behind. Nevertheless, there is potential for enhancing the performance of open-source LLMs through increasing model size, expanding pre-training data, and developing well-curated fine-tuning data. Experiments on TreatFact suggest that both previous methods and LLM-based evaluators are unable to capture factual inconsistencies in clinical summaries, posing a new challenge for FC evaluation.","sentences":["Factual inconsistency with source documents in automatically generated summaries can lead to misinformation or pose risks.","Existing factual consistency(FC) metrics are constrained by their performance, efficiency, and explainability.","Recent advances in Large language models (LLMs) have demonstrated remarkable potential in text evaluation but their effectiveness in assessing FC in summarisation remains underexplored.","Prior research has mostly focused on proprietary LLMs, leaving essential factors that affect their assessment capabilities unexplored.","Additionally, current FC evaluation benchmarks are restricted to news articles, casting doubt on the generality of the FC methods tested on them.","In this paper, we first address the gap by introducing TreatFact a dataset of LLM-generated summaries of clinical texts, annotated for FC by domain experts.","Moreover, we benchmark 11 LLMs for FC evaluation across news and clinical domains and analyse the impact of model size, prompts, pre-training and fine-tuning data.","Our findings reveal that despite proprietary models prevailing on the task, open-source LLMs lag behind.","Nevertheless, there is potential for enhancing the performance of open-source LLMs through increasing model size, expanding pre-training data, and developing well-curated fine-tuning data.","Experiments on TreatFact suggest that both previous methods and LLM-based evaluators are unable to capture factual inconsistencies in clinical summaries, posing a new challenge for FC evaluation."],"url":"http://arxiv.org/abs/2402.13758v1","category":"cs.CL"}
{"created":"2024-02-21 12:35:12","title":"Critical Behavior and Collective Modes at the Superfluid Transition in Amorphous Systems","abstract":"We investigate the critical behavior and the dynamics of the amplitude (Higgs) mode close to the superfluid-insulator quantum phase transition in an amorphous system (i.e., a system subject to topological randomness). In particular, we map the two-dimensional Bose-Hubbard Hamiltonian defined on a random Voronoi-Delaunay lattice onto a (2+1)-dimensional layered classical XY model with correlated topological disorder. We study the resulting model by laying recourse to classical Monte Carlo simulations. We specifically focus on the scalar susceptibility of the order parameter to study the dynamics of the amplitude mode. To do so, we harness the maximum entropy method to perform the analytic continuation of the scalar susceptibility to real frequencies. Our analysis shows that the amplitude mode remains delocalized in the presence of such topological disorder, quite at odds with its behavior in generic disordered systems, where the randomness localizes the Higgs mode. Furthermore, we show that the critical behavior of the topologically disordered system is identical to that of its translationally invariant counterpart, consistent with a modified Harris criterion. This suggests that the localization of the collective excitations in the presence of disorder is tied to the critical behavior of the quantum phase transition rather than a simple Anderson-localization-type interference mechanism.","sentences":["We investigate the critical behavior and the dynamics of the amplitude (Higgs) mode close to the superfluid-insulator quantum phase transition in an amorphous system (i.e., a system subject to topological randomness).","In particular, we map the two-dimensional Bose-Hubbard Hamiltonian defined on a random Voronoi-Delaunay lattice onto a (2+1)-dimensional layered classical XY model with correlated topological disorder.","We study the resulting model by laying recourse to classical Monte Carlo simulations.","We specifically focus on the scalar susceptibility of the order parameter to study the dynamics of the amplitude mode.","To do so, we harness the maximum entropy method to perform the analytic continuation of the scalar susceptibility to real frequencies.","Our analysis shows that the amplitude mode remains delocalized in the presence of such topological disorder, quite at odds with its behavior in generic disordered systems, where the randomness localizes the Higgs mode.","Furthermore, we show that the critical behavior of the topologically disordered system is identical to that of its translationally invariant counterpart, consistent with a modified Harris criterion.","This suggests that the localization of the collective excitations in the presence of disorder is tied to the critical behavior of the quantum phase transition rather than a simple Anderson-localization-type interference mechanism."],"url":"http://arxiv.org/abs/2402.13757v1","category":"cond-mat.dis-nn"}
{"created":"2024-02-21 12:30:39","title":"Reinforcement learning-assisted quantum architecture search for variational quantum algorithms","abstract":"A significant hurdle in the noisy intermediate-scale quantum (NISQ) era is identifying functional quantum circuits. These circuits must also adhere to the constraints imposed by current quantum hardware limitations. Variational quantum algorithms (VQAs), a class of quantum-classical optimization algorithms, were developed to address these challenges in the currently available quantum devices. However, the overall performance of VQAs depends on the initialization strategy of the variational circuit, the structure of the circuit (also known as ansatz), and the configuration of the cost function. Focusing on the structure of the circuit, in this thesis, we improve the performance of VQAs by automating the search for an optimal structure for the variational circuits using reinforcement learning (RL). Within the thesis, the optimality of a circuit is determined by evaluating its depth, the overall count of gates and parameters, and its accuracy in solving the given problem. The task of automating the search for optimal quantum circuits is known as quantum architecture search (QAS). The majority of research in QAS is primarily focused on a noiseless scenario. Yet, the impact of noise on the QAS remains inadequately explored. In this thesis, we tackle the issue by introducing a tensor-based quantum circuit encoding, restrictions on environment dynamics to explore the search space of possible circuits efficiently, an episode halting scheme to steer the agent to find shorter circuits, a double deep Q-network (DDQN) with an $\\epsilon$-greedy policy for better stability. The numerical experiments on noiseless and noisy quantum hardware show that in dealing with various VQAs, our RL-based QAS outperforms existing QAS. Meanwhile, the methods we propose in the thesis can be readily adapted to address a wide range of other VQAs.","sentences":["A significant hurdle in the noisy intermediate-scale quantum (NISQ) era is identifying functional quantum circuits.","These circuits must also adhere to the constraints imposed by current quantum hardware limitations.","Variational quantum algorithms (VQAs), a class of quantum-classical optimization algorithms, were developed to address these challenges in the currently available quantum devices.","However, the overall performance of VQAs depends on the initialization strategy of the variational circuit, the structure of the circuit (also known as ansatz), and the configuration of the cost function.","Focusing on the structure of the circuit, in this thesis, we improve the performance of VQAs by automating the search for an optimal structure for the variational circuits using reinforcement learning (RL).","Within the thesis, the optimality of a circuit is determined by evaluating its depth, the overall count of gates and parameters, and its accuracy in solving the given problem.","The task of automating the search for optimal quantum circuits is known as quantum architecture search (QAS).","The majority of research in QAS is primarily focused on a noiseless scenario.","Yet, the impact of noise on the QAS remains inadequately explored.","In this thesis, we tackle the issue by introducing a tensor-based quantum circuit encoding, restrictions on environment dynamics to explore the search space of possible circuits efficiently, an episode halting scheme to steer the agent to find shorter circuits, a double deep Q-network (DDQN) with an $\\epsilon$-greedy policy for better stability.","The numerical experiments on noiseless and noisy quantum hardware show that in dealing with various VQAs, our RL-based QAS outperforms existing QAS.","Meanwhile, the methods we propose in the thesis can be readily adapted to address a wide range of other VQAs."],"url":"http://arxiv.org/abs/2402.13754v1","category":"quant-ph"}
{"created":"2024-02-21 12:23:09","title":"AI-Powered Predictions for Electricity Load in Prosumer Communities","abstract":"The flexibility in electricity consumption and production in communities of residential buildings, including those with renewable energy sources and energy storage (a.k.a., prosumers), can effectively be utilized through the advancement of short-term demand response mechanisms. It is known that flexibility can further be increased if demand response is performed at the level of communities of prosumers, since aggregated groups can better coordinate electricity consumption. However, the effectiveness of such short-term optimization is highly dependent on the accuracy of electricity load forecasts both for each building as well as for the whole community. Structural variations in the electricity load profile can be associated with different exogenous factors, such as weather conditions, calendar information and day of the week, as well as user behavior. In this paper, we review a wide range of electricity load forecasting techniques, that can provide significant assistance in optimizing load consumption in prosumer communities. We present and test artificial intelligence (AI) powered short-term load forecasting methodologies that operate with black-box time series models, such as Facebook's Prophet and Long Short-term Memory (LSTM) models; season-based SARIMA and smoothing Holt-Winters models; and empirical regression-based models that utilize domain knowledge. The integration of weather forecasts into data-driven time series forecasts is also tested. Results show that the combination of persistent and regression terms (adapted to the load forecasting task) achieves the best forecast accuracy.","sentences":["The flexibility in electricity consumption and production in communities of residential buildings, including those with renewable energy sources and energy storage (a.k.a., prosumers), can effectively be utilized through the advancement of short-term demand response mechanisms.","It is known that flexibility can further be increased if demand response is performed at the level of communities of prosumers, since aggregated groups can better coordinate electricity consumption.","However, the effectiveness of such short-term optimization is highly dependent on the accuracy of electricity load forecasts both for each building as well as for the whole community.","Structural variations in the electricity load profile can be associated with different exogenous factors, such as weather conditions, calendar information and day of the week, as well as user behavior.","In this paper, we review a wide range of electricity load forecasting techniques, that can provide significant assistance in optimizing load consumption in prosumer communities.","We present and test artificial intelligence (AI) powered short-term load forecasting methodologies that operate with black-box time series models, such as Facebook's Prophet and Long Short-term Memory (LSTM) models; season-based SARIMA and smoothing Holt-Winters models; and empirical regression-based models that utilize domain knowledge.","The integration of weather forecasts into data-driven time series forecasts is also tested.","Results show that the combination of persistent and regression terms (adapted to the load forecasting task) achieves the best forecast accuracy."],"url":"http://arxiv.org/abs/2402.13752v1","category":"cs.LG"}
{"created":"2024-02-21 12:22:01","title":"Breaking the Barrier: Utilizing Large Language Models for Industrial Recommendation Systems through an Inferential Knowledge Graph","abstract":"Recommendation systems are widely used in e-commerce websites and online platforms to address information overload. However, existing systems primarily rely on historical data and user feedback, making it difficult to capture user intent transitions. Recently, Knowledge Base (KB)-based models are proposed to incorporate expert knowledge, but it struggle to adapt to new items and the evolving e-commerce environment. To address these challenges, we propose a novel Large Language Model based Complementary Knowledge Enhanced Recommendation System (LLM-KERec). It introduces an entity extractor that extracts unified concept terms from item and user information. To provide cost-effective and reliable prior knowledge, entity pairs are generated based on entity popularity and specific strategies. The large language model determines complementary relationships in each entity pair, constructing a complementary knowledge graph. Furthermore, a new complementary recall module and an Entity-Entity-Item (E-E-I) weight decision model refine the scoring of the ranking model using real complementary exposure-click samples. Extensive experiments conducted on three industry datasets demonstrate the significant performance improvement of our model compared to existing approaches. Additionally, detailed analysis shows that LLM-KERec enhances users' enthusiasm for consumption by recommending complementary items. In summary, LLM-KERec addresses the limitations of traditional recommendation systems by incorporating complementary knowledge and utilizing a large language model to capture user intent transitions, adapt to new items, and enhance recommendation efficiency in the evolving e-commerce landscape.","sentences":["Recommendation systems are widely used in e-commerce websites and online platforms to address information overload.","However, existing systems primarily rely on historical data and user feedback, making it difficult to capture user intent transitions.","Recently, Knowledge Base (KB)-based models are proposed to incorporate expert knowledge, but it struggle to adapt to new items and the evolving e-commerce environment.","To address these challenges, we propose a novel Large Language Model based Complementary Knowledge Enhanced Recommendation System (LLM-KERec).","It introduces an entity extractor that extracts unified concept terms from item and user information.","To provide cost-effective and reliable prior knowledge, entity pairs are generated based on entity popularity and specific strategies.","The large language model determines complementary relationships in each entity pair, constructing a complementary knowledge graph.","Furthermore, a new complementary recall module and an Entity-Entity-Item (E-E-I) weight decision model refine the scoring of the ranking model using real complementary exposure-click samples.","Extensive experiments conducted on three industry datasets demonstrate the significant performance improvement of our model compared to existing approaches.","Additionally, detailed analysis shows that LLM-KERec enhances users' enthusiasm for consumption by recommending complementary items.","In summary, LLM-KERec addresses the limitations of traditional recommendation systems by incorporating complementary knowledge and utilizing a large language model to capture user intent transitions, adapt to new items, and enhance recommendation efficiency in the evolving e-commerce landscape."],"url":"http://arxiv.org/abs/2402.13750v1","category":"cs.IR"}
{"created":"2024-02-21 12:17:33","title":"A Unified Knowledge Graph to Permit Interoperability of Heterogeneous Digital Evidence","abstract":"The modern digital world is highly heterogeneous, encompassing a wide variety of communications, devices, and services. This interconnectedness generates, synchronises, stores, and presents digital information in multidimensional, complex formats, often fragmented across multiple sources. When linked to misuse, this digital information becomes vital digital evidence. Integrating and harmonising these diverse formats into a unified system is crucial for comprehensively understanding evidence and its relationships. However, existing approaches to date have faced challenges limiting investigators' ability to query heterogeneous evidence across large datasets. This paper presents a novel approach in the form of a modern unified data graph. The proposed approach aims to seamlessly integrate, harmonise, and unify evidence data, enabling cross-platform interoperability, efficient data queries, and improved digital investigation performance. To demonstrate its efficacy, a case study is conducted, highlighting the benefits of the proposed approach and showcasing its effectiveness in enabling the interoperability required for advanced analytics in digital investigations.","sentences":["The modern digital world is highly heterogeneous, encompassing a wide variety of communications, devices, and services.","This interconnectedness generates, synchronises, stores, and presents digital information in multidimensional, complex formats, often fragmented across multiple sources.","When linked to misuse, this digital information becomes vital digital evidence.","Integrating and harmonising these diverse formats into a unified system is crucial for comprehensively understanding evidence and its relationships.","However, existing approaches to date have faced challenges limiting investigators' ability to query heterogeneous evidence across large datasets.","This paper presents a novel approach in the form of a modern unified data graph.","The proposed approach aims to seamlessly integrate, harmonise, and unify evidence data, enabling cross-platform interoperability, efficient data queries, and improved digital investigation performance.","To demonstrate its efficacy, a case study is conducted, highlighting the benefits of the proposed approach and showcasing its effectiveness in enabling the interoperability required for advanced analytics in digital investigations."],"url":"http://arxiv.org/abs/2402.13746v1","category":"cs.CR"}
{"created":"2024-02-21 12:17:16","title":"Infrared Imaging using thermally stable HgTe/CdS nanocrystals","abstract":"Transferring the nanocrystals (NCs) from the laboratory environment toward practical applications has raised new challenges. In the case of NCs for display and lightning, the focus was on reduced Auger recombination and maintaining luminescence at high temperatures. When it comes to infrared sensing, narrow band gap materials are required and HgTe appears as the most spectrally tunable platform. Its low-temperature synthesis reduces the growth energy cost yet also favors sintering. As a result, once coupled to a read-out circuit, the Joule effect aggregates the particles leading to a poorly defined optical edge and dramatically large dark current. Here, we demonstrate that CdS shells bring the expected thermal stability (no redshift upon annealing, reduced tendency to form amalgams and preservation of photoconduction after an atomic layer deposition process). The peculiar electronic structure of these confined particles is unveiled using k.p self-consistent simulations showing a significant exciton biding energy at around 200 meV. After shelling, the material displays a p-type behavior that favors the generation of photoconductive gain. The latter is then used to increase the external quantum","sentences":["Transferring the nanocrystals (NCs) from the laboratory environment toward practical applications has raised new challenges.","In the case of NCs for display and lightning, the focus was on reduced Auger recombination and maintaining luminescence at high temperatures.","When it comes to infrared sensing, narrow band gap materials are required and HgTe appears as the most spectrally tunable platform.","Its low-temperature synthesis reduces the growth energy cost yet also favors sintering.","As a result, once coupled to a read-out circuit, the Joule effect aggregates the particles leading to a poorly defined optical edge and dramatically large dark current.","Here, we demonstrate that CdS shells bring the expected thermal stability (no redshift upon annealing, reduced tendency to form amalgams and preservation of photoconduction after an atomic layer deposition process).","The peculiar electronic structure of these confined particles is unveiled using k.p self-consistent simulations showing a significant exciton biding energy at around 200 meV. After shelling, the material displays a p-type behavior that favors the generation of photoconductive gain.","The latter is then used to increase the external quantum"],"url":"http://arxiv.org/abs/2402.13745v1","category":"physics.app-ph"}
{"created":"2024-02-21 12:16:51","title":"Reasoning Algorithmically in Graph Neural Networks","abstract":"The development of artificial intelligence systems with advanced reasoning capabilities represents a persistent and long-standing research question. Traditionally, the primary strategy to address this challenge involved the adoption of symbolic approaches, where knowledge was explicitly represented by means of symbols and explicitly programmed rules. However, with the advent of machine learning, there has been a paradigm shift towards systems that can autonomously learn from data, requiring minimal human guidance. In light of this shift, in latest years, there has been increasing interest and efforts at endowing neural networks with the ability to reason, bridging the gap between data-driven learning and logical reasoning. Within this context, Neural Algorithmic Reasoning (NAR) stands out as a promising research field, aiming to integrate the structured and rule-based reasoning of algorithms with the adaptive learning capabilities of neural networks, typically by tasking neural models to mimic classical algorithms. In this dissertation, we provide theoretical and practical contributions to this area of research. We explore the connections between neural networks and tropical algebra, deriving powerful architectures that are aligned with algorithm execution. Furthermore, we discuss and show the ability of such neural reasoners to learn and manipulate complex algorithmic and combinatorial optimization concepts, such as the principle of strong duality. Finally, in our empirical efforts, we validate the real-world utility of NAR networks across different practical scenarios. This includes tasks as diverse as planning problems, large-scale edge classification tasks and the learning of polynomial-time approximate algorithms for NP-hard combinatorial problems. Through this exploration, we aim to showcase the potential integrating algorithmic reasoning in machine learning models.","sentences":["The development of artificial intelligence systems with advanced reasoning capabilities represents a persistent and long-standing research question.","Traditionally, the primary strategy to address this challenge involved the adoption of symbolic approaches, where knowledge was explicitly represented by means of symbols and explicitly programmed rules.","However, with the advent of machine learning, there has been a paradigm shift towards systems that can autonomously learn from data, requiring minimal human guidance.","In light of this shift, in latest years, there has been increasing interest and efforts at endowing neural networks with the ability to reason, bridging the gap between data-driven learning and logical reasoning.","Within this context, Neural Algorithmic Reasoning (NAR) stands out as a promising research field, aiming to integrate the structured and rule-based reasoning of algorithms with the adaptive learning capabilities of neural networks, typically by tasking neural models to mimic classical algorithms.","In this dissertation, we provide theoretical and practical contributions to this area of research.","We explore the connections between neural networks and tropical algebra, deriving powerful architectures that are aligned with algorithm execution.","Furthermore, we discuss and show the ability of such neural reasoners to learn and manipulate complex algorithmic and combinatorial optimization concepts, such as the principle of strong duality.","Finally, in our empirical efforts, we validate the real-world utility of NAR networks across different practical scenarios.","This includes tasks as diverse as planning problems, large-scale edge classification tasks and the learning of polynomial-time approximate algorithms for NP-hard combinatorial problems.","Through this exploration, we aim to showcase the potential integrating algorithmic reasoning in machine learning models."],"url":"http://arxiv.org/abs/2402.13744v1","category":"cs.LG"}
{"created":"2024-02-21 12:12:22","title":"$q$-Equilibrium of Gas in Spacetime of Multi-horizon Black Holes","abstract":"We investigate the possibility of describing the thermal system with different temperatures for a black hole with multiple horizons. The black hole with two horizons such as Schwarzschild-de Sitter black hole corresponds to two thermal systems with generically different temperatures. Then, it is not suitable to describe these systems with equilibrium thermodynamics corresponding to Gibbs-Boltzmann kinetic theory. In the present work, we investigate such thermal systems by using hydrostatic equilibrium thermodynamics. Assuming that the gas between the horizons obeys the Tsallis statistical mechanics, we found that it is possible to obtain the temperature gradient for the classical gas. Interestingly, the gas behaves as classical gas near the horizon and behaves like quantum gas around flat spacetime with constant temperature. As a result, the multi-horizon black holes in hydrostatic equilibrium can be in a stable configuration with the aspect of the $q$-kinetic theory.","sentences":["We investigate the possibility of describing the thermal system with different temperatures for a black hole with multiple horizons.","The black hole with two horizons such as Schwarzschild-de Sitter black hole corresponds to two thermal systems with generically different temperatures.","Then, it is not suitable to describe these systems with equilibrium thermodynamics corresponding to Gibbs-Boltzmann kinetic theory.","In the present work, we investigate such thermal systems by using hydrostatic equilibrium thermodynamics.","Assuming that the gas between the horizons obeys the Tsallis statistical mechanics, we found that it is possible to obtain the temperature gradient for the classical gas.","Interestingly, the gas behaves as classical gas near the horizon and behaves like quantum gas around flat spacetime with constant temperature.","As a result, the multi-horizon black holes in hydrostatic equilibrium can be in a stable configuration with the aspect of the $q$-kinetic theory."],"url":"http://arxiv.org/abs/2402.13742v1","category":"gr-qc"}
{"created":"2024-02-21 12:12:16","title":"Unlocking Instructive In-Context Learning with Tabular Prompting for Relational Triple Extraction","abstract":"The in-context learning (ICL) for relational triple extraction (RTE) has achieved promising performance, but still encounters two key challenges: (1) how to design effective prompts and (2) how to select proper demonstrations. Existing methods, however, fail to address these challenges appropriately. On the one hand, they usually recast RTE task to text-to-text prompting formats, which is unnatural and results in a mismatch between the output format at the pre-training time and the inference time for large language models (LLMs). On the other hand, they only utilize surface natural language features and lack consideration of triple semantics in sample selection. These issues are blocking improved performance in ICL for RTE, thus we aim to tackle prompt designing and sample selection challenges simultaneously. To this end, we devise a tabular prompting for RTE (\\textsc{TableIE}) which frames RTE task into a table generation task to incorporate explicit structured information into ICL, facilitating conversion of outputs to RTE structures. Then we propose instructive in-context learning (I$^2$CL) which only selects and annotates a few samples considering internal triple semantics in massive unlabeled samples.","sentences":["The in-context learning (ICL) for relational triple extraction (RTE) has achieved promising performance, but still encounters two key challenges: (1) how to design effective prompts and (2) how to select proper demonstrations.","Existing methods, however, fail to address these challenges appropriately.","On the one hand, they usually recast RTE task to text-to-text prompting formats, which is unnatural and results in a mismatch between the output format at the pre-training time and the inference time for large language models (LLMs).","On the other hand, they only utilize surface natural language features and lack consideration of triple semantics in sample selection.","These issues are blocking improved performance in ICL for RTE, thus we aim to tackle prompt designing and sample selection challenges simultaneously.","To this end, we devise a tabular prompting for RTE (\\textsc{TableIE}) which frames RTE task into a table generation task to incorporate explicit structured information into ICL, facilitating conversion of outputs to RTE structures.","Then we propose instructive in-context learning (I$^2$CL) which only selects and annotates a few samples considering internal triple semantics in massive unlabeled samples."],"url":"http://arxiv.org/abs/2402.13741v1","category":"cs.CL"}
{"created":"2024-02-21 12:11:28","title":"From Text to CQL: Bridging Natural Language and Corpus Search Engine","abstract":"Natural Language Processing (NLP) technologies have revolutionized the way we interact with information systems, with a significant focus on converting natural language queries into formal query languages such as SQL. However, less emphasis has been placed on the Corpus Query Language (CQL), a critical tool for linguistic research and detailed analysis within text corpora. The manual construction of CQL queries is a complex and time-intensive task that requires a great deal of expertise, which presents a notable challenge for both researchers and practitioners. This paper presents the first text-to-CQL task that aims to automate the translation of natural language into CQL. We present a comprehensive framework for this task, including a specifically curated large-scale dataset and methodologies leveraging large language models (LLMs) for effective text-to-CQL task. In addition, we established advanced evaluation metrics to assess the syntactic and semantic accuracy of the generated queries. We created innovative LLM-based conversion approaches and detailed experiments. The results demonstrate the efficacy of our methods and provide insights into the complexities of text-to-CQL task.","sentences":["Natural Language Processing (NLP) technologies have revolutionized the way we interact with information systems, with a significant focus on converting natural language queries into formal query languages such as SQL.","However, less emphasis has been placed on the Corpus Query Language (CQL), a critical tool for linguistic research and detailed analysis within text corpora.","The manual construction of CQL queries is a complex and time-intensive task that requires a great deal of expertise, which presents a notable challenge for both researchers and practitioners.","This paper presents the first text-to-CQL task that aims to automate the translation of natural language into CQL.","We present a comprehensive framework for this task, including a specifically curated large-scale dataset and methodologies leveraging large language models (LLMs) for effective text-to-CQL task.","In addition, we established advanced evaluation metrics to assess the syntactic and semantic accuracy of the generated queries.","We created innovative LLM-based conversion approaches and detailed experiments.","The results demonstrate the efficacy of our methods and provide insights into the complexities of text-to-CQL task."],"url":"http://arxiv.org/abs/2402.13740v1","category":"cs.CL"}
{"created":"2024-02-21 12:06:06","title":"SRNDiff: Short-term Rainfall Nowcasting with Condition Diffusion Model","abstract":"Diffusion models are widely used in image generation because they can generate high-quality and realistic samples. This is in contrast to generative adversarial networks (GANs) and variational autoencoders (VAEs), which have some limitations in terms of image quality.We introduce the diffusion model to the precipitation forecasting task and propose a short-term precipitation nowcasting with condition diffusion model based on historical observational data, which is referred to as SRNDiff. By incorporating an additional conditional decoder module in the denoising process, SRNDiff achieves end-to-end conditional rainfall prediction. SRNDiff is composed of two networks: a denoising network and a conditional Encoder network. The conditional network is composed of multiple independent UNet networks. These networks extract conditional feature maps at different resolutions, providing accurate conditional information that guides the diffusion model for conditional generation.SRNDiff surpasses GANs in terms of prediction accuracy, although it requires more computational resources.The SRNDiff model exhibits higher stability and efficiency during training than GANs-based approaches, and generates high-quality precipitation distribution samples that better reflect future actual precipitation conditions. This fully validates the advantages and potential of diffusion models in precipitation forecasting, providing new insights for enhancing rainfall prediction.","sentences":["Diffusion models are widely used in image generation because they can generate high-quality and realistic samples.","This is in contrast to generative adversarial networks (GANs) and variational autoencoders (VAEs), which have some limitations in terms of image quality.","We introduce the diffusion model to the precipitation forecasting task and propose a short-term precipitation nowcasting with condition diffusion model based on historical observational data, which is referred to as SRNDiff.","By incorporating an additional conditional decoder module in the denoising process, SRNDiff achieves end-to-end conditional rainfall prediction.","SRNDiff is composed of two networks: a denoising network and a conditional Encoder network.","The conditional network is composed of multiple independent UNet networks.","These networks extract conditional feature maps at different resolutions, providing accurate conditional information that guides the diffusion model for conditional generation.","SRNDiff surpasses GANs in terms of prediction accuracy, although it requires more computational resources.","The SRNDiff model exhibits higher stability and efficiency during training than GANs-based approaches, and generates high-quality precipitation distribution samples that better reflect future actual precipitation conditions.","This fully validates the advantages and potential of diffusion models in precipitation forecasting, providing new insights for enhancing rainfall prediction."],"url":"http://arxiv.org/abs/2402.13737v1","category":"cs.CV"}
{"created":"2024-02-21 12:01:37","title":"Penetrative magneto-convection of a rotating Boussinesq flow in $f$-planes","abstract":"In this study, we conducted a linear instability analysis of penetrative magneto-convection in rapidly rotating Boussinesq flows within tilted f-planes, under the influence of a uniform background magnetic field. We integrated wave theory and convection theory to elucidate the penetration dynamics in rotating magneto-convection. Our findings suggest that efficient penetration in rapidly rotating flows with weakly stratified stable layers at low latitudes can be attributed to the resonance of wave transmission near the interface between unstable and stable layers. In the context of strongly stratified flows, we derived the scaling relationships of penetrative distances $\\Delta$ with the stability parameter $\\delta$. Our calculation shows that, for both rotation-dominated and magnetism-dominated flows, $\\Delta$ obeys a scaling of $\\Delta\\sim O(\\delta^{-1/2})$. In rotation-dominated flows, we noted a general decrease in penetrative distance with increased rotational effect, and a minor decrease in penetrative distance with increased latitude. When a background magnetic field is introduced, we observed a significant shift in penetrative distance as the Elsasser number $\\Lambda$ approaches one. The penetrative distance tends to decrease when $\\Lambda \\ll 1$ and increase when $\\Lambda \\gg 1$ with the rotational effect, indicating a transition from rotation-dominated to magnetism-dominated flow. We have further investigated the impact of the background magnetic field when it is not aligned with the rotational axis. This presents a notable contrast to the case where the magnetic field is parallel to the rotational axis.","sentences":["In this study, we conducted a linear instability analysis of penetrative magneto-convection in rapidly rotating Boussinesq flows within tilted f-planes, under the influence of a uniform background magnetic field.","We integrated wave theory and convection theory to elucidate the penetration dynamics in rotating magneto-convection.","Our findings suggest that efficient penetration in rapidly rotating flows with weakly stratified stable layers at low latitudes can be attributed to the resonance of wave transmission near the interface between unstable and stable layers.","In the context of strongly stratified flows, we derived the scaling relationships of penetrative distances $\\Delta$ with the stability parameter $\\delta$.","Our calculation shows that, for both rotation-dominated and magnetism-dominated flows, $\\Delta$ obeys a scaling of $\\Delta\\sim O(\\delta^{-1/2})$.","In rotation-dominated flows, we noted a general decrease in penetrative distance with increased rotational effect, and a minor decrease in penetrative distance with increased latitude.","When a background magnetic field is introduced, we observed a significant shift in penetrative distance as the Elsasser number $\\Lambda$ approaches one.","The penetrative distance tends to decrease when $\\Lambda \\ll 1$ and increase when $\\Lambda \\gg 1$ with the rotational effect, indicating a transition from rotation-dominated to magnetism-dominated flow.","We have further investigated the impact of the background magnetic field when it is not aligned with the rotational axis.","This presents a notable contrast to the case where the magnetic field is parallel to the rotational axis."],"url":"http://arxiv.org/abs/2402.13736v1","category":"physics.flu-dyn"}
{"created":"2024-02-21 11:51:47","title":"On optimal error rates for strong approximation of SDEs with a drift coefficient of fractional Sobolev regularity","abstract":"We study strong approximation of scalar additive noise driven stochastic differential equations (SDEs) at time point $1$ in the case that the drift coefficient is bounded and has Sobolev regularity $s\\in(0,1)$. Recently, it has been shown in [arXiv:2101.12185v2 (2022)] that for such SDEs the equidistant Euler approximation achieves an $L^2$-error rate of at least $(1+s)/2$, up to an arbitrary small $\\varepsilon$, in terms of the number of evaluations of the driving Brownian motion $W$. In the present article we prove a matching lower error bound for $s\\in(1/2,1)$. More precisely we show that, for every $s\\in(1/2,1)$, the $L^2$-error rate $(1+s)/2$ can, up to a logarithmic term, not be improved in general by no numerical method based on finitely many evaluations of $W$ at fixed time points. Up to now, this result was known in the literature only for the cases $s=1/2-$ and $s=1-$.   For the proof we employ the coupling of noise technique recently introduced in [arXiv:2010.00915 (2020)] to bound the $L^2$-error of an arbitrary approximation from below by the $L^2$-distance of two occupation time functionals provided by a specifically chosen drift coefficient with Sobolev regularity $s$ and two solutions of the corresponding SDE with coupled driving Brownian motions. For the analysis of the latter distance we employ a transformation of the original SDE to overcome the problem of correlated increments of the difference of the two coupled solutions, occupation time estimates to cope with the lack of regularity of the chosen drift coefficient around the point $0$ and scaling properties of the drift coefficient.","sentences":["We study strong approximation of scalar additive noise driven stochastic differential equations (SDEs) at time point $1$ in the case that the drift coefficient is bounded and has Sobolev regularity $s\\in(0,1)$. Recently, it has been shown in [arXiv:2101.12185v2 (2022)] that for such SDEs the equidistant Euler approximation achieves an $L^2$-error rate of at least $(1+s)/2$, up to an arbitrary small $\\varepsilon$, in terms of the number of evaluations of the driving Brownian motion $W$. In the present article we prove a matching lower error bound for $s\\in(1/2,1)$. More precisely we show that, for every $s\\in(1/2,1)$, the $L^2$-error rate $(1+s)/2$ can, up to a logarithmic term, not be improved in general by no numerical method based on finitely many evaluations of $W$ at fixed time points.","Up to now, this result was known in the literature only for the cases $s=1/2-$ and $s=1-$.   For the proof we employ the coupling of noise technique recently introduced in [arXiv:2010.00915 (2020)] to bound the $L^2$-error of an arbitrary approximation from below by the $L^2$-distance of two occupation time functionals provided by a specifically chosen drift coefficient with Sobolev regularity $s$ and two solutions of the corresponding SDE with coupled driving Brownian motions.","For the analysis of the latter distance we employ a transformation of the original SDE to overcome the problem of correlated increments of the difference of the two coupled solutions, occupation time estimates to cope with the lack of regularity of the chosen drift coefficient around the point $0$ and scaling properties of the drift coefficient."],"url":"http://arxiv.org/abs/2402.13732v1","category":"math.PR"}
{"created":"2024-02-21 11:50:32","title":"The Da Vinci Code of Large Pre-trained Language Models: Deciphering Degenerate Knowledge Neurons","abstract":"This study explores the mechanism of factual knowledge storage in pre-trained language models (PLMs). Previous research suggests that factual knowledge is stored within multi-layer perceptron weights, and some storage units exhibit degeneracy, referred to as Degenerate Knowledge Neurons (DKNs). This paper provides a comprehensive definition of DKNs that covers both structural and functional aspects, pioneering the study of structures in PLMs' factual knowledge storage units. Based on this, we introduce the Neurological Topology Clustering method, which allows the formation of DKNs in any numbers and structures, leading to a more accurate DKN acquisition. Furthermore, we introduce the Neuro-Degeneracy Analytic Analysis Framework, which uniquely integrates model robustness, evolvability, and complexity for a holistic assessment of PLMs. Within this framework, our execution of 34 experiments across 2 PLMs, 4 datasets, and 6 settings highlights the critical role of DKNs. The code will be available soon.","sentences":["This study explores the mechanism of factual knowledge storage in pre-trained language models (PLMs).","Previous research suggests that factual knowledge is stored within multi-layer perceptron weights, and some storage units exhibit degeneracy, referred to as Degenerate Knowledge Neurons (DKNs).","This paper provides a comprehensive definition of DKNs that covers both structural and functional aspects, pioneering the study of structures in PLMs' factual knowledge storage units.","Based on this, we introduce the Neurological Topology Clustering method, which allows the formation of DKNs in any numbers and structures, leading to a more accurate DKN acquisition.","Furthermore, we introduce the Neuro-Degeneracy Analytic Analysis Framework, which uniquely integrates model robustness, evolvability, and complexity for a holistic assessment of PLMs.","Within this framework, our execution of 34 experiments across 2 PLMs, 4 datasets, and 6 settings highlights the critical role of DKNs.","The code will be available soon."],"url":"http://arxiv.org/abs/2402.13731v1","category":"cs.CL"}
{"created":"2024-02-21 11:46:16","title":"Hybrid Video Diffusion Models with 2D Triplane and 3D Wavelet Representation","abstract":"Generating high-quality videos that synthesize desired realistic content is a challenging task due to their intricate high-dimensionality and complexity of videos. Several recent diffusion-based methods have shown comparable performance by compressing videos to a lower-dimensional latent space, using traditional video autoencoder architecture. However, such method that employ standard frame-wise 2D and 3D convolution fail to fully exploit the spatio-temporal nature of videos. To address this issue, we propose a novel hybrid video diffusion model, called HVDM, which can capture spatio-temporal dependencies more effectively. The HVDM is trained by a hybrid video autoencoder which extracts a disentangled representation of the video including: (i) a global context information captured by a 2D projected latent (ii) a local volume information captured by 3D convolutions with wavelet decomposition (iii) a frequency information for improving the video reconstruction. Based on this disentangled representation, our hybrid autoencoder provide a more comprehensive video latent enriching the generated videos with fine structures and details. Experiments on video generation benchamarks (UCF101, SkyTimelapse, and TaiChi) demonstrate that the proposed approach achieves state-of-the-art video generation quality, showing a wide range of video applications (e.g., long video generation, image-to-video, and video dynamics control).","sentences":["Generating high-quality videos that synthesize desired realistic content is a challenging task due to their intricate high-dimensionality and complexity of videos.","Several recent diffusion-based methods have shown comparable performance by compressing videos to a lower-dimensional latent space, using traditional video autoencoder architecture.","However, such method that employ standard frame-wise 2D and 3D convolution fail to fully exploit the spatio-temporal nature of videos.","To address this issue, we propose a novel hybrid video diffusion model, called HVDM, which can capture spatio-temporal dependencies more effectively.","The HVDM is trained by a hybrid video autoencoder which extracts a disentangled representation of the video including: (i) a global context information captured by a 2D projected latent (ii) a local volume information captured by 3D convolutions with wavelet decomposition (iii) a frequency information for improving the video reconstruction.","Based on this disentangled representation, our hybrid autoencoder provide a more comprehensive video latent enriching the generated videos with fine structures and details.","Experiments on video generation benchamarks (UCF101, SkyTimelapse, and TaiChi) demonstrate that the proposed approach achieves state-of-the-art video generation quality, showing a wide range of video applications (e.g., long video generation, image-to-video, and video dynamics control)."],"url":"http://arxiv.org/abs/2402.13729v1","category":"cs.CV"}
{"created":"2024-02-21 11:35:20","title":"Bring Your Own Character: A Holistic Solution for Automatic Facial Animation Generation of Customized Characters","abstract":"Animating virtual characters has always been a fundamental research problem in virtual reality (VR). Facial animations play a crucial role as they effectively convey emotions and attitudes of virtual humans. However, creating such facial animations can be challenging, as current methods often involve utilization of expensive motion capture devices or significant investments of time and effort from human animators in tuning animation parameters. In this paper, we propose a holistic solution to automatically animate virtual human faces. In our solution, a deep learning model was first trained to retarget the facial expression from input face images to virtual human faces by estimating the blendshape coefficients. This method offers the flexibility of generating animations with characters of different appearances and blendshape topologies. Second, a practical toolkit was developed using Unity 3D, making it compatible with the most popular VR applications. The toolkit accepts both image and video as input to animate the target virtual human faces and enables users to manipulate the animation results. Furthermore, inspired by the spirit of Human-in-the-loop (HITL), we leveraged user feedback to further improve the performance of the model and toolkit, thereby increasing the customization properties to suit user preferences. The whole solution, for which we will make the code public, has the potential to accelerate the generation of facial animations for use in VR applications.","sentences":["Animating virtual characters has always been a fundamental research problem in virtual reality (VR).","Facial animations play a crucial role as they effectively convey emotions and attitudes of virtual humans.","However, creating such facial animations can be challenging, as current methods often involve utilization of expensive motion capture devices or significant investments of time and effort from human animators in tuning animation parameters.","In this paper, we propose a holistic solution to automatically animate virtual human faces.","In our solution, a deep learning model was first trained to retarget the facial expression from input face images to virtual human faces by estimating the blendshape coefficients.","This method offers the flexibility of generating animations with characters of different appearances and blendshape topologies.","Second, a practical toolkit was developed using Unity 3D, making it compatible with the most popular VR applications.","The toolkit accepts both image and video as input to animate the target virtual human faces and enables users to manipulate the animation results.","Furthermore, inspired by the spirit of Human-in-the-loop (HITL), we leveraged user feedback to further improve the performance of the model and toolkit, thereby increasing the customization properties to suit user preferences.","The whole solution, for which we will make the code public, has the potential to accelerate the generation of facial animations for use in VR applications."],"url":"http://arxiv.org/abs/2402.13724v1","category":"cs.HC"}
{"created":"2024-02-21 11:31:28","title":"Ouroboros: Speculative Decoding with Large Model Enhanced Drafting","abstract":"Drafting-then-verifying decoding methods such as speculative decoding are widely adopted training-free methods to accelerate the inference of large language models (LLMs). Instead of employing an autoregressive process to decode tokens sequentially, speculative decoding initially creates drafts with an efficient small model. Then LLMs are required to conduct verification and correction in a non-autoregressive fashion to minimize time overhead. Generating longer drafts can lead to even more significant speedups once verified, but also incurs substantial trial and error costs if it fails. Suffering from the high verification failure probability, existing decoding methods cannot draft too much content for verification at one time, achieving sub-optimal inference acceleration. In this paper, we introduce Ouroboros, which constructs a phrase candidate pool from the verification process of LLMs to provide candidates for draft generation of the small model. Thereby, Ouroboros can further improve the efficiency and effectiveness of the initial drafts. The experimental results on typical text generation tasks show that Ouroboros achieves speedups of up to 1.9x and 2.8x compared to lookahead decoding and speculative decoding, respectively. The source code of Ouroboros is available at https://github.com/thunlp/Ouroboros.","sentences":["Drafting-then-verifying decoding methods such as speculative decoding are widely adopted training-free methods to accelerate the inference of large language models (LLMs).","Instead of employing an autoregressive process to decode tokens sequentially, speculative decoding initially creates drafts with an efficient small model.","Then LLMs are required to conduct verification and correction in a non-autoregressive fashion to minimize time overhead.","Generating longer drafts can lead to even more significant speedups once verified, but also incurs substantial trial and error costs if it fails.","Suffering from the high verification failure probability, existing decoding methods cannot draft too much content for verification at one time, achieving sub-optimal inference acceleration.","In this paper, we introduce Ouroboros, which constructs a phrase candidate pool from the verification process of LLMs to provide candidates for draft generation of the small model.","Thereby, Ouroboros can further improve the efficiency and effectiveness of the initial drafts.","The experimental results on typical text generation tasks show that Ouroboros achieves speedups of up to 1.9x and 2.8x compared to lookahead decoding and speculative decoding, respectively.","The source code of Ouroboros is available at https://github.com/thunlp/Ouroboros."],"url":"http://arxiv.org/abs/2402.13720v1","category":"cs.CL"}
{"created":"2024-02-21 11:29:32","title":"Edge-Disjoint Paths in Eulerian Digraphs","abstract":"Disjoint paths problems are among the most prominent problems in combinatorial optimization. The edge- as well as vertex-disjoint paths problem, are NP-complete on directed and undirected graphs. But on undirected graphs, Robertson and Seymour (Graph Minors XIII) developed an algorithm for the vertex- and the edge-disjoint paths problem that runs in cubic time for every fixed number $p$ of terminal pairs, i.e. they proved that the problem is fixed-parameter tractable on undirected graphs. On directed graphs, Fortune, Hopcroft, and Wyllie proved that both problems are NP-complete already for $p=2$ terminal pairs. In this paper, we study the edge-disjoint paths problem (EDPP) on Eulerian digraphs, a problem that has received significant attention in the literature. Marx (Marx 2004) proved that the Eulerian EDPP is NP-complete even on structurally very simple Eulerian digraphs. On the positive side, polynomial time algorithms are known only for very restricted cases, such as $p\\leq 3$ or where the demand graph is a union of two stars (see e.g. Ibaraki, Poljak 1991; Frank 1988; Frank, Ibaraki, Nagamochi 1995).   The question of which values of $p$ the edge-disjoint paths problem can be solved in polynomial time on Eulerian digraphs has already been raised by Frank, Ibaraki, and Nagamochi (1995) almost 30 years ago. But despite considerable effort, the complexity of the problem is still wide open and is considered to be the main open problem in this area (see Chapter 4 of Bang-Jensen, Gutin 2018 for a recent survey). In this paper, we solve this long-open problem by showing that the Edge-Disjoint Paths Problem is fixed-parameter tractable on Eulerian digraphs in general (parameterized by the number of terminal pairs). The algorithm itself is reasonably simple but the proof of its correctness requires a deep structural analysis of Eulerian digraphs.","sentences":["Disjoint paths problems are among the most prominent problems in combinatorial optimization.","The edge- as well as vertex-disjoint paths problem, are NP-complete on directed and undirected graphs.","But on undirected graphs, Robertson and Seymour (Graph Minors XIII) developed an algorithm for the vertex- and the edge-disjoint paths problem that runs in cubic time for every fixed number $p$ of terminal pairs, i.e. they proved that the problem is fixed-parameter tractable on undirected graphs.","On directed graphs, Fortune, Hopcroft, and Wyllie proved that both problems are NP-complete already for $p=2$ terminal pairs.","In this paper, we study the edge-disjoint paths problem (EDPP) on Eulerian digraphs, a problem that has received significant attention in the literature.","Marx (Marx 2004) proved that the Eulerian EDPP is NP-complete even on structurally very simple Eulerian digraphs.","On the positive side, polynomial time algorithms are known only for very restricted cases, such as $p\\leq 3$ or where the demand graph is a union of two stars (see e.g. Ibaraki, Poljak 1991; Frank 1988; Frank, Ibaraki, Nagamochi 1995).   ","The question of which values of $p$ the edge-disjoint paths problem can be solved in polynomial time on Eulerian digraphs has already been raised by Frank, Ibaraki, and Nagamochi (1995) almost 30 years ago.","But despite considerable effort, the complexity of the problem is still wide open and is considered to be the main open problem in this area (see Chapter 4 of Bang-Jensen, Gutin 2018 for a recent survey).","In this paper, we solve this long-open problem by showing that the Edge-Disjoint Paths Problem is fixed-parameter tractable on Eulerian digraphs in general (parameterized by the number of terminal pairs).","The algorithm itself is reasonably simple but the proof of its correctness requires a deep structural analysis of Eulerian digraphs."],"url":"http://arxiv.org/abs/2402.13716v1","category":"cs.CC"}
{"created":"2024-02-21 11:27:31","title":"An Evaluation of Large Language Models in Bioinformatics Research","abstract":"Large language models (LLMs) such as ChatGPT have gained considerable interest across diverse research communities. Their notable ability for text completion and generation has inaugurated a novel paradigm for language-interfaced problem solving. However, the potential and efficacy of these models in bioinformatics remain incompletely explored. In this work, we study the performance LLMs on a wide spectrum of crucial bioinformatics tasks. These tasks include the identification of potential coding regions, extraction of named entities for genes and proteins, detection of antimicrobial and anti-cancer peptides, molecular optimization, and resolution of educational bioinformatics problems. Our findings indicate that, given appropriate prompts, LLMs like GPT variants can successfully handle most of these tasks. In addition, we provide a thorough analysis of their limitations in the context of complicated bioinformatics tasks. In conclusion, we believe that this work can provide new perspectives and motivate future research in the field of LLMs applications, AI for Science and bioinformatics.","sentences":["Large language models (LLMs) such as ChatGPT have gained considerable interest across diverse research communities.","Their notable ability for text completion and generation has inaugurated a novel paradigm for language-interfaced problem solving.","However, the potential and efficacy of these models in bioinformatics remain incompletely explored.","In this work, we study the performance LLMs on a wide spectrum of crucial bioinformatics tasks.","These tasks include the identification of potential coding regions, extraction of named entities for genes and proteins, detection of antimicrobial and anti-cancer peptides, molecular optimization, and resolution of educational bioinformatics problems.","Our findings indicate that, given appropriate prompts, LLMs like GPT variants can successfully handle most of these tasks.","In addition, we provide a thorough analysis of their limitations in the context of complicated bioinformatics tasks.","In conclusion, we believe that this work can provide new perspectives and motivate future research in the field of LLMs applications, AI for Science and bioinformatics."],"url":"http://arxiv.org/abs/2402.13714v1","category":"q-bio.QM"}
{"created":"2024-02-21 11:25:54","title":"DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based Graph Continual Learning","abstract":"We investigate the replay buffer in rehearsal-based approaches for graph continual learning (GCL) methods. Existing rehearsal-based GCL methods select the most representative nodes for each class and store them in a replay buffer for later use in training subsequent tasks. However, we discovered that considering only the class representativeness of each replayed node makes the replayed nodes to be concentrated around the center of each class, incurring a potential risk of overfitting to nodes residing in those regions, which aggravates catastrophic forgetting. Moreover, as the rehearsal-based approach heavily relies on a few replayed nodes to retain knowledge obtained from previous tasks, involving the replayed nodes that have irrelevant neighbors in the model training may have a significant detrimental impact on model performance. In this paper, we propose a GCL model named DSLR, specifically, we devise a coverage-based diversity (CD) approach to consider both the class representativeness and the diversity within each class of the replayed nodes. Moreover, we adopt graph structure learning (GSL) to ensure that the replayed nodes are connected to truly informative neighbors. Extensive experimental results demonstrate the effectiveness and efficiency of DSLR.","sentences":["We investigate the replay buffer in rehearsal-based approaches for graph continual learning (GCL) methods.","Existing rehearsal-based GCL methods select the most representative nodes for each class and store them in a replay buffer for later use in training subsequent tasks.","However, we discovered that considering only the class representativeness of each replayed node makes the replayed nodes to be concentrated around the center of each class, incurring a potential risk of overfitting to nodes residing in those regions, which aggravates catastrophic forgetting.","Moreover, as the rehearsal-based approach heavily relies on a few replayed nodes to retain knowledge obtained from previous tasks, involving the replayed nodes that have irrelevant neighbors in the model training may have a significant detrimental impact on model performance.","In this paper, we propose a GCL model named DSLR, specifically, we devise a coverage-based diversity (CD) approach to consider both the class representativeness and the diversity within each class of the replayed nodes.","Moreover, we adopt graph structure learning (GSL) to ensure that the replayed nodes are connected to truly informative neighbors.","Extensive experimental results demonstrate the effectiveness and efficiency of DSLR."],"url":"http://arxiv.org/abs/2402.13711v1","category":"cs.LG"}
{"created":"2024-02-21 11:23:21","title":"SaGE: Evaluating Moral Consistency in Large Language Models","abstract":"Despite recent advancements showcasing the impressive capabilities of Large Language Models (LLMs) in conversational systems, we show that even state-of-the-art LLMs are morally inconsistent in their generations, questioning their reliability (and trustworthiness in general). Prior works in LLM evaluation focus on developing ground-truth data to measure accuracy on specific tasks. However, for moral scenarios that often lack universally agreed-upon answers, consistency in model responses becomes crucial for their reliability. To address this issue, we propose an information-theoretic measure called Semantic Graph Entropy (SaGE), grounded in the concept of \"Rules of Thumb\" (RoTs) to measure a model's moral consistency. RoTs are abstract principles learned by a model and can help explain their decision-making strategies effectively. To this extent, we construct the Moral Consistency Corpus (MCC), containing 50K moral questions, responses to them by LLMs, and the RoTs that these models followed. Furthermore, to illustrate the generalizability of SaGE, we use it to investigate LLM consistency on two popular datasets -- TruthfulQA and HellaSwag. Our results reveal that task-accuracy and consistency are independent problems, and there is a dire need to investigate these issues further.","sentences":["Despite recent advancements showcasing the impressive capabilities of Large Language Models (LLMs) in conversational systems, we show that even state-of-the-art LLMs are morally inconsistent in their generations, questioning their reliability (and trustworthiness in general).","Prior works in LLM evaluation focus on developing ground-truth data to measure accuracy on specific tasks.","However, for moral scenarios that often lack universally agreed-upon answers, consistency in model responses becomes crucial for their reliability.","To address this issue, we propose an information-theoretic measure called Semantic Graph Entropy (SaGE), grounded in the concept of \"Rules of Thumb\" (RoTs) to measure a model's moral consistency.","RoTs are abstract principles learned by a model and can help explain their decision-making strategies effectively.","To this extent, we construct the Moral Consistency Corpus (MCC), containing 50K moral questions, responses to them by LLMs, and the RoTs that these models followed.","Furthermore, to illustrate the generalizability of SaGE, we use it to investigate LLM consistency on two popular datasets -- TruthfulQA and HellaSwag.","Our results reveal that task-accuracy and consistency are independent problems, and there is a dire need to investigate these issues further."],"url":"http://arxiv.org/abs/2402.13709v1","category":"cs.CL"}
{"created":"2024-02-21 11:20:31","title":"Construction of Yemilab","abstract":"The Center for Underground Physics of the Institute for Basic Science (IBS) in Korea has been planning the construction of a deep underground laboratory since 2013 to search for extremely rare interactions such as dark matter and neutrinos. In September 2022, a new underground laboratory, Yemilab, was finally completed in Jeongseon, Gangwon Province, with a depth of 1,000 m and an exclusive experimental area spanning 3,000 m$^3$. The tunnel is encased in limestone and accommodates 17 independent experimental spaces. Over two years, from 2023 to 2024, the Yangyang Underground Laboratory facilities will be relocated to Yemilab. Preparations are underway for the AMoRE-II, a neutrinoless double beta decay experiment, scheduled to begin in Q2 2024 at Yemilab. Additionally, Yemilab includes a cylindrical pit with a volume of approximately 6,300 m$^3$, designed as a multipurpose laboratory for next-generation experiments involving neutrinos, dark matter, and related research. This article provides a focused overview of the construction and structure of Yemilab.","sentences":["The Center for Underground Physics of the Institute for Basic Science (IBS) in Korea has been planning the construction of a deep underground laboratory since 2013 to search for extremely rare interactions such as dark matter and neutrinos.","In September 2022, a new underground laboratory, Yemilab, was finally completed in Jeongseon, Gangwon Province, with a depth of 1,000 m and an exclusive experimental area spanning 3,000 m$^3$. The tunnel is encased in limestone and accommodates 17 independent experimental spaces.","Over two years, from 2023 to 2024, the Yangyang Underground Laboratory facilities will be relocated to Yemilab.","Preparations are underway for the AMoRE-II, a neutrinoless double beta decay experiment, scheduled to begin in Q2 2024 at Yemilab.","Additionally, Yemilab includes a cylindrical pit with a volume of approximately 6,300 m$^3$, designed as a multipurpose laboratory for next-generation experiments involving neutrinos, dark matter, and related research.","This article provides a focused overview of the construction and structure of Yemilab."],"url":"http://arxiv.org/abs/2402.13708v1","category":"astro-ph.IM"}
{"created":"2024-02-21 11:15:56","title":"Geometric derivation and structure-preserving simulation of quasi-geostrophy on the sphere","abstract":"We present a geometric derivation of the quasi-geostrophic equations on the sphere, starting from the rotating shallow water equations. We utilise perturbation series methods in vorticity and divergence variables. The derivation employs asymptotic analysis techniques, leading to a global quasi-geostrophic potential vorticity model on the sphere without approximation of the Coriolis parameter. The resulting model forms a closed system for the evolution of potential vorticity with a rich mathematical structure, including Lagrangian and Hamiltonian descriptions. Formulated using the Lie-Poisson bracket reveals the geometric invariants of the quasi-geostrophic model. Motivated by these geometric results, simulations of quasi-geostrophic flow on the sphere are presented based on structure-preserving Lie-Poisson time-integration. We explicitly demonstrate the preservation of Casimir invariants and show that the hyperbolic quasi-geostrophic equations can be simulated in a stable manner over long time. We show the emergence of longitudonal jets, wrapped around the circumference of the sphere in a general direction that is perpendicular to the axis of rotation.","sentences":["We present a geometric derivation of the quasi-geostrophic equations on the sphere, starting from the rotating shallow water equations.","We utilise perturbation series methods in vorticity and divergence variables.","The derivation employs asymptotic analysis techniques, leading to a global quasi-geostrophic potential vorticity model on the sphere without approximation of the Coriolis parameter.","The resulting model forms a closed system for the evolution of potential vorticity with a rich mathematical structure, including Lagrangian and Hamiltonian descriptions.","Formulated using the Lie-Poisson bracket reveals the geometric invariants of the quasi-geostrophic model.","Motivated by these geometric results, simulations of quasi-geostrophic flow on the sphere are presented based on structure-preserving Lie-Poisson time-integration.","We explicitly demonstrate the preservation of Casimir invariants and show that the hyperbolic quasi-geostrophic equations can be simulated in a stable manner over long time.","We show the emergence of longitudonal jets, wrapped around the circumference of the sphere in a general direction that is perpendicular to the axis of rotation."],"url":"http://arxiv.org/abs/2402.13707v1","category":"physics.flu-dyn"}
{"created":"2024-02-21 11:07:07","title":"Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?","abstract":"The adaption of multilingual pre-trained Large Language Models (LLMs) into eloquent and helpful assistants is essential to facilitate their use across different language regions. In that spirit, we are the first to conduct an extensive study of the performance of multilingual models on parallel, multi-turn instruction-tuning benchmarks across a selection of the most-spoken Indo-European languages. We systematically examine the effects of language and instruction dataset size on a mid-sized, multilingual LLM by instruction-tuning it on parallel instruction-tuning datasets. Our results demonstrate that instruction-tuning on parallel instead of monolingual corpora benefits cross-lingual instruction following capabilities by up to 4.6%. Furthermore, we show that the Superficial Alignment Hypothesis does not hold in general, as the investigated multilingual 7B parameter model presents a counter-example requiring large-scale instruction-tuning datasets. Finally, we conduct a human annotation study to understand the alignment between human-based and GPT-4-based evaluation within multilingual chat scenarios.","sentences":["The adaption of multilingual pre-trained Large Language Models (LLMs) into eloquent and helpful assistants is essential to facilitate their use across different language regions.","In that spirit, we are the first to conduct an extensive study of the performance of multilingual models on parallel, multi-turn instruction-tuning benchmarks across a selection of the most-spoken Indo-European languages.","We systematically examine the effects of language and instruction dataset size on a mid-sized, multilingual LLM by instruction-tuning it on parallel instruction-tuning datasets.","Our results demonstrate that instruction-tuning on parallel instead of monolingual corpora benefits cross-lingual instruction following capabilities by up to 4.6%.","Furthermore, we show that the Superficial Alignment Hypothesis does not hold in general, as the investigated multilingual 7B parameter model presents a counter-example requiring large-scale instruction-tuning datasets.","Finally, we conduct a human annotation study to understand the alignment between human-based and GPT-4-based evaluation within multilingual chat scenarios."],"url":"http://arxiv.org/abs/2402.13703v1","category":"cs.CL"}
{"created":"2024-02-21 11:04:30","title":"Neural density estimation for Galactic Binaries in LISA data analysis","abstract":"The future space based gravitational wave detector LISA (Laser Interferometer Space Antenna) will observe millions of Galactic binaries constantly present in the data stream. A small fraction of this population (of the order of several thousand) will be individually resolved. One of the challenging tasks from the data analysis point of view will be to estimate the parameters of resolvable galactic binaries while disentangling them from each other and from other gravitational wave sources present in the data. This problem is quite often referred to as a global fit in the field of LISA data analysis. A Bayesian framework is often used to infer the parameters of the sources and their number. The efficiency of the sampling techniques strongly depends on the proposals, especially in the multi-dimensional parameter space. In this paper we demonstrate how we can use neural density estimators, and in particular Normalising flows, in order to build proposals which significantly improve the convergence of sampling. We also demonstrate how these methods could help in building priors based on physical models and provide an alternative way to represent the catalogue of identified gravitational wave sources.","sentences":["The future space based gravitational wave detector LISA (Laser Interferometer Space Antenna) will observe millions of Galactic binaries constantly present in the data stream.","A small fraction of this population (of the order of several thousand) will be individually resolved.","One of the challenging tasks from the data analysis point of view will be to estimate the parameters of resolvable galactic binaries while disentangling them from each other and from other gravitational wave sources present in the data.","This problem is quite often referred to as a global fit in the field of LISA data analysis.","A Bayesian framework is often used to infer the parameters of the sources and their number.","The efficiency of the sampling techniques strongly depends on the proposals, especially in the multi-dimensional parameter space.","In this paper we demonstrate how we can use neural density estimators, and in particular Normalising flows, in order to build proposals which significantly improve the convergence of sampling.","We also demonstrate how these methods could help in building priors based on physical models and provide an alternative way to represent the catalogue of identified gravitational wave sources."],"url":"http://arxiv.org/abs/2402.13701v1","category":"gr-qc"}
{"created":"2024-02-21 11:00:23","title":"Explainable Classification Techniques for Quantum Dot Device Measurements","abstract":"In the physical sciences, there is an increased need for robust feature representations of image data: image acquisition, in the generalized sense of two-dimensional data, is now widespread across a large number of fields, including quantum information science, which we consider here. While traditional image features are widely utilized in such cases, their use is rapidly being supplanted by Neural Network-based techniques that often sacrifice explainability in exchange for high accuracy. To ameliorate this trade-off, we propose a synthetic data-based technique that results in explainable features. We show, using Explainable Boosting Machines (EBMs), that this method offers superior explainability without sacrificing accuracy. Specifically, we show that there is a meaningful benefit to this technique in the context of quantum dot tuning, where human intervention is necessary at the current stage of development.","sentences":["In the physical sciences, there is an increased need for robust feature representations of image data: image acquisition, in the generalized sense of two-dimensional data, is now widespread across a large number of fields, including quantum information science, which we consider here.","While traditional image features are widely utilized in such cases, their use is rapidly being supplanted by Neural Network-based techniques that often sacrifice explainability in exchange for high accuracy.","To ameliorate this trade-off, we propose a synthetic data-based technique that results in explainable features.","We show, using Explainable Boosting Machines (EBMs), that this method offers superior explainability without sacrificing accuracy.","Specifically, we show that there is a meaningful benefit to this technique in the context of quantum dot tuning, where human intervention is necessary at the current stage of development."],"url":"http://arxiv.org/abs/2402.13699v1","category":"cs.CV"}
{"created":"2024-02-21 10:57:21","title":"Generalizable Semantic Vision Query Generation for Zero-shot Panoptic and Semantic Segmentation","abstract":"Zero-shot Panoptic Segmentation (ZPS) aims to recognize foreground instances and background stuff without images containing unseen categories in training. Due to the visual data sparsity and the difficulty of generalizing from seen to unseen categories, this task remains challenging. To better generalize to unseen classes, we propose Conditional tOken aligNment and Cycle trAnsiTion (CONCAT), to produce generalizable semantic vision queries. First, a feature extractor is trained by CON to link the vision and semantics for providing target queries. Formally, CON is proposed to align the semantic queries with the CLIP visual CLS token extracted from complete and masked images. To address the lack of unseen categories, a generator is required. However, one of the gaps in synthesizing pseudo vision queries, ie, vision queries for unseen categories, is describing fine-grained visual details through semantic embeddings. Therefore, we approach CAT to train the generator in semantic-vision and vision-semantic manners. In semantic-vision, visual query contrast is proposed to model the high granularity of vision by pulling the pseudo vision queries with the corresponding targets containing segments while pushing those without segments away. To ensure the generated queries retain semantic information, in vision-semantic, the pseudo vision queries are mapped back to semantic and supervised by real semantic embeddings. Experiments on ZPS achieve a 5.2% hPQ increase surpassing SOTA. We also examine inductive ZPS and open-vocabulary semantic segmentation and obtain comparative results while being 2 times faster in testing.","sentences":["Zero-shot Panoptic Segmentation (ZPS) aims to recognize foreground instances and background stuff without images containing unseen categories in training.","Due to the visual data sparsity and the difficulty of generalizing from seen to unseen categories, this task remains challenging.","To better generalize to unseen classes, we propose Conditional tOken aligNment and Cycle trAnsiTion (CONCAT), to produce generalizable semantic vision queries.","First, a feature extractor is trained by CON to link the vision and semantics for providing target queries.","Formally, CON is proposed to align the semantic queries with the CLIP visual CLS token extracted from complete and masked images.","To address the lack of unseen categories, a generator is required.","However, one of the gaps in synthesizing pseudo vision queries, ie, vision queries for unseen categories, is describing fine-grained visual details through semantic embeddings.","Therefore, we approach CAT to train the generator in semantic-vision and vision-semantic manners.","In semantic-vision, visual query contrast is proposed to model the high granularity of vision by pulling the pseudo vision queries with the corresponding targets containing segments while pushing those without segments away.","To ensure the generated queries retain semantic information, in vision-semantic, the pseudo vision queries are mapped back to semantic and supervised by real semantic embeddings.","Experiments on ZPS achieve a 5.2% hPQ increase surpassing SOTA.","We also examine inductive ZPS and open-vocabulary semantic segmentation and obtain comparative results while being 2 times faster in testing."],"url":"http://arxiv.org/abs/2402.13697v1","category":"cs.CV"}
{"created":"2024-02-21 10:54:12","title":"Complexity Growth and the Krylov-Wigner function","abstract":"For any state in a $D$-dimensional Hilbert space with a choice of basis, one can define a discrete version of the Wigner function -- a quasi-probability distribution which represents the state on a discrete phase space. The Wigner function can, in general, take on negative values, and the amount of negativity in the Wigner function has an operational meaning as a resource for quantum computation. In this note, we study the growth of Wigner negativity for a generic initial state under time evolution with chaotic Hamiltonians. We introduce the Krylov-Wigner function, i.e., the Wigner function defined with respect to the Krylov basis (with appropriate phases), and show that this choice of basis minimizes the early time growth of Wigner negativity in the large $D$ limit. We take this as evidence that the Krylov basis (with appropriate phases) is ideally suited for a dual, semi-classical description of chaotic quantum dynamics at large $D$. We also numerically study the time evolution of the Krylov-Wigner function and its negativity in random matrix theory for an initial pure state. We observe that the negativity broadly shows three phases: it rises gradually for a time of $O(\\sqrt{D})$, then hits a sharp ramp and finally saturates close to its upper bound of $\\sqrt{D}$.","sentences":["For any state in a $D$-dimensional Hilbert space with a choice of basis, one can define a discrete version of the Wigner function -- a quasi-probability distribution which represents the state on a discrete phase space.","The Wigner function can, in general, take on negative values, and the amount of negativity in the Wigner function has an operational meaning as a resource for quantum computation.","In this note, we study the growth of Wigner negativity for a generic initial state under time evolution with chaotic Hamiltonians.","We introduce the Krylov-Wigner function, i.e., the Wigner function defined with respect to the Krylov basis (with appropriate phases), and show that this choice of basis minimizes the early time growth of Wigner negativity in the large $D$ limit.","We take this as evidence that the Krylov basis (with appropriate phases) is ideally suited for a dual, semi-classical description of chaotic quantum dynamics at large $D$. We also numerically study the time evolution of the Krylov-Wigner function and its negativity in random matrix theory for an initial pure state.","We observe that the negativity broadly shows three phases: it rises gradually for a time of $O(\\sqrt{D})$, then hits a sharp ramp and finally saturates close to its upper bound of $\\sqrt{D}$."],"url":"http://arxiv.org/abs/2402.13694v1","category":"hep-th"}
{"created":"2024-02-21 10:53:23","title":"Reconfigurable Intelligent Surface assisted Integrated Sensing, Communication and Computation Systems","abstract":"This paper investigates a reconfigurable intelligent surface (RIS)-assisted integrated sensing, communication, and computation (ISCC) system. In this paradigm, the integrated sensing and communication (ISAC)-enabled user equipments (UEs) simultaneously detect the target and offload the computational tasks of radar sensing to the edge computing server (ECS) through their communication functionality. To enhance the efficiency of computation offloading, we deploy an RIS to mitigate the high attenuation between UEs and the ECS. A latency minimization problem is investigated with constraints on UE's transmit power, radar signal-to-interference-plus-noise ratio (SINR), RIS phase shift, and computation capability. We propose an algorithm based on the block coordinate descent (BCD) method to decouple the original problem into two subproblems, and then the computational and beamforming variables are optimized alternately utilizing efficient iterative algorithms. Simulation results demonstrate the effectiveness of our proposed algorithm.","sentences":["This paper investigates a reconfigurable intelligent surface (RIS)-assisted integrated sensing, communication, and computation (ISCC) system.","In this paradigm, the integrated sensing and communication (ISAC)-enabled user equipments (UEs) simultaneously detect the target and offload the computational tasks of radar sensing to the edge computing server (ECS) through their communication functionality.","To enhance the efficiency of computation offloading, we deploy an RIS to mitigate the high attenuation between UEs and the ECS.","A latency minimization problem is investigated with constraints on UE's transmit power, radar signal-to-interference-plus-noise ratio (SINR), RIS phase shift, and computation capability.","We propose an algorithm based on the block coordinate descent (BCD) method to decouple the original problem into two subproblems, and then the computational and beamforming variables are optimized alternately utilizing efficient iterative algorithms.","Simulation results demonstrate the effectiveness of our proposed algorithm."],"url":"http://arxiv.org/abs/2402.13692v1","category":"eess.SP"}
{"created":"2024-02-21 10:52:27","title":"Higher-order fractional equations and related time-changed pseudo-processes","abstract":"We study Cauchy problems of fractional differential equations in both space and time variables by expressing the solution in terms of ``stochastic composition\" of the solutions to two simpler problems. These Cauchy sub-problems respectively concern the space and the time differential operator involved in the main equation. We provide some probabilistic and pseudo-probabilistic applications, where the solution can be interpreted as the pseudo-transition density of a time-changed pseudo-process. To extend our results to higher order time-fractional problems, we introduce pseudo-subordinators as well as its pseudo-inverse. Finally, we present our results in the case of more general differential operators and we interpret the results by means of a linear combination of pseudo-subordinators and its inverse.","sentences":["We study Cauchy problems of fractional differential equations in both space and time variables by expressing the solution in terms of ``stochastic composition\" of the solutions to two simpler problems.","These Cauchy sub-problems respectively concern the space and the time differential operator involved in the main equation.","We provide some probabilistic and pseudo-probabilistic applications, where the solution can be interpreted as the pseudo-transition density of a time-changed pseudo-process.","To extend our results to higher order time-fractional problems, we introduce pseudo-subordinators as well as its pseudo-inverse.","Finally, we present our results in the case of more general differential operators and we interpret the results by means of a linear combination of pseudo-subordinators and its inverse."],"url":"http://arxiv.org/abs/2402.13691v1","category":"math.PR"}
{"created":"2024-02-21 10:51:57","title":"General Caputo-type fractional discrete diffusion equation for Schr\u00f6dinger operator","abstract":"This article aims to investigate the semi-classical analog of the general Caputo-type diffusion equation with time-dependent diffusion coefficient associated with the discrete Schr\\\"{o}dinger operator, $\\mathcal{H}_{\\hbar,V}:=-\\hbar^{-2}\\mathcal{L}_{\\hbar}+V$ on the lattice $\\hbar\\mathbb{Z}^{n},$ where $V$ is a non-negative multiplication operator and $\\mathcal{L}_{\\hbar}$ is the discrete Laplacian. We establish the well-posedness of the Cauchy problem for the general Caputo-type diffusion equation with a regular coefficient in the associated Sobolev-type spaces. However, it is very weakly well-posed when the diffusion coefficient has a distributional singularity. Finally, we recapture the classical solution (resp. very weak) for the general Caputo-type diffusion equation in the semi-classical limit $\\hbar\\to 0$.","sentences":["This article aims to investigate the semi-classical analog of the general Caputo-type diffusion equation with time-dependent diffusion coefficient associated with the discrete Schr\\\"{o}dinger operator, $\\mathcal{H}_{\\hbar,V}:=-\\hbar^{-2}\\mathcal{L}_{\\hbar}+V$ on the lattice $\\hbar\\mathbb{Z}^{n},$ where $V$ is a non-negative multiplication operator and $\\mathcal{L}_{\\hbar}$ is the discrete Laplacian.","We establish the well-posedness of the Cauchy problem for the general Caputo-type diffusion equation with a regular coefficient in the associated Sobolev-type spaces.","However, it is very weakly well-posed when the diffusion coefficient has a distributional singularity.","Finally, we recapture the classical solution (resp.","very weak) for the general Caputo-type diffusion equation in the semi-classical limit $\\hbar\\to 0$."],"url":"http://arxiv.org/abs/2402.13690v1","category":"math.AP"}
{"created":"2024-02-21 10:41:54","title":"An Augmented Lagrangian Method for Training Recurrent Neural Networks","abstract":"Recurrent Neural Networks (RNNs) are widely used to model sequential data in a wide range of areas, such as natural language processing, speech recognition, machine translation, and time series analysis. In this paper, we model the training process of RNNs with the ReLU activation function as a constrained optimization problem with a smooth nonconvex objective function and piecewise smooth nonconvex constraints. We prove that any feasible point of the optimization problem satisfies the no nonzero abnormal multiplier constraint qualification (NNAMCQ), and any local minimizer is a Karush-Kuhn-Tucker (KKT) point of the problem. Moreover, we propose an augmented Lagrangian method (ALM) and design an efficient block coordinate descent (BCD) method to solve the subproblems of the ALM. The update of each block of the BCD method has a closed-form solution. The stop criterion for the inner loop is easy to check and can be stopped in finite steps. Moreover, we show that the BCD method can generate a directional stationary point of the subproblem. Furthermore, we establish the global convergence of the ALM to a KKT point of the constrained optimization problem. Compared with the state-of-the-art algorithms, numerical results demonstrate the efficiency and effectiveness of the ALM for training RNNs.","sentences":["Recurrent Neural Networks (RNNs) are widely used to model sequential data in a wide range of areas, such as natural language processing, speech recognition, machine translation, and time series analysis.","In this paper, we model the training process of RNNs with the ReLU activation function as a constrained optimization problem with a smooth nonconvex objective function and piecewise smooth nonconvex constraints.","We prove that any feasible point of the optimization problem satisfies the no nonzero abnormal multiplier constraint qualification (NNAMCQ), and any local minimizer is a Karush-Kuhn-Tucker (KKT) point of the problem.","Moreover, we propose an augmented Lagrangian method (ALM) and design an efficient block coordinate descent (BCD) method to solve the subproblems of the ALM.","The update of each block of the BCD method has a closed-form solution.","The stop criterion for the inner loop is easy to check and can be stopped in finite steps.","Moreover, we show that the BCD method can generate a directional stationary point of the subproblem.","Furthermore, we establish the global convergence of the ALM to a KKT point of the constrained optimization problem.","Compared with the state-of-the-art algorithms, numerical results demonstrate the efficiency and effectiveness of the ALM for training RNNs."],"url":"http://arxiv.org/abs/2402.13687v1","category":"math.OC"}
{"created":"2024-02-21 10:38:25","title":"End-to-end simulation of particle physics events with Flow Matching and generator Oversampling","abstract":"The simulation of high-energy physics collision events is a key element for data analysis at present and future particle accelerators. The comparison of simulation predictions to data allows looking for rare deviations that can be due to new phenomena not previously observed. We show that novel machine learning algorithms, specifically Normalizing Flows and Flow Matching, can be used to replicate accurate simulations from traditional approaches with several orders of magnitude of speed-up. The classical simulation chain starts from a physics process of interest, computes energy deposits of particles and electronics response, and finally employs the same reconstruction algorithms used for data. Eventually, the data are reduced to some high-level analysis format. Instead, we propose an end-to-end approach, simulating the final data format directly from physical generator inputs, skipping any intermediate steps. We use particle jets simulation as a benchmark for comparing both discrete and continuous Normalizing Flows models. The models are validated across a variety of metrics to identify the most accurate. We discuss the scaling of performance with the increase in training data, as well as the generalization power of these models on physical processes different from the training one. We investigate sampling multiple times from the same physical generator inputs, a procedure we name oversampling, and we show that it can effectively reduce the statistical uncertainties of a dataset. This class of ML algorithms is found to be capable of learning the expected detector response independently of the physical input process. Their speed and accuracy, coupled with the stability of the training procedure, make them a compelling tool for the needs of current and future experiments.","sentences":["The simulation of high-energy physics collision events is a key element for data analysis at present and future particle accelerators.","The comparison of simulation predictions to data allows looking for rare deviations that can be due to new phenomena not previously observed.","We show that novel machine learning algorithms, specifically Normalizing Flows and Flow Matching, can be used to replicate accurate simulations from traditional approaches with several orders of magnitude of speed-up.","The classical simulation chain starts from a physics process of interest, computes energy deposits of particles and electronics response, and finally employs the same reconstruction algorithms used for data.","Eventually, the data are reduced to some high-level analysis format.","Instead, we propose an end-to-end approach, simulating the final data format directly from physical generator inputs, skipping any intermediate steps.","We use particle jets simulation as a benchmark for comparing both discrete and continuous Normalizing Flows models.","The models are validated across a variety of metrics to identify the most accurate.","We discuss the scaling of performance with the increase in training data, as well as the generalization power of these models on physical processes different from the training one.","We investigate sampling multiple times from the same physical generator inputs, a procedure we name oversampling, and we show that it can effectively reduce the statistical uncertainties of a dataset.","This class of ML algorithms is found to be capable of learning the expected detector response independently of the physical input process.","Their speed and accuracy, coupled with the stability of the training procedure, make them a compelling tool for the needs of current and future experiments."],"url":"http://arxiv.org/abs/2402.13684v1","category":"hep-ex"}
{"created":"2024-02-21 10:31:16","title":"Extrapolated Shock Tracking: Bridging shock-fitting and embedded boundary methods","abstract":"We propose a novel approach to approximate numerically shock waves. The method combines the unstructured shock-fitting approach developed in the last decade by some of the authors, with ideas coming from embedded boundary techniques. The numerical method obtained allows avoiding the re-meshing phase required by the unstructured fitting method, while guaranteeing accuracy properties very close to those of the fitting approach. This new method has many similarities with front tracking approaches, and paves the way to shock-tracking techniques truly independent on the data and mesh structure used by the flow solver. The approach is tested on several problems showing accuracy properties very close to those of more expensive fitting methods, with a considerable gain in flexibility and generality.","sentences":["We propose a novel approach to approximate numerically shock waves.","The method combines the unstructured shock-fitting approach developed in the last decade by some of the authors, with ideas coming from embedded boundary techniques.","The numerical method obtained allows avoiding the re-meshing phase required by the unstructured fitting method, while guaranteeing accuracy properties very close to those of the fitting approach.","This new method has many similarities with front tracking approaches, and paves the way to shock-tracking techniques truly independent on the data and mesh structure used by the flow solver.","The approach is tested on several problems showing accuracy properties very close to those of more expensive fitting methods, with a considerable gain in flexibility and generality."],"url":"http://arxiv.org/abs/2402.13681v1","category":"math.NA"}
{"created":"2024-02-21 10:29:41","title":"Einstein's basement -- a model for dark matter and an expanding universe?","abstract":"We present a model treating the energy-momentum relation of relativistic particles as the upper branch of a generalized energy-momentum relation emerging from a forbidden crossing between the constant energy of a massive particle and the photon line. The lower branch, a regime dubbed as Einstein's basement, gives rise to particles with different kinematics that is analysed in the low-velocity limit. Allowing for gravitational interaction between those particles, and between particles of both branches, we discuss whether particles in Einstein's basement might be suitable dark matter candidates. Moreover, the special dynamics of mixed systems leads to an expansion mechanism for regular matter in space. Tests of the presented model by astronomical observations are suggested.","sentences":["We present a model treating the energy-momentum relation of relativistic particles as the upper branch of a generalized energy-momentum relation emerging from a forbidden crossing between the constant energy of a massive particle and the photon line.","The lower branch, a regime dubbed as Einstein's basement, gives rise to particles with different kinematics that is analysed in the low-velocity limit.","Allowing for gravitational interaction between those particles, and between particles of both branches, we discuss whether particles in Einstein's basement might be suitable dark matter candidates.","Moreover, the special dynamics of mixed systems leads to an expansion mechanism for regular matter in space.","Tests of the presented model by astronomical observations are suggested."],"url":"http://arxiv.org/abs/2402.13679v1","category":"gr-qc"}
{"created":"2024-02-21 10:29:26","title":"Weak Poincar\u00e9 inequality comparisons for ideal and hybrid slice sampling","abstract":"Using the framework of weak Poincar{\\'e} inequalities, we provide a general comparison between the Hybrid and Ideal Slice Sampling Markov chains in terms of their Dirichlet forms. In particular, under suitable assumptions Hybrid Slice Sampling will inherit fast convergence from Ideal Slice Sampling and conversely. We apply our results to analyse the convergence of the Independent Metropolis--Hastings, Slice Sampling with Stepping-Out and Shrinkage, and Hit-and-Run-within-Slice Sampling algorithms.","sentences":["Using the framework of weak Poincar{\\'e} inequalities, we provide a general comparison between the Hybrid and Ideal Slice Sampling Markov chains in terms of their Dirichlet forms.","In particular, under suitable assumptions Hybrid Slice Sampling will inherit fast convergence from Ideal Slice Sampling and conversely.","We apply our results to analyse the convergence of the Independent Metropolis--Hastings, Slice Sampling with Stepping-Out and Shrinkage, and Hit-and-Run-within-Slice Sampling algorithms."],"url":"http://arxiv.org/abs/2402.13678v1","category":"stat.CO"}
{"created":"2024-02-21 10:19:37","title":"Limits of open ASEP stationary measures near a boundary","abstract":"Consider the stationary measure of open asymmetric simple exclusion process (ASEP) on the lattice $\\{1,\\dots,n\\}$. Taking $n$ to infinity while fixing the jump rates, this measure converges to a measure on the semi-infinite lattice. In the high and low density phases, we characterize the limiting measure and also show that this convergence occurs in total variation distance on a sublattice of scale $n/\\log n$. Our approach involves bounding the total variation distance using generating functions, which are further estimated through a subtle analysis of the atom masses of Askey-Wilson signed measures.","sentences":["Consider the stationary measure of open asymmetric simple exclusion process (ASEP) on the lattice $\\{1,\\dots,n\\}$. Taking $n$ to infinity while fixing the jump rates, this measure converges to a measure on the semi-infinite lattice.","In the high and low density phases, we characterize the limiting measure and also show that this convergence occurs in total variation distance on a sublattice of scale $n/\\log n$. Our approach involves bounding the total variation distance using generating functions, which are further estimated through a subtle analysis of the atom masses of Askey-Wilson signed measures."],"url":"http://arxiv.org/abs/2402.13675v1","category":"math.PR"}
{"created":"2024-02-21 10:09:56","title":"KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual Machine-Generated Text Detection","abstract":"SemEval-2024 Task 8 is focused on multigenerator, multidomain, and multilingual black-box machine-generated text detection. Such a detection is important for preventing a potential misuse of large language models (LLMs), the newest of which are very capable in generating multilingual human-like texts. We have coped with this task in multiple ways, utilizing language identification and parameter-efficient fine-tuning of smaller LLMs for text classification. We have further used the per-language classification-threshold calibration to uniquely combine fine-tuned models predictions with statistical detection metrics to improve generalization of the system detection performance. Our submitted method achieved competitive results, ranking at the fourth place, just under 1 percentage point behind the winner.","sentences":["SemEval-2024 Task 8 is focused on multigenerator, multidomain, and multilingual black-box machine-generated text detection.","Such a detection is important for preventing a potential misuse of large language models (LLMs), the newest of which are very capable in generating multilingual human-like texts.","We have coped with this task in multiple ways, utilizing language identification and parameter-efficient fine-tuning of smaller LLMs for text classification.","We have further used the per-language classification-threshold calibration to uniquely combine fine-tuned models predictions with statistical detection metrics to improve generalization of the system detection performance.","Our submitted method achieved competitive results, ranking at the fourth place, just under 1 percentage point behind the winner."],"url":"http://arxiv.org/abs/2402.13671v1","category":"cs.CL"}
{"created":"2024-02-21 10:08:13","title":"The Riemannian Convex Bundle Method","abstract":"We introduce the convex bundle method to solve convex, non-smooth optimization problems on Riemannian manifolds. Each step of our method is based on a model that involves the convex hull of previously collected subgradients, parallely transported into the current serious iterate. This approach generalizes the dual form of classical bundle subproblems in Euclidean space. We prove that, under mild conditions, the convex bundle method converges to a minimizer. Several numerical examples implemented using the Julia package Manopt.jl illustrate the performance of the proposed method and compare it to the subgradient method, the cyclic proximal point, as well as the proximal bundle algorithm from Hoseini Monjezi, Nobakhtian, Pouryayevali, 2021.","sentences":["We introduce the convex bundle method to solve convex, non-smooth optimization problems on Riemannian manifolds.","Each step of our method is based on a model that involves the convex hull of previously collected subgradients, parallely transported into the current serious iterate.","This approach generalizes the dual form of classical bundle subproblems in Euclidean space.","We prove that, under mild conditions, the convex bundle method converges to a minimizer.","Several numerical examples implemented using the Julia package Manopt.jl illustrate the performance of the proposed method and compare it to the subgradient method, the cyclic proximal point, as well as the proximal bundle algorithm from Hoseini Monjezi, Nobakhtian, Pouryayevali, 2021."],"url":"http://arxiv.org/abs/2402.13670v1","category":"math.OC"}
{"created":"2024-02-21 10:06:08","title":"Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning","abstract":"The surge in Large Language Models (LLMs) has revolutionized natural language processing, but fine-tuning them for specific tasks often encounters challenges in balancing performance and preserving general instruction-following abilities. In this paper, we posit that the distribution gap between task datasets and the LLMs serves as the primary underlying cause. To address the problem, we introduce Self-Distillation Fine-Tuning (SDFT), a novel approach that bridges the distribution gap by guiding fine-tuning with a distilled dataset generated by the model itself to match its original distribution. Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs. Our code is available at \\url{https://github.com/sail-sg/sdft}.","sentences":["The surge in Large Language Models (LLMs) has revolutionized natural language processing, but fine-tuning them for specific tasks often encounters challenges in balancing performance and preserving general instruction-following abilities.","In this paper, we posit that the distribution gap between task datasets and the LLMs serves as the primary underlying cause.","To address the problem, we introduce Self-Distillation Fine-Tuning (SDFT), a novel approach that bridges the distribution gap by guiding fine-tuning with a distilled dataset generated by the model itself to match its original distribution.","Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning.","Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs.","Our code is available at \\url{https://github.com/sail-sg/sdft}."],"url":"http://arxiv.org/abs/2402.13669v1","category":"cs.CL"}
{"created":"2024-02-21 09:59:20","title":"GCOF: Self-iterative Text Generation for Copywriting Using Large Language Model","abstract":"Large language models(LLM) such as ChatGPT have substantially simplified the generation of marketing copy, yet producing content satisfying domain specific requirements, such as effectively engaging customers, remains a significant challenge. In this work, we introduce the Genetic Copy Optimization Framework (GCOF) designed to enhance both efficiency and engagememnt of marketing copy creation. We conduct explicit feature engineering within the prompts of LLM. Additionally, we modify the crossover operator in Genetic Algorithm (GA), integrating it into the GCOF to enable automatic feature engineering. This integration facilitates a self-iterative refinement of the marketing copy. Compared to human curated copy, Online results indicate that copy produced by our framework achieves an average increase in click-through rate (CTR) of over $50\\%$.","sentences":["Large language models(LLM) such as ChatGPT have substantially simplified the generation of marketing copy, yet producing content satisfying domain specific requirements, such as effectively engaging customers, remains a significant challenge.","In this work, we introduce the Genetic Copy Optimization Framework (GCOF) designed to enhance both efficiency and engagememnt of marketing copy creation.","We conduct explicit feature engineering within the prompts of LLM.","Additionally, we modify the crossover operator in Genetic Algorithm (GA), integrating it into the GCOF to enable automatic feature engineering.","This integration facilitates a self-iterative refinement of the marketing copy.","Compared to human curated copy, Online results indicate that copy produced by our framework achieves an average increase in click-through rate (CTR) of over $50\\%$."],"url":"http://arxiv.org/abs/2402.13667v1","category":"cs.CL"}
{"created":"2024-02-21 09:55:17","title":"A five-dimensional Bianchi type V-like extension","abstract":"We uncover the solution space of a five dimensional geometry which we deem it as the direct counterpart of the Bianchi Type V cosmological model. We kinematically reduce the scale factor matrix and then, with an appropriate scaling and choice of time, we cast the spatial equations into a simple ``Kasner'' like form; thus revealing linear integrals of motion. Their number is enough so that, along with the quadratic constraint, it suffices to scan the entire space of solutions. The latter is revealed to be quite rich, containing cosmological solutions, some of which admit dimensional reduction asymptotically to four dimensions, Kundt spacetimes with vanishing type I (polynomial) curvature scalars and solutions describing periodic universes which behave like cosmological time crystals.","sentences":["We uncover the solution space of a five dimensional geometry which we deem it as the direct counterpart of the Bianchi Type V cosmological model.","We kinematically reduce the scale factor matrix and then, with an appropriate scaling and choice of time, we cast the spatial equations into a simple ``Kasner'' like form; thus revealing linear integrals of motion.","Their number is enough so that, along with the quadratic constraint, it suffices to scan the entire space of solutions.","The latter is revealed to be quite rich, containing cosmological solutions, some of which admit dimensional reduction asymptotically to four dimensions, Kundt spacetimes with vanishing type I (polynomial) curvature scalars and solutions describing periodic universes which behave like cosmological time crystals."],"url":"http://arxiv.org/abs/2402.13665v1","category":"gr-qc"}
{"created":"2024-02-21 09:52:02","title":"CMB Polarization Measurements","abstract":"The polarization of the Cosmic Microwave Background (CMB) radiation carries essential information on early stages of the Universe such as the cosmic inflation, forming cosmological structures through gravitational lensing, and the epoch of re-ionization. The signal requires high sensitivity instruments with a large number of detectors (bolometers) and low leakage of Stokes $I$ into $Q$ and $U$. The Galactic diffuse foreground emission is a limiting factor in CMB polarization measurements, requiring its characterization at both low and high frequency compared to the peak of the CMB emission, in order to be subtracted off. In this paper we describe the next generation space experiment for the measure of the CMB polarization, LiteBIRD, that is aimed to investigate the first fractions of a second of the Universe and is expected to be flown at the beginning of the next decade. Also, we describe the experiments designed for measuring the foreground emissions from our own Galaxy. Finally, we also describe sub-orbital experiments, operating and planned, as they are vehicles for the development of technologies and data reduction tools that have been and will be used in space missions.","sentences":["The polarization of the Cosmic Microwave Background (CMB) radiation carries essential information on early stages of the Universe such as the cosmic inflation, forming cosmological structures through gravitational lensing, and the epoch of re-ionization.","The signal requires high sensitivity instruments with a large number of detectors (bolometers) and low leakage of Stokes $I$ into $Q$ and $U$. The Galactic diffuse foreground emission is a limiting factor in CMB polarization measurements, requiring its characterization at both low and high frequency compared to the peak of the CMB emission, in order to be subtracted off.","In this paper we describe the next generation space experiment for the measure of the CMB polarization, LiteBIRD, that is aimed to investigate the first fractions of a second of the Universe and is expected to be flown at the beginning of the next decade.","Also, we describe the experiments designed for measuring the foreground emissions from our own Galaxy.","Finally, we also describe sub-orbital experiments, operating and planned, as they are vehicles for the development of technologies and data reduction tools that have been and will be used in space missions."],"url":"http://arxiv.org/abs/2402.13661v1","category":"astro-ph.CO"}
{"created":"2024-02-21 09:45:08","title":"Privacy-Preserving Instructions for Aligning Large Language Models","abstract":"Service providers of large language model (LLM) applications collect user instructions in the wild and use them in further aligning LLMs with users' intentions. These instructions, which potentially contain sensitive information, are annotated by human workers in the process. This poses a new privacy risk not addressed by the typical private optimization. To this end, we propose using synthetic instructions to replace real instructions in data annotation and model fine-tuning. Formal differential privacy is guaranteed by generating those synthetic instructions using privately fine-tuned generators. Crucial in achieving the desired utility is our novel filtering algorithm that matches the distribution of the synthetic instructions to that of the real ones. In both supervised fine-tuning and reinforcement learning from human feedback, our extensive experiments demonstrate the high utility of the final set of synthetic instructions by showing comparable results to real instructions. In supervised fine-tuning, models trained with private synthetic instructions outperform leading open-source models such as Vicuna.","sentences":["Service providers of large language model (LLM) applications collect user instructions in the wild and use them in further aligning LLMs with users' intentions.","These instructions, which potentially contain sensitive information, are annotated by human workers in the process.","This poses a new privacy risk not addressed by the typical private optimization.","To this end, we propose using synthetic instructions to replace real instructions in data annotation and model fine-tuning.","Formal differential privacy is guaranteed by generating those synthetic instructions using privately fine-tuned generators.","Crucial in achieving the desired utility is our novel filtering algorithm that matches the distribution of the synthetic instructions to that of the real ones.","In both supervised fine-tuning and reinforcement learning from human feedback, our extensive experiments demonstrate the high utility of the final set of synthetic instructions by showing comparable results to real instructions.","In supervised fine-tuning, models trained with private synthetic instructions outperform leading open-source models such as Vicuna."],"url":"http://arxiv.org/abs/2402.13659v1","category":"cs.CR"}
{"created":"2024-02-21 09:44:14","title":"Network nestedness in primates: a structural constraint or a biological advantage of social complexity?","abstract":"This study investigates the prevalence and implications of nestedness within primate social networks, examining its relationship with cognitive and structural factors. We analysed data from 51 primate groups across 21 species, employing network analysis to evaluate nestedness and its correlation with modularity, neocortex ratio, and group size. We used Bayesian mixed effects modelling to investigate nestedness in primate social networks, controlling for phylogenetic dependencies and exploring various factors like neocortex ratio and group size. Our findings reveal a significant occurrence of nestedness in 66% of the species studied, exceeding chance expectations. This nestedness was more pronounced in groups with less steep dominance hierarchies, contrary to traditional assumptions linking it to hierarchical social structures. A notable inverse relationship between nestedness and modularity was observed, suggesting a structural trade-off in network formation. This pattern persisted even after controlling for species-specific social behaviours, indicating a general structural feature of primate networks. Surprisingly, our analysis showed no significant correlation between nestedness and neocortex ratio or group size, challenging the social brain hypothesis and suggesting a greater role for ecological factors in cognitive evolution. This study emphasises the importance of weak links in maintaining network resilience. Overall, our research provides new insights into primate social network structures, highlighting complex interplays between network characteristics and challenging existing paradigms in cognitive and evolutionary biology.","sentences":["This study investigates the prevalence and implications of nestedness within primate social networks, examining its relationship with cognitive and structural factors.","We analysed data from 51 primate groups across 21 species, employing network analysis to evaluate nestedness and its correlation with modularity, neocortex ratio, and group size.","We used Bayesian mixed effects modelling to investigate nestedness in primate social networks, controlling for phylogenetic dependencies and exploring various factors like neocortex ratio and group size.","Our findings reveal a significant occurrence of nestedness in 66% of the species studied, exceeding chance expectations.","This nestedness was more pronounced in groups with less steep dominance hierarchies, contrary to traditional assumptions linking it to hierarchical social structures.","A notable inverse relationship between nestedness and modularity was observed, suggesting a structural trade-off in network formation.","This pattern persisted even after controlling for species-specific social behaviours, indicating a general structural feature of primate networks.","Surprisingly, our analysis showed no significant correlation between nestedness and neocortex ratio or group size, challenging the social brain hypothesis and suggesting a greater role for ecological factors in cognitive evolution.","This study emphasises the importance of weak links in maintaining network resilience.","Overall, our research provides new insights into primate social network structures, highlighting complex interplays between network characteristics and challenging existing paradigms in cognitive and evolutionary biology."],"url":"http://arxiv.org/abs/2402.13658v1","category":"q-bio.QM"}
{"created":"2024-02-21 09:42:19","title":"Higher-order and fractional discrete time crystals in Floquet-driven Rydberg atoms","abstract":"Higher-order and fractional discrete time crystals (DTCs) are exotic phases of matter where the discrete time translation symmetry is broken into higher-order and non-integer category. Generation of these unique DTCs has been widely studied theoretically in different systems. However, no current experimental methods can probe these higher-order and fractional DTCs in any quantum many-body systems. We demonstrate an experimental approach to observe higher-order and fractional DTCs in Floquet-driven Rydberg atomic gases. We have discovered multiple $n$-DTCs with integer values of $n$ = 2, 3, and 4, and others ranging up to 14, along with fractional $n$-DTCs with $n$ values beyond the integers. The system response can transition between adjacent integer DTCs, during which the fractional DTCs are investigated. Study of higher-order and fractional DTCs expands fundamental knowledge of non-equilibrium dynamics and is promising for discovery of more complex temporal symmetries beyond the single discrete time translation symmetry.","sentences":["Higher-order and fractional discrete time crystals (DTCs) are exotic phases of matter where the discrete time translation symmetry is broken into higher-order and non-integer category.","Generation of these unique DTCs has been widely studied theoretically in different systems.","However, no current experimental methods can probe these higher-order and fractional DTCs in any quantum many-body systems.","We demonstrate an experimental approach to observe higher-order and fractional DTCs in Floquet-driven Rydberg atomic gases.","We have discovered multiple $n$-DTCs with integer values of $n$ = 2, 3, and 4, and others ranging up to 14, along with fractional $n$-DTCs with $n$ values beyond the integers.","The system response can transition between adjacent integer DTCs, during which the fractional DTCs are investigated.","Study of higher-order and fractional DTCs expands fundamental knowledge of non-equilibrium dynamics and is promising for discovery of more complex temporal symmetries beyond the single discrete time translation symmetry."],"url":"http://arxiv.org/abs/2402.13657v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-21 09:42:13","title":"Numerical methods for closed-loop systems with non-autonomous data","abstract":"By computing a feedback control via the linear quadratic regulator (LQR) approach and simulating a non-linear non-autonomous closed-loop system using this feedback, we combine two numerically challenging tasks. For the first task, the computation of the feedback control, we use the non-autonomous generalized differential Riccati equation (DRE), whose solution determines the time-varying feedback gain matrix. Regarding the second task, we want to be able to simulate non-linear closed-loop systems for which it is known that the regulator is only valid for sufficiently small perturbations. Thus, one easily runs into numerical issues in the integrators when the closed-loop control varies greatly. For these systems, e.g., the A-stable implicit Euler methods fails.\\newline On the one hand, we implement non-autonomous versions of splitting schemes and BDF methods for the solution of our non-autonomous DREs. These are well-established DRE solvers in the autonomous case. On the other hand, to tackle the numerical issues in the simulation of the non-linear closed-loop system, we apply a fractional-step-theta scheme with time-adaptivity tuned specifically to this kind of challenge. That is, we additionally base the time-adaptivity on the activity of the control. We compare this approach to the more classical error-based time-adaptivity.\\newline We describe techniques to make these two tasks computable in a reasonable amount of time and are able to simulate closed-loop systems with strongly varying controls, while avoiding numerical issues. Our time-adaptivity approach requires fewer time steps than the error-based alternative and is more reliable.","sentences":["By computing a feedback control via the linear quadratic regulator (LQR) approach and simulating a non-linear non-autonomous closed-loop system using this feedback, we combine two numerically challenging tasks.","For the first task, the computation of the feedback control, we use the non-autonomous generalized differential Riccati equation (DRE), whose solution determines the time-varying feedback gain matrix.","Regarding the second task, we want to be able to simulate non-linear closed-loop systems for which it is known that the regulator is only valid for sufficiently small perturbations.","Thus, one easily runs into numerical issues in the integrators when the closed-loop control varies greatly.","For these systems, e.g., the A-stable implicit Euler methods fails.\\newline","On the one hand, we implement non-autonomous versions of splitting schemes and BDF methods for the solution of our non-autonomous DREs.","These are well-established DRE solvers in the autonomous case.","On the other hand, to tackle the numerical issues in the simulation of the non-linear closed-loop system, we apply a fractional-step-theta scheme with time-adaptivity tuned specifically to this kind of challenge.","That is, we additionally base the time-adaptivity on the activity of the control.","We compare this approach to the more classical error-based time-adaptivity.\\newline We describe techniques to make these two tasks computable in a reasonable amount of time and are able to simulate closed-loop systems with strongly varying controls, while avoiding numerical issues.","Our time-adaptivity approach requires fewer time steps than the error-based alternative and is more reliable."],"url":"http://arxiv.org/abs/2402.13656v1","category":"math.NA"}
{"created":"2024-02-21 09:37:17","title":"Robustness of Deep Neural Networks for Micro-Doppler Radar Classification","abstract":"With the great capabilities of deep classifiers for radar data processing come the risks of learning dataset-specific features that do not generalize well. In this work, the robustness of two deep convolutional architectures, trained and tested on the same data, is evaluated. When standard training practice is followed, both classifiers exhibit sensitivity to subtle temporal shifts of the input representation, an augmentation that carries minimal semantic content. Furthermore, the models are extremely susceptible to adversarial examples. Both small temporal shifts and adversarial examples are a result of a model overfitting on features that do not generalize well. As a remedy, it is shown that training on adversarial examples and temporally augmented samples can reduce this effect and lead to models that generalise better. Finally, models operating on cadence-velocity diagram representation rather than Doppler-time are demonstrated to be naturally more immune to adversarial examples.","sentences":["With the great capabilities of deep classifiers for radar data processing come the risks of learning dataset-specific features that do not generalize well.","In this work, the robustness of two deep convolutional architectures, trained and tested on the same data, is evaluated.","When standard training practice is followed, both classifiers exhibit sensitivity to subtle temporal shifts of the input representation, an augmentation that carries minimal semantic content.","Furthermore, the models are extremely susceptible to adversarial examples.","Both small temporal shifts and adversarial examples are a result of a model overfitting on features that do not generalize well.","As a remedy, it is shown that training on adversarial examples and temporally augmented samples can reduce this effect and lead to models that generalise better.","Finally, models operating on cadence-velocity diagram representation rather than Doppler-time are demonstrated to be naturally more immune to adversarial examples."],"url":"http://arxiv.org/abs/2402.13651v1","category":"cs.CV"}
{"created":"2024-02-21 09:35:57","title":"Obstacle crossing strategies for high-speed 4WD small-scale vehicle","abstract":"Unmanned ground vehicle obstacle crossing generally relies on two strategies: (i) applying a wheel torque for climbing and (ii) modifying the vehicle shape by using a wheel-leg or wheel-paddle to lift the wheel on top of the obstacle. However, most of those strategies sacrifice speed in order to have a longer contact duration between the wheels and the obstacle. This paper investigates the behaviour of a 4WD high-speed vehicle while crossing a step obstacle using a design of experiment (DoE). A 3D multibody vehicle model is equipped with a novel 2-DoF suspension system, which horizontal damping coefficient is modify to dampen wheel motion in longitudinal and vertical directions in relation to the chassis, for a given speed and obstacle height. The DoE results allow to propose a novel high-speed obstacle crossing strategy based on three metrics: (i) the kinetic energy variation of the vehicle, (ii) the contact duration between the wheel and the obstacle, and (iii) the pitch rate at the start of the ballistic phase. Experimental function are proposed to be able modify these metric in real time.","sentences":["Unmanned ground vehicle obstacle crossing generally relies on two strategies: (i) applying a wheel torque for climbing and (ii) modifying the vehicle shape by using a wheel-leg or wheel-paddle to lift the wheel on top of the obstacle.","However, most of those strategies sacrifice speed in order to have a longer contact duration between the wheels and the obstacle.","This paper investigates the behaviour of a 4WD high-speed vehicle while crossing a step obstacle using a design of experiment (DoE).","A 3D multibody vehicle model is equipped with a novel 2-DoF suspension system, which horizontal damping coefficient is modify to dampen wheel motion in longitudinal and vertical directions in relation to the chassis, for a given speed and obstacle height.","The DoE results allow to propose a novel high-speed obstacle crossing strategy based on three metrics: (i) the kinetic energy variation of the vehicle, (ii) the contact duration between the wheel and the obstacle, and (iii) the pitch rate at the start of the ballistic phase.","Experimental function are proposed to be able modify these metric in real time."],"url":"http://arxiv.org/abs/2402.13650v1","category":"cs.RO"}
{"created":"2024-02-21 09:34:41","title":"Explicit reciprocity laws for diagonal classes: higher level cases","abstract":"We generalize the $p$-adic explicit reciprocity laws for balanced diagonal classes by Darmon-Rotger and Bertolini-Seveso-Venerucci to the case of geometric balanced triples $(f,g,h)$ of modular eigenforms where $f$ is a $p$-ordinary newform, while $g$ and $h$ are allowed to be (both) supercuspidal at $p$ or (both) ramified principal series at $p$.","sentences":["We generalize the $p$-adic explicit reciprocity laws for balanced diagonal classes by Darmon-Rotger and Bertolini-Seveso-Venerucci to the case of geometric balanced triples $(f,g,h)$ of modular eigenforms where $f$ is a $p$-ordinary newform, while $g$ and $h$ are allowed to be (both) supercuspidal at $p$ or (both) ramified principal series at $p$."],"url":"http://arxiv.org/abs/2402.13648v1","category":"math.NT"}
{"created":"2024-02-21 09:28:02","title":"Unsupervised Text Style Transfer via LLMs and Attention Masking with Multi-way Interactions","abstract":"Unsupervised Text Style Transfer (UTST) has emerged as a critical task within the domain of Natural Language Processing (NLP), aiming to transfer one stylistic aspect of a sentence into another style without changing its semantics, syntax, or other attributes. This task is especially challenging given the intrinsic lack of parallel text pairings. Among existing methods for UTST tasks, attention masking approach and Large Language Models (LLMs) are deemed as two pioneering methods. However, they have shortcomings in generating unsmooth sentences and changing the original contents, respectively. In this paper, we investigate if we can combine these two methods effectively. We propose four ways of interactions, that are pipeline framework with tuned orders; knowledge distillation from LLMs to attention masking model; in-context learning with constructed parallel examples. We empirically show these multi-way interactions can improve the baselines in certain perspective of style strength, content preservation and text fluency. Experiments also demonstrate that simply conducting prompting followed by attention masking-based revision can consistently surpass the other systems, including supervised text style transfer systems. On Yelp-clean and Amazon-clean datasets, it improves the previously best mean metric by 0.5 and 3.0 absolute percentages respectively, and achieves new SOTA results.","sentences":["Unsupervised Text Style Transfer (UTST) has emerged as a critical task within the domain of Natural Language Processing (NLP), aiming to transfer one stylistic aspect of a sentence into another style without changing its semantics, syntax, or other attributes.","This task is especially challenging given the intrinsic lack of parallel text pairings.","Among existing methods for UTST tasks, attention masking approach and Large Language Models (LLMs) are deemed as two pioneering methods.","However, they have shortcomings in generating unsmooth sentences and changing the original contents, respectively.","In this paper, we investigate if we can combine these two methods effectively.","We propose four ways of interactions, that are pipeline framework with tuned orders; knowledge distillation from LLMs to attention masking model; in-context learning with constructed parallel examples.","We empirically show these multi-way interactions can improve the baselines in certain perspective of style strength, content preservation and text fluency.","Experiments also demonstrate that simply conducting prompting followed by attention masking-based revision can consistently surpass the other systems, including supervised text style transfer systems.","On Yelp-clean and Amazon-clean datasets, it improves the previously best mean metric by 0.5 and 3.0 absolute percentages respectively, and achieves new SOTA results."],"url":"http://arxiv.org/abs/2402.13647v1","category":"cs.CL"}
{"created":"2024-02-21 09:24:34","title":"Random Carleson Sequences for the Hardy space on the Polydisc and the Unit Ball","abstract":"We study the Kolmogorov 0-1 law for a random sequence with prescribed radii so that it generates a Carleson measure almost surely, both for the Hardy space on the polydisc and the Hardy space on the unit ball, thus providing improved versions of previous results of the first two authors and of a separate result of Massaneda. In the polydisc, the geometry of such sequences is not well understood, so we proceed by studying the random Gramians generated by random sequences, using tools from the theory of random matrices. Another result we prove, and that is of its own relevance, is the 0-1 law for a random sequence to be partitioned into M separated sequences with respect to the pseudo-hyperbolic distance, which is used also to describe the random sequences that are interpolating for the Bloch space on the unit disc almost surely.","sentences":["We study the Kolmogorov 0-1 law for a random sequence with prescribed radii so that it generates a Carleson measure almost surely, both for the Hardy space on the polydisc and the Hardy space on the unit ball, thus providing improved versions of previous results of the first two authors and of a separate result of Massaneda.","In the polydisc, the geometry of such sequences is not well understood, so we proceed by studying the random Gramians generated by random sequences, using tools from the theory of random matrices.","Another result we prove, and that is of its own relevance, is the 0-1 law for a random sequence to be partitioned into M separated sequences with respect to the pseudo-hyperbolic distance, which is used also to describe the random sequences that are interpolating for the Bloch space on the unit disc almost surely."],"url":"http://arxiv.org/abs/2402.13645v1","category":"math.CV"}
{"created":"2024-02-21 09:22:45","title":"Class-Aware Mask-Guided Feature Refinement for Scene Text Recognition","abstract":"Scene text recognition is a rapidly developing field that faces numerous challenges due to the complexity and diversity of scene text, including complex backgrounds, diverse fonts, flexible arrangements, and accidental occlusions. In this paper, we propose a novel approach called Class-Aware Mask-guided feature refinement (CAM) to address these challenges. Our approach introduces canonical class-aware glyph masks generated from a standard font to effectively suppress background and text style noise, thereby enhancing feature discrimination. Additionally, we design a feature alignment and fusion module to incorporate the canonical mask guidance for further feature refinement for text recognition. By enhancing the alignment between the canonical mask feature and the text feature, the module ensures more effective fusion, ultimately leading to improved recognition performance. We first evaluate CAM on six standard text recognition benchmarks to demonstrate its effectiveness. Furthermore, CAM exhibits superiority over the state-of-the-art method by an average performance gain of 4.1% across six more challenging datasets, despite utilizing a smaller model size. Our study highlights the importance of incorporating canonical mask guidance and aligned feature refinement techniques for robust scene text recognition. The code is available at https://github.com/MelosY/CAM.","sentences":["Scene text recognition is a rapidly developing field that faces numerous challenges due to the complexity and diversity of scene text, including complex backgrounds, diverse fonts, flexible arrangements, and accidental occlusions.","In this paper, we propose a novel approach called Class-Aware Mask-guided feature refinement (CAM) to address these challenges.","Our approach introduces canonical class-aware glyph masks generated from a standard font to effectively suppress background and text style noise, thereby enhancing feature discrimination.","Additionally, we design a feature alignment and fusion module to incorporate the canonical mask guidance for further feature refinement for text recognition.","By enhancing the alignment between the canonical mask feature and the text feature, the module ensures more effective fusion, ultimately leading to improved recognition performance.","We first evaluate CAM on six standard text recognition benchmarks to demonstrate its effectiveness.","Furthermore, CAM exhibits superiority over the state-of-the-art method by an average performance gain of 4.1% across six more challenging datasets, despite utilizing a smaller model size.","Our study highlights the importance of incorporating canonical mask guidance and aligned feature refinement techniques for robust scene text recognition.","The code is available at https://github.com/MelosY/CAM."],"url":"http://arxiv.org/abs/2402.13643v1","category":"cs.CV"}
{"created":"2024-02-21 09:20:09","title":"Adaptive Ridge Approach to Heteroscedastic Regression","abstract":"We propose an adaptive ridge (AR) based estimation scheme for a heteroscedastic linear model equipped with log-linear errors. We simultaneously estimate the mean and variance parameters and show new asymptotic distributional and tightness properties in a sparse setting. We also show that estimates for zero parameters shrink with more iterations under suitable assumptions for tuning parameters. We observe possible generalizations of this paper's results through simulations and will apply the estimation method in forecasting electricity consumption.","sentences":["We propose an adaptive ridge (AR) based estimation scheme for a heteroscedastic linear model equipped with log-linear errors.","We simultaneously estimate the mean and variance parameters and show new asymptotic distributional and tightness properties in a sparse setting.","We also show that estimates for zero parameters shrink with more iterations under suitable assumptions for tuning parameters.","We observe possible generalizations of this paper's results through simulations and will apply the estimation method in forecasting electricity consumption."],"url":"http://arxiv.org/abs/2402.13642v1","category":"math.ST"}
{"created":"2024-02-21 09:18:16","title":"Effective four-dimensional loop quantum black hole with a cosmological constant","abstract":"In this paper, we utilize the effective corrections of the $\\bar{\\mu}$-scheme in loop quantum black holes to obtain a 4-dimensional spherically symmetric metric with a cosmological constant. By imposing the areal gauge on the components of Ashtekar variables in the classical theory and applying the holonomy corrections, we derive the equations of motion, which can be solved to obtain the expression for the effective metric in the Painlev\\'{e}-Gullstrand coordinates. A comparison with the $\\Lambda=0$ case reveals minimal modifications near the outer horizon, while significant differences are observed far from the outer horizon. Moreover, the physical properties of these quantum-corrected solutions are also discussed.","sentences":["In this paper, we utilize the effective corrections of the $\\bar{\\mu}$-scheme in loop quantum black holes to obtain a 4-dimensional spherically symmetric metric with a cosmological constant.","By imposing the areal gauge on the components of Ashtekar variables in the classical theory and applying the holonomy corrections, we derive the equations of motion, which can be solved to obtain the expression for the effective metric in the Painlev\\'{e}-Gullstrand coordinates.","A comparison with the $\\Lambda=0$ case reveals minimal modifications near the outer horizon, while significant differences are observed far from the outer horizon.","Moreover, the physical properties of these quantum-corrected solutions are also discussed."],"url":"http://arxiv.org/abs/2402.13638v1","category":"gr-qc"}
{"created":"2024-02-21 09:15:46","title":"The METRIC-framework for assessing data quality for trustworthy AI in medicine: a systematic review","abstract":"The adoption of machine learning (ML) and, more specifically, deep learning (DL) applications into all major areas of our lives is underway. The development of trustworthy AI is especially important in medicine due to the large implications for patients' lives. While trustworthiness concerns various aspects including ethical, technical and privacy requirements, we focus on the importance of data quality (training/test) in DL. Since data quality dictates the behaviour of ML products, evaluating data quality will play a key part in the regulatory approval of medical AI products. We perform a systematic review following PRISMA guidelines using the databases PubMed and ACM Digital Library. We identify 2362 studies, out of which 62 records fulfil our eligibility criteria. From this literature, we synthesise the existing knowledge on data quality frameworks and combine it with the perspective of ML applications in medicine. As a result, we propose the METRIC-framework, a specialised data quality framework for medical training data comprising 15 awareness dimensions, along which developers of medical ML applications should investigate a dataset. This knowledge helps to reduce biases as a major source of unfairness, increase robustness, facilitate interpretability and thus lays the foundation for trustworthy AI in medicine. Incorporating such systematic assessment of medical datasets into regulatory approval processes has the potential to accelerate the approval of ML products and builds the basis for new standards.","sentences":["The adoption of machine learning (ML) and, more specifically, deep learning (DL) applications into all major areas of our lives is underway.","The development of trustworthy AI is especially important in medicine due to the large implications for patients' lives.","While trustworthiness concerns various aspects including ethical, technical and privacy requirements, we focus on the importance of data quality (training/test) in DL.","Since data quality dictates the behaviour of ML products, evaluating data quality will play a key part in the regulatory approval of medical AI products.","We perform a systematic review following PRISMA guidelines using the databases PubMed and ACM Digital Library.","We identify 2362 studies, out of which 62 records fulfil our eligibility criteria.","From this literature, we synthesise the existing knowledge on data quality frameworks and combine it with the perspective of ML applications in medicine.","As a result, we propose the METRIC-framework, a specialised data quality framework for medical training data comprising 15 awareness dimensions, along which developers of medical ML applications should investigate a dataset.","This knowledge helps to reduce biases as a major source of unfairness, increase robustness, facilitate interpretability and thus lays the foundation for trustworthy AI in medicine.","Incorporating such systematic assessment of medical datasets into regulatory approval processes has the potential to accelerate the approval of ML products and builds the basis for new standards."],"url":"http://arxiv.org/abs/2402.13635v1","category":"cs.LG"}
{"created":"2024-02-21 09:13:08","title":"Learning Dual-arm Object Rearrangement for Cartesian Robots","abstract":"This work focuses on the dual-arm object rearrangement problem abstracted from a realistic industrial scenario of Cartesian robots. The goal of this problem is to transfer all the objects from sources to targets with the minimum total completion time. To achieve the goal, the core idea is to develop an effective object-to-arm task assignment strategy for minimizing the cumulative task execution time and maximizing the dual-arm cooperation efficiency. One of the difficulties in the task assignment is the scalability problem. As the number of objects increases, the computation time of traditional offline-search-based methods grows strongly for computational complexity. Encouraged by the adaptability of reinforcement learning (RL) in long-sequence task decisions, we propose an online task assignment decision method based on RL, and the computation time of our method only increases linearly with the number of objects. Further, we design an attention-based network to model the dependencies between the input states during the whole task execution process to help find the most reasonable object-to-arm correspondence in each task assignment round. In the experimental part, we adapt some search-based methods to this specific setting and compare our method with them. Experimental result shows that our approach achieves outperformance over search-based methods in total execution time and computational efficiency, and also verifies the generalization of our method to different numbers of objects. In addition, we show the effectiveness of our method deployed on the real robot in the supplementary video.","sentences":["This work focuses on the dual-arm object rearrangement problem abstracted from a realistic industrial scenario of Cartesian robots.","The goal of this problem is to transfer all the objects from sources to targets with the minimum total completion time.","To achieve the goal, the core idea is to develop an effective object-to-arm task assignment strategy for minimizing the cumulative task execution time and maximizing the dual-arm cooperation efficiency.","One of the difficulties in the task assignment is the scalability problem.","As the number of objects increases, the computation time of traditional offline-search-based methods grows strongly for computational complexity.","Encouraged by the adaptability of reinforcement learning (RL) in long-sequence task decisions, we propose an online task assignment decision method based on RL, and the computation time of our method only increases linearly with the number of objects.","Further, we design an attention-based network to model the dependencies between the input states during the whole task execution process to help find the most reasonable object-to-arm correspondence in each task assignment round.","In the experimental part, we adapt some search-based methods to this specific setting and compare our method with them.","Experimental result shows that our approach achieves outperformance over search-based methods in total execution time and computational efficiency, and also verifies the generalization of our method to different numbers of objects.","In addition, we show the effectiveness of our method deployed on the real robot in the supplementary video."],"url":"http://arxiv.org/abs/2402.13634v1","category":"cs.RO"}
{"created":"2024-02-21 09:06:31","title":"UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural Language","abstract":"Foundation models like ChatGPT and GPT-4 have revolutionized artificial intelligence, exhibiting remarkable abilities to generalize across a wide array of tasks and applications beyond their initial training objectives. However, when this concept is applied to graph learning, a stark contrast emerges. Graph learning has predominantly focused on single-graph models, tailored to specific tasks or datasets, lacking the ability to transfer learned knowledge to different domains. This limitation stems from the inherent complexity and diversity of graph structures, along with the different feature and label spaces specific to graph data. In this paper, we present our UniGraph framework, designed to train a graph foundation model capable of generalizing to unseen graphs and tasks across diverse domains. Unlike single-graph models that use pre-computed node features of varying dimensions as input, our approach leverages Text-Attributed Graphs (TAGs) for unifying node representations. We propose a cascaded architecture of Language Models (LMs) and Graph Neural Networks (GNNs) as backbone networks with a self-supervised training objective based on Masked Graph Modeling (MGM). We introduce graph instruction tuning using Large Language Models (LLMs) to enable zero-shot prediction ability. Our comprehensive experiments across various graph learning tasks and domains demonstrate the model's effectiveness in self-supervised representation learning on unseen graphs, few-shot in-context transfer, and zero-shot transfer, even surpassing or matching the performance of GNNs that have undergone supervised training on target datasets.","sentences":["Foundation models like ChatGPT and GPT-4 have revolutionized artificial intelligence, exhibiting remarkable abilities to generalize across a wide array of tasks and applications beyond their initial training objectives.","However, when this concept is applied to graph learning, a stark contrast emerges.","Graph learning has predominantly focused on single-graph models, tailored to specific tasks or datasets, lacking the ability to transfer learned knowledge to different domains.","This limitation stems from the inherent complexity and diversity of graph structures, along with the different feature and label spaces specific to graph data.","In this paper, we present our UniGraph framework, designed to train a graph foundation model capable of generalizing to unseen graphs and tasks across diverse domains.","Unlike single-graph models that use pre-computed node features of varying dimensions as input, our approach leverages Text-Attributed Graphs (TAGs) for unifying node representations.","We propose a cascaded architecture of Language Models (LMs) and Graph Neural Networks (GNNs) as backbone networks with a self-supervised training objective based on Masked Graph Modeling (MGM).","We introduce graph instruction tuning using Large Language Models (LLMs) to enable zero-shot prediction ability.","Our comprehensive experiments across various graph learning tasks and domains demonstrate the model's effectiveness in self-supervised representation learning on unseen graphs, few-shot in-context transfer, and zero-shot transfer, even surpassing or matching the performance of GNNs that have undergone supervised training on target datasets."],"url":"http://arxiv.org/abs/2402.13630v1","category":"cs.LG"}
{"created":"2024-02-21 09:04:45","title":"Improving Building Temperature Forecasting: A Data-driven Approach with System Scenario Clustering","abstract":"Heat, Ventilation and Air Conditioning (HVAC) systems play a critical role in maintaining a comfortable thermal environment and cost approximately 40% of primary energy usage in the building sector. For smart energy management in buildings, usage patterns and their resulting profiles allow the improvement of control systems with prediction capabilities. However, for large-scale HVAC system management, it is difficult to construct a detailed model for each subsystem. In this paper, a new data-driven room temperature prediction model is proposed based on the k-means clustering method. The proposed data-driven temperature prediction approach extracts the system operation feature through historical data analysis and further simplifies the system-level model to improve generalization and computational efficiency. We evaluate the proposed approach in the real world. The results demonstrated that our approach can significantly reduce modeling time without reducing prediction accuracy.","sentences":["Heat, Ventilation and Air Conditioning (HVAC) systems play a critical role in maintaining a comfortable thermal environment and cost approximately 40% of primary energy usage in the building sector.","For smart energy management in buildings, usage patterns and their resulting profiles allow the improvement of control systems with prediction capabilities.","However, for large-scale HVAC system management, it is difficult to construct a detailed model for each subsystem.","In this paper, a new data-driven room temperature prediction model is proposed based on the k-means clustering method.","The proposed data-driven temperature prediction approach extracts the system operation feature through historical data analysis and further simplifies the system-level model to improve generalization and computational efficiency.","We evaluate the proposed approach in the real world.","The results demonstrated that our approach can significantly reduce modeling time without reducing prediction accuracy."],"url":"http://arxiv.org/abs/2402.13628v1","category":"cs.LG"}
{"created":"2024-02-21 09:02:39","title":"Algorithms for Claims Trading","abstract":"The recent banking crisis has again emphasized the importance of understanding and mitigating systemic risk in financial networks. In this paper, we study a market-driven approach to rescue a bank in distress based on the idea of claims trading, a notion defined in Chapter 11 of the U.S. Bankruptcy Code. We formalize the idea in the context of financial networks by Eisenberg and Noe. For two given banks v and w, we consider the operation that w takes over some claims of v and in return gives liquidity to v to ultimately rescue v. We study the structural properties and computational complexity of decision and optimization problems for several variants of claims trading.   When trading incoming edges of v, we show that there is no trade in which both banks v and w strictly improve their assets. We therefore consider creditor-positive trades, in which v profits strictly and w remains indifferent. For a given set C of incoming edges of v, we provide an efficient algorithm to compute payments by w that result in maximal assets of v. When the set C must also be chosen, the problem becomes weakly NP-hard. Our main result here is a bicriteria FPTAS to compute an approximate trade. The approximate trade results in nearly the optimal amount of assets of v in any exact trade. Our results extend to the case in which banks use general monotone payment functions and the emerging clearing state can be computed efficiently.   In contrast, for trading outgoing edges of v, the goal is to maximize the increase in assets for the creditors of v. Notably, for these results the characteristics of the payment functions of the banks are essential. For payments ranking creditors one by one, we show NP-hardness of approximation within a factor polynomial in the network size, when the set of claims C is part of the input or not. Instead, for proportional payments, our results indicate more favorable conditions.","sentences":["The recent banking crisis has again emphasized the importance of understanding and mitigating systemic risk in financial networks.","In this paper, we study a market-driven approach to rescue a bank in distress based on the idea of claims trading, a notion defined in Chapter 11 of the U.S. Bankruptcy Code.","We formalize the idea in the context of financial networks by Eisenberg and Noe.","For two given banks v and w, we consider the operation that w takes over some claims of v and in return gives liquidity to v to ultimately rescue v. We study the structural properties and computational complexity of decision and optimization problems for several variants of claims trading.   ","When trading incoming edges of v, we show that there is no trade in which both banks v and w strictly improve their assets.","We therefore consider creditor-positive trades, in which v profits strictly and w remains indifferent.","For a given set C of incoming edges of v, we provide an efficient algorithm to compute payments by w that result in maximal assets of v. When the set C must also be chosen, the problem becomes weakly NP-hard.","Our main result here is a bicriteria FPTAS to compute an approximate trade.","The approximate trade results in nearly the optimal amount of assets of v in any exact trade.","Our results extend to the case in which banks use general monotone payment functions and the emerging clearing state can be computed efficiently.   ","In contrast, for trading outgoing edges of v, the goal is to maximize the increase in assets for the creditors of v. Notably, for these results the characteristics of the payment functions of the banks are essential.","For payments ranking creditors one by one, we show NP-hardness of approximation within a factor polynomial in the network size, when the set of claims C is part of the input or not.","Instead, for proportional payments, our results indicate more favorable conditions."],"url":"http://arxiv.org/abs/2402.13627v1","category":"cs.GT"}
{"created":"2024-02-21 08:54:47","title":"MORE: Multi-mOdal REtrieval Augmented Generative Commonsense Reasoning","abstract":"Since commonsense information has been recorded significantly less frequently than its existence, language models pre-trained by text generation have difficulty to learn sufficient commonsense knowledge. Several studies have leveraged text retrieval to augment the models' commonsense ability. Unlike text, images capture commonsense information inherently but little effort has been paid to effectively utilize them. In this work, we propose a novel Multi-mOdal REtrieval (MORE) augmentation framework, to leverage both text and images to enhance the commonsense ability of language models. Extensive experiments on the Common-Gen task have demonstrated the efficacy of MORE based on the pre-trained models of both single and multiple modalities.","sentences":["Since commonsense information has been recorded significantly less frequently than its existence, language models pre-trained by text generation have difficulty to learn sufficient commonsense knowledge.","Several studies have leveraged text retrieval to augment the models' commonsense ability.","Unlike text, images capture commonsense information inherently but little effort has been paid to effectively utilize them.","In this work, we propose a novel Multi-mOdal REtrieval (MORE) augmentation framework, to leverage both text and images to enhance the commonsense ability of language models.","Extensive experiments on the Common-Gen task have demonstrated the efficacy of MORE based on the pre-trained models of both single and multiple modalities."],"url":"http://arxiv.org/abs/2402.13625v1","category":"cs.CL"}
{"created":"2024-02-21 08:53:04","title":"Towards Linear Spanners in All Temporal Cliques","abstract":"Many real-world networks, like transportation networks and social networks, are dynamic in the sense that the edge set may change over time, but these changes are known in advance. This behavior is captured by the temporal graphs model, which has recently become a trending topic in theoretical computer science. A core open problem in the field is to prove the existence of linear-size temporal spanners in temporal cliques, i.e., sparse subgraphs of complete temporal graphs that ensure all-pairs reachability via temporal paths. So far, the best known result is the existence of temporal spanners with $\\mathcal{O}(n\\log n)$ many edges. We present significant progress towards proving that linear-size temporal spanners exist in all temporal cliques.   We adapt techniques used in previous works and heavily expand and generalize them to provide a simpler and more intuitive proof of the $\\mathcal{O}(n\\log n)$ bound. Moreover, we use our novel approach to show that a large class of temporal cliques, called edge-pivot graphs, admit linear-size temporal spanners. To contrast this, we investigate other classes of temporal cliques that do not belong to the class of edge-pivot graphs. We introduce two such graph classes and we develop novel techniques for establishing the existence of linear temporal spanners in these graph classes as well.","sentences":["Many real-world networks, like transportation networks and social networks, are dynamic in the sense that the edge set may change over time, but these changes are known in advance.","This behavior is captured by the temporal graphs model, which has recently become a trending topic in theoretical computer science.","A core open problem in the field is to prove the existence of linear-size temporal spanners in temporal cliques, i.e., sparse subgraphs of complete temporal graphs that ensure all-pairs reachability via temporal paths.","So far, the best known result is the existence of temporal spanners with $\\mathcal{O}(n\\log n)$ many edges.","We present significant progress towards proving that linear-size temporal spanners exist in all temporal cliques.   ","We adapt techniques used in previous works and heavily expand and generalize them to provide a simpler and more intuitive proof of the $\\mathcal{O}(n\\log n)$ bound.","Moreover, we use our novel approach to show that a large class of temporal cliques, called edge-pivot graphs, admit linear-size temporal spanners.","To contrast this, we investigate other classes of temporal cliques that do not belong to the class of edge-pivot graphs.","We introduce two such graph classes and we develop novel techniques for establishing the existence of linear temporal spanners in these graph classes as well."],"url":"http://arxiv.org/abs/2402.13624v1","category":"cs.DM"}
{"created":"2024-02-21 08:50:33","title":"Analysis of Bootstrap and Subsampling in High-dimensional Regularized Regression","abstract":"We investigate popular resampling methods for estimating the uncertainty of statistical models, such as subsampling, bootstrap and the jackknife, and their performance in high-dimensional supervised regression tasks. We provide a tight asymptotic description of the biases and variances estimated by these methods in the context of generalized linear models, such as ridge and logistic regression, taking the limit where the number of samples $n$ and dimension $d$ of the covariates grow at a comparable fixed rate $\\alpha\\!=\\! n/d$. Our findings are three-fold: i) resampling methods are fraught with problems in high dimensions and exhibit the double-descent-like behavior typical of these situations; ii) only when $\\alpha$ is large enough do they provide consistent and reliable error estimations (we give convergence rates); iii) in the over-parametrized regime $\\alpha\\!<\\!1$ relevant to modern machine learning practice, their predictions are not consistent, even with optimal regularization.","sentences":["We investigate popular resampling methods for estimating the uncertainty of statistical models, such as subsampling, bootstrap and the jackknife, and their performance in high-dimensional supervised regression tasks.","We provide a tight asymptotic description of the biases and variances estimated by these methods in the context of generalized linear models, such as ridge and logistic regression, taking the limit where the number of samples $n$ and dimension $d$ of the covariates grow at a comparable fixed rate $\\alpha\\!=\\! n/d$. Our findings are three-fold: i) resampling methods are fraught with problems in high dimensions and exhibit the double-descent-like behavior typical of these situations; ii) only when $\\alpha$ is large enough do they provide consistent and reliable error estimations (we give convergence rates); iii) in the over-parametrized regime $\\alpha\\!<\\!1$ relevant to modern machine learning practice, their predictions are not consistent, even with optimal regularization."],"url":"http://arxiv.org/abs/2402.13622v1","category":"stat.ML"}
{"created":"2024-02-21 08:45:58","title":"Covariant projective representations of Hilbert-Lie groups","abstract":"Hilbert--Lie groups are Lie groups whose Lie algebra is a real Hilbert space whose scalar product is invariant under the adjoint action. These infinite-dimensional Lie groups are the closest relatives to compact Lie groups. Here we study unitary representations of these groups from various perspectives. First, we address norm-continuous, also called bounded, representations: they are well-known for simple groups, but the general picture is more complicated. Our first main result is a characterization of the discrete decomposability of all bounded representations in terms of boundedness of the set of coroots. We also show that bounded representations of type II and III exist if the set of coroots is unbounded. Second, we use covariance with respect to a one-parameter group of automorphisms to implement some regularity. Here we develop some perturbation theory based on half Lie groups that reduces matters to the case where a ``maximal torus'' is fixed, so that compatible weight decompositions can be studied. Third, we extend the context to projective representations which are covariant for a one-parameter group of automorphisms. Here important families of representations arise from ``bounded extremal weights'', and for these the corresponding central extensions can be determined explicitly, together with all one-parameter groups for which a covariant extension exists.","sentences":["Hilbert--Lie groups are Lie groups whose Lie algebra is a real Hilbert space whose scalar product is invariant under the adjoint action.","These infinite-dimensional Lie groups are the closest relatives to compact Lie groups.","Here we study unitary representations of these groups from various perspectives.","First, we address norm-continuous, also called bounded, representations: they are well-known for simple groups, but the general picture is more complicated.","Our first main result is a characterization of the discrete decomposability of all bounded representations in terms of boundedness of the set of coroots.","We also show that bounded representations of type II and III exist if the set of coroots is unbounded.","Second, we use covariance with respect to a one-parameter group of automorphisms to implement some regularity.","Here we develop some perturbation theory based on half Lie groups that reduces matters to the case where a ``maximal torus'' is fixed, so that compatible weight decompositions can be studied.","Third, we extend the context to projective representations which are covariant for a one-parameter group of automorphisms.","Here important families of representations arise from ``bounded extremal weights'', and for these the corresponding central extensions can be determined explicitly, together with all one-parameter groups for which a covariant extension exists."],"url":"http://arxiv.org/abs/2402.13619v1","category":"math-ph"}
{"created":"2024-02-21 08:44:32","title":"A Monolithic Cybersecurity Architecture for Power Electronic Systems","abstract":"Power electronic systems (PES) face significant threats from various data availability and integrity attacks, significantly affecting the performance of communication networks and power system operation. As a result, several attack detection and reconstruction techniques are deployed, which makes it a costly \\& complex cybersecurity operational platform with significant room for incremental extensions for mitigation against future threats. Unlike the said traditional arrangements, our paper introduces a foundational approach by establishing a monolithic cybersecurity architecture (MCA) via incorporating semantic principles into the sampling process for distributed energy resources (DERs). This unified approach concurrently compensates for the intrusion challenges posed by cyber attacks by reconstructing signals using the dynamics of the inner control layer. This reconstruction considers essential semantic attributes, like Priority, Freshness, and Relevance to ensure resilient dynamic performance. Hence, the proposed scheme promises a generalized route to concurrently tackle a global set of cyber attacks in elevating the resilience of PES. Finally, rigorous validation on a modified IEEE 69-bus distribution system and a real-world South California Edison (SCE) 47-bus network, using OPAL-RT under diverse operating conditions, underscores its robustness, model-free design capability, scalability, and adaptability to dynamic cyber graphs and system reconfiguration.","sentences":["Power electronic systems (PES) face significant threats from various data availability and integrity attacks, significantly affecting the performance of communication networks and power system operation.","As a result, several attack detection and reconstruction techniques are deployed, which makes it a costly \\& complex cybersecurity operational platform with significant room for incremental extensions for mitigation against future threats.","Unlike the said traditional arrangements, our paper introduces a foundational approach by establishing a monolithic cybersecurity architecture (MCA) via incorporating semantic principles into the sampling process for distributed energy resources (DERs).","This unified approach concurrently compensates for the intrusion challenges posed by cyber attacks by reconstructing signals using the dynamics of the inner control layer.","This reconstruction considers essential semantic attributes, like Priority, Freshness, and Relevance to ensure resilient dynamic performance.","Hence, the proposed scheme promises a generalized route to concurrently tackle a global set of cyber attacks in elevating the resilience of PES.","Finally, rigorous validation on a modified IEEE 69-bus distribution system and a real-world South California Edison (SCE) 47-bus network, using OPAL-RT under diverse operating conditions, underscores its robustness, model-free design capability, scalability, and adaptability to dynamic cyber graphs and system reconfiguration."],"url":"http://arxiv.org/abs/2402.13617v1","category":"eess.SY"}
{"created":"2024-02-21 08:42:53","title":"YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information","abstract":"Today's deep learning methods focus on how to design the most appropriate objective functions so that the prediction results of the model can be closest to the ground truth. Meanwhile, an appropriate architecture that can facilitate acquisition of enough information for prediction has to be designed. Existing methods ignore a fact that when input data undergoes layer-by-layer feature extraction and spatial transformation, large amount of information will be lost. This paper will delve into the important issues of data loss when data is transmitted through deep networks, namely information bottleneck and reversible functions. We proposed the concept of programmable gradient information (PGI) to cope with the various changes required by deep networks to achieve multiple objectives. PGI can provide complete input information for the target task to calculate objective function, so that reliable gradient information can be obtained to update network weights. In addition, a new lightweight network architecture -- Generalized Efficient Layer Aggregation Network (GELAN), based on gradient path planning is designed. GELAN's architecture confirms that PGI has gained superior results on lightweight models. We verified the proposed GELAN and PGI on MS COCO dataset based object detection. The results show that GELAN only uses conventional convolution operators to achieve better parameter utilization than the state-of-the-art methods developed based on depth-wise convolution. PGI can be used for variety of models from lightweight to large. It can be used to obtain complete information, so that train-from-scratch models can achieve better results than state-of-the-art models pre-trained using large datasets, the comparison results are shown in Figure 1. The source codes are at: https://github.com/WongKinYiu/yolov9.","sentences":["Today's deep learning methods focus on how to design the most appropriate objective functions so that the prediction results of the model can be closest to the ground truth.","Meanwhile, an appropriate architecture that can facilitate acquisition of enough information for prediction has to be designed.","Existing methods ignore a fact that when input data undergoes layer-by-layer feature extraction and spatial transformation, large amount of information will be lost.","This paper will delve into the important issues of data loss when data is transmitted through deep networks, namely information bottleneck and reversible functions.","We proposed the concept of programmable gradient information (PGI) to cope with the various changes required by deep networks to achieve multiple objectives.","PGI can provide complete input information for the target task to calculate objective function, so that reliable gradient information can be obtained to update network weights.","In addition, a new lightweight network architecture -- Generalized Efficient Layer Aggregation Network (GELAN), based on gradient path planning is designed.","GELAN's architecture confirms that PGI has gained superior results on lightweight models.","We verified the proposed GELAN and PGI on MS COCO dataset based object detection.","The results show that GELAN only uses conventional convolution operators to achieve better parameter utilization than the state-of-the-art methods developed based on depth-wise convolution.","PGI can be used for variety of models from lightweight to large.","It can be used to obtain complete information, so that train-from-scratch models can achieve better results than state-of-the-art models pre-trained using large datasets, the comparison results are shown in Figure 1.","The source codes are at: https://github.com/WongKinYiu/yolov9."],"url":"http://arxiv.org/abs/2402.13616v1","category":"cs.CV"}
{"created":"2024-02-21 08:40:04","title":"Analyizing the Conjunction Fallacy as a Fact","abstract":"Since the seminal paper by Tversky and Kahneman, the conjunction fallacy has been the subject of multiple debates and become a fundamental challenge for cognitive theories in decision-making. In this article, we take a rather uncommon perspective on this phenomenon. Instead of trying to explain the nature or causes of the conjunction fallacy (intensional definition), we analyze its range of factual possibilities (extensional definition). We show that the majority of research on the conjunction fallacy, according to our sample of experiments reviewed which covers literature between 1983 and 2016, has focused on a narrow part of the a priori factual possibilities, implying that explanations of the conjunction fallacy are fundamentally biased by the short scope of possibilities explored. The latter is a rather curious aspect of the research evolution in the conjunction fallacy considering that the very nature of it is motivated by extensional considerations.","sentences":["Since the seminal paper by Tversky and Kahneman, the conjunction fallacy has been the subject of multiple debates and become a fundamental challenge for cognitive theories in decision-making.","In this article, we take a rather uncommon perspective on this phenomenon.","Instead of trying to explain the nature or causes of the conjunction fallacy (intensional definition), we analyze its range of factual possibilities (extensional definition).","We show that the majority of research on the conjunction fallacy, according to our sample of experiments reviewed which covers literature between 1983 and 2016, has focused on a narrow part of the a priori factual possibilities, implying that explanations of the conjunction fallacy are fundamentally biased by the short scope of possibilities explored.","The latter is a rather curious aspect of the research evolution in the conjunction fallacy considering that the very nature of it is motivated by extensional considerations."],"url":"http://arxiv.org/abs/2402.13615v1","category":"cs.AI"}
{"created":"2024-02-21 08:29:11","title":"Catalogue of nearby blue and near-solar gas metallicity SDSS dwarf galaxies","abstract":"A less explored aspect of dwarf galaxies is their metallicity evolution. Generally, dwarfs have lower metallicities than Hubble sequence late type galaxies but in reality, dwarfs span a wide range of metallicities with several open questions regarding the formation and evolution of the lowest and the highest metallicity dwarfs. We present a catalogue of 3459 blue, nearby, star forming dwarf galaxies extracted from SDSS DR16 including calculation of their metallicities using the mean of several calibrators. To compile our catalogue we applied redshift, absolute magnitude, stellar mass, optical diameter, and line flux signal to noise criteria. This produced a catalogue from the upper end of the dwarf galaxy stellar mass range. Our catalogued dwarfs have blue g - i colours and Hbeta equivalent widths, indicative of having undergone a recent episode of star formation, although their star formation rates (SFR) suggest only a moderate to low enhancement in star formation, similar to the SFRs in low surface brightness and evolved tidal dwarfs. While the catalogued dwarfs cover a range of metallicities, their mean metallicity is about 0.2 dex below solar metallicity, indicating relatively chemically evolved galaxies. The vast majority of the catalogue, with clean photometry, are relatively isolated dwarfs with only modest star formation rates and a narrow range of g - i colour, consistent with internally driven episodic mild bursts of star formation. The presented catalogue's robust metallicity estimates for nearby SDSS dwarf galaxies will help target future studies to understand the physical processes driving the metallicity evolution of dwarfs.","sentences":["A less explored aspect of dwarf galaxies is their metallicity evolution.","Generally, dwarfs have lower metallicities than Hubble sequence late type galaxies but in reality, dwarfs span a wide range of metallicities with several open questions regarding the formation and evolution of the lowest and the highest metallicity dwarfs.","We present a catalogue of 3459 blue, nearby, star forming dwarf galaxies extracted from SDSS DR16 including calculation of their metallicities using the mean of several calibrators.","To compile our catalogue we applied redshift, absolute magnitude, stellar mass, optical diameter, and line flux signal to noise criteria.","This produced a catalogue from the upper end of the dwarf galaxy stellar mass range.","Our catalogued dwarfs have blue g - i colours and Hbeta equivalent widths, indicative of having undergone a recent episode of star formation, although their star formation rates (SFR) suggest only a moderate to low enhancement in star formation, similar to the SFRs in low surface brightness and evolved tidal dwarfs.","While the catalogued dwarfs cover a range of metallicities, their mean metallicity is about 0.2 dex below solar metallicity, indicating relatively chemically evolved galaxies.","The vast majority of the catalogue, with clean photometry, are relatively isolated dwarfs with only modest star formation rates and a narrow range of g - i colour, consistent with internally driven episodic mild bursts of star formation.","The presented catalogue's robust metallicity estimates for nearby SDSS dwarf galaxies will help target future studies to understand the physical processes driving the metallicity evolution of dwarfs."],"url":"http://arxiv.org/abs/2402.13612v1","category":"astro-ph.GA"}
{"created":"2024-02-21 08:26:43","title":"Data-driven Discovery with Large Generative Models","abstract":"With the accumulation of data at an unprecedented rate, its potential to fuel scientific discovery is growing exponentially. This position paper urges the Machine Learning (ML) community to exploit the capabilities of large generative models (LGMs) to develop automated systems for end-to-end data-driven discovery -- a paradigm encompassing the search and verification of hypotheses purely from a set of provided datasets, without the need for additional data collection or physical experiments. We first outline several desiderata for an ideal data-driven discovery system. Then, through DATAVOYAGER, a proof-of-concept utilizing GPT-4, we demonstrate how LGMs fulfill several of these desiderata -- a feat previously unattainable -- while also highlighting important limitations in the current system that open up opportunities for novel ML research. We contend that achieving accurate, reliable, and robust end-to-end discovery systems solely through the current capabilities of LGMs is challenging. We instead advocate for fail-proof tool integration, along with active user moderation through feedback mechanisms, to foster data-driven scientific discoveries with efficiency and reproducibility.","sentences":["With the accumulation of data at an unprecedented rate, its potential to fuel scientific discovery is growing exponentially.","This position paper urges the Machine Learning (ML) community to exploit the capabilities of large generative models (LGMs) to develop automated systems for end-to-end data-driven discovery -- a paradigm encompassing the search and verification of hypotheses purely from a set of provided datasets, without the need for additional data collection or physical experiments.","We first outline several desiderata for an ideal data-driven discovery system.","Then, through DATAVOYAGER, a proof-of-concept utilizing GPT-4, we demonstrate how LGMs fulfill several of these desiderata -- a feat previously unattainable -- while also highlighting important limitations in the current system that open up opportunities for novel ML research.","We contend that achieving accurate, reliable, and robust end-to-end discovery systems solely through the current capabilities of LGMs is challenging.","We instead advocate for fail-proof tool integration, along with active user moderation through feedback mechanisms, to foster data-driven scientific discoveries with efficiency and reproducibility."],"url":"http://arxiv.org/abs/2402.13610v1","category":"cs.CL"}
{"created":"2024-02-21 08:22:46","title":"VOOM: Robust Visual Object Odometry and Mapping using Hierarchical Landmarks","abstract":"In recent years, object-oriented simultaneous localization and mapping (SLAM) has attracted increasing attention due to its ability to provide high-level semantic information while maintaining computational efficiency. Some researchers have attempted to enhance localization accuracy by integrating the modeled object residuals into bundle adjustment. However, few have demonstrated better results than feature-based visual SLAM systems, as the generic coarse object models, such as cuboids or ellipsoids, are less accurate than feature points. In this paper, we propose a Visual Object Odometry and Mapping framework VOOM using high-level objects and low-level points as the hierarchical landmarks in a coarse-to-fine manner instead of directly using object residuals in bundle adjustment. Firstly, we introduce an improved observation model and a novel data association method for dual quadrics, employed to represent physical objects. It facilitates the creation of a 3D map that closely reflects reality. Next, we use object information to enhance the data association of feature points and consequently update the map. In the visual object odometry backend, the updated map is employed to further optimize the camera pose and the objects. Meanwhile, local bundle adjustment is performed utilizing the objects and points-based covisibility graphs in our visual object mapping process. Experiments show that VOOM outperforms both object-oriented SLAM and feature points SLAM systems such as ORB-SLAM2 in terms of localization. The implementation of our method is available at https://github.com/yutongwangBIT/VOOM.git.","sentences":["In recent years, object-oriented simultaneous localization and mapping (SLAM) has attracted increasing attention due to its ability to provide high-level semantic information while maintaining computational efficiency.","Some researchers have attempted to enhance localization accuracy by integrating the modeled object residuals into bundle adjustment.","However, few have demonstrated better results than feature-based visual SLAM systems, as the generic coarse object models, such as cuboids or ellipsoids, are less accurate than feature points.","In this paper, we propose a Visual Object Odometry and Mapping framework VOOM using high-level objects and low-level points as the hierarchical landmarks in a coarse-to-fine manner instead of directly using object residuals in bundle adjustment.","Firstly, we introduce an improved observation model and a novel data association method for dual quadrics, employed to represent physical objects.","It facilitates the creation of a 3D map that closely reflects reality.","Next, we use object information to enhance the data association of feature points and consequently update the map.","In the visual object odometry backend, the updated map is employed to further optimize the camera pose and the objects.","Meanwhile, local bundle adjustment is performed utilizing the objects and points-based covisibility graphs in our visual object mapping process.","Experiments show that VOOM outperforms both object-oriented SLAM and feature points SLAM systems such as ORB-SLAM2 in terms of localization.","The implementation of our method is available at https://github.com/yutongwangBIT/VOOM.git."],"url":"http://arxiv.org/abs/2402.13609v1","category":"cs.RO"}
{"created":"2024-02-21 08:20:06","title":"A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models","abstract":"The tendency of Large Language Models to generate hallucinations and exhibit overconfidence in predictions raises concerns regarding their reliability. Confidence or uncertainty estimations indicating the extent of trustworthiness of a model's response are essential to developing reliable AI systems. Current research primarily focuses on LLM confidence estimations in English, remaining a void for other widely used languages and impeding the global development of reliable AI applications. This paper introduces a comprehensive investigation of Multi-lingual confidence estimation (MlingConf) on LLMs. First, we introduce an elaborated and expert-checked multilingual QA dataset. Second, we delve into the performance of confidence estimations and examine how these confidence scores can enhance LLM performance through self-refinement across diverse languages. Finally, we propose a cross-lingual confidence estimation method to achieve more precise confidence scores. The experimental results showcase the performance of various confidence estimation methods across different languages as well as present that our proposed cross-lingual confidence estimation technique significantly enhances confidence estimation and outperforms several baseline methods.","sentences":["The tendency of Large Language Models to generate hallucinations and exhibit overconfidence in predictions raises concerns regarding their reliability.","Confidence or uncertainty estimations indicating the extent of trustworthiness of a model's response are essential to developing reliable AI systems.","Current research primarily focuses on LLM confidence estimations in English, remaining a void for other widely used languages and impeding the global development of reliable AI applications.","This paper introduces a comprehensive investigation of Multi-lingual confidence estimation (MlingConf) on LLMs.","First, we introduce an elaborated and expert-checked multilingual QA dataset.","Second, we delve into the performance of confidence estimations and examine how these confidence scores can enhance LLM performance through self-refinement across diverse languages.","Finally, we propose a cross-lingual confidence estimation method to achieve more precise confidence scores.","The experimental results showcase the performance of various confidence estimation methods across different languages as well as present that our proposed cross-lingual confidence estimation technique significantly enhances confidence estimation and outperforms several baseline methods."],"url":"http://arxiv.org/abs/2402.13606v1","category":"cs.CL"}
{"created":"2024-02-21 08:09:51","title":"Coding Theorems for Repetition and Superposition Codes over Binary-Input Output-Symmetric Channels","abstract":"This paper is concerned with a class of low density generator matrix codes (LDGM), called repetition and superposition (RaS) codes, which have been proved to be capacity-achieving over binary-input output-symmetric (BIOS) channels in terms of bit-error rate (BER). We prove with a recently proposed framework that the RaS codes are also capacity-achieving over BIOS channels in terms of frame-error rate (FER). With this new framework, the theorem for the RaS codes can be generalized to source coding and joint source and channel coding (JSCC). In particular, we prove with this framework that the corresponding low-density parity-check (LDPC) codes, as an enlarged ensemble of quasi-cyclic LDPC (QC-LDPC) codes, can also achieve the capacity. To further improve the iterative decoding performance, we consider the convolutional RaS (Conv-RaS) code ensemble and prove it to be capacity-achieving over BIOS channels in terms of the first error event probability. The construction of Conv-RaS codes is flexible with rate (defined as the ratio of the input length to the encoding output length) ranging from less than one (typically for channel codes) to greater than one (typically for source codes), which can be implemented as a universal JSCC scheme, as confirmed by simulations.","sentences":["This paper is concerned with a class of low density generator matrix codes (LDGM), called repetition and superposition (RaS) codes, which have been proved to be capacity-achieving over binary-input output-symmetric (BIOS) channels in terms of bit-error rate (BER).","We prove with a recently proposed framework that the RaS codes are also capacity-achieving over BIOS channels in terms of frame-error rate (FER).","With this new framework, the theorem for the RaS codes can be generalized to source coding and joint source and channel coding (JSCC).","In particular, we prove with this framework that the corresponding low-density parity-check (LDPC) codes, as an enlarged ensemble of quasi-cyclic LDPC (QC-LDPC) codes, can also achieve the capacity.","To further improve the iterative decoding performance, we consider the convolutional RaS (Conv-RaS) code ensemble and prove it to be capacity-achieving over BIOS channels in terms of the first error event probability.","The construction of Conv-RaS codes is flexible with rate (defined as the ratio of the input length to the encoding output length) ranging from less than one (typically for channel codes) to greater than one (typically for source codes), which can be implemented as a universal JSCC scheme, as confirmed by simulations."],"url":"http://arxiv.org/abs/2402.13603v1","category":"cs.IT"}
{"created":"2024-02-21 08:09:05","title":"Hybrid Reasoning Based on Large Language Models for Autonomous Car Driving","abstract":"Large Language Models (LLMs) have garnered significant attention for their ability to understand text and images, generate human-like text, and perform complex reasoning tasks. However, their ability to generalize this advanced reasoning with a combination of natural language text for decision-making in dynamic situations requires further exploration. In this study, we investigate how well LLMs can adapt and apply a combination of arithmetic and common-sense reasoning, particularly in autonomous driving scenarios. We hypothesize that LLMs hybrid reasoning abilities can improve autonomous driving by enabling them to analyze detected object and sensor data, understand driving regulations and physical laws, and offer additional context. This addresses complex scenarios, like decisions in low visibility (due to weather conditions), where traditional methods might fall short. We evaluated Large Language Models (LLMs) based on accuracy by comparing their answers with human-generated ground truth inside CARLA. The results showed that when a combination of images (detected objects) and sensor data is fed into the LLM, it can offer precise information for brake and throttle control in autonomous vehicles across various weather conditions. This formulation and answers can assist in decision-making for auto-pilot systems.","sentences":["Large Language Models (LLMs) have garnered significant attention for their ability to understand text and images, generate human-like text, and perform complex reasoning tasks.","However, their ability to generalize this advanced reasoning with a combination of natural language text for decision-making in dynamic situations requires further exploration.","In this study, we investigate how well LLMs can adapt and apply a combination of arithmetic and common-sense reasoning, particularly in autonomous driving scenarios.","We hypothesize that LLMs hybrid reasoning abilities can improve autonomous driving by enabling them to analyze detected object and sensor data, understand driving regulations and physical laws, and offer additional context.","This addresses complex scenarios, like decisions in low visibility (due to weather conditions), where traditional methods might fall short.","We evaluated Large Language Models (LLMs) based on accuracy by comparing their answers with human-generated ground truth inside CARLA.","The results showed that when a combination of images (detected objects) and sensor data is fed into the LLM, it can offer precise information for brake and throttle control in autonomous vehicles across various weather conditions.","This formulation and answers can assist in decision-making for auto-pilot systems."],"url":"http://arxiv.org/abs/2402.13602v1","category":"cs.CV"}
{"created":"2024-02-21 08:04:16","title":"Approximation and estimation of scale functions for spectrally negative Levy processes","abstract":"The scale function holds significant importance within the fluctuation theory of Levy processes, particularly in addressing exit problems. However, its definition is established through the Laplace transform, thereby lacking explicit representations in general. This paper introduces a novel series representation for this scale function, employing Laguerre polynomials to construct a uniformly convergent approximate sequence. Additionally, we derive statistical inference based on specific discrete observations, presenting estimators of scale functions that are asymptotically normal.","sentences":["The scale function holds significant importance within the fluctuation theory of Levy processes, particularly in addressing exit problems.","However, its definition is established through the Laplace transform, thereby lacking explicit representations in general.","This paper introduces a novel series representation for this scale function, employing Laguerre polynomials to construct a uniformly convergent approximate sequence.","Additionally, we derive statistical inference based on specific discrete observations, presenting estimators of scale functions that are asymptotically normal."],"url":"http://arxiv.org/abs/2402.13599v1","category":"math.ST"}
{"created":"2024-02-21 08:03:27","title":"User-LLM: Efficient LLM Contextualization with User Embeddings","abstract":"Large language models (LLMs) have revolutionized natural language processing. However, effectively incorporating complex and potentially noisy user interaction data remains a challenge. To address this, we propose User-LLM, a novel framework that leverages user embeddings to contextualize LLMs. These embeddings, distilled from diverse user interactions using self-supervised pretraining, capture latent user preferences and their evolution over time. We integrate these user embeddings with LLMs through cross-attention and soft-prompting, enabling LLMs to dynamically adapt to user context. Our comprehensive experiments on MovieLens, Amazon Review, and Google Local Review datasets demonstrate significant performance gains across various tasks. Notably, our approach outperforms text-prompt-based contextualization on long sequence tasks and tasks that require deep user understanding while being computationally efficient. We further incorporate Perceiver layers to streamline the integration between user encoders and LLMs, reducing computational demands.","sentences":["Large language models (LLMs) have revolutionized natural language processing.","However, effectively incorporating complex and potentially noisy user interaction data remains a challenge.","To address this, we propose User-LLM, a novel framework that leverages user embeddings to contextualize LLMs.","These embeddings, distilled from diverse user interactions using self-supervised pretraining, capture latent user preferences and their evolution over time.","We integrate these user embeddings with LLMs through cross-attention and soft-prompting, enabling LLMs to dynamically adapt to user context.","Our comprehensive experiments on MovieLens, Amazon Review, and Google Local Review datasets demonstrate significant performance gains across various tasks.","Notably, our approach outperforms text-prompt-based contextualization on long sequence tasks and tasks that require deep user understanding while being computationally efficient.","We further incorporate Perceiver layers to streamline the integration between user encoders and LLMs, reducing computational demands."],"url":"http://arxiv.org/abs/2402.13598v1","category":"cs.CL"}
{"created":"2024-02-21 07:57:28","title":"Constraining anisotropic universe under $f(R,T)$ theory of gravity","abstract":"We try to find the possibility of a Bianchi V universe in the modified gravitational field theory of $f(R,T)$. We have considered a Lagrangian model in the connection between the trace of the energy-momentum tensor $T$ and the Ricci scalar $R$. In order to solve the field equations a power law for the scaling factor was also considered. To make a comparison of the model parameters with the observational data, we put constraints on the model under the datasets of the Hubble parameter, Baryon Acoustic Oscillations, Pantheon, joint datasets of Hubble parameter + Pantheon, and collective datasets of the Hubble parameter + Baryon Acoustic Oscillations + Pantheon. The outcomes for the Hubble parameter in the present epoch are reasonably acceptable, especially since our estimation of this $H_0$ is remarkably consistent with various recent Planck Collaboration studies that utilize the $\\Lambda$-CDM model.","sentences":["We try to find the possibility of a Bianchi V universe in the modified gravitational field theory of $f(R,T)$. We have considered a Lagrangian model in the connection between the trace of the energy-momentum tensor $T$ and the Ricci scalar $R$.","In order to solve the field equations a power law for the scaling factor was also considered.","To make a comparison of the model parameters with the observational data, we put constraints on the model under the datasets of the Hubble parameter, Baryon Acoustic Oscillations, Pantheon, joint datasets of Hubble parameter + Pantheon, and collective datasets of the Hubble parameter + Baryon Acoustic Oscillations + Pantheon.","The outcomes for the Hubble parameter in the present epoch are reasonably acceptable, especially since our estimation of this $H_0$ is remarkably consistent with various recent Planck Collaboration studies that utilize the $\\Lambda$-CDM model."],"url":"http://arxiv.org/abs/2402.13596v1","category":"gr-qc"}
{"created":"2024-02-21 07:55:33","title":"A cutting plane algorithm for globally solving low dimensional k-means clustering problems","abstract":"Clustering is one of the most fundamental tools in data science and machine learning, and k-means clustering is one of the most common such methods. There is a variety of approximate algorithms for the k-means problem, but computing the globally optimal solution is in general NP-hard. In this paper we consider the k-means problem for instances with low dimensional data and formulate it as a structured concave assignment problem. This allows us to exploit the low dimensional structure and solve the problem to global optimality within reasonable time for large data sets with several clusters. The method builds on iteratively solving a small concave problem and a large linear programming problem. This gives a sequence of feasible solutions along with bounds which we show converges to zero optimality gap. The paper combines methods from global optimization theory to accelerate the procedure, and we provide numerical results on their performance.","sentences":["Clustering is one of the most fundamental tools in data science and machine learning, and k-means clustering is one of the most common such methods.","There is a variety of approximate algorithms for the k-means problem, but computing the globally optimal solution is in general NP-hard.","In this paper we consider the k-means problem for instances with low dimensional data and formulate it as a structured concave assignment problem.","This allows us to exploit the low dimensional structure and solve the problem to global optimality within reasonable time for large data sets with several clusters.","The method builds on iteratively solving a small concave problem and a large linear programming problem.","This gives a sequence of feasible solutions along with bounds which we show converges to zero optimality gap.","The paper combines methods from global optimization theory to accelerate the procedure, and we provide numerical results on their performance."],"url":"http://arxiv.org/abs/2402.13595v1","category":"math.OC"}
{"created":"2024-02-21 07:52:26","title":"Knowledge Graph Enhanced Large Language Model Editing","abstract":"Large language models (LLMs) are pivotal in advancing natural language processing (NLP) tasks, yet their efficacy is hampered by inaccuracies and outdated knowledge. Model editing emerges as a promising solution to address these challenges. However, existing editing methods struggle to track and incorporate changes in knowledge associated with edits, which limits the generalization ability of postedit LLMs in processing edited knowledge. To tackle these problems, we propose a novel model editing method that leverages knowledge graphs for enhancing LLM editing, namely GLAME. Specifically, we first utilize a knowledge graph augmentation module to uncover associated knowledge that has changed due to editing, obtaining its internal representations within LLMs. This approach allows knowledge alterations within LLMs to be reflected through an external graph structure. Subsequently, we design a graph-based knowledge edit module to integrate structured knowledge into the model editing. This ensures that the updated parameters reflect not only the modifications of the edited knowledge but also the changes in other associated knowledge resulting from the editing process. Comprehensive experiments conducted on GPT-J and GPT-2 XL demonstrate that GLAME significantly improves the generalization capabilities of post-edit LLMs in employing edited knowledge.","sentences":["Large language models (LLMs) are pivotal in advancing natural language processing (NLP) tasks, yet their efficacy is hampered by inaccuracies and outdated knowledge.","Model editing emerges as a promising solution to address these challenges.","However, existing editing methods struggle to track and incorporate changes in knowledge associated with edits, which limits the generalization ability of postedit LLMs in processing edited knowledge.","To tackle these problems, we propose a novel model editing method that leverages knowledge graphs for enhancing LLM editing, namely GLAME.","Specifically, we first utilize a knowledge graph augmentation module to uncover associated knowledge that has changed due to editing, obtaining its internal representations within LLMs.","This approach allows knowledge alterations within LLMs to be reflected through an external graph structure.","Subsequently, we design a graph-based knowledge edit module to integrate structured knowledge into the model editing.","This ensures that the updated parameters reflect not only the modifications of the edited knowledge but also the changes in other associated knowledge resulting from the editing process.","Comprehensive experiments conducted on GPT-J and GPT-2 XL demonstrate that GLAME significantly improves the generalization capabilities of post-edit LLMs in employing edited knowledge."],"url":"http://arxiv.org/abs/2402.13593v1","category":"cs.CL"}
{"created":"2024-02-21 18:44:38","title":"Information Elicitation in Agency Games","abstract":"Rapid progress in scalable, commoditized tools for data collection and data processing has made it possible for firms and policymakers to employ ever more complex metrics as guides for decision-making. These developments have highlighted a prevailing challenge -- deciding *which* metrics to compute. In particular, a firm's ability to compute a wider range of existing metrics does not address the problem of *unknown unknowns*, which reflects informational limitations on the part of the firm. To guide the choice of metrics in the face of this informational problem, we turn to the evaluated agents themselves, who may have more information than a principal about how to measure outcomes effectively. We model this interaction as a simple agency game, where we ask: *When does an agent have an incentive to reveal the observability of a cost-correlated variable to the principal?* There are two effects: better information reduces the agent's information rents but also makes some projects go forward that otherwise would fail. We show that the agent prefers to reveal information that exposes a strong enough differentiation between high and low costs. Expanding the agent's action space to include the ability to *garble* their information, we show that the agent often prefers to garble over full revelation. Still, giving the agent the ability to garble can lead to higher total welfare. Our model has analogies with price discrimination, and we leverage some of these synergies to analyze total welfare.","sentences":["Rapid progress in scalable, commoditized tools for data collection and data processing has made it possible for firms and policymakers to employ ever more complex metrics as guides for decision-making.","These developments have highlighted a prevailing challenge -- deciding *which* metrics to compute.","In particular, a firm's ability to compute a wider range of existing metrics does not address the problem of *unknown unknowns*, which reflects informational limitations on the part of the firm.","To guide the choice of metrics in the face of this informational problem, we turn to the evaluated agents themselves, who may have more information than a principal about how to measure outcomes effectively.","We model this interaction as a simple agency game, where we ask: *When does an agent have an incentive to reveal the observability of a cost-correlated variable to the principal?","*","There are two effects: better information reduces the agent's information rents but also makes some projects go forward that otherwise would fail.","We show that the agent prefers to reveal information that exposes a strong enough differentiation between high and low costs.","Expanding the agent's action space to include the ability to *garble* their information, we show that the agent often prefers to garble over full revelation.","Still, giving the agent the ability to garble can lead to higher total welfare.","Our model has analogies with price discrimination, and we leverage some of these synergies to analyze total welfare."],"url":"http://arxiv.org/abs/2402.14005v1","category":"cs.GT"}
{"created":"2024-02-21 16:22:21","title":"Leveraging Collection-Wide Similarities for Unsupervised Document Structure Extraction","abstract":"Document collections of various domains, e.g., legal, medical, or financial, often share some underlying collection-wide structure, which captures information that can aid both human users and structure-aware models. We propose to identify the typical structure of document within a collection, which requires to capture recurring topics across the collection, while abstracting over arbitrary header paraphrases, and ground each topic to respective document locations. These requirements pose several challenges: headers that mark recurring topics frequently differ in phrasing, certain section headers are unique to individual documents and do not reflect the typical structure, and the order of topics can vary between documents. Subsequently, we develop an unsupervised graph-based method which leverages both inter- and intra-document similarities, to extract the underlying collection-wide structure. Our evaluations on three diverse domains in both English and Hebrew indicate that our method extracts meaningful collection-wide structure, and we hope that future work will leverage our method for multi-document applications and structure-aware models.","sentences":["Document collections of various domains, e.g., legal, medical, or financial, often share some underlying collection-wide structure, which captures information that can aid both human users and structure-aware models.","We propose to identify the typical structure of document within a collection, which requires to capture recurring topics across the collection, while abstracting over arbitrary header paraphrases, and ground each topic to respective document locations.","These requirements pose several challenges: headers that mark recurring topics frequently differ in phrasing, certain section headers are unique to individual documents and do not reflect the typical structure, and the order of topics can vary between documents.","Subsequently, we develop an unsupervised graph-based method which leverages both inter- and intra-document similarities, to extract the underlying collection-wide structure.","Our evaluations on three diverse domains in both English and Hebrew indicate that our method extracts meaningful collection-wide structure, and we hope that future work will leverage our method for multi-document applications and structure-aware models."],"url":"http://arxiv.org/abs/2402.13906v1","category":"cs.CL"}
{"created":"2024-02-21 15:13:00","title":"Measurement of energy correlators inside jets and determination of the strong coupling $\u03b1_\\mathrm{S}(m_\\mathrm{Z})$","abstract":"Energy correlators that describe energy-weighted distances between two or three particles in a jet are measured using an event sample of $\\sqrt{s}$ = 13 TeV proton-proton collisions collected by the CMS experiment and corresponding to an integrated luminosity of 36.3 fb$^{-1}$. The measured distributions reveal two key features of the strong interaction: confinement and asymptotic freedom. By comparing the ratio of the two measured distributions with theoretical calculations that resum collinear emissions at approximate next-to-next-to-leading logarithmic accuracy matched to a next-to-leading order calculation, the strong coupling is determined at the Z boson mass: $\\alpha_\\mathrm{S}(m_\\mathrm{Z})$ = 0.1229$^{+0.0040}_{-0.0050}$, the most precise $\\alpha_\\mathrm{S}(m_\\mathrm{Z})$ value obtained using jet substructure observables.","sentences":["Energy correlators that describe energy-weighted distances between two or three particles in a jet are measured using an event sample of $\\sqrt{s}$ = 13 TeV proton-proton collisions collected by the CMS experiment and corresponding to an integrated luminosity of 36.3 fb$^{-1}$. The measured distributions reveal two key features of the strong interaction: confinement and asymptotic freedom.","By comparing the ratio of the two measured distributions with theoretical calculations that resum collinear emissions at approximate next-to-next-to-leading logarithmic accuracy matched to a next-to-leading order calculation, the strong coupling is determined at the Z boson mass: $\\alpha_\\mathrm{S}(m_\\mathrm{Z})$ = 0.1229$^{+0.0040}_{-0.0050}$, the most precise $\\alpha_\\mathrm{S}(m_\\mathrm{Z})$ value obtained using jet substructure observables."],"url":"http://arxiv.org/abs/2402.13864v1","category":"hep-ex"}
{"created":"2024-02-21 14:59:49","title":"What we can learn from TikTok through its Research API","abstract":"TikTok is a social media platform that has gained immense popularity over the last few years, particularly among younger demographics, due to the viral trends and challenges shared worldwide. The recent release of a free Research API opens doors to collect data on posted videos, associated comments, and user activities. Our study focuses on evaluating the reliability of results returned by the Research API, by collecting and analyzing a random sample of TikTok videos posted in a span of 6 years. Our preliminary results are instrumental for future research that aims to study the platform, highlighting caveats on the geographical distribution of videos and on the global prevalence of viral hashtags.","sentences":["TikTok is a social media platform that has gained immense popularity over the last few years, particularly among younger demographics, due to the viral trends and challenges shared worldwide.","The recent release of a free Research API opens doors to collect data on posted videos, associated comments, and user activities.","Our study focuses on evaluating the reliability of results returned by the Research API, by collecting and analyzing a random sample of TikTok videos posted in a span of 6 years.","Our preliminary results are instrumental for future research that aims to study the platform, highlighting caveats on the geographical distribution of videos and on the global prevalence of viral hashtags."],"url":"http://arxiv.org/abs/2402.13855v1","category":"cs.CY"}
{"created":"2024-02-21 14:29:27","title":"Design of a Miniature Underwater Vehicle and Data Collection System for Indoor Experimentation","abstract":"This paper describes the design of a miniature uncrewed underwater vehicle (MiniUUV) and related instrumentation for indoor experimentation. The MiniUUV was developed using 3D printed components and low-cost, off-the-shelf electronics. The vehicle uses a propeller differential propulsion drive and a peristaltic pump with a syringe for buoyancy control. A water tank with an overhead camera system was constructed to allow for convenient indoor data collection in a controlled environment. Several tests were conducted to demonstrate the capabilities of the MiniUUV and data collection system, including buoyancy pump actuation tests and straight line, circular, and zig-zag motion tests on the surface. During each planar motion test an AprilTag was attached to the MiniUUV and an overhead camera system obtained video recordings that were processed offline to estimate vehicle position, surge velocity, sway velocity, yaw angle, and yaw rate.","sentences":["This paper describes the design of a miniature uncrewed underwater vehicle (MiniUUV) and related instrumentation for indoor experimentation.","The MiniUUV was developed using 3D printed components and low-cost, off-the-shelf electronics.","The vehicle uses a propeller differential propulsion drive and a peristaltic pump with a syringe for buoyancy control.","A water tank with an overhead camera system was constructed to allow for convenient indoor data collection in a controlled environment.","Several tests were conducted to demonstrate the capabilities of the MiniUUV and data collection system, including buoyancy pump actuation tests and straight line, circular, and zig-zag motion tests on the surface.","During each planar motion test an AprilTag was attached to the MiniUUV and an overhead camera system obtained video recordings that were processed offline to estimate vehicle position, surge velocity, sway velocity, yaw angle, and yaw rate."],"url":"http://arxiv.org/abs/2402.13837v1","category":"cs.RO"}
{"created":"2024-02-21 13:53:25","title":"An Empirical Study on Oculus Virtual Reality Applications: Security and Privacy Perspectives","abstract":"Although Virtual Reality (VR) has accelerated its prevalent adoption in emerging metaverse applications, it is not a fundamentally new technology. On one hand, most VR operating systems (OS) are based on off-the-shelf mobile OS. As a result, VR apps also inherit privacy and security deficiencies from conventional mobile apps. On the other hand, in contrast to conventional mobile apps, VR apps can achieve immersive experience via diverse VR devices, such as head-mounted displays, body sensors, and controllers though achieving this requires the extensive collection of privacy-sensitive human biometrics. Moreover, VR apps have been typically implemented by 3D gaming engines (e.g., Unity), which also contain intrinsic security vulnerabilities. Inappropriate use of these technologies may incur privacy leaks and security vulnerabilities although these issues have not received significant attention compared to the proliferation of diverse VR apps. In this paper, we develop a security and privacy assessment tool, namely the VR-SP detector for VR apps. The VR-SP detector has integrated program static analysis tools and privacy-policy analysis methods. Using the VR-SP detector, we conduct a comprehensive empirical study on 500 popular VR apps. We obtain the original apps from the popular Oculus and SideQuest app stores and extract APK files via the Meta Oculus Quest 2 device. We evaluate security vulnerabilities and privacy data leaks of these VR apps by VR app analysis, taint analysis, and privacy-policy analysis. We find that a number of security vulnerabilities and privacy leaks widely exist in VR apps. Moreover, our results also reveal conflicting representations in the privacy policies of these apps and inconsistencies of the actual data collection with the privacy-policy statements of the apps. Based on these findings, we make suggestions for the future development of VR apps.","sentences":["Although Virtual Reality (VR) has accelerated its prevalent adoption in emerging metaverse applications, it is not a fundamentally new technology.","On one hand, most VR operating systems (OS) are based on off-the-shelf mobile OS.","As a result, VR apps also inherit privacy and security deficiencies from conventional mobile apps.","On the other hand, in contrast to conventional mobile apps, VR apps can achieve immersive experience via diverse VR devices, such as head-mounted displays, body sensors, and controllers though achieving this requires the extensive collection of privacy-sensitive human biometrics.","Moreover, VR apps have been typically implemented by 3D gaming engines (e.g., Unity), which also contain intrinsic security vulnerabilities.","Inappropriate use of these technologies may incur privacy leaks and security vulnerabilities although these issues have not received significant attention compared to the proliferation of diverse VR apps.","In this paper, we develop a security and privacy assessment tool, namely the VR-SP detector for VR apps.","The VR-SP detector has integrated program static analysis tools and privacy-policy analysis methods.","Using the VR-SP detector, we conduct a comprehensive empirical study on 500 popular VR apps.","We obtain the original apps from the popular Oculus and SideQuest app stores and extract APK files via the Meta Oculus Quest 2 device.","We evaluate security vulnerabilities and privacy data leaks of these VR apps by VR app analysis, taint analysis, and privacy-policy analysis.","We find that a number of security vulnerabilities and privacy leaks widely exist in VR apps.","Moreover, our results also reveal conflicting representations in the privacy policies of these apps and inconsistencies of the actual data collection with the privacy-policy statements of the apps.","Based on these findings, we make suggestions for the future development of VR apps."],"url":"http://arxiv.org/abs/2402.13815v1","category":"cs.SE"}
{"created":"2024-02-21 13:45:07","title":"A search for bottom-type vector-like quark pair production in dileptonic and fully hadronic final states in proton-proton collisions at $\\sqrt{s}$ = 13 TeV","abstract":"A search is described for the production of a pair of bottom-type vector-like quarks (B VLQs) with mass greater than 1000 GeV. Each B VLQ decays into a b quark and a Higgs boson, a b quark and a Z boson, or a t quark and a W boson. This analysis considers both fully hadronic final states and those containing a charged lepton pair from a Z boson decay. The products of the H $to$ bb boson decay and of the hadronic Z or W boson decays can be resolved as two distinct jets or merged into a single jet, so the final states are classified by the number of reconstructed jets. The analysis uses data corresponding to an integrated luminosity of 138 fb$^{-1}$ collected in proton-proton collisions at $\\sqrt{s}$ = 13 TeV with the CMS detector at the LHC from 2016 to 2018. No excess over the expected background is observed. Lower limits are set on the B VLQ mass at 95% confidence level. These depend on the B VLQ branching fractions and are 1570 and 1540 GeV for 100% B $\\to$ bH and 100% B $\\to$ bZ, respectively. In most cases, the mass limits obtained exceed previous limits by at least 100 GeV.","sentences":["A search is described for the production of a pair of bottom-type vector-like quarks (B VLQs) with mass greater than 1000 GeV.","Each B VLQ decays into a b quark and a Higgs boson, a b quark and a Z boson, or a t quark and a W boson.","This analysis considers both fully hadronic final states and those containing a charged lepton pair from a Z boson decay.","The products of the H $to$ bb boson decay and of the hadronic Z or W boson decays can be resolved as two distinct jets or merged into a single jet, so the final states are classified by the number of reconstructed jets.","The analysis uses data corresponding to an integrated luminosity of 138 fb$^{-1}$ collected in proton-proton collisions at $\\sqrt{s}$ = 13 TeV with the CMS detector at the LHC from 2016 to 2018.","No excess over the expected background is observed.","Lower limits are set on the B VLQ mass at 95% confidence level.","These depend on the B VLQ branching fractions and are 1570 and 1540 GeV for 100% B $\\to$ bH and 100% B $\\to$ bZ, respectively.","In most cases, the mass limits obtained exceed previous limits by at least 100 GeV."],"url":"http://arxiv.org/abs/2402.13808v1","category":"hep-ex"}
{"created":"2024-02-21 10:54:47","title":"How Do Microservice API Patterns Impact Understandability? A Controlled Experiment","abstract":"Microservices expose their functionality via remote Application Programming Interfaces (APIs), e.g., based on HTTP or asynchronous messaging technology. To solve recurring problems in this design space, Microservice API Patterns (MAPs) have emerged to capture the collective experience of the API design community. At present, there is a lack of empirical evidence for the effectiveness of these patterns, e.g., how they impact understandability and API usability. We therefore conducted a controlled experiment with 6 microservice patterns to evaluate their impact on understandability with 65 diverse participants. Additionally, we wanted to study how demographics like years of professional experience or experience with MAPs influence the effects of the patterns. Per pattern, we constructed two API examples, each in a pattern version \"P\" and a functionally equivalent non-pattern version \"N\" (24 in total). Based on a crossover design, participants had to answer comprehension questions, while we measured the time. For five of the six patterns, we identified a significant positive impact on understandability, i.e., participants answered faster and / or more correctly for \"P\". However, effect sizes were mostly small, with one pattern showing a medium effect. The correlations between performance and demographics seem to suggest that certain patterns may introduce additional complexity; people experienced with MAPs will profit more from their effects. This has important implications for training and education around MAPs and other patterns.","sentences":["Microservices expose their functionality via remote Application Programming Interfaces (APIs), e.g., based on HTTP or asynchronous messaging technology.","To solve recurring problems in this design space, Microservice API Patterns (MAPs) have emerged to capture the collective experience of the API design community.","At present, there is a lack of empirical evidence for the effectiveness of these patterns, e.g., how they impact understandability and API usability.","We therefore conducted a controlled experiment with 6 microservice patterns to evaluate their impact on understandability with 65 diverse participants.","Additionally, we wanted to study how demographics like years of professional experience or experience with MAPs influence the effects of the patterns.","Per pattern, we constructed two API examples, each in a pattern version \"P\" and a functionally equivalent non-pattern version \"N\" (24 in total).","Based on a crossover design, participants had to answer comprehension questions, while we measured the time.","For five of the six patterns, we identified a significant positive impact on understandability, i.e., participants answered faster and / or more correctly for \"P\".","However, effect sizes were mostly small, with one pattern showing a medium effect.","The correlations between performance and demographics seem to suggest that certain patterns may introduce additional complexity; people experienced with MAPs will profit more from their effects.","This has important implications for training and education around MAPs and other patterns."],"url":"http://arxiv.org/abs/2402.13696v1","category":"cs.SE"}
{"created":"2024-02-21 07:39:04","title":"PI-CoF: A Bilevel Optimization Framework for Solving Active Learning Problems using Physics-Information","abstract":"Physics informed neural networks (PINNs) have recently been proposed as surrogate models for solving process optimization problems. However, in an active learning setting collecting enough data for reliably training PINNs poses a challenge. This study proposes a broadly applicable method for incorporating physics information into existing machine learning (ML) models of any type. The proposed method - referred to as PI-CoF for Physics-Informed Correction Factors - introduces additive or multiplicative correction factors for pointwise inference, which are identified by solving a regularized unconstrained optimization problem for reconciliation of physics information and ML model predictions. When ML models are used in an optimization context, using the proposed approach translates into a bilevel optimization problem, where the reconciliation problem is solved as an inner problem each time before evaluating the objective and constraint functions of the outer problem. The utility of the proposed approach is demonstrated through a numerical example, emphasizing constraint satisfaction in a safe Bayesian optimization (BO) setting. Furthermore, a simulation study is carried out by using PI-CoF for the real-time optimization of a fuel cell system. The results show reduced fuel consumption and better reference tracking performance when using the proposed PI-CoF approach in comparison to a constrained BO algorithm not using physics information.","sentences":["Physics informed neural networks (PINNs) have recently been proposed as surrogate models for solving process optimization problems.","However, in an active learning setting collecting enough data for reliably training PINNs poses a challenge.","This study proposes a broadly applicable method for incorporating physics information into existing machine learning (ML) models of any type.","The proposed method - referred to as PI-CoF for Physics-Informed Correction Factors - introduces additive or multiplicative correction factors for pointwise inference, which are identified by solving a regularized unconstrained optimization problem for reconciliation of physics information and ML model predictions.","When ML models are used in an optimization context, using the proposed approach translates into a bilevel optimization problem, where the reconciliation problem is solved as an inner problem each time before evaluating the objective and constraint functions of the outer problem.","The utility of the proposed approach is demonstrated through a numerical example, emphasizing constraint satisfaction in a safe Bayesian optimization (BO) setting.","Furthermore, a simulation study is carried out by using PI-CoF for the real-time optimization of a fuel cell system.","The results show reduced fuel consumption and better reference tracking performance when using the proposed PI-CoF approach in comparison to a constrained BO algorithm not using physics information."],"url":"http://arxiv.org/abs/2402.13588v1","category":"eess.SY"}
{"created":"2024-02-21 07:38:29","title":"A Multimodal In-Context Tuning Approach for E-Commerce Product Description Generation","abstract":"In this paper, we propose a new setting for generating product descriptions from images, augmented by marketing keywords. It leverages the combined power of visual and textual information to create descriptions that are more tailored to the unique features of products. For this setting, previous methods utilize visual and textual encoders to encode the image and keywords and employ a language model-based decoder to generate the product description. However, the generated description is often inaccurate and generic since same-category products have similar copy-writings, and optimizing the overall framework on large-scale samples makes models concentrate on common words yet ignore the product features. To alleviate the issue, we present a simple and effective Multimodal In-Context Tuning approach, named ModICT, which introduces a similar product sample as the reference and utilizes the in-context learning capability of language models to produce the description. During training, we keep the visual encoder and language model frozen, focusing on optimizing the modules responsible for creating multimodal in-context references and dynamic prompts. This approach preserves the language generation prowess of large language models (LLMs), facilitating a substantial increase in description diversity. To assess the effectiveness of ModICT across various language model scales and types, we collect data from three distinct product categories within the E-commerce domain. Extensive experiments demonstrate that ModICT significantly improves the accuracy (by up to 3.3% on Rouge-L) and diversity (by up to 9.4% on D-5) of generated results compared to conventional methods. Our findings underscore the potential of ModICT as a valuable tool for enhancing automatic generation of product descriptions in a wide range of applications.","sentences":["In this paper, we propose a new setting for generating product descriptions from images, augmented by marketing keywords.","It leverages the combined power of visual and textual information to create descriptions that are more tailored to the unique features of products.","For this setting, previous methods utilize visual and textual encoders to encode the image and keywords and employ a language model-based decoder to generate the product description.","However, the generated description is often inaccurate and generic since same-category products have similar copy-writings, and optimizing the overall framework on large-scale samples makes models concentrate on common words yet ignore the product features.","To alleviate the issue, we present a simple and effective Multimodal In-Context Tuning approach, named ModICT, which introduces a similar product sample as the reference and utilizes the in-context learning capability of language models to produce the description.","During training, we keep the visual encoder and language model frozen, focusing on optimizing the modules responsible for creating multimodal in-context references and dynamic prompts.","This approach preserves the language generation prowess of large language models (LLMs), facilitating a substantial increase in description diversity.","To assess the effectiveness of ModICT across various language model scales and types, we collect data from three distinct product categories within the E-commerce domain.","Extensive experiments demonstrate that ModICT significantly improves the accuracy (by up to 3.3% on Rouge-L) and diversity (by up to 9.4% on D-5) of generated results compared to conventional methods.","Our findings underscore the potential of ModICT as a valuable tool for enhancing automatic generation of product descriptions in a wide range of applications."],"url":"http://arxiv.org/abs/2402.13587v1","category":"cs.CL"}
{"created":"2024-02-21 07:34:17","title":"Period-Luminosity-Metallicity-Color Relations of Late-type Contact Binaries in the Big Data Era","abstract":"Binary stars ubiquitous throughout the universe are important. Contact binaries (CBs) possessing Period-Luminosity (PL) relations could be adopted as distance tracers. The PL relations of CBs are influenced by metallicity abundance and color index, which are connected to both the radius and luminosity of stars. Here we propose fine relations of Period-Luminosity-Metallicity-Color (PLZC) from the ultraviolet to infrared bands based on current surveys. The accuracy of the distance estimation is 6\\% and 8\\%, respectively, depending on the PLZC relations of the CBs in the infrared and optical bands of the collected data. PLZC models are still more accurate than PLC models in determining intrinsic luminosity, notwithstanding their limited improvement. Meanwhile, these relations based on synthetic photometry are also calibrated. On the basis of the synthetic photometry, a 6\\% accuracy of distance is estimated. The measured or synthetic data of PLZC or PLC relations in infrared bands comes first in the list of suggestions for distance estimations, and is followed by the measured data of optical bands.","sentences":["Binary stars ubiquitous throughout the universe are important.","Contact binaries (CBs) possessing Period-Luminosity (PL) relations could be adopted as distance tracers.","The PL relations of CBs are influenced by metallicity abundance and color index, which are connected to both the radius and luminosity of stars.","Here we propose fine relations of Period-Luminosity-Metallicity-Color (PLZC) from the ultraviolet to infrared bands based on current surveys.","The accuracy of the distance estimation is 6\\% and 8\\%, respectively, depending on the PLZC relations of the CBs in the infrared and optical bands of the collected data.","PLZC models are still more accurate than PLC models in determining intrinsic luminosity, notwithstanding their limited improvement.","Meanwhile, these relations based on synthetic photometry are also calibrated.","On the basis of the synthetic photometry, a 6\\% accuracy of distance is estimated.","The measured or synthetic data of PLZC or PLC relations in infrared bands comes first in the list of suggestions for distance estimations, and is followed by the measured data of optical bands."],"url":"http://arxiv.org/abs/2402.13585v1","category":"astro-ph.SR"}
{"created":"2024-02-21 07:26:06","title":"Mastering the Game of Guandan with Deep Reinforcement Learning and Behavior Regulating","abstract":"Games are a simplified model of reality and often serve as a favored platform for Artificial Intelligence (AI) research. Much of the research is concerned with game-playing agents and their decision making processes. The game of Guandan (literally, \"throwing eggs\") is a challenging game where even professional human players struggle to make the right decision at times. In this paper we propose a framework named GuanZero for AI agents to master this game using Monte-Carlo methods and deep neural networks. The main contribution of this paper is about regulating agents' behavior through a carefully designed neural network encoding scheme. We then demonstrate the effectiveness of the proposed framework by comparing it with state-of-the-art approaches.","sentences":["Games are a simplified model of reality and often serve as a favored platform for Artificial Intelligence (AI) research.","Much of the research is concerned with game-playing agents and their decision making processes.","The game of Guandan (literally, \"throwing eggs\") is a challenging game where even professional human players struggle to make the right decision at times.","In this paper we propose a framework named GuanZero for AI agents to master this game using Monte-Carlo methods and deep neural networks.","The main contribution of this paper is about regulating agents' behavior through a carefully designed neural network encoding scheme.","We then demonstrate the effectiveness of the proposed framework by comparing it with state-of-the-art approaches."],"url":"http://arxiv.org/abs/2402.13582v1","category":"cs.AI"}
{"created":"2024-02-21 07:15:16","title":"Flexible Physical Camouflage Generation Based on a Differential Approach","abstract":"This study introduces a novel approach to neural rendering, specifically tailored for adversarial camouflage, within an extensive 3D rendering framework. Our method, named FPA, goes beyond traditional techniques by faithfully simulating lighting conditions and material variations, ensuring a nuanced and realistic representation of textures on a 3D target. To achieve this, we employ a generative approach that learns adversarial patterns from a diffusion model. This involves incorporating a specially designed adversarial loss and covert constraint loss to guarantee the adversarial and covert nature of the camouflage in the physical world. Furthermore, we showcase the effectiveness of the proposed camouflage in sticker mode, demonstrating its ability to cover the target without compromising adversarial information. Through empirical and physical experiments, FPA exhibits strong performance in terms of attack success rate and transferability. Additionally, the designed sticker-mode camouflage, coupled with a concealment constraint, adapts to the environment, yielding diverse styles of texture. Our findings highlight the versatility and efficacy of the FPA approach in adversarial camouflage applications.","sentences":["This study introduces a novel approach to neural rendering, specifically tailored for adversarial camouflage, within an extensive 3D rendering framework.","Our method, named FPA, goes beyond traditional techniques by faithfully simulating lighting conditions and material variations, ensuring a nuanced and realistic representation of textures on a 3D target.","To achieve this, we employ a generative approach that learns adversarial patterns from a diffusion model.","This involves incorporating a specially designed adversarial loss and covert constraint loss to guarantee the adversarial and covert nature of the camouflage in the physical world.","Furthermore, we showcase the effectiveness of the proposed camouflage in sticker mode, demonstrating its ability to cover the target without compromising adversarial information.","Through empirical and physical experiments, FPA exhibits strong performance in terms of attack success rate and transferability.","Additionally, the designed sticker-mode camouflage, coupled with a concealment constraint, adapts to the environment, yielding diverse styles of texture.","Our findings highlight the versatility and efficacy of the FPA approach in adversarial camouflage applications."],"url":"http://arxiv.org/abs/2402.13575v1","category":"cs.CV"}
{"created":"2024-02-21 07:10:28","title":"ToDo: Token Downsampling for Efficient Generation of High-Resolution Images","abstract":"Attention mechanism has been crucial for image diffusion models, however, their quadratic computational complexity limits the sizes of images we can process within reasonable time and memory constraints. This paper investigates the importance of dense attention in generative image models, which often contain redundant features, making them suitable for sparser attention mechanisms. We propose a novel training-free method ToDo that relies on token downsampling of key and value tokens to accelerate Stable Diffusion inference by up to 2x for common sizes and up to 4.5x or more for high resolutions like 2048x2048. We demonstrate that our approach outperforms previous methods in balancing efficient throughput and fidelity.","sentences":["Attention mechanism has been crucial for image diffusion models, however, their quadratic computational complexity limits the sizes of images we can process within reasonable time and memory constraints.","This paper investigates the importance of dense attention in generative image models, which often contain redundant features, making them suitable for sparser attention mechanisms.","We propose a novel training-free method ToDo that relies on token downsampling of key and value tokens to accelerate Stable Diffusion inference by up to 2x for common sizes and up to 4.5x or more for high resolutions like 2048x2048.","We demonstrate that our approach outperforms previous methods in balancing efficient throughput and fidelity."],"url":"http://arxiv.org/abs/2402.13573v1","category":"cs.CV"}
{"created":"2024-02-21 07:07:54","title":"On the Expressive Power of a Variant of the Looped Transformer","abstract":"Besides natural language processing, transformers exhibit extraordinary performance in solving broader applications, including scientific computing and computer vision. Previous works try to explain this from the expressive power and capability perspectives that standard transformers are capable of performing some algorithms. To empower transformers with algorithmic capabilities and motivated by the recently proposed looped transformer (Yang et al., 2024; Giannou et al., 2023), we design a novel transformer block, dubbed Algorithm Transformer (abbreviated as AlgoFormer). Compared with the standard transformer and vanilla looped transformer, the proposed AlgoFormer can achieve significantly higher expressiveness in algorithm representation when using the same number of parameters. In particular, inspired by the structure of human-designed learning algorithms, our transformer block consists of a pre-transformer that is responsible for task pre-processing, a looped transformer for iterative optimization algorithms, and a post-transformer for producing the desired results after post-processing. We provide theoretical evidence of the expressive power of the AlgoFormer in solving some challenging problems, mirroring human-designed algorithms. Furthermore, some theoretical and empirical results are presented to show that the designed transformer has the potential to be smarter than human-designed algorithms. Experimental results demonstrate the empirical superiority of the proposed transformer in that it outperforms the standard transformer and vanilla looped transformer in some challenging tasks.","sentences":["Besides natural language processing, transformers exhibit extraordinary performance in solving broader applications, including scientific computing and computer vision.","Previous works try to explain this from the expressive power and capability perspectives that standard transformers are capable of performing some algorithms.","To empower transformers with algorithmic capabilities and motivated by the recently proposed looped transformer (Yang et al., 2024; Giannou et al., 2023), we design a novel transformer block, dubbed Algorithm Transformer (abbreviated as AlgoFormer).","Compared with the standard transformer and vanilla looped transformer, the proposed AlgoFormer can achieve significantly higher expressiveness in algorithm representation when using the same number of parameters.","In particular, inspired by the structure of human-designed learning algorithms, our transformer block consists of a pre-transformer that is responsible for task pre-processing, a looped transformer for iterative optimization algorithms, and a post-transformer for producing the desired results after post-processing.","We provide theoretical evidence of the expressive power of the AlgoFormer in solving some challenging problems, mirroring human-designed algorithms.","Furthermore, some theoretical and empirical results are presented to show that the designed transformer has the potential to be smarter than human-designed algorithms.","Experimental results demonstrate the empirical superiority of the proposed transformer in that it outperforms the standard transformer and vanilla looped transformer in some challenging tasks."],"url":"http://arxiv.org/abs/2402.13572v1","category":"cs.LG"}
{"created":"2024-02-21 07:05:51","title":"Multilingual Coreference Resolution in Low-resource South Asian Languages","abstract":"Coreference resolution involves the task of identifying text spans within a discourse that pertain to the same real-world entity. While this task has been extensively explored in the English language, there has been a notable scarcity of publicly accessible resources and models for coreference resolution in South Asian languages. We introduce a Translated dataset for Multilingual Coreference Resolution (TransMuCoRes) in 31 South Asian languages using off-the-shelf tools for translation and word-alignment. Nearly all of the predicted translations successfully pass a sanity check, and 75% of English references align with their predicted translations. Using multilingual encoders, two off-the-shelf coreference resolution models were trained on a concatenation of TransMuCoRes and a Hindi coreference resolution dataset with manual annotations. The best performing model achieved a score of 64 and 68 for LEA F1 and CoNLL F1, respectively, on our test-split of Hindi golden set. This study is the first to evaluate an end-to-end coreference resolution model on a Hindi golden set. Furthermore, this work underscores the limitations of current coreference evaluation metrics when applied to datasets with split antecedents, advocating for the development of more suitable evaluation metrics.","sentences":["Coreference resolution involves the task of identifying text spans within a discourse that pertain to the same real-world entity.","While this task has been extensively explored in the English language, there has been a notable scarcity of publicly accessible resources and models for coreference resolution in South Asian languages.","We introduce a Translated dataset for Multilingual Coreference Resolution (TransMuCoRes) in 31 South Asian languages using off-the-shelf tools for translation and word-alignment.","Nearly all of the predicted translations successfully pass a sanity check, and 75% of English references align with their predicted translations.","Using multilingual encoders, two off-the-shelf coreference resolution models were trained on a concatenation of TransMuCoRes and a Hindi coreference resolution dataset with manual annotations.","The best performing model achieved a score of 64 and 68 for LEA F1 and CoNLL F1, respectively, on our test-split of Hindi golden set.","This study is the first to evaluate an end-to-end coreference resolution model on a Hindi golden set.","Furthermore, this work underscores the limitations of current coreference evaluation metrics when applied to datasets with split antecedents, advocating for the development of more suitable evaluation metrics."],"url":"http://arxiv.org/abs/2402.13571v1","category":"cs.CL"}
{"created":"2024-02-21 06:57:07","title":"Spot Check Equivalence: an Interpretable Metric for Information Elicitation Mechanisms","abstract":"Because high-quality data is like oxygen for AI systems, effectively eliciting information from crowdsourcing workers has become a first-order problem for developing high-performance machine learning algorithms. Two prevalent paradigms, spot-checking and peer prediction, enable the design of mechanisms to evaluate and incentivize high-quality data from human labelers. So far, at least three metrics have been proposed to compare the performances of these techniques [33, 8, 3]. However, different metrics lead to divergent and even contradictory results in various contexts. In this paper, we harmonize these divergent stories, showing that two of these metrics are actually the same within certain contexts and explain the divergence of the third. Moreover, we unify these different contexts by introducing \\textit{Spot Check Equivalence}, which offers an interpretable metric for the effectiveness of a peer prediction mechanism. Finally, we present two approaches to compute spot check equivalence in various contexts, where simulation results verify the effectiveness of our proposed metric.","sentences":["Because high-quality data is like oxygen for AI systems, effectively eliciting information from crowdsourcing workers has become a first-order problem for developing high-performance machine learning algorithms.","Two prevalent paradigms, spot-checking and peer prediction, enable the design of mechanisms to evaluate and incentivize high-quality data from human labelers.","So far, at least three metrics have been proposed to compare the performances of these techniques [33, 8, 3].","However, different metrics lead to divergent and even contradictory results in various contexts.","In this paper, we harmonize these divergent stories, showing that two of these metrics are actually the same within certain contexts and explain the divergence of the third.","Moreover, we unify these different contexts by introducing \\textit{Spot Check Equivalence}, which offers an interpretable metric for the effectiveness of a peer prediction mechanism.","Finally, we present two approaches to compute spot check equivalence in various contexts, where simulation results verify the effectiveness of our proposed metric."],"url":"http://arxiv.org/abs/2402.13567v1","category":"cs.LG"}
{"created":"2024-02-21 06:34:46","title":"Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment","abstract":"Evaluating and Rethinking the current landscape of Large Multimodal Models (LMMs), we observe that widely-used visual-language projection approaches (e.g., Q-former or MLP) focus on the alignment of image-text descriptions yet ignore the visual knowledge-dimension alignment, i.e., connecting visuals to their relevant knowledge. Visual knowledge plays a significant role in analyzing, inferring, and interpreting information from visuals, helping improve the accuracy of answers to knowledge-based visual questions. In this paper, we mainly explore improving LMMs with visual-language knowledge alignment, especially aimed at challenging knowledge-based visual question answering (VQA). To this end, we present a Cognitive Visual-Language Mapper (CVLM), which contains a pretrained Visual Knowledge Aligner (VKA) and a Fine-grained Knowledge Adapter (FKA) used in the multimodal instruction tuning stage. Specifically, we design the VKA based on the interaction between a small language model and a visual encoder, training it on collected image-knowledge pairs to achieve visual knowledge acquisition and projection. FKA is employed to distill the fine-grained visual knowledge of an image and inject it into Large Language Models (LLMs). We conduct extensive experiments on knowledge-based VQA benchmarks and experimental results show that CVLM significantly improves the performance of LMMs on knowledge-based VQA (average gain by 5.0%). Ablation studies also verify the effectiveness of VKA and FKA, respectively.","sentences":["Evaluating and Rethinking the current landscape of Large Multimodal Models (LMMs), we observe that widely-used visual-language projection approaches (e.g., Q-former or MLP) focus on the alignment of image-text descriptions yet ignore the visual knowledge-dimension alignment, i.e., connecting visuals to their relevant knowledge.","Visual knowledge plays a significant role in analyzing, inferring, and interpreting information from visuals, helping improve the accuracy of answers to knowledge-based visual questions.","In this paper, we mainly explore improving LMMs with visual-language knowledge alignment, especially aimed at challenging knowledge-based visual question answering (VQA).","To this end, we present a Cognitive Visual-Language Mapper (CVLM), which contains a pretrained Visual Knowledge Aligner (VKA) and a Fine-grained Knowledge Adapter (FKA) used in the multimodal instruction tuning stage.","Specifically, we design the VKA based on the interaction between a small language model and a visual encoder, training it on collected image-knowledge pairs to achieve visual knowledge acquisition and projection.","FKA is employed to distill the fine-grained visual knowledge of an image and inject it into Large Language Models (LLMs).","We conduct extensive experiments on knowledge-based VQA benchmarks and experimental results show that CVLM significantly improves the performance of LMMs on knowledge-based VQA (average gain by 5.0%).","Ablation studies also verify the effectiveness of VKA and FKA, respectively."],"url":"http://arxiv.org/abs/2402.13561v1","category":"cs.CL"}
{"created":"2024-02-21 06:27:04","title":"The Main Electrode System of the Nab Experiment and the Analysis of the Performance in the Measurement of the Fierz Term b","abstract":"The Nab collaboration will study free neutron beta decay at the Spallation Neutron Source at Oak Ridge National Lab. A neutron decays into a proton, an electron and an anti-neutrino in this process, where the energy of the outgoing protons and electrons are collected to determine (1) the electron-antineutrino correlation co-efficient $a$ to the precision of |$\\Delta a/a$|<=$10^{-3}$ and (2) the Fierz interference term $b$ to the precision of |$\\Delta b/b$|<=$3*10^{-3}$. From the measurement of $a$, we can calculate the axial-vector to vector coupling ratio $\\lambda$. Together with the neutron lifetime measurement, we could also calculate the upper right element of the Cabbibo-Kobayashi-Maskawa Matrix, and test the unitarity of that matrix. The measurement of $b$ could shed light on the physics beyond the Standard Model since $b$ is predicted to be 0 by the $V-A$ structure of weak interaction in the Standard Model. This thesis presents the design of Nab electrode system, and the solution to a major systematic effect in the measurement of $a$: the requirement of having a low electrical field environment in the neutron decay region. The electrode system has been built and installed successfully, and as our characterization and its analysis shows, the electrode system meets the required specifications. This thesis also gives a systematic uncertainty study for $b$ measurement and provides a table of the requirements for the $b$ measurement...","sentences":["The Nab collaboration will study free neutron beta decay at the Spallation Neutron Source at Oak Ridge National Lab.","A neutron decays into a proton, an electron and an anti-neutrino in this process, where the energy of the outgoing protons and electrons are collected to determine (1) the electron-antineutrino correlation co-efficient $a$ to the precision of |$\\Delta a/a$|<=$10^{-3}$ and (2) the Fierz interference term $b$ to the precision of |$\\Delta b/b$|<=$3*10^{-3}$. From the measurement of $a$, we can calculate the axial-vector to vector coupling ratio $\\lambda$. Together with the neutron lifetime measurement, we could also calculate the upper right element of the Cabbibo-Kobayashi-Maskawa Matrix, and test the unitarity of that matrix.","The measurement of $b$ could shed light on the physics beyond the Standard Model since $b$ is predicted to be 0 by the $V-A$ structure of weak interaction in the Standard Model.","This thesis presents the design of Nab electrode system, and the solution to a major systematic effect in the measurement of $a$: the requirement of having a low electrical field environment in the neutron decay region.","The electrode system has been built and installed successfully, and as our characterization and its analysis shows, the electrode system meets the required specifications.","This thesis also gives a systematic uncertainty study for $b$ measurement and provides a table of the requirements for the $b$ measurement..."],"url":"http://arxiv.org/abs/2402.13559v1","category":"physics.ins-det"}
{"created":"2024-02-21 06:25:54","title":"Inductive Graph Alignment Prompt: Bridging the Gap between Graph Pre-training and Inductive Fine-tuning From Spectral Perspective","abstract":"The \"Graph pre-training and fine-tuning\" paradigm has significantly improved Graph Neural Networks(GNNs) by capturing general knowledge without manual annotations for downstream tasks. However, due to the immense gap of data and tasks between the pre-training and fine-tuning stages, the model performance is still limited. Inspired by prompt fine-tuning in Natural Language Processing(NLP), many endeavors have been made to bridge the gap in graph domain. But existing methods simply reformulate the form of fine-tuning tasks to the pre-training ones. With the premise that the pre-training graphs are compatible with the fine-tuning ones, these methods typically operate in transductive setting. In order to generalize graph pre-training to inductive scenario where the fine-tuning graphs might significantly differ from pre-training ones, we propose a novel graph prompt based method called Inductive Graph Alignment Prompt(IGAP). Firstly, we unify the mainstream graph pre-training frameworks and analyze the essence of graph pre-training from graph spectral theory. Then we identify the two sources of the data gap in inductive setting: (i) graph signal gap and (ii) graph structure gap. Based on the insight of graph pre-training, we propose to bridge the graph signal gap and the graph structure gap with learnable prompts in the spectral space. A theoretical analysis ensures the effectiveness of our method. At last, we conduct extensive experiments among nodes classification and graph classification tasks under the transductive, semi-inductive and inductive settings. The results demonstrate that our proposed method can successfully bridge the data gap under different settings.","sentences":["The \"Graph pre-training and fine-tuning\" paradigm has significantly improved Graph Neural Networks(GNNs) by capturing general knowledge without manual annotations for downstream tasks.","However, due to the immense gap of data and tasks between the pre-training and fine-tuning stages, the model performance is still limited.","Inspired by prompt fine-tuning in Natural Language Processing(NLP), many endeavors have been made to bridge the gap in graph domain.","But existing methods simply reformulate the form of fine-tuning tasks to the pre-training ones.","With the premise that the pre-training graphs are compatible with the fine-tuning ones, these methods typically operate in transductive setting.","In order to generalize graph pre-training to inductive scenario where the fine-tuning graphs might significantly differ from pre-training ones, we propose a novel graph prompt based method called Inductive Graph Alignment Prompt(IGAP).","Firstly, we unify the mainstream graph pre-training frameworks and analyze the essence of graph pre-training from graph spectral theory.","Then we identify the two sources of the data gap in inductive setting: (i) graph signal gap and (ii) graph structure gap.","Based on the insight of graph pre-training, we propose to bridge the graph signal gap and the graph structure gap with learnable prompts in the spectral space.","A theoretical analysis ensures the effectiveness of our method.","At last, we conduct extensive experiments among nodes classification and graph classification tasks under the transductive, semi-inductive and inductive settings.","The results demonstrate that our proposed method can successfully bridge the data gap under different settings."],"url":"http://arxiv.org/abs/2402.13556v1","category":"cs.LG"}
{"created":"2024-02-21 06:25:35","title":"Full-Atom Peptide Design with Geometric Latent Diffusion","abstract":"Peptide design plays a pivotal role in therapeutics, allowing brand new possibility to leverage target binding sites that are previously undruggable. Most existing methods are either inefficient or only concerned with the target-agnostic design of 1D sequences. In this paper, we propose a generative model for full-atom \\textbf{Pep}tide design with \\textbf{G}eometric \\textbf{LA}tent \\textbf{D}iffusion (PepGLAD). We first collect a dataset consisting of both 1D sequences and 3D structures from Protein Data Bank (PDB) and literature, for the training of PepGLAD. We then identify two major challenges of leveraging current diffusion-based models for peptide design: the full-atom geometry and the variable binding geometry. To tackle the first challenge, PepGLAD derives a variational autoencoder that first encodes full-atom residues of variable size into fixed-dimensional latent representations, and then decodes back to the residue space after conducting the diffusion process in the latent space. For the second issue, PepGLAD explores a receptor-specific affine transformation to convert the 3D coordinates into a shared standard space, enabling better generalization ability across different binding shapes. Remarkably, our method improves diversity and \\emph{in silico} success rate by 18% and 8% in sequence-structure co-design, and achieves 26% absolute gain in recalling the reference binding conformation.","sentences":["Peptide design plays a pivotal role in therapeutics, allowing brand new possibility to leverage target binding sites that are previously undruggable.","Most existing methods are either inefficient or only concerned with the target-agnostic design of 1D sequences.","In this paper, we propose a generative model for full-atom \\textbf{Pep}tide design with \\textbf{G}eometric \\textbf{LA}tent \\textbf{D}iffusion (PepGLAD).","We first collect a dataset consisting of both 1D sequences and 3D structures from Protein Data Bank (PDB) and literature, for the training of PepGLAD.","We then identify two major challenges of leveraging current diffusion-based models for peptide design: the full-atom geometry and the variable binding geometry.","To tackle the first challenge, PepGLAD derives a variational autoencoder that first encodes full-atom residues of variable size into fixed-dimensional latent representations, and then decodes back to the residue space after conducting the diffusion process in the latent space.","For the second issue, PepGLAD explores a receptor-specific affine transformation to convert the 3D coordinates into a shared standard space, enabling better generalization ability across different binding shapes.","Remarkably, our method improves diversity and \\emph{in silico} success rate by 18% and 8% in sequence-structure co-design, and achieves 26% absolute gain in recalling the reference binding conformation."],"url":"http://arxiv.org/abs/2402.13555v1","category":"q-bio.BM"}
{"created":"2024-02-21 06:22:41","title":"Generative AI for Secure Physical Layer Communications: A Survey","abstract":"Generative Artificial Intelligence (GAI) stands at the forefront of AI innovation, demonstrating rapid advancement and unparalleled proficiency in generating diverse content. Beyond content creation, GAI has significant analytical abilities to learn complex data distribution, offering numerous opportunities to resolve security issues. In the realm of security from physical layer perspectives, traditional AI approaches frequently struggle, primarily due to their limited capacity to dynamically adjust to the evolving physical attributes of transmission channels and the complexity of contemporary cyber threats. This adaptability and analytical depth are precisely where GAI excels. Therefore, in this paper, we offer an extensive survey on the various applications of GAI in enhancing security within the physical layer of communication networks. We first emphasize the importance of advanced GAI models in this area, including Generative Adversarial Networks (GANs), Autoencoders (AEs), Variational Autoencoders (VAEs), and Diffusion Models (DMs). We delve into the roles of GAI in addressing challenges of physical layer security, focusing on communication confidentiality, authentication, availability, resilience, and integrity. Furthermore, we also present future research directions focusing model improvements, multi-scenario deployment, resource-efficient optimization, and secure semantic communication, highlighting the multifaceted potential of GAI to address emerging challenges in secure physical layer communications and sensing.","sentences":["Generative Artificial Intelligence (GAI) stands at the forefront of AI innovation, demonstrating rapid advancement and unparalleled proficiency in generating diverse content.","Beyond content creation, GAI has significant analytical abilities to learn complex data distribution, offering numerous opportunities to resolve security issues.","In the realm of security from physical layer perspectives, traditional AI approaches frequently struggle, primarily due to their limited capacity to dynamically adjust to the evolving physical attributes of transmission channels and the complexity of contemporary cyber threats.","This adaptability and analytical depth are precisely where GAI excels.","Therefore, in this paper, we offer an extensive survey on the various applications of GAI in enhancing security within the physical layer of communication networks.","We first emphasize the importance of advanced GAI models in this area, including Generative Adversarial Networks (GANs), Autoencoders (AEs), Variational Autoencoders (VAEs), and Diffusion Models (DMs).","We delve into the roles of GAI in addressing challenges of physical layer security, focusing on communication confidentiality, authentication, availability, resilience, and integrity.","Furthermore, we also present future research directions focusing model improvements, multi-scenario deployment, resource-efficient optimization, and secure semantic communication, highlighting the multifaceted potential of GAI to address emerging challenges in secure physical layer communications and sensing."],"url":"http://arxiv.org/abs/2402.13553v1","category":"cs.CR"}
{"created":"2024-02-21 06:11:03","title":"Are LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues","abstract":"A successful negotiation demands a deep comprehension of the conversation context, Theory-of-Mind (ToM) skills to infer the partner's motives, as well as strategic reasoning and effective communication, making it challenging for automated systems. Given the remarkable performance of LLMs across a variety of NLP tasks, in this work, we aim to understand how LLMs can advance different aspects of negotiation research, ranging from designing dialogue systems to providing pedagogical feedback and scaling up data collection practices. To this end, we devise a methodology to analyze the multifaceted capabilities of LLMs across diverse dialogue scenarios covering all the time stages of a typical negotiation interaction. Our analysis adds to the increasing evidence for the superiority of GPT-4 across various tasks while also providing insights into specific tasks that remain difficult for LLMs. For instance, the models correlate poorly with human players when making subjective assessments about the negotiation dialogues and often struggle to generate responses that are contextually appropriate as well as strategically advantageous.","sentences":["A successful negotiation demands a deep comprehension of the conversation context, Theory-of-Mind (ToM) skills to infer the partner's motives, as well as strategic reasoning and effective communication, making it challenging for automated systems.","Given the remarkable performance of LLMs across a variety of NLP tasks, in this work, we aim to understand how LLMs can advance different aspects of negotiation research, ranging from designing dialogue systems to providing pedagogical feedback and scaling up data collection practices.","To this end, we devise a methodology to analyze the multifaceted capabilities of LLMs across diverse dialogue scenarios covering all the time stages of a typical negotiation interaction.","Our analysis adds to the increasing evidence for the superiority of GPT-4 across various tasks while also providing insights into specific tasks that remain difficult for LLMs.","For instance, the models correlate poorly with human players when making subjective assessments about the negotiation dialogues and often struggle to generate responses that are contextually appropriate as well as strategically advantageous."],"url":"http://arxiv.org/abs/2402.13550v1","category":"cs.CL"}
{"created":"2024-02-21 05:56:52","title":"LLMs Meet Long Video: Advancing Long Video Comprehension with An Interactive Visual Adapter in LLMs","abstract":"Long video understanding is a significant and ongoing challenge in the intersection of multimedia and artificial intelligence. Employing large language models (LLMs) for comprehending video becomes an emerging and promising method. However, this approach incurs high computational costs due to the extensive array of video tokens, experiences reduced visual clarity as a consequence of token aggregation, and confronts challenges arising from irrelevant visual tokens while answering video-related questions. To alleviate these issues, we present an Interactive Visual Adapter (IVA) within LLMs, designed to enhance interaction with fine-grained visual elements. Specifically, we first transform long videos into temporal video tokens via leveraging a visual encoder alongside a pretrained causal transformer, then feed them into LLMs with the video instructions. Subsequently, we integrated IVA, which contains a lightweight temporal frame selector and a spatial feature interactor, within the internal blocks of LLMs to capture instruction-aware and fine-grained visual signals. Consequently, the proposed video-LLM facilitates a comprehensive understanding of long video content through appropriate long video modeling and precise visual interactions. We conducted extensive experiments on nine video understanding benchmarks and experimental results show that our interactive visual adapter significantly improves the performance of video LLMs on long video QA tasks. Ablation studies further verify the effectiveness of IVA in long and short video understandings.","sentences":["Long video understanding is a significant and ongoing challenge in the intersection of multimedia and artificial intelligence.","Employing large language models (LLMs) for comprehending video becomes an emerging and promising method.","However, this approach incurs high computational costs due to the extensive array of video tokens, experiences reduced visual clarity as a consequence of token aggregation, and confronts challenges arising from irrelevant visual tokens while answering video-related questions.","To alleviate these issues, we present an Interactive Visual Adapter (IVA) within LLMs, designed to enhance interaction with fine-grained visual elements.","Specifically, we first transform long videos into temporal video tokens via leveraging a visual encoder alongside a pretrained causal transformer, then feed them into LLMs with the video instructions.","Subsequently, we integrated IVA, which contains a lightweight temporal frame selector and a spatial feature interactor, within the internal blocks of LLMs to capture instruction-aware and fine-grained visual signals.","Consequently, the proposed video-LLM facilitates a comprehensive understanding of long video content through appropriate long video modeling and precise visual interactions.","We conducted extensive experiments on nine video understanding benchmarks and experimental results show that our interactive visual adapter significantly improves the performance of video LLMs on long video QA tasks.","Ablation studies further verify the effectiveness of IVA in long and short video understandings."],"url":"http://arxiv.org/abs/2402.13546v1","category":"cs.CL"}
{"created":"2024-02-21 05:41:34","title":"ARL2: Aligning Retrievers for Black-box Large Language Models via Self-guided Adaptive Relevance Labeling","abstract":"Retrieval-augmented generation enhances large language models (LLMs) by incorporating relevant information from external knowledge sources. This enables LLMs to adapt to specific domains and mitigate hallucinations in knowledge-intensive tasks. However, existing retrievers are often misaligned with LLMs due to their separate training processes and the black-box nature of LLMs. To address this challenge, we propose ARL2, a retriever learning technique that harnesses LLMs as labelers. ARL2 leverages LLMs to annotate and score relevant evidence, enabling learning the retriever from robust LLM supervision. Furthermore, ARL2 uses an adaptive self-training strategy for curating high-quality and diverse relevance data, which can effectively reduce the annotation cost. Extensive experiments demonstrate the effectiveness of ARL2, achieving accuracy improvements of 5.4% on NQ and 4.6% on MMLU compared to the state-of-the-art methods. Additionally, ARL2 exhibits robust transfer learning capabilities and strong zero-shot generalization abilities. Our code will be published at \\url{https://github.com/zhanglingxi-cs/ARL2}.","sentences":["Retrieval-augmented generation enhances large language models (LLMs) by incorporating relevant information from external knowledge sources.","This enables LLMs to adapt to specific domains and mitigate hallucinations in knowledge-intensive tasks.","However, existing retrievers are often misaligned with LLMs due to their separate training processes and the black-box nature of LLMs.","To address this challenge, we propose ARL2, a retriever learning technique that harnesses LLMs as labelers.","ARL2 leverages LLMs to annotate and score relevant evidence, enabling learning the retriever from robust LLM supervision.","Furthermore, ARL2 uses an adaptive self-training strategy for curating high-quality and diverse relevance data, which can effectively reduce the annotation cost.","Extensive experiments demonstrate the effectiveness of ARL2, achieving accuracy improvements of 5.4% on NQ and 4.6% on MMLU compared to the state-of-the-art methods.","Additionally, ARL2 exhibits robust transfer learning capabilities and strong zero-shot generalization abilities.","Our code will be published at \\url{https://github.com/zhanglingxi-cs/ARL2}."],"url":"http://arxiv.org/abs/2402.13542v1","category":"cs.CL"}
{"created":"2024-02-21 05:14:30","title":"Exploring the Limits of Semantic Image Compression at Micro-bits per Pixel","abstract":"Traditional methods, such as JPEG, perform image compression by operating on structural information, such as pixel values or frequency content. These methods are effective to bitrates around one bit per pixel (bpp) and higher at standard image sizes. In contrast, text-based semantic compression directly stores concepts and their relationships using natural language, which has evolved with humans to efficiently represent these salient concepts. These methods can operate at extremely low bitrates by disregarding structural information like location, size, and orientation. In this work, we use GPT-4V and DALL-E3 from OpenAI to explore the quality-compression frontier for image compression and identify the limitations of current technology. We push semantic compression as low as 100 $\\mu$bpp (up to $10,000\\times$ smaller than JPEG) by introducing an iterative reflection process to improve the decoded image. We further hypothesize this 100 $\\mu$bpp level represents a soft limit on semantic compression at standard image resolutions.","sentences":["Traditional methods, such as JPEG, perform image compression by operating on structural information, such as pixel values or frequency content.","These methods are effective to bitrates around one bit per pixel (bpp) and higher at standard image sizes.","In contrast, text-based semantic compression directly stores concepts and their relationships using natural language, which has evolved with humans to efficiently represent these salient concepts.","These methods can operate at extremely low bitrates by disregarding structural information like location, size, and orientation.","In this work, we use GPT-4V and DALL-E3 from OpenAI to explore the quality-compression frontier for image compression and identify the limitations of current technology.","We push semantic compression as low as 100 $\\mu$bpp (up to $10,000\\times$ smaller than JPEG) by introducing an iterative reflection process to improve the decoded image.","We further hypothesize this 100 $\\mu$bpp level represents a soft limit on semantic compression at standard image resolutions."],"url":"http://arxiv.org/abs/2402.13536v1","category":"cs.CV"}
{"created":"2024-02-21 05:04:29","title":"An Effective Incorporating Heterogeneous Knowledge Curriculum Learning for Sequence Labeling","abstract":"Sequence labeling models often benefit from incorporating external knowledge. However, this practice introduces data heterogeneity and complicates the model with additional modules, leading to increased expenses for training a high-performing model. To address this challenge, we propose a two-stage curriculum learning (TCL) framework specifically designed for sequence labeling tasks. The TCL framework enhances training by gradually introducing data instances from easy to hard, aiming to improve both performance and training speed. Furthermore, we explore different metrics for assessing the difficulty levels of sequence labeling tasks. Through extensive experimentation on six Chinese word segmentation (CWS) and Part-of-speech tagging (POS) datasets, we demonstrate the effectiveness of our model in enhancing the performance of sequence labeling models. Additionally, our analysis indicates that TCL accelerates training and alleviates the slow training problem associated with complex models.","sentences":["Sequence labeling models often benefit from incorporating external knowledge.","However, this practice introduces data heterogeneity and complicates the model with additional modules, leading to increased expenses for training a high-performing model.","To address this challenge, we propose a two-stage curriculum learning (TCL) framework specifically designed for sequence labeling tasks.","The TCL framework enhances training by gradually introducing data instances from easy to hard, aiming to improve both performance and training speed.","Furthermore, we explore different metrics for assessing the difficulty levels of sequence labeling tasks.","Through extensive experimentation on six Chinese word segmentation (CWS) and Part-of-speech tagging (POS) datasets, we demonstrate the effectiveness of our model in enhancing the performance of sequence labeling models.","Additionally, our analysis indicates that TCL accelerates training and alleviates the slow training problem associated with complex models."],"url":"http://arxiv.org/abs/2402.13534v1","category":"cs.CL"}
{"created":"2024-02-21 05:03:17","title":"FinGPT-HPC: Efficient Pretraining and Finetuning Large Language Models for Financial Applications with High-Performance Computing","abstract":"Large language models (LLMs) are computationally intensive. The computation workload and the memory footprint grow quadratically with the dimension (layer width). Most of LLMs' parameters come from the linear layers of the transformer structure and are highly redundant. These linear layers contribute more than 80% of the computation workload and 99% of the model size. To pretrain and finetune LLMs efficiently, there are three major challenges to address: 1) reducing redundancy of the linear layers; 2) reducing GPU memory footprint; 3) improving GPU utilization when using distributed training. Prior methods, such as LoRA and QLoRA, utilized low-rank matrices and quantization to reduce the number of trainable parameters and model size, respectively. However, the resulting model still consumes a large amount of GPU memory. In this paper, we present high-performance GPU-based methods that exploit low-rank structures to pretrain and finetune LLMs for financial applications. We replace one conventional linear layer of the transformer structure with two narrower linear layers, which allows us to reduce the number of parameters by several orders of magnitude. By quantizing the parameters into low precision (8-bit and 4-bit), the memory consumption of the resulting model is further reduced. Compared with existing LLMs, our methods achieve a speedup of 1.3X and a model compression ratio of 2.64X for pretaining without accuracy drop. For finetuning, our methods achieve an average accuracy increase of 6.3% and 24.0% in general tasks and financial tasks, respectively, and GPU memory consumption ratio of 6.3X. The sizes of our models are smaller than 0.59 GB, allowing inference on a smartphone.","sentences":["Large language models (LLMs) are computationally intensive.","The computation workload and the memory footprint grow quadratically with the dimension (layer width).","Most of LLMs' parameters come from the linear layers of the transformer structure and are highly redundant.","These linear layers contribute more than 80% of the computation workload and 99% of the model size.","To pretrain and finetune LLMs efficiently, there are three major challenges to address: 1) reducing redundancy of the linear layers; 2) reducing GPU memory footprint; 3) improving GPU utilization when using distributed training.","Prior methods, such as LoRA and QLoRA, utilized low-rank matrices and quantization to reduce the number of trainable parameters and model size, respectively.","However, the resulting model still consumes a large amount of GPU memory.","In this paper, we present high-performance GPU-based methods that exploit low-rank structures to pretrain and finetune LLMs for financial applications.","We replace one conventional linear layer of the transformer structure with two narrower linear layers, which allows us to reduce the number of parameters by several orders of magnitude.","By quantizing the parameters into low precision (8-bit and 4-bit), the memory consumption of the resulting model is further reduced.","Compared with existing LLMs, our methods achieve a speedup of 1.3X and a model compression ratio of 2.64X for pretaining without accuracy drop.","For finetuning, our methods achieve an average accuracy increase of 6.3% and 24.0% in general tasks and financial tasks, respectively, and GPU memory consumption ratio of 6.3X. The sizes of our models are smaller than 0.59 GB, allowing inference on a smartphone."],"url":"http://arxiv.org/abs/2402.13533v1","category":"cs.LG"}
{"created":"2024-02-21 04:43:12","title":"MatchNAS: Optimizing Edge AI in Sparse-Label Data Contexts via Automating Deep Neural Network Porting for Mobile Deployment","abstract":"Recent years have seen the explosion of edge intelligence with powerful Deep Neural Networks (DNNs). One popular scheme is training DNNs on powerful cloud servers and subsequently porting them to mobile devices after being lightweight. Conventional approaches manually specialized DNNs for various edge platforms and retrain them with real-world data. However, as the number of platforms increases, these approaches become labour-intensive and computationally prohibitive. Additionally, real-world data tends to be sparse-label, further increasing the difficulty of lightweight models. In this paper, we propose MatchNAS, a novel scheme for porting DNNs to mobile devices. Specifically, we simultaneously optimise a large network family using both labelled and unlabelled data and then automatically search for tailored networks for different hardware platforms. MatchNAS acts as an intermediary that bridges the gap between cloud-based DNNs and edge-based DNNs.","sentences":["Recent years have seen the explosion of edge intelligence with powerful Deep Neural Networks (DNNs).","One popular scheme is training DNNs on powerful cloud servers and subsequently porting them to mobile devices after being lightweight.","Conventional approaches manually specialized DNNs for various edge platforms and retrain them with real-world data.","However, as the number of platforms increases, these approaches become labour-intensive and computationally prohibitive.","Additionally, real-world data tends to be sparse-label, further increasing the difficulty of lightweight models.","In this paper, we propose MatchNAS, a novel scheme for porting DNNs to mobile devices.","Specifically, we simultaneously optimise a large network family using both labelled and unlabelled data and then automatically search for tailored networks for different hardware platforms.","MatchNAS acts as an intermediary that bridges the gap between cloud-based DNNs and edge-based DNNs."],"url":"http://arxiv.org/abs/2402.13525v1","category":"cs.LG"}
{"created":"2024-02-21 04:10:12","title":"Test-Driven Development for Code Generation","abstract":"Large language models (LLMs) like GPT4, have shown proficiency in generating code snippets from problem statements. Traditionally software development by humans followed a similar methodology of writing code from problem statements or requirements. However, in the past, there have been several studies that have shown the value of test-driven development (TDD) where humans write tests based on problem statements before the code for the functionality is written. In the context of LLM-based code generation, one obvious benefit of TDD is that the developer then knows for sure if the generated code has passed all the given tests or not. Therefore, in this paper, we want to empirically evaluate the hypothesis: giving the problem statements and tests as input to GPT4 is better than just giving the problem statement as input. To test our hypothesis, we build a framework TGen. In our experiments on the MBPP, HumanEval and CodeChef datasets, we consistently find that including tests solves more programming problems than not including them. Thus we show that TDD is a better development model than just using a problem statement when using GPT4 for code generation tasks.","sentences":["Large language models (LLMs) like GPT4, have shown proficiency in generating code snippets from problem statements.","Traditionally software development by humans followed a similar methodology of writing code from problem statements or requirements.","However, in the past, there have been several studies that have shown the value of test-driven development (TDD) where humans write tests based on problem statements before the code for the functionality is written.","In the context of LLM-based code generation, one obvious benefit of TDD is that the developer then knows for sure if the generated code has passed all the given tests or not.","Therefore, in this paper, we want to empirically evaluate the hypothesis: giving the problem statements and tests as input to GPT4 is better than just giving the problem statement as input.","To test our hypothesis, we build a framework TGen.","In our experiments on the MBPP, HumanEval and CodeChef datasets, we consistently find that including tests solves more programming problems than not including them.","Thus we show that TDD is a better development model than just using a problem statement when using GPT4 for code generation tasks."],"url":"http://arxiv.org/abs/2402.13521v1","category":"cs.SE"}
{"created":"2024-02-21 04:00:54","title":"RITFIS: Robust input testing framework for LLMs-based intelligent software","abstract":"The dependence of Natural Language Processing (NLP) intelligent software on Large Language Models (LLMs) is increasingly prominent, underscoring the necessity for robustness testing. Current testing methods focus solely on the robustness of LLM-based software to prompts. Given the complexity and diversity of real-world inputs, studying the robustness of LLMbased software in handling comprehensive inputs (including prompts and examples) is crucial for a thorough understanding of its performance.   To this end, this paper introduces RITFIS, a Robust Input Testing Framework for LLM-based Intelligent Software. To our knowledge, RITFIS is the first framework designed to assess the robustness of LLM-based intelligent software against natural language inputs. This framework, based on given threat models and prompts, primarily defines the testing process as a combinatorial optimization problem. Successful test cases are determined by a goal function, creating a transformation space for the original examples through perturbation means, and employing a series of search methods to filter cases that meet both the testing objectives and language constraints. RITFIS, with its modular design, offers a comprehensive method for evaluating the robustness of LLMbased intelligent software.   RITFIS adapts 17 automated testing methods, originally designed for Deep Neural Network (DNN)-based intelligent software, to the LLM-based software testing scenario. It demonstrates the effectiveness of RITFIS in evaluating LLM-based intelligent software through empirical validation. However, existing methods generally have limitations, especially when dealing with lengthy texts and structurally complex threat models. Therefore, we conducted a comprehensive analysis based on five metrics and provided insightful testing method optimization strategies, benefiting both researchers and everyday users.","sentences":["The dependence of Natural Language Processing (NLP) intelligent software on Large Language Models (LLMs) is increasingly prominent, underscoring the necessity for robustness testing.","Current testing methods focus solely on the robustness of LLM-based software to prompts.","Given the complexity and diversity of real-world inputs, studying the robustness of LLMbased software in handling comprehensive inputs (including prompts and examples) is crucial for a thorough understanding of its performance.   ","To this end, this paper introduces RITFIS, a Robust Input Testing Framework for LLM-based Intelligent Software.","To our knowledge, RITFIS is the first framework designed to assess the robustness of LLM-based intelligent software against natural language inputs.","This framework, based on given threat models and prompts, primarily defines the testing process as a combinatorial optimization problem.","Successful test cases are determined by a goal function, creating a transformation space for the original examples through perturbation means, and employing a series of search methods to filter cases that meet both the testing objectives and language constraints.","RITFIS, with its modular design, offers a comprehensive method for evaluating the robustness of LLMbased intelligent software.   ","RITFIS adapts 17 automated testing methods, originally designed for Deep Neural Network (DNN)-based intelligent software, to the LLM-based software testing scenario.","It demonstrates the effectiveness of RITFIS in evaluating LLM-based intelligent software through empirical validation.","However, existing methods generally have limitations, especially when dealing with lengthy texts and structurally complex threat models.","Therefore, we conducted a comprehensive analysis based on five metrics and provided insightful testing method optimization strategies, benefiting both researchers and everyday users."],"url":"http://arxiv.org/abs/2402.13518v1","category":"cs.SE"}
{"created":"2024-02-21 03:59:52","title":"Round Trip Translation Defence against Large Language Model Jailbreaking Attacks","abstract":"Large language models (LLMs) are susceptible to social-engineered attacks that are human-interpretable but require a high level of comprehension for LLMs to counteract. Existing defensive measures can only mitigate less than half of these attacks at most. To address this issue, we propose the Round Trip Translation (RTT) method, the first algorithm specifically designed to defend against social-engineered attacks on LLMs. RTT paraphrases the adversarial prompt and generalizes the idea conveyed, making it easier for LLMs to detect induced harmful behavior. This method is versatile, lightweight, and transferrable to different LLMs. Our defense successfully mitigated over 70% of Prompt Automatic Iterative Refinement (PAIR) attacks, which is currently the most effective defense to the best of our knowledge. We are also the first to attempt mitigating the MathsAttack and reduced its attack success rate by almost 40%. Our code is publicly available at https://github.com/Cancanxxx/Round_Trip_Translation_Defence","sentences":["Large language models (LLMs) are susceptible to social-engineered attacks that are human-interpretable but require a high level of comprehension for LLMs to counteract.","Existing defensive measures can only mitigate less than half of these attacks at most.","To address this issue, we propose the Round Trip Translation (RTT) method, the first algorithm specifically designed to defend against social-engineered attacks on LLMs.","RTT paraphrases the adversarial prompt and generalizes the idea conveyed, making it easier for LLMs to detect induced harmful behavior.","This method is versatile, lightweight, and transferrable to different LLMs.","Our defense successfully mitigated over 70% of Prompt Automatic Iterative Refinement (PAIR) attacks, which is currently the most effective defense to the best of our knowledge.","We are also the first to attempt mitigating the MathsAttack and reduced its attack success rate by almost 40%.","Our code is publicly available at https://github.com/Cancanxxx/Round_Trip_Translation_Defence"],"url":"http://arxiv.org/abs/2402.13517v1","category":"cs.CL"}
{"created":"2024-02-21 03:58:49","title":"ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models","abstract":"Activation sparsity refers to the existence of considerable weakly-contributed elements among activation outputs. As a prevalent property of the models using the ReLU activation function, it has been proven a promising paradigm to boost model inference efficiency. Nevertheless, most large language models (LLMs) adopt activation functions without intrinsic activation sparsity (e.g., GELU and Swish). Some recent efforts have explored introducing ReLU or its variants as the substitutive activation function to help LLMs achieve activation sparsity and inference acceleration, but few can simultaneously obtain high sparsity and comparable model performance. This paper introduces an effective sparsification method named \"ProSparse\" to push LLMs for higher activation sparsity without decreasing model performance. Specifically, after substituting the activation function of LLMs with ReLU, ProSparse adopts progressive sparsity regularization with a factor smoothly increasing along sine curves in multiple stages. This can enhance activation sparsity and alleviate performance degradation by avoiding radical shifts in activation distribution. With ProSparse, we obtain high sparsity of 89.32% and 88.80% for LLaMA2-7B and LLaMA2-13B, respectively, achieving comparable performance to their original Swish-activated versions. Our inference acceleration experiments further demonstrate the practical acceleration brought by higher activation sparsity.","sentences":["Activation sparsity refers to the existence of considerable weakly-contributed elements among activation outputs.","As a prevalent property of the models using the ReLU activation function, it has been proven a promising paradigm to boost model inference efficiency.","Nevertheless, most large language models (LLMs) adopt activation functions without intrinsic activation sparsity (e.g., GELU and Swish).","Some recent efforts have explored introducing ReLU or its variants as the substitutive activation function to help LLMs achieve activation sparsity and inference acceleration, but few can simultaneously obtain high sparsity and comparable model performance.","This paper introduces an effective sparsification method named \"ProSparse\" to push LLMs for higher activation sparsity without decreasing model performance.","Specifically, after substituting the activation function of LLMs with ReLU, ProSparse adopts progressive sparsity regularization with a factor smoothly increasing along sine curves in multiple stages.","This can enhance activation sparsity and alleviate performance degradation by avoiding radical shifts in activation distribution.","With ProSparse, we obtain high sparsity of 89.32% and 88.80% for LLaMA2-7B and LLaMA2-13B, respectively, achieving comparable performance to their original Swish-activated versions.","Our inference acceleration experiments further demonstrate the practical acceleration brought by higher activation sparsity."],"url":"http://arxiv.org/abs/2402.13516v1","category":"cs.LG"}
{"created":"2024-02-21 03:55:02","title":"Self-DC: When to retrieve and When to generate? Self Divide-and-Conquer for Compositional Unknown Questions","abstract":"Retrieve-then-read and generate-then-read are two typical solutions to handle unknown and known questions in open-domain question-answering, while the former retrieves necessary external knowledge and the later prompt the large language models to generate internal known knowledge encoded in the parameters. However, few of previous works consider the compositional unknown questions, which consist of several known or unknown sub-questions. Thus, simple binary classification (known or unknown) becomes sub-optimal and inefficient since it will call external retrieval excessively for each compositional unknown question. To this end, we propose the first Compositional unknown Question-Answering dataset (CuQA), and introduce a Self Divide-and-Conquer (Self-DC) framework to empower LLMs to adaptively call different methods on-demand, resulting in better performance and efficiency. Experimental results on two datasets (CuQA and FreshQA) demonstrate that Self-DC can achieve comparable or even better performance with much more less retrieval times compared with several strong baselines.","sentences":["Retrieve-then-read and generate-then-read are two typical solutions to handle unknown and known questions in open-domain question-answering, while the former retrieves necessary external knowledge and the later prompt the large language models to generate internal known knowledge encoded in the parameters.","However, few of previous works consider the compositional unknown questions, which consist of several known or unknown sub-questions.","Thus, simple binary classification (known or unknown) becomes sub-optimal and inefficient since it will call external retrieval excessively for each compositional unknown question.","To this end, we propose the first Compositional unknown Question-Answering dataset (CuQA), and introduce a Self Divide-and-Conquer (Self-DC) framework to empower LLMs to adaptively call different methods on-demand, resulting in better performance and efficiency.","Experimental results on two datasets (CuQA and FreshQA) demonstrate that Self-DC can achieve comparable or even better performance with much more less retrieval times compared with several strong baselines."],"url":"http://arxiv.org/abs/2402.13514v1","category":"cs.CL"}
{"created":"2024-02-21 03:51:34","title":"From Self-Attention to Markov Models: Unveiling the Dynamics of Generative Transformers","abstract":"Modern language models rely on the transformer architecture and attention mechanism to perform language understanding and text generation. In this work, we study learning a 1-layer self-attention model from a set of prompts and associated output data sampled from the model. We first establish a precise mapping between the self-attention mechanism and Markov models: Inputting a prompt to the model samples the output token according to a context-conditioned Markov chain (CCMC) which weights the transition matrix of a base Markov chain. Additionally, incorporating positional encoding results in position-dependent scaling of the transition probabilities. Building on this formalism, we develop identifiability/coverage conditions for the prompt distribution that guarantee consistent estimation and establish sample complexity guarantees under IID samples. Finally, we study the problem of learning from a single output trajectory generated from an initial prompt. We characterize an intriguing winner-takes-all phenomenon where the generative process implemented by self-attention collapses into sampling a limited subset of tokens due to its non-mixing nature. This provides a mathematical explanation to the tendency of modern LLMs to generate repetitive text. In summary, the equivalence to CCMC provides a simple but powerful framework to study self-attention and its properties.","sentences":["Modern language models rely on the transformer architecture and attention mechanism to perform language understanding and text generation.","In this work, we study learning a 1-layer self-attention model from a set of prompts and associated output data sampled from the model.","We first establish a precise mapping between the self-attention mechanism and Markov models: Inputting a prompt to the model samples the output token according to a context-conditioned Markov chain (CCMC) which weights the transition matrix of a base Markov chain.","Additionally, incorporating positional encoding results in position-dependent scaling of the transition probabilities.","Building on this formalism, we develop identifiability/coverage conditions for the prompt distribution that guarantee consistent estimation and establish sample complexity guarantees under IID samples.","Finally, we study the problem of learning from a single output trajectory generated from an initial prompt.","We characterize an intriguing winner-takes-all phenomenon where the generative process implemented by self-attention collapses into sampling a limited subset of tokens due to its non-mixing nature.","This provides a mathematical explanation to the tendency of modern LLMs to generate repetitive text.","In summary, the equivalence to CCMC provides a simple but powerful framework to study self-attention and its properties."],"url":"http://arxiv.org/abs/2402.13512v1","category":"cs.LG"}
{"created":"2024-02-21 03:43:49","title":"Prediction of the Economic Behavior of Fishery Biotechnology Companies Based on Machine Learning-Based Deep Metacellular Automata","abstract":"Ocean warming significantly affects the fishing industry, with species like Scottish herring and mackerel migrating northwards. Our research, a fusion of artificial intelligence, data science, and operations research, addresses this crisis. Using Long Short Term Memory networks, we forecast sea surface temperatures (SST) and model fish migratory patterns with Enhanced Cellular Automata. A corrective factor within our model adjusts for human impact on SST, guiding diverse mitigation scenarios. We apply operational research to strategize responses, including the modernization of fishing vessels as a less costly alternative to relocation. Our data-driven approach, suggesting fleet modernization, strategic relocation, and product diversification, offers an effective approach to mitigating the threats to the ocean warming phenomenon.","sentences":["Ocean warming significantly affects the fishing industry, with species like Scottish herring and mackerel migrating northwards.","Our research, a fusion of artificial intelligence, data science, and operations research, addresses this crisis.","Using Long Short Term Memory networks, we forecast sea surface temperatures (SST) and model fish migratory patterns with Enhanced Cellular Automata.","A corrective factor within our model adjusts for human impact on SST, guiding diverse mitigation scenarios.","We apply operational research to strategize responses, including the modernization of fishing vessels as a less costly alternative to relocation.","Our data-driven approach, suggesting fleet modernization, strategic relocation, and product diversification, offers an effective approach to mitigating the threats to the ocean warming phenomenon."],"url":"http://arxiv.org/abs/2402.13509v1","category":"stat.AP"}
{"created":"2024-02-21 03:25:40","title":"Statistical Analyses of Solar Prominences and Active Region Features in 304 \u00c5 Filtergrams detected via Deep Learning","abstract":"Solar active regions (ARs) are areas on the Sun with very strong magnetic fields where various activities take place. Prominences are one of the typical solar features in the solar atmosphere, whose eruptions often lead to solar flares and coronal mass ejections (CMEs). Therefore, studying their morphological features and their relationship with solar activity is useful in predicting eruptive events and in understanding the long-term evolution of solar activities. A huge amount of data have been collected from various ground-based telescopes and satellites. The massive data make human inspection difficult. For this purpose, we developed an automated detection method for prominences and ARs above the solar limb based on deep learning techniques. We applied it to process the 304 \\AA data obtained by SDO/AIA from 2010 May 13 to 2020 December 31. Besides the butterfly diagrams and latitudinal migrations of the prominences and ARs during solar cycle 24, the variations of their morphological features (such as the locations, areas, heights, and widths) with the calendar years and the latitude bands were analyzed. Most of these statistical results based on our new method are in agreement with previous studies, which also guarantees the validity of our method. The N-S asymmetry indices of the prominences and ARs show that the northern hemisphere dominates in solar cycle 24, except for 2012--2015, and 2020 for ARs. The high-latitude prominences show much stronger N-S asymmetry that the northern hemisphere is dominant in $\\sim$2011 and $\\sim$2015 and the southern hemisphere is dominant during 2016--2019.","sentences":["Solar active regions (ARs) are areas on the Sun with very strong magnetic fields where various activities take place.","Prominences are one of the typical solar features in the solar atmosphere, whose eruptions often lead to solar flares and coronal mass ejections (CMEs).","Therefore, studying their morphological features and their relationship with solar activity is useful in predicting eruptive events and in understanding the long-term evolution of solar activities.","A huge amount of data have been collected from various ground-based telescopes and satellites.","The massive data make human inspection difficult.","For this purpose, we developed an automated detection method for prominences and ARs above the solar limb based on deep learning techniques.","We applied it to process the 304 \\AA data obtained by SDO/AIA from 2010 May 13 to 2020 December 31.","Besides the butterfly diagrams and latitudinal migrations of the prominences and ARs during solar cycle 24, the variations of their morphological features (such as the locations, areas, heights, and widths) with the calendar years and the latitude bands were analyzed.","Most of these statistical results based on our new method are in agreement with previous studies, which also guarantees the validity of our method.","The N-S asymmetry indices of the prominences and ARs show that the northern hemisphere dominates in solar cycle 24, except for 2012--2015, and 2020 for ARs.","The high-latitude prominences show much stronger N-S asymmetry that the northern hemisphere is dominant in $\\sim$2011 and $\\sim$2015 and the southern hemisphere is dominant during 2016--2019."],"url":"http://arxiv.org/abs/2402.13502v1","category":"astro-ph.SR"}
{"created":"2024-02-21 03:21:29","title":"Benchmarking and Dissecting the Nvidia Hopper GPU Architecture","abstract":"Graphics processing units (GPUs) are continually evolving to cater to the computational demands of contemporary general-purpose workloads, particularly those driven by artificial intelligence (AI) utilizing deep learning techniques. A substantial body of studies have been dedicated to dissecting the microarchitectural metrics characterizing diverse GPU generations, which helps researchers understand the hardware details and leverage them to optimize the GPU programs. However, the latest Hopper GPUs present a set of novel attributes, including new tensor cores supporting FP8, DPX, and distributed shared memory. Their details still remain mysterious in terms of performance and operational characteristics. In this research, we propose an extensive benchmarking study focused on the Hopper GPU. The objective is to unveil its microarchitectural intricacies through an examination of the new instruction-set architecture (ISA) of Nvidia GPUs and the utilization of new CUDA APIs. Our approach involves two main aspects. Firstly, we conduct conventional latency and throughput comparison benchmarks across the three most recent GPU architectures, namely Hopper, Ada, and Ampere. Secondly, we delve into a comprehensive discussion and benchmarking of the latest Hopper features, encompassing the Hopper DPX dynamic programming (DP) instruction set, distributed shared memory, and the availability of FP8 tensor cores. The microbenchmarking results we present offer a deeper understanding of the novel GPU AI function units and programming features introduced by the Hopper architecture. This newfound understanding is expected to greatly facilitate software optimization and modeling efforts for GPU architectures. To the best of our knowledge, this study makes the first attempt to demystify the tensor core performance and programming instruction sets unique to Hopper GPUs.","sentences":["Graphics processing units (GPUs) are continually evolving to cater to the computational demands of contemporary general-purpose workloads, particularly those driven by artificial intelligence (AI) utilizing deep learning techniques.","A substantial body of studies have been dedicated to dissecting the microarchitectural metrics characterizing diverse GPU generations, which helps researchers understand the hardware details and leverage them to optimize the GPU programs.","However, the latest Hopper GPUs present a set of novel attributes, including new tensor cores supporting FP8, DPX, and distributed shared memory.","Their details still remain mysterious in terms of performance and operational characteristics.","In this research, we propose an extensive benchmarking study focused on the Hopper GPU.","The objective is to unveil its microarchitectural intricacies through an examination of the new instruction-set architecture (ISA) of Nvidia GPUs and the utilization of new CUDA APIs.","Our approach involves two main aspects.","Firstly, we conduct conventional latency and throughput comparison benchmarks across the three most recent GPU architectures, namely Hopper, Ada, and Ampere.","Secondly, we delve into a comprehensive discussion and benchmarking of the latest Hopper features, encompassing the Hopper DPX dynamic programming (DP) instruction set, distributed shared memory, and the availability of FP8 tensor cores.","The microbenchmarking results we present offer a deeper understanding of the novel GPU AI function units and programming features introduced by the Hopper architecture.","This newfound understanding is expected to greatly facilitate software optimization and modeling efforts for GPU architectures.","To the best of our knowledge, this study makes the first attempt to demystify the tensor core performance and programming instruction sets unique to Hopper GPUs."],"url":"http://arxiv.org/abs/2402.13499v1","category":"cs.AR"}
{"created":"2024-02-21 03:09:21","title":"GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis","abstract":"Large Language Models (LLMs) face threats from unsafe prompts. Existing methods for detecting unsafe prompts are primarily online moderation APIs or finetuned LLMs. These strategies, however, often require extensive and resource-intensive data collection and training processes. In this study, we propose GradSafe, which effectively detects unsafe prompts by scrutinizing the gradients of safety-critical parameters in LLMs. Our methodology is grounded in a pivotal observation: the gradients of an LLM's loss for unsafe prompts paired with compliance response exhibit similar patterns on certain safety-critical parameters. In contrast, safe prompts lead to markedly different gradient patterns. Building on this observation, GradSafe analyzes the gradients from prompts (paired with compliance responses) to accurately detect unsafe prompts. We show that GradSafe, applied to Llama-2 without further training, outperforms Llama Guard, despite its extensive finetuning with a large dataset, in detecting unsafe prompts. This superior performance is consistent across both zero-shot and adaptation scenarios, as evidenced by our evaluations on the ToxicChat and XSTest. The source code is available at https://github.com/xyq7/GradSafe.","sentences":["Large Language Models (LLMs) face threats from unsafe prompts.","Existing methods for detecting unsafe prompts are primarily online moderation APIs or finetuned LLMs.","These strategies, however, often require extensive and resource-intensive data collection and training processes.","In this study, we propose GradSafe, which effectively detects unsafe prompts by scrutinizing the gradients of safety-critical parameters in LLMs.","Our methodology is grounded in a pivotal observation: the gradients of an LLM's loss for unsafe prompts paired with compliance response exhibit similar patterns on certain safety-critical parameters.","In contrast, safe prompts lead to markedly different gradient patterns.","Building on this observation, GradSafe analyzes the gradients from prompts (paired with compliance responses) to accurately detect unsafe prompts.","We show that GradSafe, applied to Llama-2 without further training, outperforms Llama Guard, despite its extensive finetuning with a large dataset, in detecting unsafe prompts.","This superior performance is consistent across both zero-shot and adaptation scenarios, as evidenced by our evaluations on the ToxicChat and XSTest.","The source code is available at https://github.com/xyq7/GradSafe."],"url":"http://arxiv.org/abs/2402.13494v1","category":"cs.CL"}
{"created":"2024-02-21 02:45:46","title":"Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks","abstract":"Despite large successes of recent language models on diverse tasks, they suffer from severe performance degeneration in low-resource settings with limited training data available. Many existing works tackle this problem by generating synthetic data from the training data and then training models on them, recently using Large Language Models (LLMs). However, in low-resource settings, the amount of seed data samples to use for data augmentation is very small, which makes generated samples suboptimal and less diverse. To tackle this challenge, we propose a novel method that augments training data by incorporating a wealth of examples from other datasets, along with the given training data. Specifically, we first retrieve the relevant instances from other datasets, such as their input-output pairs or contexts, based on their similarities with the given seed data, and then prompt LLMs to generate new samples with the contextual information within and across the original and retrieved samples. This approach can ensure that the generated data is not only relevant but also more diverse than what could be achieved using the limited seed data alone. We validate our proposed Retrieval-Augmented Data Augmentation (RADA) framework on multiple datasets under low-resource settings of training and test-time data augmentation scenarios, on which it outperforms existing LLM-powered data augmentation baselines.","sentences":["Despite large successes of recent language models on diverse tasks, they suffer from severe performance degeneration in low-resource settings with limited training data available.","Many existing works tackle this problem by generating synthetic data from the training data and then training models on them, recently using Large Language Models (LLMs).","However, in low-resource settings, the amount of seed data samples to use for data augmentation is very small, which makes generated samples suboptimal and less diverse.","To tackle this challenge, we propose a novel method that augments training data by incorporating a wealth of examples from other datasets, along with the given training data.","Specifically, we first retrieve the relevant instances from other datasets, such as their input-output pairs or contexts, based on their similarities with the given seed data, and then prompt LLMs to generate new samples with the contextual information within and across the original and retrieved samples.","This approach can ensure that the generated data is not only relevant but also more diverse than what could be achieved using the limited seed data alone.","We validate our proposed Retrieval-Augmented Data Augmentation (RADA) framework on multiple datasets under low-resource settings of training and test-time data augmentation scenarios, on which it outperforms existing LLM-powered data augmentation baselines."],"url":"http://arxiv.org/abs/2402.13482v1","category":"cs.CL"}
{"created":"2024-02-21 02:44:33","title":"Learning to Model Diverse Driving Behaviors in Highly Interactive Autonomous Driving Scenarios with Multi-Agent Reinforcement Learning","abstract":"Autonomous vehicles trained through Multi-Agent Reinforcement Learning (MARL) have shown impressive results in many driving scenarios. However, the performance of these trained policies can be impacted when faced with diverse driving styles and personalities, particularly in highly interactive situations. This is because conventional MARL algorithms usually operate under the assumption of fully cooperative behavior among all agents and focus on maximizing team rewards during training. To address this issue, we introduce the Personality Modeling Network (PeMN), which includes a cooperation value function and personality parameters to model the varied interactions in high-interactive scenarios. The PeMN also enables the training of a background traffic flow with diverse behaviors, thereby improving the performance and generalization of the ego vehicle. Our extensive experimental studies, which incorporate different personality parameters in high-interactive driving scenarios, demonstrate that the personality parameters effectively model diverse driving styles and that policies trained with PeMN demonstrate better generalization compared to traditional MARL methods.","sentences":["Autonomous vehicles trained through Multi-Agent Reinforcement Learning (MARL) have shown impressive results in many driving scenarios.","However, the performance of these trained policies can be impacted when faced with diverse driving styles and personalities, particularly in highly interactive situations.","This is because conventional MARL algorithms usually operate under the assumption of fully cooperative behavior among all agents and focus on maximizing team rewards during training.","To address this issue, we introduce the Personality Modeling Network (PeMN), which includes a cooperation value function and personality parameters to model the varied interactions in high-interactive scenarios.","The PeMN also enables the training of a background traffic flow with diverse behaviors, thereby improving the performance and generalization of the ego vehicle.","Our extensive experimental studies, which incorporate different personality parameters in high-interactive driving scenarios, demonstrate that the personality parameters effectively model diverse driving styles and that policies trained with PeMN demonstrate better generalization compared to traditional MARL methods."],"url":"http://arxiv.org/abs/2402.13481v1","category":"cs.RO"}
{"created":"2024-02-21 02:16:59","title":"Multi-scale Spatio-temporal Transformer-based Imbalanced Longitudinal Learning for Glaucoma Forecasting from Irregular Time Series Images","abstract":"Glaucoma is one of the major eye diseases that leads to progressive optic nerve fiber damage and irreversible blindness, afflicting millions of individuals. Glaucoma forecast is a good solution to early screening and intervention of potential patients, which is helpful to prevent further deterioration of the disease. It leverages a series of historical fundus images of an eye and forecasts the likelihood of glaucoma occurrence in the future. However, the irregular sampling nature and the imbalanced class distribution are two challenges in the development of disease forecasting approaches. To this end, we introduce the Multi-scale Spatio-temporal Transformer Network (MST-former) based on the transformer architecture tailored for sequential image inputs, which can effectively learn representative semantic information from sequential images on both temporal and spatial dimensions. Specifically, we employ a multi-scale structure to extract features at various resolutions, which can largely exploit rich spatial information encoded in each image. Besides, we design a time distance matrix to scale time attention in a non-linear manner, which could effectively deal with the irregularly sampled data. Furthermore, we introduce a temperature-controlled Balanced Softmax Cross-entropy loss to address the class imbalance issue. Extensive experiments on the Sequential fundus Images for Glaucoma Forecast (SIGF) dataset demonstrate the superiority of the proposed MST-former method, achieving an AUC of 98.6% for glaucoma forecasting. Besides, our method shows excellent generalization capability on the Alzheimer's Disease Neuroimaging Initiative (ADNI) MRI dataset, with an accuracy of 90.3% for mild cognitive impairment and Alzheimer's disease prediction, outperforming the compared method by a large margin.","sentences":["Glaucoma is one of the major eye diseases that leads to progressive optic nerve fiber damage and irreversible blindness, afflicting millions of individuals.","Glaucoma forecast is a good solution to early screening and intervention of potential patients, which is helpful to prevent further deterioration of the disease.","It leverages a series of historical fundus images of an eye and forecasts the likelihood of glaucoma occurrence in the future.","However, the irregular sampling nature and the imbalanced class distribution are two challenges in the development of disease forecasting approaches.","To this end, we introduce the Multi-scale Spatio-temporal Transformer Network (MST-former) based on the transformer architecture tailored for sequential image inputs, which can effectively learn representative semantic information from sequential images on both temporal and spatial dimensions.","Specifically, we employ a multi-scale structure to extract features at various resolutions, which can largely exploit rich spatial information encoded in each image.","Besides, we design a time distance matrix to scale time attention in a non-linear manner, which could effectively deal with the irregularly sampled data.","Furthermore, we introduce a temperature-controlled Balanced Softmax Cross-entropy loss to address the class imbalance issue.","Extensive experiments on the Sequential fundus Images for Glaucoma Forecast (SIGF) dataset demonstrate the superiority of the proposed MST-former method, achieving an AUC of 98.6% for glaucoma forecasting.","Besides, our method shows excellent generalization capability on the Alzheimer's Disease Neuroimaging Initiative (ADNI) MRI dataset, with an accuracy of 90.3% for mild cognitive impairment and Alzheimer's disease prediction, outperforming the compared method by a large margin."],"url":"http://arxiv.org/abs/2402.13475v1","category":"cs.CV"}
{"created":"2024-02-21 01:45:47","title":"Leveraging Demonstrator-perceived Precision for Safe Interactive Imitation Learning of Clearance-limited Tasks","abstract":"Interactive imitation learning is an efficient, model-free method through which a robot can learn a task by repetitively iterating an execution of a learning policy and a data collection by querying human demonstrations. However, deploying unmatured policies for clearance-limited tasks, like industrial insertion, poses significant collision risks. For such tasks, a robot should detect the collision risks and request intervention by ceding control to a human when collisions are imminent. The former requires an accurate model of the environment, a need that significantly limits the scope of IIL applications. In contrast, humans implicitly demonstrate environmental precision by adjusting their behavior to avoid collisions when performing tasks. Inspired by human behavior, this paper presents a novel interactive learning method that uses demonstrator-perceived precision as a criterion for human intervention called Demonstrator-perceived Precision-aware Interactive Imitation Learning (DPIIL). DPIIL captures precision by observing the speed-accuracy trade-off exhibited in human demonstrations and cedes control to a human to avoid collisions in states where high precision is estimated. DPIIL improves the safety of interactive policy learning and ensures efficiency without explicitly providing precise information of the environment. We assessed DPIIL's effectiveness through simulations and real-robot experiments that trained a UR5e 6-DOF robotic arm to perform assembly tasks. Our results significantly improved training safety, and our best performance compared favorably with other learning methods.","sentences":["Interactive imitation learning is an efficient, model-free method through which a robot can learn a task by repetitively iterating an execution of a learning policy and a data collection by querying human demonstrations.","However, deploying unmatured policies for clearance-limited tasks, like industrial insertion, poses significant collision risks.","For such tasks, a robot should detect the collision risks and request intervention by ceding control to a human when collisions are imminent.","The former requires an accurate model of the environment, a need that significantly limits the scope of IIL applications.","In contrast, humans implicitly demonstrate environmental precision by adjusting their behavior to avoid collisions when performing tasks.","Inspired by human behavior, this paper presents a novel interactive learning method that uses demonstrator-perceived precision as a criterion for human intervention called Demonstrator-perceived Precision-aware Interactive Imitation Learning (DPIIL).","DPIIL captures precision by observing the speed-accuracy trade-off exhibited in human demonstrations and cedes control to a human to avoid collisions in states where high precision is estimated.","DPIIL improves the safety of interactive policy learning and ensures efficiency without explicitly providing precise information of the environment.","We assessed DPIIL's effectiveness through simulations and real-robot experiments that trained a UR5e 6-DOF robotic arm to perform assembly tasks.","Our results significantly improved training safety, and our best performance compared favorably with other learning methods."],"url":"http://arxiv.org/abs/2402.13466v1","category":"cs.RO"}
{"created":"2024-02-21 01:44:15","title":"Unsupervised learning based object detection using Contrastive Learning","abstract":"Training image-based object detectors presents formidable challenges, as it entails not only the complexities of object detection but also the added intricacies of precisely localizing objects within potentially diverse and noisy environments. However, the collection of imagery itself can often be straightforward; for instance, cameras mounted in vehicles can effortlessly capture vast amounts of data in various real-world scenarios. In light of this, we introduce a groundbreaking method for training single-stage object detectors through unsupervised/self-supervised learning.   Our state-of-the-art approach has the potential to revolutionize the labeling process, substantially reducing the time and cost associated with manual annotation. Furthermore, it paves the way for previously unattainable research opportunities, particularly for large, diverse, and challenging datasets lacking extensive labels.   In contrast to prevalent unsupervised learning methods that primarily target classification tasks, our approach takes on the unique challenge of object detection. We pioneer the concept of intra-image contrastive learning alongside inter-image counterparts, enabling the acquisition of crucial location information essential for object detection. The method adeptly learns and represents this location information, yielding informative heatmaps. Our results showcase an outstanding accuracy of \\textbf{89.2\\%}, marking a significant breakthrough of approximately \\textbf{15x} over random initialization in the realm of unsupervised object detection within the field of computer vision.","sentences":["Training image-based object detectors presents formidable challenges, as it entails not only the complexities of object detection but also the added intricacies of precisely localizing objects within potentially diverse and noisy environments.","However, the collection of imagery itself can often be straightforward; for instance, cameras mounted in vehicles can effortlessly capture vast amounts of data in various real-world scenarios.","In light of this, we introduce a groundbreaking method for training single-stage object detectors through unsupervised/self-supervised learning.   ","Our state-of-the-art approach has the potential to revolutionize the labeling process, substantially reducing the time and cost associated with manual annotation.","Furthermore, it paves the way for previously unattainable research opportunities, particularly for large, diverse, and challenging datasets lacking extensive labels.   ","In contrast to prevalent unsupervised learning methods that primarily target classification tasks, our approach takes on the unique challenge of object detection.","We pioneer the concept of intra-image contrastive learning alongside inter-image counterparts, enabling the acquisition of crucial location information essential for object detection.","The method adeptly learns and represents this location information, yielding informative heatmaps.","Our results showcase an outstanding accuracy of \\textbf{89.2\\%}, marking a significant breakthrough of approximately \\textbf{15x} over random initialization in the realm of unsupervised object detection within the field of computer vision."],"url":"http://arxiv.org/abs/2402.13465v1","category":"cs.CV"}
{"created":"2024-02-21 01:39:56","title":"RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models","abstract":"The application scope of large language models (LLMs) is increasingly expanding. In practical use, users might provide feedback based on the model's output, hoping for a responsive model that can complete responses according to their feedback. Whether the model can appropriately respond to users' refuting feedback and consistently follow through with execution has not been thoroughly analyzed. In light of this, this paper proposes a comprehensive benchmark, RefuteBench, covering tasks such as question answering, machine translation, and email writing. The evaluation aims to assess whether models can positively accept feedback in form of refuting instructions and whether they can consistently adhere to user demands throughout the conversation. We conduct evaluations on numerous LLMs and find that LLMs are stubborn, i.e. exhibit inclination to their internal knowledge, often failing to comply with user feedback. Additionally, as the length of the conversation increases, models gradually forget the user's stated feedback and roll back to their own responses. We further propose a recall-and-repeat prompts as a simple and effective way to enhance the model's responsiveness to feedback.","sentences":["The application scope of large language models (LLMs) is increasingly expanding.","In practical use, users might provide feedback based on the model's output, hoping for a responsive model that can complete responses according to their feedback.","Whether the model can appropriately respond to users' refuting feedback and consistently follow through with execution has not been thoroughly analyzed.","In light of this, this paper proposes a comprehensive benchmark, RefuteBench, covering tasks such as question answering, machine translation, and email writing.","The evaluation aims to assess whether models can positively accept feedback in form of refuting instructions and whether they can consistently adhere to user demands throughout the conversation.","We conduct evaluations on numerous LLMs and find that LLMs are stubborn, i.e. exhibit inclination to their internal knowledge, often failing to comply with user feedback.","Additionally, as the length of the conversation increases, models gradually forget the user's stated feedback and roll back to their own responses.","We further propose a recall-and-repeat prompts as a simple and effective way to enhance the model's responsiveness to feedback."],"url":"http://arxiv.org/abs/2402.13463v1","category":"cs.CL"}
{"created":"2024-02-21 01:35:26","title":"Potential and Challenges of Model Editing for Social Debiasing","abstract":"Large language models (LLMs) trained on vast corpora suffer from inevitable stereotype biases. Mitigating these biases with fine-tuning could be both costly and data-hungry. Model editing methods, which focus on modifying LLMs in a post-hoc manner, are of great potential to address debiasing. However, it lacks a comprehensive study that facilitates both internal and external model editing methods, supports various bias types, as well as understands the pros and cons of applying editing methods to stereotypical debiasing. To mitigate this gap, we carefully formulate social debiasing into an editing problem and benchmark seven existing model editing algorithms on stereotypical debiasing, i.e., debias editing. Our findings in three scenarios reveal both the potential and challenges of debias editing: (1) Existing model editing methods can effectively preserve knowledge and mitigate biases, while the generalization of debias effect from edited sentences to semantically equivalent sentences is limited.(2) Sequential editing highlights the robustness of SERAC (Mitchell et al. 2022b), while internal editing methods degenerate with the number of edits. (3) Model editing algorithms achieve generalization towards unseen biases both within the same type and from different types. In light of these findings, we further propose two simple but effective methods to improve debias editing, and experimentally show the effectiveness of the proposed methods.","sentences":["Large language models (LLMs) trained on vast corpora suffer from inevitable stereotype biases.","Mitigating these biases with fine-tuning could be both costly and data-hungry.","Model editing methods, which focus on modifying LLMs in a post-hoc manner, are of great potential to address debiasing.","However, it lacks a comprehensive study that facilitates both internal and external model editing methods, supports various bias types, as well as understands the pros and cons of applying editing methods to stereotypical debiasing.","To mitigate this gap, we carefully formulate social debiasing into an editing problem and benchmark seven existing model editing algorithms on stereotypical debiasing, i.e., debias editing.","Our findings in three scenarios reveal both the potential and challenges of debias editing: (1) Existing model editing methods can effectively preserve knowledge and mitigate biases, while the generalization of debias effect from edited sentences to semantically equivalent sentences is limited.(2)","Sequential editing highlights the robustness of SERAC (Mitchell et al. 2022b), while internal editing methods degenerate with the number of edits.","(3) Model editing algorithms achieve generalization towards unseen biases both within the same type and from different types.","In light of these findings, we further propose two simple but effective methods to improve debias editing, and experimentally show the effectiveness of the proposed methods."],"url":"http://arxiv.org/abs/2402.13462v1","category":"cs.CL"}
{"created":"2024-02-21 01:26:39","title":"LLM Jailbreak Attack versus Defense Techniques -- A Comprehensive Study","abstract":"Large Language Models (LLMS) have increasingly become central to generating content with potential societal impacts. Notably, these models have demonstrated capabilities for generating content that could be deemed harmful. To mitigate these risks, researchers have adopted safety training techniques to align model outputs with societal values to curb the generation of malicious content. However, the phenomenon of \"jailbreaking\", where carefully crafted prompts elicit harmful responses from models, persists as a significant challenge. This research conducts a comprehensive analysis of existing studies on jailbreaking LLMs and their defense techniques. We meticulously investigate nine attack techniques and seven defense techniques applied across three distinct language models: Vicuna, LLama, and GPT-3.5 Turbo. We aim to evaluate the effectiveness of these attack and defense techniques. Our findings reveal that existing white-box attacks underperform compared to universal techniques and that including special tokens in the input significantly affects the likelihood of successful attacks. This research highlights the need to concentrate on the security facets of LLMs. Additionally, we contribute to the field by releasing our datasets and testing framework, aiming to foster further research into LLM security. We believe these contributions will facilitate the exploration of security measures within this domain.","sentences":["Large Language Models (LLMS) have increasingly become central to generating content with potential societal impacts.","Notably, these models have demonstrated capabilities for generating content that could be deemed harmful.","To mitigate these risks, researchers have adopted safety training techniques to align model outputs with societal values to curb the generation of malicious content.","However, the phenomenon of \"jailbreaking\", where carefully crafted prompts elicit harmful responses from models, persists as a significant challenge.","This research conducts a comprehensive analysis of existing studies on jailbreaking LLMs and their defense techniques.","We meticulously investigate nine attack techniques and seven defense techniques applied across three distinct language models: Vicuna, LLama, and","GPT-3.5 Turbo.","We aim to evaluate the effectiveness of these attack and defense techniques.","Our findings reveal that existing white-box attacks underperform compared to universal techniques and that including special tokens in the input significantly affects the likelihood of successful attacks.","This research highlights the need to concentrate on the security facets of LLMs.","Additionally, we contribute to the field by releasing our datasets and testing framework, aiming to foster further research into LLM security.","We believe these contributions will facilitate the exploration of security measures within this domain."],"url":"http://arxiv.org/abs/2402.13457v1","category":"cs.CR"}
{"created":"2024-02-21 01:11:28","title":"LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based on Twitter Data","abstract":"Prior research on Twitter (now X) data has provided positive evidence of its utility in developing supplementary health surveillance systems. In this study, we present a new framework to surveil public health, focusing on mental health (MH) outcomes. We hypothesize that locally posted tweets are indicative of local MH outcomes and collect tweets posted from 765 neighborhoods (census block groups) in the USA. We pair these tweets from each neighborhood with the corresponding MH outcome reported by the Center for Disease Control (CDC) to create a benchmark dataset, LocalTweets. With LocalTweets, we present the first population-level evaluation task for Twitter-based MH surveillance systems. We then develop an efficient and effective method, LocalHealth, for predicting MH outcomes based on LocalTweets. When used with GPT3.5, LocalHealth achieves the highest F1-score and accuracy of 0.7429 and 79.78\\%, respectively, a 59\\% improvement in F1-score over the GPT3.5 in zero-shot setting. We also utilize LocalHealth to extrapolate CDC's estimates to proxy unreported neighborhoods, achieving an F1-score of 0.7291. Our work suggests that Twitter data can be effectively leveraged to simulate neighborhood-level MH outcomes.","sentences":["Prior research on Twitter (now X) data has provided positive evidence of its utility in developing supplementary health surveillance systems.","In this study, we present a new framework to surveil public health, focusing on mental health (MH) outcomes.","We hypothesize that locally posted tweets are indicative of local MH outcomes and collect tweets posted from 765 neighborhoods (census block groups) in the USA.","We pair these tweets from each neighborhood with the corresponding MH outcome reported by the Center for Disease Control (CDC) to create a benchmark dataset, LocalTweets.","With LocalTweets, we present the first population-level evaluation task for Twitter-based MH surveillance systems.","We then develop an efficient and effective method, LocalHealth, for predicting MH outcomes based on LocalTweets.","When used with GPT3.5, LocalHealth achieves the highest F1-score and accuracy of 0.7429 and 79.78\\%, respectively, a 59\\% improvement in F1-score over the GPT3.5 in zero-shot setting.","We also utilize LocalHealth to extrapolate CDC's estimates to proxy unreported neighborhoods, achieving an F1-score of 0.7291.","Our work suggests that Twitter data can be effectively leveraged to simulate neighborhood-level MH outcomes."],"url":"http://arxiv.org/abs/2402.13452v1","category":"cs.SI"}
{"created":"2024-02-21 00:49:42","title":"ED-Copilot: Reduce Emergency Department Wait Time with Language Model Diagnostic Assistance","abstract":"In the emergency department (ED), patients undergo triage and multiple laboratory tests before diagnosis. This process is time-consuming, and causes ED crowding which significantly impacts patient mortality, medical errors, staff burnout, etc. This work proposes (time) cost-effective diagnostic assistance that explores the potential of artificial intelligence (AI) systems in assisting ED clinicians to make time-efficient and accurate diagnoses. Using publicly available patient data, we collaborate with ED clinicians to curate MIMIC-ED-Assist, a benchmark that measures the ability of AI systems in suggesting laboratory tests that minimize ED wait times, while correctly predicting critical outcomes such as death. We develop ED-Copilot which sequentially suggests patient-specific laboratory tests and makes diagnostic predictions. ED-Copilot uses a pre-trained bio-medical language model to encode patient information and reinforcement learning to minimize ED wait time and maximize prediction accuracy of critical outcomes. On MIMIC-ED-Assist, ED-Copilot improves prediction accuracy over baselines while halving average wait time from four hours to two hours. Ablation studies demonstrate the importance of model scale and use of a bio-medical language model. Further analyses reveal the necessity of personalized laboratory test suggestions for diagnosing patients with severe cases, as well as the potential of ED-Copilot in providing ED clinicians with informative laboratory test recommendations. Our code is available at https://github.com/cxcscmu/ED-Copilot.","sentences":["In the emergency department (ED), patients undergo triage and multiple laboratory tests before diagnosis.","This process is time-consuming, and causes ED crowding which significantly impacts patient mortality, medical errors, staff burnout, etc.","This work proposes (time) cost-effective diagnostic assistance that explores the potential of artificial intelligence (AI) systems in assisting ED clinicians to make time-efficient and accurate diagnoses.","Using publicly available patient data, we collaborate with ED clinicians to curate MIMIC-ED-Assist, a benchmark that measures the ability of AI systems in suggesting laboratory tests that minimize ED wait times, while correctly predicting critical outcomes such as death.","We develop ED-Copilot which sequentially suggests patient-specific laboratory tests and makes diagnostic predictions.","ED-Copilot uses a pre-trained bio-medical language model to encode patient information and reinforcement learning to minimize ED wait time and maximize prediction accuracy of critical outcomes.","On MIMIC-ED-Assist, ED-Copilot improves prediction accuracy over baselines while halving average wait time from four hours to two hours.","Ablation studies demonstrate the importance of model scale and use of a bio-medical language model.","Further analyses reveal the necessity of personalized laboratory test suggestions for diagnosing patients with severe cases, as well as the potential of ED-Copilot in providing ED clinicians with informative laboratory test recommendations.","Our code is available at https://github.com/cxcscmu/ED-Copilot."],"url":"http://arxiv.org/abs/2402.13448v1","category":"cs.CL"}
{"created":"2024-02-21 00:16:08","title":"A Neuro-Symbolic Approach to Multi-Agent RL for Interpretability and Probabilistic Decision Making","abstract":"Multi-agent reinforcement learning (MARL) is well-suited for runtime decision-making in optimizing the performance of systems where multiple agents coexist and compete for shared resources. However, applying common deep learning-based MARL solutions to real-world problems suffers from issues of interpretability, sample efficiency, partial observability, etc. To address these challenges, we present an event-driven formulation, where decision-making is handled by distributed co-operative MARL agents using neuro-symbolic methods. The recently introduced neuro-symbolic Logical Neural Networks (LNN) framework serves as a function approximator for the RL, to train a rules-based policy that is both logical and interpretable by construction. To enable decision-making under uncertainty and partial observability, we developed a novel probabilistic neuro-symbolic framework, Probabilistic Logical Neural Networks (PLNN), which combines the capabilities of logical reasoning with probabilistic graphical models. In PLNN, the upward/downward inference strategy, inherited from LNN, is coupled with belief bounds by setting the activation function for the logical operator associated with each neural network node to a probability-respecting generalization of the Fr\\'echet inequalities. These PLNN nodes form the unifying element that combines probabilistic logic and Bayes Nets, permitting inference for variables with unobserved states. We demonstrate our contributions by addressing key MARL challenges for power sharing in a system-on-chip application.","sentences":["Multi-agent reinforcement learning (MARL) is well-suited for runtime decision-making in optimizing the performance of systems where multiple agents coexist and compete for shared resources.","However, applying common deep learning-based MARL solutions to real-world problems suffers from issues of interpretability, sample efficiency, partial observability, etc.","To address these challenges, we present an event-driven formulation, where decision-making is handled by distributed co-operative MARL agents using neuro-symbolic methods.","The recently introduced neuro-symbolic Logical Neural Networks (LNN) framework serves as a function approximator for the RL, to train a rules-based policy that is both logical and interpretable by construction.","To enable decision-making under uncertainty and partial observability, we developed a novel probabilistic neuro-symbolic framework, Probabilistic Logical Neural Networks (PLNN), which combines the capabilities of logical reasoning with probabilistic graphical models.","In PLNN, the upward/downward inference strategy, inherited from LNN, is coupled with belief bounds by setting the activation function for the logical operator associated with each neural network node to a probability-respecting generalization of the Fr\\'echet inequalities.","These PLNN nodes form the unifying element that combines probabilistic logic and Bayes Nets, permitting inference for variables with unobserved states.","We demonstrate our contributions by addressing key MARL challenges for power sharing in a system-on-chip application."],"url":"http://arxiv.org/abs/2402.13440v1","category":"cs.AI"}
{"created":"2024-02-21 00:11:13","title":"Sketching AI Concepts with Capabilities and Examples: AI Innovation in the Intensive Care Unit","abstract":"Advances in artificial intelligence (AI) have enabled unprecedented capabilities, yet innovation teams struggle when envisioning AI concepts. Data science teams think of innovations users do not want, while domain experts think of innovations that cannot be built. A lack of effective ideation seems to be a breakdown point. How might multidisciplinary teams identify buildable and desirable use cases? This paper presents a first hand account of ideating AI concepts to improve critical care medicine. As a team of data scientists, clinicians, and HCI researchers, we conducted a series of design workshops to explore more effective approaches to AI concept ideation and problem formulation. We detail our process, the challenges we encountered, and practices and artifacts that proved effective. We discuss the research implications for improved collaboration and stakeholder engagement, and discuss the role HCI might play in reducing the high failure rate experienced in AI innovation.","sentences":["Advances in artificial intelligence (AI) have enabled unprecedented capabilities, yet innovation teams struggle when envisioning AI concepts.","Data science teams think of innovations users do not want, while domain experts think of innovations that cannot be built.","A lack of effective ideation seems to be a breakdown point.","How might multidisciplinary teams identify buildable and desirable use cases?","This paper presents a first hand account of ideating AI concepts to improve critical care medicine.","As a team of data scientists, clinicians, and HCI researchers, we conducted a series of design workshops to explore more effective approaches to AI concept ideation and problem formulation.","We detail our process, the challenges we encountered, and practices and artifacts that proved effective.","We discuss the research implications for improved collaboration and stakeholder engagement, and discuss the role HCI might play in reducing the high failure rate experienced in AI innovation."],"url":"http://arxiv.org/abs/2402.13437v1","category":"cs.HC"}
{"created":"2024-02-20 23:54:02","title":"DrBenchmark: A Large Language Understanding Evaluation Benchmark for French Biomedical Domain","abstract":"The biomedical domain has sparked a significant interest in the field of Natural Language Processing (NLP), which has seen substantial advancements with pre-trained language models (PLMs). However, comparing these models has proven challenging due to variations in evaluation protocols across different models. A fair solution is to aggregate diverse downstream tasks into a benchmark, allowing for the assessment of intrinsic PLMs qualities from various perspectives. Although still limited to few languages, this initiative has been undertaken in the biomedical field, notably English and Chinese. This limitation hampers the evaluation of the latest French biomedical models, as they are either assessed on a minimal number of tasks with non-standardized protocols or evaluated using general downstream tasks. To bridge this research gap and account for the unique sensitivities of French, we present the first-ever publicly available French biomedical language understanding benchmark called DrBenchmark. It encompasses 20 diversified tasks, including named-entity recognition, part-of-speech tagging, question-answering, semantic textual similarity, and classification. We evaluate 8 state-of-the-art pre-trained masked language models (MLMs) on general and biomedical-specific data, as well as English specific MLMs to assess their cross-lingual capabilities. Our experiments reveal that no single model excels across all tasks, while generalist models are sometimes still competitive.","sentences":["The biomedical domain has sparked a significant interest in the field of Natural Language Processing (NLP), which has seen substantial advancements with pre-trained language models (PLMs).","However, comparing these models has proven challenging due to variations in evaluation protocols across different models.","A fair solution is to aggregate diverse downstream tasks into a benchmark, allowing for the assessment of intrinsic PLMs qualities from various perspectives.","Although still limited to few languages, this initiative has been undertaken in the biomedical field, notably English and Chinese.","This limitation hampers the evaluation of the latest French biomedical models, as they are either assessed on a minimal number of tasks with non-standardized protocols or evaluated using general downstream tasks.","To bridge this research gap and account for the unique sensitivities of French, we present the first-ever publicly available French biomedical language understanding benchmark called DrBenchmark.","It encompasses 20 diversified tasks, including named-entity recognition, part-of-speech tagging, question-answering, semantic textual similarity, and classification.","We evaluate 8 state-of-the-art pre-trained masked language models (MLMs) on general and biomedical-specific data, as well as English specific MLMs to assess their cross-lingual capabilities.","Our experiments reveal that no single model excels across all tasks, while generalist models are sometimes still competitive."],"url":"http://arxiv.org/abs/2402.13432v1","category":"cs.CL"}
{"created":"2024-02-20 23:49:25","title":"LinkSAGE: Optimizing Job Matching Using Graph Neural Networks","abstract":"We present LinkSAGE, an innovative framework that integrates Graph Neural Networks (GNNs) into large-scale personalized job matching systems, designed to address the complex dynamics of LinkedIns extensive professional network. Our approach capitalizes on a novel job marketplace graph, the largest and most intricate of its kind in industry, with billions of nodes and edges. This graph is not merely extensive but also richly detailed, encompassing member and job nodes along with key attributes, thus creating an expansive and interwoven network. A key innovation in LinkSAGE is its training and serving methodology, which effectively combines inductive graph learning on a heterogeneous, evolving graph with an encoder-decoder GNN model. This methodology decouples the training of the GNN model from that of existing Deep Neural Nets (DNN) models, eliminating the need for frequent GNN retraining while maintaining up-to-date graph signals in near realtime, allowing for the effective integration of GNN insights through transfer learning. The subsequent nearline inference system serves the GNN encoder within a real-world setting, significantly reducing online latency and obviating the need for costly real-time GNN infrastructure. Validated across multiple online A/B tests in diverse product scenarios, LinkSAGE demonstrates marked improvements in member engagement, relevance matching, and member retention, confirming its generalizability and practical impact.","sentences":["We present LinkSAGE, an innovative framework that integrates Graph Neural Networks (GNNs) into large-scale personalized job matching systems, designed to address the complex dynamics of LinkedIns extensive professional network.","Our approach capitalizes on a novel job marketplace graph, the largest and most intricate of its kind in industry, with billions of nodes and edges.","This graph is not merely extensive but also richly detailed, encompassing member and job nodes along with key attributes, thus creating an expansive and interwoven network.","A key innovation in LinkSAGE is its training and serving methodology, which effectively combines inductive graph learning on a heterogeneous, evolving graph with an encoder-decoder GNN model.","This methodology decouples the training of the GNN model from that of existing Deep Neural Nets (DNN) models, eliminating the need for frequent GNN retraining while maintaining up-to-date graph signals in near realtime, allowing for the effective integration of GNN insights through transfer learning.","The subsequent nearline inference system serves the GNN encoder within a real-world setting, significantly reducing online latency and obviating the need for costly real-time GNN infrastructure.","Validated across multiple online A/B tests in diverse product scenarios, LinkSAGE demonstrates marked improvements in member engagement, relevance matching, and member retention, confirming its generalizability and practical impact."],"url":"http://arxiv.org/abs/2402.13430v1","category":"cs.LG"}
{"created":"2024-02-20 23:38:46","title":"Quantitative causality, causality-guided scientific discovery, and causal machine learning","abstract":"It has been said, arguably, that causality analysis should pave a promising way to interpretable deep learning and generalization. Incorporation of causality into artificial intelligence (AI) algorithms, however, is challenged with its vagueness, non-quantitiveness, computational inefficiency, etc. During the past 18 years, these challenges have been essentially resolved, with the establishment of a rigorous formalism of causality analysis initially motivated from atmospheric predictability. This not only opens a new field in the atmosphere-ocean science, namely, information flow, but also has led to scientific discoveries in other disciplines, such as quantum mechanics, neuroscience, financial economics, etc., through various applications. This note provides a brief review of the decade-long effort, including a list of major theoretical results, a sketch of the causal deep learning framework, and some representative real-world applications in geoscience pertaining to this journal, such as those on the anthropogenic cause of global warming, the decadal prediction of El Ni\\~no Modoki, the forecasting of an extreme drought in China, among others.","sentences":["It has been said, arguably, that causality analysis should pave a promising way to interpretable deep learning and generalization.","Incorporation of causality into artificial intelligence (AI) algorithms, however, is challenged with its vagueness, non-quantitiveness, computational inefficiency, etc.","During the past 18 years, these challenges have been essentially resolved, with the establishment of a rigorous formalism of causality analysis initially motivated from atmospheric predictability.","This not only opens a new field in the atmosphere-ocean science, namely, information flow, but also has led to scientific discoveries in other disciplines, such as quantum mechanics, neuroscience, financial economics, etc., through various applications.","This note provides a brief review of the decade-long effort, including a list of major theoretical results, a sketch of the causal deep learning framework, and some representative real-world applications in geoscience pertaining to this journal, such as those on the anthropogenic cause of global warming, the decadal prediction of El Ni\\~no Modoki, the forecasting of an extreme drought in China, among others."],"url":"http://arxiv.org/abs/2402.13427v1","category":"cs.AI"}
{"created":"2024-02-20 23:29:41","title":"Investigating the Histogram Loss in Regression","abstract":"It is becoming increasingly common in regression to train neural networks that model the entire distribution even if only the mean is required for prediction. This additional modeling often comes with performance gain and the reasons behind the improvement are not fully known. This paper investigates a recent approach to regression, the Histogram Loss, which involves learning the conditional distribution of the target variable by minimizing the cross-entropy between a target distribution and a flexible histogram prediction. We design theoretical and empirical analyses to determine why and when this performance gain appears, and how different components of the loss contribute to it. Our results suggest that the benefits of learning distributions in this setup come from improvements in optimization rather than learning a better representation. We then demonstrate the viability of the Histogram Loss in common deep learning applications without a need for costly hyperparameter tuning.","sentences":["It is becoming increasingly common in regression to train neural networks that model the entire distribution even if only the mean is required for prediction.","This additional modeling often comes with performance gain and the reasons behind the improvement are not fully known.","This paper investigates a recent approach to regression, the Histogram Loss, which involves learning the conditional distribution of the target variable by minimizing the cross-entropy between a target distribution and a flexible histogram prediction.","We design theoretical and empirical analyses to determine why and when this performance gain appears, and how different components of the loss contribute to it.","Our results suggest that the benefits of learning distributions in this setup come from improvements in optimization rather than learning a better representation.","We then demonstrate the viability of the Histogram Loss in common deep learning applications without a need for costly hyperparameter tuning."],"url":"http://arxiv.org/abs/2402.13425v1","category":"cs.LG"}
{"created":"2024-02-20 23:20:36","title":"Context-Aware Quantitative Risk Assessment Machine Learning Model for Drivers Distraction","abstract":"Risk mitigation techniques are critical to avoiding accidents associated with driving behaviour. We provide a novel Multi-Class Driver Distraction Risk Assessment (MDDRA) model that considers the vehicle, driver, and environmental data during a journey. MDDRA categorises the driver on a risk matrix as safe, careless, or dangerous. It offers flexibility in adjusting the parameters and weights to consider each event on a specific severity level. We collect real-world data using the Field Operation Test (TeleFOT), covering drivers using the same routes in the East Midlands, United Kingdom (UK). The results show that reducing road accidents caused by driver distraction is possible. We also study the correlation between distraction (driver, vehicle, and environment) and the classification severity based on a continuous distraction severity score. Furthermore, we apply machine learning techniques to classify and predict driver distraction according to severity levels to aid the transition of control from the driver to the vehicle (vehicle takeover) when a situation is deemed risky. The Ensemble Bagged Trees algorithm performed best, with an accuracy of 96.2%.","sentences":["Risk mitigation techniques are critical to avoiding accidents associated with driving behaviour.","We provide a novel Multi-Class Driver Distraction Risk Assessment (MDDRA) model that considers the vehicle, driver, and environmental data during a journey.","MDDRA categorises the driver on a risk matrix as safe, careless, or dangerous.","It offers flexibility in adjusting the parameters and weights to consider each event on a specific severity level.","We collect real-world data using the Field Operation Test (TeleFOT), covering drivers using the same routes in the East Midlands, United Kingdom (UK).","The results show that reducing road accidents caused by driver distraction is possible.","We also study the correlation between distraction (driver, vehicle, and environment) and the classification severity based on a continuous distraction severity score.","Furthermore, we apply machine learning techniques to classify and predict driver distraction according to severity levels to aid the transition of control from the driver to the vehicle (vehicle takeover) when a situation is deemed risky.","The Ensemble Bagged Trees algorithm performed best, with an accuracy of 96.2%."],"url":"http://arxiv.org/abs/2402.13421v1","category":"cs.LG"}
{"created":"2024-02-20 23:17:07","title":"Reward Bound for Behavioral Guarantee of Model-based Planning Agents","abstract":"Recent years have seen an emerging interest in the trustworthiness of machine learning-based agents in the wild, especially in robotics, to provide safety assurance for the industry. Obtaining behavioral guarantees for these agents remains an important problem. In this work, we focus on guaranteeing a model-based planning agent reaches a goal state within a specific future time step. We show that there exists a lower bound for the reward at the goal state, such that if the said reward is below that bound, it is impossible to obtain such a guarantee. By extension, we show how to enforce preferences over multiple goals.","sentences":["Recent years have seen an emerging interest in the trustworthiness of machine learning-based agents in the wild, especially in robotics, to provide safety assurance for the industry.","Obtaining behavioral guarantees for these agents remains an important problem.","In this work, we focus on guaranteeing a model-based planning agent reaches a goal state within a specific future time step.","We show that there exists a lower bound for the reward at the goal state, such that if the said reward is below that bound, it is impossible to obtain such a guarantee.","By extension, we show how to enforce preferences over multiple goals."],"url":"http://arxiv.org/abs/2402.13419v1","category":"cs.AI"}
{"created":"2024-02-20 22:45:00","title":"Scaling physics-informed hard constraints with mixture-of-experts","abstract":"Imposing known physical constraints, such as conservation laws, during neural network training introduces an inductive bias that can improve accuracy, reliability, convergence, and data efficiency for modeling physical dynamics. While such constraints can be softly imposed via loss function penalties, recent advancements in differentiable physics and optimization improve performance by incorporating PDE-constrained optimization as individual layers in neural networks. This enables a stricter adherence to physical constraints. However, imposing hard constraints significantly increases computational and memory costs, especially for complex dynamical systems. This is because it requires solving an optimization problem over a large number of points in a mesh, representing spatial and temporal discretizations, which greatly increases the complexity of the constraint. To address this challenge, we develop a scalable approach to enforce hard physical constraints using Mixture-of-Experts (MoE), which can be used with any neural network architecture. Our approach imposes the constraint over smaller decomposed domains, each of which is solved by an \"expert\" through differentiable optimization. During training, each expert independently performs a localized backpropagation step by leveraging the implicit function theorem; the independence of each expert allows for parallelization across multiple GPUs. Compared to standard differentiable optimization, our scalable approach achieves greater accuracy in the neural PDE solver setting for predicting the dynamics of challenging non-linear systems. We also improve training stability and require significantly less computation time during both training and inference stages.","sentences":["Imposing known physical constraints, such as conservation laws, during neural network training introduces an inductive bias that can improve accuracy, reliability, convergence, and data efficiency for modeling physical dynamics.","While such constraints can be softly imposed via loss function penalties, recent advancements in differentiable physics and optimization improve performance by incorporating PDE-constrained optimization as individual layers in neural networks.","This enables a stricter adherence to physical constraints.","However, imposing hard constraints significantly increases computational and memory costs, especially for complex dynamical systems.","This is because it requires solving an optimization problem over a large number of points in a mesh, representing spatial and temporal discretizations, which greatly increases the complexity of the constraint.","To address this challenge, we develop a scalable approach to enforce hard physical constraints using Mixture-of-Experts (MoE), which can be used with any neural network architecture.","Our approach imposes the constraint over smaller decomposed domains, each of which is solved by an \"expert\" through differentiable optimization.","During training, each expert independently performs a localized backpropagation step by leveraging the implicit function theorem; the independence of each expert allows for parallelization across multiple GPUs.","Compared to standard differentiable optimization, our scalable approach achieves greater accuracy in the neural PDE solver setting for predicting the dynamics of challenging non-linear systems.","We also improve training stability and require significantly less computation time during both training and inference stages."],"url":"http://arxiv.org/abs/2402.13412v1","category":"cs.LG"}
{"created":"2024-02-20 21:58:40","title":"Learning and Sustaining Shared Normative Systems via Bayesian Rule Induction in Markov Games","abstract":"A universal feature of human societies is the adoption of systems of rules and norms in the service of cooperative ends. How can we build learning agents that do the same, so that they may flexibly cooperate with the human institutions they are embedded in? We hypothesize that agents can achieve this by assuming there exists a shared set of norms that most others comply with while pursuing their individual desires, even if they do not know the exact content of those norms. By assuming shared norms, a newly introduced agent can infer the norms of an existing population from observations of compliance and violation. Furthermore, groups of agents can converge to a shared set of norms, even if they initially diverge in their beliefs about what the norms are. This in turn enables the stability of the normative system: since agents can bootstrap common knowledge of the norms, this leads the norms to be widely adhered to, enabling new entrants to rapidly learn those norms. We formalize this framework in the context of Markov games and demonstrate its operation in a multi-agent environment via approximately Bayesian rule induction of obligative and prohibitive norms. Using our approach, agents are able to rapidly learn and sustain a variety of cooperative institutions, including resource management norms and compensation for pro-social labor, promoting collective welfare while still allowing agents to act in their own interests.","sentences":["A universal feature of human societies is the adoption of systems of rules and norms in the service of cooperative ends.","How can we build learning agents that do the same, so that they may flexibly cooperate with the human institutions they are embedded in?","We hypothesize that agents can achieve this by assuming there exists a shared set of norms that most others comply with while pursuing their individual desires, even if they do not know the exact content of those norms.","By assuming shared norms, a newly introduced agent can infer the norms of an existing population from observations of compliance and violation.","Furthermore, groups of agents can converge to a shared set of norms, even if they initially diverge in their beliefs about what the norms are.","This in turn enables the stability of the normative system: since agents can bootstrap common knowledge of the norms, this leads the norms to be widely adhered to, enabling new entrants to rapidly learn those norms.","We formalize this framework in the context of Markov games and demonstrate its operation in a multi-agent environment via approximately Bayesian rule induction of obligative and prohibitive norms.","Using our approach, agents are able to rapidly learn and sustain a variety of cooperative institutions, including resource management norms and compensation for pro-social labor, promoting collective welfare while still allowing agents to act in their own interests."],"url":"http://arxiv.org/abs/2402.13399v1","category":"cs.AI"}
{"created":"2024-02-20 21:57:03","title":"Xling: A Learned Filter Framework for Accelerating High-Dimensional Approximate Similarity Join","abstract":"Similarity join finds all pairs of close points within a given distance threshold. Many similarity join methods have been proposed, but they are usually not efficient on high-dimensional space due to the curse of dimensionality and data-unawareness. We investigate the possibility of using metric space Bloom filter (MSBF), a family of data structures checking if a query point has neighbors in a multi-dimensional space, to speed up similarity join. However, there are several challenges when applying MSBF to similarity join, including excessive information loss, data-unawareness and hard constraint on the distance metric. In this paper, we propose Xling, a generic framework to build a learning-based metric space filter with any existing regression model, aiming at accurately predicting whether a query point has enough number of neighbors. The framework provides a suite of optimization strategies to further improve the prediction quality based on the learning model, which has demonstrated significantly higher prediction quality than existing MSBF. We also propose XJoin, one of the first filter-based similarity join methods, based on Xling. By predicting and skipping those queries without enough neighbors, XJoin can effectively reduce unnecessary neighbor searching and therefore it achieves a remarkable acceleration. Benefiting from the generalization capability of deep learning models, XJoin can be easily transferred onto new dataset (in similar distribution) without re-training. Furthermore, Xling is not limited to being applied in XJoin, instead, it acts as a flexible plugin that can be inserted to any loop-based similarity join methods for a speedup.","sentences":["Similarity join finds all pairs of close points within a given distance threshold.","Many similarity join methods have been proposed, but they are usually not efficient on high-dimensional space due to the curse of dimensionality and data-unawareness.","We investigate the possibility of using metric space Bloom filter (MSBF), a family of data structures checking if a query point has neighbors in a multi-dimensional space, to speed up similarity join.","However, there are several challenges when applying MSBF to similarity join, including excessive information loss, data-unawareness and hard constraint on the distance metric.","In this paper, we propose Xling, a generic framework to build a learning-based metric space filter with any existing regression model, aiming at accurately predicting whether a query point has enough number of neighbors.","The framework provides a suite of optimization strategies to further improve the prediction quality based on the learning model, which has demonstrated significantly higher prediction quality than existing MSBF.","We also propose XJoin, one of the first filter-based similarity join methods, based on Xling.","By predicting and skipping those queries without enough neighbors, XJoin can effectively reduce unnecessary neighbor searching and therefore it achieves a remarkable acceleration.","Benefiting from the generalization capability of deep learning models, XJoin can be easily transferred onto new dataset (in similar distribution) without re-training.","Furthermore, Xling is not limited to being applied in XJoin, instead, it acts as a flexible plugin that can be inserted to any loop-based similarity join methods for a speedup."],"url":"http://arxiv.org/abs/2402.13397v1","category":"cs.DB"}
{"created":"2024-02-20 21:30:35","title":"A Tale of Two Peas-In-A-Pod: The Kepler-323 and Kepler-104 Systems","abstract":"In order to understand the relationship between planet multiplicity, mass, and composition, we present newly measured masses of five planets in two planetary systems: Kepler-323 and Kepler-104. We used the HIRES instrument at the W.M. Keck Observatory to collect 79 new radial velocity measurements (RVs) for Kepler-323, which we combined with 48 literature RVs from TNG/HARPS-N. We also conducted a reanalysis of the Kepler-104 system, using 44 previously published RV measurements. Kepler-323 b and c have masses of $2.0^{+1.2}_{-1.1}$ M$_\\oplus$ and 6.5$\\pm1.6$ M$_\\oplus$, respectively, whereas the three Kepler-104 planets are more massive (10.0$\\pm2.8$ M$_\\oplus$, $7.1^{+3.8}_{-3.5}$ M$_\\oplus$, and $5.5^{+4.6}_{-3.5}$ M$_\\oplus$ for planets b, c, and d, respectively). The Kepler-104 planets have densities consistent with rocky cores overlaid with gaseous envelopes ($4.1^{+1.2}_{-1.1}$ g/cc, $2.9^{+1.7}_{-1.5}$ g/cc, and $1.6^{+1.5}_{-1.1}$ g/cc respectively), whereas the Kepler-323 planets are consistent with having rocky compositions ($4.5^{+2.8}_{-2.4}$ g/cc and $9.9^{+2.7}_{-2.5}$ g/cc). The Kepler-104 system has among the lowest values for gap complexity ($\\mathcal{C}$ = 0.004) and mass partitioning ($\\mathcal{Q}$ = 0.03); whereas, the Kepler-323 planets have a mass partitioning similar to that of the Inner Solar System ($\\mathcal{Q}$ = 0.28 and $\\mathcal{Q}$ = 0.24, respectively). For both exoplanet systems, the uncertainty in the mass partitioning is affected equally by (1) individual mass errors of the planets and (2) the possible existence of undetected low-mass planets, meaning that both improved mass characterization and improved sensitivity to low-mass planets in these systems would better elucidate the mass distribution among the planets.","sentences":["In order to understand the relationship between planet multiplicity, mass, and composition, we present newly measured masses of five planets in two planetary systems: Kepler-323 and Kepler-104.","We used the HIRES instrument at the W.M. Keck Observatory to collect 79 new radial velocity measurements (RVs) for Kepler-323, which we combined with 48 literature RVs from TNG/HARPS-N.","We also conducted a reanalysis of the Kepler-104 system, using 44 previously published RV measurements.","Kepler-323 b and c have masses of $2.0^{+1.2}_{-1.1}$ M$_\\oplus$ and 6.5$\\pm1.6$ M$_\\oplus$, respectively, whereas the three Kepler-104 planets are more massive (10.0$\\pm2.8$ M$_\\oplus$, $7.1^{+3.8}_{-3.5}$ M$_\\oplus$, and $5.5^{+4.6}_{-3.5}$ M$_\\oplus$ for planets b, c, and d, respectively).","The Kepler-104 planets have densities consistent with rocky cores overlaid with gaseous envelopes ($4.1^{+1.2}_{-1.1}$ g/cc, $2.9^{+1.7}_{-1.5}$ g/cc, and $1.6^{+1.5}_{-1.1}$ g/cc respectively), whereas the Kepler-323 planets are consistent with having rocky compositions ($4.5^{+2.8}_{-2.4}$ g/cc and $9.9^{+2.7}_{-2.5}$ g/cc).","The Kepler-104 system has among the lowest values for gap complexity ($\\mathcal{C}$ = 0.004) and mass partitioning ($\\mathcal{Q}$ = 0.03); whereas, the Kepler-323 planets have a mass partitioning similar to that of the Inner Solar System ($\\mathcal{Q}$ = 0.28 and $\\mathcal{Q}$ = 0.24, respectively).","For both exoplanet systems, the uncertainty in the mass partitioning is affected equally by (1) individual mass errors of the planets and (2) the possible existence of undetected low-mass planets, meaning that both improved mass characterization and improved sensitivity to low-mass planets in these systems would better elucidate the mass distribution among the planets."],"url":"http://arxiv.org/abs/2402.13386v1","category":"astro-ph.EP"}
{"created":"2024-02-20 21:13:38","title":"Toward TransfORmers: Revolutionizing the Solution of Mixed Integer Programs with Transformers","abstract":"In this study, we introduce an innovative deep learning framework that employs a transformer model to address the challenges of mixed-integer programs, specifically focusing on the Capacitated Lot Sizing Problem (CLSP). Our approach, to our knowledge, is the first to utilize transformers to predict the binary variables of a mixed-integer programming (MIP) problem. Specifically, our approach harnesses the encoder decoder transformer's ability to process sequential data, making it well-suited for predicting binary variables indicating production setup decisions in each period of the CLSP. This problem is inherently dynamic, and we need to handle sequential decision making under constraints. We present an efficient algorithm in which CLSP solutions are learned through a transformer neural network. The proposed post-processed transformer algorithm surpasses the state-of-the-art solver, CPLEX and Long Short-Term Memory (LSTM) in solution time, optimal gap, and percent infeasibility over 240K benchmark CLSP instances tested. After the ML model is trained, conducting inference on the model, including post-processing, reduces the MIP into a linear program (LP). This transforms the ML-based algorithm, combined with an LP solver, into a polynomial-time approximation algorithm to solve a well-known NP-Hard problem, with almost perfect solution quality.","sentences":["In this study, we introduce an innovative deep learning framework that employs a transformer model to address the challenges of mixed-integer programs, specifically focusing on the Capacitated Lot Sizing Problem (CLSP).","Our approach, to our knowledge, is the first to utilize transformers to predict the binary variables of a mixed-integer programming (MIP) problem.","Specifically, our approach harnesses the encoder decoder transformer's ability to process sequential data, making it well-suited for predicting binary variables indicating production setup decisions in each period of the CLSP.","This problem is inherently dynamic, and we need to handle sequential decision making under constraints.","We present an efficient algorithm in which CLSP solutions are learned through a transformer neural network.","The proposed post-processed transformer algorithm surpasses the state-of-the-art solver, CPLEX and Long Short-Term Memory (LSTM) in solution time, optimal gap, and percent infeasibility over 240K benchmark CLSP instances tested.","After the ML model is trained, conducting inference on the model, including post-processing, reduces the MIP into a linear program (LP).","This transforms the ML-based algorithm, combined with an LP solver, into a polynomial-time approximation algorithm to solve a well-known NP-Hard problem, with almost perfect solution quality."],"url":"http://arxiv.org/abs/2402.13380v1","category":"cs.AI"}
{"created":"2024-02-20 20:49:22","title":"The Uncanny Valley: A Comprehensive Analysis of Diffusion Models","abstract":"Through Diffusion Models (DMs), we have made significant advances in generating high-quality images. Our exploration of these models delves deeply into their core operational principles by systematically investigating key aspects across various DM architectures: i) noise schedules, ii) samplers, and iii) guidance. Our comprehensive examination of these models sheds light on their hidden fundamental mechanisms, revealing the concealed foundational elements that are essential for their effectiveness. Our analyses emphasize the hidden key factors that determine model performance, offering insights that contribute to the advancement of DMs. Past findings show that the configuration of noise schedules, samplers, and guidance is vital to the quality of generated images; however, models reach a stable level of quality across different configurations at a remarkably similar point, revealing that the decisive factors for optimal performance predominantly reside in the diffusion process dynamics and the structural design of the model's network, rather than the specifics of configuration details. Our comparative analysis reveals that Denoising Diffusion Probabilistic Model (DDPM)-based diffusion dynamics consistently outperform the Noise Conditioned Score Network (NCSN)-based ones, not only when evaluated in their original forms but also when continuous through Stochastic Differential Equation (SDE)-based implementations.","sentences":["Through Diffusion Models (DMs), we have made significant advances in generating high-quality images.","Our exploration of these models delves deeply into their core operational principles by systematically investigating key aspects across various DM architectures: i) noise schedules, ii) samplers, and iii) guidance.","Our comprehensive examination of these models sheds light on their hidden fundamental mechanisms, revealing the concealed foundational elements that are essential for their effectiveness.","Our analyses emphasize the hidden key factors that determine model performance, offering insights that contribute to the advancement of DMs.","Past findings show that the configuration of noise schedules, samplers, and guidance is vital to the quality of generated images; however, models reach a stable level of quality across different configurations at a remarkably similar point, revealing that the decisive factors for optimal performance predominantly reside in the diffusion process dynamics and the structural design of the model's network, rather than the specifics of configuration details.","Our comparative analysis reveals that Denoising Diffusion Probabilistic Model (DDPM)-based diffusion dynamics consistently outperform the Noise Conditioned Score Network (NCSN)-based ones, not only when evaluated in their original forms but also when continuous through Stochastic Differential Equation (SDE)-based implementations."],"url":"http://arxiv.org/abs/2402.13369v1","category":"cs.LG"}
{"created":"2024-02-20 20:44:40","title":"Statistical curriculum learning: An elimination algorithm achieving an oracle risk","abstract":"We consider a statistical version of curriculum learning (CL) in a parametric prediction setting. The learner is required to estimate a target parameter vector, and can adaptively collect samples from either the target model, or other source models that are similar to the target model, but less noisy. We consider three types of learners, depending on the level of side-information they receive. The first two, referred to as strong/weak-oracle learners, receive high/low degrees of information about the models, and use these to learn. The third, a fully adaptive learner, estimates the target parameter vector without any prior information. In the single source case, we propose an elimination learning method, whose risk matches that of a strong-oracle learner. In the multiple source case, we advocate that the risk of the weak-oracle learner is a realistic benchmark for the risk of adaptive learners. We develop an adaptive multiple elimination-rounds CL algorithm, and characterize instance-dependent conditions for its risk to match that of the weak-oracle learner. We consider instance-dependent minimax lower bounds, and discuss the challenges associated with defining the class of instances for the bound. We derive two minimax lower bounds, and determine the conditions under which the performance weak-oracle learner is minimax optimal.","sentences":["We consider a statistical version of curriculum learning (CL) in a parametric prediction setting.","The learner is required to estimate a target parameter vector, and can adaptively collect samples from either the target model, or other source models that are similar to the target model, but less noisy.","We consider three types of learners, depending on the level of side-information they receive.","The first two, referred to as strong/weak-oracle learners, receive high/low degrees of information about the models, and use these to learn.","The third, a fully adaptive learner, estimates the target parameter vector without any prior information.","In the single source case, we propose an elimination learning method, whose risk matches that of a strong-oracle learner.","In the multiple source case, we advocate that the risk of the weak-oracle learner is a realistic benchmark for the risk of adaptive learners.","We develop an adaptive multiple elimination-rounds CL algorithm, and characterize instance-dependent conditions for its risk to match that of the weak-oracle learner.","We consider instance-dependent minimax lower bounds, and discuss the challenges associated with defining the class of instances for the bound.","We derive two minimax lower bounds, and determine the conditions under which the performance weak-oracle learner is minimax optimal."],"url":"http://arxiv.org/abs/2402.13366v1","category":"cs.LG"}
{"created":"2024-02-20 20:02:21","title":"KetGPT -- Dataset Augmentation of Quantum Circuits using Transformers","abstract":"Quantum algorithms, represented as quantum circuits, can be used as benchmarks for assessing the performance of quantum systems. Existing datasets, widely utilized in the field, suffer from limitations in size and versatility, leading researchers to employ randomly generated circuits. Random circuits are, however, not representative benchmarks as they lack the inherent properties of real quantum algorithms for which the quantum systems are manufactured. This shortage of `useful' quantum benchmarks poses a challenge to advancing the development and comparison of quantum compilers and hardware.   This research aims to enhance the existing quantum circuit datasets by generating what we refer to as `realistic-looking' circuits by employing the Transformer machine learning architecture. For this purpose, we introduce KetGPT, a tool that generates synthetic circuits in OpenQASM language, whose structure is based on quantum circuits derived from existing quantum algorithms and follows the typical patterns of human-written algorithm-based code (e.g., order of gates and qubits). Our three-fold verification process, involving manual inspection and Qiskit framework execution, transformer-based classification, and structural analysis, demonstrates the efficacy of KetGPT in producing large amounts of additional circuits that closely align with algorithm-based structures. Beyond benchmarking, we envision KetGPT contributing substantially to AI-driven quantum compilers and systems.","sentences":["Quantum algorithms, represented as quantum circuits, can be used as benchmarks for assessing the performance of quantum systems.","Existing datasets, widely utilized in the field, suffer from limitations in size and versatility, leading researchers to employ randomly generated circuits.","Random circuits are, however, not representative benchmarks as they lack the inherent properties of real quantum algorithms for which the quantum systems are manufactured.","This shortage of `useful' quantum benchmarks poses a challenge to advancing the development and comparison of quantum compilers and hardware.   ","This research aims to enhance the existing quantum circuit datasets by generating what we refer to as `realistic-looking' circuits by employing the Transformer machine learning architecture.","For this purpose, we introduce KetGPT, a tool that generates synthetic circuits in OpenQASM language, whose structure is based on quantum circuits derived from existing quantum algorithms and follows the typical patterns of human-written algorithm-based code (e.g., order of gates and qubits).","Our three-fold verification process, involving manual inspection and Qiskit framework execution, transformer-based classification, and structural analysis, demonstrates the efficacy of KetGPT in producing large amounts of additional circuits that closely align with algorithm-based structures.","Beyond benchmarking, we envision KetGPT contributing substantially to AI-driven quantum compilers and systems."],"url":"http://arxiv.org/abs/2402.13352v1","category":"quant-ph"}
{"created":"2024-02-20 20:00:43","title":"Malicious Reconfigurable Intelligent Surfaces: How Impactful can Destructive Beamforming be?","abstract":"Reconfigurable intelligent surfaces (RISs) have demonstrated significant potential for enhancing communication system performance if properly configured. However, a RIS might also pose a risk to the network security. In this letter, we explore the impact of a malicious RIS on a multi-user multiple-input single-output (MISO) system when the system is unaware of the RIS's malicious intentions. The objective of the malicious RIS is to degrade the \\ac{SNR} of a specific \\ac{UE}, with the option of preserving the SNR of the other UEs, making the attack harder to detect. To achieve this goal, we derive the optimal RIS phase-shift pattern, assuming perfect channel state information (CSI) at the hacker. We then relax this assumption by introducing CSI uncertainties and subsequently determine the RIS's phase-shift pattern using a robust optimization approach. Our simulations reveal a direct proportionality between the performance degradation caused by the malicious RIS and the number of reflective elements, along with resilience toward CSI uncertainties.","sentences":["Reconfigurable intelligent surfaces (RISs) have demonstrated significant potential for enhancing communication system performance if properly configured.","However, a RIS might also pose a risk to the network security.","In this letter, we explore the impact of a malicious RIS on a multi-user multiple-input single-output (MISO) system when the system is unaware of the RIS's malicious intentions.","The objective of the malicious RIS is to degrade the \\ac{SNR} of a specific \\ac{UE}, with the option of preserving the SNR of the other UEs, making the attack harder to detect.","To achieve this goal, we derive the optimal RIS phase-shift pattern, assuming perfect channel state information (CSI) at the hacker.","We then relax this assumption by introducing CSI uncertainties and subsequently determine the RIS's phase-shift pattern using a robust optimization approach.","Our simulations reveal a direct proportionality between the performance degradation caused by the malicious RIS and the number of reflective elements, along with resilience toward CSI uncertainties."],"url":"http://arxiv.org/abs/2402.13351v1","category":"eess.SP"}
{"created":"2024-02-20 19:53:15","title":"Aria Everyday Activities Dataset","abstract":"We present Aria Everyday Activities (AEA) Dataset, an egocentric multimodal open dataset recorded using Project Aria glasses. AEA contains 143 daily activity sequences recorded by multiple wearers in five geographically diverse indoor locations. Each of the recording contains multimodal sensor data recorded through the Project Aria glasses. In addition, AEA provides machine perception data including high frequency globally aligned 3D trajectories, scene point cloud, per-frame 3D eye gaze vector and time aligned speech transcription. In this paper, we demonstrate a few exemplar research applications enabled by this dataset, including neural scene reconstruction and prompted segmentation. AEA is an open source dataset that can be downloaded from projectaria.com. We are also providing open-source implementations and examples of how to use the dataset in Project Aria Tools.","sentences":["We present Aria Everyday Activities (AEA) Dataset, an egocentric multimodal open dataset recorded using Project Aria glasses.","AEA contains 143 daily activity sequences recorded by multiple wearers in five geographically diverse indoor locations.","Each of the recording contains multimodal sensor data recorded through the Project Aria glasses.","In addition, AEA provides machine perception data including high frequency globally aligned 3D trajectories, scene point cloud, per-frame 3D eye gaze vector and time aligned speech transcription.","In this paper, we demonstrate a few exemplar research applications enabled by this dataset, including neural scene reconstruction and prompted segmentation.","AEA is an open source dataset that can be downloaded from projectaria.com.","We are also providing open-source implementations and examples of how to use the dataset in Project Aria Tools."],"url":"http://arxiv.org/abs/2402.13349v1","category":"cs.CV"}
{"created":"2024-02-20 19:27:15","title":"Modular invariance and thermal effective field theory in CFT","abstract":"We use thermal effective field theory to derive that the coefficient of the first subleading piece of the thermal free energy, $c_1$, is equal to the coefficient of the subleading piece of the Casimir energy on $S^1 \\times S^{d-2}$ for $d \\geq 4$. We conjecture that this coefficient obeys a sign constraint $c_1 \\geq 0$ in CFT and collect some evidence for this bound. We discuss various applications of the thermal effective field theory, including placing the CFT on different spatial backgrounds and turning on chemical potentials for $U(1)$ charge and angular momentum. Along the way, we derive the high-temperature partition function on a sphere with arbitrary angular velocities using only time dilation and length contraction.","sentences":["We use thermal effective field theory to derive that the coefficient of the first subleading piece of the thermal free energy, $c_1$, is equal to the coefficient of the subleading piece of the Casimir energy on $S^1 \\times S^{d-2}$ for $d \\geq","4$.","We conjecture that this coefficient obeys a sign constraint $c_1 \\geq 0$ in CFT and collect some evidence for this bound.","We discuss various applications of the thermal effective field theory, including placing the CFT on different spatial backgrounds and turning on chemical potentials for $U(1)$ charge and angular momentum.","Along the way, we derive the high-temperature partition function on a sphere with arbitrary angular velocities using only time dilation and length contraction."],"url":"http://arxiv.org/abs/2402.13337v1","category":"hep-th"}
{"created":"2024-02-20 19:24:35","title":"Abstract Hardy inequalities: The case p=1","abstract":"Boundedness of an abstract formulation of Hardy operators between Lebesgue spaces over general measure spaces is studied and, when the domain is L^1, shown to be equivalent to the existence of a Hardy inequality on the half line with general Borel measures. This is done by extending the greatest decreasing minorant construction to general measure spaces depending on a totally ordered collection of measurable sets, called an ordered core. A functional description of the greatest decreasing minorant is given, and for a large class of ordered cores, a pointwise description is provided. As an application, characterizations of Hardy inequalities for metric measure spaces are given, we note that the metric measure space is not required to admit a polar decomposition.","sentences":["Boundedness of an abstract formulation of Hardy operators between Lebesgue spaces over general measure spaces is studied and, when the domain is L^1, shown to be equivalent to the existence of a Hardy inequality on the half line with general Borel measures.","This is done by extending the greatest decreasing minorant construction to general measure spaces depending on a totally ordered collection of measurable sets, called an ordered core.","A functional description of the greatest decreasing minorant is given, and for a large class of ordered cores, a pointwise description is provided.","As an application, characterizations of Hardy inequalities for metric measure spaces are given, we note that the metric measure space is not required to admit a polar decomposition."],"url":"http://arxiv.org/abs/2402.13335v1","category":"math.FA"}
{"created":"2024-02-20 19:08:24","title":"Deep Hedging with Market Impact","abstract":"Dynamic hedging is the practice of periodically transacting financial instruments to offset the risk caused by an investment or a liability. Dynamic hedging optimization can be framed as a sequential decision problem; thus, Reinforcement Learning (RL) models were recently proposed to tackle this task. However, existing RL works for hedging do not consider market impact caused by the finite liquidity of traded instruments. Integrating such feature can be crucial to achieve optimal performance when hedging options on stocks with limited liquidity. In this paper, we propose a novel general market impact dynamic hedging model based on Deep Reinforcement Learning (DRL) that considers several realistic features such as convex market impacts, and impact persistence through time. The optimal policy obtained from the DRL model is analysed using several option hedging simulations and compared to commonly used procedures such as delta hedging. Results show our DRL model behaves better in contexts of low liquidity by, among others: 1) learning the extent to which portfolio rebalancing actions should be dampened or delayed to avoid high costs, 2) factoring in the impact of features not considered by conventional approaches, such as previous hedging errors through the portfolio value, and the underlying asset's drift (i.e. the magnitude of its expected return).","sentences":["Dynamic hedging is the practice of periodically transacting financial instruments to offset the risk caused by an investment or a liability.","Dynamic hedging optimization can be framed as a sequential decision problem; thus, Reinforcement Learning (RL) models were recently proposed to tackle this task.","However, existing RL works for hedging do not consider market impact caused by the finite liquidity of traded instruments.","Integrating such feature can be crucial to achieve optimal performance when hedging options on stocks with limited liquidity.","In this paper, we propose a novel general market impact dynamic hedging model based on Deep Reinforcement Learning (DRL) that considers several realistic features such as convex market impacts, and impact persistence through time.","The optimal policy obtained from the DRL model is analysed using several option hedging simulations and compared to commonly used procedures such as delta hedging.","Results show our DRL model behaves better in contexts of low liquidity by, among others: 1) learning the extent to which portfolio rebalancing actions should be dampened or delayed to avoid high costs, 2) factoring in the impact of features not considered by conventional approaches, such as previous hedging errors through the portfolio value, and the underlying asset's drift (i.e. the magnitude of its expected return)."],"url":"http://arxiv.org/abs/2402.13326v1","category":"q-fin.CP"}
{"created":"2024-02-20 19:02:08","title":"Kinetic Theory of Stellar Systems: A Tutorial","abstract":"Stellar systems - star clusters, galaxies, dark matter haloes, and so on - are ubiquitous characters in the evolutionary tale of our Universe. This tutorial article is an introduction to the collective dynamical evolution of the very large numbers of stars and/or other self-gravitating objects that comprise such systems, i.e. their kinetic theory.   We begin by introducing the basic phenomenology of stellar systems, and explaining why and when we must develop a kinetic theory that transcends the traditional two-body relaxation picture of Chandrasekhar. We study the orbits that comprise stellar systems, how those orbits are modified by perturbations, how a system responds self-consistently to fluctuations in its gravitational potential, and how one can predict the long term fate of a stellar system in various dynamical regimes. Though our treatment is necessarily mathematical, we develop the formalism only to the extent that it facilitates real calculations. We give many examples throughout the text of the equations being applied to topics of major astrophysical importance.   Furthermore, in the 1960s and 1970s the kinetic theory of stellar systems was a fledgling subject which developed in tandem with the kinetic theory of plasmas. However, the two fields have long since diverged. Yet once one has become fluent in both Plasmaish and Galacticese, and has a dictionary relating the two, one can pull ideas directly from one field to solve a problem in the other. Therefore, another aim of this tutorial article is to provide our plasma colleagues with a jargon-light understanding of the key properties of stellar systems, to point out the many direct analogies between stellar- and plasma-kinetic calculations, and ultimately to convince them that stellar dynamics and plasma kinetics are, in a deep and beautiful and useful sense, the same thing.","sentences":["Stellar systems - star clusters, galaxies, dark matter haloes, and so on - are ubiquitous characters in the evolutionary tale of our Universe.","This tutorial article is an introduction to the collective dynamical evolution of the very large numbers of stars and/or other self-gravitating objects that comprise such systems, i.e. their kinetic theory.   ","We begin by introducing the basic phenomenology of stellar systems, and explaining why and when we must develop a kinetic theory that transcends the traditional two-body relaxation picture of Chandrasekhar.","We study the orbits that comprise stellar systems, how those orbits are modified by perturbations, how a system responds self-consistently to fluctuations in its gravitational potential, and how one can predict the long term fate of a stellar system in various dynamical regimes.","Though our treatment is necessarily mathematical, we develop the formalism only to the extent that it facilitates real calculations.","We give many examples throughout the text of the equations being applied to topics of major astrophysical importance.   ","Furthermore, in the 1960s and 1970s the kinetic theory of stellar systems was a fledgling subject which developed in tandem with the kinetic theory of plasmas.","However, the two fields have long since diverged.","Yet once one has become fluent in both Plasmaish and Galacticese, and has a dictionary relating the two, one can pull ideas directly from one field to solve a problem in the other.","Therefore, another aim of this tutorial article is to provide our plasma colleagues with a jargon-light understanding of the key properties of stellar systems, to point out the many direct analogies between stellar- and plasma-kinetic calculations, and ultimately to convince them that stellar dynamics and plasma kinetics are, in a deep and beautiful and useful sense, the same thing."],"url":"http://arxiv.org/abs/2402.13322v1","category":"astro-ph.GA"}
{"created":"2024-02-20 19:00:08","title":"Neutrino Rate Predictions for FASER","abstract":"The Forward Search Experiment (FASER) at CERN's Large Hadron Collider (LHC) has recently directly detected the first collider neutrinos. Neutrinos play an important role in all FASER analyses, either as signal or background, and it is therefore essential to understand the neutrino event rates. In this study, we update previous simulations and present prescriptions for theoretical predictions of neutrino fluxes and cross sections, together with their associated uncertainties. With these results, we discuss the potential for possible measurements that could be carried out in the coming years with the FASER neutrino data to be collected in LHC Run 3 and Run 4.","sentences":["The Forward Search Experiment (FASER) at CERN's Large Hadron Collider (LHC) has recently directly detected the first collider neutrinos.","Neutrinos play an important role in all FASER analyses, either as signal or background, and it is therefore essential to understand the neutrino event rates.","In this study, we update previous simulations and present prescriptions for theoretical predictions of neutrino fluxes and cross sections, together with their associated uncertainties.","With these results, we discuss the potential for possible measurements that could be carried out in the coming years with the FASER neutrino data to be collected in LHC Run 3 and Run 4."],"url":"http://arxiv.org/abs/2402.13318v1","category":"hep-ex"}
{"created":"2024-02-20 18:58:23","title":"Vision System Prototype for Inspection and Monitoring with a Smart Camera","abstract":"This paper presents the design of an artificial vision system prototype for automatic inspection and monitoring of objects over a conveyor belt and using a Smart camera 2D BOA-INS. The prototype consists of a conveyor belt and an embedded system based on an Arduino Mega card for system control, and it has as main peripherals the smart camera, a direct current motor, a photoelectric sensor, LED illumination and LEDs indicating the status (good or defect) of each evaluated object. The application of the prototype is for educational purposes, so that undergraduate, master and diploma students can simulate a continuous production line, controlled by an embedded system, and perform quality control by monitoring through a visual system and a personal computer. This allows implementing the topics of embedded systems, artificial vision, artificial intelligence, pattern recognition, automatic control, as well as automation of real processes.","sentences":["This paper presents the design of an artificial vision system prototype for automatic inspection and monitoring of objects over a conveyor belt and using a Smart camera 2D BOA-INS.","The prototype consists of a conveyor belt and an embedded system based on an Arduino Mega card for system control, and it has as main peripherals the smart camera, a direct current motor, a photoelectric sensor, LED illumination and LEDs indicating the status (good or defect) of each evaluated object.","The application of the prototype is for educational purposes, so that undergraduate, master and diploma students can simulate a continuous production line, controlled by an embedded system, and perform quality control by monitoring through a visual system and a personal computer.","This allows implementing the topics of embedded systems, artificial vision, artificial intelligence, pattern recognition, automatic control, as well as automation of real processes."],"url":"http://arxiv.org/abs/2402.13306v1","category":"cs.CV"}
{"created":"2024-02-20 15:01:11","title":"Harmful algal bloom forecasting. A comparison between stream and batch learning","abstract":"Diarrhetic Shellfish Poisoning (DSP) is a global health threat arising from shellfish contaminated with toxins produced by dinoflagellates. The condition, with its widespread incidence, high morbidity rate, and persistent shellfish toxicity, poses risks to public health and the shellfish industry. High biomass of toxin-producing algae such as DSP are known as Harmful Algal Blooms (HABs). Monitoring and forecasting systems are crucial for mitigating HABs impact. Predicting harmful algal blooms involves a time-series-based problem with a strong historical seasonal component, however, recent anomalies due to changes in meteorological and oceanographic events have been observed. Stream Learning stands out as one of the most promising approaches for addressing time-series-based problems with concept drifts. However, its efficacy in predicting HABs remains unproven and needs to be tested in comparison with Batch Learning. Historical data availability is a critical point in developing predictive systems. In oceanography, the available data collection can have some constrains and limitations, which has led to exploring new tools to obtain more exhaustive time series. In this study, a machine learning workflow for predicting the number of cells of a toxic dinoflagellate, Dinophysis acuminata, was developed with several key advancements. Seven machine learning algorithms were compared within two learning paradigms. Notably, the output data from CROCO, the ocean hydrodynamic model, was employed as the primary dataset, palliating the limitation of time-continuous historical data. This study highlights the value of models interpretability, fair models comparison methodology, and the incorporation of Stream Learning models. The model DoME, with an average R2 of 0.77 in the 3-day-ahead prediction, emerged as the most effective and interpretable predictor, outperforming the other algorithms.","sentences":["Diarrhetic Shellfish Poisoning (DSP) is a global health threat arising from shellfish contaminated with toxins produced by dinoflagellates.","The condition, with its widespread incidence, high morbidity rate, and persistent shellfish toxicity, poses risks to public health and the shellfish industry.","High biomass of toxin-producing algae such as DSP are known as Harmful Algal Blooms (HABs).","Monitoring and forecasting systems are crucial for mitigating HABs impact.","Predicting harmful algal blooms involves a time-series-based problem with a strong historical seasonal component, however, recent anomalies due to changes in meteorological and oceanographic events have been observed.","Stream Learning stands out as one of the most promising approaches for addressing time-series-based problems with concept drifts.","However, its efficacy in predicting HABs remains unproven and needs to be tested in comparison with Batch Learning.","Historical data availability is a critical point in developing predictive systems.","In oceanography, the available data collection can have some constrains and limitations, which has led to exploring new tools to obtain more exhaustive time series.","In this study, a machine learning workflow for predicting the number of cells of a toxic dinoflagellate, Dinophysis acuminata, was developed with several key advancements.","Seven machine learning algorithms were compared within two learning paradigms.","Notably, the output data from CROCO, the ocean hydrodynamic model, was employed as the primary dataset, palliating the limitation of time-continuous historical data.","This study highlights the value of models interpretability, fair models comparison methodology, and the incorporation of Stream Learning models.","The model DoME, with an average R2 of 0.77 in the 3-day-ahead prediction, emerged as the most effective and interpretable predictor, outperforming the other algorithms."],"url":"http://arxiv.org/abs/2402.13304v1","category":"cs.LG"}
{"created":"2024-02-20 13:41:35","title":"Structure-informed Positional Encoding for Music Generation","abstract":"Music generated by deep learning methods often suffers from a lack of coherence and long-term organization. Yet, multi-scale hierarchical structure is a distinctive feature of music signals. To leverage this information, we propose a structure-informed positional encoding framework for music generation with Transformers. We design three variants in terms of absolute, relative and non-stationary positional information. We comprehensively test them on two symbolic music generation tasks: next-timestep prediction and accompaniment generation. As a comparison, we choose multiple baselines from the literature and demonstrate the merits of our methods using several musically-motivated evaluation metrics. In particular, our methods improve the melodic and structural consistency of the generated pieces.","sentences":["Music generated by deep learning methods often suffers from a lack of coherence and long-term organization.","Yet, multi-scale hierarchical structure is a distinctive feature of music signals.","To leverage this information, we propose a structure-informed positional encoding framework for music generation with Transformers.","We design three variants in terms of absolute, relative and non-stationary positional information.","We comprehensively test them on two symbolic music generation tasks: next-timestep prediction and accompaniment generation.","As a comparison, we choose multiple baselines from the literature and demonstrate the merits of our methods using several musically-motivated evaluation metrics.","In particular, our methods improve the melodic and structural consistency of the generated pieces."],"url":"http://arxiv.org/abs/2402.13301v1","category":"cs.SD"}
{"created":"2024-02-21 18:56:28","title":"Performance Evaluation and Analysis of Thresholding-based Interference Mitigation for Automotive Radar Systems","abstract":"In automotive radar, time-domain thresholding (TD-TH) and time-frequency domain thresholding (TFD-TH) are crucial techniques underpinning numerous interference mitigation methods. Despite their importance, comprehensive evaluations of these methods in dense traffic scenarios with different types of interference are limited. In this study, we segment automotive radar interference into three distinct categories. Utilizing the in-house traffic scenario and automotive radar simulator, we evaluate interference mitigation methods across multiple metrics: probability of detection, signal-to-interference-plus-noise ratio, and phase error involving hundreds of targets and dozens of interfering radars. The numerical results highlight that TFD-TH is more effective than TD-TH, particularly as the density and signal correlation of interfering radars escalate.","sentences":["In automotive radar, time-domain thresholding (TD-TH) and time-frequency domain thresholding (TFD-TH) are crucial techniques underpinning numerous interference mitigation methods.","Despite their importance, comprehensive evaluations of these methods in dense traffic scenarios with different types of interference are limited.","In this study, we segment automotive radar interference into three distinct categories.","Utilizing the in-house traffic scenario and automotive radar simulator, we evaluate interference mitigation methods across multiple metrics: probability of detection, signal-to-interference-plus-noise ratio, and phase error involving hundreds of targets and dozens of interfering radars.","The numerical results highlight that TFD-TH is more effective than TD-TH, particularly as the density and signal correlation of interfering radars escalate."],"url":"http://arxiv.org/abs/2402.14018v1","category":"eess.SP"}
{"created":"2024-02-21 18:55:20","title":"Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment","abstract":"Large Language Models (LLMs) are powerful zero-shot assessors and are increasingly used in real-world situations such as for written exams or benchmarking systems. Despite this, no existing work has analyzed the vulnerability of judge-LLMs against adversaries attempting to manipulate outputs. This work presents the first study on the adversarial robustness of assessment LLMs, where we search for short universal phrases that when appended to texts can deceive LLMs to provide high assessment scores. Experiments on SummEval and TopicalChat demonstrate that both LLM-scoring and pairwise LLM-comparative assessment are vulnerable to simple concatenation attacks, where in particular LLM-scoring is very susceptible and can yield maximum assessment scores irrespective of the input text quality. Interestingly, such attacks are transferable and phrases learned on smaller open-source LLMs can be applied to larger closed-source models, such as GPT3.5. This highlights the pervasive nature of the adversarial vulnerabilities across different judge-LLM sizes, families and methods. Our findings raise significant concerns on the reliability of LLMs-as-a-judge methods, and underscore the importance of addressing vulnerabilities in LLM assessment methods before deployment in high-stakes real-world scenarios.","sentences":["Large Language Models (LLMs) are powerful zero-shot assessors and are increasingly used in real-world situations such as for written exams or benchmarking systems.","Despite this, no existing work has analyzed the vulnerability of judge-LLMs against adversaries attempting to manipulate outputs.","This work presents the first study on the adversarial robustness of assessment LLMs, where we search for short universal phrases that when appended to texts can deceive LLMs to provide high assessment scores.","Experiments on SummEval and TopicalChat demonstrate that both LLM-scoring and pairwise LLM-comparative assessment are vulnerable to simple concatenation attacks, where in particular LLM-scoring is very susceptible and can yield maximum assessment scores irrespective of the input text quality.","Interestingly, such attacks are transferable and phrases learned on smaller open-source LLMs can be applied to larger closed-source models, such as GPT3.5.","This highlights the pervasive nature of the adversarial vulnerabilities across different judge-LLM sizes, families and methods.","Our findings raise significant concerns on the reliability of LLMs-as-a-judge methods, and underscore the importance of addressing vulnerabilities in LLM assessment methods before deployment in high-stakes real-world scenarios."],"url":"http://arxiv.org/abs/2402.14016v1","category":"cs.CL"}
{"created":"2024-02-21 18:52:20","title":"Misalignment, Learning, and Ranking: Harnessing Users Limited Attention","abstract":"In digital health and EdTech, recommendation systems face a significant challenge: users often choose impulsively, in ways that conflict with the platform's long-term payoffs. This misalignment makes it difficult to effectively learn to rank items, as it may hinder exploration of items with greater long-term payoffs. Our paper tackles this issue by utilizing users' limited attention spans. We propose a model where a platform presents items with unknown payoffs to the platform in a ranked list to $T$ users over time. Each user selects an item by first considering a prefix window of these ranked items and then picking the highest preferred item in that window (and the platform observes its payoff for this item). We study the design of online bandit algorithms that obtain vanishing regret against hindsight optimal benchmarks.   We first consider adversarial window sizes and stochastic iid payoffs. We design an active-elimination-based algorithm that achieves an optimal instance-dependent regret bound of $O(\\log(T))$, by showing matching regret upper and lower bounds. The key idea is using the combinatorial structure of the problem to either obtain a large payoff from each item or to explore by getting a sample from that item. This method systematically narrows down the item choices to enhance learning efficiency and payoff.   Second, we consider adversarial payoffs and stochastic iid window sizes. We start from the full-information problem of finding the permutation that maximizes the expected payoff. By a novel combinatorial argument, we characterize the polytope of admissible item selection probabilities by a permutation and show it has a polynomial-size representation. Using this representation, we show how standard algorithms for adversarial online linear optimization in the space of admissible probabilities can be used to obtain a polynomial-time algorithm with $O(\\sqrt{T})$ regret.","sentences":["In digital health and EdTech, recommendation systems face a significant challenge: users often choose impulsively, in ways that conflict with the platform's long-term payoffs.","This misalignment makes it difficult to effectively learn to rank items, as it may hinder exploration of items with greater long-term payoffs.","Our paper tackles this issue by utilizing users' limited attention spans.","We propose a model where a platform presents items with unknown payoffs to the platform in a ranked list to $T$ users over time.","Each user selects an item by first considering a prefix window of these ranked items and then picking the highest preferred item in that window (and the platform observes its payoff for this item).","We study the design of online bandit algorithms that obtain vanishing regret against hindsight optimal benchmarks.   ","We first consider adversarial window sizes and stochastic iid payoffs.","We design an active-elimination-based algorithm that achieves an optimal instance-dependent regret bound of $O(\\log(T))$, by showing matching regret upper and lower bounds.","The key idea is using the combinatorial structure of the problem to either obtain a large payoff from each item or to explore by getting a sample from that item.","This method systematically narrows down the item choices to enhance learning efficiency and payoff.   ","Second, we consider adversarial payoffs and stochastic iid window sizes.","We start from the full-information problem of finding the permutation that maximizes the expected payoff.","By a novel combinatorial argument, we characterize the polytope of admissible item selection probabilities by a permutation and show it has a polynomial-size representation.","Using this representation, we show how standard algorithms for adversarial online linear optimization in the space of admissible probabilities can be used to obtain a polynomial-time algorithm with $O(\\sqrt{T})$ regret."],"url":"http://arxiv.org/abs/2402.14013v1","category":"cs.LG"}
{"created":"2024-02-21 18:51:42","title":"Chasing Convex Functions with Long-term Constraints","abstract":"We introduce and study a family of online metric problems with long-term constraints. In these problems, an online player makes decisions $\\mathbf{x}_t$ in a metric space $(X,d)$ to simultaneously minimize their hitting cost $f_t(\\mathbf{x}_t)$ and switching cost as determined by the metric. Over the time horizon $T$, the player must satisfy a long-term demand constraint $\\sum_{t} c(\\mathbf{x}_t) \\geq 1$, where $c(\\mathbf{x}_t)$ denotes the fraction of demand satisfied at time $t$. Such problems can find a wide array of applications to online resource allocation in sustainable energy and computing systems. We devise optimal competitive and learning-augmented algorithms for specific instantiations of these problems, and further show that our proposed algorithms perform well in numerical experiments.","sentences":["We introduce and study a family of online metric problems with long-term constraints.","In these problems, an online player makes decisions $\\mathbf{x}_t$ in a metric space $(X,d)$ to simultaneously minimize their hitting cost $f_t(\\mathbf{x}_t)$ and switching cost as determined by the metric.","Over the time horizon $T$, the player must satisfy a long-term demand constraint $\\sum_{t} c(\\mathbf{x}_t) \\geq 1$, where $c(\\mathbf{x}_t)$ denotes the fraction of demand satisfied at time $t$. Such problems can find a wide array of applications to online resource allocation in sustainable energy and computing systems.","We devise optimal competitive and learning-augmented algorithms for specific instantiations of these problems, and further show that our proposed algorithms perform well in numerical experiments."],"url":"http://arxiv.org/abs/2402.14012v1","category":"cs.DS"}
{"created":"2024-02-21 18:50:50","title":"Two photons everywhere","abstract":"We discuss two-photon physics, taking for illustration the particular but topical case of resonance fluorescence. We show that the basic concepts of interferences and correlations provide at the two-photon level an independent and drastically different picture than at the one-photon level, with landscapes of correlations that reveal various processes by spanning over all the possible frequencies at which the system can emit. Such landscapes typically present lines of photon bunching and circles of antibunching. The theoretical edifice to account for these features rests on two pillars: i) a theory of frequency-resolved photon correlations and ii) admixing classical and quantum fields. While experimental efforts have been to date concentrated on correlations between spectral peaks, strong correlations exist between photons emitted away from the peaks, which are accessible only through multiphoton observables. These could be exploited for both fundamental understanding of quantum-optical processes as well as applications by harnessing these unsuspected resources.","sentences":["We discuss two-photon physics, taking for illustration the particular but topical case of resonance fluorescence.","We show that the basic concepts of interferences and correlations provide at the two-photon level an independent and drastically different picture than at the one-photon level, with landscapes of correlations that reveal various processes by spanning over all the possible frequencies at which the system can emit.","Such landscapes typically present lines of photon bunching and circles of antibunching.","The theoretical edifice to account for these features rests on two pillars: i) a theory of frequency-resolved photon correlations and ii) admixing classical and quantum fields.","While experimental efforts have been to date concentrated on correlations between spectral peaks, strong correlations exist between photons emitted away from the peaks, which are accessible only through multiphoton observables.","These could be exploited for both fundamental understanding of quantum-optical processes as well as applications by harnessing these unsuspected resources."],"url":"http://arxiv.org/abs/2402.14010v1","category":"quant-ph"}
{"created":"2024-02-21 18:48:38","title":"Interplay between 2D ferromagnetism and transport at the surface of FeSi","abstract":"FeSi is a curious example of a $d$-electron system that manifests many of the same phenomena associated with $f$-electron Kondo insulators, including conducting surface states with potentially non-trivial topology. Here we investigate the magnetization and magnetotransport of these surface states and how a 2D ferromagnetic state at the surface of FeSi influences the surface conductivity. We confirm the 2D ferromagnetism via a systematic study of magnetization on groups of filtered fragments with increasing surface area-to-volume ratios, identifying characteristic temperatures and magnetic fields associated with the ordered state. The paramagnetic to ferromagnetic transition appears broadened, suggesting disorder, which allows spin fluctuations to manifest up to at least 9 T at 2 K. This highlights the need to understand the relation between the disorder of the 2D ferromagnetism and the surface conductivity in FeSi.","sentences":["FeSi is a curious example of a $d$-electron system that manifests many of the same phenomena associated with $f$-electron Kondo insulators, including conducting surface states with potentially non-trivial topology.","Here we investigate the magnetization and magnetotransport of these surface states and how a 2D ferromagnetic state at the surface of FeSi influences the surface conductivity.","We confirm the 2D ferromagnetism via a systematic study of magnetization on groups of filtered fragments with increasing surface area-to-volume ratios, identifying characteristic temperatures and magnetic fields associated with the ordered state.","The paramagnetic to ferromagnetic transition appears broadened, suggesting disorder, which allows spin fluctuations to manifest up to at least 9 T at 2 K.","This highlights the need to understand the relation between the disorder of the 2D ferromagnetism and the surface conductivity in FeSi."],"url":"http://arxiv.org/abs/2402.14006v1","category":"cond-mat.str-el"}
{"created":"2024-02-21 18:35:27","title":"Asymptotics of Learning with Deep Structured (Random) Features","abstract":"For a large class of feature maps we provide a tight asymptotic characterisation of the test error associated with learning the readout layer, in the high-dimensional limit where the input dimension, hidden layer widths, and number of training samples are proportionally large. This characterization is formulated in terms of the population covariance of the features. Our work is partially motivated by the problem of learning with Gaussian rainbow neural networks, namely deep non-linear fully-connected networks with random but structured weights, whose row-wise covariances are further allowed to depend on the weights of previous layers. For such networks we also derive a closed-form formula for the feature covariance in terms of the weight matrices. We further find that in some cases our results can capture feature maps learned by deep, finite-width neural networks trained under gradient descent.","sentences":["For a large class of feature maps we provide a tight asymptotic characterisation of the test error associated with learning the readout layer, in the high-dimensional limit where the input dimension, hidden layer widths, and number of training samples are proportionally large.","This characterization is formulated in terms of the population covariance of the features.","Our work is partially motivated by the problem of learning with Gaussian rainbow neural networks, namely deep non-linear fully-connected networks with random but structured weights, whose row-wise covariances are further allowed to depend on the weights of previous layers.","For such networks we also derive a closed-form formula for the feature covariance in terms of the weight matrices.","We further find that in some cases our results can capture feature maps learned by deep, finite-width neural networks trained under gradient descent."],"url":"http://arxiv.org/abs/2402.13999v1","category":"stat.ML"}
{"created":"2024-02-21 18:19:20","title":"FedADMM-InSa: An Inexact and Self-Adaptive ADMM for Federated Learning","abstract":"Federated learning (FL) is a promising framework for learning from distributed data while maintaining privacy. The development of efficient FL algorithms encounters various challenges, including heterogeneous data and systems, limited communication capacities, and constrained local computational resources. Recently developed FedADMM methods show great resilience to both data and system heterogeneity. However, they still suffer from performance deterioration if the hyperparameters are not carefully tuned. To address this issue, we propose an inexact and self-adaptive FedADMM algorithm, termed FedADMM-InSa. First, we design an inexactness criterion for the clients' local updates to eliminate the need for empirically setting the local training accuracy. This inexactness criterion can be assessed by each client independently based on its unique condition, thereby reducing the local computational cost and mitigating the undesirable straggle effect. The convergence of the resulting inexact ADMM is proved under the assumption of strongly convex loss functions. Additionally, we present a self-adaptive scheme that dynamically adjusts each client's penalty parameter, enhancing algorithm robustness by mitigating the need for empirical penalty parameter choices for each client. Extensive numerical experiments on both synthetic and real-world datasets are conducted. As validated by some numerical tests, our proposed algorithm can reduce the clients' local computational load significantly and also accelerate the learning process compared to the vanilla FedADMM.","sentences":["Federated learning (FL) is a promising framework for learning from distributed data while maintaining privacy.","The development of efficient FL algorithms encounters various challenges, including heterogeneous data and systems, limited communication capacities, and constrained local computational resources.","Recently developed FedADMM methods show great resilience to both data and system heterogeneity.","However, they still suffer from performance deterioration if the hyperparameters are not carefully tuned.","To address this issue, we propose an inexact and self-adaptive FedADMM algorithm, termed FedADMM-InSa.","First, we design an inexactness criterion for the clients' local updates to eliminate the need for empirically setting the local training accuracy.","This inexactness criterion can be assessed by each client independently based on its unique condition, thereby reducing the local computational cost and mitigating the undesirable straggle effect.","The convergence of the resulting inexact ADMM is proved under the assumption of strongly convex loss functions.","Additionally, we present a self-adaptive scheme that dynamically adjusts each client's penalty parameter, enhancing algorithm robustness by mitigating the need for empirical penalty parameter choices for each client.","Extensive numerical experiments on both synthetic and real-world datasets are conducted.","As validated by some numerical tests, our proposed algorithm can reduce the clients' local computational load significantly and also accelerate the learning process compared to the vanilla FedADMM."],"url":"http://arxiv.org/abs/2402.13989v1","category":"cs.LG"}
{"created":"2024-02-21 18:17:17","title":"Hamiltonian Descent and Coordinate Hamiltonian Descent","abstract":"We propose an optimization algorithm called Hamiltonian Descent, which is a direct counterpart of classical Hamiltonian Monte Carlo in sampling. We show that Hamiltonian Descent has a better convergence rate than the Chebyshev method for solving a linear system of equations, albeit with a high computational cost. To overcome the cost issue, we then propose Coordinate Hamiltonian Descent and its parallelizable variant, which turns out to encapsulate the classical Gauss-Seidel method, Successive Over-relaxation, Jacobi method, and more. The result not only offers a new perspective on these existing algorithms but also leads to a broader class of update schemes that guarantee the convergence.","sentences":["We propose an optimization algorithm called Hamiltonian Descent, which is a direct counterpart of classical Hamiltonian Monte Carlo in sampling.","We show that Hamiltonian Descent has a better convergence rate than the Chebyshev method for solving a linear system of equations, albeit with a high computational cost.","To overcome the cost issue, we then propose Coordinate Hamiltonian Descent and its parallelizable variant, which turns out to encapsulate the classical Gauss-Seidel method, Successive Over-relaxation, Jacobi method, and more.","The result not only offers a new perspective on these existing algorithms but also leads to a broader class of update schemes that guarantee the convergence."],"url":"http://arxiv.org/abs/2402.13988v1","category":"math.OC"}
{"created":"2024-02-21 18:16:48","title":"A Simple and Yet Fairly Effective Defense for Graph Neural Networks","abstract":"Graph Neural Networks (GNNs) have emerged as the dominant approach for machine learning on graph-structured data. However, concerns have arisen regarding the vulnerability of GNNs to small adversarial perturbations. Existing defense methods against such perturbations suffer from high time complexity and can negatively impact the model's performance on clean graphs. To address these challenges, this paper introduces NoisyGNNs, a novel defense method that incorporates noise into the underlying model's architecture. We establish a theoretical connection between noise injection and the enhancement of GNN robustness, highlighting the effectiveness of our approach. We further conduct extensive empirical evaluations on the node classification task to validate our theoretical findings, focusing on two popular GNNs: the GCN and GIN. The results demonstrate that NoisyGNN achieves superior or comparable defense performance to existing methods while minimizing added time complexity. The NoisyGNN approach is model-agnostic, allowing it to be integrated with different GNN architectures. Successful combinations of our NoisyGNN approach with existing defense techniques demonstrate even further improved adversarial defense results. Our code is publicly available at: https://github.com/Sennadir/NoisyGNN.","sentences":["Graph Neural Networks (GNNs) have emerged as the dominant approach for machine learning on graph-structured data.","However, concerns have arisen regarding the vulnerability of GNNs to small adversarial perturbations.","Existing defense methods against such perturbations suffer from high time complexity and can negatively impact the model's performance on clean graphs.","To address these challenges, this paper introduces NoisyGNNs, a novel defense method that incorporates noise into the underlying model's architecture.","We establish a theoretical connection between noise injection and the enhancement of GNN robustness, highlighting the effectiveness of our approach.","We further conduct extensive empirical evaluations on the node classification task to validate our theoretical findings, focusing on two popular GNNs: the GCN and GIN.","The results demonstrate that NoisyGNN achieves superior or comparable defense performance to existing methods while minimizing added time complexity.","The NoisyGNN approach is model-agnostic, allowing it to be integrated with different GNN architectures.","Successful combinations of our NoisyGNN approach with existing defense techniques demonstrate even further improved adversarial defense results.","Our code is publicly available at: https://github.com/Sennadir/NoisyGNN."],"url":"http://arxiv.org/abs/2402.13987v1","category":"cs.LG"}
{"created":"2024-02-21 18:16:05","title":"WEAK $G$-IDENTITIES FOR THE PAIR $(M_2( \\mathbb{C}),sl_2( \\mathbb{C}))$","abstract":"In this paper we study algebras acted on by a finite group $G$ and the corresponding $G$-identities. Let $M_2( \\mathbb{C})$ be the $2\\times 2$ matrix algebra over the field of complex numbers $ \\mathbb{C}$ and let $sl_2( \\mathbb{C})$ be the Lie algebra of traceless matrices in $M_2( \\mathbb{C})$. Assume that $G$ is a finite group acting as a group of automorphisms on $M_2( \\mathbb{C})$. These groups were described in the Nineteenth century, they consist of the finite subgroups of $PGL_2( \\mathbb{C})$, which are, up to conjugacy, the cyclic groups $ \\mathbb{Z}_n$, the dihedral groups $D_n$ (of order $2n$), the alternating groups $ A_4$ and $A_5$, and the symmetric group $S_4$. The $G$-identities for $M_2( \\mathbb{C})$ were described by Berele. The finite groups acting on $sl_2( \\mathbb{C})$ are the same as those acting on $M_2( \\mathbb{C})$. The $G$-identities for the Lie algebra of the traceless $sl_2( \\mathbb{C})$ were obtained by Mortari and by the second author. We study the weak $G$-identities of the pair $(M_2( \\mathbb{C}), sl_2( \\mathbb{C}))$, when $G$ is a finite group. Since every automorphism of the pair is an automorphism for $M_2( \\mathbb{C})$, it follows from this that $G$ is one of the groups above. In this paper we obtain bases of the weak $G$-identities for the pair $(M_2( \\mathbb{C}), sl_2( \\mathbb{C}))$ when $G$ is a finite group acting as a group of automorphisms.","sentences":["In this paper we study algebras acted on by a finite group $G$ and the corresponding $G$-identities.","Let $M_2( \\mathbb{C})$ be the $2\\times 2$ matrix algebra over the field of complex numbers $ \\mathbb{C}$ and let $sl_2( \\mathbb{C})$ be the Lie algebra of traceless matrices in $M_2( \\mathbb{C})$. Assume that $G$ is a finite group acting as a group of automorphisms on $M_2( \\mathbb{C})$. These groups were described in the Nineteenth century, they consist of the finite subgroups of $PGL_2( \\mathbb{C})$, which are, up to conjugacy, the cyclic groups $ \\mathbb{Z}_n$, the dihedral groups $D_n$ (of order $2n$), the alternating groups $ A_4$ and $A_5$, and the symmetric group $S_4$. The $G$-identities for $M_2( \\mathbb{C})$ were described by Berele.","The finite groups acting on $sl_2( \\mathbb{C})$ are the same as those acting on $M_2( \\mathbb{C})$. The $G$-identities for the Lie algebra of the traceless $sl_2( \\mathbb{C})$ were obtained by Mortari and by the second author.","We study the weak $G$-identities of the pair $(M_2( \\mathbb{C}), sl_2( \\mathbb{C}))$, when $G$ is a finite group.","Since every automorphism of the pair is an automorphism for $M_2( \\mathbb{C})$, it follows from this that $G$ is one of the groups above.","In this paper we obtain bases of the weak $G$-identities for the pair $(M_2( \\mathbb{C}), sl_2( \\mathbb{C}))$ when $G$ is a finite group acting as a group of automorphisms."],"url":"http://arxiv.org/abs/2402.13986v1","category":"math.RA"}
{"created":"2024-02-21 18:00:05","title":"Quantum spin Hall effect protected by spin U(1) quasi-symmetry","abstract":"Quantum spin Hall (QSH) effect, where electrons with opposite spin channels are deflected to opposite sides of a two-dimensional system with a quantized conductance, was believed to be characterized by a nontrivial topological index $Z_{2}$. However, spin mixing effects in realistic materials often lead to deviation of the spin Hall conductance from exact quantization. In this Letter, we present a universal symmetry indicator for diagnosing QSH effect in realistic materials, termed spin $U$(1) quasi-symmetry. Such a symmetry eliminates the first-order spin-mixing perturbation and thus protects the near-quantization of SHC, applicable to time-reversal-preserved cases with either $Z_{2}=1$ or $Z_{2}=0$, as well as time-reversal-broken scenarios. We propose that spin $U$(1) quasi-symmetry is hidden in the subspace spanned by the doublets with unquenched orbital momentum and emerges when SOC is present, which can be realized in 19 crystallographic point groups. Particularly, we identify a previous overlooked even spin Chern phase with a trivial $Z_{2}$ index as an ideal platform for achieving a near-double-quantized SHC, as exemplified by twisted bilayer transition metal dichalcogenides and monolayer RuBr$_{3}$. Our work offers a new perspective for understanding QSH effect and significantly expands the material pool for the screening of exemplary material candidates.","sentences":["Quantum spin Hall (QSH) effect, where electrons with opposite spin channels are deflected to opposite sides of a two-dimensional system with a quantized conductance, was believed to be characterized by a nontrivial topological index $Z_{2}$. However, spin mixing effects in realistic materials often lead to deviation of the spin Hall conductance from exact quantization.","In this Letter, we present a universal symmetry indicator for diagnosing QSH effect in realistic materials, termed spin $U$(1) quasi-symmetry.","Such a symmetry eliminates the first-order spin-mixing perturbation and thus protects the near-quantization of SHC, applicable to time-reversal-preserved cases with either $Z_{2}=1$ or $Z_{2}=0$, as well as time-reversal-broken scenarios.","We propose that spin $U$(1) quasi-symmetry is hidden in the subspace spanned by the doublets with unquenched orbital momentum and emerges when SOC is present, which can be realized in 19 crystallographic point groups.","Particularly, we identify a previous overlooked even spin Chern phase with a trivial $Z_{2}$ index as an ideal platform for achieving a near-double-quantized SHC, as exemplified by twisted bilayer transition metal dichalcogenides and monolayer RuBr$_{3}$. Our work offers a new perspective for understanding QSH effect and significantly expands the material pool for the screening of exemplary material candidates."],"url":"http://arxiv.org/abs/2402.13974v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-21 17:58:10","title":"Linear-Time Graph Neural Networks for Scalable Recommendations","abstract":"In an era of information explosion, recommender systems are vital tools to deliver personalized recommendations for users. The key of recommender systems is to forecast users' future behaviors based on previous user-item interactions. Due to their strong expressive power of capturing high-order connectivities in user-item interaction data, recent years have witnessed a rising interest in leveraging Graph Neural Networks (GNNs) to boost the prediction performance of recommender systems. Nonetheless, classic Matrix Factorization (MF) and Deep Neural Network (DNN) approaches still play an important role in real-world large-scale recommender systems due to their scalability advantages. Despite the existence of GNN-acceleration solutions, it remains an open question whether GNN-based recommender systems can scale as efficiently as classic MF and DNN methods. In this paper, we propose a Linear-Time Graph Neural Network (LTGNN) to scale up GNN-based recommender systems to achieve comparable scalability as classic MF approaches while maintaining GNNs' powerful expressiveness for superior prediction accuracy. Extensive experiments and ablation studies are presented to validate the effectiveness and scalability of the proposed algorithm. Our implementation based on PyTorch is available.","sentences":["In an era of information explosion, recommender systems are vital tools to deliver personalized recommendations for users.","The key of recommender systems is to forecast users' future behaviors based on previous user-item interactions.","Due to their strong expressive power of capturing high-order connectivities in user-item interaction data, recent years have witnessed a rising interest in leveraging Graph Neural Networks (GNNs) to boost the prediction performance of recommender systems.","Nonetheless, classic Matrix Factorization (MF) and Deep Neural Network (DNN) approaches still play an important role in real-world large-scale recommender systems due to their scalability advantages.","Despite the existence of GNN-acceleration solutions, it remains an open question whether GNN-based recommender systems can scale as efficiently as classic MF and DNN methods.","In this paper, we propose a Linear-Time Graph Neural Network (LTGNN) to scale up GNN-based recommender systems to achieve comparable scalability as classic MF approaches while maintaining GNNs' powerful expressiveness for superior prediction accuracy.","Extensive experiments and ablation studies are presented to validate the effectiveness and scalability of the proposed algorithm.","Our implementation based on PyTorch is available."],"url":"http://arxiv.org/abs/2402.13973v1","category":"cs.IR"}
{"created":"2024-02-21 17:49:56","title":"Study of nanodiamond photocathodes for MPGD-based detectors of single photons","abstract":"The proposed new Electron-Ion Collider poses a technical and intellectual challenge for the detector design to accommodate the long-term diverse physics goals envisaged by the program. This requires a 4{\\pi} detector system capable of reconstructing the energy and momentum of final state particles with high precision. The Electron-Ion Collider also requires identification of particles of different masses over a wide momentum range. A diverse spectrum of Particle Identification detectors has been proposed. Of the four types of detectors for hadron identification, three are based on Ring Imaging Cherenkov Counter technologies, and one is realized by the Time of Flight method. The quest for a novel photocathode, sensitive in the far vacuum ultra violet wavelength range and more robust than cesium iodide, motivated an R&D programme to explore nano-diamond (ND) based photocathodes, started by a collaboration between INFN and CNR Bari and INFN Trieste. Systematic measurements of the photo emission in different Ar-CH4 and Ar-CO2 gas mixtures with various types of ND powders and Hydrogenated ND (H-ND) powders are reported. A first study of the response of THGEMs coated with different photocathode materials is presented. The progress of this R&D programme and the results obtained so far by these exploratory studies are described.","sentences":["The proposed new Electron-Ion Collider poses a technical and intellectual challenge for the detector design to accommodate the long-term diverse physics goals envisaged by the program.","This requires a 4{\\pi} detector system capable of reconstructing the energy and momentum of final state particles with high precision.","The Electron-Ion Collider also requires identification of particles of different masses over a wide momentum range.","A diverse spectrum of Particle Identification detectors has been proposed.","Of the four types of detectors for hadron identification, three are based on Ring Imaging Cherenkov Counter technologies, and one is realized by the Time of Flight method.","The quest for a novel photocathode, sensitive in the far vacuum ultra violet wavelength range and more robust than cesium iodide, motivated an R&D programme to explore nano-diamond (ND) based photocathodes, started by a collaboration between INFN and CNR Bari and INFN Trieste.","Systematic measurements of the photo emission in different Ar-CH4 and Ar-CO2 gas mixtures with various types of ND powders and Hydrogenated ND (H-ND) powders are reported.","A first study of the response of THGEMs coated with different photocathode materials is presented.","The progress of this R&D programme and the results obtained so far by these exploratory studies are described."],"url":"http://arxiv.org/abs/2402.13966v1","category":"physics.ins-det"}
{"created":"2024-02-21 17:46:06","title":"A robust near-field body area network based on coaxially-shielded textile metamaterial","abstract":"A body area network (BAN) involving wearable sensors populated around the human body can continuously monitor physiological signals, finding applications in personal healthcare and athletic evaluation. Existing near-field communication (NFC)-enabled BAN solutions, while facilitating reliable and secure interconnection among battery-free sensors, face challenges such as limited spectral stability against external interference. Here we demonstrate a textile metamaterial featuring a coaxially-shielded internal structure designed to mitigate interference from extraneous loadings. The metamaterial can be patterned onto clothing to form a scalable, customizable network, enabling communication between NFC-enabled devices and developed battery-free textile NFC sensing nodes placed within the network. Proof of concept demonstration shows the metamaterial's robustness against mechanical deformation and exposure to lossy, conductive saline solutions, underscoring its potential applications in wet environments, particularly in athletic activities involving water or significant perspiration, offering insights for the future development of radio frequency components for a robust BAN at the system level.","sentences":["A body area network (BAN) involving wearable sensors populated around the human body can continuously monitor physiological signals, finding applications in personal healthcare and athletic evaluation.","Existing near-field communication (NFC)-enabled BAN solutions, while facilitating reliable and secure interconnection among battery-free sensors, face challenges such as limited spectral stability against external interference.","Here we demonstrate a textile metamaterial featuring a coaxially-shielded internal structure designed to mitigate interference from extraneous loadings.","The metamaterial can be patterned onto clothing to form a scalable, customizable network, enabling communication between NFC-enabled devices and developed battery-free textile NFC sensing nodes placed within the network.","Proof of concept demonstration shows the metamaterial's robustness against mechanical deformation and exposure to lossy, conductive saline solutions, underscoring its potential applications in wet environments, particularly in athletic activities involving water or significant perspiration, offering insights for the future development of radio frequency components for a robust BAN at the system level."],"url":"http://arxiv.org/abs/2402.13962v1","category":"physics.app-ph"}
{"created":"2024-02-21 17:45:03","title":"Evaluating Ground State Energies of Chemical Systems with Low-Depth Quantum Circuits and High Accuracy","abstract":"Solving electronic structure problems is considered one of the most promising applications of quantum computing. However, due to limitations imposed by the coherence time of qubits in the Noisy Intermediate Scale Quantum (NISQ) era or the capabilities of early fault-tolerant quantum devices, it is vital to design algorithms with low-depth circuits. In this work, we develop an enhanced Variational Quantum Eigensolver (VQE) ansatz based on the Qubit Coupled Cluster (QCC) approach, which demands optimization over only $n$ parameters rather than the usual $n+2m$ parameters, where $n$ represents the number of Pauli string time evolution gates $e^{-itP}$, and $m$ is the number of qubits involved. We evaluate the ground state energies of $\\mathrm{O_3}$, $\\mathrm{Li_4}$, and $\\mathrm{Cr_2}$, using CAS(2,2), (4,4) and (6,6) respectively in conjunction with our enhanced QCC ansatz, UCCSD (Unitary Coupled Cluster Single Double) ansatz, and canonical CCSD method as the active space solver, and compare with CASCI results. Finally, we assess our enhanced QCC ansatz on two distinct quantum hardware, IBM Kolkata and Quantinuum H1-1.","sentences":["Solving electronic structure problems is considered one of the most promising applications of quantum computing.","However, due to limitations imposed by the coherence time of qubits in the Noisy Intermediate Scale Quantum (NISQ) era or the capabilities of early fault-tolerant quantum devices, it is vital to design algorithms with low-depth circuits.","In this work, we develop an enhanced Variational Quantum Eigensolver (VQE) ansatz based on the Qubit Coupled Cluster (QCC) approach, which demands optimization over only $n$ parameters rather than the usual $n+2m$ parameters, where $n$ represents the number of Pauli string time evolution gates $e^{-itP}$, and $m$ is the number of qubits involved.","We evaluate the ground state energies of $\\mathrm{O_3}$, $\\mathrm{Li_4}$, and $\\mathrm{Cr_2}$, using CAS(2,2), (4,4) and (6,6) respectively in conjunction with our enhanced QCC ansatz, UCCSD (Unitary Coupled Cluster Single Double) ansatz, and canonical CCSD method as the active space solver, and compare with CASCI results.","Finally, we assess our enhanced QCC ansatz on two distinct quantum hardware, IBM Kolkata and Quantinuum H1-1."],"url":"http://arxiv.org/abs/2402.13960v1","category":"quant-ph"}
{"created":"2024-02-21 17:41:17","title":"Retention Induced Biases in a Recommendation System with Heterogeneous Users","abstract":"I examine a conceptual model of a recommendation system (RS) with user inflow and churn dynamics. When inflow and churn balance out, the user distribution reaches a steady state. Changing the recommendation algorithm alters the steady state and creates a transition period. During this period, the RS behaves differently from its new steady state. In particular, A/B experiment metrics obtained in transition periods are biased indicators of the RS's long term performance. Scholars and practitioners, however, often conduct A/B tests shortly after introducing new algorithms to validate their effectiveness. This A/B experiment paradigm, widely regarded as the gold standard for assessing RS improvements, may consequently yield false conclusions. I also briefly discuss the data bias caused by the user retention dynamics.","sentences":["I examine a conceptual model of a recommendation system (RS) with user inflow and churn dynamics.","When inflow and churn balance out, the user distribution reaches a steady state.","Changing the recommendation algorithm alters the steady state and creates a transition period.","During this period, the RS behaves differently from its new steady state.","In particular, A/B experiment metrics obtained in transition periods are biased indicators of the RS's long term performance.","Scholars and practitioners, however, often conduct A/B tests shortly after introducing new algorithms to validate their effectiveness.","This A/B experiment paradigm, widely regarded as the gold standard for assessing RS improvements, may consequently yield false conclusions.","I also briefly discuss the data bias caused by the user retention dynamics."],"url":"http://arxiv.org/abs/2402.13959v1","category":"cs.IR"}
{"created":"2024-02-21 17:37:30","title":"Advancing Audio Fingerprinting Accuracy Addressing Background Noise and Distortion Challenges","abstract":"Audio fingerprinting, exemplified by pioneers like Shazam, has transformed digital audio recognition. However, existing systems struggle with accuracy in challenging conditions, limiting broad applicability. This research proposes an AI and ML integrated audio fingerprinting algorithm to enhance accuracy. Built on the Dejavu Project's foundations, the study emphasizes real-world scenario simulations with diverse background noises and distortions. Signal processing, central to Dejavu's model, includes the Fast Fourier Transform, spectrograms, and peak extraction. The \"constellation\" concept and fingerprint hashing enable unique song identification. Performance evaluation attests to 100% accuracy within a 5-second audio input, with a system showcasing predictable matching speed for efficiency. Storage analysis highlights the critical space-speed trade-off for practical implementation. This research advances audio fingerprinting's adaptability, addressing challenges in varied environments and applications.","sentences":["Audio fingerprinting, exemplified by pioneers like Shazam, has transformed digital audio recognition.","However, existing systems struggle with accuracy in challenging conditions, limiting broad applicability.","This research proposes an AI and ML integrated audio fingerprinting algorithm to enhance accuracy.","Built on the Dejavu Project's foundations, the study emphasizes real-world scenario simulations with diverse background noises and distortions.","Signal processing, central to Dejavu's model, includes the Fast Fourier Transform, spectrograms, and peak extraction.","The \"constellation\" concept and fingerprint hashing enable unique song identification.","Performance evaluation attests to 100% accuracy within a 5-second audio input, with a system showcasing predictable matching speed for efficiency.","Storage analysis highlights the critical space-speed trade-off for practical implementation.","This research advances audio fingerprinting's adaptability, addressing challenges in varied environments and applications."],"url":"http://arxiv.org/abs/2402.13957v1","category":"cs.SD"}
{"created":"2024-02-21 17:29:34","title":"Aaronson-Ambainis Conjecture Is True For Random Restrictions","abstract":"In an attempt to show that the acceptance probability of a quantum query algorithm making $q$ queries can be well-approximated almost everywhere by a classical decision tree of depth $\\leq \\text{poly}(q)$, Aaronson and Ambainis proposed the following conjecture: let $f: \\{ \\pm 1\\}^n \\rightarrow [0,1]$ be a degree $d$ polynomial with variance $\\geq \\epsilon$. Then, there exists a coordinate of $f$ with influence $\\geq \\text{poly} (\\epsilon, 1/d)$.   We show that for any polynomial $f: \\{ \\pm 1\\}^n \\rightarrow [0,1]$ of degree $d$ $(d \\geq 2)$ and variance $\\text{Var}[f] \\geq 1/d$, if $\\rho$ denotes a random restriction with survival probability $\\dfrac{\\log(d)}{C_1 d}$, $$ \\text{Pr} \\left[f_{\\rho} \\text{ has a coordinate with influence} \\geq \\dfrac{\\text{Var}[f]^2 }{d^{C_2}} \\right] \\geq \\dfrac{\\text{Var}[f] \\log(d)}{50C_1 d}$$ where $C_1, C_2>0$ are universal constants. Thus, Aaronson-Ambainis conjecture is true for a non-negligible fraction of random restrictions of the given polynomial assuming its variance is not too low.","sentences":["In an attempt to show that the acceptance probability of a quantum query algorithm making $q$ queries can be well-approximated almost everywhere by a classical decision tree of depth $\\leq \\text{poly}(q)$, Aaronson and Ambainis proposed the following conjecture: let $f: \\{ \\pm 1\\}^n \\rightarrow","[0,1]$ be a degree $d$ polynomial with variance $\\geq \\epsilon$. Then, there exists a coordinate of $f$ with influence $\\geq \\text{poly} (\\epsilon, 1/d)$.   We show that for any polynomial $f: \\{ \\pm 1\\}^n \\rightarrow","[0,1]$ of degree $d$ $(d \\geq 2)$ and variance $\\text{Var}[f] \\geq 1/d$, if $\\rho$ denotes a random restriction with survival probability $\\dfrac{\\log(d)}{C_1 d}$, $$ \\text{Pr} \\left[f_{\\rho} \\text{ has a coordinate with influence} \\geq \\dfrac{\\text{Var}[f]^2 }{d^{C_2}} \\right] \\geq \\dfrac{\\text{Var}[f] \\log(d)}{50C_1 d}$$ where $C_1, C_2>0$ are universal constants.","Thus, Aaronson-Ambainis conjecture is true for a non-negligible fraction of random restrictions of the given polynomial assuming its variance is not too low."],"url":"http://arxiv.org/abs/2402.13952v1","category":"cs.CC"}
{"created":"2024-02-21 17:23:29","title":"Improved Syndrome-based Neural Decoder for Linear Block Codes","abstract":"In this work, we investigate the problem of neural-based error correction decoding, and more specifically, the new so-called syndrome-based decoding technique introduced to tackle scalability in the training phase for larger code sizes. We improve on previous works in terms of allowing full decoding of the message rather than codewords, allowing thus the application to non-systematic codes, and proving that the single-message training property is still viable. The suggested system is implemented and tested on polar codes of sizes (64,32) and (128,64), and a BCH of size (63,51), leading to a significant improvement in both Bit Error Rate (BER) and Frame Error Rate (FER), with gains between 0.3dB and 1dB for the implemented codes in the high Signal-to-Noise Ratio (SNR) regime.","sentences":["In this work, we investigate the problem of neural-based error correction decoding, and more specifically, the new so-called syndrome-based decoding technique introduced to tackle scalability in the training phase for larger code sizes.","We improve on previous works in terms of allowing full decoding of the message rather than codewords, allowing thus the application to non-systematic codes, and proving that the single-message training property is still viable.","The suggested system is implemented and tested on polar codes of sizes (64,32) and (128,64), and a BCH of size (63,51), leading to a significant improvement in both Bit Error Rate (BER) and Frame Error Rate (FER), with gains between 0.3dB and 1dB for the implemented codes in the high Signal-to-Noise Ratio (SNR) regime."],"url":"http://arxiv.org/abs/2402.13948v1","category":"cs.IT"}
{"created":"2024-02-21 17:11:07","title":"The Maintenance of Coherent Vortex Topology by Lagrangian Chaos in Drift-Rossby Wave Turbulence","abstract":"This work introduces the \"potential vorticity (PV) bucket brigade\", a conceptual mechanism for explaining the resilience of coherent vortex structures in drift-Rossby wave turbulence, critical for understanding turbulent transport in magnetically confined fusion plasmas and geophysical flows. Drawing parallels with the PV staircase, we show how spatially inhomogeneous patterns of mixing can reinforce, rather than destroy non-zonal structures in the flow. We accomplish this through an exact stochastic Lagrangian representation of vorticity transport, together with a wave eigenmode near-integrability property which elucidates the relationship between coherent flow topology and fluid relabeling symmetries. For concreteness, we demonstrate these ideas in a transitional regime of gradient-driven drift wave turbulence modeled by the flux-balanced Hasegawa-Wakatani equations. However, the tools we develop here are model-agnostic and have potential relevance to fluid and plasma systems well beyond the model studied here.","sentences":["This work introduces the \"potential vorticity (PV) bucket brigade\", a conceptual mechanism for explaining the resilience of coherent vortex structures in drift-Rossby wave turbulence, critical for understanding turbulent transport in magnetically confined fusion plasmas and geophysical flows.","Drawing parallels with the PV staircase, we show how spatially inhomogeneous patterns of mixing can reinforce, rather than destroy non-zonal structures in the flow.","We accomplish this through an exact stochastic Lagrangian representation of vorticity transport, together with a wave eigenmode near-integrability property which elucidates the relationship between coherent flow topology and fluid relabeling symmetries.","For concreteness, we demonstrate these ideas in a transitional regime of gradient-driven drift wave turbulence modeled by the flux-balanced Hasegawa-Wakatani equations.","However, the tools we develop here are model-agnostic and have potential relevance to fluid and plasma systems well beyond the model studied here."],"url":"http://arxiv.org/abs/2402.13942v1","category":"physics.plasm-ph"}
{"created":"2024-02-21 17:09:29","title":"On the topological classification of complex plane curve singularities","abstract":"This final degree project is devoted to study the topological classification of complex plane curves. These are subsets of $\\mathbb{C}^2$ that can be described by an equation $f(x,y)=0$. Loosely speaking, curves are said to be equivalent in a topological sense whenever they are ambient homeomorphic, i.e., there exists an orientation-preserving homeomorphism of the ambient space carrying one curve to the other. The project's aim is to develop operative and clear conditions to determine whether two curves are equivalent. Curves will be shown to be decomposable into branches: sets that can be explicitly parametrised in the form $x=\\phi(t), y=\\psi(t)$. These parametric expressions will be analysed to extract a complete numerical invariant for the classification of branches: the Puiseux characteristic. The intermediate key result to lend this notion with topological weight is that a branch can be completely described through its associated knot, arising from the intersection of the branch with a small enough 3-sphere. The combination of these above-mentioned facts will then culminate in the project's most powerful result, which assures that two curves are equivalent if and only if their branches share the same Puiseux characteristics and intersection numbers.","sentences":["This final degree project is devoted to study the topological classification of complex plane curves.","These are subsets of $\\mathbb{C}^2$ that can be described by an equation $f(x,y)=0$. Loosely speaking, curves are said to be equivalent in a topological sense whenever they are ambient homeomorphic, i.e., there exists an orientation-preserving homeomorphism of the ambient space carrying one curve to the other.","The project's aim is to develop operative and clear conditions to determine whether two curves are equivalent.","Curves will be shown to be decomposable into branches: sets that can be explicitly parametrised in the form $x=\\phi(t),","y=\\psi(t)$. These parametric expressions will be analysed to extract a complete numerical invariant for the classification of branches: the Puiseux characteristic.","The intermediate key result to lend this notion with topological weight is that a branch can be completely described through its associated knot, arising from the intersection of the branch with a small enough 3-sphere.","The combination of these above-mentioned facts will then culminate in the project's most powerful result, which assures that two curves are equivalent if and only if their branches share the same Puiseux characteristics and intersection numbers."],"url":"http://arxiv.org/abs/2402.13941v1","category":"math.AG"}
{"created":"2024-02-21 17:01:49","title":"Completeness of the space of separable measures in the Kantorovich-Rubinshte\u012dn metric","abstract":"We consider the space $M(X)$ of separable measures on the Borel $\\sigma$-algebra ${\\cal B}(X)$ of a metric space $X$. The space $M(X)$ is furnished with the Kantorovich-Rubinshte\\u{i}n metric known also as the ``Hutchinson distance''. We prove that $M(X)$ is complete if and only if $X$ is complete. We consider applications of this theorem in the theory of self-similar fractals.","sentences":["We consider the space $M(X)$ of separable measures on the Borel $\\sigma$-algebra ${\\cal B}(X)$ of a metric space $X$. The space $M(X)$ is furnished with the Kantorovich-Rubinshte\\u{i}n metric known also as the ``Hutchinson distance''.","We prove that $M(X)$ is complete if and only if $X$ is complete.","We consider applications of this theorem in the theory of self-similar fractals."],"url":"http://arxiv.org/abs/2402.13935v1","category":"math.MG"}
{"created":"2024-02-21 16:57:59","title":"Planetary atmospheres Through Time: Effects of Mass Loss and Thermal Evolution","abstract":"Atmospheric mass loss is a fundamental phenomenon shaping the structure and evolution of planetary atmospheres. It can engage processes ranging from global interactions with the host star and large-scale hydrodynamic outflows to essentially microphysical kinetic effects. The relevance of these processes is expected to change between planets of different properties and at different stages in planetary and stellar evolution. The early evolution of planetary atmospheres, as well as atmospheric escape from close-in planets hosting hydrogen-dominated atmospheres, is thought to be driven by thermal hydrodynamic escape, while the kinetic non-thermal effects are most relevant for the long-term evolution of planets with secondary atmospheres, similar to the inner planets in the Solar System. The relative input of different mechanisms, hence, the mass loss rate, shows a complicated dependence on planetary parameters and the parameters of the host star, where the latter evolve strongly with time. It results in a large variety of possible evolution paths of planetary atmospheres.","sentences":["Atmospheric mass loss is a fundamental phenomenon shaping the structure and evolution of planetary atmospheres.","It can engage processes ranging from global interactions with the host star and large-scale hydrodynamic outflows to essentially microphysical kinetic effects.","The relevance of these processes is expected to change between planets of different properties and at different stages in planetary and stellar evolution.","The early evolution of planetary atmospheres, as well as atmospheric escape from close-in planets hosting hydrogen-dominated atmospheres, is thought to be driven by thermal hydrodynamic escape, while the kinetic non-thermal effects are most relevant for the long-term evolution of planets with secondary atmospheres, similar to the inner planets in the Solar System.","The relative input of different mechanisms, hence, the mass loss rate, shows a complicated dependence on planetary parameters and the parameters of the host star, where the latter evolve strongly with time.","It results in a large variety of possible evolution paths of planetary atmospheres."],"url":"http://arxiv.org/abs/2402.13931v1","category":"astro-ph.EP"}
{"created":"2024-02-21 16:52:26","title":"Enhancing Reinforcement Learning Agents with Local Guides","abstract":"This paper addresses the problem of integrating local guide policies into a Reinforcement Learning agent. For this, we show how to adapt existing algorithms to this setting before introducing a novel algorithm based on a noisy policy-switching procedure. This approach builds on a proper Approximate Policy Evaluation (APE) scheme to provide a perturbation that carefully leads the local guides towards better actions. We evaluated our method on a set of classical Reinforcement Learning problems, including safety-critical systems where the agent cannot enter some areas at the risk of triggering catastrophic consequences. In all the proposed environments, our agent proved to be efficient at leveraging those policies to improve the performance of any APE-based Reinforcement Learning algorithm, especially in its first learning stages.","sentences":["This paper addresses the problem of integrating local guide policies into a Reinforcement Learning agent.","For this, we show how to adapt existing algorithms to this setting before introducing a novel algorithm based on a noisy policy-switching procedure.","This approach builds on a proper Approximate Policy Evaluation (APE) scheme to provide a perturbation that carefully leads the local guides towards better actions.","We evaluated our method on a set of classical Reinforcement Learning problems, including safety-critical systems where the agent cannot enter some areas at the risk of triggering catastrophic consequences.","In all the proposed environments, our agent proved to be efficient at leveraging those policies to improve the performance of any APE-based Reinforcement Learning algorithm, especially in its first learning stages."],"url":"http://arxiv.org/abs/2402.13930v1","category":"cs.LG"}
{"created":"2024-02-21 16:40:29","title":"Development of multi-physics finite element model to investigate electromagnetic forming and simultaneous multi-point perforations of aluminium tube","abstract":"Electromagnetic forming and perforations (EMFP) are complex and innovative high strain rate processes that involve electromagnetic-mechanical interactions for simultaneous metal forming and perforations. Instead of spending costly resources on repetitive experimental work, a properly designed numerical model can be effectively used for detailed analysis and characterization of the complex process. A coupled finite element (FE) model is considered for analyzing the multi-physics of the EMFP because of its robustness and improved accuracy. In this work, a detailed understanding of the process has been achieved by numerically simulating forming and perforations of Al6061-T6 tube for 12 holes and 36 holes with two different punches, i.e., pointed and concave punches using Ls-Dyna software. In order to shed light on EMFP physics, a comparison between experimental data and the formulated numerical simulation has been carried out to compare the average hole diameter and the number of perforated holes, for different types of punches and a range of discharge energies. The simulated results show acceptable agreement with experimental studies, with maximum deviations being less than or equal to 6%, which clearly illustrates the efficacy and capability of the developed coupled Multi-physics FE model.","sentences":["Electromagnetic forming and perforations (EMFP) are complex and innovative high strain rate processes that involve electromagnetic-mechanical interactions for simultaneous metal forming and perforations.","Instead of spending costly resources on repetitive experimental work, a properly designed numerical model can be effectively used for detailed analysis and characterization of the complex process.","A coupled finite element (FE) model is considered for analyzing the multi-physics of the EMFP because of its robustness and improved accuracy.","In this work, a detailed understanding of the process has been achieved by numerically simulating forming and perforations of Al6061-T6 tube for 12 holes and 36 holes with two different punches, i.e., pointed and concave punches using Ls-Dyna software.","In order to shed light on EMFP physics, a comparison between experimental data and the formulated numerical simulation has been carried out to compare the average hole diameter and the number of perforated holes, for different types of punches and a range of discharge energies.","The simulated results show acceptable agreement with experimental studies, with maximum deviations being less than or equal to 6%, which clearly illustrates the efficacy and capability of the developed coupled Multi-physics FE model."],"url":"http://arxiv.org/abs/2402.13922v1","category":"math.NA"}
{"created":"2024-02-21 16:39:28","title":"Practical algorithms for Hierarchical overlap graphs","abstract":"Genome assembly is a prominent problem studied in bioinformatics, which computes the source string using a set of its overlapping substrings. Classically, genome assembly uses assembly graphs built using this set of substrings to compute the source string efficiently, having a tradeoff between scalability and avoiding information loss. The scalable de Bruijn graphs come at the price of losing crucial overlap information. The complete overlap information is stored in overlap graphs using quadratic space. Hierarchical overlap graphs [IPL20] (HOG) overcome these limitations, avoiding information loss despite using linear space. After a series of suboptimal improvements, Khan and Park et al. simultaneously presented two optimal algorithms [CPM2021], where only the former was seemingly practical.   We empirically analyze all the practical algorithms for computing HOG, where the optimal algorithm [CPM2021] outperforms the previous algorithms as expected, though at the expense of extra memory. However, it uses non-intuitive approach and non-trivial data structures. We present arguably the most intuitive algorithm, using only elementary arrays, which is also optimal. Our algorithm empirically proves even better for both time and memory over all the algorithms, highlighting its significance in both theory and practice.   We further explore the applications of hierarchical overlap graphs to solve various forms of suffix-prefix queries on a set of strings. Loukides et al. [CPM2023] recently presented state-of-the-art algorithms for these queries. However, these algorithms require complex black-box data structures and are seemingly impractical. Our algorithms, despite failing to match the state-of-the-art algorithms theoretically, answer different queries ranging from 0.01-100 milliseconds for a data set having around a billion characters.","sentences":["Genome assembly is a prominent problem studied in bioinformatics, which computes the source string using a set of its overlapping substrings.","Classically, genome assembly uses assembly graphs built using this set of substrings to compute the source string efficiently, having a tradeoff between scalability and avoiding information loss.","The scalable de Bruijn graphs come at the price of losing crucial overlap information.","The complete overlap information is stored in overlap graphs using quadratic space.","Hierarchical overlap graphs [IPL20] (HOG) overcome these limitations, avoiding information loss despite using linear space.","After a series of suboptimal improvements, Khan and Park et al. simultaneously presented two optimal algorithms","[CPM2021], where only the former was seemingly practical.   ","We empirically analyze all the practical algorithms for computing HOG, where the optimal algorithm","[CPM2021] outperforms the previous algorithms as expected, though at the expense of extra memory.","However, it uses non-intuitive approach and non-trivial data structures.","We present arguably the most intuitive algorithm, using only elementary arrays, which is also optimal.","Our algorithm empirically proves even better for both time and memory over all the algorithms, highlighting its significance in both theory and practice.   ","We further explore the applications of hierarchical overlap graphs to solve various forms of suffix-prefix queries on a set of strings.","Loukides et al.","[CPM2023] recently presented state-of-the-art algorithms for these queries.","However, these algorithms require complex black-box data structures and are seemingly impractical.","Our algorithms, despite failing to match the state-of-the-art algorithms theoretically, answer different queries ranging from 0.01-100 milliseconds for a data set having around a billion characters."],"url":"http://arxiv.org/abs/2402.13920v1","category":"cs.DS"}
{"created":"2024-02-21 16:32:43","title":"BenchCloudVision: A Benchmark Analysis of Deep Learning Approaches for Cloud Detection and Segmentation in Remote Sensing Imagery","abstract":"Satellites equipped with optical sensors capture high-resolution imagery, providing valuable insights into various environmental phenomena. In recent years, there has been a surge of research focused on addressing some challenges in remote sensing, ranging from water detection in diverse landscapes to the segmentation of mountainous and terrains. Ongoing investigations goals to enhance the precision and efficiency of satellite imagery analysis. Especially, there is a growing emphasis on developing methodologies for accurate water body detection, snow and clouds, important for environmental monitoring, resource management, and disaster response. Within this context, this paper focus on the cloud segmentation from remote sensing imagery. Accurate remote sensing data analysis can be challenging due to the presence of clouds in optical sensor-based applications. The quality of resulting products such as applications and research is directly impacted by cloud detection, which plays a key role in the remote sensing data processing pipeline. This paper examines seven cutting-edge semantic segmentation and detection algorithms applied to clouds identification, conducting a benchmark analysis to evaluate their architectural approaches and identify the most performing ones. To increase the model's adaptability, critical elements including the type of imagery and the amount of spectral bands used during training are analyzed. Additionally, this research tries to produce machine learning algorithms that can perform cloud segmentation using only a few spectral bands, including RGB and RGBN-IR combinations. The model's flexibility for a variety of applications and user scenarios is assessed by using imagery from Sentinel-2 and Landsat-8 as datasets. This benchmark can be reproduced using the material from this github link: \\url{https://github.com/toelt-llc/cloud\\_segmentation\\_comparative}.","sentences":["Satellites equipped with optical sensors capture high-resolution imagery, providing valuable insights into various environmental phenomena.","In recent years, there has been a surge of research focused on addressing some challenges in remote sensing, ranging from water detection in diverse landscapes to the segmentation of mountainous and terrains.","Ongoing investigations goals to enhance the precision and efficiency of satellite imagery analysis.","Especially, there is a growing emphasis on developing methodologies for accurate water body detection, snow and clouds, important for environmental monitoring, resource management, and disaster response.","Within this context, this paper focus on the cloud segmentation from remote sensing imagery.","Accurate remote sensing data analysis can be challenging due to the presence of clouds in optical sensor-based applications.","The quality of resulting products such as applications and research is directly impacted by cloud detection, which plays a key role in the remote sensing data processing pipeline.","This paper examines seven cutting-edge semantic segmentation and detection algorithms applied to clouds identification, conducting a benchmark analysis to evaluate their architectural approaches and identify the most performing ones.","To increase the model's adaptability, critical elements including the type of imagery and the amount of spectral bands used during training are analyzed.","Additionally, this research tries to produce machine learning algorithms that can perform cloud segmentation using only a few spectral bands, including RGB and RGBN-IR combinations.","The model's flexibility for a variety of applications and user scenarios is assessed by using imagery from Sentinel-2 and Landsat-8 as datasets.","This benchmark can be reproduced using the material from this github link: \\url{https://github.com/toelt-llc/cloud\\_segmentation\\_comparative}."],"url":"http://arxiv.org/abs/2402.13918v1","category":"cs.CV"}
{"created":"2024-02-21 16:09:49","title":"Room-temperature quantum sensing with photoexcited triplet electrons in organic crystals","abstract":"Quantum sensors have notably advanced high-sensitivity magnetic field detection. Here, we report quantum sensors constructed from polarized spin-triplet electrons in photoexcited organic chromophores, specifically focusing on pentacene-doped para-terphenyl (${\\approx}$0.1%). We demonstrate essential quantum sensing properties at room temperature: electronic optical polarization and state-dependent fluorescence contrast, by leveraging differential pumping and relaxation rates between triplet and ground states. We measure high optically detected magnetic resonance (ODMR) contrast ${\\approx}16.8\\%$ of the triplet states at room temperature, along with long coherence times under spin echo and CPMG sequences, $T_2{=}2.7\\mu$s and $T_2^{DD}{=}18.4\\mu$s respectively, limited only by the triplet lifetimes. The material offers several advantages for quantum sensing, including the ability to grow large ($cm$-scale) crystals at low cost, the absence of paramagnetic impurities, and the diamagnetism of electronic states used for sensing when not optically illuminated. Utilizing pentacene as a representative of a broader class of spin triplet-polarizable organic molecules, this study highlights new potential for quantum sensing in chemical systems.","sentences":["Quantum sensors have notably advanced high-sensitivity magnetic field detection.","Here, we report quantum sensors constructed from polarized spin-triplet electrons in photoexcited organic chromophores, specifically focusing on pentacene-doped para-terphenyl (${\\approx}$0.1%).","We demonstrate essential quantum sensing properties at room temperature: electronic optical polarization and state-dependent fluorescence contrast, by leveraging differential pumping and relaxation rates between triplet and ground states.","We measure high optically detected magnetic resonance (ODMR) contrast ${\\approx}16.8\\%$ of the triplet states at room temperature, along with long coherence times under spin echo and CPMG sequences, $T_2{=}2.7\\mu$s and $T_2^{DD}{=}18.4\\mu$s respectively, limited only by the triplet lifetimes.","The material offers several advantages for quantum sensing, including the ability to grow large ($cm$-scale) crystals at low cost, the absence of paramagnetic impurities, and the diamagnetism of electronic states used for sensing when not optically illuminated.","Utilizing pentacene as a representative of a broader class of spin triplet-polarizable organic molecules, this study highlights new potential for quantum sensing in chemical systems."],"url":"http://arxiv.org/abs/2402.13898v1","category":"quant-ph"}
{"created":"2024-02-21 16:09:17","title":"HOMULA-RIR: A Room Impulse Response Dataset for Teleconferencing and Spatial Audio Applications Acquired Through Higher-Order Microphones and Uniform Linear Microphone Arrays","abstract":"In this paper, we present HOMULA-RIR, a dataset of room impulse responses (RIRs) acquired using both higher-order microphones (HOMs) and a uniform linear array (ULA), in order to model a remote attendance teleconferencing scenario. Specifically, measurements were performed in a seminar room, where a 64-microphone ULA was used as a multichannel audio acquisition system in the proximity of the speakers, while HOMs were used to model 25 attendees actually present in the seminar room. The HOMs cover a wide area of the room, making the dataset suitable also for applications of virtual acoustics. Through the measurement of the reverberation time and clarity index, and sample applications such as source localization and separation, we demonstrate the effectiveness of the HOMULA-RIR dataset.","sentences":["In this paper, we present HOMULA-RIR, a dataset of room impulse responses (RIRs) acquired using both higher-order microphones (HOMs) and a uniform linear array (ULA), in order to model a remote attendance teleconferencing scenario.","Specifically, measurements were performed in a seminar room, where a 64-microphone ULA was used as a multichannel audio acquisition system in the proximity of the speakers, while HOMs were used to model 25 attendees actually present in the seminar room.","The HOMs cover a wide area of the room, making the dataset suitable also for applications of virtual acoustics.","Through the measurement of the reverberation time and clarity index, and sample applications such as source localization and separation, we demonstrate the effectiveness of the HOMULA-RIR dataset."],"url":"http://arxiv.org/abs/2402.13896v1","category":"eess.AS"}
{"created":"2024-02-21 16:00:36","title":"di-Langlands correspondence and extended observables","abstract":"We explore the $\\textit{difference Langlands correspondence}$ using the four dimensional ${\\mathcal{N}}=2$ super-QCD. Surface defects and surface observables play the crucial role. As an application, we give the first construction of the full set of quantum integrals, i.e. commuting differential operators, such that the partition function of the so-called regular monodromy surface defect is their joint eigenvectors in an evaluation module over the Yangian $Y(\\mathfrak{gl}(2))$, making it the wavefunction of a $N$-site $\\mathfrak{gl}(2)$ spin chain with bi-infinite spin modules. We construct the $\\mathbf{Q}$- and $\\tilde{\\mathbf{Q}}$-surface observables which are believed to be the $Q$-operators on the bi-infinite module over the Yangian $Y(\\mathfrak{gl}(2))$, and compute their eigenvalues, the $Q$-functions, as vevs of the surface observables.","sentences":["We explore the $\\textit{difference Langlands correspondence}$ using the four dimensional ${\\mathcal{N}}=2$ super-QCD.","Surface defects and surface observables play the crucial role.","As an application, we give the first construction of the full set of quantum integrals, i.e. commuting differential operators, such that the partition function of the so-called regular monodromy surface defect is their joint eigenvectors in an evaluation module over the Yangian $Y(\\mathfrak{gl}(2))$, making it the wavefunction of a $N$-site $\\mathfrak{gl}(2)$ spin chain with bi-infinite spin modules.","We construct the $\\mathbf{Q}$- and $\\tilde{\\mathbf{Q}}$-surface observables which are believed to be the $Q$-operators on the bi-infinite module over the Yangian $Y(\\mathfrak{gl}(2))$, and compute their eigenvalues, the $Q$-functions, as vevs of the surface observables."],"url":"http://arxiv.org/abs/2402.13888v1","category":"hep-th"}
{"created":"2024-02-21 15:58:13","title":"Multigap superconductivity in lithium intercalated bilayer Mo$_2$C","abstract":"Interlayer coupling can significantly influence the physical properties of layered transition metal compounds. The superconductivity in layered Mo$_2$C systems, belonging to the emergent family of MXene, has garnered considerable attention. However, the impact of interlayer coupling on superconductivity, and the anisotropic superconducting properties in these systems are not yet clear. By performing first-principles calculations of electron-phonon coupling and anisotropic superconducting properties, we show that the interlayer coupling in bilayer 1$T$-Mo$_2$C suppresses superconductivity, resulting in a significant drop in superconducting transition temperature ($T_{\\mathrm{c}}$) from 4.2 $K$ in its monolayer form to nearly 0 $K$. By introducing lithium atoms into the interlayer space of the bilayer, the interlayer coupling can be effectively weakened, transforming the system into a two-gap superconductor with a $T_{\\mathrm{c}}$ above 10 $K$. A 3\\% tensile strain can further transform the system into a three-gap superconductor with a significantly enhanced $T_{\\mathrm{c}}$ of approximately 24.7 $K$, which is very high in the Mo$_2$C related systems. The enhancement of the superconductivity induced by the strain is mainly due to the downshift of an energy band with a flat dispersion to the energy near the Fermi level. The in-plane vibrations of Mo atoms and the $d$-orbital electrons of Mo atoms are most important for the formation of the superconductivity. Our method can also be applied to multilayer Mo$_2$C systems. Given the successful synthesis of layered Mo$_2$C systems and the experimental realization of alkaline metal atom depositions, our work presents a practically feasible strategy for achieving high $T_{\\mathrm{c}}$ and multigap superconductivity in layered Mo$_2$C.","sentences":["Interlayer coupling can significantly influence the physical properties of layered transition metal compounds.","The superconductivity in layered Mo$_2$C systems, belonging to the emergent family of MXene, has garnered considerable attention.","However, the impact of interlayer coupling on superconductivity, and the anisotropic superconducting properties in these systems are not yet clear.","By performing first-principles calculations of electron-phonon coupling and anisotropic superconducting properties, we show that the interlayer coupling in bilayer 1$T$-Mo$_2$C suppresses superconductivity, resulting in a significant drop in superconducting transition temperature ($T_{\\mathrm{c}}$) from 4.2 $K$ in its monolayer form to nearly 0 $K$. By introducing lithium atoms into the interlayer space of the bilayer, the interlayer coupling can be effectively weakened, transforming the system into a two-gap superconductor with a $T_{\\mathrm{c}}$ above 10 $K$. A 3\\% tensile strain can further transform the system into a three-gap superconductor with a significantly enhanced $T_{\\mathrm{c}}$ of approximately 24.7 $K$, which is very high in the Mo$_2$C related systems.","The enhancement of the superconductivity induced by the strain is mainly due to the downshift of an energy band with a flat dispersion to the energy near the Fermi level.","The in-plane vibrations of Mo atoms and the $d$-orbital electrons of Mo atoms are most important for the formation of the superconductivity.","Our method can also be applied to multilayer Mo$_2$C systems.","Given the successful synthesis of layered Mo$_2$C systems and the experimental realization of alkaline metal atom depositions, our work presents a practically feasible strategy for achieving high $T_{\\mathrm{c}}$ and multigap superconductivity in layered Mo$_2$C."],"url":"http://arxiv.org/abs/2402.13886v1","category":"cond-mat.supr-con"}
{"created":"2024-02-21 15:53:32","title":"Hyperon polarization measurements in heavy-ion collisions","abstract":"In non-central heavy-ion collisions, a large orbital angular momentum of the colliding system is produced, which is then partially transferred to the created medium, resulting in the particle polarization on average along the initial angular momentum, known as global polarization. It was predicted almost 20 years ago and the first observation of $\\Lambda$ global polarization was made by the STAR Collaboration in 2017. Since then, a lot of progress have been made in the polarization measurements including global polarization of multistrange hyperons and the polarization along the beam direction induced by azimuthal anisotropic flow. In these proceedings, we present recent experimental progress on the hyperon polarization measurements in heavy-ion collisions.","sentences":["In non-central heavy-ion collisions, a large orbital angular momentum of the colliding system is produced, which is then partially transferred to the created medium, resulting in the particle polarization on average along the initial angular momentum, known as global polarization.","It was predicted almost 20 years ago and the first observation of $\\Lambda$ global polarization was made by the STAR Collaboration in 2017.","Since then, a lot of progress have been made in the polarization measurements including global polarization of multistrange hyperons and the polarization along the beam direction induced by azimuthal anisotropic flow.","In these proceedings, we present recent experimental progress on the hyperon polarization measurements in heavy-ion collisions."],"url":"http://arxiv.org/abs/2402.13884v1","category":"nucl-ex"}
{"created":"2024-02-21 15:33:52","title":"Time-reversal in a dipolar quantum many-body spin system","abstract":"Time reversal in a macroscopic system is contradicting daily experience. It is practically impossible to restore a shattered cup to its original state by just time reversing the microscopic dynamics that led to its breakage. Yet, with the precise control capabilities provided by modern quantum technology, the unitary evolution of a quantum system can be reversed in time. Here, we implement a time-reversal protocol in a dipolar interacting, isolated many-body spin system represented by Rydberg states in an atomic gas. By changing the states encoding the spin, we flip the sign of the interaction Hamiltonian, and demonstrate the reversal of the relaxation dynamics of the magnetization by letting a demagnetized many-body state evolve back-in-time into a magnetized state. We elucidate the role of atomic motion using the concept of a Loschmidt echo. Finally, by combining the approach with Floquet engineering, we demonstrate time reversal for a large family of spin models with different symmetries. Our method of state transfer is applicable across a wide range of quantum simulation platforms and has applications far beyond quantum many-body physics, reaching from quantum-enhanced sensing to quantum information scrambling.","sentences":["Time reversal in a macroscopic system is contradicting daily experience.","It is practically impossible to restore a shattered cup to its original state by just time reversing the microscopic dynamics that led to its breakage.","Yet, with the precise control capabilities provided by modern quantum technology, the unitary evolution of a quantum system can be reversed in time.","Here, we implement a time-reversal protocol in a dipolar interacting, isolated many-body spin system represented by Rydberg states in an atomic gas.","By changing the states encoding the spin, we flip the sign of the interaction Hamiltonian, and demonstrate the reversal of the relaxation dynamics of the magnetization by letting a demagnetized many-body state evolve back-in-time into a magnetized state.","We elucidate the role of atomic motion using the concept of a Loschmidt echo.","Finally, by combining the approach with Floquet engineering, we demonstrate time reversal for a large family of spin models with different symmetries.","Our method of state transfer is applicable across a wide range of quantum simulation platforms and has applications far beyond quantum many-body physics, reaching from quantum-enhanced sensing to quantum information scrambling."],"url":"http://arxiv.org/abs/2402.13873v1","category":"quant-ph"}
{"created":"2024-02-21 15:32:23","title":"Analytical and numerical studies for integrable and non-integrable fractional discrete modified Korteweg-de Vries hierarchies","abstract":"Under investigation in this paper is the fractional integrable and non-integrable discrete modified Korteweg-de Vries hierarchies. The linear dispersion relations, completeness relations, inverse scattering transform, and fractional soliton solutions of the fractional integrable discrete modified Korteweg-de Vries hierarchy will be explored. The inverse scattering problem will be solved accurately by using Gel'fand-Levitan-Marchenko (GLM) equations and Riemann-Hilbert (RH) problem. The peak velocity of fractional soliton solutions will be analyzed. The numerical solutions of the non-integrable fractional averaged discrete modified Korteweg-de Vries equation which has a simpler form than the integrable one will be obtained by a split-step fourier method.","sentences":["Under investigation in this paper is the fractional integrable and non-integrable discrete modified Korteweg-de Vries hierarchies.","The linear dispersion relations, completeness relations, inverse scattering transform, and fractional soliton solutions of the fractional integrable discrete modified Korteweg-de Vries hierarchy will be explored.","The inverse scattering problem will be solved accurately by using Gel'fand-Levitan-Marchenko (GLM) equations and Riemann-Hilbert (RH) problem.","The peak velocity of fractional soliton solutions will be analyzed.","The numerical solutions of the non-integrable fractional averaged discrete modified Korteweg-de Vries equation which has a simpler form than the integrable one will be obtained by a split-step fourier method."],"url":"http://arxiv.org/abs/2402.13872v1","category":"nlin.SI"}
{"created":"2024-02-21 15:13:43","title":"Variable Projection Algorithms: Theoretical Insights and A Novel Approach for Problems with Large Residual","abstract":"This paper delves into an in-depth exploration of the Variable Projection (VP) algorithm, a powerful tool for solving separable nonlinear optimization problems across multiple domains, including system identification, image processing, and machine learning. We first establish a theoretical framework to examine the effect of the approximate treatment of the coupling relationship among parameters on the local convergence of the VP algorithm and theoretically prove that the Kaufman's VP algorithm can achieve a similar convergence rate as the Golub \\& Pereyra's form. These studies fill the gap in the existing convergence theory analysis, and provide a solid foundation for understanding the mechanism of VP algorithm and broadening its application horizons. Furthermore, drawing inspiration from these theoretical revelations, we design a refined VP algorithm for handling separable nonlinear optimization problems characterized by large residual, called VPLR, which boosts the convergence performance by addressing the interdependence of parameters within the separable model and by continually correcting the approximated Hessian matrix to counteract the influence of large residual during the iterative process. The effectiveness of this refined algorithm is corroborated through numerical experimentation.","sentences":["This paper delves into an in-depth exploration of the Variable Projection (VP) algorithm, a powerful tool for solving separable nonlinear optimization problems across multiple domains, including system identification, image processing, and machine learning.","We first establish a theoretical framework to examine the effect of the approximate treatment of the coupling relationship among parameters on the local convergence of the VP algorithm and theoretically prove that the Kaufman's VP algorithm can achieve a similar convergence rate as the Golub \\& Pereyra's form.","These studies fill the gap in the existing convergence theory analysis, and provide a solid foundation for understanding the mechanism of VP algorithm and broadening its application horizons.","Furthermore, drawing inspiration from these theoretical revelations, we design a refined VP algorithm for handling separable nonlinear optimization problems characterized by large residual, called VPLR, which boosts the convergence performance by addressing the interdependence of parameters within the separable model and by continually correcting the approximated Hessian matrix to counteract the influence of large residual during the iterative process.","The effectiveness of this refined algorithm is corroborated through numerical experimentation."],"url":"http://arxiv.org/abs/2402.13865v1","category":"math.OC"}
{"created":"2024-02-21 15:11:18","title":"Repulsive Casimir force from a Majorana zero-mode","abstract":"Fu and Kane have taught us that a Majorana zero-mode appears on the quantum spin Hall edge at the interface with a superconductor. If a magnetic scatterer is placed on the edge, the zero-point energy of massless edge excitations exerts a force on the scatterer. This is the fermionic analogue of the electromagnetic Casimir effect. We show that the Majorana zero-mode produces a repulsive Casimir force, pushing the scatterer away from the superconductor. Unlike some other signatures of Majorana zero-modes, the repulsive Casimir force is directly tied to the topological invariant of the system (the sign of the determinant of the reflection matrix from the superconductor).","sentences":["Fu and Kane have taught us that a Majorana zero-mode appears on the quantum spin Hall edge at the interface with a superconductor.","If a magnetic scatterer is placed on the edge, the zero-point energy of massless edge excitations exerts a force on the scatterer.","This is the fermionic analogue of the electromagnetic Casimir effect.","We show that the Majorana zero-mode produces a repulsive Casimir force, pushing the scatterer away from the superconductor.","Unlike some other signatures of Majorana zero-modes, the repulsive Casimir force is directly tied to the topological invariant of the system (the sign of the determinant of the reflection matrix from the superconductor)."],"url":"http://arxiv.org/abs/2402.13862v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-21 15:09:56","title":"The large-scale environment of 3CR radio galaxies at z<0.3","abstract":"The question of whether and how the properties of radio galaxies (RGs) are connected with the large-scale environment is still an open issue. For this work we measured the large-scale galaxies' density around RGs present in the revised Third Cambridge Catalog of radio sources (3CR) with 0.02 < z < 0.3. The goal is to determine whether the accretion mode and morphology of RGs are related to the richness of the environment. We considered RGs at 0.05 < z < 0.3 for a comparison between optical spectroscopic classes, and those within 0.02 < z < 0.1 to study the differences between the radio morphological types. Photometric data from the Panoramic Survey Telescope & Rapid Response System (Pan-STARRS) survey were used to search for \"red sequences\" within an area of 500 kpc of radius around each RG. We find that 1) RGs span over a large range of local galaxies' density, from isolated sources to those in rich environments, 2) the richness distributions of the various classes are not statistically different, and 3) the radio luminosity is not connected with the source environment. Our results suggest that the RG properties are independent of the local galaxies density, which is in agreement with some previous analyses, but contrasting with other studies. We discuss the possible origin of this discrepancy. An analysis of a larger sample is needed to put out results on a stronger statistical basis.","sentences":["The question of whether and how the properties of radio galaxies (RGs) are connected with the large-scale environment is still an open issue.","For this work we measured the large-scale galaxies' density around RGs present in the revised Third Cambridge Catalog of radio sources (3CR) with 0.02 < z < 0.3.","The goal is to determine whether the accretion mode and morphology of RGs are related to the richness of the environment.","We considered RGs at 0.05 < z < 0.3 for a comparison between optical spectroscopic classes, and those within 0.02 < z < 0.1 to study the differences between the radio morphological types.","Photometric data from the Panoramic Survey Telescope & Rapid Response System (Pan-STARRS) survey were used to search for \"red sequences\" within an area of 500 kpc of radius around each RG.","We find that 1) RGs span over a large range of local galaxies' density, from isolated sources to those in rich environments, 2) the richness distributions of the various classes are not statistically different, and 3) the radio luminosity is not connected with the source environment.","Our results suggest that the RG properties are independent of the local galaxies density, which is in agreement with some previous analyses, but contrasting with other studies.","We discuss the possible origin of this discrepancy.","An analysis of a larger sample is needed to put out results on a stronger statistical basis."],"url":"http://arxiv.org/abs/2402.13859v1","category":"astro-ph.GA"}
{"created":"2024-02-21 15:09:51","title":"Diversity-Aware $k$-Maximum Inner Product Search Revisited","abstract":"The $k$-Maximum Inner Product Search ($k$MIPS) serves as a foundational component in recommender systems and various data mining tasks. However, while most existing $k$MIPS approaches prioritize the efficient retrieval of highly relevant items for users, they often neglect an equally pivotal facet of search results: \\emph{diversity}. To bridge this gap, we revisit and refine the diversity-aware $k$MIPS (D$k$MIPS) problem by incorporating two well-known diversity objectives -- minimizing the average and maximum pairwise item similarities within the results -- into the original relevance objective. This enhancement, inspired by Maximal Marginal Relevance (MMR), offers users a controllable trade-off between relevance and diversity. We introduce \\textsc{Greedy} and \\textsc{DualGreedy}, two linear scan-based algorithms tailored for D$k$MIPS. They both achieve data-dependent approximations and, when aiming to minimize the average pairwise similarity, \\textsc{DualGreedy} attains an approximation ratio of $1/4$ with an additive term for regularization. To further improve query efficiency, we integrate a lightweight Ball-Cone Tree (BC-Tree) index with the two algorithms. Finally, comprehensive experiments on ten real-world data sets demonstrate the efficacy of our proposed methods, showcasing their capability to efficiently deliver diverse and relevant search results to users.","sentences":["The $k$-Maximum Inner Product Search ($k$MIPS) serves as a foundational component in recommender systems and various data mining tasks.","However, while most existing $k$MIPS approaches prioritize the efficient retrieval of highly relevant items for users, they often neglect an equally pivotal facet of search results: \\emph{diversity}.","To bridge this gap, we revisit and refine the diversity-aware $k$MIPS (D$k$MIPS) problem by incorporating two well-known diversity objectives -- minimizing the average and maximum pairwise item similarities within the results -- into the original relevance objective.","This enhancement, inspired by Maximal Marginal Relevance (MMR), offers users a controllable trade-off between relevance and diversity.","We introduce \\textsc{Greedy} and \\textsc{DualGreedy}, two linear scan-based algorithms tailored for D$k$MIPS.","They both achieve data-dependent approximations and, when aiming to minimize the average pairwise similarity, \\textsc{DualGreedy} attains an approximation ratio of $1/4$ with an additive term for regularization.","To further improve query efficiency, we integrate a lightweight Ball-Cone Tree (BC-Tree) index with the two algorithms.","Finally, comprehensive experiments on ten real-world data sets demonstrate the efficacy of our proposed methods, showcasing their capability to efficiently deliver diverse and relevant search results to users."],"url":"http://arxiv.org/abs/2402.13858v1","category":"cs.IR"}
{"created":"2024-02-21 15:06:51","title":"Replicable Learning of Large-Margin Halfspaces","abstract":"We provide efficient replicable algorithms for the problem of learning large-margin halfspaces. Our results improve upon the algorithms provided by Impagliazzo, Lei, Pitassi, and Sorrell [STOC, 2022]. We design the first dimension-independent replicable algorithms for this task which runs in polynomial time, is proper, and has strictly improved sample complexity compared to the one achieved by Impagliazzo et al. [2022] with respect to all the relevant parameters. Moreover, our first algorithm has sample complexity that is optimal with respect to the accuracy parameter $\\epsilon$. We also design an SGD-based replicable algorithm that, in some parameters' regimes, achieves better sample and time complexity than our first algorithm.   Departing from the requirement of polynomial time algorithms, using the DP-to-Replicability reduction of Bun, Gaboardi, Hopkins, Impagliazzo, Lei, Pitassi, Sorrell, and Sivakumar [STOC, 2023], we show how to obtain a replicable algorithm for large-margin halfspaces with improved sample complexity with respect to the margin parameter $\\tau$, but running time doubly exponential in $1/\\tau^2$ and worse sample complexity dependence on $\\epsilon$ than one of our previous algorithms. We then design an improved algorithm with better sample complexity than all three of our previous algorithms and running time exponential in $1/\\tau^{2}$.","sentences":["We provide efficient replicable algorithms for the problem of learning large-margin halfspaces.","Our results improve upon the algorithms provided by Impagliazzo, Lei, Pitassi, and Sorrell","[STOC, 2022].","We design the first dimension-independent replicable algorithms for this task which runs in polynomial time, is proper, and has strictly improved sample complexity compared to the one achieved by Impagliazzo et al.","[2022] with respect to all the relevant parameters.","Moreover, our first algorithm has sample complexity that is optimal with respect to the accuracy parameter $\\epsilon$. We also design an SGD-based replicable algorithm that, in some parameters' regimes, achieves better sample and time complexity than our first algorithm.   ","Departing from the requirement of polynomial time algorithms, using the DP-to-Replicability reduction of Bun, Gaboardi, Hopkins, Impagliazzo, Lei, Pitassi, Sorrell, and","Sivakumar","[STOC, 2023], we show how to obtain a replicable algorithm for large-margin halfspaces with improved sample complexity with respect to the margin parameter $\\tau$, but running time doubly exponential in $1/\\tau^2$ and worse sample complexity dependence on $\\epsilon$ than one of our previous algorithms.","We then design an improved algorithm with better sample complexity than all three of our previous algorithms and running time exponential in $1/\\tau^{2}$."],"url":"http://arxiv.org/abs/2402.13857v1","category":"cs.LG"}
{"created":"2024-02-21 14:59:46","title":"Gaussian beams and Calder\u00f3n problems for connections at large frequency","abstract":"We consider the inverse problem of recovering a connection on a complex vector bundle over a compact smooth Riemannian manifold with boundary from a Dirichlet-to-Neumann (DN) map at a high fixed frequency. We construct Gaussian beams using the language of jet bundles and show that their value at the boundary can be recovered from those DN maps. This allows us to show injectivity up to gauge on manifolds whose non-abelian X-ray transform is injective. We also study DN maps with a cubic nonlinearity and show how to recover the broken non-abelian X-ray transform from them. This transform maps a connection to its parallel transport along broken geodesics with endpoints on the boundary. We show that the broken non-abelian X-ray transform is always injective up to gauge equivalence, regardless of the manifold's geometry.","sentences":["We consider the inverse problem of recovering a connection on a complex vector bundle over a compact smooth Riemannian manifold with boundary from a Dirichlet-to-Neumann (DN) map at a high fixed frequency.","We construct Gaussian beams using the language of jet bundles and show that their value at the boundary can be recovered from those DN maps.","This allows us to show injectivity up to gauge on manifolds whose non-abelian X-ray transform is injective.","We also study DN maps with a cubic nonlinearity and show how to recover the broken non-abelian X-ray transform from them.","This transform maps a connection to its parallel transport along broken geodesics with endpoints on the boundary.","We show that the broken non-abelian X-ray transform is always injective up to gauge equivalence, regardless of the manifold's geometry."],"url":"http://arxiv.org/abs/2402.13854v1","category":"math.AP"}
{"created":"2024-02-21 14:50:44","title":"Partially hyperbolic diffeomorphisms that are center fixing","abstract":"We show that every transitive dynamically coherent partially hyperbolic diffeomorphism with a one-dimensional center foliation $\\W^c$ satisfying that $f(W)=W$ for every leaf $W\\in \\W^c$ is a discretized Anosov flow.","sentences":["We show that every transitive dynamically coherent partially hyperbolic diffeomorphism with a one-dimensional center foliation $\\W^c$ satisfying that $f(W)=W$ for every leaf $W\\in \\W^c$ is a discretized Anosov flow."],"url":"http://arxiv.org/abs/2402.13849v1","category":"math.DS"}
{"created":"2024-02-21 14:42:23","title":"Investigation of QGP-like properties via identified particles production in oxygen-oxygen collisions at \\sqrt{s_{\\mathrm{NN}}} from EPOS4","abstract":"The Large Hadron Collider (LHC) aims to inject oxygen (${}^{16}O$) ions in next run into its experiments. This include anticipated one-day physics run focusing on \\oo collisions at center-of-mass energy \\sevenn. In this study, we present the production of identified particles ($\\pi^\\pm$, $K^\\pm$ and $p(\\overline{p})$) as well as the bulk-properties of the medium produced in \\oo collisions at \\sqrt{s_{\\mathrm{NN}}} from EPOS4 simulations. We report the transverse momentum ({\\ppt}) spectra, integrated yield (\\dNdy), charged particle multiplicity (\\dndeta), mean transverse momentum (\\mpt), \\ppt differential particle ratios ($K/\\pi$ and $p/\\pi$) for different centrality classes. We found that EPOS4 simulations reasonably reproduce the shapes and distributions of all observables. The freeze-out parameters have been extracted by fitting the \\ppt spectra of identified particles simultaneously by the Boltzmann Gibbs Blast Wave model. We found the presence of significant radial flow (\\mbeta) and comparable kinetic freeze-out temperature (\\tkin) with \\pbpb collisions at LHC energies. Additionally, a strong anti-correlation is found between \\tkin and \\mbeta from central to peripheral collisions. We also compare the freeze-out parameters from EPOS4 with AMPT and it is observed that the flow velocity \\mbeta from EPOS4 is relatively higher compared to that of AMPT. Furthermore, the foreseen data from \\oo collisions at the LHC, when available, will help to better understand the heavy-ion-like behaviour in small systems. This is due to final state multiplicity overlap in \\oo collisions with \\pp, \\ppb and \\pbpb collisions, and it may also help to put possible constraints on the model parameters .","sentences":["The Large Hadron Collider (LHC) aims to inject oxygen (${}^{16}O$) ions in next run into its experiments.","This include anticipated one-day physics run focusing on \\oo collisions at center-of-mass energy \\sevenn.","In this study, we present the production of identified particles ($\\pi^\\pm$, $K^\\pm$ and $p(\\overline{p})$) as well as the bulk-properties of the medium produced in \\oo collisions at \\sqrt{s_{\\mathrm{NN}}} from EPOS4 simulations.","We report the transverse momentum ({\\ppt}) spectra, integrated yield (\\dNdy), charged particle multiplicity (\\dndeta), mean transverse momentum (\\mpt), \\ppt differential particle ratios ($K/\\pi$ and $p/\\pi$) for different centrality classes.","We found that EPOS4 simulations reasonably reproduce the shapes and distributions of all observables.","The freeze-out parameters have been extracted by fitting the \\ppt spectra of identified particles simultaneously by the Boltzmann Gibbs Blast Wave model.","We found the presence of significant radial flow (\\mbeta) and comparable kinetic freeze-out temperature (\\tkin) with \\pbpb collisions at LHC energies.","Additionally, a strong anti-correlation is found between \\tkin and \\mbeta from central to peripheral collisions.","We also compare the freeze-out parameters from EPOS4 with AMPT and it is observed that the flow velocity \\mbeta from EPOS4 is relatively higher compared to that of AMPT.","Furthermore, the foreseen data from \\oo collisions at the LHC, when available, will help to better understand the heavy-ion-like behaviour in small systems.","This is due to final state multiplicity overlap in \\oo collisions with \\pp, \\ppb and \\pbpb collisions, and it may also help to put possible constraints on the model parameters ."],"url":"http://arxiv.org/abs/2402.13843v1","category":"hep-ph"}
{"created":"2024-02-21 14:41:05","title":"Cold isospin asymmetric baryonic rich matter in nonlocal NJL-like models","abstract":"We study the features of low energy strong interactions for a system at zero temperature and finite baryon and isospin chemical potentials, in the framework of a Nambu--Jona-Lasinio-like model that includes nonlocal four-point interactions. We analyze the phase transitions corresponding to chiral symmetry restoration and pion condensation, comparing our results with those obtained from local NJL-like models and lattice QCD calculations.","sentences":["We study the features of low energy strong interactions for a system at zero temperature and finite baryon and isospin chemical potentials, in the framework of a Nambu--Jona-Lasinio-like model that includes nonlocal four-point interactions.","We analyze the phase transitions corresponding to chiral symmetry restoration and pion condensation, comparing our results with those obtained from local NJL-like models and lattice QCD calculations."],"url":"http://arxiv.org/abs/2402.13842v1","category":"hep-ph"}
{"created":"2024-02-21 14:22:20","title":"MLXP: A framework for conducting replicable Machine Learning eXperiments in Python","abstract":"Replicability in machine learning (ML) research is increasingly concerning due to the utilization of complex non-deterministic algorithms and the dependence on numerous hyper-parameter choices, such as model architecture and training datasets. Ensuring reproducible and replicable results is crucial for advancing the field, yet often requires significant technical effort to conduct systematic and well-organized experiments that yield robust conclusions. Several tools have been developed to facilitate experiment management and enhance reproducibility; however, they often introduce complexity that hinders adoption within the research community, despite being well-handled in industrial settings. To address the challenge of low adoption, we propose MLXP, an open-source, simple, and lightweight experiment management tool based on Python, available at https://github.com/inria-thoth/mlxp . MLXP streamlines the experimental process with minimal practitioner overhead while ensuring a high level of reproducibility.","sentences":["Replicability in machine learning (ML) research is increasingly concerning due to the utilization of complex non-deterministic algorithms and the dependence on numerous hyper-parameter choices, such as model architecture and training datasets.","Ensuring reproducible and replicable results is crucial for advancing the field, yet often requires significant technical effort to conduct systematic and well-organized experiments that yield robust conclusions.","Several tools have been developed to facilitate experiment management and enhance reproducibility; however, they often introduce complexity that hinders adoption within the research community, despite being well-handled in industrial settings.","To address the challenge of low adoption, we propose MLXP, an open-source, simple, and lightweight experiment management tool based on Python, available at https://github.com/inria-thoth/mlxp .","MLXP streamlines the experimental process with minimal practitioner overhead while ensuring a high level of reproducibility."],"url":"http://arxiv.org/abs/2402.13831v1","category":"cs.LG"}
{"created":"2024-02-21 13:55:53","title":"Khronos: A Unified Approach for Spatio-Temporal Metric-Semantic SLAM in Dynamic Environments","abstract":"Perceiving and understanding highly dynamic and changing environments is a crucial capability for robot autonomy. While large strides have been made towards developing dynamic SLAM approaches that estimate the robot pose accurately, a lesser emphasis has been put on the construction of dense spatio-temporal representations of the robot environment. A detailed understanding of the scene and its evolution through time is crucial for long-term robot autonomy and essential to tasks that require long-term reasoning, such as operating effectively in environments shared with humans and other agents and thus are subject to short and long-term dynamics. To address this challenge, this work defines the Spatio-temporal Metric-semantic SLAM (SMS) problem, and presents a framework to factorize and solve it efficiently. We show that the proposed factorization suggests a natural organization of a spatio-temporal perception system, where a fast process tracks short-term dynamics in an active temporal window, while a slower process reasons over long-term changes in the environment using a factor graph formulation. We provide an efficient implementation of the proposed spatio-temporal perception approach, that we call Khronos, and show that it unifies exiting interpretations of short-term and long-term dynamics and is able to construct a dense spatio-temporal map in real-time. We provide simulated and real results, showing that the spatio-temporal maps built by Khronos are an accurate reflection of a 3D scene over time and that Khronos outperforms baselines across multiple metrics. We further validate our approach on two heterogeneous robots in challenging, large-scale real-world environments.","sentences":["Perceiving and understanding highly dynamic and changing environments is a crucial capability for robot autonomy.","While large strides have been made towards developing dynamic SLAM approaches that estimate the robot pose accurately, a lesser emphasis has been put on the construction of dense spatio-temporal representations of the robot environment.","A detailed understanding of the scene and its evolution through time is crucial for long-term robot autonomy and essential to tasks that require long-term reasoning, such as operating effectively in environments shared with humans and other agents and thus are subject to short and long-term dynamics.","To address this challenge, this work defines the Spatio-temporal Metric-semantic SLAM (SMS) problem, and presents a framework to factorize and solve it efficiently.","We show that the proposed factorization suggests a natural organization of a spatio-temporal perception system, where a fast process tracks short-term dynamics in an active temporal window, while a slower process reasons over long-term changes in the environment using a factor graph formulation.","We provide an efficient implementation of the proposed spatio-temporal perception approach, that we call Khronos, and show that it unifies exiting interpretations of short-term and long-term dynamics and is able to construct a dense spatio-temporal map in real-time.","We provide simulated and real results, showing that the spatio-temporal maps built by Khronos are an accurate reflection of a 3D scene over time and that Khronos outperforms baselines across multiple metrics.","We further validate our approach on two heterogeneous robots in challenging, large-scale real-world environments."],"url":"http://arxiv.org/abs/2402.13817v1","category":"cs.RO"}
{"created":"2024-02-21 13:50:46","title":"Voice-Driven Mortality Prediction in Hospitalized Heart Failure Patients: A Machine Learning Approach Enhanced with Diagnostic Biomarkers","abstract":"Addressing heart failure (HF) as a prevalent global health concern poses difficulties in implementing innovative approaches for enhanced patient care. Predicting mortality rates in HF patients, in particular, is difficult yet critical, necessitating individualized care, proactive management, and enabling educated decision-making to enhance outcomes. Recently, the significance of voice biomarkers coupled with Machine Learning (ML) has surged, demonstrating remarkable efficacy, particularly in predicting heart failure. The synergy of voice analysis and ML algorithms provides a non-invasive and easily accessible means to evaluate patients' health. However, there is a lack of voice biomarkers for predicting mortality rates among heart failure patients with standardized speech protocols. Here, we demonstrate a powerful and effective ML model for predicting mortality rates in hospitalized HF patients through the utilization of voice biomarkers. By seamlessly integrating voice biomarkers into routine patient monitoring, this strategy has the potential to improve patient outcomes, optimize resource allocation, and advance patient-centered HF management. In this study, a Machine Learning system, specifically a logistic regression model, is trained to predict patients' 5-year mortality rates using their speech as input. The model performs admirably and consistently, as demonstrated by cross-validation and statistical approaches (p-value < 0.001). Furthermore, integrating NT-proBNP, a diagnostic biomarker in HF, improves the model's predictive accuracy substantially.","sentences":["Addressing heart failure (HF) as a prevalent global health concern poses difficulties in implementing innovative approaches for enhanced patient care.","Predicting mortality rates in HF patients, in particular, is difficult yet critical, necessitating individualized care, proactive management, and enabling educated decision-making to enhance outcomes.","Recently, the significance of voice biomarkers coupled with Machine Learning (ML) has surged, demonstrating remarkable efficacy, particularly in predicting heart failure.","The synergy of voice analysis and ML algorithms provides a non-invasive and easily accessible means to evaluate patients' health.","However, there is a lack of voice biomarkers for predicting mortality rates among heart failure patients with standardized speech protocols.","Here, we demonstrate a powerful and effective ML model for predicting mortality rates in hospitalized HF patients through the utilization of voice biomarkers.","By seamlessly integrating voice biomarkers into routine patient monitoring, this strategy has the potential to improve patient outcomes, optimize resource allocation, and advance patient-centered HF management.","In this study, a Machine Learning system, specifically a logistic regression model, is trained to predict patients' 5-year mortality rates using their speech as input.","The model performs admirably and consistently, as demonstrated by cross-validation and statistical approaches (p-value < 0.001).","Furthermore, integrating NT-proBNP, a diagnostic biomarker in HF, improves the model's predictive accuracy substantially."],"url":"http://arxiv.org/abs/2402.13812v1","category":"cs.LG"}
{"created":"2024-02-21 13:49:15","title":"Robustness of diabatic enhancement in quantum annealing","abstract":"In adiabatic quantum annealing, the speed with which an anneal can be run, while still achieving a high final ground state fidelity, is dictated by the size of the minimum gap that appears between the ground and first excited state in the annealing spectrum. To avoid the exponential slowdown associated with exponentially closing gaps, diabatic transitions to higher energy levels may be exploited in such a way that the system returns to the ground state before the end of the anneal. In certain cases, this is facilitated by the original annealing spectrum. However, there are also examples where careful manipulation of the annealing Hamiltonian has been used to alter the spectrum to create a diabatic path to the ground state. Since diabatic transitions depend on the evolution rate and the gap sizes in the spectrum, it is important to consider the sensitivity of any potential enhancement to changes in the anneal time as well as any parameters involved in the manipulation of the spectrum. We explore this sensitivity using annealing spectra containing an exponentially closing gap and an additional, tuneable, small gap created by a catalyst. We find that there is a trade-off between the precision needed in the catalyst strength and the anneal time in order to maintain the enhancement to the final ground state fidelity.","sentences":["In adiabatic quantum annealing, the speed with which an anneal can be run, while still achieving a high final ground state fidelity, is dictated by the size of the minimum gap that appears between the ground and first excited state in the annealing spectrum.","To avoid the exponential slowdown associated with exponentially closing gaps, diabatic transitions to higher energy levels may be exploited in such a way that the system returns to the ground state before the end of the anneal.","In certain cases, this is facilitated by the original annealing spectrum.","However, there are also examples where careful manipulation of the annealing Hamiltonian has been used to alter the spectrum to create a diabatic path to the ground state.","Since diabatic transitions depend on the evolution rate and the gap sizes in the spectrum, it is important to consider the sensitivity of any potential enhancement to changes in the anneal time as well as any parameters involved in the manipulation of the spectrum.","We explore this sensitivity using annealing spectra containing an exponentially closing gap and an additional, tuneable, small gap created by a catalyst.","We find that there is a trade-off between the precision needed in the catalyst strength and the anneal time in order to maintain the enhancement to the final ground state fidelity."],"url":"http://arxiv.org/abs/2402.13811v1","category":"quant-ph"}
{"created":"2024-02-21 13:47:51","title":"The Expected Loss of Preconditioned Langevin Dynamics Reveals the Hessian Rank","abstract":"Langevin dynamics (LD) is widely used for sampling from distributions and for optimization. In this work, we derive a closed-form expression for the expected loss of preconditioned LD near stationary points of the objective function. We use the fact that at the vicinity of such points, LD reduces to an Ornstein-Uhlenbeck process, which is amenable to convenient mathematical treatment. Our analysis reveals that when the preconditioning matrix satisfies a particular relation with respect to the noise covariance, LD's expected loss becomes proportional to the rank of the objective's Hessian. We illustrate the applicability of this result in the context of neural networks, where the Hessian rank has been shown to capture the complexity of the predictor function but is usually computationally hard to probe. Finally, we use our analysis to compare SGD-like and Adam-like preconditioners and identify the regimes under which each of them leads to a lower expected loss.","sentences":["Langevin dynamics (LD) is widely used for sampling from distributions and for optimization.","In this work, we derive a closed-form expression for the expected loss of preconditioned LD near stationary points of the objective function.","We use the fact that at the vicinity of such points, LD reduces to an Ornstein-Uhlenbeck process, which is amenable to convenient mathematical treatment.","Our analysis reveals that when the preconditioning matrix satisfies a particular relation with respect to the noise covariance, LD's expected loss becomes proportional to the rank of the objective's Hessian.","We illustrate the applicability of this result in the context of neural networks, where the Hessian rank has been shown to capture the complexity of the predictor function but is usually computationally hard to probe.","Finally, we use our analysis to compare SGD-like and Adam-like preconditioners and identify the regimes under which each of them leads to a lower expected loss."],"url":"http://arxiv.org/abs/2402.13810v1","category":"cs.LG"}
{"created":"2024-02-21 13:27:14","title":"AFPR-CIM: An Analog-Domain Floating-Point RRAM-based Compute-In-Memory Architecture with Dynamic Range Adaptive FP-ADC","abstract":"Power consumption has become the major concern in neural network accelerators for edge devices. The novel non-volatile-memory (NVM) based computing-in-memory (CIM) architecture has shown great potential for better energy efficiency. However, most of the recent NVM-CIM solutions mainly focus on fixed-point calculation and are not applicable to floating-point (FP) processing. In this paper, we propose an analog-domain floating-point CIM architecture (AFPR-CIM) based on resistive random-access memory (RRAM). A novel adaptive dynamic-range FP-ADC is designed to convert the analog computation results into FP codes. Output current with high dynamic range is converted to a normalized voltage range for readout, to prevent precision loss at low power consumption. Moreover, a novel FP-DAC is also implemented which reconstructs FP digital codes into analog values to perform analog computation. The proposed AFPR-CIM architecture enables neural network acceleration with FP8 (E2M5) activation for better accuracy and energy efficiency. Evaluation results show that AFPR-CIM can achieve 19.89 TFLOPS/W energy efficiency and 1474.56 GOPS throughput. Compared to traditional FP8 accelerator, digital FP-CIM, and analog INT8-CIM, this work achieves 4.135x, 5.376x, and 2.841x energy efficiency enhancement, respectively.","sentences":["Power consumption has become the major concern in neural network accelerators for edge devices.","The novel non-volatile-memory (NVM) based computing-in-memory (CIM) architecture has shown great potential for better energy efficiency.","However, most of the recent NVM-CIM solutions mainly focus on fixed-point calculation and are not applicable to floating-point (FP) processing.","In this paper, we propose an analog-domain floating-point CIM architecture (AFPR-CIM) based on resistive random-access memory (RRAM).","A novel adaptive dynamic-range FP-ADC is designed to convert the analog computation results into FP codes.","Output current with high dynamic range is converted to a normalized voltage range for readout, to prevent precision loss at low power consumption.","Moreover, a novel FP-DAC is also implemented which reconstructs FP digital codes into analog values to perform analog computation.","The proposed AFPR-CIM architecture enables neural network acceleration with FP8 (E2M5) activation for better accuracy and energy efficiency.","Evaluation results show that AFPR-CIM can achieve 19.89 TFLOPS/W energy efficiency and 1474.56 GOPS throughput.","Compared to traditional FP8 accelerator, digital FP-CIM, and analog INT8-CIM, this work achieves 4.135x, 5.376x, and 2.841x energy efficiency enhancement, respectively."],"url":"http://arxiv.org/abs/2402.13798v1","category":"eess.SY"}
{"created":"2024-02-21 13:26:00","title":"Scalable Methods for Brick Kiln Detection and Compliance Monitoring from Satellite Imagery: A Deployment Case Study in India","abstract":"Air pollution kills 7 million people annually. Brick manufacturing industry is the second largest consumer of coal contributing to 8%-14% of air pollution in Indo-Gangetic plain (highly populated tract of land in the Indian subcontinent). As brick kilns are an unorganized sector and present in large numbers, detecting policy violations such as distance from habitat is non-trivial. Air quality and other domain experts rely on manual human annotation to maintain brick kiln inventory. Previous work used computer vision based machine learning methods to detect brick kilns from satellite imagery but they are limited to certain geographies and labeling the data is laborious. In this paper, we propose a framework to deploy a scalable brick kiln detection system for large countries such as India and identify 7477 new brick kilns from 28 districts in 5 states in the Indo-Gangetic plain. We then showcase efficient ways to check policy violations such as high spatial density of kilns and abnormal increase over time in a region. We show that 90% of brick kilns in Delhi-NCR violate a density-based policy. Our framework can be directly adopted by the governments across the world to automate the policy regulations around brick kilns.","sentences":["Air pollution kills 7 million people annually.","Brick manufacturing industry is the second largest consumer of coal contributing to 8%-14% of air pollution in Indo-Gangetic plain (highly populated tract of land in the Indian subcontinent).","As brick kilns are an unorganized sector and present in large numbers, detecting policy violations such as distance from habitat is non-trivial.","Air quality and other domain experts rely on manual human annotation to maintain brick kiln inventory.","Previous work used computer vision based machine learning methods to detect brick kilns from satellite imagery but they are limited to certain geographies and labeling the data is laborious.","In this paper, we propose a framework to deploy a scalable brick kiln detection system for large countries such as India and identify 7477 new brick kilns from 28 districts in 5 states in the Indo-Gangetic plain.","We then showcase efficient ways to check policy violations such as high spatial density of kilns and abnormal increase over time in a region.","We show that 90% of brick kilns in Delhi-NCR violate a density-based policy.","Our framework can be directly adopted by the governments across the world to automate the policy regulations around brick kilns."],"url":"http://arxiv.org/abs/2402.13796v1","category":"cs.CV"}
{"created":"2024-02-21 13:21:37","title":"Trustworthy Distributed Certification of Program Execution","abstract":"Verifying the execution of a program is complicated and often limited by the inability to validate the code's correctness. It is a crucial aspect of scientific research, where it is needed to ensure the reproducibility and validity of experimental results. Similarly, in customer software testing, it is difficult for customers to verify that their specific program version was tested or executed at all. Existing state-of-the-art solutions, such as hardware-based approaches, constraint solvers, and verifiable computation systems, do not provide definitive proof of execution, which hinders reliable testing and analysis of program results. In this paper, we propose an innovative approach that combines a prototype programming language called Mona with a certification protocol OCCP to enable the distributed and decentralized re-execution of program segments. Our protocol allows for certification of program segments in a distributed, immutable, and trustworthy system without the need for naive re-execution, resulting in significant improvements in terms of time and computational resources used. We also explore the use of blockchain technology to manage the protocol workflow following other approaches in this space. Our approach offers a promising solution to the challenges of program execution verification and opens up opportunities for further research and development in this area. Our findings demonstrate the efficiency of our approach in reducing the number of program executions compared to existing state-of-the-art methods, thus improving the efficiency of certifying program executions.","sentences":["Verifying the execution of a program is complicated and often limited by the inability to validate the code's correctness.","It is a crucial aspect of scientific research, where it is needed to ensure the reproducibility and validity of experimental results.","Similarly, in customer software testing, it is difficult for customers to verify that their specific program version was tested or executed at all.","Existing state-of-the-art solutions, such as hardware-based approaches, constraint solvers, and verifiable computation systems, do not provide definitive proof of execution, which hinders reliable testing and analysis of program results.","In this paper, we propose an innovative approach that combines a prototype programming language called Mona with a certification protocol OCCP to enable the distributed and decentralized re-execution of program segments.","Our protocol allows for certification of program segments in a distributed, immutable, and trustworthy system without the need for naive re-execution, resulting in significant improvements in terms of time and computational resources used.","We also explore the use of blockchain technology to manage the protocol workflow following other approaches in this space.","Our approach offers a promising solution to the challenges of program execution verification and opens up opportunities for further research and development in this area.","Our findings demonstrate the efficiency of our approach in reducing the number of program executions compared to existing state-of-the-art methods, thus improving the efficiency of certifying program executions."],"url":"http://arxiv.org/abs/2402.13792v1","category":"cs.SE"}
{"created":"2024-02-21 13:19:07","title":"Nonlocal-to-Local Convergence for a Cahn-Hilliard Tumor Growth Model","abstract":"We consider a local Cahn-Hilliard-type model for tumor growth as well as a nonlocal model where, compared to the local system, the Laplacian in the equation for the chemical potential is replaced by a nonlocal operator. The latter is defined as a convolution integral with suitable kernels parametrized by a small parameter. For sufficiently smooth bounded domains in three dimensions, we prove convergence of weak solutions of the nonlocal model towards strong solutions of the local model together with convergence rates with respect to the small parameter. The proof is done via a Gronwall-type argument and a convergence result with rates for the nonlocal integral operator towards the Laplacian due to Abels, Hurm arXiv:2307.02264.","sentences":["We consider a local Cahn-Hilliard-type model for tumor growth as well as a nonlocal model where, compared to the local system, the Laplacian in the equation for the chemical potential is replaced by a nonlocal operator.","The latter is defined as a convolution integral with suitable kernels parametrized by a small parameter.","For sufficiently smooth bounded domains in three dimensions, we prove convergence of weak solutions of the nonlocal model towards strong solutions of the local model together with convergence rates with respect to the small parameter.","The proof is done via a Gronwall-type argument and a convergence result with rates for the nonlocal integral operator towards the Laplacian due to Abels, Hurm arXiv:2307.02264."],"url":"http://arxiv.org/abs/2402.13790v1","category":"math.AP"}
{"created":"2024-02-21 13:00:44","title":"Preserving Near-Optimal Gradient Sparsification Cost for Scalable Distributed Deep Learning","abstract":"Communication overhead is a major obstacle to scaling distributed training systems. Gradient sparsification is a potential optimization approach to reduce the communication volume without significant loss of model fidelity. However, existing gradient sparsification methods have low scalability owing to inefficient design of their algorithms, which raises the communication overhead significantly. In particular, gradient build-up and inadequate sparsity control methods degrade the sparsification performance considerably. Moreover, communication traffic increases drastically owing to workload imbalance of gradient selection between workers.   To address these challenges, we propose a novel gradient sparsification scheme called ExDyna. In ExDyna, the gradient tensor of the model comprises fined-grained blocks, and contiguous blocks are grouped into non-overlapping partitions. Each worker selects gradients in its exclusively allocated partition so that gradient build-up never occurs. To balance the workload of gradient selection between workers, ExDyna adjusts the topology of partitions by comparing the workloads of adjacent partitions. In addition, ExDyna supports online threshold scaling, which estimates the accurate threshold of gradient selection on-the-fly. Accordingly, ExDyna can satisfy the user-required sparsity level during a training period regardless of models and datasets. Therefore, ExDyna can enhance the scalability of distributed training systems by preserving near-optimal gradient sparsification cost. In experiments, ExDyna outperformed state-of-the-art sparsifiers in terms of training speed and sparsification performance while achieving high accuracy.","sentences":["Communication overhead is a major obstacle to scaling distributed training systems.","Gradient sparsification is a potential optimization approach to reduce the communication volume without significant loss of model fidelity.","However, existing gradient sparsification methods have low scalability owing to inefficient design of their algorithms, which raises the communication overhead significantly.","In particular, gradient build-up and inadequate sparsity control methods degrade the sparsification performance considerably.","Moreover, communication traffic increases drastically owing to workload imbalance of gradient selection between workers.   ","To address these challenges, we propose a novel gradient sparsification scheme called ExDyna.","In ExDyna, the gradient tensor of the model comprises fined-grained blocks, and contiguous blocks are grouped into non-overlapping partitions.","Each worker selects gradients in its exclusively allocated partition so that gradient build-up never occurs.","To balance the workload of gradient selection between workers, ExDyna adjusts the topology of partitions by comparing the workloads of adjacent partitions.","In addition, ExDyna supports online threshold scaling, which estimates the accurate threshold of gradient selection on-the-fly.","Accordingly, ExDyna can satisfy the user-required sparsity level during a training period regardless of models and datasets.","Therefore, ExDyna can enhance the scalability of distributed training systems by preserving near-optimal gradient sparsification cost.","In experiments, ExDyna outperformed state-of-the-art sparsifiers in terms of training speed and sparsification performance while achieving high accuracy."],"url":"http://arxiv.org/abs/2402.13781v1","category":"cs.LG"}
{"created":"2024-02-21 12:50:15","title":"Parameter identification algorithm for a LTV system with partially unknown state matrix","abstract":"In this paper an adaptive state observer and parameter identification algorithm for a linear time-varying system are developed under condition that the state matrix of the system contains unknown time-varying parameters of a known form. The state vector is observed using only output and input measurements without identification of the unknown parameters. When the state vector estimate is obtained, the identification algorithm is applied to find unknown parameters of the system.","sentences":["In this paper an adaptive state observer and parameter identification algorithm for a linear time-varying system are developed under condition that the state matrix of the system contains unknown time-varying parameters of a known form.","The state vector is observed using only output and input measurements without identification of the unknown parameters.","When the state vector estimate is obtained, the identification algorithm is applied to find unknown parameters of the system."],"url":"http://arxiv.org/abs/2402.13772v1","category":"eess.SY"}
{"created":"2024-02-21 12:43:41","title":"Democratizing Uncertainty Quantification","abstract":"Uncertainty Quantification (UQ) is vital to safety-critical model-based analyses, but the widespread adoption of sophisticated UQ methods is limited by technical complexity. In this paper, we introduce UM-Bridge (the UQ and Modeling Bridge), a high-level abstraction and software protocol that facilitates universal interoperability of UQ software with simulation codes. It breaks down the technical complexity of advanced UQ applications and enables separation of concerns between experts. UM-Bridge democratizes UQ by allowing effective interdisciplinary collaboration, accelerating the development of advanced UQ methods, and making it easy to perform UQ analyses from prototype to High Performance Computing (HPC) scale.   In addition, we present a library of ready-to-run UQ benchmark problems, all easily accessible through UM-Bridge. These benchmarks support UQ methodology research, enabling reproducible performance comparisons. We demonstrate UM-Bridge with several scientific applications, harnessing HPC resources even using UQ codes not designed with HPC support.","sentences":["Uncertainty Quantification (UQ) is vital to safety-critical model-based analyses, but the widespread adoption of sophisticated UQ methods is limited by technical complexity.","In this paper, we introduce UM-Bridge (the UQ and Modeling Bridge), a high-level abstraction and software protocol that facilitates universal interoperability of UQ software with simulation codes.","It breaks down the technical complexity of advanced UQ applications and enables separation of concerns between experts.","UM-Bridge democratizes UQ by allowing effective interdisciplinary collaboration, accelerating the development of advanced UQ methods, and making it easy to perform UQ analyses from prototype to High Performance Computing (HPC) scale.   ","In addition, we present a library of ready-to-run UQ benchmark problems, all easily accessible through UM-Bridge.","These benchmarks support UQ methodology research, enabling reproducible performance comparisons.","We demonstrate UM-Bridge with several scientific applications, harnessing HPC resources even using UQ codes not designed with HPC support."],"url":"http://arxiv.org/abs/2402.13768v1","category":"cs.MS"}
{"created":"2024-02-21 12:38:48","title":"Music Style Transfer with Time-Varying Inversion of Diffusion Models","abstract":"With the development of diffusion models, text-guided image style transfer has demonstrated high-quality controllable synthesis results. However, the utilization of text for diverse music style transfer poses significant challenges, primarily due to the limited availability of matched audio-text datasets. Music, being an abstract and complex art form, exhibits variations and intricacies even within the same genre, thereby making accurate textual descriptions challenging. This paper presents a music style transfer approach that effectively captures musical attributes using minimal data. We introduce a novel time-varying textual inversion module to precisely capture mel-spectrogram features at different levels. During inference, we propose a bias-reduced stylization technique to obtain stable results. Experimental results demonstrate that our method can transfer the style of specific instruments, as well as incorporate natural sounds to compose melodies. Samples and source code are available at https://lsfhuihuiff.github.io/MusicTI/.","sentences":["With the development of diffusion models, text-guided image style transfer has demonstrated high-quality controllable synthesis results.","However, the utilization of text for diverse music style transfer poses significant challenges, primarily due to the limited availability of matched audio-text datasets.","Music, being an abstract and complex art form, exhibits variations and intricacies even within the same genre, thereby making accurate textual descriptions challenging.","This paper presents a music style transfer approach that effectively captures musical attributes using minimal data.","We introduce a novel time-varying textual inversion module to precisely capture mel-spectrogram features at different levels.","During inference, we propose a bias-reduced stylization technique to obtain stable results.","Experimental results demonstrate that our method can transfer the style of specific instruments, as well as incorporate natural sounds to compose melodies.","Samples and source code are available at https://lsfhuihuiff.github.io/MusicTI/."],"url":"http://arxiv.org/abs/2402.13763v1","category":"cs.SD"}
{"created":"2024-02-21 12:34:31","title":"High-throughput Visual Nano-drone to Nano-drone Relative Localization using Onboard Fully Convolutional Networks","abstract":"Relative drone-to-drone localization is a fundamental building block for any swarm operations. We address this task in the context of miniaturized nano-drones, i.e., 10cm in diameter, which show an ever-growing interest due to novel use cases enabled by their reduced form factor. The price for their versatility comes with limited onboard resources, i.e., sensors, processing units, and memory, which limits the complexity of the onboard algorithms. A traditional solution to overcome these limitations is represented by lightweight deep learning models directly deployed aboard nano-drones. This work tackles the challenging relative pose estimation between nano-drones using only a gray-scale low-resolution camera and an ultra-low-power System-on-Chip (SoC) hosted onboard. We present a vertically integrated system based on a novel vision-based fully convolutional neural network (FCNN), which runs at 39Hz within 101mW onboard a Crazyflie nano-drone extended with the GWT GAP8 SoC. We compare our FCNN against three State-of-the-Art (SoA) systems. Considering the best-performing SoA approach, our model results in an R-squared improvement from 32 to 47% on the horizontal image coordinate and from 18 to 55% on the vertical image coordinate, on a real-world dataset of 30k images. Finally, our in-field tests show a reduction of the average tracking error of 37% compared to a previous SoA work and an endurance performance up to the entire battery lifetime of 4 minutes.","sentences":["Relative drone-to-drone localization is a fundamental building block for any swarm operations.","We address this task in the context of miniaturized nano-drones, i.e., 10cm in diameter, which show an ever-growing interest due to novel use cases enabled by their reduced form factor.","The price for their versatility comes with limited onboard resources, i.e., sensors, processing units, and memory, which limits the complexity of the onboard algorithms.","A traditional solution to overcome these limitations is represented by lightweight deep learning models directly deployed aboard nano-drones.","This work tackles the challenging relative pose estimation between nano-drones using only a gray-scale low-resolution camera and an ultra-low-power System-on-Chip (SoC) hosted onboard.","We present a vertically integrated system based on a novel vision-based fully convolutional neural network (FCNN), which runs at 39Hz within 101mW onboard a Crazyflie nano-drone extended with the GWT GAP8 SoC. We compare our FCNN against three State-of-the-Art (SoA) systems.","Considering the best-performing SoA approach, our model results in an R-squared improvement from 32 to 47% on the horizontal image coordinate and from 18 to 55% on the vertical image coordinate, on a real-world dataset of 30k images.","Finally, our in-field tests show a reduction of the average tracking error of 37% compared to a previous SoA work and an endurance performance up to the entire battery lifetime of 4 minutes."],"url":"http://arxiv.org/abs/2402.13756v1","category":"cs.CV"}
{"created":"2024-02-21 12:22:59","title":"Multi-step topological transitions among meron and skyrmion crystals in a centrosymmetric magnet","abstract":"Topological swirling spin textures, such as skyrmions and merons, have recently attracted much attention as a unique building block for high-density magnetic information devices. The controlled transformation among different types of such quasi-particles is an important challenge, while it was previously achieved only in a few non-centrosymmetric systems characterized by Dzyaloshinskii-Moriya interaction. Here, we report an experimental discovery of multi-step topological transitions among a variety of meron and skyrmion crystal states in a centrosymmetric magnet GdRu$_2$Ge$_2$. By performing the detailed magnetic structure analysis based on resonant X-ray and neutron scattering experiments as well as electron transport measurements, we have found that this compound hosts periodic lattice of elliptic skyrmions, meron/anti-meron pairs, and circular skyrmions as a function of external magnetic field. The diameter of these objects is as small as 2.7 nm, which is almost two orders of magnitude smaller than typical non-centrosymmetric magnets. Such an intricate manner of topological magnetic transitions are well reproduced by a theoretical model considering the competition between RKKY interactions at inequivalent wave vectors. The present findings demonstrate that even a simple centrosymmetric magnet with competing interactions can be a promising material platform to realize a richer variety of nanometric magnetic quasi-particles with distinctive symmetry and topology, whose stability may be tunable by various external stimuli.","sentences":["Topological swirling spin textures, such as skyrmions and merons, have recently attracted much attention as a unique building block for high-density magnetic information devices.","The controlled transformation among different types of such quasi-particles is an important challenge, while it was previously achieved only in a few non-centrosymmetric systems characterized by Dzyaloshinskii-Moriya interaction.","Here, we report an experimental discovery of multi-step topological transitions among a variety of meron and skyrmion crystal states in a centrosymmetric magnet GdRu$_2$Ge$_2$. By performing the detailed magnetic structure analysis based on resonant X-ray and neutron scattering experiments as well as electron transport measurements, we have found that this compound hosts periodic lattice of elliptic skyrmions, meron/anti-meron pairs, and circular skyrmions as a function of external magnetic field.","The diameter of these objects is as small as 2.7 nm, which is almost two orders of magnitude smaller than typical non-centrosymmetric magnets.","Such an intricate manner of topological magnetic transitions are well reproduced by a theoretical model considering the competition between RKKY interactions at inequivalent wave vectors.","The present findings demonstrate that even a simple centrosymmetric magnet with competing interactions can be a promising material platform to realize a richer variety of nanometric magnetic quasi-particles with distinctive symmetry and topology, whose stability may be tunable by various external stimuli."],"url":"http://arxiv.org/abs/2402.13751v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-21 12:20:49","title":"Possible origins of anomalous H$\\,$I gas around MHONGOOSE galaxy, NGC 5068","abstract":"The existing reservoirs of neutral atomic hydrogen gas (H$\\,$I) in galaxies are insufficient to have maintained the observed levels of star formation without some kind of replenishment. {This refuelling of the H$\\,$I reservoirs} is likely to occur at column densities an order of magnitude lower than previous observational limits (N$_{\\rm{H\\,I}\\, limit} \\sim 10^{19}\\,$cm$^{-2}$ at 30$''$ resolution over a linewidth of $20\\,$km/s). In this paper, we present recent deep H$\\,$I observations of NGC 5068, a nearby isolated star-forming galaxy observed by MeerKAT as part of the MHONGOOSE survey. With these new data, we are able to detect low column density H$\\,$I around NGC 5068 with a $3\\sigma$ detection limit of N$_{\\rm{H\\,I}} = 6.4 \\times 10^{17}\\,$cm$^{-2}$ at 90$''$ resolution over a $20\\,$km/s linewidth. The high sensitivity and resolution of the MeerKAT data reveal a complex morphology of the H$\\,$I in this galaxy -- a regularly rotating inner disk coincident with the main star-forming disk of the galaxy, a warped outer disk of low column density gas (N$_{\\rm{H\\,I}} < 9 \\times 10^{19}\\,$cm$^{-2}$), in addition to clumps of gas on the north west side of the galaxy. We employ a simple two disk model that describe the inner and outer disks, and are able to identify anomalous gas that deviates from the rotation of the main galaxy. The morphology and the kinematics of the anomalous gas suggest a possible extra-galactic origin. We explore a number of possible origin scenarios that may explain the anomalous gas, and conclude that fresh accretion is the most likely scenario.","sentences":["The existing reservoirs of neutral atomic hydrogen gas (H$\\,$I) in galaxies are insufficient to have maintained the observed levels of star formation without some kind of replenishment.","{This refuelling of the H$\\,$I reservoirs} is likely to occur at column densities an order of magnitude lower than previous observational limits (N$_{\\rm{H\\,I}\\, limit} \\sim 10^{19}\\,$cm$^{-2}$ at 30$''$ resolution over a linewidth of $20\\,$km/s).","In this paper, we present recent deep H$\\,$I observations of NGC 5068, a nearby isolated star-forming galaxy observed by MeerKAT as part of the MHONGOOSE survey.","With these new data, we are able to detect low column density H$\\,$I around NGC 5068 with a $3\\sigma$ detection limit of N$_{\\rm{H\\,I}} = 6.4 \\times 10^{17}\\,$cm$^{-2}$ at 90$''$ resolution over a $20\\,$km/s linewidth.","The high sensitivity and resolution of the MeerKAT data reveal a complex morphology of the H$\\,$I in this galaxy -- a regularly rotating inner disk coincident with the main star-forming disk of the galaxy, a warped outer disk of low column density gas (N$_{\\rm{H\\,I}} < 9","\\times 10^{19}\\,$cm$^{-2}$), in addition to clumps of gas on the north west side of the galaxy.","We employ a simple two disk model that describe the inner and outer disks, and are able to identify anomalous gas that deviates from the rotation of the main galaxy.","The morphology and the kinematics of the anomalous gas suggest a possible extra-galactic origin.","We explore a number of possible origin scenarios that may explain the anomalous gas, and conclude that fresh accretion is the most likely scenario."],"url":"http://arxiv.org/abs/2402.13749v1","category":"astro-ph.GA"}
{"created":"2024-02-21 11:55:36","title":"Laplace's first law of errors applied to diffusive motion","abstract":"In biological, glassy, and active systems, various tracers exhibit Laplace-like, i.e., exponential, spreading of the diffusing packet of particles. The limitations of the central limit theorem in fully capturing the behaviors of such diffusive processes, especially in the tails, have been studied using the continuous time random walk model. For cases when the jump length distribution is super-exponential, e.g., a Gaussian, we use large deviations theory and relate it to the appearance of exponential tails. When the jump length distribution is sub-exponential the packet of spreading particles is described by the big jump principle. We demonstrate the applicability of our approach for finite time, indicating that rare events and the asymptotics of the large deviations rate function can be sampled for large length scales within a reasonably short measurement time.","sentences":["In biological, glassy, and active systems, various tracers exhibit Laplace-like, i.e., exponential, spreading of the diffusing packet of particles.","The limitations of the central limit theorem in fully capturing the behaviors of such diffusive processes, especially in the tails, have been studied using the continuous time random walk model.","For cases when the jump length distribution is super-exponential, e.g., a Gaussian, we use large deviations theory and relate it to the appearance of exponential tails.","When the jump length distribution is sub-exponential the packet of spreading particles is described by the big jump principle.","We demonstrate the applicability of our approach for finite time, indicating that rare events and the asymptotics of the large deviations rate function can be sampled for large length scales within a reasonably short measurement time."],"url":"http://arxiv.org/abs/2402.13733v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-21 11:33:09","title":"Exploiting Adaptive Contextual Masking for Aspect-Based Sentiment Analysis","abstract":"Aspect-Based Sentiment Analysis (ABSA) is a fine-grained linguistics problem that entails the extraction of multifaceted aspects, opinions, and sentiments from the given text. Both standalone and compound ABSA tasks have been extensively used in the literature to examine the nuanced information present in online reviews and social media posts. Current ABSA methods often rely on static hyperparameters for attention-masking mechanisms, which can struggle with context adaptation and may overlook the unique relevance of words in varied situations. This leads to challenges in accurately analyzing complex sentences containing multiple aspects with differing sentiments. In this work, we present adaptive masking methods that remove irrelevant tokens based on context to assist in Aspect Term Extraction and Aspect Sentiment Classification subtasks of ABSA. We show with our experiments that the proposed methods outperform the baseline methods in terms of accuracy and F1 scores on four benchmark online review datasets. Further, we show that the proposed methods can be extended with multiple adaptations and demonstrate a qualitative analysis of the proposed approach using sample text for aspect term extraction.","sentences":["Aspect-Based Sentiment Analysis (ABSA) is a fine-grained linguistics problem that entails the extraction of multifaceted aspects, opinions, and sentiments from the given text.","Both standalone and compound ABSA tasks have been extensively used in the literature to examine the nuanced information present in online reviews and social media posts.","Current ABSA methods often rely on static hyperparameters for attention-masking mechanisms, which can struggle with context adaptation and may overlook the unique relevance of words in varied situations.","This leads to challenges in accurately analyzing complex sentences containing multiple aspects with differing sentiments.","In this work, we present adaptive masking methods that remove irrelevant tokens based on context to assist in Aspect Term Extraction and Aspect Sentiment Classification subtasks of ABSA.","We show with our experiments that the proposed methods outperform the baseline methods in terms of accuracy and F1 scores on four benchmark online review datasets.","Further, we show that the proposed methods can be extended with multiple adaptations and demonstrate a qualitative analysis of the proposed approach using sample text for aspect term extraction."],"url":"http://arxiv.org/abs/2402.13722v1","category":"cs.CL"}
{"created":"2024-02-21 11:31:01","title":"Informative Simultaneous Confidence Intervals for Graphical Test Procedures","abstract":"Simultaneous confidence intervals (SCIs) that are compatible with a given closed test procedure are often non-informative. More precisely, for a one-sided null hypothesis, the bound of the SCI can stick to the border of the null hypothesis, irrespective of how far the point estimate deviates from the null hypothesis. This has been illustrated for the Bonferroni-Holm and fall-back procedures, for which alternative SCIs have been suggested, that are free of this deficiency. These informative SCIs are not fully compatible with the initial multiple test, but are close to it and hence provide similar power advantages. They provide a multiple hypothesis test with strong family-wise error rate control that can be used in replacement of the initial multiple test. The current paper extends previous work for informative SCIs to graphical test procedures. The information gained from the newly suggested SCIs is shown to be always increasing with increasing evidence against a null hypothesis. The new SCIs provide a compromise between information gain and the goal to reject as many hypotheses as possible. The SCIs are defined via a family of dual graphs and the projection method. A simple iterative algorithm for the computation of the intervals is provided. A simulation study illustrates the results for a complex graphical test procedure.","sentences":["Simultaneous confidence intervals (SCIs) that are compatible with a given closed test procedure are often non-informative.","More precisely, for a one-sided null hypothesis, the bound of the SCI can stick to the border of the null hypothesis, irrespective of how far the point estimate deviates from the null hypothesis.","This has been illustrated for the Bonferroni-Holm and fall-back procedures, for which alternative SCIs have been suggested, that are free of this deficiency.","These informative SCIs are not fully compatible with the initial multiple test, but are close to it and hence provide similar power advantages.","They provide a multiple hypothesis test with strong family-wise error rate control that can be used in replacement of the initial multiple test.","The current paper extends previous work for informative SCIs to graphical test procedures.","The information gained from the newly suggested SCIs is shown to be always increasing with increasing evidence against a null hypothesis.","The new SCIs provide a compromise between information gain and the goal to reject as many hypotheses as possible.","The SCIs are defined via a family of dual graphs and the projection method.","A simple iterative algorithm for the computation of the intervals is provided.","A simulation study illustrates the results for a complex graphical test procedure."],"url":"http://arxiv.org/abs/2402.13719v1","category":"stat.ME"}
{"created":"2024-02-21 11:29:03","title":"Probabilistic Constellation Shaping for Enhancing Spectral Efficiency in NOMA VLC Systems","abstract":"The limited modulation bandwidth of the light emitting diodes (LEDs) presents a challenge in the development of practical high-data-rate visible light communication (VLC) systems. In this paper, a novel adaptive coded probabilistic shaping (PS)-based nonorthogonal multiple access (NOMA) scheme is proposed to improve spectral efficiency (SE) of VLC systems in multiuser uplink communication scenarios. The proposed scheme adapts its rate to the optical signal-to-noise ratio (OSNR) by utilizing non-uniformly distributed discrete constellation symbols and low complexity channel encoder. Furthermore, an alternate optimization algorithm is proposed to determine the optimal channel coding rate, constellation spacing, and probability mass function (PMF) of each user. The extensive numerical results show that the proposed PS-based NOMA scheme closely approaches the capacity of NOMA with fine granularity. Presented results demonstrate the effectiveness of our scheme in improving the SE of VLC systems in multiuser scenarios. For instance, our scheme exhibits substantial SE gains over existing schemes, namely, the pairwise coded modulation (PCM), geometric shaping (GS), and uniform-distribution schemes. These findings highlight the potential of our approach to significantly enhance VLC systems.","sentences":["The limited modulation bandwidth of the light emitting diodes (LEDs) presents a challenge in the development of practical high-data-rate visible light communication (VLC) systems.","In this paper, a novel adaptive coded probabilistic shaping (PS)-based nonorthogonal multiple access (NOMA) scheme is proposed to improve spectral efficiency (SE) of VLC systems in multiuser uplink communication scenarios.","The proposed scheme adapts its rate to the optical signal-to-noise ratio (OSNR) by utilizing non-uniformly distributed discrete constellation symbols and low complexity channel encoder.","Furthermore, an alternate optimization algorithm is proposed to determine the optimal channel coding rate, constellation spacing, and probability mass function (PMF) of each user.","The extensive numerical results show that the proposed PS-based NOMA scheme closely approaches the capacity of NOMA with fine granularity.","Presented results demonstrate the effectiveness of our scheme in improving the SE of VLC systems in multiuser scenarios.","For instance, our scheme exhibits substantial SE gains over existing schemes, namely, the pairwise coded modulation (PCM), geometric shaping (GS), and uniform-distribution schemes.","These findings highlight the potential of our approach to significantly enhance VLC systems."],"url":"http://arxiv.org/abs/2402.13715v1","category":"cs.IT"}
{"created":"2024-02-21 11:26:30","title":"$S$-integral preperiodic points for monomial semigroups over number fields","abstract":"We consider semigroup dynamical systems defined by several monnomials over a number field $K$. We prove a finiteness result for preperiodic points of such systems which are $S$-integral with respect to a non-preperiodic point $\\beta$, which is uniform as $\\beta$ varies over number fields of bounded degree. This generalises results of Baker, Ih and Rumely, which were made uniform by Yap, and verifies a special case of a natural generalisation of a conjecture of Ih.","sentences":["We consider semigroup dynamical systems defined by several monnomials over a number field $K$. We prove a finiteness result for preperiodic points of such systems which are $S$-integral with respect to a non-preperiodic point $\\beta$, which is uniform as $\\beta$ varies over number fields of bounded degree.","This generalises results of Baker, Ih and Rumely, which were made uniform by Yap, and verifies a special case of a natural generalisation of a conjecture of Ih."],"url":"http://arxiv.org/abs/2402.13713v1","category":"math.NT"}
{"created":"2024-02-21 11:26:11","title":"On multiplicative dependence between elements of polynomial orbits","abstract":"We classify the pairs of polynomials $f,g \\in \\mathbb{C}[X]$ having orbits satisfying infinitely many multiplicative dependence relations, extending a result of Ghioca, Tucker and Zieve. Moreover, we show that given $f_1,\\ldots, f_n$ from a certain class of polynomials with integer coefficients, the vectors of indices $(m_1,\\ldots,m_n)$ such that $f_1^{m_1}(0),\\ldots,f_n^{m_n}(0)$ are multiplictively dependent are sparse. We also classify the pairs $f,g \\in \\mathbb{Q}[X]$ such that there are infinitely many $(x,y) \\in \\mathbb{Z}^2$ satisfying $f(x)^k=g(y)^\\ell$ for some (possibly varying) non-zero integers $k,\\ell$.","sentences":["We classify the pairs of polynomials $f,g \\in \\mathbb{C}[X]$ having orbits satisfying infinitely many multiplicative dependence relations, extending a result of Ghioca, Tucker and Zieve.","Moreover, we show that given $f_1,\\ldots, f_n$ from a certain class of polynomials with integer coefficients, the vectors of indices $(m_1,\\ldots,m_n)$ such that $f_1^{m_1}(0),\\ldots,f_n^{m_n}(0)$ are multiplictively dependent are sparse.","We also classify the pairs $f,g \\in \\mathbb{Q}[X]$ such that there are infinitely many $(x,y) \\in","\\mathbb{Z}^2$ satisfying $f(x)^k=g(y)^\\ell$ for some (possibly varying) non-zero integers $k,\\ell$."],"url":"http://arxiv.org/abs/2402.13712v1","category":"math.NT"}
{"created":"2024-02-21 11:12:52","title":"Linear-Quadratic optimal control for boundary controlled networks of waves","abstract":"Linear-Quadratic optimal controls are computed for a class of boundary controlled, boundary observed hyperbolic infinite-dimensional systems, which may be viewed as networks of waves. The main results of this manuscript consist in converting the infinite-dimensional continuous-time systems into infinite-dimensional discrete-time systems for which the operators dynamics are matrices, in solving the LQ-optimal control problem in discrete-time and then in interpreting the solution in the continuous-time variables, giving rise to the optimal boundary control input. The results are applied to two examples, a small network of three vibrating strings and a co-current heat-exchanger, for which boundary sensors and actuators are considered.","sentences":["Linear-Quadratic optimal controls are computed for a class of boundary controlled, boundary observed hyperbolic infinite-dimensional systems, which may be viewed as networks of waves.","The main results of this manuscript consist in converting the infinite-dimensional continuous-time systems into infinite-dimensional discrete-time systems for which the operators dynamics are matrices, in solving the LQ-optimal control problem in discrete-time and then in interpreting the solution in the continuous-time variables, giving rise to the optimal boundary control input.","The results are applied to two examples, a small network of three vibrating strings and a co-current heat-exchanger, for which boundary sensors and actuators are considered."],"url":"http://arxiv.org/abs/2402.13706v1","category":"math.OC"}
{"created":"2024-02-21 11:10:45","title":"Hyperuniformity and optimal transport of point processes","abstract":"We examine optimal matchings or transport between two stationary point processes and in particular, from a point process to the (integer) lattice or the Lebesgue measure respectively. The main focus of the article is the implication of hyperuniformity (reduced variance fluctuations in point processes) to optimal transport: in dimension $2$, we show that the typical matching cost has finite second moment under a mild logarithmic integrability condition on the reduced pair correlation measure, showing that most planar hyperuniform point processes are $ L^2$-perturbed lattices. Our method does not formally require assumptions on the correlation measure or the variance behaviour and it retrieves known sharp bounds for neutral integrable systems such as Poisson processes, and also applies to hyperfluctuating systems. The proof relies on the estimation of the optimal transport cost between point processes restricted to large windows for a well-chosen cost through their Fourier-Stieljes transforms, related to their structure factor. The existence of an infinite matching is obtained through a compactness argument on the space of random stationary measures.","sentences":["We examine optimal matchings or transport between two stationary point processes and in particular, from a point process to the (integer) lattice or the Lebesgue measure respectively.","The main focus of the article is the implication of hyperuniformity (reduced variance fluctuations in point processes) to optimal transport: in dimension $2$, we show that the typical matching cost has finite second moment under a mild logarithmic integrability condition on the reduced pair correlation measure, showing that most planar hyperuniform point processes are $ L^2$-perturbed lattices.","Our method does not formally require assumptions on the correlation measure or the variance behaviour and it retrieves known sharp bounds for neutral integrable systems such as Poisson processes, and also applies to hyperfluctuating systems.","The proof relies on the estimation of the optimal transport cost between point processes restricted to large windows for a well-chosen cost through their Fourier-Stieljes transforms, related to their structure factor.","The existence of an infinite matching is obtained through a compactness argument on the space of random stationary measures."],"url":"http://arxiv.org/abs/2402.13705v1","category":"math.PR"}
{"created":"2024-02-21 10:40:15","title":"Data-Driven Forecasting of Non-Equilibrium Solid-State Dynamics","abstract":"We present a data-driven approach to efficiently approximate nonlinear transient dynamics in solid-state systems. Our proposed machine-learning model combines a dimensionality reduction stage with a nonlinear vector autoregression scheme. We report an outstanding time-series forecasting performance combined with an easy to deploy model and an inexpensive training routine. Our results are of great relevance as they have the potential to massively accelerate multi-physics simulation software and thereby guide to future development of solid-state based technologies.","sentences":["We present a data-driven approach to efficiently approximate nonlinear transient dynamics in solid-state systems.","Our proposed machine-learning model combines a dimensionality reduction stage with a nonlinear vector autoregression scheme.","We report an outstanding time-series forecasting performance combined with an easy to deploy model and an inexpensive training routine.","Our results are of great relevance as they have the potential to massively accelerate multi-physics simulation software and thereby guide to future development of solid-state based technologies."],"url":"http://arxiv.org/abs/2402.13685v1","category":"physics.comp-ph"}
{"created":"2024-02-21 10:30:52","title":"A Pontryagin Maximum Principle for agent-based models with convex state space","abstract":"We derive a first order optimality condition for a class of agent-based systems, as well as for their mean-field counterpart. A relevant difficulty of our analysis is that the state equation is formulated on possibly infinite-dimensional convex subsets of Banach spaces, as required by some problems in multi-population dynamics. Due to the lack of a linear structure and of local compactness, the usual tools of needle variations and linearisation procedures used to derive Pontryagin type conditions have to be generalised to the setting at hand. This is done by considering suitable notions of differentials and by a careful inspection of the underlying functional structures.","sentences":["We derive a first order optimality condition for a class of agent-based systems, as well as for their mean-field counterpart.","A relevant difficulty of our analysis is that the state equation is formulated on possibly infinite-dimensional convex subsets of Banach spaces, as required by some problems in multi-population dynamics.","Due to the lack of a linear structure and of local compactness, the usual tools of needle variations and linearisation procedures used to derive Pontryagin type conditions have to be generalised to the setting at hand.","This is done by considering suitable notions of differentials and by a careful inspection of the underlying functional structures."],"url":"http://arxiv.org/abs/2402.13680v1","category":"math.AP"}
{"created":"2024-02-21 10:21:50","title":"An empirical view of the extended atmosphere and inner envelope of the AGB star R Doradus I. Physical model based on CO lines","abstract":"The mass loss experienced on the asymptotic giant branch (AGB) at the end of the lives of low- and intermediate-mass stars is widely accepted to rely on radiation pressure acting on dust grains formed in the extended AGB atmospheres. The interaction of convection, stellar pulsation, and heating and cooling processes cause the density, velocity and temperature distributions in the inner regions of the envelope to be complex, making the dust-formation process difficult to calculate. Hence, characterising the extended atmospheres and inner outflow empirically is paramount to advance our understanding of the dust-formation and wind-driving processes.   To this end, we observe the AGB star R Dor using ALMA and modelled the $^{12}$CO $v=0, J=2-1$, $v=1, J=2-1$ and $3-2$ and $^{13}$CO $v=0, J=3-2$ lines using the 3D radiative transfer code LIME up to a distance of $\\sim 4$ times the radius of the star at sub-mm wavelengths.   We find a complex velocity field with structure down to scales at least equal to the resolution of the observations. The observed maps are well reproduced assuming spherical symmetry for the gas temperature and density distributions. We find the radial profiles of these two quantities to be very steep close to the star and shallower for radii larger than $\\sim 1.6$ times the stellar sub-mm radius. This change is consistent with the transition between extended atmosphere and outflow. We constrain the standard deviation of the stochastic velocity distribution in the large-scale outflow to be $\\lesssim 0.4$ km/s. We observe two emission blobs in the CO $v=0, J=2-1$ line and find their gas densities and radial velocities to be substantially larger than those of the surrounding gas. Monitoring the evolution of these blobs will lead to a better understanding of the role of these structures in the mass-loss process of R Dor.","sentences":["The mass loss experienced on the asymptotic giant branch (AGB) at the end of the lives of low- and intermediate-mass stars is widely accepted to rely on radiation pressure acting on dust grains formed in the extended AGB atmospheres.","The interaction of convection, stellar pulsation, and heating and cooling processes cause the density, velocity and temperature distributions in the inner regions of the envelope to be complex, making the dust-formation process difficult to calculate.","Hence, characterising the extended atmospheres and inner outflow empirically is paramount to advance our understanding of the dust-formation and wind-driving processes.   ","To this end, we observe the AGB star R Dor using ALMA and modelled the $^{12}$CO $v=0, J=2-1$, $v=1, J=2-1$ and $3-2$ and $^{13}$CO $v=0, J=3-2$ lines using the 3D radiative transfer code LIME up to a distance of $\\sim 4$ times the radius of the star at sub-mm wavelengths.   ","We find a complex velocity field with structure down to scales at least equal to the resolution of the observations.","The observed maps are well reproduced assuming spherical symmetry for the gas temperature and density distributions.","We find the radial profiles of these two quantities to be very steep close to the star and shallower for radii larger than $\\sim 1.6$ times the stellar sub-mm radius.","This change is consistent with the transition between extended atmosphere and outflow.","We constrain the standard deviation of the stochastic velocity distribution in the large-scale outflow to be $\\lesssim 0.4$ km/s. We observe two emission blobs in the CO $v=0, J=2-1$ line and find their gas densities and radial velocities to be substantially larger than those of the surrounding gas.","Monitoring the evolution of these blobs will lead to a better understanding of the role of these structures in the mass-loss process of R Dor."],"url":"http://arxiv.org/abs/2402.13676v1","category":"astro-ph.SR"}
{"created":"2024-02-21 10:17:23","title":"Computing Transiting Exoplanet Parameters with 1D Convolutional Neural Networks","abstract":"The transit method allows the detection and characterization of planetary systems by analyzing stellar light curves. Convolutional neural networks appear to offer a viable solution for automating these analyses. In this research, two 1D convolutional neural network models, which work with simulated light curves in which transit-like signals were injected, are presented. One model operates on complete light curves and estimates the orbital period, and the other one operates on phase-folded light curves and estimates the semimajor axis of the orbit and the square of the planet-to-star radius ratio. Both models were tested on real data from TESS light curves with confirmed planets to ensure that they are able to work with real data. The results obtained show that 1D CNNs are able to characterize transiting exoplanets from their host star's detrended light curve and, furthermore, reducing both the required time and computational costs compared with the current detection and characterization algorithms.","sentences":["The transit method allows the detection and characterization of planetary systems by analyzing stellar light curves.","Convolutional neural networks appear to offer a viable solution for automating these analyses.","In this research, two 1D convolutional neural network models, which work with simulated light curves in which transit-like signals were injected, are presented.","One model operates on complete light curves and estimates the orbital period, and the other one operates on phase-folded light curves and estimates the semimajor axis of the orbit and the square of the planet-to-star radius ratio.","Both models were tested on real data from TESS light curves with confirmed planets to ensure that they are able to work with real data.","The results obtained show that 1D CNNs are able to characterize transiting exoplanets from their host star's detrended light curve and, furthermore, reducing both the required time and computational costs compared with the current detection and characterization algorithms."],"url":"http://arxiv.org/abs/2402.13673v1","category":"astro-ph.EP"}
{"created":"2024-02-21 09:56:56","title":"Measurement Uncertainty: Relating the uncertainties of physical and virtual measurements","abstract":"In the context of industrially mass-manufactured products, quality management is based on physically inspecting a small sample from a large batch and reasoning about the batch's quality conformance. When complementing physical inspections with predictions from machine learning models, it is crucial that the uncertainty of the prediction is known. Otherwise, the application of established quality management concepts is not legitimate. Deterministic (machine learning) models lack quantification of their predictive uncertainty and are therefore unsuitable. Probabilistic (machine learning) models provide a predictive uncertainty along with the prediction. However, a concise relationship is missing between the measurement uncertainty of physical inspections and the predictive uncertainty of probabilistic models in their application in quality management. Here, we show how the predictive uncertainty of probabilistic (machine learning) models is related to the measurement uncertainty of physical inspections. This enables the use of probabilistic models for virtual inspections and integrates them into existing quality management concepts. Thus, we can provide a virtual measurement for any quality characteristic based on the process data and achieve a 100 percent inspection rate. In the field of Predictive Quality, the virtual measurement is of great interest. Based on our results, physical inspections with a low sampling rate can be accompanied by virtual measurements that allow an inspection rate of 100 percent. We add substantial value, especially to complex process chains, as faulty products/parts are identified promptly and upcoming process steps can be aborted.","sentences":["In the context of industrially mass-manufactured products, quality management is based on physically inspecting a small sample from a large batch and reasoning about the batch's quality conformance.","When complementing physical inspections with predictions from machine learning models, it is crucial that the uncertainty of the prediction is known.","Otherwise, the application of established quality management concepts is not legitimate.","Deterministic (machine learning) models lack quantification of their predictive uncertainty and are therefore unsuitable.","Probabilistic (machine learning) models provide a predictive uncertainty along with the prediction.","However, a concise relationship is missing between the measurement uncertainty of physical inspections and the predictive uncertainty of probabilistic models in their application in quality management.","Here, we show how the predictive uncertainty of probabilistic (machine learning) models is related to the measurement uncertainty of physical inspections.","This enables the use of probabilistic models for virtual inspections and integrates them into existing quality management concepts.","Thus, we can provide a virtual measurement for any quality characteristic based on the process data and achieve a 100 percent inspection rate.","In the field of Predictive Quality, the virtual measurement is of great interest.","Based on our results, physical inspections with a low sampling rate can be accompanied by virtual measurements that allow an inspection rate of 100 percent.","We add substantial value, especially to complex process chains, as faulty products/parts are identified promptly and upcoming process steps can be aborted."],"url":"http://arxiv.org/abs/2402.13666v1","category":"stat.AP"}
{"created":"2024-02-21 09:40:26","title":"Improving a Proportional Integral Controller with Reinforcement Learning on a Throttle Valve Benchmark","abstract":"This paper presents a learning-based control strategy for non-linear throttle valves with an asymmetric hysteresis, leading to a near-optimal controller without requiring any prior knowledge about the environment. We start with a carefully tuned Proportional Integrator (PI) controller and exploit the recent advances in Reinforcement Learning (RL) with Guides to improve the closed-loop behavior by learning from the additional interactions with the valve. We test the proposed control method in various scenarios on three different valves, all highlighting the benefits of combining both PI and RL frameworks to improve control performance in non-linear stochastic systems. In all the experimental test cases, the resulting agent has a better sample efficiency than traditional RL agents and outperforms the PI controller.","sentences":["This paper presents a learning-based control strategy for non-linear throttle valves with an asymmetric hysteresis, leading to a near-optimal controller without requiring any prior knowledge about the environment.","We start with a carefully tuned Proportional Integrator (PI) controller and exploit the recent advances in Reinforcement Learning (RL) with Guides to improve the closed-loop behavior by learning from the additional interactions with the valve.","We test the proposed control method in various scenarios on three different valves, all highlighting the benefits of combining both PI and RL frameworks to improve control performance in non-linear stochastic systems.","In all the experimental test cases, the resulting agent has a better sample efficiency than traditional RL agents and outperforms the PI controller."],"url":"http://arxiv.org/abs/2402.13654v1","category":"eess.SY"}
{"created":"2024-02-21 09:23:18","title":"Bifurcation of time crystals in driven and dissipative Rydberg atomic gas","abstract":"A time crystal is an exotic phase of matter where time-translational symmetry is broken; this phase differs from the spatial symmetry breaking induced in crystals in space. Lots of experiments report the transition from a thermal equilibrium phase to time crystal phase. However, there is no experimental method to probe the bifurcation effect of distinct time crystals in quantum many-body systems. Here, in a driven and dissipative many-body Rydberg atom system, we observe multiple continuous dissipative time crystals and emergence of more complex temporal symmetries beyond the single time crystal phase. Bifurcation of time crystals in strongly interacting Rydberg atoms is observed; the process manifests as a transition from a time crystal state of long temporal order to one of short temporal order, or vice versa. By manipulating the driving field parameters, we observe the time crystal's bistability and a hysteresis loop. These investigations indicate new possibilities for control and manipulation of the temporal symmetries of non-equilibrium systems.","sentences":["A time crystal is an exotic phase of matter where time-translational symmetry is broken; this phase differs from the spatial symmetry breaking induced in crystals in space.","Lots of experiments report the transition from a thermal equilibrium phase to time crystal phase.","However, there is no experimental method to probe the bifurcation effect of distinct time crystals in quantum many-body systems.","Here, in a driven and dissipative many-body Rydberg atom system, we observe multiple continuous dissipative time crystals and emergence of more complex temporal symmetries beyond the single time crystal phase.","Bifurcation of time crystals in strongly interacting Rydberg atoms is observed; the process manifests as a transition from a time crystal state of long temporal order to one of short temporal order, or vice versa.","By manipulating the driving field parameters, we observe the time crystal's bistability and a hysteresis loop.","These investigations indicate new possibilities for control and manipulation of the temporal symmetries of non-equilibrium systems."],"url":"http://arxiv.org/abs/2402.13644v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-21 09:18:09","title":"Unravelling Organisational Rule Systems in Requirements Engineering","abstract":"Context and motivation: Requirements engineering of complex IT systems needs to manage the many, and often vague and conflicting, organisational rules that exist in the context of a modern enterprise. At the same time, IT systems affect the organisation, essentially setting new rules on how the organisation should work. Question/problem: Gathering requirements for an IT system involves understanding the complex rules that govern an organisation. The research question is: How can the holistic properties of organisational rules be conceptualised? Principal ideas/results: This paper introduces the concept of organisational rule systems that may be used to describe complex organisational rules. The concept and its components are presented as a conceptual framework, which in turn is condensed into a conceptual framework diagram. The framework is grounded in a critical literature review. Contribution: The conceptual framework will, as a first step of a wider research agenda, help requirements engineers understand the influence of organisational rules.","sentences":["Context and motivation: Requirements engineering of complex IT systems needs to manage the many, and often vague and conflicting, organisational rules that exist in the context of a modern enterprise.","At the same time, IT systems affect the organisation, essentially setting new rules on how the organisation should work.","Question/problem: Gathering requirements for an IT system involves understanding the complex rules that govern an organisation.","The research question is: How can the holistic properties of organisational rules be conceptualised?","Principal ideas/results: This paper introduces the concept of organisational rule systems that may be used to describe complex organisational rules.","The concept and its components are presented as a conceptual framework, which in turn is condensed into a conceptual framework diagram.","The framework is grounded in a critical literature review.","Contribution: The conceptual framework will, as a first step of a wider research agenda, help requirements engineers understand the influence of organisational rules."],"url":"http://arxiv.org/abs/2402.13637v1","category":"cs.SE"}
{"created":"2024-02-21 09:09:33","title":"Ordering Topological Descriptors","abstract":"Recent developments in shape reconstruction and comparison call for the use of many different types of topological descriptors (persistence diagrams, Euler characteristic functions, etc.). We establish a framework that allows for quantitative comparisons of topological descriptor types and therefore may be used as a tool in more rigorously justifying choices made in applications. We then use this framework to partially order a set of six common topological descriptor types. In particular, the resulting poset gives insight into the advantages of using verbose rather than concise topological descriptors. We then provide lower bounds on the size of sets of descriptors that are complete discrete invariants of simplicial complexes, both tight and worst case. This work sets up a rigorous theory that allows for future comparisons and analysis of topological descriptor types.","sentences":["Recent developments in shape reconstruction and comparison call for the use of many different types of topological descriptors (persistence diagrams, Euler characteristic functions, etc.).","We establish a framework that allows for quantitative comparisons of topological descriptor types and therefore may be used as a tool in more rigorously justifying choices made in applications.","We then use this framework to partially order a set of six common topological descriptor types.","In particular, the resulting poset gives insight into the advantages of using verbose rather than concise topological descriptors.","We then provide lower bounds on the size of sets of descriptors that are complete discrete invariants of simplicial complexes, both tight and worst case.","This work sets up a rigorous theory that allows for future comparisons and analysis of topological descriptor types."],"url":"http://arxiv.org/abs/2402.13632v1","category":"cs.CG"}
{"created":"2024-02-21 08:50:40","title":"FLAME: Self-Supervised Low-Resource Taxonomy Expansion using Large Language Models","abstract":"Taxonomies represent an arborescence hierarchical structure that establishes relationships among entities to convey knowledge within a specific domain. Each edge in the taxonomy signifies a hypernym-hyponym relationship. Taxonomies find utility in various real-world applications, such as e-commerce search engines and recommendation systems. Consequently, there arises a necessity to enhance these taxonomies over time. However, manually curating taxonomies with neoteric data presents challenges due to limitations in available human resources and the exponential growth of data. Therefore, it becomes imperative to develop automatic taxonomy expansion methods. Traditional supervised taxonomy expansion approaches encounter difficulties stemming from limited resources, primarily due to the small size of existing taxonomies. This scarcity of training data often leads to overfitting. In this paper, we propose FLAME, a novel approach for taxonomy expansion in low-resource environments by harnessing the capabilities of large language models that are trained on extensive real-world knowledge. LLMs help compensate for the scarcity of domain-specific knowledge. Specifically, FLAME leverages prompting in few-shot settings to extract the inherent knowledge within the LLMs, ascertaining the hypernym entities within the taxonomy. Furthermore, it employs reinforcement learning to fine-tune the large language models, resulting in more accurate predictions. Experiments on three real-world benchmark datasets demonstrate the effectiveness of FLAME in real-world scenarios, achieving a remarkable improvement of 18.5% in accuracy and 12.3% in Wu & Palmer metric over eight baselines. Furthermore, we elucidate the strengths and weaknesses of FLAME through an extensive case study, error analysis and ablation studies on the benchmarks.","sentences":["Taxonomies represent an arborescence hierarchical structure that establishes relationships among entities to convey knowledge within a specific domain.","Each edge in the taxonomy signifies a hypernym-hyponym relationship.","Taxonomies find utility in various real-world applications, such as e-commerce search engines and recommendation systems.","Consequently, there arises a necessity to enhance these taxonomies over time.","However, manually curating taxonomies with neoteric data presents challenges due to limitations in available human resources and the exponential growth of data.","Therefore, it becomes imperative to develop automatic taxonomy expansion methods.","Traditional supervised taxonomy expansion approaches encounter difficulties stemming from limited resources, primarily due to the small size of existing taxonomies.","This scarcity of training data often leads to overfitting.","In this paper, we propose FLAME, a novel approach for taxonomy expansion in low-resource environments by harnessing the capabilities of large language models that are trained on extensive real-world knowledge.","LLMs help compensate for the scarcity of domain-specific knowledge.","Specifically, FLAME leverages prompting in few-shot settings to extract the inherent knowledge within the LLMs, ascertaining the hypernym entities within the taxonomy.","Furthermore, it employs reinforcement learning to fine-tune the large language models, resulting in more accurate predictions.","Experiments on three real-world benchmark datasets demonstrate the effectiveness of FLAME in real-world scenarios, achieving a remarkable improvement of 18.5% in accuracy and 12.3% in Wu & Palmer metric over eight baselines.","Furthermore, we elucidate the strengths and weaknesses of FLAME through an extensive case study, error analysis and ablation studies on the benchmarks."],"url":"http://arxiv.org/abs/2402.13623v1","category":"cs.CL"}
{"created":"2024-02-21 08:44:50","title":"Strong Linearizability using Primitives with Consensus Number 2","abstract":"A powerful tool for designing complex concurrent programs is through composition with object implementations from lower-level primitives. Strongly-linearizable implementations allow to preserve hyper-properties, e.g., probabilistic guarantees of randomized programs. However, the only known wait-free strongly-linearizable implementations for many objects rely on compare&swap, a universal primitive that allows any number of processes to solve consensus. This is despite the fact that these objects have wait-free linearizable implementations from read / write primitives, which do not support consensus. This paper investigates a middle-ground, asking whether there are wait-free strongly-linearizable implementations from realistic primitives such as test&set or fetch&add, whose consensus number is 2.   We show that many objects with consensus number 1 have wait-free strongly-linearizable implementations from fetch&add. We also show that several objects with consensus number 2 have wait-free or lock-free implementations from other objects with consensus number 2. In contrast, we prove that even when fetch&add, swap and test&set primitives are used, some objects with consensus number 2 do not have lock-free strongly-linearizable implementations. This includes queues and stacks, as well as relaxed variants thereof.","sentences":["A powerful tool for designing complex concurrent programs is through composition with object implementations from lower-level primitives.","Strongly-linearizable implementations allow to preserve hyper-properties, e.g., probabilistic guarantees of randomized programs.","However, the only known wait-free strongly-linearizable implementations for many objects rely on compare&swap, a universal primitive that allows any number of processes to solve consensus.","This is despite the fact that these objects have wait-free linearizable implementations from read / write primitives, which do not support consensus.","This paper investigates a middle-ground, asking whether there are wait-free strongly-linearizable implementations from realistic primitives such as test&set or fetch&add, whose consensus number is 2.   ","We show that many objects with consensus number 1 have wait-free strongly-linearizable implementations from fetch&add.","We also show that several objects with consensus number 2 have wait-free or lock-free implementations from other objects with consensus number 2.","In contrast, we prove that even when fetch&add, swap and test&set primitives are used, some objects with consensus number 2 do not have lock-free strongly-linearizable implementations.","This includes queues and stacks, as well as relaxed variants thereof."],"url":"http://arxiv.org/abs/2402.13618v1","category":"cs.DC"}
{"created":"2024-02-21 08:31:29","title":"Developing a $\u03bc$Bq/m$^{3}$ level $^{226}$Ra concentration in water measurement system for the Jiangmen Underground Neutrino Observatory","abstract":"The Jiangmen Underground Neutrino Observatory (JUNO), a 20~kton multi-purpose low background Liquid Scintillator (LS) detector, was proposed primarily to determine the neutrino mass ordering. To suppress the radioactivity from the surrounding rocks and tag cosmic muons, the JUNO central detector is submerged in a Water Cherenkov Detector (WCD). In addition to being used in the WCD, ultrapure water is used in LS filling, for which the $^{226}$Ra concentration in water needs to be less than 50~$\\mu$Bq/m$^3$. To precisely measure the $^{226}$Ra concentration in water, a 6.0~$\\mu$Bq/m$^3$ $^{226}$Ra concentration in water measurement system has been developed. In this paper, the detail of the measurement system as well as the $^{226}$Ra concentration measurement result in regular EWII ultrapure water will be presented.","sentences":["The Jiangmen Underground Neutrino Observatory (JUNO), a 20~kton multi-purpose low background Liquid Scintillator (LS) detector, was proposed primarily to determine the neutrino mass ordering.","To suppress the radioactivity from the surrounding rocks and tag cosmic muons, the JUNO central detector is submerged in a Water Cherenkov Detector (WCD).","In addition to being used in the WCD, ultrapure water is used in LS filling, for which the $^{226}$Ra concentration in water needs to be less than 50~$\\mu$Bq/m$^3$. To precisely measure the $^{226}$Ra concentration in water, a 6.0~$\\mu$Bq/m$^3$ $^{226}$Ra concentration in water measurement system has been developed.","In this paper, the detail of the measurement system as well as the $^{226}$Ra concentration measurement result in regular EWII ultrapure water will be presented."],"url":"http://arxiv.org/abs/2402.13614v1","category":"physics.ins-det"}
{"created":"2024-02-21 08:21:48","title":"Convergence Acceleration of Markov Chain Monte Carlo-based Gradient Descent by Deep Unfolding","abstract":"This study proposes a trainable sampling-based solver for combinatorial optimization problems (COPs) using a deep-learning technique called deep unfolding. The proposed solver is based on the Ohzeki method that combines Markov-chain Monte-Carlo (MCMC) and gradient descent, and its step sizes are trained by minimizing a loss function. In the training process, we propose a sampling-based gradient estimation that substitutes auto-differentiation with a variance estimation, thereby circumventing the failure of back propagation due to the non-differentiability of MCMC. The numerical results for a few COPs demonstrated that the proposed solver significantly accelerated the convergence speed compared with the original Ohzeki method.","sentences":["This study proposes a trainable sampling-based solver for combinatorial optimization problems (COPs) using a deep-learning technique called deep unfolding.","The proposed solver is based on the Ohzeki method that combines Markov-chain Monte-Carlo (MCMC) and gradient descent, and its step sizes are trained by minimizing a loss function.","In the training process, we propose a sampling-based gradient estimation that substitutes auto-differentiation with a variance estimation, thereby circumventing the failure of back propagation due to the non-differentiability of MCMC.","The numerical results for a few COPs demonstrated that the proposed solver significantly accelerated the convergence speed compared with the original Ohzeki method."],"url":"http://arxiv.org/abs/2402.13608v1","category":"cond-mat.dis-nn"}
{"created":"2024-02-21 08:10:43","title":"Breaking the HISCO Barrier: Automatic Occupational Standardization with OccCANINE","abstract":"This paper introduces a new tool, OccCANINE, to automatically transform occupational descriptions into the HISCO classification system. The manual work involved in processing and classifying occupational descriptions is error-prone, tedious, and time-consuming. We finetune a preexisting language model (CANINE) to do this automatically thereby performing in seconds and minutes what previously took days and weeks. The model is trained on 14 million pairs of occupational descriptions and HISCO codes in 13 different languages contributed by 22 different sources. Our approach is shown to have accuracy, recall and precision above 90 percent. Our tool breaks the metaphorical HISCO barrier and makes this data readily available for analysis of occupational structures with broad applicability in economics, economic history and various related disciplines.","sentences":["This paper introduces a new tool, OccCANINE, to automatically transform occupational descriptions into the HISCO classification system.","The manual work involved in processing and classifying occupational descriptions is error-prone, tedious, and time-consuming.","We finetune a preexisting language model (CANINE) to do this automatically thereby performing in seconds and minutes what previously took days and weeks.","The model is trained on 14 million pairs of occupational descriptions and HISCO codes in 13 different languages contributed by 22 different sources.","Our approach is shown to have accuracy, recall and precision above 90 percent.","Our tool breaks the metaphorical HISCO barrier and makes this data readily available for analysis of occupational structures with broad applicability in economics, economic history and various related disciplines."],"url":"http://arxiv.org/abs/2402.13604v1","category":"cs.CL"}
{"created":"2024-02-21 07:59:44","title":"Near-Field Multiuser Beam-Training for Extremely Large-Scale MIMO Systems","abstract":"Extremely large-scale multiple-input multiple-output (XL-MIMO) systems are capable of improving spectral efficiency by employing far more antennas than conventional massive MIMO at the base station (BS). However, beam training in multiuser XL-MIMO systems is challenging. To tackle these issues, we conceive a three-phase graph neural network (GNN)-based beam training scheme for multiuser XL-MIMO systems. In the first phase, only far-field wide beams have to be tested for each user and the GNN is utilized to map the beamforming gain information of the far-field wide beams to the optimal near-field beam for each user. In addition, the proposed GNN-based scheme can exploit the position-correlation between adjacent users for further improvement of the accuracy of beam training. In the second phase, a beam allocation scheme based on the probability vectors produced at the outputs of GNNs is proposed to address the above beam-direction conflicts between users. In the third phase, the hybrid TBF is designed for further reducing the inter-user interference. Our simulation results show that the proposed scheme improves the beam training performance of the benchmarks. Moreover, the performance of the proposed beam training scheme approaches that of an exhaustive search, despite requiring only about 7% of the pilot overhead.","sentences":["Extremely large-scale multiple-input multiple-output (XL-MIMO) systems are capable of improving spectral efficiency by employing far more antennas than conventional massive MIMO at the base station (BS).","However, beam training in multiuser XL-MIMO systems is challenging.","To tackle these issues, we conceive a three-phase graph neural network (GNN)-based beam training scheme for multiuser XL-MIMO systems.","In the first phase, only far-field wide beams have to be tested for each user and the GNN is utilized to map the beamforming gain information of the far-field wide beams to the optimal near-field beam for each user.","In addition, the proposed GNN-based scheme can exploit the position-correlation between adjacent users for further improvement of the accuracy of beam training.","In the second phase, a beam allocation scheme based on the probability vectors produced at the outputs of GNNs is proposed to address the above beam-direction conflicts between users.","In the third phase, the hybrid TBF is designed for further reducing the inter-user interference.","Our simulation results show that the proposed scheme improves the beam training performance of the benchmarks.","Moreover, the performance of the proposed beam training scheme approaches that of an exhaustive search, despite requiring only about 7% of the pilot overhead."],"url":"http://arxiv.org/abs/2402.13597v1","category":"cs.IT"}
{"created":"2024-02-21 07:50:55","title":"Hypercomplex structures arising from twistor spaces","abstract":"A hyperk\\\"ahler manifold is defined as a Riemannian manifold endowed with three covariantly constant complex structures that are quaternionically related. A twistor space is characterized as a holomorphic fiber bundle $p: \\mathcal{Z} \\rightarrow \\mathbb{CP}^1$ possesses properties such as a family of holomorphic sections whose normal bundle is $\\bigoplus^{2n}\\mathcal{O}(1)$, a holomorphic section of $\\Lambda^2(N\\mathcal{Z})\\otimes p^*(\\mathcal{O}(2))$ that defines a symplectic form on each fiber, and a compatible real structure. According to the Hitchin-Karlhede-Lindstr\\\"om-Ro\\v{c}ek theorem (Comm. Math. Phys., 108(4):535-589, 1987), there exists a hyperk\\\"ahler metric on the parameter space $M$ for the real sections of $\\mathcal{Z}$. Utilizing the Kodaira-Spencer deformation theory, we facilitate the construction of a hypercomplex structure on $M$, predicated upon more relaxed presuppositions concerning $\\mathcal{Z}$. This effort enriches our understanding of the classical theorem by Hitchin-Karlhede-Lindstr\\\"om-Ro\\v{c}ek.","sentences":["A hyperk\\\"ahler manifold is defined as a Riemannian manifold endowed with three covariantly constant complex structures that are quaternionically related.","A twistor space is characterized as a holomorphic fiber bundle $p: \\mathcal{Z} \\rightarrow \\mathbb{CP}^1$ possesses properties such as a family of holomorphic sections whose normal bundle is $\\bigoplus^{2n}\\mathcal{O}(1)$, a holomorphic section of $\\Lambda^2(N\\mathcal{Z})\\otimes p^*(\\mathcal{O}(2))$ that defines a symplectic form on each fiber, and a compatible real structure.","According to the Hitchin-Karlhede-Lindstr\\\"om-Ro\\v{c}ek theorem (Comm.","Math.","Phys., 108(4):535-589, 1987), there exists a hyperk\\\"ahler metric on the parameter space $M$ for the real sections of $\\mathcal{Z}$. Utilizing the Kodaira-Spencer deformation theory, we facilitate the construction of a hypercomplex structure on $M$, predicated upon more relaxed presuppositions concerning $\\mathcal{Z}$. This effort enriches our understanding of the classical theorem by Hitchin-Karlhede-Lindstr\\\"om-Ro\\v{c}ek."],"url":"http://arxiv.org/abs/2402.13592v1","category":"math.DG"}
{"created":"2024-02-21 07:47:01","title":"On 1-skeleton of the cut polytopes","abstract":"Given an undirected graph $G = (V,E)$, the cut polytope $\\mathrm{CUT}(G)$ is defined as the convex hull of the incidence vectors of all cuts in $G$. The 1-skeleton of $\\mathrm{CUT}(G)$ is a graph whose vertex set is the vertex set of the polytope, and the edge set is the set of geometric edges or one-dimensional faces of the polytope. We study the diameter and the clique number of 1-skeleton of cut polytopes for several classes of graphs. These characteristics are of interest since they estimate the computational complexity of the max-cut problem for certain computational models and classes of algorithms. It is established that while the diameter of the 1-skeleton of a cut polytope does not exceed $|V|-1$ for any connected graph, the clique number varies significantly depending on the class of graphs. For trees, cacti, and almost trees (2), the clique number is linear in the dimension, whereas for complete bipartite and $k$-partite graphs, it is superpolynomial.","sentences":["Given an undirected graph $G = (V,E)$, the cut polytope $\\mathrm{CUT}(G)$ is defined as the convex hull of the incidence vectors of all cuts in $G$. The 1-skeleton of $\\mathrm{CUT}(G)$ is a graph whose vertex set is the vertex set of the polytope, and the edge set is the set of geometric edges or one-dimensional faces of the polytope.","We study the diameter and the clique number of 1-skeleton of cut polytopes for several classes of graphs.","These characteristics are of interest since they estimate the computational complexity of the max-cut problem for certain computational models and classes of algorithms.","It is established that while the diameter of the 1-skeleton of a cut polytope does not exceed $|V|-1$ for any connected graph, the clique number varies significantly depending on the class of graphs.","For trees, cacti, and almost trees (2), the clique number is linear in the dimension, whereas for complete bipartite and $k$-partite graphs, it is superpolynomial."],"url":"http://arxiv.org/abs/2402.13591v1","category":"math.CO"}
{"created":"2024-02-21 07:45:05","title":"Tunable topological phases in nanographene-based spin-1/2 alternating-exchange Heisenberg chains","abstract":"Unlocking the potential of topological order within many-body spin systems has long been a central pursuit in the realm of quantum materials. Despite extensive efforts, the quest for a versatile platform enabling site-selective spin manipulation, essential for tuning and probing diverse topological phases, has persisted. Here, we utilize on-surface synthesis to construct spin-1/2 alternating-exchange Heisenberg (AH) chains[1] with antiferromagnetic couplings $J_1$ and $J_2$ by covalently linking Clar's goblets -- nanographenes each hosting two antiferromagnetically-coupled unpaired electrons[2]. Utilizing scanning tunneling microscopy, we exert atomic-scale control over the spin chain lengths, parities and exchange-coupling terminations, and probe their magnetic response by means of inelastic tunneling spectroscopy. Our investigation confirms the gapped nature of bulk excitations in the chains, known as triplons[3]. Besides, the triplon dispersion relation is successfully extracted from the spatial variation of tunneling spectral amplitudes. Furthermore, depending on the parity and termination of chains, we observe varying numbers of in-gap $S=1/2$ edge spins, enabling the determination of the degeneracy of distinct topological ground states in the thermodynamic limit-either 1, 2, or 4. By monitoring interactions between these edge spins, we identify the exponential decay of spin correlations. Our experimental findings, corroborated by theoretical calculations, present a phase-controlled many-body platform, opening promising avenues toward the development of spin-based quantum devices.","sentences":["Unlocking the potential of topological order within many-body spin systems has long been a central pursuit in the realm of quantum materials.","Despite extensive efforts, the quest for a versatile platform enabling site-selective spin manipulation, essential for tuning and probing diverse topological phases, has persisted.","Here, we utilize on-surface synthesis to construct spin-1/2 alternating-exchange Heisenberg (AH) chains[1] with antiferromagnetic couplings $J_1$ and $J_2$ by covalently linking Clar's goblets -- nanographenes each hosting two antiferromagnetically-coupled unpaired electrons[2].","Utilizing scanning tunneling microscopy, we exert atomic-scale control over the spin chain lengths, parities and exchange-coupling terminations, and probe their magnetic response by means of inelastic tunneling spectroscopy.","Our investigation confirms the gapped nature of bulk excitations in the chains, known as triplons[3].","Besides, the triplon dispersion relation is successfully extracted from the spatial variation of tunneling spectral amplitudes.","Furthermore, depending on the parity and termination of chains, we observe varying numbers of in-gap $S=1/2$ edge spins, enabling the determination of the degeneracy of distinct topological ground states in the thermodynamic limit-either 1, 2, or 4.","By monitoring interactions between these edge spins, we identify the exponential decay of spin correlations.","Our experimental findings, corroborated by theoretical calculations, present a phase-controlled many-body platform, opening promising avenues toward the development of spin-based quantum devices."],"url":"http://arxiv.org/abs/2402.13590v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-21 07:36:51","title":"Delay-Aware Semantic Sampling in Power Electronic Systems","abstract":"In power electronic systems (PES), attacks on data availability such as latency attacks, data dropouts, and time-synchronization attacks (TSAs) continue to pose significant threats to both the communication network and the control system performance. As per the conventional norms of communication engineering, PES still rely on time synchronized sampling, which translates every received message with equal importance. In this paper, we go beyond event-triggered sampling/estimation to integrate semantic principles into the sampling process for each distributed energy resource (DER), which not only compensates for delayed communicated signals by reconstruction of a new signal from the inner control layer dynamics, but also evaluates the reconstruction stage using key semantic requirements, namely Freshness, Relevance and Priority for good dynamic performance. As a result, the sparsity provided by event-driven sampling of internal control loop dynamics translates as semantics in PES. The proposed scheme has been extensively tested and validated on a modified IEEE 37-bus AC distribution system, under many operating conditions and noisy environment in OPAL-RT environment to establish its robustness, model-free design ability and adaptive behavior to dynamic cyber graph topologies.","sentences":["In power electronic systems (PES), attacks on data availability such as latency attacks, data dropouts, and time-synchronization attacks (TSAs) continue to pose significant threats to both the communication network and the control system performance.","As per the conventional norms of communication engineering, PES still rely on time synchronized sampling, which translates every received message with equal importance.","In this paper, we go beyond event-triggered sampling/estimation to integrate semantic principles into the sampling process for each distributed energy resource (DER), which not only compensates for delayed communicated signals by reconstruction of a new signal from the inner control layer dynamics, but also evaluates the reconstruction stage using key semantic requirements, namely Freshness, Relevance and Priority for good dynamic performance.","As a result, the sparsity provided by event-driven sampling of internal control loop dynamics translates as semantics in PES.","The proposed scheme has been extensively tested and validated on a modified IEEE 37-bus AC distribution system, under many operating conditions and noisy environment in OPAL-RT environment to establish its robustness, model-free design ability and adaptive behavior to dynamic cyber graph topologies."],"url":"http://arxiv.org/abs/2402.13586v1","category":"eess.SY"}
{"created":"2024-02-21 07:27:18","title":"LongWanjuan: Towards Systematic Measurement for Long Text Quality","abstract":"The quality of training data are crucial for enhancing the long-text capabilities of foundation models. Despite existing efforts to refine data quality through heuristic rules and evaluations based on data diversity and difficulty, there's a lack of systematic approaches specifically tailored for assessing long texts. Addressing this gap, our work systematically measures the quality of long texts by evaluating three fundamental linguistic dimensions: coherence, cohesion, and complexity. Drawing inspiration from the aforementioned three dimensions, we introduce a suite of metrics designed to evaluate the quality of long texts, encompassing both statistical and pre-trained language model-based ones. Leveraging these metrics, we present LongWanjuan, a bilingual dataset specifically tailored to enhance the training of language models for long-text tasks with over 160B tokens. In LongWanjuan, we categorize long texts into holistic, aggregated, and chaotic types, enabling a detailed analysis of long-text quality. Furthermore, we devise a data mixture recipe that strategically balances different types of long texts within LongWanjuan, leading to significant improvements in model performance on long-text tasks. The code and dataset are available at https://github.com/OpenLMLab/LongWanjuan.","sentences":["The quality of training data are crucial for enhancing the long-text capabilities of foundation models.","Despite existing efforts to refine data quality through heuristic rules and evaluations based on data diversity and difficulty, there's a lack of systematic approaches specifically tailored for assessing long texts.","Addressing this gap, our work systematically measures the quality of long texts by evaluating three fundamental linguistic dimensions: coherence, cohesion, and complexity.","Drawing inspiration from the aforementioned three dimensions, we introduce a suite of metrics designed to evaluate the quality of long texts, encompassing both statistical and pre-trained language model-based ones.","Leveraging these metrics, we present LongWanjuan, a bilingual dataset specifically tailored to enhance the training of language models for long-text tasks with over 160B tokens.","In LongWanjuan, we categorize long texts into holistic, aggregated, and chaotic types, enabling a detailed analysis of long-text quality.","Furthermore, we devise a data mixture recipe that strategically balances different types of long texts within LongWanjuan, leading to significant improvements in model performance on long-text tasks.","The code and dataset are available at https://github.com/OpenLMLab/LongWanjuan."],"url":"http://arxiv.org/abs/2402.13583v1","category":"cs.CL"}
{"created":"2024-02-21 07:16:29","title":"BBA: Bi-Modal Behavioral Alignment for Reasoning with Large Vision-Language Models","abstract":"Multimodal reasoning stands as a pivotal capability for large vision-language models (LVLMs). The integration with Domain-Specific Languages (DSL), offering precise visual representations, equips these models with the opportunity to execute more accurate reasoning in complex and professional domains. However, the vanilla Chain-of-Thought (CoT) prompting method faces challenges in effectively leveraging the unique strengths of visual and DSL representations, primarily due to their differing reasoning mechanisms. Additionally, it often falls short in addressing critical steps in multi-step reasoning tasks. To mitigate these challenges, we introduce the \\underline{B}i-Modal \\underline{B}ehavioral \\underline{A}lignment (BBA) prompting method, designed to maximize the potential of DSL in augmenting complex multi-modal reasoning tasks. This method initiates by guiding LVLMs to create separate reasoning chains for visual and DSL representations. Subsequently, it aligns these chains by addressing any inconsistencies, thus achieving a cohesive integration of behaviors from different modalities. Our experiments demonstrate that BBA substantially improves the performance of GPT-4V(ision) on geometry problem solving ($28.34\\% \\to 34.22\\%$), chess positional advantage prediction ($42.08\\% \\to 46.99\\%$) and molecular property prediction ($77.47\\% \\to 83.52\\%$).","sentences":["Multimodal reasoning stands as a pivotal capability for large vision-language models (LVLMs).","The integration with Domain-Specific Languages (DSL), offering precise visual representations, equips these models with the opportunity to execute more accurate reasoning in complex and professional domains.","However, the vanilla Chain-of-Thought (CoT) prompting method faces challenges in effectively leveraging the unique strengths of visual and DSL representations, primarily due to their differing reasoning mechanisms.","Additionally, it often falls short in addressing critical steps in multi-step reasoning tasks.","To mitigate these challenges, we introduce the \\underline{B}i-Modal \\underline{B}ehavioral \\underline{A}lignment (BBA) prompting method, designed to maximize the potential of DSL in augmenting complex multi-modal reasoning tasks.","This method initiates by guiding LVLMs to create separate reasoning chains for visual and DSL representations.","Subsequently, it aligns these chains by addressing any inconsistencies, thus achieving a cohesive integration of behaviors from different modalities.","Our experiments demonstrate that BBA substantially improves the performance of GPT-4V(ision) on geometry problem solving ($28.34\\% \\to 34.22\\%$), chess positional advantage prediction ($42.08\\% \\to 46.99\\%$) and molecular property prediction ($77.47\\% \\to 83.52\\%$)."],"url":"http://arxiv.org/abs/2402.13577v1","category":"cs.CL"}
{"created":"2024-02-21 06:57:33","title":"The Effect of AGN Feedback on the Lyman-\u03b1 Forest Signature of Galaxy Protoclusters at z~2.3","abstract":"The intergalactic medium (IGM) in the vicinity of galaxy protoclusters are interesting testbeds to study complex baryonic effects such as gravitational shocks and feedback. Here, we utilize hydrodynamical simulations from the SIMBA and The Three Hundred suites to study the mechanisms influencing large-scale Lyman-$\\alpha$ transmission in 2<z<2.5 protoclusters observed in the COSMOS field. We focus on the matter overdensity-Lyman-$\\alpha$ transmission relation $(\\delta_m-\\delta_F)$ on Megaparsec-scales in these protoclusters, which is hypothesized to be sensitive to the feedback implementations. The lower-density regions represented by the SIMBA-100 cosmological volume trace the power-law $\\delta_m-\\delta_F$ relationship often known as the fluctuating Gunn-Peterson approximation (FGPA). This trend is continued into higher-density regions covered by the 300-GadgetMUSIC simulations that implement stellar feedback only. The 300-GadgetX and 300-SIMBA simulations, with AGN thermal and AGN jet feedback respectively, exhibit progressively more Lyman-$\\alpha$ transmission at fixed overdensity. Compared with the 7 protoclusters observed in the CLAMATO$\\times$COSTCO data, only 2 appear consistent with the FGPA. The others exhibit clear deviations: 4 follow the trend of AGN X-ray thermal feedback models while the COSTCO-I protocluster appears to reflect intense jet feedback. The large discrepancy with the stellar-feedback-only 300-GadgetMUSIC model disfavours large-scale heating from gravitational collapse and/or stellar feedback. This indicates that some form of AGN feedback is likely at play in the observed protoclusters, and possibly long-ranged AGN jets in the case of COSTCO-I. While more detailed and resolved simulations are required to move forward, our findings open new avenues for probing AGN feedback at Cosmic Noon.","sentences":["The intergalactic medium (IGM) in the vicinity of galaxy protoclusters are interesting testbeds to study complex baryonic effects such as gravitational shocks and feedback.","Here, we utilize hydrodynamical simulations from the SIMBA and The Three Hundred suites to study the mechanisms influencing large-scale Lyman-$\\alpha$ transmission in 2<z<2.5 protoclusters observed in the COSMOS field.","We focus on the matter overdensity-Lyman-$\\alpha$ transmission relation $(\\delta_m-\\delta_F)$ on Megaparsec-scales in these protoclusters, which is hypothesized to be sensitive to the feedback implementations.","The lower-density regions represented by the SIMBA-100 cosmological volume trace the power-law $\\delta_m-\\delta_F$ relationship often known as the fluctuating Gunn-Peterson approximation (FGPA).","This trend is continued into higher-density regions covered by the 300-GadgetMUSIC simulations that implement stellar feedback only.","The 300-GadgetX and 300-SIMBA simulations, with AGN thermal and AGN jet feedback respectively, exhibit progressively more Lyman-$\\alpha$ transmission at fixed overdensity.","Compared with the 7 protoclusters observed in the CLAMATO$\\times$COSTCO data, only 2 appear consistent with the FGPA.","The others exhibit clear deviations: 4 follow the trend of AGN X-ray thermal feedback models while the COSTCO-I protocluster appears to reflect intense jet feedback.","The large discrepancy with the stellar-feedback-only 300-GadgetMUSIC model disfavours large-scale heating from gravitational collapse and/or stellar feedback.","This indicates that some form of AGN feedback is likely at play in the observed protoclusters, and possibly long-ranged AGN jets in the case of COSTCO-I. While more detailed and resolved simulations are required to move forward, our findings open new avenues for probing AGN feedback at Cosmic Noon."],"url":"http://arxiv.org/abs/2402.13568v1","category":"astro-ph.GA"}
{"created":"2024-02-21 06:34:32","title":"Design and characterization of individual addressing optics based on multi-channel acousto-optic modulator for $^{171}$Yb$^+$ qubits","abstract":"We present the design and characterization of individual addressing optics based on a multi-channel acousto-optic modulator (AOM) for trapped ytterbium-171 ions. The design parameters of the individual addressing system were determined based on the tradeoff between the expected crosstalk and the required numerical aperture of the projection objective lens. The target beam diameter and separation were 1.90 $\\mu$m and 4.28 $\\mu$m, respectively. The individual beams shaped by the projection optics were characterized by an imaging sensor and a field probe ion. The resulting effective beam diameters and separations were approximately 2.34--2.36 $\\mu$m and 4.31 $\\mu$m, respectively, owing to residual aberration.","sentences":["We present the design and characterization of individual addressing optics based on a multi-channel acousto-optic modulator (AOM) for trapped ytterbium-171 ions.","The design parameters of the individual addressing system were determined based on the tradeoff between the expected crosstalk and the required numerical aperture of the projection objective lens.","The target beam diameter and separation were 1.90 $\\mu$m and 4.28 $\\mu$m, respectively.","The individual beams shaped by the projection optics were characterized by an imaging sensor and a field probe ion.","The resulting effective beam diameters and separations were approximately 2.34--2.36 $\\mu$m and 4.31 $\\mu$m, respectively, owing to residual aberration."],"url":"http://arxiv.org/abs/2402.13560v1","category":"quant-ph"}
{"created":"2024-02-21 06:26:48","title":"Mutual information scrambling in Ising spin chain","abstract":"We consider a chain of spin-half particles of a finite length, evolved with the mixed-field Ising Hamiltonian and impose open boundary condition. We simulate the time evolution of entanglement entropy and mutual information following quench from the N\\'eel state in this system using tensor networks. We find that the entanglement entropy for non-integrable systems saturates to a constant value at late times, however it continues to oscillate for integrable systems. We also find that mutual information peaks as a function of distance between intervals decay faster for non-integrable systems compared to integrable systems, in agreement with the conclusion of \\cite{Alba:2019ybw} for XXZ chains. We compare the oscillations in entanglement entropy evolution obtained from simulations in the integrable case with analytic results from quasi-particle picture and find agreement.","sentences":["We consider a chain of spin-half particles of a finite length, evolved with the mixed-field Ising Hamiltonian and impose open boundary condition.","We simulate the time evolution of entanglement entropy and mutual information following quench from the N\\'eel state in this system using tensor networks.","We find that the entanglement entropy for non-integrable systems saturates to a constant value at late times, however it continues to oscillate for integrable systems.","We also find that mutual information peaks as a function of distance between intervals decay faster for non-integrable systems compared to integrable systems, in agreement with the conclusion of \\cite{Alba:2019ybw} for XXZ chains.","We compare the oscillations in entanglement entropy evolution obtained from simulations in the integrable case with analytic results from quasi-particle picture and find agreement."],"url":"http://arxiv.org/abs/2402.13558v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-21 06:23:31","title":"Secrecy Performance Analysis of Space-to-Ground Optical Satellite Communications","abstract":"Free-space optics (FSO)-based satellite communication systems have recently received considerable attention due to their enhanced capacity compared to their radio frequency (RF) counterparts. This paper analyzes the performance of physical layer security of space-to-ground intensity modulation/direct detection FSO satellite links under the effect of atmospheric loss, misalignment, cloud attenuation, and atmospheric turbulence-induced fading. Specifically, a wiretap channel consisting of a legitimate transmitter Alice (i.e., the satellite), a legitimate user Bob, and an eavesdropper Eve over turbulence channels modeled by the Fisher-Snedecor $\\mathcal{F}$ distribution is considered. The secrecy performance in terms of the average secrecy capacity, secrecy outage probability, and strictly positive secrecy capacity are derived in closed-form. Simulation results reveal significant impacts of satellite altitude, zenith angle, and turbulence strength on the secrecy performance.","sentences":["Free-space optics (FSO)-based satellite communication systems have recently received considerable attention due to their enhanced capacity compared to their radio frequency (RF) counterparts.","This paper analyzes the performance of physical layer security of space-to-ground intensity modulation/direct detection FSO satellite links under the effect of atmospheric loss, misalignment, cloud attenuation, and atmospheric turbulence-induced fading.","Specifically, a wiretap channel consisting of a legitimate transmitter Alice (i.e., the satellite), a legitimate user Bob, and an eavesdropper Eve over turbulence channels modeled by the Fisher-Snedecor $\\mathcal{F}$ distribution is considered.","The secrecy performance in terms of the average secrecy capacity, secrecy outage probability, and strictly positive secrecy capacity are derived in closed-form.","Simulation results reveal significant impacts of satellite altitude, zenith angle, and turbulence strength on the secrecy performance."],"url":"http://arxiv.org/abs/2402.13554v1","category":"cs.IT"}
{"created":"2024-02-21 06:14:17","title":"Confluence of Logically Constrained Rewrite Systems Revisited","abstract":"We show that (local) confluence of terminating locally constrained rewrite systems is undecidable, even when the underlying theory is decidable. Several confluence criteria for logically constrained rewrite systems are known. These were obtained by replaying existing proofs for plain term rewrite systems in a constrained setting, involving a non-trivial effort. We present a simple transformation from logically constrained rewrite systems to term rewrite systems such that critical pairs of the latter correspond to constrained critical pairs of the former. The usefulness of the transformation is illustrated by lifting the advanced confluence results based on (almost) development closed critical pairs as well as on parallel critical pairs to the constrained setting.","sentences":["We show that (local) confluence of terminating locally constrained rewrite systems is undecidable, even when the underlying theory is decidable.","Several confluence criteria for logically constrained rewrite systems are known.","These were obtained by replaying existing proofs for plain term rewrite systems in a constrained setting, involving a non-trivial effort.","We present a simple transformation from logically constrained rewrite systems to term rewrite systems such that critical pairs of the latter correspond to constrained critical pairs of the former.","The usefulness of the transformation is illustrated by lifting the advanced confluence results based on (almost) development closed critical pairs as well as on parallel critical pairs to the constrained setting."],"url":"http://arxiv.org/abs/2402.13552v1","category":"cs.LO"}
{"created":"2024-02-21 06:10:27","title":"Q-learning-based Joint Design of Adaptive Modulation and Precoding for Physical Layer Security in Visible Light Communications","abstract":"There has been an increasing interest in physical layer security (PLS), which, compared with conventional cryptography, offers a unique approach to guaranteeing information confidentiality against eavesdroppers. In this paper, we study a joint design of adaptive $M$-ary pulse amplitude modulation (PAM) and precoding, which aims to optimize wiretap visible-light channels' secrecy capacity and bit error rate (BER) performances. The proposed design is motivated by higher-order modulation, which results in better secrecy capacity at the expense of a higher BER. On the other hand, a proper precoding design, which can manipulate the received signal quality at the legitimate user and the eavesdropper, can also enhance secrecy performance and influence the BER. A reward function that considers the secrecy capacity and the BERs of the legitimate user's (Bob) and the eavesdropper's (Eve) channels is introduced and maximized. Due to the non-linearity and complexity of the reward function, it is challenging to solve the optical design using classical optimization techniques. Therefore, reinforcement learning-based designs using Q-learning and Deep Q-learning are proposed to maximize the reward function. Simulation results verify that compared with the baseline designs, the proposed joint designs achieve better reward values while maintaining the BER of Bob's channel (Eve's channel) well below (above) the pre-FEC (forward error correction) BER threshold.","sentences":["There has been an increasing interest in physical layer security (PLS), which, compared with conventional cryptography, offers a unique approach to guaranteeing information confidentiality against eavesdroppers.","In this paper, we study a joint design of adaptive $M$-ary pulse amplitude modulation (PAM) and precoding, which aims to optimize wiretap visible-light channels' secrecy capacity and bit error rate (BER) performances.","The proposed design is motivated by higher-order modulation, which results in better secrecy capacity at the expense of a higher BER.","On the other hand, a proper precoding design, which can manipulate the received signal quality at the legitimate user and the eavesdropper, can also enhance secrecy performance and influence the BER.","A reward function that considers the secrecy capacity and the BERs of the legitimate user's (Bob) and the eavesdropper's (Eve) channels is introduced and maximized.","Due to the non-linearity and complexity of the reward function, it is challenging to solve the optical design using classical optimization techniques.","Therefore, reinforcement learning-based designs using Q-learning and Deep Q-learning are proposed to maximize the reward function.","Simulation results verify that compared with the baseline designs, the proposed joint designs achieve better reward values while maintaining the BER of Bob's channel (Eve's channel) well below (above) the pre-FEC (forward error correction) BER threshold."],"url":"http://arxiv.org/abs/2402.13549v1","category":"cs.IT"}
{"created":"2024-02-21 06:07:33","title":"DiffPLF: A Conditional Diffusion Model for Probabilistic Forecasting of EV Charging Load","abstract":"Due to the vast electric vehicle (EV) penetration to distribution grid, charging load forecasting is essential to promote charging station operation and demand-side management.However, the stochastic charging behaviors and associated exogenous factors render future charging load patterns quite volatile and hard to predict. Accordingly, we devise a novel Diffusion model termed DiffPLF for Probabilistic Load Forecasting of EV charging, which can explicitly approximate the predictive load distribution conditioned on historical data and related covariates. Specifically, we leverage a denoising diffusion model, which can progressively convert the Gaussian prior to real time-series data by learning a reversal of the diffusion process. Besides, we couple such diffusion model with a cross-attention-based conditioning mechanism to execute conditional generation for possible charging demand profiles. We also propose a task-informed fine-tuning technique to better adapt DiffPLF to the probabilistic time-series forecasting task and acquire more accurate and reliable predicted intervals. Finally, we conduct multiple experiments to validate the superiority of DiffPLF to predict complex temporal patterns of erratic charging load and carry out controllable generation based on certain covariate. Results demonstrate that we can attain a notable rise of 39.58% and 49.87% on MAE and CRPS respectively compared to the conventional method.","sentences":["Due to the vast electric vehicle (EV) penetration to distribution grid, charging load forecasting is essential to promote charging station operation and demand-side management.","However, the stochastic charging behaviors and associated exogenous factors render future charging load patterns quite volatile and hard to predict.","Accordingly, we devise a novel Diffusion model termed DiffPLF for Probabilistic Load Forecasting of EV charging, which can explicitly approximate the predictive load distribution conditioned on historical data and related covariates.","Specifically, we leverage a denoising diffusion model, which can progressively convert the Gaussian prior to real time-series data by learning a reversal of the diffusion process.","Besides, we couple such diffusion model with a cross-attention-based conditioning mechanism to execute conditional generation for possible charging demand profiles.","We also propose a task-informed fine-tuning technique to better adapt DiffPLF to the probabilistic time-series forecasting task and acquire more accurate and reliable predicted intervals.","Finally, we conduct multiple experiments to validate the superiority of DiffPLF to predict complex temporal patterns of erratic charging load and carry out controllable generation based on certain covariate.","Results demonstrate that we can attain a notable rise of 39.58% and 49.87% on MAE and CRPS respectively compared to the conventional method."],"url":"http://arxiv.org/abs/2402.13548v1","category":"cs.LG"}
{"created":"2024-02-21 05:37:25","title":"Scientific Impact of novel Instrumentation: the Case of MUSE","abstract":"In the process of transforming science cases into a viable and affordable design for a novel instrument, there is the problem of how to gauge their scientific impact, especially when they end up in competing top level requirements that can be incompatible with each other. This research note presents a case study for scientific impact of the integral field spectrograph MUSE in terms of number of refereed publications from 2014 to 2024 as a figure of merit, broken down by different research areas. The analysis is based on the Basic ESO Publication Statistics service (BEPS) and NASA's Astrophysics Data System (ADS).","sentences":["In the process of transforming science cases into a viable and affordable design for a novel instrument, there is the problem of how to gauge their scientific impact, especially when they end up in competing top level requirements that can be incompatible with each other.","This research note presents a case study for scientific impact of the integral field spectrograph MUSE in terms of number of refereed publications from 2014 to 2024 as a figure of merit, broken down by different research areas.","The analysis is based on the Basic ESO Publication Statistics service (BEPS) and NASA's Astrophysics Data System (ADS)."],"url":"http://arxiv.org/abs/2402.13540v1","category":"cs.DL"}
{"created":"2024-02-21 05:30:06","title":"The Staggered Mesh Method: Accurate Exact Exchange towards the Thermodynamic Limit for Solids","abstract":"In periodic systems, the Hartree-Fock (HF) exchange energy exhibits the slowest convergence of all HF energy components as the system size approaches the thermodynamic limit. We demonstrate that the recently proposed staggered mesh method for Fock exchange energy [Xing, Li, and Lin, Math. Comp., 2024], which is specifically designed to sidestep certain singularities in exchange energy evaluation, can expedite the finite-size convergence rate for the exact exchange energy across a range of insulators and semiconductors when compared to the regular and truncated Coulomb methods. This remains true even for two computationally cheaper versions of this new method, which we call Non-SCF and Split-SCF staggered mesh. Additionally, a sequence of numerical tests on simple solids showcases the staggered mesh method's ability to improve convergence towards the thermodynamic limit for band gaps, bulk moduli, equilibrium lattice dimensions, energies, and phonon force constants.","sentences":["In periodic systems, the Hartree-Fock (HF) exchange energy exhibits the slowest convergence of all HF energy components as the system size approaches the thermodynamic limit.","We demonstrate that the recently proposed staggered mesh method for Fock exchange energy [Xing, Li, and Lin, Math.","Comp., 2024], which is specifically designed to sidestep certain singularities in exchange energy evaluation, can expedite the finite-size convergence rate for the exact exchange energy across a range of insulators and semiconductors when compared to the regular and truncated Coulomb methods.","This remains true even for two computationally cheaper versions of this new method, which we call Non-SCF and Split-SCF staggered mesh.","Additionally, a sequence of numerical tests on simple solids showcases the staggered mesh method's ability to improve convergence towards the thermodynamic limit for band gaps, bulk moduli, equilibrium lattice dimensions, energies, and phonon force constants."],"url":"http://arxiv.org/abs/2402.13538v1","category":"physics.comp-ph"}
{"created":"2024-02-21 05:03:07","title":"Backdoor Attacks on Dense Passage Retrievers for Disseminating Misinformation","abstract":"Dense retrievers and retrieval-augmented language models have been widely used in various NLP applications. Despite being designed to deliver reliable and secure outcomes, the vulnerability of retrievers to potential attacks remains unclear, raising concerns about their security. In this paper, we introduce a novel scenario where the attackers aim to covertly disseminate targeted misinformation, such as hate speech or advertisement, through a retrieval system. To achieve this, we propose a perilous backdoor attack triggered by grammar errors in dense passage retrieval. Our approach ensures that attacked models can function normally for standard queries but are manipulated to return passages specified by the attacker when users unintentionally make grammatical mistakes in their queries. Extensive experiments demonstrate the effectiveness and stealthiness of our proposed attack method. When a user query is error-free, our model consistently retrieves accurate information while effectively filtering out misinformation from the top-k results. However, when a query contains grammar errors, our system shows a significantly higher success rate in fetching the targeted content.","sentences":["Dense retrievers and retrieval-augmented language models have been widely used in various NLP applications.","Despite being designed to deliver reliable and secure outcomes, the vulnerability of retrievers to potential attacks remains unclear, raising concerns about their security.","In this paper, we introduce a novel scenario where the attackers aim to covertly disseminate targeted misinformation, such as hate speech or advertisement, through a retrieval system.","To achieve this, we propose a perilous backdoor attack triggered by grammar errors in dense passage retrieval.","Our approach ensures that attacked models can function normally for standard queries but are manipulated to return passages specified by the attacker when users unintentionally make grammatical mistakes in their queries.","Extensive experiments demonstrate the effectiveness and stealthiness of our proposed attack method.","When a user query is error-free, our model consistently retrieves accurate information while effectively filtering out misinformation from the top-k results.","However, when a query contains grammar errors, our system shows a significantly higher success rate in fetching the targeted content."],"url":"http://arxiv.org/abs/2402.13532v1","category":"cs.CL"}
{"created":"2024-02-21 04:58:41","title":"Private Gradient Descent for Linear Regression: Tighter Error Bounds and Instance-Specific Uncertainty Estimation","abstract":"We provide an improved analysis of standard differentially private gradient descent for linear regression under the squared error loss. Under modest assumptions on the input, we characterize the distribution of the iterate at each time step.   Our analysis leads to new results on the algorithm's accuracy: for a proper fixed choice of hyperparameters, the sample complexity depends only linearly on the dimension of the data. This matches the dimension-dependence of the (non-private) ordinary least squares estimator as well as that of recent private algorithms that rely on sophisticated adaptive gradient-clipping schemes (Varshney et al., 2022; Liu et al., 2023).   Our analysis of the iterates' distribution also allows us to construct confidence intervals for the empirical optimizer which adapt automatically to the variance of the algorithm on a particular data set. We validate our theorems through experiments on synthetic data.","sentences":["We provide an improved analysis of standard differentially private gradient descent for linear regression under the squared error loss.","Under modest assumptions on the input, we characterize the distribution of the iterate at each time step.   ","Our analysis leads to new results on the algorithm's accuracy: for a proper fixed choice of hyperparameters, the sample complexity depends only linearly on the dimension of the data.","This matches the dimension-dependence of the (non-private) ordinary least squares estimator as well as that of recent private algorithms that rely on sophisticated adaptive gradient-clipping schemes (Varshney et al., 2022; Liu et al., 2023).   ","Our analysis of the iterates' distribution also allows us to construct confidence intervals for the empirical optimizer which adapt automatically to the variance of the algorithm on a particular data set.","We validate our theorems through experiments on synthetic data."],"url":"http://arxiv.org/abs/2402.13531v1","category":"cs.LG"}
{"created":"2024-02-21 04:54:17","title":"Dimensions of $\u03c4$-tilting modules over path algebras and preprojective algebras of Dynkin type","abstract":"In this paper, we introduce a new generating function called $d$-polynomial for the dimensions of $\\tau$-tilting modules over a given finite dimensional algebra. Firstly, we study basic properties of $d$-polynomials and show that it can be realized as a certain sum of the $f$-polynomials of the simplicial complexes arising from $\\tau$-rigid pairs. Secondly, we give explicit formulas of $d$-polynomials for preprojective algebras and path algebras of Dynkin quivers by using a close relation with $W$-Eulerian polynomials and $W$-Narayana polynomials. Thirdly, we consider the ordinary and exponential generating functions defined from $d$-polynomials and give closed-form expressions in the case of preprojective algebras and path algebras of Dynkin type $\\mathbb{A}$.","sentences":["In this paper, we introduce a new generating function called $d$-polynomial for the dimensions of $\\tau$-tilting modules over a given finite dimensional algebra.","Firstly, we study basic properties of $d$-polynomials and show that it can be realized as a certain sum of the $f$-polynomials of the simplicial complexes arising from $\\tau$-rigid pairs.","Secondly, we give explicit formulas of $d$-polynomials for preprojective algebras and path algebras of Dynkin quivers by using a close relation with $W$-Eulerian polynomials and $W$-Narayana polynomials.","Thirdly, we consider the ordinary and exponential generating functions defined from $d$-polynomials and give closed-form expressions in the case of preprojective algebras and path algebras of Dynkin type $\\mathbb{A}$."],"url":"http://arxiv.org/abs/2402.13527v1","category":"math.RT"}
{"created":"2024-02-21 04:09:23","title":"Engineering Hierarchical Symmetries","abstract":"We present a general driving protocol for many-body systems to generate a sequence of prethermal regimes, each exhibiting a lower symmetry than the preceding one. We provide an explicit construction of effective Hamiltonians exhibiting these symmetries. This imprints emergent quasi-conservation laws hierarchically, enabling us to engineer the respective symmetries and concomitant orders in nonequilibrium matter. We provide explicit examples, including spatiotemporal and topological phenomena, as well as a spin chain realizing the symmetry ladder $\\text{SU(2)}{\\rightarrow}\\text{U(1)} {\\rightarrow} \\mathbb{Z}_2{\\rightarrow} E$.","sentences":["We present a general driving protocol for many-body systems to generate a sequence of prethermal regimes, each exhibiting a lower symmetry than the preceding one.","We provide an explicit construction of effective Hamiltonians exhibiting these symmetries.","This imprints emergent quasi-conservation laws hierarchically, enabling us to engineer the respective symmetries and concomitant orders in nonequilibrium matter.","We provide explicit examples, including spatiotemporal and topological phenomena, as well as a spin chain realizing the symmetry ladder $\\text{SU(2)}{\\rightarrow}\\text{U(1)} {\\rightarrow} \\mathbb{Z}_2{\\rightarrow} E$."],"url":"http://arxiv.org/abs/2402.13519v1","category":"quant-ph"}
{"created":"2024-02-21 03:55:16","title":"Spin-selective transport in a correlated double quantum dot-Majorana wire system","abstract":"In this work we investigate the spin-dependent transport through a double quantum dot embedded in a ferromagnetic tunnel junction and side attached to a topological superconducting nanowire hosting Majorana zero-energy modes. We focus on the transport regime when the Majorana mode leaks into the double quantum dot competing with the two-stage Kondo effect and the ferromagnetic-contact-induced exchange field. In particular, we determine the system's spectral properties and analyze the temperature dependence of the spin-resolved linear conductance by means of the numerical renormalization group method. Our study reveals unique signatures of the interplay between the spin-resolved tunneling, the Kondo effect and the Majorana modes, which are visible in the transport characteristics. In particular, we uncover a competing character of the coupling to topological superconductor and that to ferromagnetic leads, which can be observed already for very low spin polarization of the electrodes. This is signaled by an almost complete quenching of the conductance in one of the spin channels which is revealed through perfect conductance spin polarization. Moreover, we show that the conductance spin polarization can change sign depending on the magnitude of spin imbalance in the leads and strength of interaction with topological wire. Thus, our work demonstrates that even minuscule spin polarization of tunneling processes can have large impact on the transport properties of the system.","sentences":["In this work we investigate the spin-dependent transport through a double quantum dot embedded in a ferromagnetic tunnel junction and side attached to a topological superconducting nanowire hosting Majorana zero-energy modes.","We focus on the transport regime when the Majorana mode leaks into the double quantum dot competing with the two-stage Kondo effect and the ferromagnetic-contact-induced exchange field.","In particular, we determine the system's spectral properties and analyze the temperature dependence of the spin-resolved linear conductance by means of the numerical renormalization group method.","Our study reveals unique signatures of the interplay between the spin-resolved tunneling, the Kondo effect and the Majorana modes, which are visible in the transport characteristics.","In particular, we uncover a competing character of the coupling to topological superconductor and that to ferromagnetic leads, which can be observed already for very low spin polarization of the electrodes.","This is signaled by an almost complete quenching of the conductance in one of the spin channels which is revealed through perfect conductance spin polarization.","Moreover, we show that the conductance spin polarization can change sign depending on the magnitude of spin imbalance in the leads and strength of interaction with topological wire.","Thus, our work demonstrates that even minuscule spin polarization of tunneling processes can have large impact on the transport properties of the system."],"url":"http://arxiv.org/abs/2402.13515v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-21 03:51:56","title":"Guac: Energy-Aware and SSA-Based Generation of Coarse-Grained Merged Accelerators from LLVM-IR","abstract":"Designing accelerators for resource- and power-constrained applications is a daunting task. High-level Synthesis (HLS) addresses these constraints through resource sharing, an optimization at the HLS binding stage that maps multiple operations to the same functional unit.   However, resource sharing is often limited to reusing instructions within a basic block. Instead of searching globally for the best control and dataflow graphs (CDFGs) to combine, it is constrained by existing instruction mappings and schedules.   Coarse-grained function merging (CGFM) at the intermediate representation (IR) level can reuse control and dataflow patterns without dealing with the post-scheduling complexity of mapping operations onto functional units, wires, and registers. The merged functions produced by CGFM can be translated to RTL by HLS, yielding Coarse Grained Merged Accelerators (CGMAs). CGMAs are especially profitable across applications with similar data- and control-flow patterns. Prior work has used CGFM to generate CGMAs without regard for which CGFM algorithms best optimize area, power, and energy costs.   We propose Guac, an energy-aware and SSA-based (static single assignment) CGMA generation methodology. Guac implements a novel ensemble of cost models for efficient CGMA generation. We also show that CGFM algorithms using SSA form to merge control- and dataflow graphs outperform prior non-SSA CGFM designs. We demonstrate significant area, power, and energy savings with respect to the state of the art. In particular, Guac more than doubles energy savings with respect to the closest related work while using a strong resource-sharing baseline.","sentences":["Designing accelerators for resource- and power-constrained applications is a daunting task.","High-level Synthesis (HLS) addresses these constraints through resource sharing, an optimization at the HLS binding stage that maps multiple operations to the same functional unit.   ","However, resource sharing is often limited to reusing instructions within a basic block.","Instead of searching globally for the best control and dataflow graphs (CDFGs) to combine, it is constrained by existing instruction mappings and schedules.   ","Coarse-grained function merging (CGFM) at the intermediate representation (IR) level can reuse control and dataflow patterns without dealing with the post-scheduling complexity of mapping operations onto functional units, wires, and registers.","The merged functions produced by CGFM can be translated to RTL by HLS, yielding Coarse Grained Merged Accelerators (CGMAs).","CGMAs are especially profitable across applications with similar data- and control-flow patterns.","Prior work has used CGFM to generate CGMAs without regard for which CGFM algorithms best optimize area, power, and energy costs.   ","We propose Guac, an energy-aware and SSA-based (static single assignment) CGMA generation methodology.","Guac implements a novel ensemble of cost models for efficient CGMA generation.","We also show that CGFM algorithms using SSA form to merge control- and dataflow graphs outperform prior non-SSA CGFM designs.","We demonstrate significant area, power, and energy savings with respect to the state of the art.","In particular, Guac more than doubles energy savings with respect to the closest related work while using a strong resource-sharing baseline."],"url":"http://arxiv.org/abs/2402.13513v1","category":"cs.AR"}
{"created":"2024-02-21 03:39:38","title":"Structural analysis and the sum of nodes' betweenness centrality in complex networks","abstract":"Structural analysis in network science is finding the information hidden from the topology structure of complex networks. Many methods have already been proposed in the research on the structural analysis of complex networks to find the different structural information of networks. In this work, the sum of nodes' betweenness centrality (SBC) is used as a new structural index to check how the structure of the complex networks changes in the process of the network's growth. We build two four different processes of network growth to check how the structure change will be manifested by the SBC. We find that when the networks are under Barab\\'asi-Albert rule, the value of SBC for each network grows like a logarithmic function. However, when the rule that guides the network's growth is the Erd\\H{o}s-R\\'enyi rule, the value of SBC will converge to a fixed value. It means the rules that guide the network's growth can be illustrated by the change of the SBC in the process of the network's growth. In other words, in the structure analysis of complex networks, the sum of nodes' betweenness centrality can be used as an index to check what kinds of rules guide the network's growth.","sentences":["Structural analysis in network science is finding the information hidden from the topology structure of complex networks.","Many methods have already been proposed in the research on the structural analysis of complex networks to find the different structural information of networks.","In this work, the sum of nodes' betweenness centrality (SBC) is used as a new structural index to check how the structure of the complex networks changes in the process of the network's growth.","We build two four different processes of network growth to check how the structure change will be manifested by the SBC.","We find that when the networks are under Barab\\'asi-Albert rule, the value of SBC for each network grows like a logarithmic function.","However, when the rule that guides the network's growth is the Erd\\H{o}s-R\\'enyi rule, the value of SBC will converge to a fixed value.","It means the rules that guide the network's growth can be illustrated by the change of the SBC in the process of the network's growth.","In other words, in the structure analysis of complex networks, the sum of nodes' betweenness centrality can be used as an index to check what kinds of rules guide the network's growth."],"url":"http://arxiv.org/abs/2402.13507v1","category":"physics.soc-ph"}
{"created":"2024-02-21 03:25:14","title":"Leveraging Translation For Optimal Recall: Tailoring LLM Personalization With User Profiles","abstract":"This paper explores a novel technique for improving recall in cross-language information retrieval (CLIR) systems using iterative query refinement grounded in the user's lexical-semantic space. The proposed methodology combines multi-level translation, semantic embedding-based expansion, and user profile-centered augmentation to address the challenge of matching variance between user queries and relevant documents. Through an initial BM25 retrieval, translation into intermediate languages, embedding lookup of similar terms, and iterative re-ranking, the technique aims to expand the scope of potentially relevant results personalized to the individual user. Comparative experiments on news and Twitter datasets demonstrate superior performance over baseline BM25 ranking for the proposed approach across ROUGE metrics. The translation methodology also showed maintained semantic accuracy through the multi-step process. This personalized CLIR framework paves the path for improved context-aware retrieval attentive to the nuances of user language.","sentences":["This paper explores a novel technique for improving recall in cross-language information retrieval (CLIR) systems using iterative query refinement grounded in the user's lexical-semantic space.","The proposed methodology combines multi-level translation, semantic embedding-based expansion, and user profile-centered augmentation to address the challenge of matching variance between user queries and relevant documents.","Through an initial BM25 retrieval, translation into intermediate languages, embedding lookup of similar terms, and iterative re-ranking, the technique aims to expand the scope of potentially relevant results personalized to the individual user.","Comparative experiments on news and Twitter datasets demonstrate superior performance over baseline BM25 ranking for the proposed approach across ROUGE metrics.","The translation methodology also showed maintained semantic accuracy through the multi-step process.","This personalized CLIR framework paves the path for improved context-aware retrieval attentive to the nuances of user language."],"url":"http://arxiv.org/abs/2402.13500v1","category":"cs.IR"}
{"created":"2024-02-21 03:21:14","title":"The Lay Person's Guide to Biomedicine: Orchestrating Large Language Models","abstract":"Automated lay summarisation (LS) aims to simplify complex technical documents into a more accessible format to non-experts. Existing approaches using pre-trained language models, possibly augmented with external background knowledge, tend to struggle with effective simplification and explanation. Moreover, automated methods that can effectively assess the `layness' of generated summaries are lacking. Recently, large language models (LLMs) have demonstrated a remarkable capacity for text simplification, background information generation, and text evaluation. This has motivated our systematic exploration into using LLMs to generate and evaluate lay summaries of biomedical articles. We propose a novel \\textit{Explain-then-Summarise} LS framework, which leverages LLMs to generate high-quality background knowledge to improve supervised LS. We also evaluate the performance of LLMs for zero-shot LS and propose two novel LLM-based LS evaluation metrics, which assess layness from multiple perspectives. Finally, we conduct a human assessment of generated lay summaries. Our experiments reveal that LLM-generated background information can support improved supervised LS. Furthermore, our novel zero-shot LS evaluation metric demonstrates a high degree of alignment with human preferences. We conclude that LLMs have an important part to play in improving both the performance and evaluation of LS methods.","sentences":["Automated lay summarisation (LS) aims to simplify complex technical documents into a more accessible format to non-experts.","Existing approaches using pre-trained language models, possibly augmented with external background knowledge, tend to struggle with effective simplification and explanation.","Moreover, automated methods that can effectively assess the `layness' of generated summaries are lacking.","Recently, large language models (LLMs) have demonstrated a remarkable capacity for text simplification, background information generation, and text evaluation.","This has motivated our systematic exploration into using LLMs to generate and evaluate lay summaries of biomedical articles.","We propose a novel \\textit{Explain-then-Summarise} LS framework, which leverages LLMs to generate high-quality background knowledge to improve supervised LS.","We also evaluate the performance of LLMs for zero-shot LS and propose two novel LLM-based LS evaluation metrics, which assess layness from multiple perspectives.","Finally, we conduct a human assessment of generated lay summaries.","Our experiments reveal that LLM-generated background information can support improved supervised LS.","Furthermore, our novel zero-shot LS evaluation metric demonstrates a high degree of alignment with human preferences.","We conclude that LLMs have an important part to play in improving both the performance and evaluation of LS methods."],"url":"http://arxiv.org/abs/2402.13498v1","category":"cs.CL"}
{"created":"2024-02-21 03:14:37","title":"Can One Embedding Fit All? A Multi-Interest Learning Paradigm Towards Improving User Interest Diversity Fairness","abstract":"Recommender systems (RSs) have gained widespread applications across various domains owing to the superior ability to capture users' interests. However, the complexity and nuanced nature of users' interests, which span a wide range of diversity, pose a significant challenge in delivering fair recommendations. In practice, user preferences vary significantly; some users show a clear preference toward certain item categories, while others have a broad interest in diverse ones. Even though it is expected that all users should receive high-quality recommendations, the effectiveness of RSs in catering to this disparate interest diversity remains under-explored.   In this work, we investigate whether users with varied levels of interest diversity are treated fairly. Our empirical experiments reveal an inherent disparity: users with broader interests often receive lower-quality recommendations. To mitigate this, we propose a multi-interest framework that uses multiple (virtual) interest embeddings rather than single ones to represent users. Specifically, the framework consists of stacked multi-interest representation layers, which include an interest embedding generator that derives virtual interests from shared parameters, and a center embedding aggregator that facilitates multi-hop aggregation. Experiments demonstrate the effectiveness of the framework in achieving better trade-off between fairness and utility across various datasets and backbones.","sentences":["Recommender systems (RSs) have gained widespread applications across various domains owing to the superior ability to capture users' interests.","However, the complexity and nuanced nature of users' interests, which span a wide range of diversity, pose a significant challenge in delivering fair recommendations.","In practice, user preferences vary significantly; some users show a clear preference toward certain item categories, while others have a broad interest in diverse ones.","Even though it is expected that all users should receive high-quality recommendations, the effectiveness of RSs in catering to this disparate interest diversity remains under-explored.   ","In this work, we investigate whether users with varied levels of interest diversity are treated fairly.","Our empirical experiments reveal an inherent disparity: users with broader interests often receive lower-quality recommendations.","To mitigate this, we propose a multi-interest framework that uses multiple (virtual) interest embeddings rather than single ones to represent users.","Specifically, the framework consists of stacked multi-interest representation layers, which include an interest embedding generator that derives virtual interests from shared parameters, and a center embedding aggregator that facilitates multi-hop aggregation.","Experiments demonstrate the effectiveness of the framework in achieving better trade-off between fairness and utility across various datasets and backbones."],"url":"http://arxiv.org/abs/2402.13495v1","category":"cs.IR"}
{"created":"2024-02-21 03:05:50","title":"Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval Augmentation to Language Models","abstract":"While large language models (LMs) demonstrate remarkable performance, they encounter challenges in providing accurate responses when queried for information beyond their pre-trained memorization. Although augmenting them with relevant external information can mitigate these issues, failure to consider the necessity of retrieval may adversely affect overall performance. Previous research has primarily focused on examining how entities influence retrieval models and knowledge recall in LMs, leaving other aspects relatively unexplored. In this work, our goal is to offer a more detailed, fact-centric analysis by exploring the effects of combinations of entities and relations. To facilitate this, we construct a new question answering (QA) dataset called WiTQA (Wikipedia Triple Question Answers). This dataset includes questions about entities and relations of various popularity levels, each accompanied by a supporting passage. Our extensive experiments with diverse LMs and retrievers reveal when retrieval does not consistently enhance LMs from the viewpoints of fact-centric popularity.Confirming earlier findings, we observe that larger LMs excel in recalling popular facts. However, they notably encounter difficulty with infrequent entity-relation pairs compared to retrievers. Interestingly, they can effectively retain popular relations of less common entities. We demonstrate the efficacy of our finer-grained metric and insights through an adaptive retrieval system that selectively employs retrieval and recall based on the frequencies of entities and relations in the question.","sentences":["While large language models (LMs) demonstrate remarkable performance, they encounter challenges in providing accurate responses when queried for information beyond their pre-trained memorization.","Although augmenting them with relevant external information can mitigate these issues, failure to consider the necessity of retrieval may adversely affect overall performance.","Previous research has primarily focused on examining how entities influence retrieval models and knowledge recall in LMs, leaving other aspects relatively unexplored.","In this work, our goal is to offer a more detailed, fact-centric analysis by exploring the effects of combinations of entities and relations.","To facilitate this, we construct a new question answering (QA) dataset called WiTQA (Wikipedia Triple Question Answers).","This dataset includes questions about entities and relations of various popularity levels, each accompanied by a supporting passage.","Our extensive experiments with diverse LMs and retrievers reveal when retrieval does not consistently enhance LMs from the viewpoints of fact-centric popularity.","Confirming earlier findings, we observe that larger LMs excel in recalling popular facts.","However, they notably encounter difficulty with infrequent entity-relation pairs compared to retrievers.","Interestingly, they can effectively retain popular relations of less common entities.","We demonstrate the efficacy of our finer-grained metric and insights through an adaptive retrieval system that selectively employs retrieval and recall based on the frequencies of entities and relations in the question."],"url":"http://arxiv.org/abs/2402.13492v1","category":"cs.CL"}
{"created":"2024-02-21 03:03:11","title":"Algebraic Riccati Tensor Equations with Applications in Multilinear Control Systems","abstract":"In a recent interesting paper [8], Chen et al. initialized the control-theoretic study of a class of discrete-time multilinear time-invariant (MLTI) control systems, where system states, inputs and outputs are all tensors endowed with the Einstein product. Criteria for fundamental system-theoretic notions such as stability, reachability and observability are established by means of tensor decomposition. The purpose of this paper is to continue this novel research direction. Specifically, we focus on continuous-time MLTI control systems. We define Hamiltonian tensors and symplectic tensors and establish the Schur-Hamiltonian tensor decomposition and symplectic tensor singular value decomposition (SVD). Based on these we propose the algebraic Riccati tensor equation (ARTE) and show that it has a unique positive semidefinite solution if the system is stablizable and detectable. A tensor-based Newton method is proposed to find numerical solutions of the ARTE. The tensor version of the bounded real lemma is also established. A first-order robustness analysis of the ARTE is conducted. Finally, a numerical example is used to demonstrate the proposed theory and algorithms.","sentences":["In a recent interesting paper [8], Chen et al. initialized the control-theoretic study of a class of discrete-time multilinear time-invariant (MLTI) control systems, where system states, inputs and outputs are all tensors endowed with the Einstein product.","Criteria for fundamental system-theoretic notions such as stability, reachability and observability are established by means of tensor decomposition.","The purpose of this paper is to continue this novel research direction.","Specifically, we focus on continuous-time MLTI control systems.","We define Hamiltonian tensors and symplectic tensors and establish the Schur-Hamiltonian tensor decomposition and symplectic tensor singular value decomposition (SVD).","Based on these we propose the algebraic Riccati tensor equation (ARTE) and show that it has a unique positive semidefinite solution if the system is stablizable and detectable.","A tensor-based Newton method is proposed to find numerical solutions of the ARTE.","The tensor version of the bounded real lemma is also established.","A first-order robustness analysis of the ARTE is conducted.","Finally, a numerical example is used to demonstrate the proposed theory and algorithms."],"url":"http://arxiv.org/abs/2402.13491v1","category":"math.OC"}
{"created":"2024-02-21 18:59:13","title":"Coercing LLMs to do and reveal (almost) anything","abstract":"It has recently been shown that adversarial attacks on large language models (LLMs) can \"jailbreak\" the model into making harmful statements. In this work, we argue that the spectrum of adversarial attacks on LLMs is much larger than merely jailbreaking. We provide a broad overview of possible attack surfaces and attack goals. Based on a series of concrete examples, we discuss, categorize and systematize attacks that coerce varied unintended behaviors, such as misdirection, model control, denial-of-service, or data extraction.   We analyze these attacks in controlled experiments, and find that many of them stem from the practice of pre-training LLMs with coding capabilities, as well as the continued existence of strange \"glitch\" tokens in common LLM vocabularies that should be removed for security reasons.","sentences":["It has recently been shown that adversarial attacks on large language models (LLMs) can \"jailbreak\" the model into making harmful statements.","In this work, we argue that the spectrum of adversarial attacks on LLMs is much larger than merely jailbreaking.","We provide a broad overview of possible attack surfaces and attack goals.","Based on a series of concrete examples, we discuss, categorize and systematize attacks that coerce varied unintended behaviors, such as misdirection, model control, denial-of-service, or data extraction.   ","We analyze these attacks in controlled experiments, and find that many of them stem from the practice of pre-training LLMs with coding capabilities, as well as the continued existence of strange \"glitch\" tokens in common LLM vocabularies that should be removed for security reasons."],"url":"http://arxiv.org/abs/2402.14020v1","category":"cs.LG"}
{"created":"2024-02-21 18:57:18","title":"Maxentropy completion and properties of some partially defined Stationary Markov chains","abstract":"We consider a stationary Markovian evolution with values on a disjointly partitioned set space $I\\sqcup {\\cal E}$. The evolution is visible (in the sense of knowing the transition probabilities) on the states in $I$ but not for the states in ${\\cal E}$. One only knows some partial information on the transition probabilities on ${\\cal E}$, the input and output transition probabilities and some constraints of the transition probabilities on ${\\cal E}$. Under some conditions we supply the transition probabilities on ${\\cal E}$ that satisfies the maximum entropy principle.","sentences":["We consider a stationary Markovian evolution with values on a disjointly partitioned set space $I\\sqcup {\\cal E}$.","The evolution is visible (in the sense of knowing the transition probabilities) on the states in $I$ but not for the states in ${\\cal E}$. One only knows some partial information on the transition probabilities on ${\\cal E}$, the input and output transition probabilities and some constraints of the transition probabilities on ${\\cal E}$.","Under some conditions we supply the transition probabilities on ${\\cal E}$ that satisfies the maximum entropy principle."],"url":"http://arxiv.org/abs/2402.14019v1","category":"math.PR"}
{"created":"2024-02-21 18:23:16","title":"Analysing The Impact of Sequence Composition on Language Model Pre-Training","abstract":"Most language model pre-training frameworks concatenate multiple documents into fixed-length sequences and use causal masking to compute the likelihood of each token given its context; this strategy is widely adopted due to its simplicity and efficiency. However, to this day, the influence of the pre-training sequence composition strategy on the generalisation properties of the model remains under-explored. In this work, we find that applying causal masking can lead to the inclusion of distracting information from previous documents during pre-training, which negatively impacts the performance of the models on language modelling and downstream tasks. In intra-document causal masking, the likelihood of each token is only conditioned on the previous tokens in the same document, eliminating potential distracting information from previous documents and significantly improving performance. Furthermore, we find that concatenating related documents can reduce some potential distractions during pre-training, and our proposed efficient retrieval-based sequence construction method, BM25Chunk, can improve in-context learning (+11.6\\%), knowledge memorisation (+9.8\\%), and context utilisation (+7.2\\%) abilities of language models without sacrificing efficiency.","sentences":["Most language model pre-training frameworks concatenate multiple documents into fixed-length sequences and use causal masking to compute the likelihood of each token given its context; this strategy is widely adopted due to its simplicity and efficiency.","However, to this day, the influence of the pre-training sequence composition strategy on the generalisation properties of the model remains under-explored.","In this work, we find that applying causal masking can lead to the inclusion of distracting information from previous documents during pre-training, which negatively impacts the performance of the models on language modelling and downstream tasks.","In intra-document causal masking, the likelihood of each token is only conditioned on the previous tokens in the same document, eliminating potential distracting information from previous documents and significantly improving performance.","Furthermore, we find that concatenating related documents can reduce some potential distractions during pre-training, and our proposed efficient retrieval-based sequence construction method, BM25Chunk, can improve in-context learning (+11.6\\%), knowledge memorisation (+9.8\\%), and context utilisation (+7.2\\%) abilities of language models without sacrificing efficiency."],"url":"http://arxiv.org/abs/2402.13991v1","category":"cs.CL"}
{"created":"2024-02-21 17:45:58","title":"New directions in algebraic statistics: Three challenges from 2023","abstract":"In the last quarter of a century, algebraic statistics has established itself as an expanding field which uses multilinear algebra, commutative algebra, computational algebra, geometry, and combinatorics to tackle problems in mathematical statistics. These developments have found applications in a growing number of areas, including biology, neuroscience, economics, and social sciences.   Naturally, new connections continue to be made with other areas of mathematics and statistics. This paper outlines three such connections: to statistical models used in educational testing, to a classification problem for a family of nonparametric regression models, and to phase transition phenomena under uniform sampling of contingency tables. We illustrate the motivating problems, each of which is for algebraic statistics a new direction, and demonstrate an enhancement of related methodologies.","sentences":["In the last quarter of a century, algebraic statistics has established itself as an expanding field which uses multilinear algebra, commutative algebra, computational algebra, geometry, and combinatorics to tackle problems in mathematical statistics.","These developments have found applications in a growing number of areas, including biology, neuroscience, economics, and social sciences.   ","Naturally, new connections continue to be made with other areas of mathematics and statistics.","This paper outlines three such connections: to statistical models used in educational testing, to a classification problem for a family of nonparametric regression models, and to phase transition phenomena under uniform sampling of contingency tables.","We illustrate the motivating problems, each of which is for algebraic statistics a new direction, and demonstrate an enhancement of related methodologies."],"url":"http://arxiv.org/abs/2402.13961v1","category":"math.ST"}
{"created":"2024-02-21 17:33:13","title":"Measuring Social Biases in Masked Language Models by Proxy of Prediction Quality","abstract":"Social and political scientists often aim to discover and measure distinct biases from text data representations (embeddings). Innovative transformer-based language models produce contextually-aware token embeddings and have achieved state-of-the-art performance for a variety of natural language tasks, but have been shown to encode unwanted biases for downstream applications. In this paper, we evaluate the social biases encoded by transformers trained with the masked language modeling objective using proposed proxy functions within an iterative masking experiment to measure the quality of transformer models' predictions, and assess the preference of MLMs towards disadvantaged and advantaged groups. We compare bias estimations with those produced by other evaluation methods using two benchmark datasets, finding relatively high religious and disability biases across considered MLMs and low gender bias in one dataset relative to the other. Our measures outperform others in their agreement with human annotators. We extend on previous work by evaluating social biases introduced after re-training an MLM under the masked language modeling objective (w.r.t. the model's pre-trained base), and find that proposed measures produce more accurate estimations of relative preference for biased sentences between transformers than others based on our methods.","sentences":["Social and political scientists often aim to discover and measure distinct biases from text data representations (embeddings).","Innovative transformer-based language models produce contextually-aware token embeddings and have achieved state-of-the-art performance for a variety of natural language tasks, but have been shown to encode unwanted biases for downstream applications.","In this paper, we evaluate the social biases encoded by transformers trained with the masked language modeling objective using proposed proxy functions within an iterative masking experiment to measure the quality of transformer models' predictions, and assess the preference of MLMs towards disadvantaged and advantaged groups.","We compare bias estimations with those produced by other evaluation methods using two benchmark datasets, finding relatively high religious and disability biases across considered MLMs and low gender bias in one dataset relative to the other.","Our measures outperform others in their agreement with human annotators.","We extend on previous work by evaluating social biases introduced after re-training an MLM under the masked language modeling objective (w.r.t.","the model's pre-trained base), and find that proposed measures produce more accurate estimations of relative preference for biased sentences between transformers than others based on our methods."],"url":"http://arxiv.org/abs/2402.13954v1","category":"cs.CL"}
{"created":"2024-02-21 17:00:41","title":"Powerful Large-scale Inference in High Dimensional Mediation Analysis","abstract":"In genome-wide epigenetic studies, exposures (e.g., Single Nucleotide Polymorphisms) affect outcomes (e.g., gene expression) through intermediate variables such as DNA methylation. Mediation analysis offers a way to study these intermediate variables and identify the presence or absence of causal mediation effects. Testing for mediation effects lead to a composite null hypothesis. Existing methods like the Sobel's test or the Max-P test are often underpowered because 1) statistical inference is often conducted based on distributions determined under a subset of the null and 2) they are not designed to shoulder the multiple testing burden. To tackle these issues, we introduce a technique called MLFDR (Mediation Analysis using Local False Discovery Rates) for high dimensional mediation analysis, which uses the local False Discovery Rates based on the coefficients of the structural equation model specifying the mediation relationship to construct a rejection region. We have shown theoretically as well as through simulation studies that in the high-dimensional setting, the new method of identifying the mediating variables controls the FDR asymptotically and performs better with respect to power than several existing methods such as DACT (Liu et al.)and JS-mixture (Dai et al).","sentences":["In genome-wide epigenetic studies, exposures (e.g., Single Nucleotide Polymorphisms) affect outcomes (e.g., gene expression) through intermediate variables such as DNA methylation.","Mediation analysis offers a way to study these intermediate variables and identify the presence or absence of causal mediation effects.","Testing for mediation effects lead to a composite null hypothesis.","Existing methods like the Sobel's test or the Max-P test are often underpowered because 1) statistical inference is often conducted based on distributions determined under a subset of the null and 2) they are not designed to shoulder the multiple testing burden.","To tackle these issues, we introduce a technique called MLFDR (Mediation Analysis using Local False Discovery Rates) for high dimensional mediation analysis, which uses the local False Discovery Rates based on the coefficients of the structural equation model specifying the mediation relationship to construct a rejection region.","We have shown theoretically as well as through simulation studies that in the high-dimensional setting, the new method of identifying the mediating variables controls the FDR asymptotically and performs better with respect to power than several existing methods such as DACT (Liu et al.)and JS-mixture (Dai et al)."],"url":"http://arxiv.org/abs/2402.13933v1","category":"stat.ME"}
{"created":"2024-02-21 16:31:45","title":"Bias correction of wind power forecasts with SCADA data and continuous learning","abstract":"Wind energy plays a critical role in the transition towards renewable energy sources. However, the uncertainty and variability of wind can impede its full potential and the necessary growth of wind power capacity. To mitigate these challenges, wind power forecasting methods are employed for applications in power management, energy trading, or maintenance scheduling. In this work, we present, evaluate, and compare four machine learning-based wind power forecasting models. Our models correct and improve 48-hour forecasts extracted from a numerical weather prediction (NWP) model. The models are evaluated on datasets from a wind park comprising 65 wind turbines. The best improvement in forecasting error and mean bias was achieved by a convolutional neural network, reducing the average NRMSE down to 22%, coupled with a significant reduction in mean bias, compared to a NRMSE of 35% from the strongly biased baseline model using uncorrected NWP forecasts. Our findings further indicate that changes to neural network architectures play a minor role in affecting the forecasting performance, and that future research should rather investigate changes in the model pipeline. Moreover, we introduce a continuous learning strategy, which is shown to achieve the highest forecasting performance improvements when new data is made available.","sentences":["Wind energy plays a critical role in the transition towards renewable energy sources.","However, the uncertainty and variability of wind can impede its full potential and the necessary growth of wind power capacity.","To mitigate these challenges, wind power forecasting methods are employed for applications in power management, energy trading, or maintenance scheduling.","In this work, we present, evaluate, and compare four machine learning-based wind power forecasting models.","Our models correct and improve 48-hour forecasts extracted from a numerical weather prediction (NWP) model.","The models are evaluated on datasets from a wind park comprising 65 wind turbines.","The best improvement in forecasting error and mean bias was achieved by a convolutional neural network, reducing the average NRMSE down to 22%, coupled with a significant reduction in mean bias, compared to a NRMSE of 35% from the strongly biased baseline model using uncorrected NWP forecasts.","Our findings further indicate that changes to neural network architectures play a minor role in affecting the forecasting performance, and that future research should rather investigate changes in the model pipeline.","Moreover, we introduce a continuous learning strategy, which is shown to achieve the highest forecasting performance improvements when new data is made available."],"url":"http://arxiv.org/abs/2402.13916v1","category":"cs.LG"}
{"created":"2024-02-21 16:01:06","title":"A unified Bayesian framework for interval hypothesis testing in clinical trials","abstract":"The American Statistical Association (ASA) statement on statistical significance and P-values \\cite{wasserstein2016asa} cautioned statisticians against making scientific decisions solely on the basis of traditional P-values. The statement delineated key issues with P-values, including a lack of transparency, an inability to quantify evidence in support of the null hypothesis, and an inability to measure the size of an effect or the importance of a result. In this article, we demonstrate that the interval null hypothesis framework (instead of the point null hypothesis framework), when used in tandem with Bayes factor-based tests, is instrumental in circumnavigating the key issues of P-values. Further, we note that specifying prior densities for Bayes factors is challenging and has been a reason for criticism of Bayesian hypothesis testing in existing literature. We address this by adapting Bayes factors directly based on common test statistics. We demonstrate, through numerical experiments and real data examples, that the proposed Bayesian interval hypothesis testing procedures can be calibrated to ensure frequentist error control while retaining their inherent interpretability. Finally, we illustrate the improved flexibility and applicability of the proposed methods by providing coherent frameworks for competitive landscape analysis and end-to-end Bayesian hypothesis tests in the context of reporting clinical trial outcomes.","sentences":["The American Statistical Association (ASA) statement on statistical significance and P-values \\cite{wasserstein2016asa} cautioned statisticians against making scientific decisions solely on the basis of traditional P-values.","The statement delineated key issues with P-values, including a lack of transparency, an inability to quantify evidence in support of the null hypothesis, and an inability to measure the size of an effect or the importance of a result.","In this article, we demonstrate that the interval null hypothesis framework (instead of the point null hypothesis framework), when used in tandem with Bayes factor-based tests, is instrumental in circumnavigating the key issues of P-values.","Further, we note that specifying prior densities for Bayes factors is challenging and has been a reason for criticism of Bayesian hypothesis testing in existing literature.","We address this by adapting Bayes factors directly based on common test statistics.","We demonstrate, through numerical experiments and real data examples, that the proposed Bayesian interval hypothesis testing procedures can be calibrated to ensure frequentist error control while retaining their inherent interpretability.","Finally, we illustrate the improved flexibility and applicability of the proposed methods by providing coherent frameworks for competitive landscape analysis and end-to-end Bayesian hypothesis tests in the context of reporting clinical trial outcomes."],"url":"http://arxiv.org/abs/2402.13890v1","category":"stat.ME"}
{"created":"2024-02-21 14:22:52","title":"Uncovering the Sign of Nuclear Deformations: Prolate or Oblate Shape via Low-Energy $\u03b1$ Inelastic Scattering","abstract":"Background: Understanding nuclear shape is a crucial problem in nuclear physics. In particular, determining the sign of quadrupole deformation, i.e., whether prolate or oblate, remains a challenging problem. Purpose: Our aim is to propose a method for determining the sign of quadrupole deformation using $\\alpha$ inelastic scattering data and to demonstrate its effectiveness. Method: Our approach is the standard coupled-channel method based on the macroscopic model. We utilize the nuclear reorientation effect, a phenomenon associated with the self coupling of excited states, as a probe sensitive to the sign of deformation. Results: We first provide an overview of how the reorientation effect influences inelastic scattering cross sections, and numerically confirm its validity in realistic cases. We then demonstrate that the sign of deformation can be uniquely determined from inelastic scattering cross section data. Conclusion: Our technique offers a systematic approach for determining the sign of deformation in both stable and unstable nuclei. The broad applicability of $\\alpha$ inelastic scattering will make it a valuable tool to study shape of nuclei, especially unstable nuclei.","sentences":["Background: Understanding nuclear shape is a crucial problem in nuclear physics.","In particular, determining the sign of quadrupole deformation, i.e., whether prolate or oblate, remains a challenging problem.","Purpose:","Our aim is to propose a method for determining the sign of quadrupole deformation using $\\alpha$ inelastic scattering data and to demonstrate its effectiveness.","Method: Our approach is the standard coupled-channel method based on the macroscopic model.","We utilize the nuclear reorientation effect, a phenomenon associated with the self coupling of excited states, as a probe sensitive to the sign of deformation.","Results:","We first provide an overview of how the reorientation effect influences inelastic scattering cross sections, and numerically confirm its validity in realistic cases.","We then demonstrate that the sign of deformation can be uniquely determined from inelastic scattering cross section data.","Conclusion: Our technique offers a systematic approach for determining the sign of deformation in both stable and unstable nuclei.","The broad applicability of $\\alpha$ inelastic scattering will make it a valuable tool to study shape of nuclei, especially unstable nuclei."],"url":"http://arxiv.org/abs/2402.13832v1","category":"nucl-th"}
{"created":"2024-02-21 14:20:51","title":"The Brauer-Siegel ratio for prime cyclotomic fields","abstract":"The Brauer-Siegel theorem concerns the size of the product of the class number and the regulator of a number field $K$. We derive bounds for this product in case $K$ is a prime cyclotomic field, distinguishing between whether there is a Siegel zero or not. In particular, we make a result of Tatuzawa (1953) more explicit. Our theoretical advancements are complemented by numerical illustrations that are consistent with our findings.","sentences":["The Brauer-Siegel theorem concerns the size of the product of the class number and the regulator of a number field $K$. We derive bounds for this product in case $K$ is a prime cyclotomic field, distinguishing between whether there is a Siegel zero or not.","In particular, we make a result of Tatuzawa (1953) more explicit.","Our theoretical advancements are complemented by numerical illustrations that are consistent with our findings."],"url":"http://arxiv.org/abs/2402.13830v1","category":"math.NT"}
{"created":"2024-02-21 14:18:38","title":"The Kummer ratio of the relative class number for prime cyclotomic fields","abstract":"Kummer's conjecture predicts the asymptotic growth of the relative class number of prime cyclotomic fields. We substantially improve the known bounds of Kummer's ratio under three scenarios: no Siegel zero, presence of Siegel zero and assuming the Riemann Hypothesis for the Dirichlet $L$-series attached to odd characters only. The numerical work in this paper extends and improves on our earlier preprint (arXiv:1908.01152) and demonstrates our theoretical results.","sentences":["Kummer's conjecture predicts the asymptotic growth of the relative class number of prime cyclotomic fields.","We substantially improve the known bounds of Kummer's ratio under three scenarios: no Siegel zero, presence of Siegel zero and assuming the Riemann Hypothesis for the Dirichlet $L$-series attached to odd characters only.","The numerical work in this paper extends and improves on our earlier preprint (arXiv:1908.01152) and demonstrates our theoretical results."],"url":"http://arxiv.org/abs/2402.13829v1","category":"math.NT"}
{"created":"2024-02-21 13:22:31","title":"Filtrations on derived category of twisted K3 surfaces","abstract":"In this paper, we study the Shen-Yin-Zhao type filtration on the derived category of twisted K3 surfaces. We introduce the so called twisted Beauville-Voisin class on a twisted K3 surface and use it to extend the work in \\cite{OG13, SYZ20} to the case of twisted K3 surfaces. It shares many nice properties as the untwisted case. For instance, the filtration is preserved under any Fourier-Mukai transformation. As an application, we obtain a new filtration on the Chow group of zero cycles on the Bridgeland moduli space of twisted K3 surfaces. It is birational invariant and can be viewed as a candidate for the conjectural Beauville-Voisin filtration. We compare this filtration with Voisin's filtration and show they coincide on the locally free loci. Some other applications (such as Bloch's conjecture for birational automorphisms) are discussed at the end.","sentences":["In this paper, we study the Shen-Yin-Zhao type filtration on the derived category of twisted K3 surfaces.","We introduce the so called twisted Beauville-Voisin class on a twisted K3 surface and use it to extend the work in \\cite{OG13, SYZ20} to the case of twisted K3 surfaces.","It shares many nice properties as the untwisted case.","For instance, the filtration is preserved under any Fourier-Mukai transformation.","As an application, we obtain a new filtration on the Chow group of zero cycles on the Bridgeland moduli space of twisted K3 surfaces.","It is birational invariant and can be viewed as a candidate for the conjectural Beauville-Voisin filtration.","We compare this filtration with Voisin's filtration and show they coincide on the locally free loci.","Some other applications (such as Bloch's conjecture for birational automorphisms) are discussed at the end."],"url":"http://arxiv.org/abs/2402.13793v1","category":"math.AG"}
{"created":"2024-02-21 13:14:45","title":"Fairness Rising from the Ranks: HITS and PageRank on Homophilic Networks","abstract":"In this paper, we investigate the conditions under which link analysis algorithms prevent minority groups from reaching high ranking slots. We find that the most common link-based algorithms using centrality metrics, such as PageRank and HITS, can reproduce and even amplify bias against minority groups in networks. Yet, their behavior differs: one one hand, we empirically show that PageRank mirrors the degree distribution for most of the ranking positions and it can equalize representation of minorities among the top ranked nodes; on the other hand, we find that HITS amplifies pre-existing bias in homophilic networks through a novel theoretical analysis, supported by empirical results. We find the root cause of bias amplification in HITS to be the level of homophily present in the network, modeled through an evolving network model with two communities. We illustrate our theoretical analysis on both synthetic and real datasets and we present directions for future work.","sentences":["In this paper, we investigate the conditions under which link analysis algorithms prevent minority groups from reaching high ranking slots.","We find that the most common link-based algorithms using centrality metrics, such as PageRank and HITS, can reproduce and even amplify bias against minority groups in networks.","Yet, their behavior differs: one one hand, we empirically show that PageRank mirrors the degree distribution for most of the ranking positions and it can equalize representation of minorities among the top ranked nodes; on the other hand, we find that HITS amplifies pre-existing bias in homophilic networks through a novel theoretical analysis, supported by empirical results.","We find the root cause of bias amplification in HITS to be the level of homophily present in the network, modeled through an evolving network model with two communities.","We illustrate our theoretical analysis on both synthetic and real datasets and we present directions for future work."],"url":"http://arxiv.org/abs/2402.13787v1","category":"cs.SI"}
{"created":"2024-02-21 12:59:50","title":"Efficient timing jitter simulation for passively mode-locked semiconductor lasers","abstract":"Efficient simulation of the timing jitter in passively mode-locking lasers is key to their numerical investigation and optimization. We introduce a method based on the pulse-period fluctuation auto-correlation function and compare it against established methods with respect to their estimate error. Potential improvements of the computational cost by about two orders of magnitude are reported. This advantage may facilitate larger parameter studies of passively mode-locked laser on small scale clusters or even desktop computers and thereby guide the target-oriented design of future lasers with ultra low timing jitter.","sentences":["Efficient simulation of the timing jitter in passively mode-locking lasers is key to their numerical investigation and optimization.","We introduce a method based on the pulse-period fluctuation auto-correlation function and compare it against established methods with respect to their estimate error.","Potential improvements of the computational cost by about two orders of magnitude are reported.","This advantage may facilitate larger parameter studies of passively mode-locked laser on small scale clusters or even desktop computers and thereby guide the target-oriented design of future lasers with ultra low timing jitter."],"url":"http://arxiv.org/abs/2402.13780v1","category":"physics.optics"}
{"created":"2024-02-21 12:37:41","title":"Optimizing the Cavity-Arm Ratio of V-Shaped Semiconductor Disk Lasers","abstract":"Passively mode-locked semiconductor disk lasers have received tremendous attention from both science and industry. Their relatively inexpensive production combined with excellent pulse performance and great emission wavelength flexibility make them suitable laser candidates for applications ranging from frequency comb tomography to spectroscopy. However, due to the interaction of the active medium dynamics and the device geometry, emission instabilities occur at high pump powers and thereby limit their performance potential. Hence, understanding those instabilities becomes critical for an optimal laser design. Using a delay-differential equation model, we are able to detect, understand, and classify three distinct instabilities that limit the maximum achievable pump power for the fundamental mode-locking state and link them to characteristic positive net-gain windows. We furthermore derive a simple analytic approximation in order to quantitatively describe the stability boundary. Our results enable us to predict the optimal laser cavity configuration with respect to positive net-gain instabilities and are therefore of great relevance for the future development of passively mode-locking semiconductor disk lasers.","sentences":["Passively mode-locked semiconductor disk lasers have received tremendous attention from both science and industry.","Their relatively inexpensive production combined with excellent pulse performance and great emission wavelength flexibility make them suitable laser candidates for applications ranging from frequency comb tomography to spectroscopy.","However, due to the interaction of the active medium dynamics and the device geometry, emission instabilities occur at high pump powers and thereby limit their performance potential.","Hence, understanding those instabilities becomes critical for an optimal laser design.","Using a delay-differential equation model, we are able to detect, understand, and classify three distinct instabilities that limit the maximum achievable pump power for the fundamental mode-locking state and link them to characteristic positive net-gain windows.","We furthermore derive a simple analytic approximation in order to quantitatively describe the stability boundary.","Our results enable us to predict the optimal laser cavity configuration with respect to positive net-gain instabilities and are therefore of great relevance for the future development of passively mode-locking semiconductor disk lasers."],"url":"http://arxiv.org/abs/2402.13761v1","category":"physics.optics"}
{"created":"2024-02-21 11:57:10","title":"Testing Outlier Detection Algorithms for Identifying Early-Stage Solute Clusters in Atom Probe Tomography","abstract":"Atom probe tomography is commonly used to study solute clustering and precipitation in materials. However, standard techniques, such as the density based spatial clustering applications with noise (DBSCAN) perform poorly with respect to small clusters of less than 25 atoms. This is a fundamental limitation of density-based clustering techniques due to the usage of Nmin, an arbitrary lower limit placed on cluster sizes. Therefore, this paper attempts to consider atom probe clustering as an outlier detection problem of which KNN, LOF, LUNAR algorithms were tested against a simulated dataset and compared to the standard method, for a range of cluster sizes. The decision score output of the algorithms was then auto thresholded by the Karcher mean to remove human bias. Each of the major models tested outperforms DBSCAN for cluster sizes of less than 25 atoms but underperforms for sizes greater than 30 atoms. However, the new combined kNN and DBSCAN method presented was able to perform well at all cluster sizes. The combined kNN and DBSCAN method is presented as a possible new standard approach to identifying solute clusters in atom probe tomography.","sentences":["Atom probe tomography is commonly used to study solute clustering and precipitation in materials.","However, standard techniques, such as the density based spatial clustering applications with noise (DBSCAN) perform poorly with respect to small clusters of less than 25 atoms.","This is a fundamental limitation of density-based clustering techniques due to the usage of Nmin, an arbitrary lower limit placed on cluster sizes.","Therefore, this paper attempts to consider atom probe clustering as an outlier detection problem of which KNN, LOF, LUNAR algorithms were tested against a simulated dataset and compared to the standard method, for a range of cluster sizes.","The decision score output of the algorithms was then auto thresholded by the Karcher mean to remove human bias.","Each of the major models tested outperforms DBSCAN for cluster sizes of less than 25 atoms but underperforms for sizes greater than 30 atoms.","However, the new combined kNN and DBSCAN method presented was able to perform well at all cluster sizes.","The combined kNN and DBSCAN method is presented as a possible new standard approach to identifying solute clusters in atom probe tomography."],"url":"http://arxiv.org/abs/2402.13734v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-21 11:30:29","title":"$\\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens","abstract":"Processing and reasoning over long contexts is crucial for many practical applications of Large Language Models (LLMs), such as document comprehension and agent construction. Despite recent strides in making LLMs process contexts with more than 100K tokens, there is currently a lack of a standardized benchmark to evaluate this long-context capability. Existing public benchmarks typically focus on contexts around 10K tokens, limiting the assessment and comparison of LLMs in processing longer contexts. In this paper, we propose $\\infty$Bench, the first LLM benchmark featuring an average data length surpassing 100K tokens. $\\infty$Bench comprises synthetic and realistic tasks spanning diverse domains, presented in both English and Chinese. The tasks in $\\infty$Bench are designed to require well understanding of long dependencies in contexts, and make simply retrieving a limited number of passages from contexts not sufficient for these tasks. In our experiments, based on $\\infty$Bench, we evaluate the state-of-the-art proprietary and open-source LLMs tailored for processing long contexts. The results indicate that existing long context LLMs still require significant advancements to effectively process 100K+ context. We further present three intriguing analyses regarding the behavior of LLMs processing long context.","sentences":["Processing and reasoning over long contexts is crucial for many practical applications of Large Language Models (LLMs), such as document comprehension and agent construction.","Despite recent strides in making LLMs process contexts with more than 100K tokens, there is currently a lack of a standardized benchmark to evaluate this long-context capability.","Existing public benchmarks typically focus on contexts around 10K tokens, limiting the assessment and comparison of LLMs in processing longer contexts.","In this paper, we propose $\\infty$Bench, the first LLM benchmark featuring an average data length surpassing 100K tokens.","$\\infty$Bench comprises synthetic and realistic tasks spanning diverse domains, presented in both English and Chinese.","The tasks in $\\infty$Bench are designed to require well understanding of long dependencies in contexts, and make simply retrieving a limited number of passages from contexts not sufficient for these tasks.","In our experiments, based on $\\infty$Bench, we evaluate the state-of-the-art proprietary and open-source LLMs tailored for processing long contexts.","The results indicate that existing long context LLMs still require significant advancements to effectively process 100K+ context.","We further present three intriguing analyses regarding the behavior of LLMs processing long context."],"url":"http://arxiv.org/abs/2402.13718v1","category":"cs.CL"}
{"created":"2024-02-21 11:25:22","title":"RESTRuler: Towards Automatically Identifying Violations of RESTful Design Rules in Web APIs","abstract":"RESTful APIs based on HTTP are one of the most important ways to make data and functionality available to applications and software services. However, the quality of the API design strongly impacts API understandability and usability, and many rules have been specified for this. While we have evidence for the effectiveness of many design rules, it is still difficult for practitioners to identify rule violations in their design. We therefore present RESTRuler, a Java-based open-source tool that uses static analysis to detect design rule violations in OpenAPI descriptions. The current prototype supports 14 rules that go beyond simple syntactic checks and partly rely on natural language processing. The modular architecture also makes it easy to implement new rules. To evaluate RESTRuler, we conducted a benchmark with over 2,300 public OpenAPI descriptions and asked 7 API experts to construct 111 complicated rule violations. For robustness, RESTRuler successfully analyzed 99% of the used real-world OpenAPI definitions, with some failing due to excessive size. For performance efficiency, the tool performed well for the majority of files and could analyze 84% in less than 23 seconds with low CPU and RAM usage. Lastly, for effectiveness, RESTRuler achieved a precision of 91% (ranging from 60% to 100% per rule) and recall of 68% (ranging from 46% to 100%). Based on these variations between rule implementations, we identified several opportunities for improvements. While RESTRuler is still a research prototype, the evaluation suggests that the tool is quite robust to errors, resource-efficient for most APIs, and shows good precision and decent recall. Practitioners can use it to improve the quality of their API design.","sentences":["RESTful APIs based on HTTP are one of the most important ways to make data and functionality available to applications and software services.","However, the quality of the API design strongly impacts API understandability and usability, and many rules have been specified for this.","While we have evidence for the effectiveness of many design rules, it is still difficult for practitioners to identify rule violations in their design.","We therefore present RESTRuler, a Java-based open-source tool that uses static analysis to detect design rule violations in OpenAPI descriptions.","The current prototype supports 14 rules that go beyond simple syntactic checks and partly rely on natural language processing.","The modular architecture also makes it easy to implement new rules.","To evaluate RESTRuler, we conducted a benchmark with over 2,300 public OpenAPI descriptions and asked 7 API experts to construct 111 complicated rule violations.","For robustness, RESTRuler successfully analyzed 99% of the used real-world OpenAPI definitions, with some failing due to excessive size.","For performance efficiency, the tool performed well for the majority of files and could analyze 84% in less than 23 seconds with low CPU and RAM usage.","Lastly, for effectiveness, RESTRuler achieved a precision of 91% (ranging from 60% to 100% per rule) and recall of 68% (ranging from 46% to 100%).","Based on these variations between rule implementations, we identified several opportunities for improvements.","While RESTRuler is still a research prototype, the evaluation suggests that the tool is quite robust to errors, resource-efficient for most APIs, and shows good precision and decent recall.","Practitioners can use it to improve the quality of their API design."],"url":"http://arxiv.org/abs/2402.13710v1","category":"cs.SE"}
{"created":"2024-02-21 11:07:29","title":"On multiplicatively dependent vectors of polynomial values","abstract":"Given polynomials $f_1,\\ldots,f_n$ in $m$ variables with integral coefficients, we give upper bounds for the number of integral $m$-tuples $\\mathbf{u}_1,\\ldots, \\mathbf{u}_n$ of bounded height such that $f_1(\\mathbf{u}_1), \\ldots, f_n(\\mathbf{u}_n)$ are multiplicatively dependent. We also prove, under certain conditions, a finiteness result for $\\mathbf{u} \\in \\mathbb{Z}^m$ with relatively prime entries such that $f_1(\\mathbf{u}),\\ldots,f_n(\\mathbf{u})$ are multiplicatively dependent.","sentences":["Given polynomials $f_1,\\ldots,f_n$ in $m$ variables with integral coefficients, we give upper bounds for the number of integral $m$-tuples $\\mathbf{u}_1,\\ldots, \\mathbf{u}_n$ of bounded height such that $f_1(\\mathbf{u}_1), \\ldots, f_n(\\mathbf{u}_n)$ are multiplicatively dependent.","We also prove, under certain conditions, a finiteness result for $\\mathbf{u} \\in \\mathbb{Z}^m$ with relatively prime entries such that $f_1(\\mathbf{u}),\\ldots,f_n(\\mathbf{u})$ are multiplicatively dependent."],"url":"http://arxiv.org/abs/2402.13704v1","category":"math.NT"}
{"created":"2024-02-21 11:04:23","title":"On the Conflict of Robustness and Learning in Collaborative Machine Learning","abstract":"Collaborative Machine Learning (CML) allows participants to jointly train a machine learning model while keeping their training data private. In scenarios where privacy is a strong requirement, such as health-related applications, safety is also a primary concern. This means that privacy-preserving CML processes must produce models that output correct and reliable decisions \\emph{even in the presence of potentially untrusted participants}. In response to this issue, researchers propose to use \\textit{robust aggregators} that rely on metrics which help filter out malicious contributions that could compromise the training process. In this work, we formalize the landscape of robust aggregators in the literature. Our formalization allows us to show that existing robust aggregators cannot fulfill their goal: either they use distance-based metrics that cannot accurately identify targeted malicious updates; or propose methods whose success is in direct conflict with the ability of CML participants to learn from others and therefore cannot eliminate the risk of manipulation without preventing learning.","sentences":["Collaborative Machine Learning (CML) allows participants to jointly train a machine learning model while keeping their training data private.","In scenarios where privacy is a strong requirement, such as health-related applications, safety is also a primary concern.","This means that privacy-preserving CML processes must produce models that output correct and reliable decisions \\emph{even in the presence of potentially untrusted participants}.","In response to this issue, researchers propose to use \\textit{robust aggregators} that rely on metrics which help filter out malicious contributions that could compromise the training process.","In this work, we formalize the landscape of robust aggregators in the literature.","Our formalization allows us to show that existing robust aggregators cannot fulfill their goal: either they use distance-based metrics that cannot accurately identify targeted malicious updates; or propose methods whose success is in direct conflict with the ability of CML participants to learn from others and therefore cannot eliminate the risk of manipulation without preventing learning."],"url":"http://arxiv.org/abs/2402.13700v1","category":"cs.LG"}
{"created":"2024-02-21 10:40:35","title":"Reentrant condensation of sparsely charged polyelectrolytes induced by multivalent salts: Interplay between electrostatic and non-electrostatic interactions","abstract":"We study the reentrant condensation of sparsely charged polyelectrolytes in dilute solutions of multivalent ions, whose phase-transition mechanism remains under debate. We propose a mean-field model by recasting the concept of a \"physical crosslinking\" effect, which can predict the essential features of the reentrant condensation including the phase diagram of sparsely charged polyelectrolyte. The model unveils that the strong adsorption between the ionic monomers and multivalent ions can be at the origin of the peculiar phenomenon that rather low concentrations of multivalent salts trigger both collapse and re-entry transitions. The analytical solution of the model indicates that a minimum of coupling energy due to sharing multivalent salt ions between ionic monomers is essential for a phase transition to occur, which can explain the enigmatic observation that the sparsely charged polyelectrolytes can only show phase transition in a dilute solution of salts with selective multivalency. Our analytical calculations also show that the incompatibility of the uncharged moieties of the polyelectrolytes with solvent (water) is critical to regulate phase behaviors of sparsely charged polyelectrolytes in aqueous solutions. This is in agreement with recent experimental investigations on solution properties of amphiphilic proteins. We envisage that this work will shed light on the understanding of reentrant condensation in biological processes where the multivalent ions bound to biopolymers (such as RNAs and proteins) plays an essential role, which remains understood poorly.","sentences":["We study the reentrant condensation of sparsely charged polyelectrolytes in dilute solutions of multivalent ions, whose phase-transition mechanism remains under debate.","We propose a mean-field model by recasting the concept of a \"physical crosslinking\" effect, which can predict the essential features of the reentrant condensation including the phase diagram of sparsely charged polyelectrolyte.","The model unveils that the strong adsorption between the ionic monomers and multivalent ions can be at the origin of the peculiar phenomenon that rather low concentrations of multivalent salts trigger both collapse and re-entry transitions.","The analytical solution of the model indicates that a minimum of coupling energy due to sharing multivalent salt ions between ionic monomers is essential for a phase transition to occur, which can explain the enigmatic observation that the sparsely charged polyelectrolytes can only show phase transition in a dilute solution of salts with selective multivalency.","Our analytical calculations also show that the incompatibility of the uncharged moieties of the polyelectrolytes with solvent (water) is critical to regulate phase behaviors of sparsely charged polyelectrolytes in aqueous solutions.","This is in agreement with recent experimental investigations on solution properties of amphiphilic proteins.","We envisage that this work will shed light on the understanding of reentrant condensation in biological processes where the multivalent ions bound to biopolymers (such as RNAs and proteins) plays an essential role, which remains understood poorly."],"url":"http://arxiv.org/abs/2402.13686v1","category":"cond-mat.soft"}
{"created":"2024-02-21 10:36:44","title":"Dynamics of bubble migration in a square channel flow of a viscoelastic fluid","abstract":"Cross-stream migration of a deformable bubble is investigated computationally in a pressure-driven channel flow of a viscoelastic fluid via interface-resolved simulations. The flow equations are solved fully coupled with the Giesekus model equations using the front-tracking method and extensive simulations are performed for a wide range of flow parameters to reveal the effects of bubble deformability, fluid elasticity, shear-thinning, and fluid inertia on the bubble migration dynamics. Migration rate of a bubble is found to be much higher than that of a solid particle under similar flow conditions mainly due to free-slip condition on its surface. It is observed that direction of bubble migration can be altered by varying shear-thinning of the ambient fluid. With a strong shear-thinning, the bubble migrates towards the wall while it migrates towards the center of the channel in a purely elastic fluid without shear-thinning. An onset of elastic flow instability is observed beyond a critical Weissenberg number, which in turn causes a path instability even for a nearly spherical bubble. An inertial path instability is also observed once bubble deformation exceeds a critical value. Shear-thinning is found to be suppressing the path instability in a viscoelastic fluid with a high polymer concentration whereas it reverses its role and promotes path instability in a dilute polymer solution. It is found that bubble migration towards wall induces a secondary flow with a velocity that is about an order of magnitude higher than the one induced by a solid particle under similar flow conditions.","sentences":["Cross-stream migration of a deformable bubble is investigated computationally in a pressure-driven channel flow of a viscoelastic fluid via interface-resolved simulations.","The flow equations are solved fully coupled with the Giesekus model equations using the front-tracking method and extensive simulations are performed for a wide range of flow parameters to reveal the effects of bubble deformability, fluid elasticity, shear-thinning, and fluid inertia on the bubble migration dynamics.","Migration rate of a bubble is found to be much higher than that of a solid particle under similar flow conditions mainly due to free-slip condition on its surface.","It is observed that direction of bubble migration can be altered by varying shear-thinning of the ambient fluid.","With a strong shear-thinning, the bubble migrates towards the wall while it migrates towards the center of the channel in a purely elastic fluid without shear-thinning.","An onset of elastic flow instability is observed beyond a critical Weissenberg number, which in turn causes a path instability even for a nearly spherical bubble.","An inertial path instability is also observed once bubble deformation exceeds a critical value.","Shear-thinning is found to be suppressing the path instability in a viscoelastic fluid with a high polymer concentration whereas it reverses its role and promotes path instability in a dilute polymer solution.","It is found that bubble migration towards wall induces a secondary flow with a velocity that is about an order of magnitude higher than the one induced by a solid particle under similar flow conditions."],"url":"http://arxiv.org/abs/2402.13683v1","category":"physics.flu-dyn"}
{"created":"2024-02-21 10:34:53","title":"The effect of lightning on the atmospheric chemistry of exoplanets and potential biosignatures","abstract":"Lightning has been suggested to play a role in triggering the occurrence of bio-ready chemical species. Future missions (PLATO, ARIEL, HWO, LIFE) and ground-based ELTs will investigate the atmospheres of potentially habitable exoplanets. We aim to study the effect of lightning on the atmospheric chemistry, how it affects false-positive and false-negative biosignatures, and if its effect would be observable on an exo-Earth and on TRAPPIST-1 planets. We use a combination of laboratory experiments, photochemical and radiative transfer modelling. With spark discharge experiments in N2-CO2-H2 gas mixtures, representing a range of possible rocky-planet atmospheres, we investigate the production of potential lightning signatures (CO, NO), possible biosignature gases (N2O, NH3, CH4), and important prebiotic precursors (HCN, Urea). Photochemical simulations are conducted for oxygen-rich and anoxic atmospheres for rocky planets in the habitable zones of the Sun and TRAPPIST-1 for a range of lightning flash rates. Synthetic spectra are calculated using SMART to study the atmosphere's reflectance, emission, and transmission spectra. Lightning enhances the spectral features of NO, NO2, and, in some cases, CO; CH4 and C2H6 may be enhanced indirectly. Lightning at a flash rate slightly higher than on modern Earth can mask the ozone features of an oxygen-rich, biotic atmosphere, making it harder to detect the biosphere. Lightning flash rates at least ten times higher than on modern Earth can mask the presence of ozone in the anoxic, abiotic atmosphere of a planet orbiting a late M dwarf, reducing the potential for a false-positive life-detection. The threshold lightning rates to eliminate oxygen and ozone false positive biosignatures on planets orbiting ultra-cool dwarfs is up to ten times higher than the modern flash rate, suggesting that lightning cannot always prevent these false-positive scenarios.","sentences":["Lightning has been suggested to play a role in triggering the occurrence of bio-ready chemical species.","Future missions (PLATO, ARIEL, HWO, LIFE) and ground-based ELTs will investigate the atmospheres of potentially habitable exoplanets.","We aim to study the effect of lightning on the atmospheric chemistry, how it affects false-positive and false-negative biosignatures, and if its effect would be observable on an exo-Earth and on TRAPPIST-1 planets.","We use a combination of laboratory experiments, photochemical and radiative transfer modelling.","With spark discharge experiments in N2-CO2-H2 gas mixtures, representing a range of possible rocky-planet atmospheres, we investigate the production of potential lightning signatures (CO, NO), possible biosignature gases (N2O, NH3, CH4), and important prebiotic precursors (HCN, Urea).","Photochemical simulations are conducted for oxygen-rich and anoxic atmospheres for rocky planets in the habitable zones of the Sun and TRAPPIST-1 for a range of lightning flash rates.","Synthetic spectra are calculated using SMART to study the atmosphere's reflectance, emission, and transmission spectra.","Lightning enhances the spectral features of NO, NO2, and, in some cases, CO; CH4 and C2H6 may be enhanced indirectly.","Lightning at a flash rate slightly higher than on modern Earth can mask the ozone features of an oxygen-rich, biotic atmosphere, making it harder to detect the biosphere.","Lightning flash rates at least ten times higher than on modern Earth can mask the presence of ozone in the anoxic, abiotic atmosphere of a planet orbiting a late M dwarf, reducing the potential for a false-positive life-detection.","The threshold lightning rates to eliminate oxygen and ozone false positive biosignatures on planets orbiting ultra-cool dwarfs is up to ten times higher than the modern flash rate, suggesting that lightning cannot always prevent these false-positive scenarios."],"url":"http://arxiv.org/abs/2402.13682v1","category":"astro-ph.EP"}
{"created":"2024-02-21 10:10:39","title":"Permutation groups of prime power degree and $p$-complements","abstract":"A $p$-complement in a finite group $G$ is a subgroup $H$ of $p$-power index and of order coprime to $p$. Solvable groups have $p$-complements for all primes dividing their order, but non-solvable groups rarely have any. If $G$ has a $p$-complement $H$ then $G$ acts as a transitive permutation group of $p$-power degree on the cosets of $H$. Extending earlier work of Guralnick and of Cai and Zhang, we classify the almost simple groups which have transitive permutation representations of prime power degree, and those which have $p$-complements. In all cases these are maximal subgroups; in some cases they are mutually conjugate, but in others they form two conjugacy classes, transposed by an outer automorphism. We deduce that every primitive permutation group of prime power degree has a regular subgroup, and that any two faithful primitive representations of a group, of the same prime power degree, are equivalent under automorphisms. Questions of existence of prime power representations and $p$-complements in groups with socle ${\\rm PSL}_d(q)$ lead to deep unsolved problems in Number Theory.","sentences":["A $p$-complement in a finite group $G$ is a subgroup $H$ of $p$-power index and of order coprime to $p$. Solvable groups have $p$-complements for all primes dividing their order, but non-solvable groups rarely have any.","If $G$ has a $p$-complement $H$ then $G$ acts as a transitive permutation group of $p$-power degree on the cosets of $H$. Extending earlier work of Guralnick and of Cai and Zhang, we classify the almost simple groups which have transitive permutation representations of prime power degree, and those which have $p$-complements.","In all cases these are maximal subgroups; in some cases they are mutually conjugate, but in others they form two conjugacy classes, transposed by an outer automorphism.","We deduce that every primitive permutation group of prime power degree has a regular subgroup, and that any two faithful primitive representations of a group, of the same prime power degree, are equivalent under automorphisms.","Questions of existence of prime power representations and $p$-complements in groups with socle ${\\rm PSL}_d(q)$ lead to deep unsolved problems in Number Theory."],"url":"http://arxiv.org/abs/2402.13672v1","category":"math.GR"}
{"created":"2024-02-21 09:52:22","title":"A Method For Bounding Tail Probabilities","abstract":"We present a method for upper and lower bounding the right and the left tail probabilities of continuous random variables (RVs). For the right tail probability of RV $X$ with probability density function $f_X(x)$, this method requires first setting a continuous, positive, and strictly decreasing function $g_X(x)$ such that $-f_X(x)/g'_X(x)$ is a decreasing and increasing function, $\\forall x>x_0$, which results in upper and lower bounds, respectively, given in the form $-f_X(x) g_X(x)/g'_X(x)$, $\\forall x>x_0$, where $x_0$ is some point. Similarly, for the upper and lower bounds on the left tail probability of $X$, this method requires first setting a continuous, positive, and strictly increasing function $g_X(x)$ such that $f_X(x)/g'_X(x)$ is an increasing and decreasing function, $\\forall x<x_0$, which results in upper and lower bounds, respectively, given in the form $f_X(x) g_X(x)/g'_X(x)$, $\\forall x<x_0$. We provide some examples of good candidates for the function $g_X(x)$. We also establish connections between the new bounds and Markov's inequality and Chernoff's bound. In addition, we provide an iterative method for obtaining ever tighter lower and upper bounds, under certain conditions. Finally, we provide numerical examples, where we show the tightness of these bounds, for some chosen $g_X(x)$.","sentences":["We present a method for upper and lower bounding the right and the left tail probabilities of continuous random variables (RVs).","For the right tail probability of RV $X$ with probability density function $f_X(x)$, this method requires first setting a continuous, positive, and strictly decreasing function $g_X(x)$ such that $-f_X(x)/g'_X(x)$ is a decreasing and increasing function, $\\forall x>x_0$, which results in upper and lower bounds, respectively, given in the form $-f_X(x) g_X(x)/g'_X(x)$, $\\forall x>x_0$, where $x_0$ is some point.","Similarly, for the upper and lower bounds on the left tail probability of $X$, this method requires first setting a continuous, positive, and strictly increasing function $g_X(x)$ such that $f_X(x)/g'_X(x)$ is an increasing and decreasing function, $\\forall x<x_0$, which results in upper and lower bounds, respectively, given in the form $f_X(x) g_X(x)/g'_X(x)$, $\\forall x<x_0$.","We provide some examples of good candidates for the function $g_X(x)$. We also establish connections between the new bounds and Markov's inequality and Chernoff's bound.","In addition, we provide an iterative method for obtaining ever tighter lower and upper bounds, under certain conditions.","Finally, we provide numerical examples, where we show the tightness of these bounds, for some chosen $g_X(x)$."],"url":"http://arxiv.org/abs/2402.13662v1","category":"math.PR"}
{"created":"2024-02-21 09:41:56","title":"Stable Update of Regression Trees","abstract":"Updating machine learning models with new information usually improves their predictive performance, yet, in many applications, it is also desirable to avoid changing the model predictions too much. This property is called stability. In most cases when stability matters, so does explainability. We therefore focus on the stability of an inherently explainable machine learning method, namely regression trees. We aim to use the notion of empirical stability and design algorithms for updating regression trees that provide a way to balance between predictability and empirical stability. To achieve this, we propose a regularization method, where data points are weighted based on the uncertainty in the initial model. The balance between predictability and empirical stability can be adjusted through hyperparameters. This regularization method is evaluated in terms of loss and stability and assessed on a broad range of data characteristics. The results show that the proposed update method improves stability while achieving similar or better predictive performance. This shows that it is possible to achieve both predictive and stable results when updating regression trees.","sentences":["Updating machine learning models with new information usually improves their predictive performance, yet, in many applications, it is also desirable to avoid changing the model predictions too much.","This property is called stability.","In most cases when stability matters, so does explainability.","We therefore focus on the stability of an inherently explainable machine learning method, namely regression trees.","We aim to use the notion of empirical stability and design algorithms for updating regression trees that provide a way to balance between predictability and empirical stability.","To achieve this, we propose a regularization method, where data points are weighted based on the uncertainty in the initial model.","The balance between predictability and empirical stability can be adjusted through hyperparameters.","This regularization method is evaluated in terms of loss and stability and assessed on a broad range of data characteristics.","The results show that the proposed update method improves stability while achieving similar or better predictive performance.","This shows that it is possible to achieve both predictive and stable results when updating regression trees."],"url":"http://arxiv.org/abs/2402.13655v1","category":"cs.LG"}
{"created":"2024-02-21 09:18:44","title":"Green AI: A Preliminary Empirical Study on Energy Consumption in DL Models Across Different Runtime Infrastructures","abstract":"Deep Learning (DL) frameworks such as PyTorch and TensorFlow include runtime infrastructures responsible for executing trained models on target hardware, managing memory, data transfers, and multi-accelerator execution, if applicable. Additionally, it is a common practice to deploy pre-trained models on environments distinct from their native development settings. This led to the introduction of interchange formats such as ONNX, which includes its runtime infrastructure, and ONNX Runtime, which work as standard formats that can be used across diverse DL frameworks and languages. Even though these runtime infrastructures have a great impact on inference performance, no previous paper has investigated their energy efficiency. In this study, we monitor the energy consumption and inference time in the runtime infrastructures of three well-known DL frameworks as well as ONNX, using three various DL models. To have nuance in our investigation, we also examine the impact of using different execution providers. We find out that the performance and energy efficiency of DL are difficult to predict. One framework, MXNet, outperforms both PyTorch and TensorFlow for the computer vision models using batch size 1, due to efficient GPU usage and thus low CPU usage. However, batch size 64 makes PyTorch and MXNet practically indistinguishable, while TensorFlow is outperformed consistently. For BERT, PyTorch exhibits the best performance. Converting the models to ONNX usually yields significant performance improvements but the ONNX converted ResNet model with batch size 64 consumes approximately 10% more energy and time than the original PyTorch model.","sentences":["Deep Learning (DL) frameworks such as PyTorch and TensorFlow include runtime infrastructures responsible for executing trained models on target hardware, managing memory, data transfers, and multi-accelerator execution, if applicable.","Additionally, it is a common practice to deploy pre-trained models on environments distinct from their native development settings.","This led to the introduction of interchange formats such as ONNX, which includes its runtime infrastructure, and ONNX Runtime, which work as standard formats that can be used across diverse DL frameworks and languages.","Even though these runtime infrastructures have a great impact on inference performance, no previous paper has investigated their energy efficiency.","In this study, we monitor the energy consumption and inference time in the runtime infrastructures of three well-known DL frameworks as well as ONNX, using three various DL models.","To have nuance in our investigation, we also examine the impact of using different execution providers.","We find out that the performance and energy efficiency of DL are difficult to predict.","One framework, MXNet, outperforms both PyTorch and TensorFlow for the computer vision models using batch size 1, due to efficient GPU usage and thus low CPU usage.","However, batch size 64 makes PyTorch and MXNet practically indistinguishable, while TensorFlow is outperformed consistently.","For BERT, PyTorch exhibits the best performance.","Converting the models to ONNX usually yields significant performance improvements but the ONNX converted ResNet model with batch size 64 consumes approximately 10% more energy and time than the original PyTorch model."],"url":"http://arxiv.org/abs/2402.13640v1","category":"cs.SE"}
{"created":"2024-02-21 09:18:42","title":"Sliding-mediated ferroelectric phase transition in CuInP2S6 under pressure","abstract":"Interlayer stacking order has recently emerged as a unique degree of freedom to control crystal symmetry and physical properties in two-dimensional van der Waals (vdW) materials and heterostructures. By tuning the layer stacking pattern, symmetry-breaking and electric polarization can be created in otherwise non-polar crystals, whose polarization reversal depends on the interlayer sliding motion. Herein, we demonstrate that in a vdW layered ferroelectric, its existing polarization is closely coupled to the interlayer sliding driven by hydrostatic pressure. Through combined structural, electrical, vibrational characterizations, and theoretical calculations, we clearly map out the structural evolution of CuInP2S6 under pressure. A tendency towards a high polarization state is observed in the low-pressure region, followed by an interlayer-sliding-mediated phase transition from a monoclinic to a trigonal phase. Along the transformation pathway, the displacive-instable Cu ion serves as a pivot point that regulates the interlayer interaction in response to external pressure. The rich phase diagram of CuInP2S6, which is enabled by stacking orders, sheds light on the physics of vdW ferroelectricity and opens an alternative route to tailoring long-range order in vdW layered crystals.","sentences":["Interlayer stacking order has recently emerged as a unique degree of freedom to control crystal symmetry and physical properties in two-dimensional van der Waals (vdW) materials and heterostructures.","By tuning the layer stacking pattern, symmetry-breaking and electric polarization can be created in otherwise non-polar crystals, whose polarization reversal depends on the interlayer sliding motion.","Herein, we demonstrate that in a vdW layered ferroelectric, its existing polarization is closely coupled to the interlayer sliding driven by hydrostatic pressure.","Through combined structural, electrical, vibrational characterizations, and theoretical calculations, we clearly map out the structural evolution of CuInP2S6 under pressure.","A tendency towards a high polarization state is observed in the low-pressure region, followed by an interlayer-sliding-mediated phase transition from a monoclinic to a trigonal phase.","Along the transformation pathway, the displacive-instable Cu ion serves as a pivot point that regulates the interlayer interaction in response to external pressure.","The rich phase diagram of CuInP2S6, which is enabled by stacking orders, sheds light on the physics of vdW ferroelectricity and opens an alternative route to tailoring long-range order in vdW layered crystals."],"url":"http://arxiv.org/abs/2402.13639v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-21 09:06:04","title":"Adversarial Purification and Fine-tuning for Robust UDC Image Restoration","abstract":"This study delves into the enhancement of Under-Display Camera (UDC) image restoration models, focusing on their robustness against adversarial attacks. Despite its innovative approach to seamless display integration, UDC technology faces unique image degradation challenges exacerbated by the susceptibility to adversarial perturbations. Our research initially conducts an in-depth robustness evaluation of deep-learning-based UDC image restoration models by employing several white-box and black-box attacking methods. This evaluation is pivotal in understanding the vulnerabilities of current UDC image restoration techniques. Following the assessment, we introduce a defense framework integrating adversarial purification with subsequent fine-tuning processes. First, our approach employs diffusion-based adversarial purification, effectively neutralizing adversarial perturbations. Then, we apply the fine-tuning methodologies to refine the image restoration models further, ensuring that the quality and fidelity of the restored images are maintained. The effectiveness of our proposed approach is validated through extensive experiments, showing marked improvements in resilience against typical adversarial attacks.","sentences":["This study delves into the enhancement of Under-Display Camera (UDC) image restoration models, focusing on their robustness against adversarial attacks.","Despite its innovative approach to seamless display integration, UDC technology faces unique image degradation challenges exacerbated by the susceptibility to adversarial perturbations.","Our research initially conducts an in-depth robustness evaluation of deep-learning-based UDC image restoration models by employing several white-box and black-box attacking methods.","This evaluation is pivotal in understanding the vulnerabilities of current UDC image restoration techniques.","Following the assessment, we introduce a defense framework integrating adversarial purification with subsequent fine-tuning processes.","First, our approach employs diffusion-based adversarial purification, effectively neutralizing adversarial perturbations.","Then, we apply the fine-tuning methodologies to refine the image restoration models further, ensuring that the quality and fidelity of the restored images are maintained.","The effectiveness of our proposed approach is validated through extensive experiments, showing marked improvements in resilience against typical adversarial attacks."],"url":"http://arxiv.org/abs/2402.13629v1","category":"eess.IV"}
{"created":"2024-02-21 07:43:18","title":"Affective Computing for Healthcare: Recent Trends, Applications, Challenges, and Beyond","abstract":"Affective computing, which aims to recognize, interpret, and understand human emotions, provides benefits in healthcare, such as improving patient care and enhancing doctor-patient communication. However, there is a noticeable absence of a comprehensive summary of recent advancements in affective computing for healthcare, which could pose difficulties for researchers entering this field. To address this, our paper aims to provide an extensive literature review of related studies published in the last five years. We begin by analyzing trends, benefits, and limitations of recent datasets and affective computing methods devised for healthcare. Subsequently, we highlight several healthcare application hotspots of current technologies that could be promising for real-world deployment. Through our analysis, we identify and discuss some ongoing challenges in the field as evidenced by the literature. Concluding with a thorough review, we further offer potential future research directions and hope our findings and insights could guide related researchers to make better contributions to the evolution of affective computing in healthcare.","sentences":["Affective computing, which aims to recognize, interpret, and understand human emotions, provides benefits in healthcare, such as improving patient care and enhancing doctor-patient communication.","However, there is a noticeable absence of a comprehensive summary of recent advancements in affective computing for healthcare, which could pose difficulties for researchers entering this field.","To address this, our paper aims to provide an extensive literature review of related studies published in the last five years.","We begin by analyzing trends, benefits, and limitations of recent datasets and affective computing methods devised for healthcare.","Subsequently, we highlight several healthcare application hotspots of current technologies that could be promising for real-world deployment.","Through our analysis, we identify and discuss some ongoing challenges in the field as evidenced by the literature.","Concluding with a thorough review, we further offer potential future research directions and hope our findings and insights could guide related researchers to make better contributions to the evolution of affective computing in healthcare."],"url":"http://arxiv.org/abs/2402.13589v1","category":"cs.HC"}
{"created":"2024-02-21 07:31:47","title":"WinoViz: Probing Visual Properties of Objects Under Different States","abstract":"Humans perceive and comprehend different visual properties of an object based on specific contexts. For instance, we know that a banana turns brown ``when it becomes rotten,'' whereas it appears green ``when it is unripe.'' Previous studies on probing visual commonsense knowledge have primarily focused on examining language models' understanding of typical properties (e.g., colors and shapes) of objects. We present WinoViz, a text-only evaluation dataset, consisting of 1,380 examples that probe the reasoning abilities of language models regarding variant visual properties of objects under different contexts or states. Our task is challenging since it requires pragmatic reasoning (finding intended meanings) and visual knowledge reasoning. We also present multi-hop data, a more challenging version of our data, which requires multi-step reasoning chains to solve our task. In our experimental analysis, our findings are: a) Large language models such as GPT-4 demonstrate effective performance, but when it comes to multi-hop data, their performance is significantly degraded. b) Large models perform well on pragmatic reasoning, but visual knowledge reasoning is a bottleneck in our task. c) Vision-language models outperform their language-model counterparts. d) A model with machine-generated images performs poorly in our task. This is due to the poor quality of the generated images.","sentences":["Humans perceive and comprehend different visual properties of an object based on specific contexts.","For instance, we know that a banana turns brown ``when it becomes rotten,'' whereas it appears green ``when it is unripe.''","Previous studies on probing visual commonsense knowledge have primarily focused on examining language models' understanding of typical properties (e.g., colors and shapes) of objects.","We present WinoViz, a text-only evaluation dataset, consisting of 1,380 examples that probe the reasoning abilities of language models regarding variant visual properties of objects under different contexts or states.","Our task is challenging since it requires pragmatic reasoning (finding intended meanings) and visual knowledge reasoning.","We also present multi-hop data, a more challenging version of our data, which requires multi-step reasoning chains to solve our task.","In our experimental analysis, our findings are: a) Large language models such as GPT-4 demonstrate effective performance, but when it comes to multi-hop data, their performance is significantly degraded.","b) Large models perform well on pragmatic reasoning, but visual knowledge reasoning is a bottleneck in our task.","c) Vision-language models outperform their language-model counterparts.","d)","A model with machine-generated images performs poorly in our task.","This is due to the poor quality of the generated images."],"url":"http://arxiv.org/abs/2402.13584v1","category":"cs.CL"}
{"created":"2024-02-21 07:17:10","title":"TransGOP: Transformer-Based Gaze Object Prediction","abstract":"Gaze object prediction aims to predict the location and category of the object that is watched by a human. Previous gaze object prediction works use CNN-based object detectors to predict the object's location. However, we find that Transformer-based object detectors can predict more accurate object location for dense objects in retail scenarios. Moreover, the long-distance modeling capability of the Transformer can help to build relationships between the human head and the gaze object, which is important for the GOP task. To this end, this paper introduces Transformer into the fields of gaze object prediction and proposes an end-to-end Transformer-based gaze object prediction method named TransGOP. Specifically, TransGOP uses an off-the-shelf Transformer-based object detector to detect the location of objects and designs a Transformer-based gaze autoencoder in the gaze regressor to establish long-distance gaze relationships. Moreover, to improve gaze heatmap regression, we propose an object-to-gaze cross-attention mechanism to let the queries of the gaze autoencoder learn the global-memory position knowledge from the object detector. Finally, to make the whole framework end-to-end trained, we propose a Gaze Box loss to jointly optimize the object detector and gaze regressor by enhancing the gaze heatmap energy in the box of the gaze object. Extensive experiments on the GOO-Synth and GOO-Real datasets demonstrate that our TransGOP achieves state-of-the-art performance on all tracks, i.e., object detection, gaze estimation, and gaze object prediction. Our code will be available at https://github.com/chenxi-Guo/TransGOP.git.","sentences":["Gaze object prediction aims to predict the location and category of the object that is watched by a human.","Previous gaze object prediction works use CNN-based object detectors to predict the object's location.","However, we find that Transformer-based object detectors can predict more accurate object location for dense objects in retail scenarios.","Moreover, the long-distance modeling capability of the Transformer can help to build relationships between the human head and the gaze object, which is important for the GOP task.","To this end, this paper introduces Transformer into the fields of gaze object prediction and proposes an end-to-end Transformer-based gaze object prediction method named TransGOP.","Specifically, TransGOP uses an off-the-shelf Transformer-based object detector to detect the location of objects and designs a Transformer-based gaze autoencoder in the gaze regressor to establish long-distance gaze relationships.","Moreover, to improve gaze heatmap regression, we propose an object-to-gaze cross-attention mechanism to let the queries of the gaze autoencoder learn the global-memory position knowledge from the object detector.","Finally, to make the whole framework end-to-end trained, we propose a Gaze Box loss to jointly optimize the object detector and gaze regressor by enhancing the gaze heatmap energy in the box of the gaze object.","Extensive experiments on the GOO-Synth and GOO-Real datasets demonstrate that our TransGOP achieves state-of-the-art performance on all tracks, i.e., object detection, gaze estimation, and gaze object prediction.","Our code will be available at https://github.com/chenxi-Guo/TransGOP.git."],"url":"http://arxiv.org/abs/2402.13578v1","category":"cs.CV"}
{"created":"2024-02-21 06:59:35","title":"Capillary levelling of thin liquid films of power-law rheology","abstract":"We find solutions that describe the levelling of a thin fluid film coating a substrate, comprising a non-Newtonian power law fluid, under the influence of surface tension. We consider the evolution from both periodic and localised initial conditions. These solutions exhibit the generic property that the profiles are weakly singular (that is, higher order derivatives do not exist), at points where the pressure gradient vanishes. Numerical solutions of the thin film equation are shown to approach one of the two cases for appropriate initial conditions.","sentences":["We find solutions that describe the levelling of a thin fluid film coating a substrate, comprising a non-Newtonian power law fluid, under the influence of surface tension.","We consider the evolution from both periodic and localised initial conditions.","These solutions exhibit the generic property that the profiles are weakly singular (that is, higher order derivatives do not exist), at points where the pressure gradient vanishes.","Numerical solutions of the thin film equation are shown to approach one of the two cases for appropriate initial conditions."],"url":"http://arxiv.org/abs/2402.13569v1","category":"physics.flu-dyn"}
{"created":"2024-02-21 06:39:56","title":"Modular forms and hierarchical Yukawa couplings in heterotic Calabi-Yau compactifications","abstract":"We study the modular symmetry in heterotic string theory on Calabi-Yau threefolds. In particular, we examine whether moduli-dependent holomorphic Yukawa couplings are described by modular forms in the context of heterotic string theory with standard embedding. We find that $SL(2,\\mathbb{Z})$ modular symmetry emerges in asymptotic regions of the Calabi-Yau moduli space. The instanton-corrected holomorphic Yukawa couplings are then given by modular forms under $SL(2,\\mathbb{Z})$ or its congruence subgroups such as $\\Gamma_0(3)$ and $\\Gamma_0(4)$. In addition to the modular symmetry, it turns out that another coupling selection rule controls the structure of holomorphic Yukawa couplings. Furthermore, the coexistence of both the positive and negative modular weights for matter fields leads to a hierarchical structure of matter field K\\\"ahler metric. Thus, these holomorphic modular forms and the matter field K\\\"ahler metric play an important role in realizing a hierarchical structure of physical Yukawa couplings.","sentences":["We study the modular symmetry in heterotic string theory on Calabi-Yau threefolds.","In particular, we examine whether moduli-dependent holomorphic Yukawa couplings are described by modular forms in the context of heterotic string theory with standard embedding.","We find that $SL(2,\\mathbb{Z})$ modular symmetry emerges in asymptotic regions of the Calabi-Yau moduli space.","The instanton-corrected holomorphic Yukawa couplings are then given by modular forms under $SL(2,\\mathbb{Z})$ or its congruence subgroups such as $\\Gamma_0(3)$ and $\\Gamma_0(4)$. In addition to the modular symmetry, it turns out that another coupling selection rule controls the structure of holomorphic Yukawa couplings.","Furthermore, the coexistence of both the positive and negative modular weights for matter fields leads to a hierarchical structure of matter field K\\\"ahler metric.","Thus, these holomorphic modular forms and the matter field K\\\"ahler metric play an important role in realizing a hierarchical structure of physical Yukawa couplings."],"url":"http://arxiv.org/abs/2402.13563v1","category":"hep-th"}
{"created":"2024-02-21 06:37:07","title":"Analysis of Multi-Source Language Training in Cross-Lingual Transfer","abstract":"The successful adaptation of multilingual language models (LMs) to a specific language-task pair critically depends on the availability of data tailored for that condition. While cross-lingual transfer (XLT) methods have contributed to addressing this data scarcity problem, there still exists ongoing debate about the mechanisms behind their effectiveness. In this work, we focus on one of promising assumptions about inner workings of XLT, that it encourages multilingual LMs to place greater emphasis on language-agnostic or task-specific features. We test this hypothesis by examining how the patterns of XLT change with a varying number of source languages involved in the process. Our experimental findings show that the use of multiple source languages in XLT-a technique we term Multi-Source Language Training (MSLT)-leads to increased mingling of embedding spaces for different languages, supporting the claim that XLT benefits from making use of language-independent information. On the other hand, we discover that using an arbitrary combination of source languages does not always guarantee better performance. We suggest simple heuristics for identifying effective language combinations for MSLT and empirically prove its effectiveness.","sentences":["The successful adaptation of multilingual language models (LMs) to a specific language-task pair critically depends on the availability of data tailored for that condition.","While cross-lingual transfer (XLT) methods have contributed to addressing this data scarcity problem, there still exists ongoing debate about the mechanisms behind their effectiveness.","In this work, we focus on one of promising assumptions about inner workings of XLT, that it encourages multilingual LMs to place greater emphasis on language-agnostic or task-specific features.","We test this hypothesis by examining how the patterns of XLT change with a varying number of source languages involved in the process.","Our experimental findings show that the use of multiple source languages in XLT-a technique we term Multi-Source Language Training (MSLT)-leads to increased mingling of embedding spaces for different languages, supporting the claim that XLT benefits from making use of language-independent information.","On the other hand, we discover that using an arbitrary combination of source languages does not always guarantee better performance.","We suggest simple heuristics for identifying effective language combinations for MSLT and empirically prove its effectiveness."],"url":"http://arxiv.org/abs/2402.13562v1","category":"cs.CL"}
{"created":"2024-02-21 05:10:32","title":"Minimal warm inflation with a heavy QCD axion","abstract":"We propose the first model of warm inflation in which the particle production emerges directly from coupling the inflaton to Standard Model particles. Warm inflation, an early epoch of sustained accelerated expansion at finite temperature, is a compelling alternative to cold inflation, with distinct predictions for inflationary observables such as the amplitude of fluctuations, the spectral tilt, the tensor-to-scalar ratio, and non-gaussianities. In our model a heavy QCD axion acts as the warm inflaton whose coupling to Standard Model gluons sources the thermal bath during warm inflation. Axion-like couplings to non-Abelian gauge bosons have been considered before as a successful microphysical theory with emerging thermal friction that can maintain finite temperature during inflation via sphaleron heating. However, the presence of light fermions charged under the non- Abelian group suppresses particle production, hindering a realization of warm inflation by coupling to QCD. We point out that the Standard Model quarks can be heavy during warm inflation if the Higgs field resides in a high-energy second minimum which restores efficient sphaleron heating. A subsequent large reheating temperature is required to allow the Higgs field to relax to its electroweak minimum. Exploring a scenario in which hybrid inflation provides the large reheating temperature, we show that future collider and beam dump experiments have discovery potential for a heavy QCD axion taking the role of the minimal warm inflaton.","sentences":["We propose the first model of warm inflation in which the particle production emerges directly from coupling the inflaton to Standard Model particles.","Warm inflation, an early epoch of sustained accelerated expansion at finite temperature, is a compelling alternative to cold inflation, with distinct predictions for inflationary observables such as the amplitude of fluctuations, the spectral tilt, the tensor-to-scalar ratio, and non-gaussianities.","In our model a heavy QCD axion acts as the warm inflaton whose coupling to Standard Model gluons sources the thermal bath during warm inflation.","Axion-like couplings to non-Abelian gauge bosons have been considered before as a successful microphysical theory with emerging thermal friction that can maintain finite temperature during inflation via sphaleron heating.","However, the presence of light fermions charged under the non- Abelian group suppresses particle production, hindering a realization of warm inflation by coupling to QCD.","We point out that the Standard Model quarks can be heavy during warm inflation if the Higgs field resides in a high-energy second minimum which restores efficient sphaleron heating.","A subsequent large reheating temperature is required to allow the Higgs field to relax to its electroweak minimum.","Exploring a scenario in which hybrid inflation provides the large reheating temperature, we show that future collider and beam dump experiments have discovery potential for a heavy QCD axion taking the role of the minimal warm inflaton."],"url":"http://arxiv.org/abs/2402.13535v1","category":"hep-ph"}
{"created":"2024-02-21 04:57:32","title":"Best of Many in Both Worlds: Online Resource Allocation with Predictions under Unknown Arrival Model","abstract":"Online decision-makers today can often obtain predictions on future variables, such as arrivals, demands, inventories, and so on. These predictions can be generated from simple forecasting algorithms for univariate time-series, all the way to state-of-the-art machine learning models that leverage multiple time-series and additional feature information. However, the prediction quality is often unknown to decisions-makers a priori, hence blindly following the predictions can be harmful. In this paper, we address this problem by giving algorithms that take predictions as inputs and perform robustly against the unknown prediction quality.   We consider the online resource allocation problem, one of the most generic models in revenue management and online decision-making. In this problem, a decision maker has a limited amount of resources, and requests arrive sequentially. For each request, the decision-maker needs to decide on an action, which generates a certain amount of rewards and consumes a certain amount of resources, without knowing the future requests. The decision-maker's objective is to maximize the total rewards subject to resource constraints. We take the shadow price of each resource as prediction, which can be obtained by predictions on future requests. Prediction quality is naturally defined to be the $\\ell_1$ distance between the prediction and the actual shadow price. Our main contribution is an algorithm which takes the prediction of unknown quality as an input, and achieves asymptotically optimal performance under both requests arrival models (stochastic and adversarial) without knowing the prediction quality and the requests arrival model beforehand. We show our algorithm's performance matches the best achievable performance of any algorithm had the arrival models and the accuracy of the predictions been known. We empirically validate our algorithm with experiments.","sentences":["Online decision-makers today can often obtain predictions on future variables, such as arrivals, demands, inventories, and so on.","These predictions can be generated from simple forecasting algorithms for univariate time-series, all the way to state-of-the-art machine learning models that leverage multiple time-series and additional feature information.","However, the prediction quality is often unknown to decisions-makers a priori, hence blindly following the predictions can be harmful.","In this paper, we address this problem by giving algorithms that take predictions as inputs and perform robustly against the unknown prediction quality.   ","We consider the online resource allocation problem, one of the most generic models in revenue management and online decision-making.","In this problem, a decision maker has a limited amount of resources, and requests arrive sequentially.","For each request, the decision-maker needs to decide on an action, which generates a certain amount of rewards and consumes a certain amount of resources, without knowing the future requests.","The decision-maker's objective is to maximize the total rewards subject to resource constraints.","We take the shadow price of each resource as prediction, which can be obtained by predictions on future requests.","Prediction quality is naturally defined to be the $\\ell_1$ distance between the prediction and the actual shadow price.","Our main contribution is an algorithm which takes the prediction of unknown quality as an input, and achieves asymptotically optimal performance under both requests arrival models (stochastic and adversarial) without knowing the prediction quality and the requests arrival model beforehand.","We show our algorithm's performance matches the best achievable performance of any algorithm had the arrival models and the accuracy of the predictions been known.","We empirically validate our algorithm with experiments."],"url":"http://arxiv.org/abs/2402.13530v1","category":"math.OC"}
{"created":"2024-02-21 04:55:03","title":"Infrastructure Ombudsman: Mining Future Failure Concerns from Structural Disaster Response","abstract":"Current research concentrates on studying discussions on social media related to structural failures to improve disaster response strategies. However, detecting social web posts discussing concerns about anticipatory failures is under-explored. If such concerns are channeled to the appropriate authorities, it can aid in the prevention and mitigation of potential infrastructural failures. In this paper, we develop an infrastructure ombudsman -- that automatically detects specific infrastructure concerns. Our work considers several recent structural failures in the US. We present a first-of-its-kind dataset of 2,662 social web instances for this novel task mined from Reddit and YouTube.","sentences":["Current research concentrates on studying discussions on social media related to structural failures to improve disaster response strategies.","However, detecting social web posts discussing concerns about anticipatory failures is under-explored.","If such concerns are channeled to the appropriate authorities, it can aid in the prevention and mitigation of potential infrastructural failures.","In this paper, we develop an infrastructure ombudsman -- that automatically detects specific infrastructure concerns.","Our work considers several recent structural failures in the US.","We present a first-of-its-kind dataset of 2,662 social web instances for this novel task mined from Reddit and YouTube."],"url":"http://arxiv.org/abs/2402.13528v1","category":"cs.CY"}
{"created":"2024-02-21 04:15:22","title":"RecMind: Japanese Movie Recommendation Dialogue with Seeker's Internal State","abstract":"Humans pay careful attention to the interlocutor's internal state in dialogues. For example, in recommendation dialogues, we make recommendations while estimating the seeker's internal state, such as his/her level of knowledge and interest. Since there are no existing annotated resources for the analysis, we constructed RecMind, a Japanese movie recommendation dialogue dataset with annotations of the seeker's internal state at the entity level. Each entity has a subjective label annotated by the seeker and an objective label annotated by the recommender. RecMind also features engaging dialogues with long seeker's utterances, enabling a detailed analysis of the seeker's internal state. Our analysis based on RecMind reveals that entities that the seeker has no knowledge about but has an interest in contribute to recommendation success. We also propose a response generation framework that explicitly considers the seeker's internal state, utilizing the chain-of-thought prompting. The human evaluation results show that our proposed method outperforms the baseline method in both consistency and the success of recommendations.","sentences":["Humans pay careful attention to the interlocutor's internal state in dialogues.","For example, in recommendation dialogues, we make recommendations while estimating the seeker's internal state, such as his/her level of knowledge and interest.","Since there are no existing annotated resources for the analysis, we constructed RecMind, a Japanese movie recommendation dialogue dataset with annotations of the seeker's internal state at the entity level.","Each entity has a subjective label annotated by the seeker and an objective label annotated by the recommender.","RecMind also features engaging dialogues with long seeker's utterances, enabling a detailed analysis of the seeker's internal state.","Our analysis based on RecMind reveals that entities that the seeker has no knowledge about but has an interest in contribute to recommendation success.","We also propose a response generation framework that explicitly considers the seeker's internal state, utilizing the chain-of-thought prompting.","The human evaluation results show that our proposed method outperforms the baseline method in both consistency and the success of recommendations."],"url":"http://arxiv.org/abs/2402.13522v1","category":"cs.CL"}
{"created":"2024-02-21 03:25:26","title":"Avoiding barren plateaus via Gaussian Mixture Model","abstract":"Variational quantum algorithms is one of the most representative algorithms in quantum computing, which has a wide range of applications in quantum machine learning, quantum simulation and other related fields. However, they face challenges associated with the barren plateau phenomenon, especially when dealing with large numbers of qubits, deep circuit layers, or global cost functions, making them often untrainable. In this paper, we propose a novel parameter initialization strategy based on Gaussian Mixture Models. We rigorously prove that, the proposed initialization method consistently avoids the barren plateaus problem for hardware-efficient ansatz with arbitrary length and qubits and any given cost function. Specifically, we find that the gradient norm lower bound provided by the proposed method is independent of the number of qubits $N$ and increases with the circuit depth $L$. Our results strictly highlight the significance of Gaussian Mixture model initialization strategies in determining the trainability of quantum circuits, which provides valuable guidance for future theoretical investigations and practical applications.","sentences":["Variational quantum algorithms is one of the most representative algorithms in quantum computing, which has a wide range of applications in quantum machine learning, quantum simulation and other related fields.","However, they face challenges associated with the barren plateau phenomenon, especially when dealing with large numbers of qubits, deep circuit layers, or global cost functions, making them often untrainable.","In this paper, we propose a novel parameter initialization strategy based on Gaussian Mixture Models.","We rigorously prove that, the proposed initialization method consistently avoids the barren plateaus problem for hardware-efficient ansatz with arbitrary length and qubits and any given cost function.","Specifically, we find that the gradient norm lower bound provided by the proposed method is independent of the number of qubits $N$ and increases with the circuit depth $L$. Our results strictly highlight the significance of Gaussian Mixture model initialization strategies in determining the trainability of quantum circuits, which provides valuable guidance for future theoretical investigations and practical applications."],"url":"http://arxiv.org/abs/2402.13501v1","category":"quant-ph"}
{"created":"2024-02-21 03:08:21","title":"Thermal Weibel instability induced magnetic fields co-exist with linear wakes in laser-ionized plasmas","abstract":"When a moderately intense, few-picoseconds long laser pulse ionizes gas to produce an underdense plasma column, a linear relativistic plasma wave or wake can be excited by the self-modulation instability that may prove useful for multi-bunch acceleration of externally injected electrons or positrons to high energies in a short distance. At the same time, due to the anisotropic temperature distributions of the ionized plasma electrons, the Weibel instability can self-generate magnetic fields throughout such a plasma on a few picosecond timescale. In the present paper we first show using simulations that both these effects do indeed co-exist in space and time in the plasma. Using our simulations, we make preliminary estimates of the transverse emittance growth of an externally injected beam due to the Weibel magnetic fields. We then present results of an experiment that has allowed us to measure the spatiotemporal evolution of the magnetic fields using an ultrashort relativistic electron probe beam. Both the topology and the lifetime of the Weibel instability induced magnetic fields are in reasonable agreement with the simulations.","sentences":["When a moderately intense, few-picoseconds long laser pulse ionizes gas to produce an underdense plasma column, a linear relativistic plasma wave or wake can be excited by the self-modulation instability that may prove useful for multi-bunch acceleration of externally injected electrons or positrons to high energies in a short distance.","At the same time, due to the anisotropic temperature distributions of the ionized plasma electrons, the Weibel instability can self-generate magnetic fields throughout such a plasma on a few picosecond timescale.","In the present paper we first show using simulations that both these effects do indeed co-exist in space and time in the plasma.","Using our simulations, we make preliminary estimates of the transverse emittance growth of an externally injected beam due to the Weibel magnetic fields.","We then present results of an experiment that has allowed us to measure the spatiotemporal evolution of the magnetic fields using an ultrashort relativistic electron probe beam.","Both the topology and the lifetime of the Weibel instability induced magnetic fields are in reasonable agreement with the simulations."],"url":"http://arxiv.org/abs/2402.13493v1","category":"physics.plasm-ph"}
{"created":"2024-02-21 02:50:02","title":"WiFeS observations of nearby southern Type Ia supernova host galaxies","abstract":"We present high-resolution observations of nearby ($z\\lesssim 0.1$) galaxies that have hosted Type Ia supernovae to measure systemic spectroscopic redshifts using the Wide Field Spectrograph (WiFeS) instrument on the Australian National University 2.3 m telescope at Siding Spring Observatory. While most of the galaxies targeted have previous spectroscopic redshifts, we provide demonstrably more accurate and precise redshifts with competitive uncertainties, motivated by potential systematic errors that could bias estimates of the Hubble constant ($H_0$). The WiFeS instrument is remarkably stable; after calibration, the wavelength solution varies by $\\lesssim 0.5$ \\r{A} in red and blue with no evidence of a trend over the course of several years. By virtue of the $25\\times 38$ arcsec field of view, we are always able to redshift the galactic core, or the entire galaxy in the cases where its angular extent is smaller than the field of view, reducing any errors due to galaxy rotation. We observed 185 southern SN Ia host galaxies and redshifted each via at least one spatial region of a) the core, and b) the average over the full-field/entire galaxy. Overall, we find stochastic differences between historical redshifts and our measured redshifts on the order of $\\lesssim 10^{-3}$ with a mean offset of $4.3\\times 10^{-5}$, and normalised median absolute deviation of $1.2\\times 10^{-4}$. We show that a systematic redshift offset at this level is not enough to bias cosmology, as $H_0$ shifts by $+0.1$ km s$^{-1}$ Mpc$^{-1}$ when we replace Pantheon+ redshifts with our own, but the occasional large differences are interesting to note.","sentences":["We present high-resolution observations of nearby ($z\\lesssim 0.1$) galaxies that have hosted Type Ia supernovae to measure systemic spectroscopic redshifts using the Wide Field Spectrograph (WiFeS) instrument on the Australian National University 2.3 m telescope at Siding Spring Observatory.","While most of the galaxies targeted have previous spectroscopic redshifts, we provide demonstrably more accurate and precise redshifts with competitive uncertainties, motivated by potential systematic errors that could bias estimates of the Hubble constant ($H_0$).","The WiFeS instrument is remarkably stable; after calibration, the wavelength solution varies by $\\lesssim 0.5$ \\r{A} in red and blue with no evidence of a trend over the course of several years.","By virtue of the $25\\times 38$ arcsec field of view, we are always able to redshift the galactic core, or the entire galaxy in the cases where its angular extent is smaller than the field of view, reducing any errors due to galaxy rotation.","We observed 185 southern SN Ia host galaxies and redshifted each via at least one spatial region of a) the core, and b) the average over the full-field/entire galaxy.","Overall, we find stochastic differences between historical redshifts and our measured redshifts on the order of $\\lesssim 10^{-3}$ with a mean offset of $4.3\\times 10^{-5}$, and normalised median absolute deviation of $1.2\\times 10^{-4}$.","We show that a systematic redshift offset at this level is not enough to bias cosmology, as $H_0$ shifts by $+0.1$ km s$^{-1}$ Mpc$^{-1}$ when we replace Pantheon+ redshifts with our own, but the occasional large differences are interesting to note."],"url":"http://arxiv.org/abs/2402.13484v1","category":"astro-ph.CO"}
{"created":"2024-02-21 02:19:48","title":"Exploring the dependence of the Hubble constant from the cluster-lensed supernova SN Refsdal on mass model assumptions","abstract":"The Hubble constant, $H_0$, which is a crucial parameter in astrophysics and cosmology, is under significant tension. We explore an independent technique to measure $H_0$ based on the time-delay cosmography with strong gravitational lensing of a supernova lensed by a galaxy cluster, focusing on SN Refsdal in MACS J1149.5+2223, the first gravitationally lensed supernova with resolved multiple images. We carefully examine the dependence of constraints on the Hubble constant on the choice of lens mass models, employing 23 lens mass models with different assumptions on dark matter halos and external perturbations. Remarkably, we observe that the dependence on the choice of lens mass models is not significantly large, suggesting the robustness of the constraint on the Hubble constant from SN Refsdal. We combine measurements for the 23 lens mass models to obtain $H_0=70.0^{+4.7}_{-4.9}km/s/Mpc$ assuming equal weighting. We find that best-fitting Hubble constant values correlate with radial density profiles of the lensing cluster, implying a room for improving the constraint on the Hubble constant with future observations of more multiple images. We also find a clear correlation between best-fitting Hubble constant values and magnification factors of supernova multiple images. This correlation highlights the importance of gravitationally lensed Type Ia supernovae for accurate and robust Hubble constant measurements.","sentences":["The Hubble constant, $H_0$, which is a crucial parameter in astrophysics and cosmology, is under significant tension.","We explore an independent technique to measure $H_0$ based on the time-delay cosmography with strong gravitational lensing of a supernova lensed by a galaxy cluster, focusing on SN Refsdal in MACS J1149.5+2223, the first gravitationally lensed supernova with resolved multiple images.","We carefully examine the dependence of constraints on the Hubble constant on the choice of lens mass models, employing 23 lens mass models with different assumptions on dark matter halos and external perturbations.","Remarkably, we observe that the dependence on the choice of lens mass models is not significantly large, suggesting the robustness of the constraint on the Hubble constant from SN Refsdal.","We combine measurements for the 23 lens mass models to obtain $H_0=70.0^{+4.7}_{-4.9}km/s/Mpc$ assuming equal weighting.","We find that best-fitting Hubble constant values correlate with radial density profiles of the lensing cluster, implying a room for improving the constraint on the Hubble constant with future observations of more multiple images.","We also find a clear correlation between best-fitting Hubble constant values and magnification factors of supernova multiple images.","This correlation highlights the importance of gravitationally lensed Type Ia supernovae for accurate and robust Hubble constant measurements."],"url":"http://arxiv.org/abs/2402.13476v1","category":"astro-ph.CO"}
{"created":"2024-02-21 02:02:11","title":"Generalized linear models with spatial dependence and a functional covariate","abstract":"We extend generalized functional linear models under independence to a situation in which a functional covariate is related to a scalar response variable that exhibits spatial dependence. For estimation, we apply basis expansion and truncation for dimension reduction of the covariate process followed by a composite likelihood estimating equation to handle the spatial dependency. We develop asymptotic results for the proposed model under a repeating lattice asymptotic context, allowing us to construct a confidence interval for the spatial dependence parameter and a confidence band for the parameter function. A binary conditionals model is presented as a concrete illustration and is used in simulation studies to verify the applicability of the asymptotic inferential results.","sentences":["We extend generalized functional linear models under independence to a situation in which a functional covariate is related to a scalar response variable that exhibits spatial dependence.","For estimation, we apply basis expansion and truncation for dimension reduction of the covariate process followed by a composite likelihood estimating equation to handle the spatial dependency.","We develop asymptotic results for the proposed model under a repeating lattice asymptotic context, allowing us to construct a confidence interval for the spatial dependence parameter and a confidence band for the parameter function.","A binary conditionals model is presented as a concrete illustration and is used in simulation studies to verify the applicability of the asymptotic inferential results."],"url":"http://arxiv.org/abs/2402.13472v1","category":"stat.ME"}
{"created":"2024-02-21 01:48:23","title":"Galactic diffuse emission from radio to ultra-high-energy gamma rays in light of up-to-date cosmic ray measurements","abstract":"Cosmic rays travel throughout the Galaxy, leaving traces from radio to ultra-high-energy gamma rays due to interactions with the interstellar gas, radiation field and magnetic field. Therefore, it is necessary to utilize multi-wavelength investigations on the Galactic diffuse emission to shed light on the physics of CR production and propagation. In this work, we present a spatially dependent propagation scenario, taking account of a local source contribution, while making allowances for an additional CR component freshly accelerated near their sources. In this picture, after reproducing the particle measurements at the Solar system, we calculated the intensity and compared the spectral energy distribution to observations from Fermi-LAT and LHAASO-KM2A in the gamma-ray band, and from WMAP and Planck among other radio surveys at lower energies. Multi-band data considered in conjunction, the former comparison exhibits sufficiently good consistency in favor of our model, while the latter calls for improvement in data subtraction and processing. From this standpoint, there remains potential for advanced observations at energies from milli-eVs to MeVs towards the Galactic plane, in order to evaluate our model further and more comprehensively in the future.","sentences":["Cosmic rays travel throughout the Galaxy, leaving traces from radio to ultra-high-energy gamma rays due to interactions with the interstellar gas, radiation field and magnetic field.","Therefore, it is necessary to utilize multi-wavelength investigations on the Galactic diffuse emission to shed light on the physics of CR production and propagation.","In this work, we present a spatially dependent propagation scenario, taking account of a local source contribution, while making allowances for an additional CR component freshly accelerated near their sources.","In this picture, after reproducing the particle measurements at the Solar system, we calculated the intensity and compared the spectral energy distribution to observations from Fermi-LAT and LHAASO-KM2A in the gamma-ray band, and from WMAP and Planck among other radio surveys at lower energies.","Multi-band data considered in conjunction, the former comparison exhibits sufficiently good consistency in favor of our model, while the latter calls for improvement in data subtraction and processing.","From this standpoint, there remains potential for advanced observations at energies from milli-eVs to MeVs towards the Galactic plane, in order to evaluate our model further and more comprehensively in the future."],"url":"http://arxiv.org/abs/2402.13467v1","category":"astro-ph.HE"}
{"created":"2024-02-21 01:42:39","title":"Investigating Why Clinicians Deviate from Standards of Care: Liberating Patients from Mechanical Ventilation in the ICU","abstract":"Clinical practice guidelines, care pathways, and protocols are designed to support evidence-based practices for clinicians; however, their adoption remains a challenge. We set out to investigate why clinicians deviate from the ``Wake Up and Breathe'' protocol, an evidence-based guideline for liberating patients from mechanical ventilation in the intensive care unit (ICU). We conducted over 40 hours of direct observations of live clinical workflows, 17 interviews with frontline care providers, and 4 co-design workshops at three different medical intensive care units. Our findings indicate that unlike prior literature suggests, disagreement with the protocol is not a substantial barrier to adoption. Instead, the uncertainty surrounding the application of the protocol for individual patients leads clinicians to deprioritize adoption in favor of tasks where they have high certainty. Reflecting on these insights, we identify opportunities for technical systems to help clinicians in effectively executing the protocol and discuss future directions for HCI research to support the integration of protocols into clinical practice in complex, team-based healthcare settings.","sentences":["Clinical practice guidelines, care pathways, and protocols are designed to support evidence-based practices for clinicians; however, their adoption remains a challenge.","We set out to investigate why clinicians deviate from the ``Wake Up and Breathe'' protocol, an evidence-based guideline for liberating patients from mechanical ventilation in the intensive care unit (ICU).","We conducted over 40 hours of direct observations of live clinical workflows, 17 interviews with frontline care providers, and 4 co-design workshops at three different medical intensive care units.","Our findings indicate that unlike prior literature suggests, disagreement with the protocol is not a substantial barrier to adoption.","Instead, the uncertainty surrounding the application of the protocol for individual patients leads clinicians to deprioritize adoption in favor of tasks where they have high certainty.","Reflecting on these insights, we identify opportunities for technical systems to help clinicians in effectively executing the protocol and discuss future directions for HCI research to support the integration of protocols into clinical practice in complex, team-based healthcare settings."],"url":"http://arxiv.org/abs/2402.13464v1","category":"cs.HC"}
{"created":"2024-02-21 01:29:30","title":"Non-nucleonic degrees of freedom and the spin structure of the deuteron","abstract":"Electro-disintegration of the deuteron at large $Q^2$ currently represents on of the most promising reactions which allows to probe the bound nuclear state at internal momenta comparable to the rest mass of the nucleon. Large internal momentum in this case makes non-nuncleonic states energetically more feasible and the question that we address is what are the signatures that will indicate the existence of such states in the ground state of the nuclear wave function.   To probe such states we developed a light-front formalism for relativistic description of a composite pseudo-vector system in which emerging proton and neutron are observed in electro-disintegration reaction. In leading high energy approximation our calculations show the possibility of the existence of a new ``incomplete\" P-state-like structure in the deuteron at extremely large internal momenta. The incompleteness of the observed P-state violates the angular condition for the momentum distribution, which can happen only if the deuteron contains non-nucleonic structures, such as $\\Delta\\Delta$, $N^*N$ or hidden color components. Because such states have distinctive angular momentum ($l=1$) they significantly modify the polarization properties of the deuteron wave function. As a result in addition to angular anisotropy of the LF momentum distribution of the nucleon in the deuteron one predicts strong modification of the tensor polarization asymmetry of the deuteron beyond the S- and D- wave predictions at large internal momenta in the deuteron.","sentences":["Electro-disintegration of the deuteron at large $Q^2$ currently represents on of the most promising reactions which allows to probe the bound nuclear state at internal momenta comparable to the rest mass of the nucleon.","Large internal momentum in this case makes non-nuncleonic states energetically more feasible and the question that we address is what are the signatures that will indicate the existence of such states in the ground state of the nuclear wave function.   ","To probe such states we developed a light-front formalism for relativistic description of a composite pseudo-vector system in which emerging proton and neutron are observed in electro-disintegration reaction.","In leading high energy approximation our calculations show the possibility of the existence of a new ``incomplete\" P-state-like structure in the deuteron at extremely large internal momenta.","The incompleteness of the observed P-state violates the angular condition for the momentum distribution, which can happen only if the deuteron contains non-nucleonic structures, such as $\\Delta\\Delta$, $N^*N$ or hidden color components.","Because such states have distinctive angular momentum ($l=1$) they significantly modify the polarization properties of the deuteron wave function.","As a result in addition to angular anisotropy of the LF momentum distribution of the nucleon in the deuteron one predicts strong modification of the tensor polarization asymmetry of the deuteron beyond the S- and D- wave predictions at large internal momenta in the deuteron."],"url":"http://arxiv.org/abs/2402.13458v1","category":"nucl-th"}
{"created":"2024-02-21 01:26:39","title":"Universal equation describes the shape of air bubbles trapped in ice","abstract":"Water usually contains dissolved gases, and because freezing is a purifying process these gases must be expelled for ice to form. Bubbles appear at the freezing front and are trapped into ice, making pores. These pores come in a range of sizes from microns to millimeters and their shapes are peculiar; never spherical but elongated, and usually fore-aft asymmetric. We show that these remarkable shapes result of a delicate balance between freezing, capillarity and mass diffusion. A highly non-linear ordinary differential equation suffices to describe the bubbles, with only two non-dimensional parameters representing the supersaturation and the freezing rate. Our experiments provide us with a large variety of pictures of bubble shapes. We show that all of them have their rounded tip well described by an asymptotic regime of the differential equation, and that most of them can have their full shape quantitatively matched by a full solution. This enables the measurement of the freezing conditions of ice samples, and the design of freeze-cast porous materials. Furthermore, the equation exhibits a bifurcation that explains why some bubbles grow indefinitely and make long cylindrical ``ice worms''.","sentences":["Water usually contains dissolved gases, and because freezing is a purifying process these gases must be expelled for ice to form.","Bubbles appear at the freezing front and are trapped into ice, making pores.","These pores come in a range of sizes from microns to millimeters and their shapes are peculiar; never spherical but elongated, and usually fore-aft asymmetric.","We show that these remarkable shapes result of a delicate balance between freezing, capillarity and mass diffusion.","A highly non-linear ordinary differential equation suffices to describe the bubbles, with only two non-dimensional parameters representing the supersaturation and the freezing rate.","Our experiments provide us with a large variety of pictures of bubble shapes.","We show that all of them have their rounded tip well described by an asymptotic regime of the differential equation, and that most of them can have their full shape quantitatively matched by a full solution.","This enables the measurement of the freezing conditions of ice samples, and the design of freeze-cast porous materials.","Furthermore, the equation exhibits a bifurcation that explains why some bubbles grow indefinitely and make long cylindrical ``ice worms''."],"url":"http://arxiv.org/abs/2402.13456v1","category":"cond-mat.soft"}
{"created":"2024-02-21 01:16:03","title":"A rational logit dynamic for decision-making under uncertainty: well-posedness, vanishing-noise limit, and numerical approximation","abstract":"The classical logit dynamic on a continuous action space for decision-making un-der uncertainty is generalized to the dynamic where the exponential function for the softmax part has been replaced by a rational one that includes the former as a special case. We call the new dynamic as the rational logit dynamic. The use of the rational logit function implies that the uncertainties have a longer tail than that assumed in the classical one. We show that the rational logit dynamic admits a unique measure-valued solution and the solution can be approximated using a fi-nite difference discretization. We also show that the vanishing-noise limit of the rational logit dynamic exists and is different from the best-response one, demon-strating that influences of the uncertainty tail persist in the rational logit dynamic. We finally apply the rational logit dynamic to a unique fishing competition data that has been recently acquired by the authors.","sentences":["The classical logit dynamic on a continuous action space for decision-making un-der uncertainty is generalized to the dynamic where the exponential function for the softmax part has been replaced by a rational one that includes the former as a special case.","We call the new dynamic as the rational logit dynamic.","The use of the rational logit function implies that the uncertainties have a longer tail than that assumed in the classical one.","We show that the rational logit dynamic admits a unique measure-valued solution and the solution can be approximated using a fi-nite difference discretization.","We also show that the vanishing-noise limit of the rational logit dynamic exists and is different from the best-response one, demon-strating that influences of the uncertainty tail persist in the rational logit dynamic.","We finally apply the rational logit dynamic to a unique fishing competition data that has been recently acquired by the authors."],"url":"http://arxiv.org/abs/2402.13453v1","category":"math.DS"}
{"created":"2024-02-21 00:24:36","title":"CoFRIDA: Self-Supervised Fine-Tuning for Human-Robot Co-Painting","abstract":"Prior robot painting and drawing work, such as FRIDA, has focused on decreasing the sim-to-real gap and expanding input modalities for users, but the interaction with these systems generally exists only in the input stages. To support interactive, human-robot collaborative painting, we introduce the Collaborative FRIDA (CoFRIDA) robot painting framework, which can co-paint by modifying and engaging with content already painted by a human collaborator. To improve text-image alignment, FRIDA's major weakness, our system uses pre-trained text-to-image models; however, pre-trained models in the context of real-world co-painting do not perform well because they (1) do not understand the constraints and abilities of the robot and (2) cannot perform co-painting without making unrealistic edits to the canvas and overwriting content. We propose a self-supervised fine-tuning procedure that can tackle both issues, allowing the use of pre-trained state-of-the-art text-image alignment models with robots to enable co-painting in the physical world. Our open-source approach, CoFRIDA, creates paintings and drawings that match the input text prompt more clearly than FRIDA, both from a blank canvas and one with human created work. More generally, our fine-tuning procedure successfully encodes the robot's constraints and abilities into a foundation model, showcasing promising results as an effective method for reducing sim-to-real gaps.","sentences":["Prior robot painting and drawing work, such as FRIDA, has focused on decreasing the sim-to-real gap and expanding input modalities for users, but the interaction with these systems generally exists only in the input stages.","To support interactive, human-robot collaborative painting, we introduce the Collaborative FRIDA (CoFRIDA) robot painting framework, which can co-paint by modifying and engaging with content already painted by a human collaborator.","To improve text-image alignment, FRIDA's major weakness, our system uses pre-trained text-to-image models; however, pre-trained models in the context of real-world co-painting do not perform well because they (1) do not understand the constraints and abilities of the robot and (2) cannot perform co-painting without making unrealistic edits to the canvas and overwriting content.","We propose a self-supervised fine-tuning procedure that can tackle both issues, allowing the use of pre-trained state-of-the-art text-image alignment models with robots to enable co-painting in the physical world.","Our open-source approach, CoFRIDA, creates paintings and drawings that match the input text prompt more clearly than FRIDA, both from a blank canvas and one with human created work.","More generally, our fine-tuning procedure successfully encodes the robot's constraints and abilities into a foundation model, showcasing promising results as an effective method for reducing sim-to-real gaps."],"url":"http://arxiv.org/abs/2402.13442v1","category":"cs.RO"}
{"created":"2024-02-21 00:01:17","title":"Structured Tree Alignment for Evaluation of (Speech) Constituency Parsing","abstract":"We present the structured average intersection-over-union ratio (STRUCT-IOU), a similarity metric between constituency parse trees motivated by the problem of evaluating speech parsers. STRUCT-IOU enables comparison between a constituency parse tree (over automatically recognized spoken word boundaries) with the ground-truth parse (over written words). To compute the metric, we project the ground-truth parse tree to the speech domain by forced alignment, align the projected ground-truth constituents with the predicted ones under certain structured constraints, and calculate the average IOU score across all aligned constituent pairs. STRUCT-IOU takes word boundaries into account and overcomes the challenge that the predicted words and ground truth may not have perfect one-to-one correspondence. Extending to the evaluation of text constituency parsing, we demonstrate that STRUCT-IOU shows higher tolerance to syntactically plausible parses than PARSEVAL (Black et al., 1991).","sentences":["We present the structured average intersection-over-union ratio (STRUCT-IOU), a similarity metric between constituency parse trees motivated by the problem of evaluating speech parsers.","STRUCT-IOU enables comparison between a constituency parse tree (over automatically recognized spoken word boundaries) with the ground-truth parse (over written words).","To compute the metric, we project the ground-truth parse tree to the speech domain by forced alignment, align the projected ground-truth constituents with the predicted ones under certain structured constraints, and calculate the average IOU score across all aligned constituent pairs.","STRUCT-IOU takes word boundaries into account and overcomes the challenge that the predicted words and ground truth may not have perfect one-to-one correspondence.","Extending to the evaluation of text constituency parsing, we demonstrate that STRUCT-IOU shows higher tolerance to syntactically plausible parses than PARSEVAL (Black et al., 1991)."],"url":"http://arxiv.org/abs/2402.13433v1","category":"cs.CL"}
{"created":"2024-02-20 23:22:15","title":"Induced supersolidity and hypersonic flow of a dipolar Bose-Einstein Condensate in a rotating bubble trap","abstract":"Motivated by the recent realization of space-borne Bose-Einstein Condensate (BEC) under micro-gravity conditions, we extend the understanding of ultracold dipolar bosonic gases by exploring their behavior in a novel trapping configuration known as the ``bubble trap\" topology. Utilizing the three-dimensional numerical simulations within the extended Gross-Pitaevskii framework, we unveil diverse ground state phases in such a static curved topology. Subsequently, we investigate the influence of rotation on a dipolar BEC confined to the surface of a spherical bubble. Our findings reveal that the rotation of a bubble trap with certain rotation frequencies can modify the effective local dipole-dipole interaction strength, leading to the induction of supersolidity and the formation of quantum droplets. In addition, we demonstrate that a bubble trap can sustain high circulation and the flow also persists for a longer time. Significantly, adjusting the rf detuning parameter allows the condensate to achieve hypersonic velocity. Finally, we explore the impact of drastic change in the topological nature of the trap on the rotating dipolar BEC, transitioning from a filled shell trap to a bubble trap and vice versa.","sentences":["Motivated by the recent realization of space-borne Bose-Einstein Condensate (BEC) under micro-gravity conditions, we extend the understanding of ultracold dipolar bosonic gases by exploring their behavior in a novel trapping configuration known as the ``bubble trap\" topology.","Utilizing the three-dimensional numerical simulations within the extended Gross-Pitaevskii framework, we unveil diverse ground state phases in such a static curved topology.","Subsequently, we investigate the influence of rotation on a dipolar BEC confined to the surface of a spherical bubble.","Our findings reveal that the rotation of a bubble trap with certain rotation frequencies can modify the effective local dipole-dipole interaction strength, leading to the induction of supersolidity and the formation of quantum droplets.","In addition, we demonstrate that a bubble trap can sustain high circulation and the flow also persists for a longer time.","Significantly, adjusting the rf detuning parameter allows the condensate to achieve hypersonic velocity.","Finally, we explore the impact of drastic change in the topological nature of the trap on the rotating dipolar BEC, transitioning from a filled shell trap to a bubble trap and vice versa."],"url":"http://arxiv.org/abs/2402.13422v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-20 23:04:06","title":"Unlocking the `Why' of Buying: Introducing a New Dataset and Benchmark for Purchase Reason and Post-Purchase Experience","abstract":"Explanations are crucial for enhancing user trust and understanding within modern recommendation systems. To build truly explainable systems, we need high-quality datasets that elucidate why users make choices. While previous efforts have focused on extracting users' post-purchase sentiment in reviews, they ignore the reasons behind the decision to buy.   In our work, we propose a novel purchase reason explanation task. To this end, we introduce an LLM-based approach to generate a dataset that consists of textual explanations of why real users make certain purchase decisions. We induce LLMs to explicitly distinguish between the reasons behind purchasing a product and the experience after the purchase in a user review. An automated, LLM-driven evaluation, as well as a small scale human evaluation, confirms the effectiveness of our approach to obtaining high-quality, personalized explanations. We benchmark this dataset on two personalized explanation generation tasks. We release the code and prompts to spur further research.","sentences":["Explanations are crucial for enhancing user trust and understanding within modern recommendation systems.","To build truly explainable systems, we need high-quality datasets that elucidate why users make choices.","While previous efforts have focused on extracting users' post-purchase sentiment in reviews, they ignore the reasons behind the decision to buy.   ","In our work, we propose a novel purchase reason explanation task.","To this end, we introduce an LLM-based approach to generate a dataset that consists of textual explanations of why real users make certain purchase decisions.","We induce LLMs to explicitly distinguish between the reasons behind purchasing a product and the experience after the purchase in a user review.","An automated, LLM-driven evaluation, as well as a small scale human evaluation, confirms the effectiveness of our approach to obtaining high-quality, personalized explanations.","We benchmark this dataset on two personalized explanation generation tasks.","We release the code and prompts to spur further research."],"url":"http://arxiv.org/abs/2402.13417v1","category":"cs.IR"}
{"created":"2024-02-20 23:01:16","title":"Birkhoff-James classification of norm's properties","abstract":"For an arbitrary normed space $\\mathcal X$ over a field $\\mathbb F \\in \\{ \\mathbb R, \\mathbb C \\}$, we define the directed graph $\\Gamma(\\mathcal X)$ induced by Birkhoff-James orthogonality on the projective space $\\mathbb P(\\mathcal X)$, and also its nonprojective counterpart $\\Gamma_0(\\mathcal X)$. We show that, in finite-dimensional normed spaces, $\\Gamma(\\mathcal X)$ carries all the information about the dimension, smooth points, and norm's maximal faces. It also allows to determine whether the norm is a supremum norm or not, and thus classifies finite-dimensional abelian $C^\\ast$-algebras among other normed spaces. We further establish the necessary and sufficient conditions under which the graph $\\Gamma_0(\\mathcal{R})$ of a (real or complex) Radon plane $\\mathcal{R}$ is isomorphic to the graph $\\Gamma_0(\\mathbb F^2, \\|\\cdot\\|_2)$ of the two-dimensional Hilbert space and construct examples of such nonsmooth Radon planes.","sentences":["For an arbitrary normed space $\\mathcal X$ over a field $\\mathbb F \\in \\{ \\mathbb R, \\mathbb C \\}$, we define the directed graph $\\Gamma(\\mathcal X)$ induced by Birkhoff-James orthogonality on the projective space $\\mathbb P(\\mathcal X)$, and also its nonprojective counterpart $\\Gamma_0(\\mathcal X)$.","We show that, in finite-dimensional normed spaces, $\\Gamma(\\mathcal X)$ carries all the information about the dimension, smooth points, and norm's maximal faces.","It also allows to determine whether the norm is a supremum norm or not, and thus classifies finite-dimensional abelian $C^\\ast$-algebras among other normed spaces.","We further establish the necessary and sufficient conditions under which the graph $\\Gamma_0(\\mathcal{R})$ of a (real or complex)","Radon plane $\\mathcal{R}$ is isomorphic to the graph $\\Gamma_0(\\mathbb F^2, \\|\\cdot\\|_2)$ of the two-dimensional Hilbert space and construct examples of such nonsmooth Radon planes."],"url":"http://arxiv.org/abs/2402.13416v1","category":"math.FA"}
{"created":"2024-02-20 22:56:23","title":"Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text","abstract":"Although Large Language Models (LLMs) excel at addressing straightforward reasoning tasks, they frequently struggle with difficulties when confronted by more complex multi-step reasoning due to a range of factors. Firstly, natural language often encompasses complex relationships among entities, making it challenging to maintain a clear reasoning chain over longer spans. Secondly, the abundance of linguistic diversity means that the same entities and relationships can be expressed using different terminologies and structures, complicating the task of identifying and establishing connections between multiple pieces of information. Graphs provide an effective solution to represent data rich in relational information and capture long-term dependencies among entities. To harness the potential of graphs, our paper introduces Structure Guided Prompt, an innovative three-stage task-agnostic prompting framework designed to improve the multi-step reasoning capabilities of LLMs in a zero-shot setting. This framework explicitly converts unstructured text into a graph via LLMs and instructs them to navigate this graph using task-specific strategies to formulate responses. By effectively organizing information and guiding navigation, it enables LLMs to provide more accurate and context-aware responses. Our experiments show that this framework significantly enhances the reasoning capabilities of LLMs, enabling them to excel in a broader spectrum of natural language scenarios.","sentences":["Although Large Language Models (LLMs) excel at addressing straightforward reasoning tasks, they frequently struggle with difficulties when confronted by more complex multi-step reasoning due to a range of factors.","Firstly, natural language often encompasses complex relationships among entities, making it challenging to maintain a clear reasoning chain over longer spans.","Secondly, the abundance of linguistic diversity means that the same entities and relationships can be expressed using different terminologies and structures, complicating the task of identifying and establishing connections between multiple pieces of information.","Graphs provide an effective solution to represent data rich in relational information and capture long-term dependencies among entities.","To harness the potential of graphs, our paper introduces Structure Guided Prompt, an innovative three-stage task-agnostic prompting framework designed to improve the multi-step reasoning capabilities of LLMs in a zero-shot setting.","This framework explicitly converts unstructured text into a graph via LLMs and instructs them to navigate this graph using task-specific strategies to formulate responses.","By effectively organizing information and guiding navigation, it enables LLMs to provide more accurate and context-aware responses.","Our experiments show that this framework significantly enhances the reasoning capabilities of LLMs, enabling them to excel in a broader spectrum of natural language scenarios."],"url":"http://arxiv.org/abs/2402.13415v1","category":"cs.CL"}
{"created":"2024-02-20 22:50:41","title":"Harnessing Large Language Models as Post-hoc Correctors","abstract":"As Machine Learning (ML) models grow in size and demand higher-quality training data, the expenses associated with re-training and fine-tuning these models are escalating rapidly. Inspired by recent impressive achievements of Large Language Models (LLMs) in different fields, this paper delves into the question: can LLMs efficiently improve an ML's performance at a minimal cost? We show that, through our proposed training-free framework LlmCorr, an LLM can work as a post-hoc corrector to propose corrections for the predictions of an arbitrary ML model. In particular, we form a contextual knowledge database by incorporating the dataset's label information and the ML model's predictions on the validation dataset. Leveraging the in-context learning capability of LLMs, we ask the LLM to summarise the instances in which the ML model makes mistakes and the correlation between primary predictions and true labels. Following this, the LLM can transfer its acquired knowledge to suggest corrections for the ML model's predictions. Our experimental results on the challenging molecular predictions show that LlmCorr improves the performance of a number of models by up to 39%.","sentences":["As Machine Learning (ML) models grow in size and demand higher-quality training data, the expenses associated with re-training and fine-tuning these models are escalating rapidly.","Inspired by recent impressive achievements of Large Language Models (LLMs) in different fields, this paper delves into the question: can LLMs efficiently improve an ML's performance at a minimal cost?","We show that, through our proposed training-free framework LlmCorr, an LLM can work as a post-hoc corrector to propose corrections for the predictions of an arbitrary ML model.","In particular, we form a contextual knowledge database by incorporating the dataset's label information and the ML model's predictions on the validation dataset.","Leveraging the in-context learning capability of LLMs, we ask the LLM to summarise the instances in which the ML model makes mistakes and the correlation between primary predictions and true labels.","Following this, the LLM can transfer its acquired knowledge to suggest corrections for the ML model's predictions.","Our experimental results on the challenging molecular predictions show that LlmCorr improves the performance of a number of models by up to 39%."],"url":"http://arxiv.org/abs/2402.13414v1","category":"cs.LG"}
{"created":"2024-02-20 22:34:53","title":"Bayesian Neural Networks with Domain Knowledge Priors","abstract":"Bayesian neural networks (BNNs) have recently gained popularity due to their ability to quantify model uncertainty. However, specifying a prior for BNNs that captures relevant domain knowledge is often extremely challenging. In this work, we propose a framework for integrating general forms of domain knowledge (i.e., any knowledge that can be represented by a loss function) into a BNN prior through variational inference, while enabling computationally efficient posterior inference and sampling. Specifically, our approach results in a prior over neural network weights that assigns high probability mass to models that better align with our domain knowledge, leading to posterior samples that also exhibit this behavior. We show that BNNs using our proposed domain knowledge priors outperform those with standard priors (e.g., isotropic Gaussian, Gaussian process), successfully incorporating diverse types of prior information such as fairness, physics rules, and healthcare knowledge and achieving better predictive performance. We also present techniques for transferring the learned priors across different model architectures, demonstrating their broad utility across various settings.","sentences":["Bayesian neural networks (BNNs) have recently gained popularity due to their ability to quantify model uncertainty.","However, specifying a prior for BNNs that captures relevant domain knowledge is often extremely challenging.","In this work, we propose a framework for integrating general forms of domain knowledge (i.e., any knowledge that can be represented by a loss function) into a BNN prior through variational inference, while enabling computationally efficient posterior inference and sampling.","Specifically, our approach results in a prior over neural network weights that assigns high probability mass to models that better align with our domain knowledge, leading to posterior samples that also exhibit this behavior.","We show that BNNs using our proposed domain knowledge priors outperform those with standard priors (e.g., isotropic Gaussian, Gaussian process), successfully incorporating diverse types of prior information such as fairness, physics rules, and healthcare knowledge and achieving better predictive performance.","We also present techniques for transferring the learned priors across different model architectures, demonstrating their broad utility across various settings."],"url":"http://arxiv.org/abs/2402.13410v1","category":"cs.LG"}
{"created":"2024-02-20 22:15:13","title":"Layout-to-Image Generation with Localized Descriptions using ControlNet with Cross-Attention Control","abstract":"While text-to-image diffusion models can generate highquality images from textual descriptions, they generally lack fine-grained control over the visual composition of the generated images. Some recent works tackle this problem by training the model to condition the generation process on additional input describing the desired image layout. Arguably the most popular among such methods, ControlNet, enables a high degree of control over the generated image using various types of conditioning inputs (e.g. segmentation maps). However, it still lacks the ability to take into account localized textual descriptions that indicate which image region is described by which phrase in the prompt. In this work, we show the limitations of ControlNet for the layout-to-image task and enable it to use localized descriptions using a training-free approach that modifies the crossattention scores during generation. We adapt and investigate several existing cross-attention control methods in the context of ControlNet and identify shortcomings that cause failure (concept bleeding) or image degradation under specific conditions. To address these shortcomings, we develop a novel cross-attention manipulation method in order to maintain image quality while improving control. Qualitative and quantitative experimental studies focusing on challenging cases are presented, demonstrating the effectiveness of the investigated general approach, and showing the improvements obtained by the proposed cross-attention control method.","sentences":["While text-to-image diffusion models can generate highquality images from textual descriptions, they generally lack fine-grained control over the visual composition of the generated images.","Some recent works tackle this problem by training the model to condition the generation process on additional input describing the desired image layout.","Arguably the most popular among such methods, ControlNet, enables a high degree of control over the generated image using various types of conditioning inputs (e.g. segmentation maps).","However, it still lacks the ability to take into account localized textual descriptions that indicate which image region is described by which phrase in the prompt.","In this work, we show the limitations of ControlNet for the layout-to-image task and enable it to use localized descriptions using a training-free approach that modifies the crossattention scores during generation.","We adapt and investigate several existing cross-attention control methods in the context of ControlNet and identify shortcomings that cause failure (concept bleeding) or image degradation under specific conditions.","To address these shortcomings, we develop a novel cross-attention manipulation method in order to maintain image quality while improving control.","Qualitative and quantitative experimental studies focusing on challenging cases are presented, demonstrating the effectiveness of the investigated general approach, and showing the improvements obtained by the proposed cross-attention control method."],"url":"http://arxiv.org/abs/2402.13404v1","category":"cs.CV"}
{"created":"2024-02-20 22:12:33","title":"Towards accelerating physical discovery via non-interactive and interactive multi-fidelity Bayesian Optimization: Current challenges and future opportunities","abstract":"Both computational and experimental material discovery bring forth the challenge of exploring multidimensional and often non-differentiable parameter spaces, such as phase diagrams of Hamiltonians with multiple interactions, composition spaces of combinatorial libraries, processing spaces, and molecular embedding spaces. Often these systems are expensive or time-consuming to evaluate a single instance, and hence classical approaches based on exhaustive grid or random search are too data intensive. This resulted in strong interest towards active learning methods such as Bayesian optimization (BO) where the adaptive exploration occurs based on human learning (discovery) objective. However, classical BO is based on a predefined optimization target, and policies balancing exploration and exploitation are purely data driven. In practical settings, the domain expert can pose prior knowledge on the system in form of partially known physics laws and often varies exploration policies during the experiment. Here, we explore interactive workflows building on multi-fidelity BO (MFBO), starting with classical (data-driven) MFBO, then structured (physics-driven) sMFBO, and extending it to allow human in the loop interactive iMFBO workflows for adaptive and domain expert aligned exploration. These approaches are demonstrated over highly non-smooth multi-fidelity simulation data generated from an Ising model, considering spin-spin interaction as parameter space, lattice sizes as fidelity spaces, and the objective as maximizing heat capacity. Detailed analysis and comparison show the impact of physics knowledge injection and on-the-fly human decisions for improved exploration, current challenges, and potential opportunities for algorithm development with combining data, physics and real time human decisions.","sentences":["Both computational and experimental material discovery bring forth the challenge of exploring multidimensional and often non-differentiable parameter spaces, such as phase diagrams of Hamiltonians with multiple interactions, composition spaces of combinatorial libraries, processing spaces, and molecular embedding spaces.","Often these systems are expensive or time-consuming to evaluate a single instance, and hence classical approaches based on exhaustive grid or random search are too data intensive.","This resulted in strong interest towards active learning methods such as Bayesian optimization (BO) where the adaptive exploration occurs based on human learning (discovery) objective.","However, classical BO is based on a predefined optimization target, and policies balancing exploration and exploitation are purely data driven.","In practical settings, the domain expert can pose prior knowledge on the system in form of partially known physics laws and often varies exploration policies during the experiment.","Here, we explore interactive workflows building on multi-fidelity BO (MFBO), starting with classical (data-driven) MFBO, then structured (physics-driven) sMFBO, and extending it to allow human in the loop interactive iMFBO workflows for adaptive and domain expert aligned exploration.","These approaches are demonstrated over highly non-smooth multi-fidelity simulation data generated from an Ising model, considering spin-spin interaction as parameter space, lattice sizes as fidelity spaces, and the objective as maximizing heat capacity.","Detailed analysis and comparison show the impact of physics knowledge injection and on-the-fly human decisions for improved exploration, current challenges, and potential opportunities for algorithm development with combining data, physics and real time human decisions."],"url":"http://arxiv.org/abs/2402.13402v1","category":"cs.LG"}
{"created":"2024-02-20 22:06:43","title":"Dissipative solutions to the model of a general compressible viscous fluid with the Coulomb friction law boundary condition","abstract":"We study a model of a general compressible viscous fluid subject to the Coulomb friction law boundary condition. For this model, we introduce a dissipative formulation and prove the existence of dissipative solutions. The proof of this result consists of a three-level approximation method: A Galerkin approximation, the classical parabolic regularization of the continuity equation as well as convex regularizations of the potential generating the viscous stress and the boundary terms incorporating the Coulomb friction law into the dissipative formulation. This approach combines the techniques already known from the proof of the existence of dissipative solutions to a model of general compressible viscous fluids under inflow-outflow boundary conditions as well as the proof of the existence of a weak solution to the incompressible Navier-Stokes equations under the Coulomb friction law boundary condition. It is the first time that this type of boundary condition is considered in the case of compressible flow.","sentences":["We study a model of a general compressible viscous fluid subject to the Coulomb friction law boundary condition.","For this model, we introduce a dissipative formulation and prove the existence of dissipative solutions.","The proof of this result consists of a three-level approximation method: A Galerkin approximation, the classical parabolic regularization of the continuity equation as well as convex regularizations of the potential generating the viscous stress and the boundary terms incorporating the Coulomb friction law into the dissipative formulation.","This approach combines the techniques already known from the proof of the existence of dissipative solutions to a model of general compressible viscous fluids under inflow-outflow boundary conditions as well as the proof of the existence of a weak solution to the incompressible Navier-Stokes equations under the Coulomb friction law boundary condition.","It is the first time that this type of boundary condition is considered in the case of compressible flow."],"url":"http://arxiv.org/abs/2402.13401v1","category":"math.AP"}
{"created":"2024-02-20 21:59:41","title":"The Dimension of Self-Directed Learning","abstract":"Understanding the self-directed learning complexity has been an important problem that has captured the attention of the online learning theory community since the early 1990s. Within this framework, the learner is allowed to adaptively choose its next data point in making predictions unlike the setting in adversarial online learning.   In this paper, we study the self-directed learning complexity in both the binary and multi-class settings, and we develop a dimension, namely $SDdim$, that exactly characterizes the self-directed learning mistake-bound for any concept class. The intuition behind $SDdim$ can be understood as a two-player game called the \"labelling game\". Armed with this two-player game, we calculate $SDdim$ on a whole host of examples with notable results on axis-aligned rectangles, VC dimension $1$ classes, and linear separators. We demonstrate several learnability gaps with a central focus on self-directed learning and offline sequence learning models that include either the best or worst ordering. Finally, we extend our analysis to the self-directed binary agnostic setting where we derive upper and lower bounds.","sentences":["Understanding the self-directed learning complexity has been an important problem that has captured the attention of the online learning theory community since the early 1990s.","Within this framework, the learner is allowed to adaptively choose its next data point in making predictions unlike the setting in adversarial online learning.   ","In this paper, we study the self-directed learning complexity in both the binary and multi-class settings, and we develop a dimension, namely $SDdim$, that exactly characterizes the self-directed learning mistake-bound for any concept class.","The intuition behind $SDdim$ can be understood as a two-player game called the \"labelling game\".","Armed with this two-player game, we calculate $SDdim$ on a whole host of examples with notable results on axis-aligned rectangles, VC dimension $1$ classes, and linear separators.","We demonstrate several learnability gaps with a central focus on self-directed learning and offline sequence learning models that include either the best or worst ordering.","Finally, we extend our analysis to the self-directed binary agnostic setting where we derive upper and lower bounds."],"url":"http://arxiv.org/abs/2402.13400v1","category":"stat.ML"}
{"created":"2024-02-20 21:45:00","title":"An SEIR network epidemic model with manual and digital contact tracing allowing delays","abstract":"We consider an SEIR epidemic model on a network also allowing random contacts, where recovered individuals could either recover naturally or be diagnosed. Upon diagnosis, manual contact tracing is triggered such that each infected network contact is reported, tested and isolated with some probability and after a random delay. Additionally, digital tracing (based on a tracing app) is triggered if the diagnosed individual is an app-user, and then all of its app-using infectees are immediately notified and isolated. The early phase of the epidemic with manual and/or digital tracing is approximated by different multi-type branching processes, and three respective reproduction numbers are derived. The effectiveness of both contact tracing mechanisms is numerically quantified through the reduction of the reproduction number. This shows that app-using fraction plays an essential role in the overall effectiveness of contact tracing. The relative effectiveness of manual tracing compared to digital tracing increases if: more of the transmission occurs on the network, when the tracing delay is shortened, and when the network degree distribution is heavy-tailed. For realistic values, the combined tracing case can reduce $R_0$ by $20-30\\%$, so other preventive measures are needed to reduce the reproduction number down to $1.2-1.4$ for contact tracing to make it successful in avoiding big outbreaks.","sentences":["We consider an SEIR epidemic model on a network also allowing random contacts, where recovered individuals could either recover naturally or be diagnosed.","Upon diagnosis, manual contact tracing is triggered such that each infected network contact is reported, tested and isolated with some probability and after a random delay.","Additionally, digital tracing (based on a tracing app) is triggered if the diagnosed individual is an app-user, and then all of its app-using infectees are immediately notified and isolated.","The early phase of the epidemic with manual and/or digital tracing is approximated by different multi-type branching processes, and three respective reproduction numbers are derived.","The effectiveness of both contact tracing mechanisms is numerically quantified through the reduction of the reproduction number.","This shows that app-using fraction plays an essential role in the overall effectiveness of contact tracing.","The relative effectiveness of manual tracing compared to digital tracing increases if: more of the transmission occurs on the network, when the tracing delay is shortened, and when the network degree distribution is heavy-tailed.","For realistic values, the combined tracing case can reduce $R_0$ by $20-30\\%$, so other preventive measures are needed to reduce the reproduction number down to $1.2-1.4$ for contact tracing to make it successful in avoiding big outbreaks."],"url":"http://arxiv.org/abs/2402.13392v1","category":"physics.soc-ph"}
{"created":"2024-02-20 21:40:51","title":"De-Biasing the Bias: Methods for Improving Disparity Assessments with Noisy Group Measurements","abstract":"Health care decisions are increasingly informed by clinical decision support algorithms, but these algorithms may perpetuate or increase racial and ethnic disparities in access to and quality of health care. Further complicating the problem, clinical data often have missing or poor quality racial and ethnic information, which can lead to misleading assessments of algorithmic bias. We present novel statistical methods that allow for the use of probabilities of racial/ethnic group membership in assessments of algorithm performance and quantify the statistical bias that results from error in these imputed group probabilities. We propose a sensitivity analysis approach to estimating the statistical bias that allows practitioners to assess disparities in algorithm performance under a range of assumed levels of group probability error. We also prove theoretical bounds on the statistical bias for a set of commonly used fairness metrics and describe real-world scenarios where our theoretical results are likely to apply. We present a case study using imputed race and ethnicity from the Bayesian Improved Surname Geocoding (BISG) algorithm for estimation of disparities in a clinical decision support algorithm used to inform osteoporosis treatment. Our novel methods allow policy makers to understand the range of potential disparities under a given algorithm even when race and ethnicity information is missing and to make informed decisions regarding the implementation of machine learning for clinical decision support.","sentences":["Health care decisions are increasingly informed by clinical decision support algorithms, but these algorithms may perpetuate or increase racial and ethnic disparities in access to and quality of health care.","Further complicating the problem, clinical data often have missing or poor quality racial and ethnic information, which can lead to misleading assessments of algorithmic bias.","We present novel statistical methods that allow for the use of probabilities of racial/ethnic group membership in assessments of algorithm performance and quantify the statistical bias that results from error in these imputed group probabilities.","We propose a sensitivity analysis approach to estimating the statistical bias that allows practitioners to assess disparities in algorithm performance under a range of assumed levels of group probability error.","We also prove theoretical bounds on the statistical bias for a set of commonly used fairness metrics and describe real-world scenarios where our theoretical results are likely to apply.","We present a case study using imputed race and ethnicity from the Bayesian Improved Surname Geocoding (BISG) algorithm for estimation of disparities in a clinical decision support algorithm used to inform osteoporosis treatment.","Our novel methods allow policy makers to understand the range of potential disparities under a given algorithm even when race and ethnicity information is missing and to make informed decisions regarding the implementation of machine learning for clinical decision support."],"url":"http://arxiv.org/abs/2402.13391v1","category":"stat.ME"}
{"created":"2024-02-20 21:23:30","title":"Tree semi-separable matrices: a simultaneous generalization of sequentially and hierarchically semi-separable representations","abstract":"We present a unification and generalization of sequentially and hierarchically semi-separable (SSS and HSS) matrices called tree semi-separable (TSS) matrices. Our main result is to show that any dense matrix can be expressed in a TSS format. Here, the dimensions of the generators are specified by the ranks of the Hankel blocks of the matrix. TSS matrices satisfy a graph-induced rank structure (GIRS) property. It is shown that TSS matrices generalize the algebraic properties of SSS and HSS matrices under addition, products, and inversion. Subsequently, TSS matrices admit linear time matrix-vector multiply, matrix-matrix multiply, matrix-matrix addition, inversion, and solvers.","sentences":["We present a unification and generalization of sequentially and hierarchically semi-separable (SSS and HSS) matrices called tree semi-separable (TSS) matrices.","Our main result is to show that any dense matrix can be expressed in a TSS format.","Here, the dimensions of the generators are specified by the ranks of the Hankel blocks of the matrix.","TSS matrices satisfy a graph-induced rank structure (GIRS) property.","It is shown that TSS matrices generalize the algebraic properties of SSS and HSS matrices under addition, products, and inversion.","Subsequently, TSS matrices admit linear time matrix-vector multiply, matrix-matrix multiply, matrix-matrix addition, inversion, and solvers."],"url":"http://arxiv.org/abs/2402.13381v1","category":"math.NA"}
{"created":"2024-02-20 21:09:04","title":"Referee-Meta-Learning for Fast Adaptation of Locational Fairness","abstract":"When dealing with data from distinct locations, machine learning algorithms tend to demonstrate an implicit preference of some locations over the others, which constitutes biases that sabotage the spatial fairness of the algorithm. This unfairness can easily introduce biases in subsequent decision-making given broad adoptions of learning-based solutions in practice. However, locational biases in AI are largely understudied. To mitigate biases over locations, we propose a locational meta-referee (Meta-Ref) to oversee the few-shot meta-training and meta-testing of a deep neural network. Meta-Ref dynamically adjusts the learning rates for training samples of given locations to advocate a fair performance across locations, through an explicit consideration of locational biases and the characteristics of input data. We present a three-phase training framework to learn both a meta-learning-based predictor and an integrated Meta-Ref that governs the fairness of the model. Once trained with a distribution of spatial tasks, Meta-Ref is applied to samples from new spatial tasks (i.e., regions outside the training area) to promote fairness during the fine-tune step. We carried out experiments with two case studies on crop monitoring and transportation safety, which show Meta-Ref can improve locational fairness while keeping the overall prediction quality at a similar level.","sentences":["When dealing with data from distinct locations, machine learning algorithms tend to demonstrate an implicit preference of some locations over the others, which constitutes biases that sabotage the spatial fairness of the algorithm.","This unfairness can easily introduce biases in subsequent decision-making given broad adoptions of learning-based solutions in practice.","However, locational biases in AI are largely understudied.","To mitigate biases over locations, we propose a locational meta-referee (Meta-Ref) to oversee the few-shot meta-training and meta-testing of a deep neural network.","Meta-Ref dynamically adjusts the learning rates for training samples of given locations to advocate a fair performance across locations, through an explicit consideration of locational biases and the characteristics of input data.","We present a three-phase training framework to learn both a meta-learning-based predictor and an integrated Meta-Ref that governs the fairness of the model.","Once trained with a distribution of spatial tasks, Meta-Ref is applied to samples from new spatial tasks (i.e., regions outside the training area) to promote fairness during the fine-tune step.","We carried out experiments with two case studies on crop monitoring and transportation safety, which show Meta-Ref can improve locational fairness while keeping the overall prediction quality at a similar level."],"url":"http://arxiv.org/abs/2402.13379v1","category":"cs.LG"}
{"created":"2024-02-20 20:34:34","title":"Pulsed Complex Plasma in Microgravity","abstract":"A new experimental method for creating void-free complex (``dusty'') plasmas under microgravity conditions is presented. The method is based on a pulsed operation mode of a four-channel radio-frequency generator for plasma sustainment. A dust cloud of micrometer-sized particles can be immersed in the bulk of a low temperature plasma under microgravity conditions. It typically contains a central volume depleted of particles - the void - that prevents the generation of large, continuous clouds. Experiments performed at different neutral gas pressures and discharge volumes during the microgravity phase of a parabolic flight show that the central void is closed completely once the pulsed operation mode is applied. The particle cloud shape, and the density distribution within the cloud, are practically independent on the pulse period within the investigated parameter range, and mainly depend on the overall discharge parameters neutral gas pressure and discharge volume. This indicates that the pulsed operation of the plasma source does not introduce new physical effects on the particles aside from the void closure. The proposed method has great potential for future application in experimental facilities dedicated to fundamental studies of large three-dimensional, homogeneous complex plasma systems in microgravity.","sentences":["A new experimental method for creating void-free complex (``dusty'') plasmas under microgravity conditions is presented.","The method is based on a pulsed operation mode of a four-channel radio-frequency generator for plasma sustainment.","A dust cloud of micrometer-sized particles can be immersed in the bulk of a low temperature plasma under microgravity conditions.","It typically contains a central volume depleted of particles - the void - that prevents the generation of large, continuous clouds.","Experiments performed at different neutral gas pressures and discharge volumes during the microgravity phase of a parabolic flight show that the central void is closed completely once the pulsed operation mode is applied.","The particle cloud shape, and the density distribution within the cloud, are practically independent on the pulse period within the investigated parameter range, and mainly depend on the overall discharge parameters neutral gas pressure and discharge volume.","This indicates that the pulsed operation of the plasma source does not introduce new physical effects on the particles aside from the void closure.","The proposed method has great potential for future application in experimental facilities dedicated to fundamental studies of large three-dimensional, homogeneous complex plasma systems in microgravity."],"url":"http://arxiv.org/abs/2402.13361v1","category":"physics.plasm-ph"}
{"created":"2024-02-20 20:06:38","title":"A new characterization of second-order stochastic dominance","abstract":"We provide a new characterization of second-order stochastic dominance, also known as increasing concave order. The result has an intuitive interpretation that adding a risk with negative expected value in adverse scenarios makes the resulting position generally less desirable for risk-averse agents. A similar characterization is also found for convex order and increasing convex order. The proofs techniques for the main result are based on properties of Expected Shortfall, a family of risk measures that is popular in financial regulation.","sentences":["We provide a new characterization of second-order stochastic dominance, also known as increasing concave order.","The result has an intuitive interpretation that adding a risk with negative expected value in adverse scenarios makes the resulting position generally less desirable for risk-averse agents.","A similar characterization is also found for convex order and increasing convex order.","The proofs techniques for the main result are based on properties of Expected Shortfall, a family of risk measures that is popular in financial regulation."],"url":"http://arxiv.org/abs/2402.13355v1","category":"q-fin.RM"}
{"created":"2024-02-20 19:11:58","title":"Positivity Bounds on Massive Vectors","abstract":"In this paper, we explore positivity bounds for the effective field theory~(EFT) of a single weakly coupled massive vector field. The presence of both mass and spin makes the crossing properties of the amplitudes vastly complicated -- we address this by parametrizing the amplitudes as products of a polarization matrix and a vector of appropriately chosen functions with simpler crossing properties. The resulting framework involves sum rules and null constraints that allows us to constrain any combination of low-energy observables, such as EFT amplitudes. By varying the value of the vector mass over the cutoff scale, some of our bounds asymptote to the bounds obtained in the context of photons and massless scalars. This work paves the way for future applications to e.g. non-abelian massive vectors, glueballs and theories with spin larger than one.","sentences":["In this paper, we explore positivity bounds for the effective field theory~(EFT) of a single weakly coupled massive vector field.","The presence of both mass and spin makes the crossing properties of the amplitudes vastly complicated -- we address this by parametrizing the amplitudes as products of a polarization matrix and a vector of appropriately chosen functions with simpler crossing properties.","The resulting framework involves sum rules and null constraints that allows us to constrain any combination of low-energy observables, such as EFT amplitudes.","By varying the value of the vector mass over the cutoff scale, some of our bounds asymptote to the bounds obtained in the context of photons and massless scalars.","This work paves the way for future applications to e.g. non-abelian massive vectors, glueballs and theories with spin larger than one."],"url":"http://arxiv.org/abs/2402.13327v1","category":"hep-th"}
{"created":"2024-02-20 19:02:43","title":"Quantum Pseudorandomness Cannot Be Shrunk In a Black-Box Way","abstract":"Pseudorandom Quantum States (PRS) were introduced by Ji, Liu and Song as quantum analogous to Pseudorandom Generators. They are an ensemble of states efficiently computable but computationally indistinguishable from Haar random states. Subsequent works have shown that some cryptographic primitives can be constructed from PRSs. Moreover, recent classical and quantum oracle separations of PRS from One-Way Functions strengthen the interest in a purely quantum alternative building block for quantum cryptography, potentially weaker than OWFs.   However, our lack of knowledge of extending or shrinking the number of qubits of the PRS output still makes it difficult to reproduce some of the classical proof techniques and results. Short-PRSs, that is PRSs with logarithmic size output, have been introduced in the literature along with cryptographic applications, but we still do not know how they relate to PRSs. Here we answer half of the question, by showing that it is not possible to shrink the output of a PRS from polynomial to logarithmic qubit length while still preserving the pseudorandomness property, in a relativized way. More precisely, we show that relative to Kretschmer's quantum oracle (TQC 2021) short-PRSs cannot exist (while PRSs exist, as shown by Kretschmer's work).","sentences":["Pseudorandom Quantum States (PRS) were introduced by Ji, Liu and Song as quantum analogous to Pseudorandom Generators.","They are an ensemble of states efficiently computable but computationally indistinguishable from Haar random states.","Subsequent works have shown that some cryptographic primitives can be constructed from PRSs.","Moreover, recent classical and quantum oracle separations of PRS from One-Way Functions strengthen the interest in a purely quantum alternative building block for quantum cryptography, potentially weaker than OWFs.   ","However, our lack of knowledge of extending or shrinking the number of qubits of the PRS output still makes it difficult to reproduce some of the classical proof techniques and results.","Short-PRSs, that is PRSs with logarithmic size output, have been introduced in the literature along with cryptographic applications, but we still do not know how they relate to PRSs.","Here we answer half of the question, by showing that it is not possible to shrink the output of a PRS from polynomial to logarithmic qubit length while still preserving the pseudorandomness property, in a relativized way.","More precisely, we show that relative to Kretschmer's quantum oracle (TQC 2021) short-PRSs cannot exist (while PRSs exist, as shown by Kretschmer's work)."],"url":"http://arxiv.org/abs/2402.13324v1","category":"quant-ph"}
{"created":"2024-02-20 19:00:01","title":"Spin-Triplet Pairing in Heavy Nuclei is Stable Against Deformation","abstract":"Any experimental evidence of nucleons paired in spin-triplet states will confirm the existence of an exotic phase of nuclear matter. This type of nuclear superfluidity has been hypothesized in heavy nuclei, where the antagonizing spin-orbit effects are damped, and there it oftentimes coexists with traditional spin-singlet pairing, leading to the possibility of mixed-spin pairing. Realistic nuclear deformation, not considered in such studies, could make-or-break these proposals, since its effect on triplet-pairing, and the competition (and coexistence) of the two superfluid phases, was expected to be crucial. We report on a thorough study on the effect of deformation on triplet-, singlet-, and mixed-spin pairing in the relevant region of the nuclear chart. We find that, at low isospin asymmetries, spin-triplet pairing is enhanced by deformation, while below the proton-drip line, the novel superfluid phase survives alongside the usual spin-singlet pairing. These results suggest that spin-triplet superfluidity exists in realistic nuclei and can be probed in the lab.","sentences":["Any experimental evidence of nucleons paired in spin-triplet states will confirm the existence of an exotic phase of nuclear matter.","This type of nuclear superfluidity has been hypothesized in heavy nuclei, where the antagonizing spin-orbit effects are damped, and there it oftentimes coexists with traditional spin-singlet pairing, leading to the possibility of mixed-spin pairing.","Realistic nuclear deformation, not considered in such studies, could make-or-break these proposals, since its effect on triplet-pairing, and the competition (and coexistence) of the two superfluid phases, was expected to be crucial.","We report on a thorough study on the effect of deformation on triplet-, singlet-, and mixed-spin pairing in the relevant region of the nuclear chart.","We find that, at low isospin asymmetries, spin-triplet pairing is enhanced by deformation, while below the proton-drip line, the novel superfluid phase survives alongside the usual spin-singlet pairing.","These results suggest that spin-triplet superfluidity exists in realistic nuclei and can be probed in the lab."],"url":"http://arxiv.org/abs/2402.13313v1","category":"nucl-th"}
{"created":"2024-02-20 19:00:00","title":"Full-shape analysis with simulation-based priors: constraints on single field inflation from BOSS","abstract":"We present an efficient approach to set informative physically motivated priors for EFT-based full-shape analyses of galaxy survey data. We extract these priors from simulated galaxy catalogs based on halo occupation distribution (HOD) models. As a first step, we build a joint distribution between EFT galaxy bias and HOD parameters from a set of 10,500 HOD mock catalogs. We make use of the field level EFT technique that allows for cosmic variance cancellation, enabling a precision calibration of EFT parameters from computationally inexpensive small-volume simulations. As a second step, we use neural density estimators -- normalizing flows -- to model the marginal probability density of the EFT parameters, which can be used as a prior distribution in full shape analyses. As a first application, we use our HOD-based prior distribution in a new analysis of galaxy power spectra and bispectra from the BOSS survey in the context of single field primordial non-Gaussianity. We find that our approach leads to a reduction of the posterior volume of bias parameters by an order of magnitude. We also find $f_{\\rm NL}^{\\rm equil} = 650\\pm 310$ and $f_{\\rm NL}^{\\rm ortho} = 42\\pm 130$ (at 68\\% CL) in a combined two-template analysis, representing a $\\approx 40\\%$ improvement in constraints on single field primordial non-Gaussianity, equivalent to doubling the survey volume.","sentences":["We present an efficient approach to set informative physically motivated priors for EFT-based full-shape analyses of galaxy survey data.","We extract these priors from simulated galaxy catalogs based on halo occupation distribution (HOD) models.","As a first step, we build a joint distribution between EFT galaxy bias and HOD parameters from a set of 10,500 HOD mock catalogs.","We make use of the field level EFT technique that allows for cosmic variance cancellation, enabling a precision calibration of EFT parameters from computationally inexpensive small-volume simulations.","As a second step, we use neural density estimators -- normalizing flows -- to model the marginal probability density of the EFT parameters, which can be used as a prior distribution in full shape analyses.","As a first application, we use our HOD-based prior distribution in a new analysis of galaxy power spectra and bispectra from the BOSS survey in the context of single field primordial non-Gaussianity.","We find that our approach leads to a reduction of the posterior volume of bias parameters by an order of magnitude.","We also find $f_{\\rm NL}^{\\rm equil} = 650\\pm 310$ and $f_{\\rm NL}^{\\rm ortho} = 42\\pm 130$ (at 68\\% CL) in a combined two-template analysis, representing a $\\approx 40\\%$ improvement in constraints on single field primordial non-Gaussianity, equivalent to doubling the survey volume."],"url":"http://arxiv.org/abs/2402.13310v1","category":"astro-ph.CO"}
{"created":"2024-02-21 16:02:14","title":"Overcoming Saturation in Density Ratio Estimation by Iterated Regularization","abstract":"Estimating the ratio of two probability densities from finitely many samples, is a central task in machine learning and statistics. In this work, we show that a large class of kernel methods for density ratio estimation suffers from error saturation, which prevents algorithms from achieving fast error convergence rates on highly regular learning problems. To resolve saturation, we introduce iterated regularization in density ratio estimation to achieve fast error rates. Our methods outperform its non-iteratively regularized versions on benchmarks for density ratio estimation as well as on large-scale evaluations for importance-weighted ensembling of deep unsupervised domain adaptation models.","sentences":["Estimating the ratio of two probability densities from finitely many samples, is a central task in machine learning and statistics.","In this work, we show that a large class of kernel methods for density ratio estimation suffers from error saturation, which prevents algorithms from achieving fast error convergence rates on highly regular learning problems.","To resolve saturation, we introduce iterated regularization in density ratio estimation to achieve fast error rates.","Our methods outperform its non-iteratively regularized versions on benchmarks for density ratio estimation as well as on large-scale evaluations for importance-weighted ensembling of deep unsupervised domain adaptation models."],"url":"http://arxiv.org/abs/2402.13891v1","category":"cs.LG"}
{"created":"2024-02-21 15:35:04","title":"$\\texttt{Se}^2$: $\\textit{Se}$quential Example $\\textit{Se}$lection for In-Context Learning","abstract":"The remarkable capability of large language models (LLMs) for in-context learning (ICL) needs to be activated by demonstration examples. Prior work has extensively explored the selection of examples for ICL, predominantly following the \"select then organize\" paradigm, such approaches often neglect the internal relationships between examples and exist an inconsistency between the training and inference. In this paper, we formulate the problem as a $\\textit{se}$quential $\\textit{se}$lection problem and introduce $\\texttt{Se}^2$, a sequential-aware method that leverages the LLM's feedback on varying context, aiding in capturing inter-relationships and sequential information among examples, significantly enriching the contextuality and relevance of ICL prompts. Meanwhile, we utilize beam search to seek and construct example sequences, enhancing both quality and diversity. Extensive experiments across 23 NLP tasks from 8 distinct categories illustrate that $\\texttt{Se}^2$ markedly surpasses competitive baselines and achieves 42% relative improvement over random selection. Further in-depth analysis show the effectiveness of proposed strategies, highlighting $\\texttt{Se}^2$'s exceptional stability and adaptability across various scenarios. Our code will be released to facilitate future research.","sentences":["The remarkable capability of large language models (LLMs) for in-context learning (ICL) needs to be activated by demonstration examples.","Prior work has extensively explored the selection of examples for ICL, predominantly following the \"select then organize\" paradigm, such approaches often neglect the internal relationships between examples and exist an inconsistency between the training and inference.","In this paper, we formulate the problem as a $\\textit{se}$quential $\\textit{se}$lection problem and introduce $\\texttt{Se}^2$, a sequential-aware method that leverages the LLM's feedback on varying context, aiding in capturing inter-relationships and sequential information among examples, significantly enriching the contextuality and relevance of ICL prompts.","Meanwhile, we utilize beam search to seek and construct example sequences, enhancing both quality and diversity.","Extensive experiments across 23 NLP tasks from 8 distinct categories illustrate that $\\texttt{Se}^2$ markedly surpasses competitive baselines and achieves 42% relative improvement over random selection.","Further in-depth analysis show the effectiveness of proposed strategies, highlighting $\\texttt{Se}^2$'s exceptional stability and adaptability across various scenarios.","Our code will be released to facilitate future research."],"url":"http://arxiv.org/abs/2402.13874v1","category":"cs.CL"}
{"created":"2024-02-21 13:59:55","title":"MSTAR: Multi-Scale Backbone Architecture Search for Timeseries Classification","abstract":"Most of the previous approaches to Time Series Classification (TSC) highlight the significance of receptive fields and frequencies while overlooking the time resolution. Hence, unavoidably suffered from scalability issues as they integrated an extensive range of receptive fields into classification models. Other methods, while having a better adaptation for large datasets, require manual design and yet not being able to reach the optimal architecture due to the uniqueness of each dataset. We overcome these challenges by proposing a novel multi-scale search space and a framework for Neural architecture search (NAS), which addresses both the problem of frequency and time resolution, discovering the suitable scale for a specific dataset. We further show that our model can serve as a backbone to employ a powerful Transformer module with both untrained and pre-trained weights. Our search space reaches the state-of-the-art performance on four datasets on four different domains while introducing more than ten highly fine-tuned models for each data.","sentences":["Most of the previous approaches to Time Series Classification (TSC) highlight the significance of receptive fields and frequencies while overlooking the time resolution.","Hence, unavoidably suffered from scalability issues as they integrated an extensive range of receptive fields into classification models.","Other methods, while having a better adaptation for large datasets, require manual design and yet not being able to reach the optimal architecture due to the uniqueness of each dataset.","We overcome these challenges by proposing a novel multi-scale search space and a framework for Neural architecture search (NAS), which addresses both the problem of frequency and time resolution, discovering the suitable scale for a specific dataset.","We further show that our model can serve as a backbone to employ a powerful Transformer module with both untrained and pre-trained weights.","Our search space reaches the state-of-the-art performance on four datasets on four different domains while introducing more than ten highly fine-tuned models for each data."],"url":"http://arxiv.org/abs/2402.13822v1","category":"cs.CV"}
{"created":"2024-02-21 12:33:34","title":"Adaptive Massively Parallel Coloring in Sparse Graphs","abstract":"Classic symmetry-breaking problems on graphs have gained a lot of attention in models of modern parallel computation. The Adaptive Massively Parallel Computation (AMPC) is a model that captures central challenges in data center computations. Chang et al. [PODC'2019] gave an extremely fast, constant time, algorithm for the $(\\Delta + 1)$-coloring problem, where $\\Delta$ is the maximum degree of an input graph of $n$ nodes. The algorithm works in the most restrictive low-space setting, where each machine has $n^{\\delta}$ local space for a constant $0 < \\delta < 1$.   In this work, we study the vertex-coloring problem in sparse graphs parameterized by their arboricity $\\alpha$, a standard measure for sparsity. We give deterministic algorithms that in constant, or almost constant, time give $\\text{poly}(\\alpha)$ and $O(\\alpha)$-colorings, where $\\alpha$ can be arbitrarily smaller than $\\Delta$. A strong and standard approach to compute arboricity-dependent colorings is through the Nash-Williams forest decomposition, which gives rise to an (acyclic) orientation of the edges such that each node has a small outdegree.   Our main technical contribution is giving efficient deterministic algorithms to compute these orientations and showing how to leverage them to find colorings in low-space AMPC. A key technical challenge is that the color of a node may depend on almost all of the other nodes in the graph and these dependencies cannot be stored on a single machine. Nevertheless, our novel and careful exploration technique yields the orientation, and the arboricity-dependent coloring, with a sublinear number of adaptive queries per node.","sentences":["Classic symmetry-breaking problems on graphs have gained a lot of attention in models of modern parallel computation.","The Adaptive Massively Parallel Computation (AMPC) is a model that captures central challenges in data center computations.","Chang et al.","[PODC'2019] gave an extremely fast, constant time, algorithm for the $(\\Delta + 1)$-coloring problem, where $\\Delta$ is the maximum degree of an input graph of $n$ nodes.","The algorithm works in the most restrictive low-space setting, where each machine has $n^{\\delta}$ local space for a constant $0 <","\\delta < 1$.   ","In this work, we study the vertex-coloring problem in sparse graphs parameterized by their arboricity $\\alpha$, a standard measure for sparsity.","We give deterministic algorithms that in constant, or almost constant, time give $\\text{poly}(\\alpha)$ and $O(\\alpha)$-colorings, where $\\alpha$ can be arbitrarily smaller than $\\Delta$. A strong and standard approach to compute arboricity-dependent colorings is through the Nash-Williams forest decomposition, which gives rise to an (acyclic) orientation of the edges such that each node has a small outdegree.   ","Our main technical contribution is giving efficient deterministic algorithms to compute these orientations and showing how to leverage them to find colorings in low-space AMPC.","A key technical challenge is that the color of a node may depend on almost all of the other nodes in the graph and these dependencies cannot be stored on a single machine.","Nevertheless, our novel and careful exploration technique yields the orientation, and the arboricity-dependent coloring, with a sublinear number of adaptive queries per node."],"url":"http://arxiv.org/abs/2402.13755v1","category":"cs.DC"}
{"created":"2024-02-21 12:10:57","title":"Hidden Gems in the Rough: Computational Notebooks as an Uncharted Oasis for IDEs","abstract":"In this paper, we outline potential ways for the further development of computational notebooks in Integrated Development Environments (IDEs). We discuss notebooks integration with IDEs, focusing on three main areas: facilitating experimentation, adding collaborative features, and improving code comprehension. We propose that better support of notebooks will not only benefit the notebooks, but also enhance IDEs by supporting new development processes native to notebooks. In conclusion, we suggest that adapting IDEs for more experimentation-oriented notebook processes will prepare them for the future of AI-powered programming.","sentences":["In this paper, we outline potential ways for the further development of computational notebooks in Integrated Development Environments (IDEs).","We discuss notebooks integration with IDEs, focusing on three main areas: facilitating experimentation, adding collaborative features, and improving code comprehension.","We propose that better support of notebooks will not only benefit the notebooks, but also enhance IDEs by supporting new development processes native to notebooks.","In conclusion, we suggest that adapting IDEs for more experimentation-oriented notebook processes will prepare them for the future of AI-powered programming."],"url":"http://arxiv.org/abs/2402.13739v1","category":"cs.SE"}
{"created":"2024-02-21 11:30:20","title":"Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent","abstract":"Large Language Models (LLMs) have revolutionized open-domain dialogue agents but encounter challenges in multi-character role-playing (MCRP) scenarios. To address the issue, we present Neeko, an innovative framework designed for efficient multiple characters imitation. Unlike existing methods, Neeko employs a dynamic low-rank adapter (LoRA) strategy, enabling it to adapt seamlessly to diverse characters. Our framework breaks down the role-playing process into agent pre-training, multiple characters playing, and character incremental learning, effectively handling both seen and unseen roles. This dynamic approach, coupled with distinct LoRA blocks for each character, enhances Neeko's adaptability to unique attributes, personalities, and speaking patterns. As a result, Neeko demonstrates superior performance in MCRP over most existing methods, offering more engaging and versatile user interaction experiences. Code and data are available at https://github.com/weiyifan1023/Neeko.","sentences":["Large Language Models (LLMs) have revolutionized open-domain dialogue agents but encounter challenges in multi-character role-playing (MCRP) scenarios.","To address the issue, we present Neeko, an innovative framework designed for efficient multiple characters imitation.","Unlike existing methods, Neeko employs a dynamic low-rank adapter (LoRA) strategy, enabling it to adapt seamlessly to diverse characters.","Our framework breaks down the role-playing process into agent pre-training, multiple characters playing, and character incremental learning, effectively handling both seen and unseen roles.","This dynamic approach, coupled with distinct LoRA blocks for each character, enhances Neeko's adaptability to unique attributes, personalities, and speaking patterns.","As a result, Neeko demonstrates superior performance in MCRP over most existing methods, offering more engaging and versatile user interaction experiences.","Code and data are available at https://github.com/weiyifan1023/Neeko."],"url":"http://arxiv.org/abs/2402.13717v1","category":"cs.CL"}
{"created":"2024-02-21 09:18:59","title":"FlexHB: a More Efficient and Flexible Framework for Hyperparameter Optimization","abstract":"Given a Hyperparameter Optimization(HPO) problem, how to design an algorithm to find optimal configurations efficiently? Bayesian Optimization(BO) and the multi-fidelity BO methods employ surrogate models to sample configurations based on history evaluations. More recent studies obtain better performance by integrating BO with HyperBand(HB), which accelerates evaluation by early stopping mechanism. However, these methods ignore the advantage of a suitable evaluation scheme over the default HyperBand, and the capability of BO is still constrained by skewed evaluation results. In this paper, we propose FlexHB, a new method pushing multi-fidelity BO to the limit as well as re-designing a framework for early stopping with Successive Halving(SH). Comprehensive study on FlexHB shows that (1) our fine-grained fidelity method considerably enhances the efficiency of searching optimal configurations, (2) our FlexBand framework (self-adaptive allocation of SH brackets, and global ranking of configurations in both current and past SH procedures) grants the algorithm with more flexibility and improves the anytime performance. Our method achieves superior efficiency and outperforms other methods on various HPO tasks. Empirical results demonstrate that FlexHB can achieve up to 6.9X and 11.1X speedups over the state-of-the-art MFES-HB and BOHB respectively.","sentences":["Given a Hyperparameter Optimization(HPO) problem, how to design an algorithm to find optimal configurations efficiently?","Bayesian Optimization(BO) and the multi-fidelity BO methods employ surrogate models to sample configurations based on history evaluations.","More recent studies obtain better performance by integrating BO with HyperBand(HB), which accelerates evaluation by early stopping mechanism.","However, these methods ignore the advantage of a suitable evaluation scheme over the default HyperBand, and the capability of BO is still constrained by skewed evaluation results.","In this paper, we propose FlexHB, a new method pushing multi-fidelity BO to the limit as well as re-designing a framework for early stopping with Successive Halving(SH).","Comprehensive study on FlexHB shows that (1) our fine-grained fidelity method considerably enhances the efficiency of searching optimal configurations, (2) our FlexBand framework (self-adaptive allocation of SH brackets, and global ranking of configurations in both current and past SH procedures) grants the algorithm with more flexibility and improves the anytime performance.","Our method achieves superior efficiency and outperforms other methods on various HPO tasks.","Empirical results demonstrate that FlexHB can achieve up to 6.9X and 11.1X speedups over the state-of-the-art MFES-HB and BOHB respectively."],"url":"http://arxiv.org/abs/2402.13641v1","category":"cs.LG"}
{"created":"2024-02-21 03:39:04","title":"SimPro: A Simple Probabilistic Framework Towards Realistic Long-Tailed Semi-Supervised Learning","abstract":"Recent advancements in semi-supervised learning have focused on a more realistic yet challenging task: addressing imbalances in labeled data while the class distribution of unlabeled data remains both unknown and potentially mismatched. Current approaches in this sphere often presuppose rigid assumptions regarding the class distribution of unlabeled data, thereby limiting the adaptability of models to only certain distribution ranges. In this study, we propose a novel approach, introducing a highly adaptable framework, designated as SimPro, which does not rely on any predefined assumptions about the distribution of unlabeled data. Our framework, grounded in a probabilistic model, innovatively refines the expectation-maximization (EM) algorithm by explicitly decoupling the modeling of conditional and marginal class distributions. This separation facilitates a closed-form solution for class distribution estimation during the maximization phase, leading to the formulation of a Bayes classifier. The Bayes classifier, in turn, enhances the quality of pseudo-labels in the expectation phase. Remarkably, the SimPro framework not only comes with theoretical guarantees but also is straightforward to implement. Moreover, we introduce two novel class distributions broadening the scope of the evaluation. Our method showcases consistent state-of-the-art performance across diverse benchmarks and data distribution scenarios. Our code is available at https://github.com/LeapLabTHU/SimPro.","sentences":["Recent advancements in semi-supervised learning have focused on a more realistic yet challenging task: addressing imbalances in labeled data while the class distribution of unlabeled data remains both unknown and potentially mismatched.","Current approaches in this sphere often presuppose rigid assumptions regarding the class distribution of unlabeled data, thereby limiting the adaptability of models to only certain distribution ranges.","In this study, we propose a novel approach, introducing a highly adaptable framework, designated as SimPro, which does not rely on any predefined assumptions about the distribution of unlabeled data.","Our framework, grounded in a probabilistic model, innovatively refines the expectation-maximization (EM) algorithm by explicitly decoupling the modeling of conditional and marginal class distributions.","This separation facilitates a closed-form solution for class distribution estimation during the maximization phase, leading to the formulation of a Bayes classifier.","The Bayes classifier, in turn, enhances the quality of pseudo-labels in the expectation phase.","Remarkably, the SimPro framework not only comes with theoretical guarantees but also is straightforward to implement.","Moreover, we introduce two novel class distributions broadening the scope of the evaluation.","Our method showcases consistent state-of-the-art performance across diverse benchmarks and data distribution scenarios.","Our code is available at https://github.com/LeapLabTHU/SimPro."],"url":"http://arxiv.org/abs/2402.13505v1","category":"cs.LG"}
{"created":"2024-02-21 01:33:47","title":"Approximation analysis for the minimization problem of difference-of-convex functions with Moreau envelopes","abstract":"In this work the minimization problem for the difference of convex (DC) functions is studied by using Moreau envelopes and the descent method with Moreau gradient is employed to approximate the numerical solution. The main regularization idea in this work is inspired by Hiriart-Urruty [14], Moudafi[17], regularize the components of the DC problem by adapting the different parameters and strategic matrices flexibly to evaluate the whole DC problem. It is shown that the inertial gradient method as well as the classic gradient descent scheme tend towards an approximation stationary point of the original problem.","sentences":["In this work the minimization problem for the difference of convex (DC) functions is studied by using Moreau envelopes and the descent method with Moreau gradient is employed to approximate the numerical solution.","The main regularization idea in this work is inspired by Hiriart-Urruty","[14], Moudafi[17], regularize the components of the DC problem by adapting the different parameters and strategic matrices flexibly to evaluate the whole DC problem.","It is shown that the inertial gradient method as well as the classic gradient descent scheme tend towards an approximation stationary point of the original problem."],"url":"http://arxiv.org/abs/2402.13461v1","category":"math.OC"}
{"created":"2024-02-20 20:52:21","title":"An adaptive lattice Green's function method for external flows with two unbounded and one homogeneous directions","abstract":"We solve the incompressible Navier-Stokes equations using a lattice Green's function (LGF) approach, including immersed boundaries (IB) and adaptive mesh refinement (AMR), for external flows with one homogeneous direction (e.g. infinite cylinders of arbitrary cross-section). We hybridize a Fourier collocation (pseudo-spectral) method for the homogeneous direction with a specially designed, staggered-grid finite-volume scheme on an AMR grid. The Fourier series is also truncated variably according to the refinement level in the other directions. We derive new algorithms to tabulate the LGF of the screened Poisson operator and viscous integrating factor. After adapting other algorithmic details from the fully inhomogeneous case, we validate and demonstrate the new method with transitional and turbulent flows over a circular cylinder at $Re=300$ and $Re=12,000$, respectively.","sentences":["We solve the incompressible Navier-Stokes equations using a lattice Green's function (LGF) approach, including immersed boundaries (IB) and adaptive mesh refinement (AMR), for external flows with one homogeneous direction (e.g. infinite cylinders of arbitrary cross-section).","We hybridize a Fourier collocation (pseudo-spectral) method for the homogeneous direction with a specially designed, staggered-grid finite-volume scheme on an AMR grid.","The Fourier series is also truncated variably according to the refinement level in the other directions.","We derive new algorithms to tabulate the LGF of the screened Poisson operator and viscous integrating factor.","After adapting other algorithmic details from the fully inhomogeneous case, we validate and demonstrate the new method with transitional and turbulent flows over a circular cylinder at $Re=300$ and $Re=12,000$, respectively."],"url":"http://arxiv.org/abs/2402.13370v1","category":"physics.flu-dyn"}
{"created":"2024-02-20 20:42:02","title":"A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction","abstract":"Large language models (LLMs) have demonstrated impressive abilities in generating unstructured natural language according to instructions. However, their performance can be inconsistent when tasked with producing text that adheres to specific structured formats, which is crucial in applications like named entity recognition (NER) or relation extraction (RE). To address this issue, this paper introduces an efficient method, G&O, to enhance their structured text generation capabilities. It breaks the generation into a two-step pipeline: initially, LLMs generate answers in natural language as intermediate responses. Subsequently, LLMs are asked to organize the output into the desired structure, using the intermediate responses as context. G&O effectively separates the generation of content from the structuring process, reducing the pressure of completing two orthogonal tasks simultaneously. Tested on zero-shot NER and RE, the results indicate a significant improvement in LLM performance with minimal additional efforts. This straightforward and adaptable prompting technique can also be combined with other strategies, like self-consistency, to further elevate LLM capabilities in various structured text generation tasks.","sentences":["Large language models (LLMs) have demonstrated impressive abilities in generating unstructured natural language according to instructions.","However, their performance can be inconsistent when tasked with producing text that adheres to specific structured formats, which is crucial in applications like named entity recognition (NER) or relation extraction (RE).","To address this issue, this paper introduces an efficient method, G&O, to enhance their structured text generation capabilities.","It breaks the generation into a two-step pipeline: initially, LLMs generate answers in natural language as intermediate responses.","Subsequently, LLMs are asked to organize the output into the desired structure, using the intermediate responses as context.","G&O effectively separates the generation of content from the structuring process, reducing the pressure of completing two orthogonal tasks simultaneously.","Tested on zero-shot NER and RE, the results indicate a significant improvement in LLM performance with minimal additional efforts.","This straightforward and adaptable prompting technique can also be combined with other strategies, like self-consistency, to further elevate LLM capabilities in various structured text generation tasks."],"url":"http://arxiv.org/abs/2402.13364v1","category":"cs.CL"}
{"created":"2024-02-20 20:34:04","title":"Compact stars in $f(R,L_m,T)$ gravity","abstract":"This study explores the behavior of compact stars within the framework of $f(R,L_m,T)$ gravity, focusing on the functional form $f(R,L_m,T) = R + \\alpha TL_m$. The modified Tolman-Oppenheimer-Volkoff (TOV) equations are derived and numerically solved for several values of the free parameter $\\alpha$ by considering both quark and hadronic matter -- described by realistic equations of state (EoSs). Furthermore, the stellar structure equations are adapted for two different choices of the matter Lagrangian density (namely, $L_m= p$ and $L_m= -\\rho$), laying the groundwork for our numerical analysis. As expected, we recover the traditional TOV equations in General Relativity (GR) when $\\alpha \\rightarrow 0$. Remarkably, we found that the two choices for $L_m$ have appreciably different effects on the mass-radius diagrams. Results showcase the impact of $\\alpha$ on compact star properties, while final remarks summarize key findings and discuss implications, including compatibility with observational data from NGC 6397's neutron star. Overall, this research enhances comprehension of $f(R,L_m,T)$ gravity's effects on compact star internal structures, offering insights for future investigations.","sentences":["This study explores the behavior of compact stars within the framework of $f(R,L_m,T)$ gravity, focusing on the functional form $f(R,L_m,T) = R + \\alpha TL_m$. The modified Tolman-Oppenheimer-Volkoff (TOV) equations are derived and numerically solved for several values of the free parameter $\\alpha$ by considering both quark and hadronic matter -- described by realistic equations of state (EoSs).","Furthermore, the stellar structure equations are adapted for two different choices of the matter Lagrangian density (namely, $L_m= p$ and $L_m= -\\rho$), laying the groundwork for our numerical analysis.","As expected, we recover the traditional TOV equations in General Relativity (GR) when $\\alpha \\rightarrow 0$.","Remarkably, we found that the two choices for $L_m$ have appreciably different effects on the mass-radius diagrams.","Results showcase the impact of $\\alpha$ on compact star properties, while final remarks summarize key findings and discuss implications, including compatibility with observational data from NGC 6397's neutron star.","Overall, this research enhances comprehension of $f(R,L_m,T)$ gravity's effects on compact star internal structures, offering insights for future investigations."],"url":"http://arxiv.org/abs/2402.13360v1","category":"gr-qc"}
{"created":"2024-02-20 19:49:44","title":"Dissipative spatiotemporal soliton in a driven waveguide laser","abstract":"A distributed Kerr-lens mode locking regime can be realized in a waveguide laser by spatial profiling of the pump beam, thus creating a spatio-temporal soliton. Additional slow temporal modulation of the pump source stabilizes the spatio-temporal solution in a broad range of parameters, which are defined by the dynamic gain saturation. We choose a Cr:ZnS waveguide laser as a practical example, but such a regime is feasible in various waveguide and fiber oscillators. A far-reaching analogy with Bose-Einstein condensates allows using this approach to stabilization of the weakly dissipative BECs.","sentences":["A distributed Kerr-lens mode locking regime can be realized in a waveguide laser by spatial profiling of the pump beam, thus creating a spatio-temporal soliton.","Additional slow temporal modulation of the pump source stabilizes the spatio-temporal solution in a broad range of parameters, which are defined by the dynamic gain saturation.","We choose a Cr:ZnS waveguide laser as a practical example, but such a regime is feasible in various waveguide and fiber oscillators.","A far-reaching analogy with Bose-Einstein condensates allows using this approach to stabilization of the weakly dissipative BECs."],"url":"http://arxiv.org/abs/2402.13348v1","category":"physics.optics"}
{"created":"2024-02-20 19:08:16","title":"Quantum Control for Zeno effect with noises","abstract":"The quantum Zeno effect is a distinctive phenomenon in quantum mechanics, describing the nontrivial effect of frequent projective measurements on hindering the evolution of a quantum system. However, when subjecting to environmental noises, the quantum system may dissipate and the quantum Zeno effect no longer works. This research studies the physical mechanism for the decay of the quantum Zeno effect in the presence of noises, and investigates the effect of coherent quantum controls on mitigating the decrease of the survival probability that the system stays in the initial state induced by the noises. We derive the decay rate of the survival probability with and without coherent quantum controls in general, and show that when the frequency of the projective measurements is large but finite, proper coherent controls by sufficiently strong Hamiltonians can be designed to decrease the decay rate of the survival probability. A two-level quantum system suffering from typical unitary and non-unitary noises is then considered to demonstrate the effect of the proposed coherent quantum control scheme in protecting the quantum Zeno effect against the noises. The decay rate of the survival probability is obtained in the presence of the noises, and the control Hamiltonian is further optimized analytically to minimize the decay rate by a variational approach. The evolution paths of the quantum system with the optimal coherent controls is illustrated numerically for different scenarios to explicitly show how the coherent control scheme works in lowering the decay of survival probability.","sentences":["The quantum Zeno effect is a distinctive phenomenon in quantum mechanics, describing the nontrivial effect of frequent projective measurements on hindering the evolution of a quantum system.","However, when subjecting to environmental noises, the quantum system may dissipate and the quantum Zeno effect no longer works.","This research studies the physical mechanism for the decay of the quantum Zeno effect in the presence of noises, and investigates the effect of coherent quantum controls on mitigating the decrease of the survival probability that the system stays in the initial state induced by the noises.","We derive the decay rate of the survival probability with and without coherent quantum controls in general, and show that when the frequency of the projective measurements is large but finite, proper coherent controls by sufficiently strong Hamiltonians can be designed to decrease the decay rate of the survival probability.","A two-level quantum system suffering from typical unitary and non-unitary noises is then considered to demonstrate the effect of the proposed coherent quantum control scheme in protecting the quantum Zeno effect against the noises.","The decay rate of the survival probability is obtained in the presence of the noises, and the control Hamiltonian is further optimized analytically to minimize the decay rate by a variational approach.","The evolution paths of the quantum system with the optimal coherent controls is illustrated numerically for different scenarios to explicitly show how the coherent control scheme works in lowering the decay of survival probability."],"url":"http://arxiv.org/abs/2402.13325v1","category":"quant-ph"}
{"created":"2024-02-20 13:33:16","title":"Photovoltaic performances in a cavity-coupled double quantum dots photocell","abstract":"Revealing the quantum regime of photovoltaics is crucial to enhancing the internal quantum efficiency of a double quantum dots (DQDs) photocell housed in a cavity. In this study, the performance of a quantum photovoltaic is evaluated based on the current-voltage and power-voltage characteristics in a cavity-coupled DQDs photocell. The results show that the cavity-DQDs coupling coefficient plays a dissipative role in the photovoltaic performance, and the cavity has a limited size for the photovoltaic performance. Additionally, more low-energy photons are easily absorbed by this cavity-coupled DQDs photocell compared with the case without cavity. These results may provide some strategies for improving the photoelectric conversion efficiency and internal quantum efficiency of cavity-coupled DQDs photocells.","sentences":["Revealing the quantum regime of photovoltaics is crucial to enhancing the internal quantum efficiency of a double quantum dots (DQDs)","photocell housed in a cavity.","In this study, the performance of a quantum photovoltaic is evaluated based on the current-voltage and power-voltage characteristics in a cavity-coupled DQDs photocell.","The results show that the cavity-DQDs coupling coefficient plays a dissipative role in the photovoltaic performance, and the cavity has a limited size for the photovoltaic performance.","Additionally, more low-energy photons are easily absorbed by this cavity-coupled DQDs photocell compared with the case without cavity.","These results may provide some strategies for improving the photoelectric conversion efficiency and internal quantum efficiency of cavity-coupled DQDs photocells."],"url":"http://arxiv.org/abs/2402.13300v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-20 02:07:57","title":"Evolutionary Reinforcement Learning: A Systematic Review and Future Directions","abstract":"In response to the limitations of reinforcement learning and evolutionary algorithms (EAs) in complex problem-solving, Evolutionary Reinforcement Learning (EvoRL) has emerged as a synergistic solution. EvoRL integrates EAs and reinforcement learning, presenting a promising avenue for training intelligent agents. This systematic review firstly navigates through the technological background of EvoRL, examining the symbiotic relationship between EAs and reinforcement learning algorithms. We then delve into the challenges faced by both EAs and reinforcement learning, exploring their interplay and impact on the efficacy of EvoRL. Furthermore, the review underscores the need for addressing open issues related to scalability, adaptability, sample efficiency, adversarial robustness, ethic and fairness within the current landscape of EvoRL. Finally, we propose future directions for EvoRL, emphasizing research avenues that strive to enhance self-adaptation and self-improvement, generalization, interpretability, explainability, and so on. Serving as a comprehensive resource for researchers and practitioners, this systematic review provides insights into the current state of EvoRL and offers a guide for advancing its capabilities in the ever-evolving landscape of artificial intelligence.","sentences":["In response to the limitations of reinforcement learning and evolutionary algorithms (EAs) in complex problem-solving, Evolutionary Reinforcement Learning (EvoRL) has emerged as a synergistic solution.","EvoRL integrates EAs and reinforcement learning, presenting a promising avenue for training intelligent agents.","This systematic review firstly navigates through the technological background of EvoRL, examining the symbiotic relationship between EAs and reinforcement learning algorithms.","We then delve into the challenges faced by both EAs and reinforcement learning, exploring their interplay and impact on the efficacy of EvoRL.","Furthermore, the review underscores the need for addressing open issues related to scalability, adaptability, sample efficiency, adversarial robustness, ethic and fairness within the current landscape of EvoRL.","Finally, we propose future directions for EvoRL, emphasizing research avenues that strive to enhance self-adaptation and self-improvement, generalization, interpretability, explainability, and so on.","Serving as a comprehensive resource for researchers and practitioners, this systematic review provides insights into the current state of EvoRL and offers a guide for advancing its capabilities in the ever-evolving landscape of artificial intelligence."],"url":"http://arxiv.org/abs/2402.13296v1","category":"cs.NE"}
{"created":"2024-02-19 18:59:50","title":"Observation of a phase transition from a continuous to a discrete time crystal","abstract":"Discrete (DTCs) and continuous time crystals (CTCs) are novel dynamical many-body states, that are characterized by robust self-sustained oscillations, emerging via spontaneous breaking of discrete or continuous time translation symmetry. DTCs are periodically driven systems that oscillate with a subharmonic of the drive, while CTCs are driven continuously and oscillate with a system inherent frequency. Here, we explore a phase transition from a continuous time crystal to a discrete time crystal. A CTC with a characteristic oscillation frequency $\\omega_\\mathrm{CTC}$ is prepared in a continuously pumped atom-cavity system. Modulating the pump intensity of the CTC with a frequency $\\omega_{\\mathrm{dr}}$ close to $2\\,\\omega_\\mathrm{CTC}$ leads to robust locking of $\\omega_\\mathrm{CTC}$ to $\\omega_{\\mathrm{dr}}/2$, and hence a DTC arises. This phase transition in a quantum many-body system is related to subharmonic injection locking of non-linear mechanical and electronic oscillators or lasers.","sentences":["Discrete (DTCs) and continuous time crystals (CTCs) are novel dynamical many-body states, that are characterized by robust self-sustained oscillations, emerging via spontaneous breaking of discrete or continuous time translation symmetry.","DTCs are periodically driven systems that oscillate with a subharmonic of the drive, while CTCs are driven continuously and oscillate with a system inherent frequency.","Here, we explore a phase transition from a continuous time crystal to a discrete time crystal.","A CTC with a characteristic oscillation frequency $\\omega_\\mathrm{CTC}$ is prepared in a continuously pumped atom-cavity system.","Modulating the pump intensity of the CTC with a frequency $\\omega_{\\mathrm{dr}}$ close to $2\\,\\omega_\\mathrm{CTC}$ leads to robust locking of $\\omega_\\mathrm{CTC}$ to $\\omega_{\\mathrm{dr}}/2$, and hence a DTC arises.","This phase transition in a quantum many-body system is related to subharmonic injection locking of non-linear mechanical and electronic oscillators or lasers."],"url":"http://arxiv.org/abs/2402.12378v1","category":"quant-ph"}
{"created":"2024-02-19 18:59:07","title":"FiT: Flexible Vision Transformer for Diffusion Model","abstract":"Nature is infinitely resolution-free. In the context of this reality, existing diffusion models, such as Diffusion Transformers, often face challenges when processing image resolutions outside of their trained domain. To overcome this limitation, we present the Flexible Vision Transformer (FiT), a transformer architecture specifically designed for generating images with unrestricted resolutions and aspect ratios. Unlike traditional methods that perceive images as static-resolution grids, FiT conceptualizes images as sequences of dynamically-sized tokens. This perspective enables a flexible training strategy that effortlessly adapts to diverse aspect ratios during both training and inference phases, thus promoting resolution generalization and eliminating biases induced by image cropping. Enhanced by a meticulously adjusted network structure and the integration of training-free extrapolation techniques, FiT exhibits remarkable flexibility in resolution extrapolation generation. Comprehensive experiments demonstrate the exceptional performance of FiT across a broad range of resolutions, showcasing its effectiveness both within and beyond its training resolution distribution. Repository available at https://github.com/whlzy/FiT.","sentences":["Nature is infinitely resolution-free.","In the context of this reality, existing diffusion models, such as Diffusion Transformers, often face challenges when processing image resolutions outside of their trained domain.","To overcome this limitation, we present the Flexible Vision Transformer (FiT), a transformer architecture specifically designed for generating images with unrestricted resolutions and aspect ratios.","Unlike traditional methods that perceive images as static-resolution grids, FiT conceptualizes images as sequences of dynamically-sized tokens.","This perspective enables a flexible training strategy that effortlessly adapts to diverse aspect ratios during both training and inference phases, thus promoting resolution generalization and eliminating biases induced by image cropping.","Enhanced by a meticulously adjusted network structure and the integration of training-free extrapolation techniques, FiT exhibits remarkable flexibility in resolution extrapolation generation.","Comprehensive experiments demonstrate the exceptional performance of FiT across a broad range of resolutions, showcasing its effectiveness both within and beyond its training resolution distribution.","Repository available at https://github.com/whlzy/FiT."],"url":"http://arxiv.org/abs/2402.12376v1","category":"cs.CV"}
{"created":"2024-02-19 18:58:32","title":"Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding","abstract":"As the usage of large language models (LLMs) grows, performing efficient inference with these models becomes increasingly important. While speculative decoding has recently emerged as a promising direction for speeding up inference, existing methods are limited in their ability to scale to larger speculation budgets, and adapt to different hyperparameters and hardware. This paper introduces Sequoia, a scalable, robust, and hardware-aware algorithm for speculative decoding. To attain better scalability, Sequoia introduces a dynamic programming algorithm to find the optimal tree structure for the speculated tokens. To achieve robust speculative performance, Sequoia uses a novel sampling and verification method that outperforms prior work across different decoding temperatures. Finally, Sequoia introduces a hardware-aware tree optimizer that maximizes speculative performance by automatically selecting the token tree size and depth for a given hardware platform. Evaluation shows that Sequoia improves the decoding speed of Llama2-7B, Llama2-13B, and Vicuna-33B on an A100 by up to $4.04\\times$, $3.84\\times$, and $2.37\\times$, and Llama2-70B offloading by up to $10.33\\times$ on L40.","sentences":["As the usage of large language models (LLMs) grows, performing efficient inference with these models becomes increasingly important.","While speculative decoding has recently emerged as a promising direction for speeding up inference, existing methods are limited in their ability to scale to larger speculation budgets, and adapt to different hyperparameters and hardware.","This paper introduces Sequoia, a scalable, robust, and hardware-aware algorithm for speculative decoding.","To attain better scalability, Sequoia introduces a dynamic programming algorithm to find the optimal tree structure for the speculated tokens.","To achieve robust speculative performance, Sequoia uses a novel sampling and verification method that outperforms prior work across different decoding temperatures.","Finally, Sequoia introduces a hardware-aware tree optimizer that maximizes speculative performance by automatically selecting the token tree size and depth for a given hardware platform.","Evaluation shows that Sequoia improves the decoding speed of Llama2-7B, Llama2-13B, and Vicuna-33B on an A100 by up to $4.04\\times$, $3.84\\times$, and $2.37\\times$, and Llama2-70B offloading by up to $10.33\\times$ on L40."],"url":"http://arxiv.org/abs/2402.12374v1","category":"cs.CL"}
{"created":"2024-02-19 18:33:49","title":"LoRA+: Efficient Low Rank Adaptation of Large Models","abstract":"In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$ improves performance (1-2 $\\%$ improvements) and finetuning speed (up to $\\sim$ 2X SpeedUp), at the same computational cost as LoRA.","sentences":["In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al.","(2021) leads to suboptimal finetuning of models with large width (embedding dimension).","This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate.","Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning.","We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA","adapter matrices A and B with a well-chosen ratio.","We call this proposed algorithm LoRA$+$.","In our extensive experiments, LoRA$+$ improves performance (1-2 $\\%$ improvements) and finetuning speed (up to $\\sim$ 2X SpeedUp), at the same computational cost as LoRA."],"url":"http://arxiv.org/abs/2402.12354v1","category":"cs.LG"}
{"created":"2024-02-19 18:15:09","title":"Post-Minkowskian Theory Meets the Spinning Effective-One-Body Approach for Two-Body Scattering","abstract":"Effective-one-body (EOB) waveforms employed by the LIGO-Virgo-KAGRA Collaboration have primarily been developed by resumming the post-Newtonian expansion of the relativistic two-body problem. Given the recent significant advancements in post-Minkowskian (PM) theory and gravitational self-force formalism, there is considerable interest in creating waveform models that integrate information from various perturbative methods in innovative ways. This becomes particularly crucial when tackling the accuracy challenge posed by upcoming ground-based detectors (such as the Einstein Telescope and Cosmic Explorer) and space-based detectors (such as LISA, TianQin or Taiji) expected to operate in the next decade. In this context, we present the derivation of the first spinning EOB Hamiltonian that incorporates PM results up to three-loop order: the SEOB-PM model. The model accounts for the complete hyperbolic motion, encompassing nonlocal-in-time tails. To evaluate its accuracy, we compare its predictions for the conservative scattering angle, augmented with dissipative contributions, against numerical-relativity data of non-spinning and spinning equal-mass black holes. We observe very good agreement, comparable, and in some cases slightly better to the recently proposed $w_{\\rm EOB}$-potential model, of which the SEOB-PM model is a resummation around the probe limit. Indeed, in the probe limit, the SEOB-PM Hamiltonian and scattering angles reduce to the one of a test mass in Kerr spacetime. Once complemented with nonlocal-in-time contributions for bound orbits, the SEOB-PM Hamiltonian can be utilized to generate waveform models for spinning black holes on quasi-circular orbits.","sentences":["Effective-one-body (EOB) waveforms employed by the LIGO-Virgo-KAGRA Collaboration have primarily been developed by resumming the post-Newtonian expansion of the relativistic two-body problem.","Given the recent significant advancements in post-Minkowskian (PM) theory and gravitational self-force formalism, there is considerable interest in creating waveform models that integrate information from various perturbative methods in innovative ways.","This becomes particularly crucial when tackling the accuracy challenge posed by upcoming ground-based detectors (such as the Einstein Telescope and Cosmic Explorer) and space-based detectors (such as LISA, TianQin or Taiji) expected to operate in the next decade.","In this context, we present the derivation of the first spinning EOB Hamiltonian that incorporates PM results up to three-loop order: the SEOB-PM model.","The model accounts for the complete hyperbolic motion, encompassing nonlocal-in-time tails.","To evaluate its accuracy, we compare its predictions for the conservative scattering angle, augmented with dissipative contributions, against numerical-relativity data of non-spinning and spinning equal-mass black holes.","We observe very good agreement, comparable, and in some cases slightly better to the recently proposed $w_{\\rm EOB}$-potential model, of which the SEOB-PM model is a resummation around the probe limit.","Indeed, in the probe limit, the SEOB-PM Hamiltonian and scattering angles reduce to the one of a test mass in Kerr spacetime.","Once complemented with nonlocal-in-time contributions for bound orbits, the SEOB-PM Hamiltonian can be utilized to generate waveform models for spinning black holes on quasi-circular orbits."],"url":"http://arxiv.org/abs/2402.12342v1","category":"gr-qc"}
{"created":"2024-02-19 18:13:37","title":"Designed spin-texture-lattice to control anisotropic magnon transport in antiferromagnets","abstract":"Spin waves in magnetic materials are promising information carriers for future computing technologies due to their ultra-low energy dissipation and long coherence length. Antiferromagnets are strong candidate materials due, in part, to their stability to external fields and larger group velocities. Multiferroic aniferromagnets, such as BiFeO$_3$ (BFO), have an additional degree of freedom stemming from magnetoelectric coupling, allowing for control of the magnetic structure, and thus spin waves, with electric field. Unfortunately, spin-wave propagation in BFO is not well understood due to the complexity of the magnetic structure. In this work, we explore long-range spin transport within an epitaxially engineered, electrically tunable, one-dimensional (1D) magnonic crystal. We discover a striking anisotropy in the spin transport parallel and perpendicular to the 1D crystal axis. Multiscale theory and simulation suggests that this preferential magnon conduction emerges from a combination of a population imbalance in its dispersion, as well as anisotropic structural scattering. This work provides a pathway to electrically-reconfigurable magnonic crystals in antiferromagnets.","sentences":["Spin waves in magnetic materials are promising information carriers for future computing technologies due to their ultra-low energy dissipation and long coherence length.","Antiferromagnets are strong candidate materials due, in part, to their stability to external fields and larger group velocities.","Multiferroic aniferromagnets, such as BiFeO$_3$ (BFO), have an additional degree of freedom stemming from magnetoelectric coupling, allowing for control of the magnetic structure, and thus spin waves, with electric field.","Unfortunately, spin-wave propagation in BFO is not well understood due to the complexity of the magnetic structure.","In this work, we explore long-range spin transport within an epitaxially engineered, electrically tunable, one-dimensional (1D) magnonic crystal.","We discover a striking anisotropy in the spin transport parallel and perpendicular to the 1D crystal axis.","Multiscale theory and simulation suggests that this preferential magnon conduction emerges from a combination of a population imbalance in its dispersion, as well as anisotropic structural scattering.","This work provides a pathway to electrically-reconfigurable magnonic crystals in antiferromagnets."],"url":"http://arxiv.org/abs/2402.12341v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-19 17:44:35","title":"Dynamic Environment Responsive Online Meta-Learning with Fairness Awareness","abstract":"The fairness-aware online learning framework has emerged as a potent tool within the context of continuous lifelong learning. In this scenario, the learner's objective is to progressively acquire new tasks as they arrive over time, while also guaranteeing statistical parity among various protected sub-populations, such as race and gender, when it comes to the newly introduced tasks. A significant limitation of current approaches lies in their heavy reliance on the i.i.d (independent and identically distributed) assumption concerning data, leading to a static regret analysis of the framework. Nevertheless, it's crucial to note that achieving low static regret does not necessarily translate to strong performance in dynamic environments characterized by tasks sampled from diverse distributions. In this paper, to tackle the fairness-aware online learning challenge in evolving settings, we introduce a unique regret measure, FairSAR, by incorporating long-term fairness constraints into a strongly adapted loss regret framework. Moreover, to determine an optimal model parameter at each time step, we introduce an innovative adaptive fairness-aware online meta-learning algorithm, referred to as FairSAOML. This algorithm possesses the ability to adjust to dynamic environments by effectively managing bias control and model accuracy. The problem is framed as a bi-level convex-concave optimization, considering both the model's primal and dual parameters, which pertain to its accuracy and fairness attributes, respectively. Theoretical analysis yields sub-linear upper bounds for both loss regret and the cumulative violation of fairness constraints. Our experimental evaluation on various real-world datasets in dynamic environments demonstrates that our proposed FairSAOML algorithm consistently outperforms alternative approaches rooted in the most advanced prior online learning methods.","sentences":["The fairness-aware online learning framework has emerged as a potent tool within the context of continuous lifelong learning.","In this scenario, the learner's objective is to progressively acquire new tasks as they arrive over time, while also guaranteeing statistical parity among various protected sub-populations, such as race and gender, when it comes to the newly introduced tasks.","A significant limitation of current approaches lies in their heavy reliance on the i.i.d (independent and identically distributed) assumption concerning data, leading to a static regret analysis of the framework.","Nevertheless, it's crucial to note that achieving low static regret does not necessarily translate to strong performance in dynamic environments characterized by tasks sampled from diverse distributions.","In this paper, to tackle the fairness-aware online learning challenge in evolving settings, we introduce a unique regret measure, FairSAR, by incorporating long-term fairness constraints into a strongly adapted loss regret framework.","Moreover, to determine an optimal model parameter at each time step, we introduce an innovative adaptive fairness-aware online meta-learning algorithm, referred to as FairSAOML.","This algorithm possesses the ability to adjust to dynamic environments by effectively managing bias control and model accuracy.","The problem is framed as a bi-level convex-concave optimization, considering both the model's primal and dual parameters, which pertain to its accuracy and fairness attributes, respectively.","Theoretical analysis yields sub-linear upper bounds for both loss regret and the cumulative violation of fairness constraints.","Our experimental evaluation on various real-world datasets in dynamic environments demonstrates that our proposed FairSAOML algorithm consistently outperforms alternative approaches rooted in the most advanced prior online learning methods."],"url":"http://arxiv.org/abs/2402.12319v1","category":"cs.LG"}
{"created":"2024-02-19 17:37:11","title":"Cosserat Rod Modeling and Validation for a Soft Continuum Robot with Self-Controllable Variable Curvature","abstract":"This paper introduces a Cosserat rod based mathematical model for modeling a self-controllable variable curvature soft continuum robot. This soft continuum robot has a hollow inner channel and was developed with the ability to perform variable curvature utilizing a growing spine. The growing spine is able to grow and retract while modifies its stiffness through milli-size particle (glass bubble) granular jamming. This soft continuum robot can then perform continuous curvature variation, unlike previous approaches whose curvature variation is discrete and depends on the number of locking mechanisms or manual configurations. The robot poses an emergent modeling problem due to the variable stiffness growing spine which is addressed in this paper. We investigate the property of growing spine stiffness and incorporate it into the Cosserat rod model by implementing a combined stiffness approach. We conduct experiments with the soft continuum robot in various configurations and compared the results with our developed mathematical model. The results show that the mathematical model based on the adapted Cosserat rod matches the experimental results with only a 3.3\\% error with respect to the length of the soft continuum robot.","sentences":["This paper introduces a Cosserat rod based mathematical model for modeling a self-controllable variable curvature soft continuum robot.","This soft continuum robot has a hollow inner channel and was developed with the ability to perform variable curvature utilizing a growing spine.","The growing spine is able to grow and retract while modifies its stiffness through milli-size particle (glass bubble) granular jamming.","This soft continuum robot can then perform continuous curvature variation, unlike previous approaches whose curvature variation is discrete and depends on the number of locking mechanisms or manual configurations.","The robot poses an emergent modeling problem due to the variable stiffness growing spine which is addressed in this paper.","We investigate the property of growing spine stiffness and incorporate it into the Cosserat rod model by implementing a combined stiffness approach.","We conduct experiments with the soft continuum robot in various configurations and compared the results with our developed mathematical model.","The results show that the mathematical model based on the adapted Cosserat rod matches the experimental results with only a 3.3\\% error with respect to the length of the soft continuum robot."],"url":"http://arxiv.org/abs/2402.12315v1","category":"cs.RO"}
{"created":"2024-02-19 16:47:04","title":"Adaptive Skeleton Graph Decoding","abstract":"Large language models (LLMs) have seen significant adoption for natural language tasks, owing their success to massive numbers of model parameters (e.g., 70B+); however, LLM inference incurs significant computation and memory costs. Recent approaches propose parallel decoding strategies, such as Skeleton-of-Thought (SoT), to improve performance by breaking prompts down into sub-problems that can be decoded in parallel; however, they often suffer from reduced response quality. Our key insight is that we can request additional information, specifically dependencies and difficulty, when generating the sub-problems to improve both response quality and performance. In this paper, we propose Skeleton Graph Decoding (SGD), which uses dependencies exposed between sub-problems to support information forwarding between dependent sub-problems for improved quality while exposing parallelization opportunities for decoding independent sub-problems. Additionally, we leverage difficulty estimates for each sub-problem to select an appropriately-sized model, improving performance without significantly reducing quality. Compared to standard autoregressive generation and SoT, SGD achieves a 1.69x speedup while improving quality by up to 51%.","sentences":["Large language models (LLMs) have seen significant adoption for natural language tasks, owing their success to massive numbers of model parameters (e.g., 70B+); however, LLM inference incurs significant computation and memory costs.","Recent approaches propose parallel decoding strategies, such as Skeleton-of-Thought (SoT), to improve performance by breaking prompts down into sub-problems that can be decoded in parallel; however, they often suffer from reduced response quality.","Our key insight is that we can request additional information, specifically dependencies and difficulty, when generating the sub-problems to improve both response quality and performance.","In this paper, we propose Skeleton Graph Decoding (SGD), which uses dependencies exposed between sub-problems to support information forwarding between dependent sub-problems for improved quality while exposing parallelization opportunities for decoding independent sub-problems.","Additionally, we leverage difficulty estimates for each sub-problem to select an appropriately-sized model, improving performance without significantly reducing quality.","Compared to standard autoregressive generation and SoT, SGD achieves a 1.69x speedup while improving quality by up to 51%."],"url":"http://arxiv.org/abs/2402.12280v1","category":"cs.CL"}
{"created":"2024-02-19 16:30:35","title":"End-to-end Supervised Prediction of Arbitrary-size Graphs with Partially-Masked Fused Gromov-Wasserstein Matching","abstract":"We present a novel end-to-end deep learning-based approach for Supervised Graph Prediction (SGP). We introduce an original Optimal Transport (OT)-based loss, the Partially-Masked Fused Gromov-Wasserstein loss (PM-FGW), that allows to directly leverage graph representations such as adjacency and feature matrices. PM-FGW exhibits all the desirable properties for SGP: it is node permutation invariant, sub-differentiable and handles graphs of different sizes by comparing their padded representations as well as their masking vectors. Moreover, we present a flexible transformer-based architecture that easily adapts to different types of input data. In the experimental section, three different tasks, a novel and challenging synthetic dataset (image2graph) and two real-world tasks, image2map and fingerprint2molecule - showcase the efficiency and versatility of the approach compared to competitors.","sentences":["We present a novel end-to-end deep learning-based approach for Supervised Graph Prediction (SGP).","We introduce an original Optimal Transport (OT)-based loss, the Partially-Masked Fused Gromov-Wasserstein loss (PM-FGW), that allows to directly leverage graph representations such as adjacency and feature matrices.","PM-FGW exhibits all the desirable properties for SGP: it is node permutation invariant, sub-differentiable and handles graphs of different sizes by comparing their padded representations as well as their masking vectors.","Moreover, we present a flexible transformer-based architecture that easily adapts to different types of input data.","In the experimental section, three different tasks, a novel and challenging synthetic dataset (image2graph) and two real-world tasks, image2map and fingerprint2molecule - showcase the efficiency and versatility of the approach compared to competitors."],"url":"http://arxiv.org/abs/2402.12269v1","category":"cs.LG"}
{"created":"2024-02-19 16:26:00","title":"Uncertainty quantification in fine-tuned LLMs using LoRA ensembles","abstract":"Fine-tuning large language models can improve task specific performance, although a general understanding of what the fine-tuned model has learned, forgotten and how to trust its predictions is still missing. We derive principled uncertainty quantification for fine-tuned LLMs with posterior approximations using computationally efficient low-rank adaptation ensembles. We analyze three common multiple-choice datasets using low-rank adaptation ensembles based on Mistral-7b, and draw quantitative and qualitative conclusions on their perceived complexity and model efficacy on the different target domains during and after fine-tuning. In particular, backed by the numerical experiments, we hypothesise about signals from entropic uncertainty measures for data domains that are inherently difficult for a given architecture to learn.","sentences":["Fine-tuning large language models can improve task specific performance, although a general understanding of what the fine-tuned model has learned, forgotten and how to trust its predictions is still missing.","We derive principled uncertainty quantification for fine-tuned LLMs with posterior approximations using computationally efficient low-rank adaptation ensembles.","We analyze three common multiple-choice datasets using low-rank adaptation ensembles based on Mistral-7b, and draw quantitative and qualitative conclusions on their perceived complexity and model efficacy on the different target domains during and after fine-tuning.","In particular, backed by the numerical experiments, we hypothesise about signals from entropic uncertainty measures for data domains that are inherently difficult for a given architecture to learn."],"url":"http://arxiv.org/abs/2402.12264v1","category":"cs.LG"}
{"created":"2024-02-19 15:26:19","title":"Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic Forgetting","abstract":"Although motivated by the adaptation of text-to-speech synthesis models, we argue that more generic parameter-efficient fine-tuning (PEFT) is an appropriate framework to do such adaptation. However, catastrophic forgetting remains an issue with PEFT, damaging the pre-trained model's inherent capabilities. We demonstrate that existing Bayesian learning techniques can be applied to PEFT to prevent catastrophic forgetting as long as the parameter shift of the fine-tuned layers can be calculated differentiably. In a principled series of experiments on language modeling and speech synthesis tasks, we utilize established Laplace approximations, including diagonal and Kronecker factored approaches, to regularize PEFT with the low-rank adaptation (LoRA) and compare their performance in pre-training knowledge preservation. Our results demonstrate that catastrophic forgetting can be overcome by our methods without degrading the fine-tuning performance, and using the Kronecker factored approximations produces a better preservation of the pre-training knowledge than the diagonal ones.","sentences":["Although motivated by the adaptation of text-to-speech synthesis models, we argue that more generic parameter-efficient fine-tuning (PEFT) is an appropriate framework to do such adaptation.","However, catastrophic forgetting remains an issue with PEFT, damaging the pre-trained model's inherent capabilities.","We demonstrate that existing Bayesian learning techniques can be applied to PEFT to prevent catastrophic forgetting as long as the parameter shift of the fine-tuned layers can be calculated differentiably.","In a principled series of experiments on language modeling and speech synthesis tasks, we utilize established Laplace approximations, including diagonal and Kronecker factored approaches, to regularize PEFT with the low-rank adaptation (LoRA) and compare their performance in pre-training knowledge preservation.","Our results demonstrate that catastrophic forgetting can be overcome by our methods without degrading the fine-tuning performance, and using the Kronecker factored approximations produces a better preservation of the pre-training knowledge than the diagonal ones."],"url":"http://arxiv.org/abs/2402.12220v1","category":"eess.AS"}
{"created":"2024-02-19 15:06:04","title":"Heterogeneity-aware Cross-school Electives Recommendation: a Hybrid Federated Approach","abstract":"In the era of modern education, addressing cross-school learner diversity is crucial, especially in personalized recommender systems for elective course selection. However, privacy concerns often limit cross-school data sharing, which hinders existing methods' ability to model sparse data and address heterogeneity effectively, ultimately leading to suboptimal recommendations. In response, we propose HFRec, a heterogeneity-aware hybrid federated recommender system designed for cross-school elective course recommendations. The proposed model constructs heterogeneous graphs for each school, incorporating various interactions and historical behaviors between students to integrate context and content information. We design an attention mechanism to capture heterogeneity-aware representations. Moreover, under a federated scheme, we train individual school-based models with adaptive learning settings to recommend tailored electives. Our HFRec model demonstrates its effectiveness in providing personalized elective recommendations while maintaining privacy, as it outperforms state-of-the-art models on both open-source and real-world datasets.","sentences":["In the era of modern education, addressing cross-school learner diversity is crucial, especially in personalized recommender systems for elective course selection.","However, privacy concerns often limit cross-school data sharing, which hinders existing methods' ability to model sparse data and address heterogeneity effectively, ultimately leading to suboptimal recommendations.","In response, we propose HFRec, a heterogeneity-aware hybrid federated recommender system designed for cross-school elective course recommendations.","The proposed model constructs heterogeneous graphs for each school, incorporating various interactions and historical behaviors between students to integrate context and content information.","We design an attention mechanism to capture heterogeneity-aware representations.","Moreover, under a federated scheme, we train individual school-based models with adaptive learning settings to recommend tailored electives.","Our HFRec model demonstrates its effectiveness in providing personalized elective recommendations while maintaining privacy, as it outperforms state-of-the-art models on both open-source and real-world datasets."],"url":"http://arxiv.org/abs/2402.12202v1","category":"cs.IR"}
{"created":"2024-02-19 15:00:51","title":"Nanomechanical crystalline AlN resonators with high quality factors for quantum optoelectromechanics","abstract":"High-$Q_m$ mechanical resonators are crucial for applications where low noise and long coherence time are required, as mirror suspensions, quantum cavity optomechanical devices, or nanomechanical sensors. Tensile strain in the material enables the use of dissipation dilution and strain engineering techniques, which increase the mechanical quality factor. These techniques have been employed for high-$Q_m$ mechanical resonators made from amorphous materials and, recently, from crystalline materials such as InGaP, SiC, and Si. A strained crystalline film exhibiting substantial piezoelectricity expands the capability of high-$Q_m$ nanomechanical resonators to directly utilize electronic degrees of freedom. In this work we realize nanomechanical resonators with $Q_m$ up to $2.9\\times 10^{7}$ made from tensile-strained 290 nm-thick AlN, which is an epitaxially-grown crystalline material offering strong piezoelectricity. We demonstrate nanomechanical resonators that exploit dissipation dilution and strain engineering to reach a $Q_m \\times f_m$-product approaching $10^{13}$ Hz at room temperature. We realize a novel resonator geometry, triangline, whose shape follows the Al-N bonds and offers a central pad that we pattern with a photonic crystal. This allows us to reach an optical reflectivity above 80% for efficient coupling to out-of-plane light. The presented results pave the way for quantum optoelectromechanical devices at room temperature based on tensile-strained AlN.","sentences":["High-$Q_m$ mechanical resonators are crucial for applications where low noise and long coherence time are required, as mirror suspensions, quantum cavity optomechanical devices, or nanomechanical sensors.","Tensile strain in the material enables the use of dissipation dilution and strain engineering techniques, which increase the mechanical quality factor.","These techniques have been employed for high-$Q_m$ mechanical resonators made from amorphous materials and, recently, from crystalline materials such as InGaP, SiC, and Si.","A strained crystalline film exhibiting substantial piezoelectricity expands the capability of high-$Q_m$ nanomechanical resonators to directly utilize electronic degrees of freedom.","In this work we realize nanomechanical resonators with $Q_m$ up to $2.9\\times 10^{7}$ made from tensile-strained 290 nm-thick AlN, which is an epitaxially-grown crystalline material offering strong piezoelectricity.","We demonstrate nanomechanical resonators that exploit dissipation dilution and strain engineering to reach a $Q_m \\times f_m$-product approaching $10^{13}$ Hz at room temperature.","We realize a novel resonator geometry, triangline, whose shape follows the Al-N bonds and offers a central pad that we pattern with a photonic crystal.","This allows us to reach an optical reflectivity above 80% for efficient coupling to out-of-plane light.","The presented results pave the way for quantum optoelectromechanical devices at room temperature based on tensile-strained AlN."],"url":"http://arxiv.org/abs/2402.12196v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-19 14:42:29","title":"A Riemannian rank-adaptive method for higher-order tensor completion in the tensor-train format","abstract":"In this paper a new Riemannian rank adaptive method (RRAM) is proposed for the low-rank tensor completion problem (LRTCP) formulated as a least-squares optimization problem on the algebraic variety of tensors of bounded tensor-train (TT) rank. The method iteratively optimizes over fixed-rank smooth manifolds using a Riemannian conjugate gradient algorithm from Steinlechner (2016) and gradually increases the rank by computing a descent direction in the tangent cone to the variety. Additionally, a numerical method to estimate the amount of rank increase is proposed based on a theoretical result for the stationary points of the low-rank tensor approximation problem and a definition of an estimated TT-rank. Furthermore, when the iterate comes close to a lower-rank set, the RRAM decreases the rank based on the TT-rounding algorithm from Oseledets (2011) and a definition of a numerical rank. We prove that the TT-rounding algorithm can be considered as an approximate projection onto the lower-rank set which satisfies a certain angle condition to ensure that the image is sufficiently close to that of an exact projection. Several numerical experiments are given to illustrate the use of the RRAM and its subroutines in {\\Matlab}. Furthermore, in all experiments the proposed RRAM outperforms the state-of-the-art RRAM for tensor completion in the TT format from Steinlechner (2016) in terms of computation time.","sentences":["In this paper a new Riemannian rank adaptive method (RRAM) is proposed for the low-rank tensor completion problem (LRTCP) formulated as a least-squares optimization problem on the algebraic variety of tensors of bounded tensor-train (TT) rank.","The method iteratively optimizes over fixed-rank smooth manifolds using a Riemannian conjugate gradient algorithm from Steinlechner (2016) and gradually increases the rank by computing a descent direction in the tangent cone to the variety.","Additionally, a numerical method to estimate the amount of rank increase is proposed based on a theoretical result for the stationary points of the low-rank tensor approximation problem and a definition of an estimated TT-rank.","Furthermore, when the iterate comes close to a lower-rank set, the RRAM decreases the rank based on the TT-rounding algorithm from Oseledets (2011) and a definition of a numerical rank.","We prove that the TT-rounding algorithm can be considered as an approximate projection onto the lower-rank set which satisfies a certain angle condition to ensure that the image is sufficiently close to that of an exact projection.","Several numerical experiments are given to illustrate the use of the RRAM and its subroutines in {\\Matlab}.","Furthermore, in all experiments the proposed RRAM outperforms the state-of-the-art RRAM for tensor completion in the TT format from Steinlechner (2016) in terms of computation time."],"url":"http://arxiv.org/abs/2402.12182v1","category":"math.OC"}
{"created":"2024-02-19 14:42:10","title":"Revisiting Data Augmentation in Deep Reinforcement Learning","abstract":"Various data augmentation techniques have been recently proposed in image-based deep reinforcement learning (DRL). Although they empirically demonstrate the effectiveness of data augmentation for improving sample efficiency or generalization, which technique should be preferred is not always clear. To tackle this question, we analyze existing methods to better understand them and to uncover how they are connected. Notably, by expressing the variance of the Q-targets and that of the empirical actor/critic losses of these methods, we can analyze the effects of their different components and compare them. We furthermore formulate an explanation about how these methods may be affected by choosing different data augmentation transformations in calculating the target Q-values. This analysis suggests recommendations on how to exploit data augmentation in a more principled way. In addition, we include a regularization term called tangent prop, previously proposed in computer vision, but whose adaptation to DRL is novel to the best of our knowledge. We evaluate our proposition and validate our analysis in several domains. Compared to different relevant baselines, we demonstrate that it achieves state-of-the-art performance in most environments and shows higher sample efficiency and better generalization ability in some complex environments.","sentences":["Various data augmentation techniques have been recently proposed in image-based deep reinforcement learning (DRL).","Although they empirically demonstrate the effectiveness of data augmentation for improving sample efficiency or generalization, which technique should be preferred is not always clear.","To tackle this question, we analyze existing methods to better understand them and to uncover how they are connected.","Notably, by expressing the variance of the Q-targets and that of the empirical actor/critic losses of these methods, we can analyze the effects of their different components and compare them.","We furthermore formulate an explanation about how these methods may be affected by choosing different data augmentation transformations in calculating the target Q-values.","This analysis suggests recommendations on how to exploit data augmentation in a more principled way.","In addition, we include a regularization term called tangent prop, previously proposed in computer vision, but whose adaptation to DRL is novel to the best of our knowledge.","We evaluate our proposition and validate our analysis in several domains.","Compared to different relevant baselines, we demonstrate that it achieves state-of-the-art performance in most environments and shows higher sample efficiency and better generalization ability in some complex environments."],"url":"http://arxiv.org/abs/2402.12181v1","category":"cs.LG"}
{"created":"2024-02-19 14:26:58","title":"Second-order flows for approaching stationary points of a class of non-convex energies via convex-splitting schemes","abstract":"The use of accelerated gradient flows is an emerging field in optimization, scientific computing and beyond. This paper contributes to the theoretical underpinnings of a recently-introduced computational paradigm known as second-order flows, which demonstrate significant performance particularly for the minimization of non-convex energy functionals defined on Sobolev spaces, and are characterized by novel dissipative hyperbolic partial differential equations. Our approach hinges upon convex-splitting schemes, a tool which is not only pivotal for clarifying the well-posedness of second-order flows, but also yields a versatile array of robust numerical schemes through temporal and spatial discretization. We prove the convergence to stationary points of such schemes in the semi-discrete setting. Further, we establish their convergence to time-continuous solutions as the time-step tends to zero, and perform a comprehensive error analysis in the fully discrete case. Finally, these algorithms undergo thorough testing and validation in approaching stationary points of non-convex variational models in applied sciences, such as the Ginzburg-Landau energy in phase-field modeling and a specific case of the Landau-de Gennes energy of the Q-tensor model for liquid crystals.","sentences":["The use of accelerated gradient flows is an emerging field in optimization, scientific computing and beyond.","This paper contributes to the theoretical underpinnings of a recently-introduced computational paradigm known as second-order flows, which demonstrate significant performance particularly for the minimization of non-convex energy functionals defined on Sobolev spaces, and are characterized by novel dissipative hyperbolic partial differential equations.","Our approach hinges upon convex-splitting schemes, a tool which is not only pivotal for clarifying the well-posedness of second-order flows, but also yields a versatile array of robust numerical schemes through temporal and spatial discretization.","We prove the convergence to stationary points of such schemes in the semi-discrete setting.","Further, we establish their convergence to time-continuous solutions as the time-step tends to zero, and perform a comprehensive error analysis in the fully discrete case.","Finally, these algorithms undergo thorough testing and validation in approaching stationary points of non-convex variational models in applied sciences, such as the Ginzburg-Landau energy in phase-field modeling and a specific case of the Landau-de Gennes energy of the Q-tensor model for liquid crystals."],"url":"http://arxiv.org/abs/2402.12173v1","category":"math.NA"}
{"created":"2024-02-19 14:26:41","title":"Large $N$ Schur index of $\\mathcal N=4$ SYM from semiclassical D3 brane","abstract":"We consider the refined Schur superconformal index of 4d $\\mathcal N=4$ $U(N)$ SYM and the first term of its giant-graviton expansion, first predicted in arXiv:2001.11667 using indirect superconformal algebra considerations and analytic continuation of fugacities. This correction is the leading non-perturbative correction to the index at large $N$ and we reproduce it from the semiclassical partition function of quantum D3 brane wrapped on $S^{1}\\times S^{3}$ in a twisted modification of the $AdS_{5}\\times S^{5}$ string background, depending on the index R-symmetry fugacity. Our calculation does not exploit directly supersymmetry. It is based on the determination of the partition function of the various bosonic and fermionic fluctuations on the wrapped brane whose action is conformal with specific constant holonomies along thermal cycle. We show how those partition functions may be obtained by adapting the operator counting method of Cardy to the twisted background.","sentences":["We consider the refined Schur superconformal index of 4d $\\mathcal N=4$ $U(N)$ SYM and the first term of its giant-graviton expansion, first predicted in arXiv:2001.11667 using indirect superconformal algebra considerations and analytic continuation of fugacities.","This correction is the leading non-perturbative correction to the index at large $N$ and we reproduce it from the semiclassical partition function of quantum D3 brane wrapped on $S^{1}\\times S^{3}$ in a twisted modification of the $AdS_{5}\\times S^{5}$ string background, depending on the index R-symmetry fugacity.","Our calculation does not exploit directly supersymmetry.","It is based on the determination of the partition function of the various bosonic and fermionic fluctuations on the wrapped brane whose action is conformal with specific constant holonomies along thermal cycle.","We show how those partition functions may be obtained by adapting the operator counting method of Cardy to the twisted background."],"url":"http://arxiv.org/abs/2402.12172v1","category":"hep-th"}
{"created":"2024-02-19 14:16:08","title":"Endowing Pre-trained Graph Models with Provable Fairness","abstract":"Pre-trained graph models (PGMs) aim to capture transferable inherent structural properties and apply them to different downstream tasks. Similar to pre-trained language models, PGMs also inherit biases from human society, resulting in discriminatory behavior in downstream applications. The debiasing process of existing fair methods is generally coupled with parameter optimization of GNNs. However, different downstream tasks may be associated with different sensitive attributes in reality, directly employing existing methods to improve the fairness of PGMs is inflexible and inefficient. Moreover, most of them lack a theoretical guarantee, i.e., provable lower bounds on the fairness of model predictions, which directly provides assurance in a practical scenario. To overcome these limitations, we propose a novel adapter-tuning framework that endows pre-trained graph models with provable fairness (called GraphPAR). GraphPAR freezes the parameters of PGMs and trains a parameter-efficient adapter to flexibly improve the fairness of PGMs in downstream tasks. Specifically, we design a sensitive semantic augmenter on node representations, to extend the node representations with different sensitive attribute semantics for each node. The extended representations will be used to further train an adapter, to prevent the propagation of sensitive attribute semantics from PGMs to task predictions. Furthermore, with GraphPAR, we quantify whether the fairness of each node is provable, i.e., predictions are always fair within a certain range of sensitive attribute semantics. Experimental evaluations on real-world datasets demonstrate that GraphPAR achieves state-of-the-art prediction performance and fairness on node classification task. Furthermore, based on our GraphPAR, around 90\\% nodes have provable fairness.","sentences":["Pre-trained graph models (PGMs) aim to capture transferable inherent structural properties and apply them to different downstream tasks.","Similar to pre-trained language models, PGMs also inherit biases from human society, resulting in discriminatory behavior in downstream applications.","The debiasing process of existing fair methods is generally coupled with parameter optimization of GNNs.","However, different downstream tasks may be associated with different sensitive attributes in reality, directly employing existing methods to improve the fairness of PGMs is inflexible and inefficient.","Moreover, most of them lack a theoretical guarantee, i.e., provable lower bounds on the fairness of model predictions, which directly provides assurance in a practical scenario.","To overcome these limitations, we propose a novel adapter-tuning framework that endows pre-trained graph models with provable fairness (called GraphPAR).","GraphPAR freezes the parameters of PGMs and trains a parameter-efficient adapter to flexibly improve the fairness of PGMs in downstream tasks.","Specifically, we design a sensitive semantic augmenter on node representations, to extend the node representations with different sensitive attribute semantics for each node.","The extended representations will be used to further train an adapter, to prevent the propagation of sensitive attribute semantics from PGMs to task predictions.","Furthermore, with GraphPAR, we quantify whether the fairness of each node is provable, i.e., predictions are always fair within a certain range of sensitive attribute semantics.","Experimental evaluations on real-world datasets demonstrate that GraphPAR achieves state-of-the-art prediction performance and fairness on node classification task.","Furthermore, based on our GraphPAR, around 90\\% nodes have provable fairness."],"url":"http://arxiv.org/abs/2402.12161v2","category":"cs.LG"}
{"created":"2024-02-19 13:16:10","title":"Almost sure convergence rates of adaptive increasingly rare Markov chain Monte Carlo","abstract":"We consider adaptive increasingly rare Markov chain Monte Carlo (AIR MCMC), which is an adaptive MCMC method, where the adaptation concerning the past happens less and less frequently over time. Under a contraction assumption for a Wasserstein-like function we deduce upper bounds of the convergence rate of Monte Carlo sums taking a renormalisation factor into account that is close to the one that appears in a law of the iterated logarithm. We demonstrate the applicability of our results by considering different settings, among which are those of simultaneous geometric and uniform ergodicity. All proofs are carried out on an augmented state space, including the classical non-augmented setting as a special case. In contrast to other adaptive MCMC limit theory, some technical assumptions, like diminishing adaptation, are not needed.","sentences":["We consider adaptive increasingly rare Markov chain Monte Carlo (AIR MCMC), which is an adaptive MCMC method, where the adaptation concerning the past happens less and less frequently over time.","Under a contraction assumption for a Wasserstein-like function we deduce upper bounds of the convergence rate of Monte Carlo sums taking a renormalisation factor into account that is close to the one that appears in a law of the iterated logarithm.","We demonstrate the applicability of our results by considering different settings, among which are those of simultaneous geometric and uniform ergodicity.","All proofs are carried out on an augmented state space, including the classical non-augmented setting as a special case.","In contrast to other adaptive MCMC limit theory, some technical assumptions, like diminishing adaptation, are not needed."],"url":"http://arxiv.org/abs/2402.12122v1","category":"math.NA"}
{"created":"2024-02-19 13:16:09","title":"TASTE V. A new ground-based investigation of orbital decay in the ultra-hot Jupiter WASP-12b","abstract":"The discovery of the first transiting hot Jupiters (HJs; giant planets on orbital periods shorter than $P\\sim10$ days) was announced more than twenty years ago. As both ground- and space-based follow-up observations are piling up, we are approaching the temporal baseline required to detect secular variations in their orbital parameters. In particular, several recent studies focused on constraining the efficiency of the tidal decay mechanism to better understand the evolutionary time scales of HJ migration and engulfment. This can be achieved by measuring a monotonic decrease of orbital period $\\mathrm{d}P/\\mathrm{d}t<0$ due to mechanical energy being dissipated by tidal friction. WASP-12b was the first HJ for which a tidal decay scenario appeared convincing, even though alternative explanations have been hypothesized. Here we present a new analysis based on 28 unpublished high-precision transit light curves gathered over a twelve-year baseline and combined with all the available archival data, and an updated set of stellar parameters from HARPS-N high-resolution spectra, which are consistent with a main sequence scenario, close to the hydrogen exhaustion in the core. Our values of $\\mathrm{d}P/\\mathrm{d}t$ = $-30.72 \\pm 2.67$ and $Q_{\\ast}^{'}$ = $(2.13 \\pm 0.18) \\times 10^{5}$ are statistically consistent with previous studies, and indicate that WASP-12 is undergoing fast tidal dissipation. We additionally report the presence of an excess scatter in the timing data and discuss its possible origin.","sentences":["The discovery of the first transiting hot Jupiters (HJs; giant planets on orbital periods shorter than $P\\sim10$ days) was announced more than twenty years ago.","As both ground- and space-based follow-up observations are piling up, we are approaching the temporal baseline required to detect secular variations in their orbital parameters.","In particular, several recent studies focused on constraining the efficiency of the tidal decay mechanism to better understand the evolutionary time scales of HJ migration and engulfment.","This can be achieved by measuring a monotonic decrease of orbital period $\\mathrm{d}P/\\mathrm{d}t<0$ due to mechanical energy being dissipated by tidal friction.","WASP-12b was the first HJ for which a tidal decay scenario appeared convincing, even though alternative explanations have been hypothesized.","Here we present a new analysis based on 28 unpublished high-precision transit light curves gathered over a twelve-year baseline and combined with all the available archival data, and an updated set of stellar parameters from HARPS-N high-resolution spectra, which are consistent with a main sequence scenario, close to the hydrogen exhaustion in the core.","Our values of $\\mathrm{d}P/\\mathrm{d}t$ = $-30.72 \\pm 2.67$ and $Q_{\\ast}^{'}$ = $(2.13 \\pm 0.18)","\\times 10^{5}$ are statistically consistent with previous studies, and indicate that WASP-12 is undergoing fast tidal dissipation.","We additionally report the presence of an excess scatter in the timing data and discuss its possible origin."],"url":"http://arxiv.org/abs/2402.12120v2","category":"astro-ph.EP"}
{"created":"2024-02-19 12:11:20","title":"Strengths and Weaknesses of the ETSI Adaptive DCC Algorithm: A Proposal for Improvement","abstract":"This letter studies the adaptive Decentralized Congestion Control (DCC) algorithm defined in the ETSI TS 102 687 V1.2.1 specification. We provide insights on the parameters used in the algorithm and explore the impact of those parameters on its performance. We show how the algorithm achieves good average medium utilization while protecting against congestion, but we also show how the chosen parameters can result in slow speed of convergence and long periods of unfairness in transitory situations. Finally, we propose a modification to the algorithm which results in significant improvements in speed of convergence and fairness.","sentences":["This letter studies the adaptive Decentralized Congestion Control (DCC)","algorithm defined in the ETSI TS 102 687","V1.2.1 specification.","We provide insights on the parameters used in the algorithm and explore the impact of those parameters on its performance.","We show how the algorithm achieves good average medium utilization while protecting against congestion, but we also show how the chosen parameters can result in slow speed of convergence and long periods of unfairness in transitory situations.","Finally, we propose a modification to the algorithm which results in significant improvements in speed of convergence and fairness."],"url":"http://arxiv.org/abs/2402.12089v1","category":"cs.NI"}
{"created":"2024-02-19 11:55:17","title":"Single and Multi-Objective Real-Time Optimisation of an Industrial Injection Moulding Process via a Bayesian Adaptive Design of Experiment Approach","abstract":"Minimising cycle time without inducing quality defects is a major challenge in the injection moulding (IM). Design of Experiment methods (DoE) have been widely studied for optimisation of the IM, however existing methods have limitations, including the need for a large number of experiments and a pre-determined search space. Bayesian adaptive design of experiment (ADoE) is an iterative process where the results of the previous experiments are used to make an informed selection for the next design. In this study, for the first time, an experimental ADoE approach, based on Bayesian optimisation, was developed in injection moulding using process and sensor data to optimise the quality and cycle time in real-time. A novel approach for the real-time characterisation of post-production shrinkage was introduced, utilising in-mould sensor data on temperature differential during part cooling. This characterisation approach was verified by post-production metrology results.   A single and multi-objective optimisation of the cycle time and temperature differential in an injection moulded component is proposed. The multi-objective optimisation techniques, composite desirability function and Nondominated Sorting Genetic Algorithm (NSGA-II) using Response Surface Methodology (RSM) model, are compared with the real-time novel ADoE approach. ADoE achieved almost a 50% reduction in the number of experiments required for the single optimisation of temperature differential, and an almost 30% decrease for the optimisation of temperature differential and cycle time together compared to composite desirability function and NSGA-II. Also, the optimal settings identified by ADoE for multiobjective optimisation were similar to the selected Pareto optimal solution found by the NSGA-II.","sentences":["Minimising cycle time without inducing quality defects is a major challenge in the injection moulding (IM).","Design of Experiment methods (DoE) have been widely studied for optimisation of the IM, however existing methods have limitations, including the need for a large number of experiments and a pre-determined search space.","Bayesian adaptive design of experiment (ADoE) is an iterative process where the results of the previous experiments are used to make an informed selection for the next design.","In this study, for the first time, an experimental ADoE approach, based on Bayesian optimisation, was developed in injection moulding using process and sensor data to optimise the quality and cycle time in real-time.","A novel approach for the real-time characterisation of post-production shrinkage was introduced, utilising in-mould sensor data on temperature differential during part cooling.","This characterisation approach was verified by post-production metrology results.   ","A single and multi-objective optimisation of the cycle time and temperature differential in an injection moulded component is proposed.","The multi-objective optimisation techniques, composite desirability function and Nondominated Sorting Genetic Algorithm (NSGA-II) using Response Surface Methodology (RSM) model, are compared with the real-time novel ADoE approach.","ADoE achieved almost a 50% reduction in the number of experiments required for the single optimisation of temperature differential, and an almost 30% decrease for the optimisation of temperature differential and cycle time together compared to composite desirability function and NSGA-II.","Also, the optimal settings identified by ADoE for multiobjective optimisation were similar to the selected Pareto optimal solution found by the NSGA-II."],"url":"http://arxiv.org/abs/2402.12077v1","category":"eess.SY"}
{"created":"2024-02-19 11:02:05","title":"Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models","abstract":"Catastrophic forgetting emerges as a critical challenge when fine-tuning multi-modal large language models (MLLMs), where improving performance on unseen tasks often leads to a significant performance drop on the original tasks. This paper presents a comprehensive analysis of catastrophic forgetting in MLLMs and introduces a post-training adjustment method called Model Tailor. Our method primarily preserves the pre-trained parameters while replacing a small number ($\\leq$ 10\\%) of fine-tuned parameters, maintaining $\\sim$ 99\\% effectiveness on original tasks versus pre-training, and achieving $\\sim$ 97\\% on new tasks compared to standard fine-tuning. Specifically, we derive a sparse mask to identify the \"model patch\", based on a fusion strategy that integrates salience and sensitivity analysis. Subsequently, a compensation mechanism is introduced to \"decorate the patch\", enhancing the model's performance on both target and original tasks. Additionally, our method is adaptable to multi-task scenarios. Through extensive experiments on InstructBLIP and LLaVA-1.5 in both image captioning and visual question answering tasks, our approach demonstrates significant task adaptability while preserving inherent pre-trained capabilities.","sentences":["Catastrophic forgetting emerges as a critical challenge when fine-tuning multi-modal large language models (MLLMs), where improving performance on unseen tasks often leads to a significant performance drop on the original tasks.","This paper presents a comprehensive analysis of catastrophic forgetting in MLLMs and introduces a post-training adjustment method called Model Tailor.","Our method primarily preserves the pre-trained parameters while replacing a small number ($\\leq$ 10\\%) of fine-tuned parameters, maintaining $\\sim$ 99\\% effectiveness on original tasks versus pre-training, and achieving $\\sim$ 97\\% on new tasks compared to standard fine-tuning.","Specifically, we derive a sparse mask to identify the \"model patch\", based on a fusion strategy that integrates salience and sensitivity analysis.","Subsequently, a compensation mechanism is introduced to \"decorate the patch\", enhancing the model's performance on both target and original tasks.","Additionally, our method is adaptable to multi-task scenarios.","Through extensive experiments on InstructBLIP and LLaVA-1.5 in both image captioning and visual question answering tasks, our approach demonstrates significant task adaptability while preserving inherent pre-trained capabilities."],"url":"http://arxiv.org/abs/2402.12048v1","category":"cs.CL"}
{"created":"2024-02-19 10:43:27","title":"Language Model Adaptation to Specialized Domains through Selective Masking based on Genre and Topical Characteristics","abstract":"Recent advances in pre-trained language modeling have facilitated significant progress across various natural language processing (NLP) tasks. Word masking during model training constitutes a pivotal component of language modeling in architectures like BERT. However, the prevalent method of word masking relies on random selection, potentially disregarding domain-specific linguistic attributes. In this article, we introduce an innovative masking approach leveraging genre and topicality information to tailor language models to specialized domains. Our method incorporates a ranking process that prioritizes words based on their significance, subsequently guiding the masking procedure. Experiments conducted using continual pre-training within the legal domain have underscored the efficacy of our approach on the LegalGLUE benchmark in the English language. Pre-trained language models and code are freely available for use.","sentences":["Recent advances in pre-trained language modeling have facilitated significant progress across various natural language processing (NLP) tasks.","Word masking during model training constitutes a pivotal component of language modeling in architectures like BERT.","However, the prevalent method of word masking relies on random selection, potentially disregarding domain-specific linguistic attributes.","In this article, we introduce an innovative masking approach leveraging genre and topicality information to tailor language models to specialized domains.","Our method incorporates a ranking process that prioritizes words based on their significance, subsequently guiding the masking procedure.","Experiments conducted using continual pre-training within the legal domain have underscored the efficacy of our approach on the LegalGLUE benchmark in the English language.","Pre-trained language models and code are freely available for use."],"url":"http://arxiv.org/abs/2402.12036v1","category":"cs.CL"}
{"created":"2024-02-19 10:40:32","title":"Mass-energy scattering criterion for double power Schr{\u00f6}dinger equations","abstract":"We consider the nonlinear Schr{\\\"o}dinger equation with double power nonlinearity. We extend the scattering result in [17] for all L 2-supercritical powers, specially, our results adapt to the cases of energy-supercritical nonlinearity.","sentences":["We consider the nonlinear Schr{\\\"o}dinger equation with double power nonlinearity.","We extend the scattering result in [17] for all L 2-supercritical powers, specially, our results adapt to the cases of energy-supercritical nonlinearity."],"url":"http://arxiv.org/abs/2402.13286v1","category":"math.AP"}
{"created":"2024-02-19 10:34:48","title":"Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space","abstract":"Despite the notable success of language models (LMs) in various natural language processing (NLP) tasks, the reliability of LMs is susceptible to backdoor attacks. Prior research attempts to mitigate backdoor learning while training the LMs on the poisoned dataset, yet struggles against complex backdoor attacks in real-world scenarios. In this paper, we investigate the learning mechanisms of backdoor LMs in the frequency space by Fourier analysis. Our findings indicate that the backdoor mapping presented on the poisoned datasets exhibits a more discernible inclination towards lower frequency compared to clean mapping, resulting in the faster convergence of backdoor mapping. To alleviate this dilemma, we propose Multi-Scale Low-Rank Adaptation (MuScleLoRA), which deploys multiple radial scalings in the frequency space with low-rank adaptation to the target model and further aligns the gradients when updating parameters. Through downscaling in the frequency space, MuScleLoRA encourages the model to prioritize the learning of relatively high-frequency clean mapping, consequently mitigating backdoor learning. Experimental results demonstrate that MuScleLoRA outperforms baselines significantly. Notably, MuScleLoRA reduces the average success rate of diverse backdoor attacks to below 15\\% across multiple datasets and generalizes to various backbone LMs, including BERT, RoBERTa, and Llama2. The codes are available at https://github.com/ZrW00/MuScleLoRA.","sentences":["Despite the notable success of language models (LMs) in various natural language processing (NLP) tasks, the reliability of LMs is susceptible to backdoor attacks.","Prior research attempts to mitigate backdoor learning while training the LMs on the poisoned dataset, yet struggles against complex backdoor attacks in real-world scenarios.","In this paper, we investigate the learning mechanisms of backdoor LMs in the frequency space by Fourier analysis.","Our findings indicate that the backdoor mapping presented on the poisoned datasets exhibits a more discernible inclination towards lower frequency compared to clean mapping, resulting in the faster convergence of backdoor mapping.","To alleviate this dilemma, we propose Multi-Scale Low-Rank Adaptation (MuScleLoRA), which deploys multiple radial scalings in the frequency space with low-rank adaptation to the target model and further aligns the gradients when updating parameters.","Through downscaling in the frequency space, MuScleLoRA encourages the model to prioritize the learning of relatively high-frequency clean mapping, consequently mitigating backdoor learning.","Experimental results demonstrate that MuScleLoRA outperforms baselines significantly.","Notably, MuScleLoRA reduces the average success rate of diverse backdoor attacks to below 15\\% across multiple datasets and generalizes to various backbone LMs, including BERT, RoBERTa, and Llama2.","The codes are available at https://github.com/ZrW00/MuScleLoRA."],"url":"http://arxiv.org/abs/2402.12026v1","category":"cs.CL"}
{"created":"2024-02-19 10:15:11","title":"Leveraging PAC-Bayes Theory and Gibbs Distributions for Generalization Bounds with Complexity Measures","abstract":"In statistical learning theory, a generalization bound usually involves a complexity measure imposed by the considered theoretical framework. This limits the scope of such bounds, as other forms of capacity measures or regularizations are used in algorithms. In this paper, we leverage the framework of disintegrated PAC-Bayes bounds to derive a general generalization bound instantiable with arbitrary complexity measures. One trick to prove such a result involves considering a commonly used family of distributions: the Gibbs distributions. Our bound stands in probability jointly over the hypothesis and the learning sample, which allows the complexity to be adapted to the generalization gap as it can be customized to fit both the hypothesis class and the task.","sentences":["In statistical learning theory, a generalization bound usually involves a complexity measure imposed by the considered theoretical framework.","This limits the scope of such bounds, as other forms of capacity measures or regularizations are used in algorithms.","In this paper, we leverage the framework of disintegrated PAC-Bayes bounds to derive a general generalization bound instantiable with arbitrary complexity measures.","One trick to prove such a result involves considering a commonly used family of distributions: the Gibbs distributions.","Our bound stands in probability jointly over the hypothesis and the learning sample, which allows the complexity to be adapted to the generalization gap as it can be customized to fit both the hypothesis class and the task."],"url":"http://arxiv.org/abs/2402.13285v1","category":"stat.ML"}
{"created":"2024-02-19 10:13:25","title":"An Index Policy Based on Sarsa and Q-learning for Heterogeneous Smart Target Tracking","abstract":"In solving the non-myopic radar scheduling for multiple smart target tracking within an active and passive radar network, we need to consider both short-term enhanced tracking performance and a higher probability of target maneuvering in the future with active tracking. Acquiring the long-term tracking performance while scheduling the beam resources of active and passive radars poses a challenge. To address this challenge, we model this problem as a Markov decision process consisting of parallel restless bandit processes. Each bandit process is associated with a smart target, of which the estimation state evolves according to different discrete dynamic models for different actions - whether or not the target is being tracked. The discrete state is defined by the dynamic mode. The problem exhibits the curse of dimensionality, where optimal solutions are in general intractable. We resort to heuristics through the famous restless multi-armed bandit techniques. It follows with efficient scheduling policies based on the indices that are real numbers representing the marginal rewards of taking different actions. For the inevitable practical case with unknown transition matrices, we propose a new method that utilizes the forward Sarsa and backward Q-learning to approximate the indices through adapting the state-action value functions, or equivalently the Q-functions, and propose a new policy, namely ISQ, aiming to maximize the long-term tracking rewards. Numerical results demonstrate that the proposed ISQ policy outperforms conventional Q-learning-based methods and rapidly converges to the well-known Whittle index policy with revealed state transition models, which is considered the benchmark.","sentences":["In solving the non-myopic radar scheduling for multiple smart target tracking within an active and passive radar network, we need to consider both short-term enhanced tracking performance and a higher probability of target maneuvering in the future with active tracking.","Acquiring the long-term tracking performance while scheduling the beam resources of active and passive radars poses a challenge.","To address this challenge, we model this problem as a Markov decision process consisting of parallel restless bandit processes.","Each bandit process is associated with a smart target, of which the estimation state evolves according to different discrete dynamic models for different actions - whether or not the target is being tracked.","The discrete state is defined by the dynamic mode.","The problem exhibits the curse of dimensionality, where optimal solutions are in general intractable.","We resort to heuristics through the famous restless multi-armed bandit techniques.","It follows with efficient scheduling policies based on the indices that are real numbers representing the marginal rewards of taking different actions.","For the inevitable practical case with unknown transition matrices, we propose a new method that utilizes the forward Sarsa and backward Q-learning to approximate the indices through adapting the state-action value functions, or equivalently the Q-functions, and propose a new policy, namely ISQ, aiming to maximize the long-term tracking rewards.","Numerical results demonstrate that the proposed ISQ policy outperforms conventional Q-learning-based methods and rapidly converges to the well-known Whittle index policy with revealed state transition models, which is considered the benchmark."],"url":"http://arxiv.org/abs/2402.12015v1","category":"eess.SY"}
{"created":"2024-02-19 10:02:10","title":"Moduli of Continuity in Metric Models and Extension of Liveability Indices","abstract":"Index spaces serve as valuable metric models for studying properties relevant to various applications, such as social science or economics. These properties are represented by real Lipschitz functions that describe the degree of association with each element within the underlying metric space. After determining the index value within a given sample subset, the classic McShane and Whitney formulas allow a Lipschitz regression procedure to be performed to extend the index values over the entire metric space. To improve the adaptability of the metric model to specific scenarios, this paper introduces the concept of a composition metric, which involves composing a metric with an increasing, positive and subadditive function $\\phi$. The results presented here extend well-established results for Lipschitz indices on metric spaces to composition metrics. In addition, we establish the corresponding approximation properties that facilitate the use of this functional structure. To illustrate the power and simplicity of this mathematical framework, we provide a concrete application involving the modelling of livability indices in North American cities.","sentences":["Index spaces serve as valuable metric models for studying properties relevant to various applications, such as social science or economics.","These properties are represented by real Lipschitz functions that describe the degree of association with each element within the underlying metric space.","After determining the index value within a given sample subset, the classic McShane and Whitney formulas allow a Lipschitz regression procedure to be performed to extend the index values over the entire metric space.","To improve the adaptability of the metric model to specific scenarios, this paper introduces the concept of a composition metric, which involves composing a metric with an increasing, positive and subadditive function $\\phi$. The results presented here extend well-established results for Lipschitz indices on metric spaces to composition metrics.","In addition, we establish the corresponding approximation properties that facilitate the use of this functional structure.","To illustrate the power and simplicity of this mathematical framework, we provide a concrete application involving the modelling of livability indices in North American cities."],"url":"http://arxiv.org/abs/2402.12009v1","category":"stat.ME"}
{"created":"2024-02-19 09:32:48","title":"Privacy-Preserving Low-Rank Adaptation for Latent Diffusion Models","abstract":"Low-rank adaptation (LoRA) is an efficient strategy for adapting latent diffusion models (LDMs) on a training dataset to generate specific objects by minimizing the adaptation loss. However, adapted LDMs via LoRA are vulnerable to membership inference (MI) attacks that can judge whether a particular data point belongs to private training datasets, thus facing severe risks of privacy leakage. To defend against MI attacks, we make the first effort to propose a straightforward solution: privacy-preserving LoRA (PrivateLoRA). PrivateLoRA is formulated as a min-max optimization problem where a proxy attack model is trained by maximizing its MI gain while the LDM is adapted by minimizing the sum of the adaptation loss and the proxy attack model's MI gain. However, we empirically disclose that PrivateLoRA has the issue of unstable optimization due to the large fluctuation of the gradient scale which impedes adaptation. To mitigate this issue, we propose Stable PrivateLoRA that adapts the LDM by minimizing the ratio of the adaptation loss to the MI gain, which implicitly rescales the gradient and thus stabilizes the optimization. Our comprehensive empirical results corroborate that adapted LDMs via Stable PrivateLoRA can effectively defend against MI attacks while generating high-quality images. Our code is available at https://github.com/WilliamLUO0/StablePrivateLoRA.","sentences":["Low-rank adaptation (LoRA) is an efficient strategy for adapting latent diffusion models (LDMs) on a training dataset to generate specific objects by minimizing the adaptation loss.","However, adapted LDMs via LoRA are vulnerable to membership inference (MI) attacks that can judge whether a particular data point belongs to private training datasets, thus facing severe risks of privacy leakage.","To defend against MI attacks, we make the first effort to propose a straightforward solution: privacy-preserving LoRA (PrivateLoRA).","PrivateLoRA is formulated as a min-max optimization problem where a proxy attack model is trained by maximizing its MI gain while the LDM is adapted by minimizing the sum of the adaptation loss and the proxy attack model's MI gain.","However, we empirically disclose that PrivateLoRA has the issue of unstable optimization due to the large fluctuation of the gradient scale which impedes adaptation.","To mitigate this issue, we propose Stable PrivateLoRA that adapts the LDM by minimizing the ratio of the adaptation loss to the MI gain, which implicitly rescales the gradient and thus stabilizes the optimization.","Our comprehensive empirical results corroborate that adapted LDMs via Stable PrivateLoRA can effectively defend against MI attacks while generating high-quality images.","Our code is available at https://github.com/WilliamLUO0/StablePrivateLoRA."],"url":"http://arxiv.org/abs/2402.11989v1","category":"cs.LG"}
{"created":"2024-02-19 09:26:22","title":"Buffered Streaming Edge Partitioning","abstract":"Addressing the challenges of processing massive graphs, which are prevalent in diverse fields such as social, biological, and technical networks, we introduce HeiStreamE and FreightE, two innovative (buffered) streaming algorithms designed for efficient edge partitioning of large-scale graphs. HeiStreamE utilizes an adapted Split-and-Connect graph model and a Fennel-based multilevel partitioning scheme, while FreightE partitions a hypergraph representation of the input graph. Besides ensuring superior solution quality, these approaches also overcome the limitations of existing algorithms by maintaining linear dependency on the graph size in both time and memory complexity with no dependence on the number of blocks of partition. Our comprehensive experimental analysis demonstrates that HeiStreamE outperforms current streaming algorithms and the re-streaming algorithm 2PS in partitioning quality (replication factor), and is more memory-efficient for real-world networks where the number of edges is far greater than the number of vertices. Further, FreightE is shown to produce fast and efficient partitions, particularly for higher numbers of partition blocks.","sentences":["Addressing the challenges of processing massive graphs, which are prevalent in diverse fields such as social, biological, and technical networks, we introduce HeiStreamE and FreightE, two innovative (buffered) streaming algorithms designed for efficient edge partitioning of large-scale graphs.","HeiStreamE utilizes an adapted Split-and-Connect graph model and a Fennel-based multilevel partitioning scheme, while FreightE partitions a hypergraph representation of the input graph.","Besides ensuring superior solution quality, these approaches also overcome the limitations of existing algorithms by maintaining linear dependency on the graph size in both time and memory complexity with no dependence on the number of blocks of partition.","Our comprehensive experimental analysis demonstrates that HeiStreamE outperforms current streaming algorithms and the re-streaming algorithm 2PS in partitioning quality (replication factor), and is more memory-efficient for real-world networks where the number of edges is far greater than the number of vertices.","Further, FreightE is shown to produce fast and efficient partitions, particularly for higher numbers of partition blocks."],"url":"http://arxiv.org/abs/2402.11980v1","category":"cs.DS"}
{"created":"2024-02-19 09:18:43","title":"Going Beyond Perfect Absorption: Reconfigurable Super-directive Absorbers","abstract":"In the context of electromagnetic absorption, it is obvious that for an infinite planar periodic structure illuminated by a plane wave, the maximum attainable absorptance, i.e., perfect absorption, is theoretically limited to 100% of the incident power. Here we show that an intriguing possibility of overcoming this limit arises in finite-size resonant absorbing arrays. We present a comprehensive analysis of a simple two-dimensional strip array over an infinite perfectly conducting plane, where the strips are loaded by reconfigurable impedance loads. The absorptance is defined as the ratio of the dissipated power per unit length of the strips to the incident power on the unit length of the array width. The results show that even regular arrays of impedance strips can slightly overcome the limit of 100% absorptance, while using aperiodic arrays with optimized loads, absorptance can be significantly increased as compared with the scenario where the strips are identical. In principle, by tuning the reconfigurable loads, high super-unity absorptance can be realized for all angles of illumination.","sentences":["In the context of electromagnetic absorption, it is obvious that for an infinite planar periodic structure illuminated by a plane wave, the maximum attainable absorptance, i.e., perfect absorption, is theoretically limited to 100% of the incident power.","Here we show that an intriguing possibility of overcoming this limit arises in finite-size resonant absorbing arrays.","We present a comprehensive analysis of a simple two-dimensional strip array over an infinite perfectly conducting plane, where the strips are loaded by reconfigurable impedance loads.","The absorptance is defined as the ratio of the dissipated power per unit length of the strips to the incident power on the unit length of the array width.","The results show that even regular arrays of impedance strips can slightly overcome the limit of 100% absorptance, while using aperiodic arrays with optimized loads, absorptance can be significantly increased as compared with the scenario where the strips are identical.","In principle, by tuning the reconfigurable loads, high super-unity absorptance can be realized for all angles of illumination."],"url":"http://arxiv.org/abs/2402.11971v1","category":"physics.app-ph"}
{"created":"2024-02-19 08:27:14","title":"CRAP Part II: Clutter Removal with Continuous Acquisitions Under Phase Noise","abstract":"The mitigation of clutter is an important research branch in Integrated Sensing and Communication (ISAC), one of the emerging technologies of future cellular networks. In this work, we extend our previously introduced method Clutter Removal with Acquisitions Under Phase Noise (CRAP) by means to track clutter over time. This is necessary in scenarios that require high reliability but can change dynamically, like safety applications in factory floors. To that end, exponential smoothing is leveraged to process new measurements and previous clutter information in a unique matrix using the singular value decomposition, allowing adaptation to changing environments in an efficient way.We further propose a singular value threshold based on the Marchenko-Pastur distribution to select the meaningful clutter components. Results from both simulations and measurements show that continuously updating the clutter components with new acquisitions according to our proposed algorithm Smoothed CRAP (SCRAP) enables coping with dynamic clutter environments and facilitates the detection of sensing targets.","sentences":["The mitigation of clutter is an important research branch in Integrated Sensing and Communication (ISAC), one of the emerging technologies of future cellular networks.","In this work, we extend our previously introduced method Clutter Removal with Acquisitions Under Phase Noise (CRAP) by means to track clutter over time.","This is necessary in scenarios that require high reliability but can change dynamically, like safety applications in factory floors.","To that end, exponential smoothing is leveraged to process new measurements and previous clutter information in a unique matrix using the singular value decomposition, allowing adaptation to changing environments in an efficient way.","We further propose a singular value threshold based on the Marchenko-Pastur distribution to select the meaningful clutter components.","Results from both simulations and measurements show that continuously updating the clutter components with new acquisitions according to our proposed algorithm Smoothed CRAP (SCRAP) enables coping with dynamic clutter environments and facilitates the detection of sensing targets."],"url":"http://arxiv.org/abs/2402.11939v1","category":"eess.SP"}
{"created":"2024-02-19 08:22:51","title":"Team QUST at SemEval-2024 Task 8: A Comprehensive Study of Monolingual and Multilingual Approaches for Detecting AI-generated Text","abstract":"This paper presents the participation of team QUST in Task 8 SemEval 2024. We first performed data augmentation and cleaning on the dataset to enhance model training efficiency and accuracy. In the monolingual task, we evaluated traditional deep-learning methods, multiscale positive-unlabeled framework (MPU), fine-tuning, adapters and ensemble methods. Then, we selected the top-performing models based on their accuracy from the monolingual models and evaluated them in subtasks A and B. The final model construction employed a stacking ensemble that combined fine-tuning with MPU. Our system achieved 8th (scored 8th in terms of accuracy, officially ranked 13th) place in the official test set in multilingual settings of subtask A. We release our system code at:https://github.com/warmth27/SemEval2024_QUST","sentences":["This paper presents the participation of team QUST in Task 8 SemEval 2024.","We first performed data augmentation and cleaning on the dataset to enhance model training efficiency and accuracy.","In the monolingual task, we evaluated traditional deep-learning methods, multiscale positive-unlabeled framework (MPU), fine-tuning, adapters and ensemble methods.","Then, we selected the top-performing models based on their accuracy from the monolingual models and evaluated them in subtasks A and B.","The final model construction employed a stacking ensemble that combined fine-tuning with MPU.","Our system achieved 8th (scored 8th in terms of accuracy, officially ranked 13th) place in the official test set in multilingual settings of subtask A.","We release our system code at:https://github.com/warmth27/SemEval2024_QUST"],"url":"http://arxiv.org/abs/2402.11934v1","category":"cs.CL"}
{"created":"2024-02-19 08:19:26","title":"SLADE: Detecting Dynamic Anomalies in Edge Streams without Labels via Self-Supervised Learning","abstract":"To detect anomalies in real-world graphs, such as social, email, and financial networks, various approaches have been developed. While they typically assume static input graphs, most real-world graphs grow over time, naturally represented as edge streams. In this context, we aim to achieve three goals: (a) instantly detecting anomalies as they occur, (b) adapting to dynamically changing states, and (c) handling the scarcity of dynamic anomaly labels. In this paper, we propose SLADE (Self-supervised Learning for Anomaly Detection in Edge Streams) for rapid detection of dynamic anomalies in edge streams, without relying on labels. SLADE detects the shifts of nodes into abnormal states by observing deviations in their interaction patterns over time. To this end, it trains a deep neural network to perform two self-supervised tasks: (a) minimizing drift in node representations and (b) generating long-term interaction patterns from short-term ones. Failure in these tasks for a node signals its deviation from the norm. Notably, the neural network and tasks are carefully designed so that all required operations can be performed in constant time (w.r.t. the graph size) in response to each new edge in the input stream. In dynamic anomaly detection across four real-world datasets, SLADE outperforms nine competing methods, even those leveraging label supervision.","sentences":["To detect anomalies in real-world graphs, such as social, email, and financial networks, various approaches have been developed.","While they typically assume static input graphs, most real-world graphs grow over time, naturally represented as edge streams.","In this context, we aim to achieve three goals: (a) instantly detecting anomalies as they occur, (b) adapting to dynamically changing states, and (c) handling the scarcity of dynamic anomaly labels.","In this paper, we propose SLADE (Self-supervised Learning for Anomaly Detection in Edge Streams) for rapid detection of dynamic anomalies in edge streams, without relying on labels.","SLADE detects the shifts of nodes into abnormal states by observing deviations in their interaction patterns over time.","To this end, it trains a deep neural network to perform two self-supervised tasks: (a) minimizing drift in node representations and (b) generating long-term interaction patterns from short-term ones.","Failure in these tasks for a node signals its deviation from the norm.","Notably, the neural network and tasks are carefully designed so that all required operations can be performed in constant time (w.r.t.","the graph size) in response to each new edge in the input stream.","In dynamic anomaly detection across four real-world datasets, SLADE outperforms nine competing methods, even those leveraging label supervision."],"url":"http://arxiv.org/abs/2402.11933v1","category":"cs.LG"}
{"created":"2024-02-21 18:09:12","title":"Geometry-induced wavefunction collapse","abstract":"When a quantum particle moves in a curved space, a geometric potential can arise. In spite of a long history of extensive theoretical studies, to experimentally observe the geometric potential remains to be a challenge. What are the physically observable consequences of such a geometric potential? Solving the Schrodinger equation on a truncated conic surface, we uncover a class of quantum scattering states that bear a strong resemblance with the quasi-resonant states associated with atomic collapse about a Coulomb impurity, a remarkable quantum phenomenon in which an infinite number of quasi-resonant states emerge. A characteristic defining feature of such collapse states is the infinite oscillations of the local density of states (LDOS) about the zero energy point separating the scattering from the bound states. The emergence of such states in the curved (Riemannian) space requires neither a relativistic quantum mechanism nor any Coulomb impurity: they have zero angular momentum and their origin is purely geometrical - henceforth the term geometry-induced wavefunction collapse. We establish the collapsing nature of these states through a detailed comparative analysis of the behavior of the LDOS for both the zero and finite angular-momentum states as well as the corresponding classical picture. Potential experimental schemes to realize the geometry-induced collapse states are articulated. Not only has our study uncovered an intrinsic connection between the geometric potential and atomic collapse, it also provides a method to experimentally observe and characterize geometric potentials arising from different subfields of physics. For example, in nanoscience and nanotechnology, curved geometry has become increasingly common. Our finding suggests that wavefunction collapse should be an important factor of consideration in designing and developing nanodevices.","sentences":["When a quantum particle moves in a curved space, a geometric potential can arise.","In spite of a long history of extensive theoretical studies, to experimentally observe the geometric potential remains to be a challenge.","What are the physically observable consequences of such a geometric potential?","Solving the Schrodinger equation on a truncated conic surface, we uncover a class of quantum scattering states that bear a strong resemblance with the quasi-resonant states associated with atomic collapse about a Coulomb impurity, a remarkable quantum phenomenon in which an infinite number of quasi-resonant states emerge.","A characteristic defining feature of such collapse states is the infinite oscillations of the local density of states (LDOS) about the zero energy point separating the scattering from the bound states.","The emergence of such states in the curved (Riemannian) space requires neither a relativistic quantum mechanism nor any Coulomb impurity: they have zero angular momentum and their origin is purely geometrical - henceforth the term geometry-induced wavefunction collapse.","We establish the collapsing nature of these states through a detailed comparative analysis of the behavior of the LDOS for both the zero and finite angular-momentum states as well as the corresponding classical picture.","Potential experimental schemes to realize the geometry-induced collapse states are articulated.","Not only has our study uncovered an intrinsic connection between the geometric potential and atomic collapse, it also provides a method to experimentally observe and characterize geometric potentials arising from different subfields of physics.","For example, in nanoscience and nanotechnology, curved geometry has become increasingly common.","Our finding suggests that wavefunction collapse should be an important factor of consideration in designing and developing nanodevices."],"url":"http://arxiv.org/abs/2402.13980v1","category":"quant-ph"}
{"created":"2024-02-21 17:57:40","title":"Multi-indice B-series","abstract":"We propose a novel way to describe numerical methods for ordinary differential equations via the notion of multi-indice. The main idea is to replace rooted trees in Butcher's B-series by multi-indices. The latter were introduced recently in the context of describing solutions of singular stochastic partial differential equations. The combinatorial shift away from rooted trees allows for a compressed description of numerical schemes. Moreover, these multi-indices B-series characterise uniquely the Taylor development of local and affine equivariant maps.","sentences":["We propose a novel way to describe numerical methods for ordinary differential equations via the notion of multi-indice.","The main idea is to replace rooted trees in Butcher's B-series by multi-indices.","The latter were introduced recently in the context of describing solutions of singular stochastic partial differential equations.","The combinatorial shift away from rooted trees allows for a compressed description of numerical schemes.","Moreover, these multi-indices B-series characterise uniquely the Taylor development of local and affine equivariant maps."],"url":"http://arxiv.org/abs/2402.13971v1","category":"math.NA"}
{"created":"2024-02-21 17:05:27","title":"Verifying message-passing neural networks via topology-based bounds tightening","abstract":"Since graph neural networks (GNNs) are often vulnerable to attack, we need to know when we can trust them. We develop a computationally effective approach towards providing robust certificates for message-passing neural networks (MPNNs) using a Rectified Linear Unit (ReLU) activation function. Because our work builds on mixed-integer optimization, it encodes a wide variety of subproblems, for example it admits (i) both adding and removing edges, (ii) both global and local budgets, and (iii) both topological perturbations and feature modifications. Our key technology, topology-based bounds tightening, uses graph structure to tighten bounds. We also experiment with aggressive bounds tightening to dynamically change the optimization constraints by tightening variable bounds. To demonstrate the effectiveness of these strategies, we implement an extension to the open-source branch-and-cut solver SCIP. We test on both node and graph classification problems and consider topological attacks that both add and remove edges.","sentences":["Since graph neural networks (GNNs) are often vulnerable to attack, we need to know when we can trust them.","We develop a computationally effective approach towards providing robust certificates for message-passing neural networks (MPNNs) using a Rectified Linear Unit (ReLU) activation function.","Because our work builds on mixed-integer optimization, it encodes a wide variety of subproblems, for example it admits (i) both adding and removing edges, (ii) both global and local budgets, and (iii) both topological perturbations and feature modifications.","Our key technology, topology-based bounds tightening, uses graph structure to tighten bounds.","We also experiment with aggressive bounds tightening to dynamically change the optimization constraints by tightening variable bounds.","To demonstrate the effectiveness of these strategies, we implement an extension to the open-source branch-and-cut solver SCIP.","We test on both node and graph classification problems and consider topological attacks that both add and remove edges."],"url":"http://arxiv.org/abs/2402.13937v1","category":"math.OC"}
{"created":"2024-02-21 16:05:26","title":"Partial convex hulls of coadjoint orbits and degrees of invariants","abstract":"We study properties of convex hulls of (co)adjoint orbits of compact groups, with applications to invariant theory and tensor product decompositions. The notion of partial convex hulls is introduced and applied to define two numerical invariants of a coadjoint orbit of a semisimple connected compact Lie group. It is shown that the orbits, where any one of these invariants does not exceed a given number $r$, form, upon intersection with a fixed Weyl chamber, a rational convex polyhedral cone in that chamber, related to the Littlewood-Richardson cone of the $r$-fold diagonal embedding of $K$. The numerical invariants are shown to provide lower bounds for degrees of invariant polynomials on irreducible unitary representations.","sentences":["We study properties of convex hulls of (co)adjoint orbits of compact groups, with applications to invariant theory and tensor product decompositions.","The notion of partial convex hulls is introduced and applied to define two numerical invariants of a coadjoint orbit of a semisimple connected compact Lie group.","It is shown that the orbits, where any one of these invariants does not exceed a given number $r$, form, upon intersection with a fixed Weyl chamber, a rational convex polyhedral cone in that chamber, related to the Littlewood-Richardson cone of the $r$-fold diagonal embedding of $K$. The numerical invariants are shown to provide lower bounds for degrees of invariant polynomials on irreducible unitary representations."],"url":"http://arxiv.org/abs/2402.13893v1","category":"math.RT"}
{"created":"2024-02-21 14:44:16","title":"Coupled coherent states method for tunneling dynamics: an interpretative study","abstract":"Numerical solutions of the time-dependent Schr\\\"odinger equation based on the variational principle may offer physical insight that cannot be gained by a solution using fixed grids in position and momentum space. Here we focus on the tunneling dynamics in a quartic double-well and the use of classical, trajectory-guided coherent states to gain insight into the workings of the coupled coherent states method developed by Shalashilin and Child [J. Chem. Phys. {\\bf 113}, 10028 (2000)]. It is shown that over-the-barrier classical trajectories, alone, can accurately describe the tunneling effect.","sentences":["Numerical solutions of the time-dependent Schr\\\"odinger equation based on the variational principle may offer physical insight that cannot be gained by a solution using fixed grids in position and momentum space.","Here we focus on the tunneling dynamics in a quartic double-well and the use of classical, trajectory-guided coherent states to gain insight into the workings of the coupled coherent states method developed by Shalashilin and Child [J. Chem.","Phys. {\\bf 113}, 10028 (2000)].","It is shown that over-the-barrier classical trajectories, alone, can accurately describe the tunneling effect."],"url":"http://arxiv.org/abs/2402.13847v1","category":"quant-ph"}
{"created":"2024-02-21 14:23:41","title":"High-resolution spectroscopy of the intermediate polar EX Hydrae: II. The inner disk radius","abstract":"EX Hya is one of the best studied, but still enigmatic intermediate polars. We present phase-resolved blue VLT/UVES high-resolution ($\\lambda/\\Delta \\lambda\\simeq16.000$) spectra of EX Hya taken in January 2004. Our analysis involves a unique decomposition of the Balmer line profiles into the spin-modulated line wings that represent streaming motions in the magnetosphere and the orbital-phase modulated line core that represents the accretion disk. Spectral analysis and tomography show that the division line between the two is solidly located at $\\mid\\upsilon_{rad}\\mid\\simeq1200$ km s$^{-1}$, defining the inner edge of the accretion disk at $r_{in}\\simeq{7}\\times 10^{9}$ cm or $\\sim10 R_1$ (WD radii). This large central hole allows an unimpeded view of the tall accretion curtain at the lower pole with a shock height up to $h_{sh}\\sim1 R_1$ that is required by X-ray and optical observations. Our results contradict models that advocate a small magnetosphere and a small inner disk hole. Equating $r_{in}$ with the magnetospheric radius in the orbital plane allows us to derive a magnetic moment of the WD of $\\mu_1\\simeq1.3\\times 10^{32}$ G cm$^{3}$ and a surface field strength $B_1\\sim0.35$ MG. Given a polar field strength $B_{p} \\lesssim 1.0$ MG, optical circular polarization is not expected. With an accretion rate $\\dot M = 3.9\\times10^{-11}$ $M_{\\odot}$yr$^{-1}$, the accretion torque is $G_{acc}\\simeq 2.2 \\times 10^{33}$ g cm$^{2}$s$^{-2}$. The magnetostatic torque is of similar magnitude, suggesting that EX Hya is not far from being synchronized. We measured the orbital radial-velocity amplitude of the WD, $K_1=58.7\\pm3.9$ km s$^{-1}$, and found a spin-dependent velocity modulation as well. The former is in perfect agreement with the mean velocity amplitude obtained by other researchers, confirming the published component masses $M_1\\simeq0.79 M_\\odot$ and $M_2\\simeq0.11 M_\\odot$.","sentences":["EX Hya is one of the best studied, but still enigmatic intermediate polars.","We present phase-resolved blue VLT/UVES high-resolution ($\\lambda/\\Delta \\lambda\\simeq16.000$) spectra of EX Hya taken in January 2004.","Our analysis involves a unique decomposition of the Balmer line profiles into the spin-modulated line wings that represent streaming motions in the magnetosphere and the orbital-phase modulated line core that represents the accretion disk.","Spectral analysis and tomography show that the division line between the two is solidly located at $\\mid\\upsilon_{rad}\\mid\\simeq1200$ km s$^{-1}$, defining the inner edge of the accretion disk at $r_{in}\\simeq{7}\\times 10^{9}$ cm or $\\sim10 R_1$ (WD radii).","This large central hole allows an unimpeded view of the tall accretion curtain at the lower pole with a shock height up to $h_{sh}\\sim1 R_1$ that is required by X-ray and optical observations.","Our results contradict models that advocate a small magnetosphere and a small inner disk hole.","Equating $r_{in}$ with the magnetospheric radius in the orbital plane allows us to derive a magnetic moment of the WD of $\\mu_1\\simeq1.3\\times 10^{32}$ G cm$^{3}$ and a surface field strength $B_1\\sim0.35$ MG.","Given a polar field strength $B_{p} \\lesssim 1.0$ MG, optical circular polarization is not expected.","With an accretion rate $\\dot M = 3.9\\times10^{-11}$ $M_{\\odot}$yr$^{-1}$, the accretion torque is $G_{acc}\\simeq 2.2 \\times 10^{33}$ g cm$^{2}$s$^{-2}$.","The magnetostatic torque is of similar magnitude, suggesting that EX Hya is not far from being synchronized.","We measured the orbital radial-velocity amplitude of the WD, $K_1=58.7\\pm3.9$ km s$^{-1}$, and found a spin-dependent velocity modulation as well.","The former is in perfect agreement with the mean velocity amplitude obtained by other researchers, confirming the published component masses $M_1\\simeq0.79 M_\\odot$ and $M_2\\simeq0.11 M_\\odot$."],"url":"http://arxiv.org/abs/2402.13834v1","category":"astro-ph.SR"}
{"created":"2024-02-21 14:16:49","title":"Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering of 3D Gaussian Splatting","abstract":"3D Gaussian splatting (3D-GS) is a new rendering approach that outperforms the neural radiance field (NeRF) in terms of both speed and image quality. 3D-GS represents 3D scenes by utilizing millions of 3D Gaussians and projects these Gaussians onto the 2D image plane for rendering. However, during the rendering process, a substantial number of unnecessary 3D Gaussians exist for the current view direction, resulting in significant computation costs associated with their identification. In this paper, we propose a computational reduction technique that quickly identifies unnecessary 3D Gaussians in real-time for rendering the current view without compromising image quality. This is accomplished through the offline clustering of 3D Gaussians that are close in distance, followed by the projection of these clusters onto a 2D image plane during runtime. Additionally, we analyze the bottleneck associated with the proposed technique when executed on GPUs and propose an efficient hardware architecture that seamlessly supports the proposed scheme. For the Mip-NeRF360 dataset, the proposed technique excludes 63% of 3D Gaussians on average before the 2D image projection, which reduces the overall rendering computation by almost 38.3% without sacrificing peak-signal-to-noise-ratio (PSNR). The proposed accelerator also achieves a speedup of 10.7x compared to a GPU.","sentences":["3D Gaussian splatting (3D-GS) is a new rendering approach that outperforms the neural radiance field (NeRF) in terms of both speed and image quality.","3D-GS represents 3D scenes by utilizing millions of 3D Gaussians and projects these Gaussians onto the 2D image plane for rendering.","However, during the rendering process, a substantial number of unnecessary 3D Gaussians exist for the current view direction, resulting in significant computation costs associated with their identification.","In this paper, we propose a computational reduction technique that quickly identifies unnecessary 3D Gaussians in real-time for rendering the current view without compromising image quality.","This is accomplished through the offline clustering of 3D Gaussians that are close in distance, followed by the projection of these clusters onto a 2D image plane during runtime.","Additionally, we analyze the bottleneck associated with the proposed technique when executed on GPUs and propose an efficient hardware architecture that seamlessly supports the proposed scheme.","For the Mip-NeRF360 dataset, the proposed technique excludes 63% of 3D Gaussians on average before the 2D image projection, which reduces the overall rendering computation by almost 38.3% without sacrificing peak-signal-to-noise-ratio (PSNR).","The proposed accelerator also achieves a speedup of 10.7x compared to a GPU."],"url":"http://arxiv.org/abs/2402.13827v1","category":"cs.CV"}
{"created":"2024-02-21 14:13:09","title":"Linearity of the co-moving velocity","abstract":"The co-moving velocity is a new variable in the description of immiscible two-phase flow in porous media. It is the saturation-weighted derivatives of the seepage velocities of the two immiscible fluids with respect to saturation. I show that it is linear in the derivative of the average seepage velocity with respect to the saturation. This is in accordance with recent measurements. Two parameters are needed to describe it. In terms of relative permeability theory, this linear relation leads to a differential equation relating the two relative permeabilities describing the flow. I present this equation together with a solution.","sentences":["The co-moving velocity is a new variable in the description of immiscible two-phase flow in porous media.","It is the saturation-weighted derivatives of the seepage velocities of the two immiscible fluids with respect to saturation.","I show that it is linear in the derivative of the average seepage velocity with respect to the saturation.","This is in accordance with recent measurements.","Two parameters are needed to describe it.","In terms of relative permeability theory, this linear relation leads to a differential equation relating the two relative permeabilities describing the flow.","I present this equation together with a solution."],"url":"http://arxiv.org/abs/2402.13826v1","category":"physics.flu-dyn"}
{"created":"2024-02-21 13:58:27","title":"Dupin cyclides passing through a fixed circle","abstract":"We derive algebraic equations on the coefficients of the implicit equation to characterize all Dupin cyclides passing through a fixed circle. The results are applied to solve the basic problems in CAGD about blending of Dupin cyclides along circles.","sentences":["We derive algebraic equations on the coefficients of the implicit equation to characterize all Dupin cyclides passing through a fixed circle.","The results are applied to solve the basic problems in CAGD about blending of Dupin cyclides along circles."],"url":"http://arxiv.org/abs/2402.13819v1","category":"math.AG"}
{"created":"2024-02-21 12:14:39","title":"Non-unique Ergodicity for the 2D Stochastic Navier-Stokes Equations with Derivative of Space-Time White Noise","abstract":"We prove existence of infinitely many stationary solutions as well as ergodic stationary solutions for the stochastic Navier-Stokes equations on $\\mathbb{T}^2$ \\begin{align*} \\dif u+\\div(u\\otimes u)\\dif t+\\nabla p\\dif t&=\\Delta u\\dif t + (-\\Delta)^{\\fa/2}\\dif B_t,\\ \\ \\ \\ \\div u=0,\\notag \\end{align*} driven by derivative of space-time white noise, where $\\fa\\in[0,\\frac13)$. In this setting, the solutions are not function valued and probabilistic renormalization is required to give a meaning to the equations. Finally, we show that the stationary distributions are not Gaussian distribution $N(0,\\frac12(-\\Delta)^{\\fa-1})$. The proof relies on a time-dependent decomposition and a stochastic version of the convex integration method which provides uniform moment bounds in some function spaces.","sentences":["We prove existence of infinitely many stationary solutions as well as ergodic stationary solutions for the stochastic Navier-Stokes equations on $\\mathbb{T}^2$ \\begin{align*} \\dif u+\\div(u\\otimes u)\\dif t+\\nabla p\\dif t&=\\Delta","u\\dif t + (-\\Delta)^{\\fa/2}\\dif B_t,\\ \\ \\ \\ \\div u=0,\\notag \\end{align*} driven by derivative of space-time white noise, where $\\fa\\in[0,\\frac13)$. In this setting, the solutions are not function valued and probabilistic renormalization is required to give a meaning to the equations.","Finally, we show that the stationary distributions are not Gaussian distribution $N(0,\\frac12(-\\Delta)^{\\fa-1})$.","The proof relies on a time-dependent decomposition and a stochastic version of the convex integration method which provides uniform moment bounds in some function spaces."],"url":"http://arxiv.org/abs/2402.13743v1","category":"math.PR"}
{"created":"2024-02-21 11:40:27","title":"Average gradient outer product as a mechanism for deep neural collapse","abstract":"Deep Neural Collapse (DNC) refers to the surprisingly rigid structure of the data representations in the final layers of Deep Neural Networks (DNNs). Though the phenomenon has been measured in a wide variety of settings, its emergence is only partially understood. In this work, we provide substantial evidence that DNC formation occurs primarily through deep feature learning with the average gradient outer product (AGOP). This takes a step further compared to efforts that explain neural collapse via feature-agnostic approaches, such as the unconstrained features model. We proceed by providing evidence that the right singular vectors and values of the weights are responsible for the majority of within-class variability collapse in DNNs. As shown in recent work, this singular structure is highly correlated with that of the AGOP. We then establish experimentally and theoretically that AGOP induces neural collapse in a randomly initialized neural network. In particular, we demonstrate that Deep Recursive Feature Machines, a method originally introduced as an abstraction for AGOP feature learning in convolutional neural networks, exhibits DNC.","sentences":["Deep Neural Collapse (DNC) refers to the surprisingly rigid structure of the data representations in the final layers of Deep Neural Networks (DNNs).","Though the phenomenon has been measured in a wide variety of settings, its emergence is only partially understood.","In this work, we provide substantial evidence that DNC formation occurs primarily through deep feature learning with the average gradient outer product (AGOP).","This takes a step further compared to efforts that explain neural collapse via feature-agnostic approaches, such as the unconstrained features model.","We proceed by providing evidence that the right singular vectors and values of the weights are responsible for the majority of within-class variability collapse in DNNs.","As shown in recent work, this singular structure is highly correlated with that of the AGOP.","We then establish experimentally and theoretically that AGOP induces neural collapse in a randomly initialized neural network.","In particular, we demonstrate that Deep Recursive Feature Machines, a method originally introduced as an abstraction for AGOP feature learning in convolutional neural networks, exhibits DNC."],"url":"http://arxiv.org/abs/2402.13728v1","category":"cs.LG"}
{"created":"2024-02-21 11:39:53","title":"Variable Mass and the Noisy Feynman Propagator in Scalar Fields","abstract":"We utilize a mass independent Klein-Gordon equation that is first order in a variable that plays the role of time, the approach taken in parametric time formulations. Using concepts from semigroup evolution, we examine the sign of a noisy Feynman propagator in a quantum field theory, namely, scalar electrodynamics.","sentences":["We utilize a mass independent Klein-Gordon equation that is first order in a variable that plays the role of time, the approach taken in parametric time formulations.","Using concepts from semigroup evolution, we examine the sign of a noisy Feynman propagator in a quantum field theory, namely, scalar electrodynamics."],"url":"http://arxiv.org/abs/2402.13727v1","category":"quant-ph"}
{"created":"2024-02-21 11:35:45","title":"Sparse and Structured Hopfield Networks","abstract":"Modern Hopfield networks have enjoyed recent interest due to their connection to attention in transformers. Our paper provides a unified framework for sparse Hopfield networks by establishing a link with Fenchel-Young losses. The result is a new family of Hopfield-Fenchel-Young energies whose update rules are end-to-end differentiable sparse transformations. We reveal a connection between loss margins, sparsity, and exact memory retrieval. We further extend this framework to structured Hopfield networks via the SparseMAP transformation, which can retrieve pattern associations instead of a single pattern. Experiments on multiple instance learning and text rationalization demonstrate the usefulness of our approach.","sentences":["Modern Hopfield networks have enjoyed recent interest due to their connection to attention in transformers.","Our paper provides a unified framework for sparse Hopfield networks by establishing a link with Fenchel-Young losses.","The result is a new family of Hopfield-Fenchel-Young energies whose update rules are end-to-end differentiable sparse transformations.","We reveal a connection between loss margins, sparsity, and exact memory retrieval.","We further extend this framework to structured Hopfield networks via the SparseMAP transformation, which can retrieve pattern associations instead of a single pattern.","Experiments on multiple instance learning and text rationalization demonstrate the usefulness of our approach."],"url":"http://arxiv.org/abs/2402.13725v1","category":"cs.LG"}
{"created":"2024-02-21 10:54:18","title":"Computational unique continuation with finite dimensional Neumann trace","abstract":"We consider finite element approximations of unique continuation problems subject to elliptic equations in the case where the normal derivative of the exact solution is known to reside in some finite dimensional space. To give quantitative error estimates we prove Lipschitz stability of the unique continuation problem in the global H1-norm. This stability is then leveraged to derive optimal a posteriori and a priori error estimates for a primal-dual stabilised finite method.","sentences":["We consider finite element approximations of unique continuation problems subject to elliptic equations in the case where the normal derivative of the exact solution is known to reside in some finite dimensional space.","To give quantitative error estimates we prove Lipschitz stability of the unique continuation problem in the global H1-norm.","This stability is then leveraged to derive optimal a posteriori and a priori error estimates for a primal-dual stabilised finite method."],"url":"http://arxiv.org/abs/2402.13695v1","category":"math.NA"}
{"created":"2024-02-21 10:22:08","title":"An Edge-based Interface Tracking (EBIT) Method for Multiphase Flows with Phase Change","abstract":"In this paper, the Edge-Based Interface Tracking (EBIT) method is extended to simulate multiphase flows with phase change. As a novel Front-Tracking method, the EBIT method binds interfacial markers to the Eulerian grid to achieve automatic parallelization. To include phase change effects, the energy equation for each phase is solved, with the temperature boundary condition at the interface sharply imposed. When using collocated grids, the cell-centered velocity is approximately projected. This will lead to unphysical oscillations in the presence of phase change, as the velocity will be discontinuous across the interface. It is demonstrated that this issue can be addressed by using the ghost fluid method, in which the ghost velocity is set according to the jump condition, thereby removing the discontinuity. A series of benchmark tests are performed to validate the present method. It is shown that the numerical results agree very well with the theoretical solutions and the experimental results.","sentences":["In this paper, the Edge-Based Interface Tracking (EBIT) method is extended to simulate multiphase flows with phase change.","As a novel Front-Tracking method, the EBIT method binds interfacial markers to the Eulerian grid to achieve automatic parallelization.","To include phase change effects, the energy equation for each phase is solved, with the temperature boundary condition at the interface sharply imposed.","When using collocated grids, the cell-centered velocity is approximately projected.","This will lead to unphysical oscillations in the presence of phase change, as the velocity will be discontinuous across the interface.","It is demonstrated that this issue can be addressed by using the ghost fluid method, in which the ghost velocity is set according to the jump condition, thereby removing the discontinuity.","A series of benchmark tests are performed to validate the present method.","It is shown that the numerical results agree very well with the theoretical solutions and the experimental results."],"url":"http://arxiv.org/abs/2402.13677v1","category":"physics.flu-dyn"}
{"created":"2024-02-21 09:53:45","title":"Quantum Monte Carlo and perturbative study of repulsive two-dimensional Bose-Fermi mixtures","abstract":"We derive analytically the leading beyond-mean field contributions to the zero-temperature equation of state and to the fermionic quasi-particle residue and effective mass of a dilute Bose-Fermi mixture in two dimensions. In the repulsive case, we perform quantum Monte Carlo simulations for two representative bosonic concentrations and equal masses, extending a method for correcting finite-size effects in fermionic gases to Bose-Fermi mixtures. We find good agreement between analytic expressions and numerical results for weak interactions, while significant discrepancies appear in the regime close to mechanical instability, above which we provide evidence of phase separation of the bosonic component.","sentences":["We derive analytically the leading beyond-mean field contributions to the zero-temperature equation of state and to the fermionic quasi-particle residue and effective mass of a dilute Bose-Fermi mixture in two dimensions.","In the repulsive case, we perform quantum Monte Carlo simulations for two representative bosonic concentrations and equal masses, extending a method for correcting finite-size effects in fermionic gases to Bose-Fermi mixtures.","We find good agreement between analytic expressions and numerical results for weak interactions, while significant discrepancies appear in the regime close to mechanical instability, above which we provide evidence of phase separation of the bosonic component."],"url":"http://arxiv.org/abs/2402.13664v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-21 09:52:35","title":"Continuum limit of the discrete nonlinear Klein-Gordon equation","abstract":"We study the convergence of solutions of the discrete nonlinear Klein-Gordon equation on an infinite lattice in the continuum limit, using recent tools developed in the context of nonlinear discrete dispersive equations. Our approach relies in particular on the use of bilinear estimates of the Shannon interpolation alongside controls on the growth of discrete Sobolev norms of the solution. We conclude by giving perspectives on uniform dispersive estimates for nonlinear waves on lattices.","sentences":["We study the convergence of solutions of the discrete nonlinear Klein-Gordon equation on an infinite lattice in the continuum limit, using recent tools developed in the context of nonlinear discrete dispersive equations.","Our approach relies in particular on the use of bilinear estimates of the Shannon interpolation alongside controls on the growth of discrete Sobolev norms of the solution.","We conclude by giving perspectives on uniform dispersive estimates for nonlinear waves on lattices."],"url":"http://arxiv.org/abs/2402.13663v1","category":"math.AP"}
{"created":"2024-02-21 09:07:07","title":"Delving into Dark Regions for Robust Shadow Detection","abstract":"Shadow detection is a challenging task as it requires a comprehensive understanding of shadow characteristics and global/local illumination conditions. We observe from our experiment that state-of-the-art deep methods tend to have higher error rates in differentiating shadow pixels from non-shadow pixels in dark regions (ie, regions with low-intensity values). Our key insight to this problem is that existing methods typically learn discriminative shadow features from the whole image globally, covering the full range of intensity values, and may not learn the subtle differences between shadow and non-shadow pixels in dark regions. Hence, if we can design a model to focus on a narrower range of low-intensity regions, it may be able to learn better discriminative features for shadow detection. Inspired by this insight, we propose a novel shadow detection approach that first learns global contextual cues over the entire image and then zooms into the dark regions to learn local shadow representations. To this end, we formulate an effective dark-region recommendation (DRR) module to recommend regions of low-intensity values, and a novel dark-aware shadow analysis (DASA) module to learn dark-aware shadow features from the recommended dark regions. Extensive experiments show that the proposed method outperforms the state-of-the-art methods on three popular shadow detection datasets. Code is available at https://github.com/guanhuankang/ShadowDetection2021.git.","sentences":["Shadow detection is a challenging task as it requires a comprehensive understanding of shadow characteristics and global/local illumination conditions.","We observe from our experiment that state-of-the-art deep methods tend to have higher error rates in differentiating shadow pixels from non-shadow pixels in dark regions (ie, regions with low-intensity values).","Our key insight to this problem is that existing methods typically learn discriminative shadow features from the whole image globally, covering the full range of intensity values, and may not learn the subtle differences between shadow and non-shadow pixels in dark regions.","Hence, if we can design a model to focus on a narrower range of low-intensity regions, it may be able to learn better discriminative features for shadow detection.","Inspired by this insight, we propose a novel shadow detection approach that first learns global contextual cues over the entire image and then zooms into the dark regions to learn local shadow representations.","To this end, we formulate an effective dark-region recommendation (DRR) module to recommend regions of low-intensity values, and a novel dark-aware shadow analysis (DASA) module to learn dark-aware shadow features from the recommended dark regions.","Extensive experiments show that the proposed method outperforms the state-of-the-art methods on three popular shadow detection datasets.","Code is available at https://github.com/guanhuankang/ShadowDetection2021.git."],"url":"http://arxiv.org/abs/2402.13631v1","category":"cs.CV"}
{"created":"2024-02-21 08:46:09","title":"A Riemann-Hilbert approach to the two-component modified Camassa-Holm equation on the line","abstract":"In this paper, we develop a Riemann-Hilbert approach to the Cauchy problem for the two-component modified Camassa-Holm (2-mCH) equation based on its Lax pair. Further via a series of deformations to the Riemann-Hilbert problem associated with the Cauchy problem by using the $\\bar{\\partial}$-generalization of Deift-Zhou steepest descent method, we obtain the long-time asymptotic approximations of the solutions of the 2-mCH equation in four space-time regions. Our results also confirm the soliton resolution conjecture and asymptotically stability of the $N$-soliton solutions for the 2-mCH equation.","sentences":["In this paper, we develop a Riemann-Hilbert approach to the Cauchy problem for the two-component modified Camassa-Holm (2-mCH) equation based on its Lax pair.","Further via a series of deformations to the Riemann-Hilbert problem associated with the Cauchy problem by using the $\\bar{\\partial}$-generalization of Deift-Zhou steepest descent method, we obtain the long-time asymptotic approximations of the solutions of the 2-mCH equation in four space-time regions.","Our results also confirm the soliton resolution conjecture and asymptotically stability of the $N$-soliton solutions for the 2-mCH equation."],"url":"http://arxiv.org/abs/2402.13620v1","category":"math-ph"}
{"created":"2024-02-21 05:32:33","title":"The space-time estimates for the Schr\u00f6dinger equation","abstract":"In this paper, we studied the space-time estimates for the solution to the Schr\\\"odinger equation. By polynomial partitioning, induction arguments, bilinear to linear arguments and broad norm estimates, we set up several maximal estimates for the Schr\\\"odinger equation with high-frequency input data. By these maximal estimates, we obtain the sharp global space-time estimate when $n=2$ and improve the known results in the critical cases when $n\\geq 3$. The maximal estimate for $n=2$ is also used to extend the results of the local space-time estimates for the solution to the Schr\\\"odinger equation.","sentences":["In this paper, we studied the space-time estimates for the solution to the Schr\\\"odinger equation.","By polynomial partitioning, induction arguments, bilinear to linear arguments and broad norm estimates, we set up several maximal estimates for the Schr\\\"odinger equation with high-frequency input data.","By these maximal estimates, we obtain the sharp global space-time estimate when $n=2$ and improve the known results in the critical cases when $n\\geq 3$.","The maximal estimate for $n=2$ is also used to extend the results of the local space-time estimates for the solution to the Schr\\\"odinger equation."],"url":"http://arxiv.org/abs/2402.13539v1","category":"math.CA"}
{"created":"2024-02-21 04:46:47","title":"Pion-nucleus elastic scatterings incorporating medium effects within the Eikonal-Glauber mode","abstract":"In this present investigation, we explore the elastic scattering of pions with nuclei ($\\pi$-$A$), primarily influenced by the $\\Delta$(1232) resonance, within the Eikonal-Glauber model. The medium effects are incorporated by considering nuclear-density ($\\rho_A$) dependent masses of baryons and strong coupling constants. These dependencies are computed and parameterized up to $\\mathcal{O}(\\rho_A^2)$ based on the quark-meson coupling (QMC) model. The Wood-Saxon type density profile is utilized for the bound nucleons within finite nuclei. The element $\\pi^+$-$N$ scattering cross section for the Glauber approach is determined using the conventional effective Lagrangian method. Subsequently, we analyze the total cross sections for elastic scattering with $^4$He and $^{12}$C targets. Our numerical results demonstrate a favorable agreement with JINR data for the $^4$He target, accurately reproducing the total cross-section. However, when considering the $^{12}$C target, deviations of approximately $\\lesssim10\\%$. We also consider the multiple-scattering effects inside the nucleus approximately, using the single-channel meson-baryon Bethe-Salpeter equation, resulting in the effective width broadening of the $\\Delta$ resonance to reproduce the data better.","sentences":["In this present investigation, we explore the elastic scattering of pions with nuclei ($\\pi$-$A$), primarily influenced by the $\\Delta$(1232) resonance, within the Eikonal-Glauber model.","The medium effects are incorporated by considering nuclear-density ($\\rho_A$) dependent masses of baryons and strong coupling constants.","These dependencies are computed and parameterized up to $\\mathcal{O}(\\rho_A^2)$ based on the quark-meson coupling (QMC) model.","The Wood-Saxon type density profile is utilized for the bound nucleons within finite nuclei.","The element $\\pi^+$-$N$ scattering cross section for the Glauber approach is determined using the conventional effective Lagrangian method.","Subsequently, we analyze the total cross sections for elastic scattering with $^4$He and $^{12}$C targets.","Our numerical results demonstrate a favorable agreement with JINR data for the $^4$He target, accurately reproducing the total cross-section.","However, when considering the $^{12}$C target, deviations of approximately $\\lesssim10\\%$. We also consider the multiple-scattering effects inside the nucleus approximately, using the single-channel meson-baryon Bethe-Salpeter equation, resulting in the effective width broadening of the $\\Delta$ resonance to reproduce the data better."],"url":"http://arxiv.org/abs/2402.13526v1","category":"hep-ph"}
{"created":"2024-02-21 04:36:30","title":"Balancing Spectral, Temporal and Spatial Information for EEG-based Alzheimer's Disease Classification","abstract":"The prospect of future treatment warrants the development of cost-effective screening for Alzheimer's disease (AD). A promising candidate in this regard is electroencephalography (EEG), as it is one of the most economic imaging modalities. Recent efforts in EEG analysis have shifted towards leveraging spatial information, employing novel frameworks such as graph signal processing or graph neural networks. Here, we systematically investigate the importance of spatial information relative to spectral or temporal information by varying the proportion of each dimension for AD classification. To do so, we test various dimension resolution configurations on two routine EEG datasets. We find that spatial information is consistently more relevant than temporal information and equally relevant as spectral information. These results emphasise the necessity to consider spatial information for EEG-based AD classification. On our second dataset, we further find that well-balanced feature resolutions boost classification accuracy by up to 1.6%. Our resolution-based feature extraction has the potential to improve AD classification specifically, and multivariate signal classification generally.","sentences":["The prospect of future treatment warrants the development of cost-effective screening for Alzheimer's disease (AD).","A promising candidate in this regard is electroencephalography (EEG), as it is one of the most economic imaging modalities.","Recent efforts in EEG analysis have shifted towards leveraging spatial information, employing novel frameworks such as graph signal processing or graph neural networks.","Here, we systematically investigate the importance of spatial information relative to spectral or temporal information by varying the proportion of each dimension for AD classification.","To do so, we test various dimension resolution configurations on two routine EEG datasets.","We find that spatial information is consistently more relevant than temporal information and equally relevant as spectral information.","These results emphasise the necessity to consider spatial information for EEG-based AD classification.","On our second dataset, we further find that well-balanced feature resolutions boost classification accuracy by up to 1.6%.","Our resolution-based feature extraction has the potential to improve AD classification specifically, and multivariate signal classification generally."],"url":"http://arxiv.org/abs/2402.13523v1","category":"eess.SP"}
{"created":"2024-02-21 03:48:53","title":"Mel-FullSubNet: Mel-Spectrogram Enhancement for Improving Both Speech Quality and ASR","abstract":"In this work, we propose Mel-FullSubNet, a single-channel Mel-spectrogram denoising and dereverberation network for improving both speech quality and automatic speech recognition (ASR) performance. Mel-FullSubNet takes as input the noisy and reverberant Mel-spectrogram and predicts the corresponding clean Mel-spectrogram. The enhanced Mel-spectrogram can be either transformed to speech waveform with a neural vocoder or directly used for ASR. Mel-FullSubNet encapsulates interleaved full-band and sub-band networks, for learning the full-band spectral pattern of signals and the sub-band/narrow-band properties of signals, respectively. Compared to linear-frequency domain or time-domain speech enhancement, the major advantage of Mel-spectrogram enhancement is that Mel-frequency presents speech in a more compact way and thus is easier to learn, which will benefit both speech quality and ASR. Experimental results demonstrate a significant improvement in both speech quality and ASR performance achieved by the proposed model.","sentences":["In this work, we propose Mel-FullSubNet, a single-channel Mel-spectrogram denoising and dereverberation network for improving both speech quality and automatic speech recognition (ASR) performance.","Mel-FullSubNet takes as input the noisy and reverberant Mel-spectrogram and predicts the corresponding clean Mel-spectrogram.","The enhanced Mel-spectrogram can be either transformed to speech waveform with a neural vocoder or directly used for ASR.","Mel-FullSubNet encapsulates interleaved full-band and sub-band networks, for learning the full-band spectral pattern of signals and the sub-band/narrow-band properties of signals, respectively.","Compared to linear-frequency domain or time-domain speech enhancement, the major advantage of Mel-spectrogram enhancement is that Mel-frequency presents speech in a more compact way and thus is easier to learn, which will benefit both speech quality and ASR.","Experimental results demonstrate a significant improvement in both speech quality and ASR performance achieved by the proposed model."],"url":"http://arxiv.org/abs/2402.13511v1","category":"eess.AS"}
{"created":"2024-02-21 03:45:18","title":"SealD-NeRF: Interactive Pixel-Level Editing for Dynamic Scenes by Neural Radiance Fields","abstract":"The widespread adoption of implicit neural representations, especially Neural Radiance Fields (NeRF), highlights a growing need for editing capabilities in implicit 3D models, essential for tasks like scene post-processing and 3D content creation. Despite previous efforts in NeRF editing, challenges remain due to limitations in editing flexibility and quality. The key issue is developing a neural representation that supports local edits for real-time updates. Current NeRF editing methods, offering pixel-level adjustments or detailed geometry and color modifications, are mostly limited to static scenes. This paper introduces SealD-NeRF, an extension of Seal-3D for pixel-level editing in dynamic settings, specifically targeting the D-NeRF network. It allows for consistent edits across sequences by mapping editing actions to a specific timeframe, freezing the deformation network responsible for dynamic scene representation, and using a teacher-student approach to integrate changes.","sentences":["The widespread adoption of implicit neural representations, especially Neural Radiance Fields (NeRF), highlights a growing need for editing capabilities in implicit 3D models, essential for tasks like scene post-processing and 3D content creation.","Despite previous efforts in NeRF editing, challenges remain due to limitations in editing flexibility and quality.","The key issue is developing a neural representation that supports local edits for real-time updates.","Current NeRF editing methods, offering pixel-level adjustments or detailed geometry and color modifications, are mostly limited to static scenes.","This paper introduces SealD-NeRF, an extension of Seal-3D for pixel-level editing in dynamic settings, specifically targeting the D-NeRF network.","It allows for consistent edits across sequences by mapping editing actions to a specific timeframe, freezing the deformation network responsible for dynamic scene representation, and using a teacher-student approach to integrate changes."],"url":"http://arxiv.org/abs/2402.13510v1","category":"cs.CV"}
{"created":"2024-02-21 03:14:45","title":"HetTree: Heterogeneous Tree Graph Neural Network","abstract":"The recent past has seen an increasing interest in Heterogeneous Graph Neural Networks (HGNNs) since many real-world graphs are heterogeneous in nature, from citation graphs to email graphs. However, existing methods ignore a tree hierarchy among metapaths, which is naturally constituted by different node types and relation types. In this paper, we present HetTree, a novel heterogeneous tree graph neural network that models both the graph structure and heterogeneous aspects in a scalable and effective manner. Specifically, HetTree builds a semantic tree data structure to capture the hierarchy among metapaths. Existing tree encoding techniques aggregate children nodes by weighting the contribution of children nodes based on similarity to the parent node. However, we find that this tree encoding fails to capture the entire parent-children hierarchy by only considering the parent node. Hence, HetTree uses a novel subtree attention mechanism to emphasize metapaths that are more helpful in encoding parent-children relationships. Moreover, instead of separating feature learning from label learning or treating features and labels equally by projecting them to the same latent space, HetTree proposes to match them carefully based on corresponding metapaths, which provides more accurate and richer information between node features and labels. Our evaluation of HetTree on a variety of real-world datasets demonstrates that it outperforms all existing baselines on open benchmarks and efficiently scales to large real-world graphs with millions of nodes and edges.","sentences":["The recent past has seen an increasing interest in Heterogeneous Graph Neural Networks (HGNNs) since many real-world graphs are heterogeneous in nature, from citation graphs to email graphs.","However, existing methods ignore a tree hierarchy among metapaths, which is naturally constituted by different node types and relation types.","In this paper, we present HetTree, a novel heterogeneous tree graph neural network that models both the graph structure and heterogeneous aspects in a scalable and effective manner.","Specifically, HetTree builds a semantic tree data structure to capture the hierarchy among metapaths.","Existing tree encoding techniques aggregate children nodes by weighting the contribution of children nodes based on similarity to the parent node.","However, we find that this tree encoding fails to capture the entire parent-children hierarchy by only considering the parent node.","Hence, HetTree uses a novel subtree attention mechanism to emphasize metapaths that are more helpful in encoding parent-children relationships.","Moreover, instead of separating feature learning from label learning or treating features and labels equally by projecting them to the same latent space, HetTree proposes to match them carefully based on corresponding metapaths, which provides more accurate and richer information between node features and labels.","Our evaluation of HetTree on a variety of real-world datasets demonstrates that it outperforms all existing baselines on open benchmarks and efficiently scales to large real-world graphs with millions of nodes and edges."],"url":"http://arxiv.org/abs/2402.13496v1","category":"cs.LG"}
{"created":"2024-02-21 02:14:06","title":"Modeling oncolytic virus therapy with distributed delay and non-local diffusion","abstract":"In the field of modeling the dynamics of oncolytic viruses, researchers often face the challenge of using specialized mathematical terms to explain uncertain biological phenomena. This paper introduces a basic framework for an oncolytic virus dynamics model with a general growth rate $\\mathcal{F}$ and a general nonlinear incidence term $\\mathcal{G}$. The construction and derivation of the model explain in detail the generation process and practical significance of the distributed time delays and non-local infection terms. The paper provides the existence and uniqueness of solutions to the model, as well as the existence of a global attractor. Furthermore, through two auxiliary linear partial differential equations, the threshold parameters $\\sigma_1$ are determined for sustained tumor growth and $\\lambda_1$ for successful viral invasion of tumor cells to analyze the global dynamic behavior of the model. Finally, we illustrate and analyze our abstract theoretical results through a specific example.","sentences":["In the field of modeling the dynamics of oncolytic viruses, researchers often face the challenge of using specialized mathematical terms to explain uncertain biological phenomena.","This paper introduces a basic framework for an oncolytic virus dynamics model with a general growth rate $\\mathcal{F}$ and a general nonlinear incidence term $\\mathcal{G}$. The construction and derivation of the model explain in detail the generation process and practical significance of the distributed time delays and non-local infection terms.","The paper provides the existence and uniqueness of solutions to the model, as well as the existence of a global attractor.","Furthermore, through two auxiliary linear partial differential equations, the threshold parameters $\\sigma_1$ are determined for sustained tumor growth and $\\lambda_1$ for successful viral invasion of tumor cells to analyze the global dynamic behavior of the model.","Finally, we illustrate and analyze our abstract theoretical results through a specific example."],"url":"http://arxiv.org/abs/2402.13474v1","category":"math.DS"}
{"created":"2024-02-21 00:47:00","title":"Inverse-designed Photonic Computing Core for Parallel Matrix-vector Multiplication","abstract":"On-chip optical neural networks (ONNs) have recently emerged as an attractive hardware accelerator for deep learning applications, characterized by high computing density, low latency, and compact size. As these networks rely heavily on massive matrix multiplication, photonic computing cores for matrix computation become crucial components for on-chip ONNs, which harness the degree of freedoms (DOFs) in photonics including space, wavelength and mode dimensions. However, previous photonic computing devices have not fully utilized the orthogonality and the conversion characteristic of the waveguide modes, which as we show here, allows for the simultaneous parallel computing of several independent matrix-vector multiplications within the same device. In this work, we propose an inverse-designed photonic computing core for parallel matrix-vector multiplication. The matrices are implemented through a mode conversion process, where the input fundamental modes are simultaneously converted into several orthogonal output modes. Specifically, we target the complex-valued conversion matrices between input and output modes and inversely design the dielectric distribution within the device to achieve parallel matrix-vector multiplication. As a demonstration, the proposed photonic computing core supports simultaneous parallel computing of two independent matrix-vector multiplications, with an ultra-compact footprint and high computing precision (relative error < 8%) at 1550 nm wavelength. The inverse-designed photonic computing devices hold great potential for high-performance on-chip ONNs with low energy consumption and high computing density.","sentences":["On-chip optical neural networks (ONNs) have recently emerged as an attractive hardware accelerator for deep learning applications, characterized by high computing density, low latency, and compact size.","As these networks rely heavily on massive matrix multiplication, photonic computing cores for matrix computation become crucial components for on-chip ONNs, which harness the degree of freedoms (DOFs) in photonics including space, wavelength and mode dimensions.","However, previous photonic computing devices have not fully utilized the orthogonality and the conversion characteristic of the waveguide modes, which as we show here, allows for the simultaneous parallel computing of several independent matrix-vector multiplications within the same device.","In this work, we propose an inverse-designed photonic computing core for parallel matrix-vector multiplication.","The matrices are implemented through a mode conversion process, where the input fundamental modes are simultaneously converted into several orthogonal output modes.","Specifically, we target the complex-valued conversion matrices between input and output modes and inversely design the dielectric distribution within the device to achieve parallel matrix-vector multiplication.","As a demonstration, the proposed photonic computing core supports simultaneous parallel computing of two independent matrix-vector multiplications, with an ultra-compact footprint and high computing precision (relative error < 8%) at 1550 nm wavelength.","The inverse-designed photonic computing devices hold great potential for high-performance on-chip ONNs with low energy consumption and high computing density."],"url":"http://arxiv.org/abs/2402.13447v1","category":"physics.optics"}
{"created":"2024-02-21 00:37:16","title":"The Effectiveness of Graph Contrastive Learning on Mathematical Information Retrieval","abstract":"This paper details an empirical investigation into using Graph Contrastive Learning (GCL) to generate mathematical equation representations, a critical aspect of Mathematical Information Retrieval (MIR). Our findings reveal that this simple approach consistently exceeds the performance of the current leading formula retrieval model, TangentCFT. To support ongoing research and development in this field, we have made our source code accessible to the public at https://github.com/WangPeiSyuan/GCL-Formula-Retrieval/.","sentences":["This paper details an empirical investigation into using Graph Contrastive Learning (GCL) to generate mathematical equation representations, a critical aspect of Mathematical Information Retrieval (MIR).","Our findings reveal that this simple approach consistently exceeds the performance of the current leading formula retrieval model, TangentCFT.","To support ongoing research and development in this field, we have made our source code accessible to the public at https://github.com/WangPeiSyuan/GCL-Formula-Retrieval/."],"url":"http://arxiv.org/abs/2402.13444v1","category":"cs.IR"}
{"created":"2024-02-21 00:24:34","title":"PaCKD: Pattern-Clustered Knowledge Distillation for Compressing Memory Access Prediction Models","abstract":"Deep neural networks (DNNs) have proven to be effective models for accurate Memory Access Prediction (MAP), a critical task in mitigating memory latency through data prefetching. However, existing DNN-based MAP models suffer from the challenges such as significant physical storage space and poor inference latency, primarily due to their large number of parameters. These limitations render them impractical for deployment in real-world scenarios. In this paper, we propose PaCKD, a Pattern-Clustered Knowledge Distillation approach to compress MAP models while maintaining the prediction performance. The PaCKD approach encompasses three steps: clustering memory access sequences into distinct partitions involving similar patterns, training large pattern-specific teacher models for memory access prediction for each partition, and training a single lightweight student model by distilling the knowledge from the trained pattern-specific teachers. We evaluate our approach on LSTM, MLP-Mixer, and ResNet models, as they exhibit diverse structures and are widely used for image classification tasks in order to test their effectiveness in four widely used graph applications. Compared to the teacher models with 5.406M parameters and an F1-score of 0.4626, our student models achieve a 552$\\times$ model size compression while maintaining an F1-score of 0.4538 (with a 1.92% performance drop). Our approach yields an 8.70% higher result compared to student models trained with standard knowledge distillation and an 8.88% higher result compared to student models trained without any form of knowledge distillation.","sentences":["Deep neural networks (DNNs) have proven to be effective models for accurate Memory Access Prediction (MAP), a critical task in mitigating memory latency through data prefetching.","However, existing DNN-based MAP models suffer from the challenges such as significant physical storage space and poor inference latency, primarily due to their large number of parameters.","These limitations render them impractical for deployment in real-world scenarios.","In this paper, we propose PaCKD, a Pattern-Clustered Knowledge Distillation approach to compress MAP models while maintaining the prediction performance.","The PaCKD approach encompasses three steps: clustering memory access sequences into distinct partitions involving similar patterns, training large pattern-specific teacher models for memory access prediction for each partition, and training a single lightweight student model by distilling the knowledge from the trained pattern-specific teachers.","We evaluate our approach on LSTM, MLP-Mixer, and ResNet models, as they exhibit diverse structures and are widely used for image classification tasks in order to test their effectiveness in four widely used graph applications.","Compared to the teacher models with 5.406M parameters and an F1-score of 0.4626, our student models achieve a 552$\\times$ model size compression while maintaining an F1-score of 0.4538 (with a 1.92% performance drop).","Our approach yields an 8.70% higher result compared to student models trained with standard knowledge distillation and an 8.88% higher result compared to student models trained without any form of knowledge distillation."],"url":"http://arxiv.org/abs/2402.13441v1","category":"cs.LG"}
{"created":"2024-02-20 23:53:16","title":"Game Design Inspired by Scientific Concepts of Quantum Physics","abstract":"The huge gap between the public perception of science and the reality of scientific research severely limits the scope of public engagement with science. We create and develop new theatre, film and games work and related artistic endeavors inspired by science and technology. In this paper, we describe the Quantum Games Project, one of the most recent artistic endeavors in our lab. This project consists of a series of card and digital games and an immersive experience, all of which expose the public to quantum physics without learning barriers. Quantum physics explains the counter-intuitive and surprising ways matter behaves at the subatomic level. It is an exciting and growing field that offers solutions to new technological problems. However, learning quantum physics requires several prerequisites, such as a foundation in the sciences and mathematics, and is primarily taught at the undergraduate and higher levels. As a result, the concepts may be too elusive and abstract for the general public to learn independently. This difficulty is compounded by the limited availability of easily understandable resources and teaching materials that do not employ scientific jargon and equations. Our work attempts to solve this problem by communicating the concepts of quantum physics in a way that is comprehensible and accessible to the general public. This paper provides a general overview of the development of the Quantum Games Project, focusing specifically on the Quantum Photo Booth experience, and describes how science is integrated into the very nature of the game development process and its outcome.","sentences":["The huge gap between the public perception of science and the reality of scientific research severely limits the scope of public engagement with science.","We create and develop new theatre, film and games work and related artistic endeavors inspired by science and technology.","In this paper, we describe the Quantum Games Project, one of the most recent artistic endeavors in our lab.","This project consists of a series of card and digital games and an immersive experience, all of which expose the public to quantum physics without learning barriers.","Quantum physics explains the counter-intuitive and surprising ways matter behaves at the subatomic level.","It is an exciting and growing field that offers solutions to new technological problems.","However, learning quantum physics requires several prerequisites, such as a foundation in the sciences and mathematics, and is primarily taught at the undergraduate and higher levels.","As a result, the concepts may be too elusive and abstract for the general public to learn independently.","This difficulty is compounded by the limited availability of easily understandable resources and teaching materials that do not employ scientific jargon and equations.","Our work attempts to solve this problem by communicating the concepts of quantum physics in a way that is comprehensible and accessible to the general public.","This paper provides a general overview of the development of the Quantum Games Project, focusing specifically on the Quantum Photo Booth experience, and describes how science is integrated into the very nature of the game development process and its outcome."],"url":"http://arxiv.org/abs/2402.13431v1","category":"physics.pop-ph"}
{"created":"2024-02-20 23:41:16","title":"Emergence and dynamics of delusions and hallucinations across stages in early psychosis","abstract":"Hallucinations and delusions are often grouped together within the positive symptoms of psychosis. However, recent evidence suggests they may be driven by distinct computational and neural mechanisms. Examining the time course of their emergence may provide insights into the relationship between these underlying mechanisms. Participants from the second (N = 719) and third (N = 699) iterations of the North American Prodrome Longitudinal Study (NAPLS 2 and 3) were assessed for timing of CHR-P-level delusion and hallucination onset. Pre-onset symptom patterns in first-episode psychosis patients (FEP) from the Prevention and Early Intervention Program for Psychosis (PEPP-Montreal; N = 694) were also assessed. Symptom onset was determined at baseline assessment and the evolution of symptom patterns examined over 24 months. In all three samples, participants were more likely to report the onset of delusion-spectrum symptoms prior to hallucination-spectrum symptoms (odds ratios (OR): NAPLS 2 = 4.09; NAPLS 3 = 4.14; PEPP, Z = 7.01, P < 0.001) and to present with only delusions compared to only hallucinations (OR: NAPLS 2 = 5.6; NAPLS 3 = 11.11; PEPP = 42.75). Re-emergence of delusions after remission was also more common than re-emergence of hallucinations (Ps < 0.05), and hallucinations more often resolved first (Ps < 0.001). In both CHR-P samples, ratings of delusional ideation fell with the onset of hallucinations (P = 0.007). Delusions tend to emerge before hallucinations and may play a role in their development. Further work should examine the relationship between the mechanisms driving these symptoms and its utility for diagnosis and treatment.","sentences":["Hallucinations and delusions are often grouped together within the positive symptoms of psychosis.","However, recent evidence suggests they may be driven by distinct computational and neural mechanisms.","Examining the time course of their emergence may provide insights into the relationship between these underlying mechanisms.","Participants from the second (N = 719) and third (N = 699) iterations of the North American Prodrome Longitudinal Study (NAPLS 2 and 3) were assessed for timing of CHR-P-level delusion and hallucination onset.","Pre-onset symptom patterns in first-episode psychosis patients (FEP) from the Prevention and Early Intervention Program for Psychosis (PEPP-Montreal; N = 694) were also assessed.","Symptom onset was determined at baseline assessment and the evolution of symptom patterns examined over 24 months.","In all three samples, participants were more likely to report the onset of delusion-spectrum symptoms prior to hallucination-spectrum symptoms (odds ratios (OR): NAPLS 2 = 4.09; NAPLS 3 = 4.14; PEPP, Z = 7.01, P < 0.001) and to present with only delusions compared to only hallucinations (OR: NAPLS 2 = 5.6; NAPLS 3 = 11.11; PEPP = 42.75).","Re-emergence of delusions after remission was also more common than re-emergence of hallucinations (Ps < 0.05), and hallucinations more often resolved first (Ps < 0.001).","In both CHR-P samples, ratings of delusional ideation fell with the onset of hallucinations (P = 0.007).","Delusions tend to emerge before hallucinations and may play a role in their development.","Further work should examine the relationship between the mechanisms driving these symptoms and its utility for diagnosis and treatment."],"url":"http://arxiv.org/abs/2402.13428v1","category":"q-bio.NC"}
{"created":"2024-02-20 23:06:21","title":"EvolMPNN: Predicting Mutational Effect on Homologous Proteins by Evolution Encoding","abstract":"Predicting protein properties is paramount for biological and medical advancements. Current protein engineering mutates on a typical protein, called the wild-type, to construct a family of homologous proteins and study their properties. Yet, existing methods easily neglect subtle mutations, failing to capture the effect on the protein properties. To this end, we propose EvolMPNN, Evolution-aware Message Passing Neural Network, to learn evolution-aware protein embeddings. EvolMPNN samples sets of anchor proteins, computes evolutionary information by means of residues and employs a differentiable evolution-aware aggregation scheme over these sampled anchors. This way EvolMPNNcan capture the mutation effect on proteins with respect to the anchor proteins. Afterwards, the aggregated evolution-aware embeddings are integrated with sequence embeddings to generate final comprehensive protein embeddings. Our model shows up to 6.4% better than state-of-the-art methods and attains 36x inference speedup in comparison with large pre-trained models.","sentences":["Predicting protein properties is paramount for biological and medical advancements.","Current protein engineering mutates on a typical protein, called the wild-type, to construct a family of homologous proteins and study their properties.","Yet, existing methods easily neglect subtle mutations, failing to capture the effect on the protein properties.","To this end, we propose EvolMPNN, Evolution-aware Message Passing Neural Network, to learn evolution-aware protein embeddings.","EvolMPNN samples sets of anchor proteins, computes evolutionary information by means of residues and employs a differentiable evolution-aware aggregation scheme over these sampled anchors.","This way EvolMPNNcan capture the mutation effect on proteins with respect to the anchor proteins.","Afterwards, the aggregated evolution-aware embeddings are integrated with sequence embeddings to generate final comprehensive protein embeddings.","Our model shows up to 6.4% better than state-of-the-art methods and attains 36x inference speedup in comparison with large pre-trained models."],"url":"http://arxiv.org/abs/2402.13418v1","category":"cs.LG"}
{"created":"2024-02-20 22:46:16","title":"Generation and control of population difference gratings in a three-level hydrogen atomic medium using half-cycle attosecond pulses","abstract":"Recently, the possibility of the generation and interaction of unipolar half-cycle electromagnetic pulses with quantum systems has been the subject of active research. Such pulses can have many different and interesting applications. They are able to excite quantum systems very fast. Based on the numerical solution of Maxwell-Bloch equations, this paper theoretically studies the possibility of guiding and ultrafast controlling population difference gratings by a sequence of half-cycle attosecond pulses in a three-level resonant medium. The parameters of the model medium (transition frequencies and dipole moments of the transitions) are close to those found in the hydrogen atom. We show the possibility of guiding periodic gratings and dynamic microcavities on different resonance transitions of the medium. In this case, the medium considered is an example of a spatiotemporal photonic crystal. The refractive index varies rapidly in space and time. The possibility of the use of such gratings as Bragg mirrors in ultrafast optics is discussed. The results obtained are of interest in two actively developing areas of modern optics - the physics of half-cycle pulses and space-time photonic crystals.","sentences":["Recently, the possibility of the generation and interaction of unipolar half-cycle electromagnetic pulses with quantum systems has been the subject of active research.","Such pulses can have many different and interesting applications.","They are able to excite quantum systems very fast.","Based on the numerical solution of Maxwell-Bloch equations, this paper theoretically studies the possibility of guiding and ultrafast controlling population difference gratings by a sequence of half-cycle attosecond pulses in a three-level resonant medium.","The parameters of the model medium (transition frequencies and dipole moments of the transitions) are close to those found in the hydrogen atom.","We show the possibility of guiding periodic gratings and dynamic microcavities on different resonance transitions of the medium.","In this case, the medium considered is an example of a spatiotemporal photonic crystal.","The refractive index varies rapidly in space and time.","The possibility of the use of such gratings as Bragg mirrors in ultrafast optics is discussed.","The results obtained are of interest in two actively developing areas of modern optics - the physics of half-cycle pulses and space-time photonic crystals."],"url":"http://arxiv.org/abs/2402.13413v1","category":"physics.optics"}
{"created":"2024-02-20 22:34:06","title":"An efficient finite element method for computing the response of a strain-limiting elastic solid containing a v-notch and inclusions","abstract":"Accurate triangulation of the domain plays a pivotal role in computing the numerical approximation of the differential operators. A good triangulation is the one which aids in reducing discretization errors. In a standard collocation technique, the smooth curved domain is typically triangulated with a mesh by taking points on the boundary to approximate them by polygons. However, such an approach often leads to geometrical errors which directly affect the accuracy of the numerical approximation. To restrict such geometrical errors, \\textit{isoparametric}, \\textit{subparametric}, and \\textit{iso-geometric} methods were introduced which allow the approximation of the curved surfaces (or curved line segments). In this paper, we present an efficient finite element method to approximate the solution to the elliptic boundary value problem (BVP), which governs the response of an elastic solid containing a v-notch and inclusions. The algebraically nonlinear constitutive equation along with the balance of linear momentum reduces to second-order quasi-linear elliptic partial differential equation. Our approach allows us to represent the complex curved boundaries by smooth \\textit{one-of-its-kind} point transformation. The main idea is to obtain higher-order shape functions which enable us to accurately compute the entries in the finite element matrices and vectors. A Picard-type linearization is utilized to handle the nonlinearities in the governing differential equation. The numerical results for the test cases show considerable improvement in the accuracy.","sentences":["Accurate triangulation of the domain plays a pivotal role in computing the numerical approximation of the differential operators.","A good triangulation is the one which aids in reducing discretization errors.","In a standard collocation technique, the smooth curved domain is typically triangulated with a mesh by taking points on the boundary to approximate them by polygons.","However, such an approach often leads to geometrical errors which directly affect the accuracy of the numerical approximation.","To restrict such geometrical errors, \\textit{isoparametric}, \\textit{subparametric}, and \\textit{iso-geometric} methods were introduced which allow the approximation of the curved surfaces (or curved line segments).","In this paper, we present an efficient finite element method to approximate the solution to the elliptic boundary value problem (BVP), which governs the response of an elastic solid containing a v-notch and inclusions.","The algebraically nonlinear constitutive equation along with the balance of linear momentum reduces to second-order quasi-linear elliptic partial differential equation.","Our approach allows us to represent the complex curved boundaries by smooth \\textit{one-of-its-kind} point transformation.","The main idea is to obtain higher-order shape functions which enable us to accurately compute the entries in the finite element matrices and vectors.","A Picard-type linearization is utilized to handle the nonlinearities in the governing differential equation.","The numerical results for the test cases show considerable improvement in the accuracy."],"url":"http://arxiv.org/abs/2402.13409v1","category":"math.NA"}
{"created":"2024-02-20 22:25:07","title":"Einstein metrics on homogeneous spaces $H\\times H/\u0394K$","abstract":"Given any compact homogeneous space $H/K$ with $H$ simple, we consider the new space $M=H\\times H/\\Delta K$, where $\\Delta K$ denotes diagonal embedding, and study the existence, classification and stability of $H\\times H$-invariant Einstein metrics on $M$, as a first step into the largely unexplored case of homogeneous spaces of compact non-simple Lie groups. We find unstable Einstein metrics on $M$ for most spaces $H/K$ such that their standard metric is Einstein (e.g., isotropy irreducible) and the Killing form of $\\mathfrak{k}$ is a multiple of the Killing form of $\\mathfrak{h}$ (e.g., $K$ simple), a class which contains $17$ families and $50$ individual examples. A complete classification is obtained in the case when $H/K$ is an irreducible symmetric space. We also study the behavior of the scalar curvature function on the space of all normal metrics on $M=H\\times H/\\Delta K$ (none of which is Einstein), obtaining that the standard metric is a global minimum.","sentences":["Given any compact homogeneous space $H/K$ with $H$ simple, we consider the new space $M=H\\times H/\\Delta K$, where $\\Delta K$ denotes diagonal embedding, and study the existence, classification and stability of $H\\times H$-invariant Einstein metrics on $M$, as a first step into the largely unexplored case of homogeneous spaces of compact non-simple Lie groups.","We find unstable Einstein metrics on $M$ for most spaces $H/K$ such that their standard metric is Einstein (e.g., isotropy irreducible) and the Killing form of $\\mathfrak{k}$ is a multiple of the Killing form of $\\mathfrak{h}$ (e.g., $K$ simple), a class which contains $17$ families and $50$ individual examples.","A complete classification is obtained in the case when $H/K$ is an irreducible symmetric space.","We also study the behavior of the scalar curvature function on the space of all normal metrics on $M=H\\times H/\\Delta K$ (none of which is Einstein), obtaining that the standard metric is a global minimum."],"url":"http://arxiv.org/abs/2402.13407v1","category":"math.DG"}
{"created":"2024-02-20 21:07:19","title":"Stability estimates for magnetized Vlasov equations","abstract":"We present two results related to magnetized Vlasov equations. Our first contribution concerns the stability of solutions to the magnetized Vlasov-Poisson system with a non-uniform magnetic field using the optimal transport approach introduced by Loeper [24]. We show that the extra magnetized terms can be suitably controlled by imposing stronger decay in velocity on one of the distribution functions, illustrating how the external magnetic field creates anisotropy in the evolution. This allows us to generalize the classical 2-Wasserstein stability estimate by Loeper [24, Theorem 1.2] and the recent stability estimate using a kinetic Wasserstein distance by Iacobelli [20, Theorem 3.1] to the magnetized Vlasov-Poisson system. In our second result, we extend the improved Dobrushin estimate by Iacobelli [20, Theorem 2.1] to the magnetized Vlasov equation with a uniform magnetic field.","sentences":["We present two results related to magnetized Vlasov equations.","Our first contribution concerns the stability of solutions to the magnetized Vlasov-Poisson system with a non-uniform magnetic field using the optimal transport approach introduced by Loeper [24].","We show that the extra magnetized terms can be suitably controlled by imposing stronger decay in velocity on one of the distribution functions, illustrating how the external magnetic field creates anisotropy in the evolution.","This allows us to generalize the classical 2-Wasserstein stability estimate by Loeper","[24, Theorem 1.2] and the recent stability estimate using a kinetic Wasserstein distance by Iacobelli","[20, Theorem 3.1] to the magnetized Vlasov-Poisson system.","In our second result, we extend the improved Dobrushin estimate by Iacobelli","[20, Theorem 2.1] to the magnetized Vlasov equation with a uniform magnetic field."],"url":"http://arxiv.org/abs/2402.13377v1","category":"math.AP"}
{"created":"2024-02-20 20:53:59","title":"New preconditioner strategy for solving block four-by-four linear systems: An application to the saddle-point problem from 3D Stokes equation","abstract":"We have presented a fast method for solving a specific type of block four-by-four saddlepoint problem arising from the finite element discretization of the generalized 3D Stokes problem. We analyze the eigenvalue distribution and the eigenvectors of the preconditioned matrix. Furthermore, we suggested utilizing the preconditioned global conjugate gradient method (PGCG) as a block iterative solver for handling multiple right-hand sides within the sub-system and give some new convergence results. Numerical experiments have shown that our preconditioned iterative approach is very efficient for solving the 3D Stokes problem","sentences":["We have presented a fast method for solving a specific type of block four-by-four saddlepoint problem arising from the finite element discretization of the generalized 3D Stokes problem.","We analyze the eigenvalue distribution and the eigenvectors of the preconditioned matrix.","Furthermore, we suggested utilizing the preconditioned global conjugate gradient method (PGCG) as a block iterative solver for handling multiple right-hand sides within the sub-system and give some new convergence results.","Numerical experiments have shown that our preconditioned iterative approach is very efficient for solving the 3D Stokes problem"],"url":"http://arxiv.org/abs/2402.13373v1","category":"math.NA"}
{"created":"2024-02-20 20:53:04","title":"FIDLAR: Forecast-Informed Deep Learning Architecture for Flood Mitigation","abstract":"In coastal river systems, frequent floods, often occurring during major storms or king tides, pose a severe threat to lives and property. However, these floods can be mitigated or even prevented by strategically releasing water before extreme weather events with hydraulic structures such as dams, gates, pumps, and reservoirs. A standard approach used by local water management agencies is the \"rule-based\" method, which specifies predetermined pre-releases of water based on historical and time-tested human experience, but which tends to result in excess or inadequate water release. The model predictive control (MPC), a physics-based model for prediction, is an alternative approach, albeit involving computationally intensive calculations. In this paper, we propose a Forecast Informed Deep Learning Architecture, FIDLAR, to achieve rapid and optimal flood management with precise water pre-releases. FIDLAR seamlessly integrates two neural network modules: one called the Flood Manager, which is responsible for generating water pre-release schedules, and another called the Flood Evaluator, which assesses these generated schedules. The Evaluator module is pre-trained separately, and its gradient-based feedback is used to train the Manager model, ensuring optimal water pre-releases. We have conducted experiments using FIDLAR with data from a flood-prone coastal area in South Florida, particularly susceptible to frequent storms. Results show that FIDLAR is several orders of magnitude faster than currently used physics-based approaches while outperforming baseline methods with improved water pre-release schedules. Our code is at https://github.com/JimengShi/FIDLAR/.","sentences":["In coastal river systems, frequent floods, often occurring during major storms or king tides, pose a severe threat to lives and property.","However, these floods can be mitigated or even prevented by strategically releasing water before extreme weather events with hydraulic structures such as dams, gates, pumps, and reservoirs.","A standard approach used by local water management agencies is the \"rule-based\" method, which specifies predetermined pre-releases of water based on historical and time-tested human experience, but which tends to result in excess or inadequate water release.","The model predictive control (MPC), a physics-based model for prediction, is an alternative approach, albeit involving computationally intensive calculations.","In this paper, we propose a Forecast Informed Deep Learning Architecture, FIDLAR, to achieve rapid and optimal flood management with precise water pre-releases.","FIDLAR seamlessly integrates two neural network modules: one called the Flood Manager, which is responsible for generating water pre-release schedules, and another called the Flood Evaluator, which assesses these generated schedules.","The Evaluator module is pre-trained separately, and its gradient-based feedback is used to train the Manager model, ensuring optimal water pre-releases.","We have conducted experiments using FIDLAR with data from a flood-prone coastal area in South Florida, particularly susceptible to frequent storms.","Results show that FIDLAR is several orders of magnitude faster than currently used physics-based approaches while outperforming baseline methods with improved water pre-release schedules.","Our code is at https://github.com/JimengShi/FIDLAR/."],"url":"http://arxiv.org/abs/2402.13371v1","category":"cs.LG"}
{"created":"2024-02-20 20:47:06","title":"Intrinsic Derivation of the Equations of a Snake Robot based on a Cosserat Beam Model","abstract":"In this paper, we present an intrinsic derivation of the equations ruling the dynamics motion of a snake robot dynamics. Based on a Cosserat beam model, we first show that the extended configuration space is a Lie group. Endowing it with an appropriate left invariant metric, the corresponding Euler-Poincar\\'e equations can be reduced to a system of hyperbolic PDEs in the Lie algebra $\\mathfrak{se}(3)$. We also provide the constitutive law describing the actuation in this system of PDEs.","sentences":["In this paper, we present an intrinsic derivation of the equations ruling the dynamics motion of a snake robot dynamics.","Based on a Cosserat beam model, we first show that the extended configuration space is a Lie group.","Endowing it with an appropriate left invariant metric, the corresponding Euler-Poincar\\'e equations can be reduced to a system of hyperbolic PDEs in the Lie algebra $\\mathfrak{se}(3)$. We also provide the constitutive law describing the actuation in this system of PDEs."],"url":"http://arxiv.org/abs/2402.13367v1","category":"math.OC"}
{"created":"2024-02-20 20:37:09","title":"A renormalised description of meromorphic connections over curves","abstract":"The Renormalisation Group (RG) is a systematic procedure used to regularise divergences appearing as artefacts when constructing solutions to a large class of differential problems, whether perturbatively or not. This paper is devoted to performing it in the non-perturbative context of flat sections of meromorphic connections in holomorphic principal bundles over a base complex curve, with reductive and complex structure Lie groups. Using the interpretation of Kunuhiro of RG in terms of envelopes of families of solutions, we identify these envelopes as particular flat sections associated to points on the corresponding quantum spectral curve, namely certain divisors on the base curve with local coefficients in the adjoint bundle, yielding integral representations. This shows that the non-perturbative renormalisation of flat sections of principal bundles over curves fits naturally within the description of meromorphic connections via their quantum geometry, or homology \\textit{\\`a la} Goldman, where the flat sections and their deformations are parameterized by degree zero and one chains respectively. We conclude the paper by mentioning upcoming applications.","sentences":["The Renormalisation Group (RG) is a systematic procedure used to regularise divergences appearing as artefacts when constructing solutions to a large class of differential problems, whether perturbatively or not.","This paper is devoted to performing it in the non-perturbative context of flat sections of meromorphic connections in holomorphic principal bundles over a base complex curve, with reductive and complex structure Lie groups.","Using the interpretation of Kunuhiro of RG in terms of envelopes of families of solutions, we identify these envelopes as particular flat sections associated to points on the corresponding quantum spectral curve, namely certain divisors on the base curve with local coefficients in the adjoint bundle, yielding integral representations.","This shows that the non-perturbative renormalisation of flat sections of principal bundles over curves fits naturally within the description of meromorphic connections via their quantum geometry, or homology \\textit{\\`a la} Goldman, where the flat sections and their deformations are parameterized by degree zero and one chains respectively.","We conclude the paper by mentioning upcoming applications."],"url":"http://arxiv.org/abs/2402.13362v1","category":"math-ph"}
{"created":"2024-02-20 19:48:14","title":"Edge-averaged virtual element methods for convection-diffusion and convection-dominated problems","abstract":"This manuscript develops edge-averaged virtual element (EAVE) methodologies to address convection-diffusion problems effectively in the convection-dominated regime. It introduces a variant of EAVE that ensures monotonicity (producing an $M$-matrix) on Voronoi polygonal meshes, provided their duals are Delaunay triangulations with acute angles. Furthermore, the study outlines a comprehensive framework for EAVE methodologies, introducing another variant that integrates with the stiffness matrix derived from the lowest-order virtual element method for the Poisson equation. Numerical experiments confirm the theoretical advantages of the monotonicity property and demonstrate an optimal convergence rate across various mesh configurations.","sentences":["This manuscript develops edge-averaged virtual element (EAVE) methodologies to address convection-diffusion problems effectively in the convection-dominated regime.","It introduces a variant of EAVE that ensures monotonicity (producing an $M$-matrix) on Voronoi polygonal meshes, provided their duals are Delaunay triangulations with acute angles.","Furthermore, the study outlines a comprehensive framework for EAVE methodologies, introducing another variant that integrates with the stiffness matrix derived from the lowest-order virtual element method for the Poisson equation.","Numerical experiments confirm the theoretical advantages of the monotonicity property and demonstrate an optimal convergence rate across various mesh configurations."],"url":"http://arxiv.org/abs/2402.13347v1","category":"math.NA"}
{"created":"2024-02-20 19:48:09","title":"Intrinsic expansions in large Grashof numbers for the steady states of the {N}avier--{S}tokes equations","abstract":"We enable a theory of intrinsic asymptotic expansions for the steady state solutions of the full Navier--Stokes equations. Such a theory was first developed in \\cite{FHJ} for Galerkin approximations. To overcome the lack of local compactness in infinite dimensional spaces, we introduce the notion of asymptotic expansions in nested spaces. When the inclusion maps between these spaces are compact, we establish the existence of such an asymptotic expansion for a subsequence of any bounded sequence. This consequently yields an intrinsic asymptotic expansion in a single normed space. We apply this result to the steady states of the Navier--Stokes equations by utilizing the spectral fractional Sobolev spaces. In the case of 2D periodic boundary conditions, more properties relating the terms of the asymptotic expansion are obtained.","sentences":["We enable a theory of intrinsic asymptotic expansions for the steady state solutions of the full Navier--Stokes equations.","Such a theory was first developed in \\cite{FHJ} for Galerkin approximations.","To overcome the lack of local compactness in infinite dimensional spaces, we introduce the notion of asymptotic expansions in nested spaces.","When the inclusion maps between these spaces are compact, we establish the existence of such an asymptotic expansion for a subsequence of any bounded sequence.","This consequently yields an intrinsic asymptotic expansion in a single normed space.","We apply this result to the steady states of the Navier--Stokes equations by utilizing the spectral fractional Sobolev spaces.","In the case of 2D periodic boundary conditions, more properties relating the terms of the asymptotic expansion are obtained."],"url":"http://arxiv.org/abs/2402.13346v1","category":"math.AP"}
{"created":"2024-02-20 19:19:56","title":"Double machine learning for causal hybrid modeling -- applications in the Earth sciences","abstract":"Hybrid modeling integrates machine learning with scientific knowledge with the goal of enhancing interpretability, generalization, and adherence to natural laws. Nevertheless, equifinality and regularization biases pose challenges in hybrid modeling to achieve these purposes. This paper introduces a novel approach to estimating hybrid models via a causal inference framework, specifically employing Double Machine Learning (DML) to estimate causal effects. We showcase its use for the Earth sciences on two problems related to carbon dioxide fluxes. In the $Q_{10}$ model, we demonstrate that DML-based hybrid modeling is superior in estimating causal parameters over end-to-end deep neural network (DNN) approaches, proving efficiency, robustness to bias from regularization methods, and circumventing equifinality. Our approach, applied to carbon flux partitioning, exhibits flexibility in accommodating heterogeneous causal effects. The study emphasizes the necessity of explicitly defining causal graphs and relationships, advocating for this as a general best practice. We encourage the continued exploration of causality in hybrid models for more interpretable and trustworthy results in knowledge-guided machine learning.","sentences":["Hybrid modeling integrates machine learning with scientific knowledge with the goal of enhancing interpretability, generalization, and adherence to natural laws.","Nevertheless, equifinality and regularization biases pose challenges in hybrid modeling to achieve these purposes.","This paper introduces a novel approach to estimating hybrid models via a causal inference framework, specifically employing Double Machine Learning (DML) to estimate causal effects.","We showcase its use for the Earth sciences on two problems related to carbon dioxide fluxes.","In the $Q_{10}$ model, we demonstrate that DML-based hybrid modeling is superior in estimating causal parameters over end-to-end deep neural network (DNN) approaches, proving efficiency, robustness to bias from regularization methods, and circumventing equifinality.","Our approach, applied to carbon flux partitioning, exhibits flexibility in accommodating heterogeneous causal effects.","The study emphasizes the necessity of explicitly defining causal graphs and relationships, advocating for this as a general best practice.","We encourage the continued exploration of causality in hybrid models for more interpretable and trustworthy results in knowledge-guided machine learning."],"url":"http://arxiv.org/abs/2402.13332v1","category":"cs.LG"}
{"created":"2024-02-20 19:19:47","title":"Enhanced Hallucination Detection in Neural Machine Translation through Simple Detector Aggregation","abstract":"Hallucinated translations pose significant threats and safety concerns when it comes to the practical deployment of machine translation systems. Previous research works have identified that detectors exhibit complementary performance different detectors excel at detecting different types of hallucinations. In this paper, we propose to address the limitations of individual detectors by combining them and introducing a straightforward method for aggregating multiple detectors. Our results demonstrate the efficacy of our aggregated detector, providing a promising step towards evermore reliable machine translation systems.","sentences":["Hallucinated translations pose significant threats and safety concerns when it comes to the practical deployment of machine translation systems.","Previous research works have identified that detectors exhibit complementary performance different detectors excel at detecting different types of hallucinations.","In this paper, we propose to address the limitations of individual detectors by combining them and introducing a straightforward method for aggregating multiple detectors.","Our results demonstrate the efficacy of our aggregated detector, providing a promising step towards evermore reliable machine translation systems."],"url":"http://arxiv.org/abs/2402.13331v1","category":"cs.CL"}
{"created":"2024-02-20 19:00:59","title":"Rigor with Machine Learning from Field Theory to the Poincar\u00e9 Conjecture","abstract":"Machine learning techniques are increasingly powerful, leading to many breakthroughs in the natural sciences, but they are often stochastic, error-prone, and blackbox. How, then, should they be utilized in fields such as theoretical physics and pure mathematics that place a premium on rigor and understanding? In this Perspective we discuss techniques for obtaining rigor in the natural sciences with machine learning. Non-rigorous methods may lead to rigorous results via conjecture generation or verification by reinforcement learning. We survey applications of these techniques-for-rigor ranging from string theory to the smooth $4$d Poincar\\'e conjecture in low-dimensional topology. One can also imagine building direct bridges between machine learning theory and either mathematics or theoretical physics. As examples, we describe a new approach to field theory motivated by neural network theory, and a theory of Riemannian metric flows induced by neural network gradient descent, which encompasses Perelman's formulation of the Ricci flow that was utilized to resolve the $3$d Poincar\\'e conjecture.","sentences":["Machine learning techniques are increasingly powerful, leading to many breakthroughs in the natural sciences, but they are often stochastic, error-prone, and blackbox.","How, then, should they be utilized in fields such as theoretical physics and pure mathematics that place a premium on rigor and understanding?","In this Perspective we discuss techniques for obtaining rigor in the natural sciences with machine learning.","Non-rigorous methods may lead to rigorous results via conjecture generation or verification by reinforcement learning.","We survey applications of these techniques-for-rigor ranging from string theory to the smooth $4$d Poincar\\'e conjecture in low-dimensional topology.","One can also imagine building direct bridges between machine learning theory and either mathematics or theoretical physics.","As examples, we describe a new approach to field theory motivated by neural network theory, and a theory of Riemannian metric flows induced by neural network gradient descent, which encompasses Perelman's formulation of the Ricci flow that was utilized to resolve the $3$d Poincar\\'e conjecture."],"url":"http://arxiv.org/abs/2402.13321v1","category":"hep-th"}
{"created":"2024-02-20 19:00:06","title":"Dead or Alive? How Bursty Star Formation and Patchy Dust Can Cause Temporary Quiescence in High Redshift Galaxies","abstract":"The recent discovery of a galaxy at z=7.3 with undetected optical emission lines and a blue UV to optical continuum ratio in JWST spectroscopy is surprising and needs to be explained physically. Here, we explore two possibilities that could cause such a seemingly quiescent 5e8 Msun galaxy in the early Universe: (i) stochastic variations in the star formation history (SFH) and (ii) the effect of spatially varying dust attenuation on the measured line and continuum emission properties. Both scenarios can play at the same time to amplify the effect. A stochastic star formation model (similar to realistic SFHs from hydrodynamical simulations of similar-mass galaxies) can create such observed properties if star formation is fast-varying with a correlation time of <200 Myrs given a reasonable burst amplitude of ~0.6 dex. The total time spent in this state is less than 20 Myrs, and the likelihood of such a state to occur over 500 Myrs at z=7 is ~50%. On the other hand, we show that a spectrum with blue UV continuum and lack of emission lines can be reproduced by a blue+red composite spectrum. The UV continuum is emitted from dust-free density bounded HII regions (blue component), while the red component is a dust-obscured starburst with weakened emission lines due to strong differential dust attenuation between stellar and nebular emission. Future resolving far-infrared observations with ALMA will shed light on the latter scenario.","sentences":["The recent discovery of a galaxy at z=7.3 with undetected optical emission lines and a blue UV to optical continuum ratio in JWST spectroscopy is surprising and needs to be explained physically.","Here, we explore two possibilities that could cause such a seemingly quiescent 5e8 Msun galaxy in the early Universe: (i) stochastic variations in the star formation history (SFH) and (ii) the effect of spatially varying dust attenuation on the measured line and continuum emission properties.","Both scenarios can play at the same time to amplify the effect.","A stochastic star formation model (similar to realistic SFHs from hydrodynamical simulations of similar-mass galaxies) can create such observed properties if star formation is fast-varying with a correlation time of <200 Myrs given a reasonable burst amplitude of ~0.6 dex.","The total time spent in this state is less than 20 Myrs, and the likelihood of such a state to occur over 500 Myrs at z=7 is ~50%.","On the other hand, we show that a spectrum with blue UV continuum and lack of emission lines can be reproduced by a blue+red composite spectrum.","The UV continuum is emitted from dust-free density bounded HII regions (blue component), while the red component is a dust-obscured starburst with weakened emission lines due to strong differential dust attenuation between stellar and nebular emission.","Future resolving far-infrared observations with ALMA will shed light on the latter scenario."],"url":"http://arxiv.org/abs/2402.13316v1","category":"astro-ph.GA"}
{"created":"2024-02-20 14:32:52","title":"A stochastic fluid-structure interaction problem with the Navier slip boundary condition","abstract":"We prove the existence of martingale solutions to a stochastic fluid-structure interaction problem involving a viscous, incompressible fluid flow, modeled by the Navier-Stokes equations, through a deformable elastic tube modeled by shell/membrane equations. The fluid and the structure are nonlinearly coupled via the kinematic and dynamic coupling conditions at the fluid-structure interface. This article considers the case where the structure can have unrestricted displacement and explores the Navier-slip boundary condition imposed at the fluid-structure interface, displacement of which is not known a priori and is itself a part of the solution. The proof takes a constructive approach based on a Lie splitting scheme. The geometric nonlinearity stemming from the nonlinear coupling, the possibility of random fluid domain degeneracy, the potential jumps in the tangential components of the fluid and structure velocities at the moving interface and the low regularity of the structure velocity require the development of new techniques that lead to the existence of martingale solutions.","sentences":["We prove the existence of martingale solutions to a stochastic fluid-structure interaction problem involving a viscous, incompressible fluid flow, modeled by the Navier-Stokes equations, through a deformable elastic tube modeled by shell/membrane equations.","The fluid and the structure are nonlinearly coupled via the kinematic and dynamic coupling conditions at the fluid-structure interface.","This article considers the case where the structure can have unrestricted displacement and explores the Navier-slip boundary condition imposed at the fluid-structure interface, displacement of which is not known a priori and is itself a part of the solution.","The proof takes a constructive approach based on a Lie splitting scheme.","The geometric nonlinearity stemming from the nonlinear coupling, the possibility of random fluid domain degeneracy, the potential jumps in the tangential components of the fluid and structure velocities at the moving interface and the low regularity of the structure velocity require the development of new techniques that lead to the existence of martingale solutions."],"url":"http://arxiv.org/abs/2402.13303v1","category":"math.AP"}
{"created":"2024-02-20 13:47:51","title":"Enhancing Modern Supervised Word Sense Disambiguation Models by Semantic Lexical Resources","abstract":"Supervised models for Word Sense Disambiguation (WSD) currently yield to state-of-the-art results in the most popular benchmarks. Despite the recent introduction of Word Embeddings and Recurrent Neural Networks to design powerful context-related features, the interest in improving WSD models using Semantic Lexical Resources (SLRs) is mostly restricted to knowledge-based approaches. In this paper, we enhance \"modern\" supervised WSD models exploiting two popular SLRs: WordNet and WordNet Domains. We propose an effective way to introduce semantic features into the classifiers, and we consider using the SLR structure to augment the training data. We study the effect of different types of semantic features, investigating their interaction with local contexts encoded by means of mixtures of Word Embeddings or Recurrent Neural Networks, and we extend the proposed model into a novel multi-layer architecture for WSD. A detailed experimental comparison in the recent Unified Evaluation Framework (Raganato et al., 2017) shows that the proposed approach leads to supervised models that compare favourably with the state-of-the art.","sentences":["Supervised models for Word Sense Disambiguation (WSD) currently yield to state-of-the-art results in the most popular benchmarks.","Despite the recent introduction of Word Embeddings and Recurrent Neural Networks to design powerful context-related features, the interest in improving WSD models using Semantic Lexical Resources (SLRs) is mostly restricted to knowledge-based approaches.","In this paper, we enhance \"modern\" supervised WSD models exploiting two popular SLRs: WordNet and WordNet Domains.","We propose an effective way to introduce semantic features into the classifiers, and we consider using the SLR structure to augment the training data.","We study the effect of different types of semantic features, investigating their interaction with local contexts encoded by means of mixtures of Word Embeddings or Recurrent Neural Networks, and we extend the proposed model into a novel multi-layer architecture for WSD.","A detailed experimental comparison in the recent Unified Evaluation Framework (Raganato et al., 2017) shows that the proposed approach leads to supervised models that compare favourably with the state-of-the art."],"url":"http://arxiv.org/abs/2402.13302v1","category":"cs.CL"}
{"created":"2024-02-21 16:26:59","title":"Replication Study: Enhancing Hydrological Modeling with Physics-Guided Machine Learning","abstract":"Current hydrological modeling methods combine data-driven Machine Learning (ML) algorithms and traditional physics-based models to address their respective limitations incorrect parameter estimates from rigid physics-based models and the neglect of physical process constraints by ML algorithms. Despite the accuracy of ML in outcome prediction, the integration of scientific knowledge is crucial for reliable predictions. This study introduces a Physics Informed Machine Learning (PIML) model, which merges the process understanding of conceptual hydrological models with the predictive efficiency of ML algorithms. Applied to the Anandapur sub-catchment, the PIML model demonstrates superior performance in forecasting monthly streamflow and actual evapotranspiration over both standalone conceptual models and ML algorithms, ensuring physical consistency of the outputs. This study replicates the methodologies of Bhasme, P., Vagadiya, J., & Bhatia, U. (2022) from their pivotal work on Physics Informed Machine Learning for hydrological processes, utilizing their shared code and datasets to further explore the predictive capabilities in hydrological modeling.","sentences":["Current hydrological modeling methods combine data-driven Machine Learning (ML) algorithms and traditional physics-based models to address their respective limitations incorrect parameter estimates from rigid physics-based models and the neglect of physical process constraints by ML algorithms.","Despite the accuracy of ML in outcome prediction, the integration of scientific knowledge is crucial for reliable predictions.","This study introduces a Physics Informed Machine Learning (PIML) model, which merges the process understanding of conceptual hydrological models with the predictive efficiency of ML algorithms.","Applied to the Anandapur sub-catchment, the PIML model demonstrates superior performance in forecasting monthly streamflow and actual evapotranspiration over both standalone conceptual models and ML algorithms, ensuring physical consistency of the outputs.","This study replicates the methodologies of Bhasme, P., Vagadiya, J., & Bhatia, U. (2022) from their pivotal work on Physics Informed Machine Learning for hydrological processes, utilizing their shared code and datasets to further explore the predictive capabilities in hydrological modeling."],"url":"http://arxiv.org/abs/2402.13911v1","category":"cs.LG"}
{"created":"2024-02-21 13:19:58","title":"Opening the Black-Box: A Systematic Review on Explainable AI in Remote Sensing","abstract":"In recent years, black-box machine learning approaches have become a dominant modeling paradigm for knowledge extraction in Remote Sensing. Despite the potential benefits of uncovering the inner workings of these models with explainable AI, a comprehensive overview summarizing the used explainable AI methods and their objectives, findings, and challenges in Remote Sensing applications is still missing. In this paper, we address this issue by performing a systematic review to identify the key trends of how explainable AI is used in Remote Sensing and shed light on novel explainable AI approaches and emerging directions that tackle specific Remote Sensing challenges. We also reveal the common patterns of explanation interpretation, discuss the extracted scientific insights in Remote Sensing, and reflect on the approaches used for explainable AI methods evaluation. Our review provides a complete summary of the state-of-the-art in the field. Further, we give a detailed outlook on the challenges and promising research directions, representing a basis for novel methodological development and a useful starting point for new researchers in the field of explainable AI in Remote Sensing.","sentences":["In recent years, black-box machine learning approaches have become a dominant modeling paradigm for knowledge extraction in Remote Sensing.","Despite the potential benefits of uncovering the inner workings of these models with explainable AI, a comprehensive overview summarizing the used explainable AI methods and their objectives, findings, and challenges in Remote Sensing applications is still missing.","In this paper, we address this issue by performing a systematic review to identify the key trends of how explainable AI is used in Remote Sensing and shed light on novel explainable AI approaches and emerging directions that tackle specific Remote Sensing challenges.","We also reveal the common patterns of explanation interpretation, discuss the extracted scientific insights in Remote Sensing, and reflect on the approaches used for explainable AI methods evaluation.","Our review provides a complete summary of the state-of-the-art in the field.","Further, we give a detailed outlook on the challenges and promising research directions, representing a basis for novel methodological development and a useful starting point for new researchers in the field of explainable AI in Remote Sensing."],"url":"http://arxiv.org/abs/2402.13791v1","category":"cs.LG"}
{"created":"2024-02-21 11:35:19","title":"The Effect of Batch Size on Contrastive Self-Supervised Speech Representation Learning","abstract":"Foundation models in speech are often trained using many GPUs, which implicitly leads to large effective batch sizes. In this paper we study the effect of batch size on pre-training, both in terms of statistics that can be monitored during training, and in the effect on the performance of a downstream fine-tuning task. By using batch sizes varying from 87.5 seconds to 80 minutes of speech we show that, for a fixed amount of iterations, larger batch sizes result in better pre-trained models. However, there is lower limit for stability, and an upper limit for effectiveness. We then show that the quality of the pre-trained model depends mainly on the amount of speech data seen during training, i.e., on the product of batch size and number of iterations. All results are produced with an independent implementation of the wav2vec 2.0 architecture, which to a large extent reproduces the results of the original work (arXiv:2006.11477). Our extensions can help researchers choose effective operating conditions when studying self-supervised learning in speech, and hints towards benchmarking self-supervision with a fixed amount of seen data. Code and model checkpoints are available at https://github.com/nikvaessen/w2v2-batch-size.","sentences":["Foundation models in speech are often trained using many GPUs, which implicitly leads to large effective batch sizes.","In this paper we study the effect of batch size on pre-training, both in terms of statistics that can be monitored during training, and in the effect on the performance of a downstream fine-tuning task.","By using batch sizes varying from 87.5 seconds to 80 minutes of speech we show that, for a fixed amount of iterations, larger batch sizes result in better pre-trained models.","However, there is lower limit for stability, and an upper limit for effectiveness.","We then show that the quality of the pre-trained model depends mainly on the amount of speech data seen during training, i.e., on the product of batch size and number of iterations.","All results are produced with an independent implementation of the wav2vec 2.0 architecture, which to a large extent reproduces the results of the original work (arXiv:2006.11477).","Our extensions can help researchers choose effective operating conditions when studying self-supervised learning in speech, and hints towards benchmarking self-supervision with a fixed amount of seen data.","Code and model checkpoints are available at https://github.com/nikvaessen/w2v2-batch-size."],"url":"http://arxiv.org/abs/2402.13723v1","category":"cs.SD"}
{"created":"2024-02-21 09:45:47","title":"Finding Incompatibles Blocks for Reliable JPEG Steganalysis","abstract":"This article presents a refined notion of incompatible JPEG images for a quality factor of 100. It can be used to detect the presence of steganographic schemes embedding in DCT coefficients. We show that, within the JPEG pipeline, the combination of the DCT transform with the quantization function can map several distinct blocks in the pixel domain to the same block in the DCT domain. However, not every DCT block can be obtained: we call those blocks incompatible. In particular, incompatibility can happen when DCT coefficients are manually modified to embed a message. We show that the problem of distinguishing compatible blocks from incompatible ones is an inverse problem with or without solution and we propose two different methods to solve it. The first one is heuristic-based, fast to find a solution if it exists. The second is formulated as an Integer Linear Programming problem and can detect incompatible blocks only for a specific DCT transform in a reasonable amount of time. We show that the probability for a block to become incompatible only relies on the number of modifications. Finally, using the heuristic algorithm we can derive a Likelihood Ratio Test depending on the number of compatible blocks per image to perform steganalysis. We simulate the result of this test and show that it outperforms a deep learning detector e-SRNet for every payload between 0.001 and 0.01 bpp by using only 10% of the blocks from 256x256 images. A Selection-Channel-Aware version of the test is even more powerful and outperforms e-SRNet while using only 1% of the blocks.","sentences":["This article presents a refined notion of incompatible JPEG images for a quality factor of 100.","It can be used to detect the presence of steganographic schemes embedding in DCT coefficients.","We show that, within the JPEG pipeline, the combination of the DCT transform with the quantization function can map several distinct blocks in the pixel domain to the same block in the DCT domain.","However, not every DCT block can be obtained: we call those blocks incompatible.","In particular, incompatibility can happen when DCT coefficients are manually modified to embed a message.","We show that the problem of distinguishing compatible blocks from incompatible ones is an inverse problem with or without solution and we propose two different methods to solve it.","The first one is heuristic-based, fast to find a solution if it exists.","The second is formulated as an Integer Linear Programming problem and can detect incompatible blocks only for a specific DCT transform in a reasonable amount of time.","We show that the probability for a block to become incompatible only relies on the number of modifications.","Finally, using the heuristic algorithm we can derive a Likelihood Ratio Test depending on the number of compatible blocks per image to perform steganalysis.","We simulate the result of this test and show that it outperforms a deep learning detector e-SRNet for every payload between 0.001 and 0.01 bpp by using only 10% of the blocks from 256x256 images.","A Selection-Channel-Aware version of the test is even more powerful and outperforms e-SRNet while using only 1% of the blocks."],"url":"http://arxiv.org/abs/2402.13660v1","category":"cs.CR"}
{"created":"2024-02-21 09:38:17","title":"PQA: Zero-shot Protein Question Answering for Free-form Scientific Enquiry with Large Language Models","abstract":"We introduce the novel task of zero-shot Protein Question Answering (PQA) for free-form scientific enquiry. Given a previously unseen protein sequence and a natural language question, the task is to deliver a scientifically accurate answer. This task not only supports future biological research, but could also provide a test bed for assessing the scientific precision of large language models (LLMs). We contribute the first specialized dataset for PQA model training, containing 257K protein sequences annotated with 1.97M scientific question-answer pairs. Additionally, we propose and study several novel biologically relevant benchmarks for scientific PQA. Employing two robust multi-modal architectures, we establish an initial state-of-the-art performance for PQA and reveal key performance factors through ablation studies. Our comprehensive PQA framework, named Pika, including dataset, code, model checkpoints, and a user-friendly demo, is openly accessible on github.com/EMCarrami/Pika, promoting wider research and application in the field.","sentences":["We introduce the novel task of zero-shot Protein Question Answering (PQA) for free-form scientific enquiry.","Given a previously unseen protein sequence and a natural language question, the task is to deliver a scientifically accurate answer.","This task not only supports future biological research, but could also provide a test bed for assessing the scientific precision of large language models (LLMs).","We contribute the first specialized dataset for PQA model training, containing 257K protein sequences annotated with 1.97M scientific question-answer pairs.","Additionally, we propose and study several novel biologically relevant benchmarks for scientific PQA.","Employing two robust multi-modal architectures, we establish an initial state-of-the-art performance for PQA and reveal key performance factors through ablation studies.","Our comprehensive PQA framework, named Pika, including dataset, code, model checkpoints, and a user-friendly demo, is openly accessible on github.com/EMCarrami/Pika, promoting wider research and application in the field."],"url":"http://arxiv.org/abs/2402.13653v1","category":"cs.LG"}
{"created":"2024-02-21 09:34:45","title":"Learning control strategy in soft robotics through a set of configuration spaces","abstract":"The ability of a soft robot to perform specific tasks is determined by its contact configuration, and transitioning between configurations is often necessary to reach a desired position or manipulate an object. Based on this observation, we propose a method for controlling soft robots that involves defining a graph of configuration spaces. Different agents, whether learned or not (convex optimization, expert trajectory, and collision detection), use the structure of the graph to solve the desired task. The graph and the agents are part of the prior knowledge that is intuitively integrated into the learning process. They are used to combine different optimization methods, improve sample efficiency, and provide interpretability. We construct the graph based on the contact configurations and demonstrate its effectiveness through two scenarios, a deformable beam in contact with its environment and a soft manipulator, where it outperforms the baseline in terms of stability, learning speed, and interpretability.","sentences":["The ability of a soft robot to perform specific tasks is determined by its contact configuration, and transitioning between configurations is often necessary to reach a desired position or manipulate an object.","Based on this observation, we propose a method for controlling soft robots that involves defining a graph of configuration spaces.","Different agents, whether learned or not (convex optimization, expert trajectory, and collision detection), use the structure of the graph to solve the desired task.","The graph and the agents are part of the prior knowledge that is intuitively integrated into the learning process.","They are used to combine different optimization methods, improve sample efficiency, and provide interpretability.","We construct the graph based on the contact configurations and demonstrate its effectiveness through two scenarios, a deformable beam in contact with its environment and a soft manipulator, where it outperforms the baseline in terms of stability, learning speed, and interpretability."],"url":"http://arxiv.org/abs/2402.13649v1","category":"cs.RO"}
{"created":"2024-02-21 09:27:44","title":"A Large Dimensional Analysis of Multi-task Semi-Supervised Learning","abstract":"This article conducts a large dimensional study of a simple yet quite versatile classification model, encompassing at once multi-task and semi-supervised learning, and taking into account uncertain labeling. Using tools from random matrix theory, we characterize the asymptotics of some key functionals, which allows us on the one hand to predict the performances of the algorithm, and on the other hand to reveal some counter-intuitive guidance on how to use it efficiently. The model, powerful enough to provide good performance guarantees, is also straightforward enough to provide strong insights into its behavior.","sentences":["This article conducts a large dimensional study of a simple yet quite versatile classification model, encompassing at once multi-task and semi-supervised learning, and taking into account uncertain labeling.","Using tools from random matrix theory, we characterize the asymptotics of some key functionals, which allows us on the one hand to predict the performances of the algorithm, and on the other hand to reveal some counter-intuitive guidance on how to use it efficiently.","The model, powerful enough to provide good performance guarantees, is also straightforward enough to provide strong insights into its behavior."],"url":"http://arxiv.org/abs/2402.13646v1","category":"stat.ML"}
{"created":"2024-02-21 09:17:51","title":"A Unified Framework and Dataset for Assessing Gender Bias in Vision-Language Models","abstract":"Large vision-language models (VLMs) are widely getting adopted in industry and academia. In this work we build a unified framework to systematically evaluate gender-profession bias in VLMs. Our evaluation encompasses all supported inference modes of the recent VLMs, including image-to-text, text-to-text, text-to-image, and image-to-image. We construct a synthetic, high-quality dataset of text and images that blurs gender distinctions across professional actions to benchmark gender bias. In our benchmarking of recent vision-language models (VLMs), we observe that different input-output modalities result in distinct bias magnitudes and directions. We hope our work will help guide future progress in improving VLMs to learn socially unbiased representations. We will release our data and code.","sentences":["Large vision-language models (VLMs) are widely getting adopted in industry and academia.","In this work we build a unified framework to systematically evaluate gender-profession bias in VLMs.","Our evaluation encompasses all supported inference modes of the recent VLMs, including image-to-text, text-to-text, text-to-image, and image-to-image.","We construct a synthetic, high-quality dataset of text and images that blurs gender distinctions across professional actions to benchmark gender bias.","In our benchmarking of recent vision-language models (VLMs), we observe that different input-output modalities result in distinct bias magnitudes and directions.","We hope our work will help guide future progress in improving VLMs to learn socially unbiased representations.","We will release our data and code."],"url":"http://arxiv.org/abs/2402.13636v1","category":"cs.CV"}
{"created":"2024-02-21 08:29:26","title":"Overview of the VLSP 2023 -- ComOM Shared Task: A Data Challenge for Comparative Opinion Mining from Vietnamese Product Reviews","abstract":"This paper presents a comprehensive overview of the Comparative Opinion Mining from Vietnamese Product Reviews shared task (ComOM), held as part of the 10$^{th}$ International Workshop on Vietnamese Language and Speech Processing (VLSP 2023). The primary objective of this shared task is to advance the field of natural language processing by developing techniques that proficiently extract comparative opinions from Vietnamese product reviews. Participants are challenged to propose models that adeptly extract a comparative \"quintuple\" from a comparative sentence, encompassing Subject, Object, Aspect, Predicate, and Comparison Type Label. We construct a human-annotated dataset comprising $120$ documents, encompassing $7427$ non-comparative sentences and $2468$ comparisons within $1798$ sentences. Participating models undergo evaluation and ranking based on the Exact match macro-averaged quintuple F1 score.","sentences":["This paper presents a comprehensive overview of the Comparative Opinion Mining from Vietnamese Product Reviews shared task (ComOM), held as part of the 10$^{th}$ International Workshop on Vietnamese Language and Speech Processing (VLSP 2023).","The primary objective of this shared task is to advance the field of natural language processing by developing techniques that proficiently extract comparative opinions from Vietnamese product reviews.","Participants are challenged to propose models that adeptly extract a comparative \"quintuple\" from a comparative sentence, encompassing Subject, Object, Aspect, Predicate, and Comparison Type Label.","We construct a human-annotated dataset comprising $120$ documents, encompassing $7427$ non-comparative sentences and $2468$ comparisons within $1798$ sentences.","Participating models undergo evaluation and ranking based on the Exact match macro-averaged quintuple F1 score."],"url":"http://arxiv.org/abs/2402.13613v1","category":"cs.CL"}
{"created":"2024-02-21 07:18:23","title":"Learning Pixel-wise Continuous Depth Representation via Clustering for Depth Completion","abstract":"Depth completion is a long-standing challenge in computer vision, where classification-based methods have made tremendous progress in recent years. However, most existing classification-based methods rely on pre-defined pixel-shared and discrete depth values as depth categories. This representation fails to capture the continuous depth values that conform to the real depth distribution, leading to depth smearing in boundary regions. To address this issue, we revisit depth completion from the clustering perspective and propose a novel clustering-based framework called CluDe which focuses on learning the pixel-wise and continuous depth representation. The key idea of CluDe is to iteratively update the pixel-shared and discrete depth representation to its corresponding pixel-wise and continuous counterpart, driven by the real depth distribution. Specifically, CluDe first utilizes depth value clustering to learn a set of depth centers as the depth representation. While these depth centers are pixel-shared and discrete, they are more in line with the real depth distribution compared to pre-defined depth categories. Then, CluDe estimates offsets for these depth centers, enabling their dynamic adjustment along the depth axis of the depth distribution to generate the pixel-wise and continuous depth representation. Extensive experiments demonstrate that CluDe successfully reduces depth smearing around object boundaries by utilizing pixel-wise and continuous depth representation. Furthermore, CluDe achieves state-of-the-art performance on the VOID datasets and outperforms classification-based methods on the KITTI dataset.","sentences":["Depth completion is a long-standing challenge in computer vision, where classification-based methods have made tremendous progress in recent years.","However, most existing classification-based methods rely on pre-defined pixel-shared and discrete depth values as depth categories.","This representation fails to capture the continuous depth values that conform to the real depth distribution, leading to depth smearing in boundary regions.","To address this issue, we revisit depth completion from the clustering perspective and propose a novel clustering-based framework called CluDe which focuses on learning the pixel-wise and continuous depth representation.","The key idea of CluDe is to iteratively update the pixel-shared and discrete depth representation to its corresponding pixel-wise and continuous counterpart, driven by the real depth distribution.","Specifically, CluDe first utilizes depth value clustering to learn a set of depth centers as the depth representation.","While these depth centers are pixel-shared and discrete, they are more in line with the real depth distribution compared to pre-defined depth categories.","Then, CluDe estimates offsets for these depth centers, enabling their dynamic adjustment along the depth axis of the depth distribution to generate the pixel-wise and continuous depth representation.","Extensive experiments demonstrate that CluDe successfully reduces depth smearing around object boundaries by utilizing pixel-wise and continuous depth representation.","Furthermore, CluDe achieves state-of-the-art performance on the VOID datasets and outperforms classification-based methods on the KITTI dataset."],"url":"http://arxiv.org/abs/2402.13579v1","category":"cs.CV"}
{"created":"2024-02-21 06:55:20","title":"Event-aware Video Corpus Moment Retrieval","abstract":"Video Corpus Moment Retrieval (VCMR) is a practical video retrieval task focused on identifying a specific moment within a vast corpus of untrimmed videos using the natural language query. Existing methods for VCMR typically rely on frame-aware video retrieval, calculating similarities between the query and video frames to rank videos based on maximum frame similarity.However, this approach overlooks the semantic structure embedded within the information between frames, namely, the event, a crucial element for human comprehension of videos. Motivated by this, we propose EventFormer, a model that explicitly utilizes events within videos as fundamental units for video retrieval. The model extracts event representations through event reasoning and hierarchical event encoding. The event reasoning module groups consecutive and visually similar frame representations into events, while the hierarchical event encoding encodes information at both the frame and event levels. We also introduce anchor multi-head self-attenion to encourage Transformer to capture the relevance of adjacent content in the video. The training of EventFormer is conducted by two-branch contrastive learning and dual optimization for two sub-tasks of VCMR. Extensive experiments on TVR, ANetCaps, and DiDeMo benchmarks show the effectiveness and efficiency of EventFormer in VCMR, achieving new state-of-the-art results. Additionally, the effectiveness of EventFormer is also validated on partially relevant video retrieval task.","sentences":["Video Corpus Moment Retrieval (VCMR) is a practical video retrieval task focused on identifying a specific moment within a vast corpus of untrimmed videos using the natural language query.","Existing methods for VCMR typically rely on frame-aware video retrieval, calculating similarities between the query and video frames to rank videos based on maximum frame similarity.","However, this approach overlooks the semantic structure embedded within the information between frames, namely, the event, a crucial element for human comprehension of videos.","Motivated by this, we propose EventFormer, a model that explicitly utilizes events within videos as fundamental units for video retrieval.","The model extracts event representations through event reasoning and hierarchical event encoding.","The event reasoning module groups consecutive and visually similar frame representations into events, while the hierarchical event encoding encodes information at both the frame and event levels.","We also introduce anchor multi-head self-attenion to encourage Transformer to capture the relevance of adjacent content in the video.","The training of EventFormer is conducted by two-branch contrastive learning and dual optimization for two sub-tasks of VCMR.","Extensive experiments on TVR, ANetCaps, and DiDeMo benchmarks show the effectiveness and efficiency of EventFormer in VCMR, achieving new state-of-the-art results.","Additionally, the effectiveness of EventFormer is also validated on partially relevant video retrieval task."],"url":"http://arxiv.org/abs/2402.13566v1","category":"cs.CV"}
{"created":"2024-02-21 06:14:04","title":"Graph Representation of Narrative Context: Coherence Dependency via Retrospective Questions","abstract":"This work introduces a novel and practical paradigm for narrative comprehension, stemming from the observation that individual passages within narratives are often cohesively related than being isolated. We therefore propose to formulate a graph upon narratives dubbed NARCO that depicts a task-agnostic coherence dependency of the entire context. Especially, edges in NARCO encompass retrospective free-form questions between two context snippets reflecting high-level coherent relations, inspired by the cognitive perception of humans who constantly reinstate relevant events from prior context. Importantly, our graph is instantiated through our designed two-stage LLM prompting, thereby without reliance on human annotations. We present three unique studies on its practical utility, examining the edge efficacy via recap identification, local context augmentation via plot retrieval, and broader applications exemplified by long document QA. Experiments suggest that our approaches leveraging NARCO yield performance boost across all three tasks.","sentences":["This work introduces a novel and practical paradigm for narrative comprehension, stemming from the observation that individual passages within narratives are often cohesively related than being isolated.","We therefore propose to formulate a graph upon narratives dubbed NARCO that depicts a task-agnostic coherence dependency of the entire context.","Especially, edges in NARCO encompass retrospective free-form questions between two context snippets reflecting high-level coherent relations, inspired by the cognitive perception of humans who constantly reinstate relevant events from prior context.","Importantly, our graph is instantiated through our designed two-stage LLM prompting, thereby without reliance on human annotations.","We present three unique studies on its practical utility, examining the edge efficacy via recap identification, local context augmentation via plot retrieval, and broader applications exemplified by long document QA.","Experiments suggest that our approaches leveraging NARCO yield performance boost across all three tasks."],"url":"http://arxiv.org/abs/2402.13551v1","category":"cs.CL"}
{"created":"2024-02-21 06:04:53","title":"ActiveRAG: Revealing the Treasures of Knowledge via Active Learning","abstract":"Retrieval Augmented Generation (RAG) has introduced a new paradigm for Large Language Models (LLMs), aiding in the resolution of knowledge-intensive tasks. However, current RAG models position LLMs as passive knowledge receptors, thereby restricting their capacity for learning and comprehending external knowledge. In this paper, we present ActiveRAG, an innovative RAG framework that shifts from passive knowledge acquisition to an active learning mechanism. This approach utilizes the Knowledge Construction mechanism to develop a deeper understanding of external knowledge by associating it with previously acquired or memorized knowledge. Subsequently, it designs the Cognitive Nexus mechanism to incorporate the outcomes from both chains of thought and knowledge construction, thereby calibrating the intrinsic cognition of LLMs. Our experimental results demonstrate that ActiveRAG surpasses previous RAG models, achieving a 5% improvement on question-answering datasets. All data and codes are available at https://github.com/OpenMatch/ActiveRAG.","sentences":["Retrieval Augmented Generation (RAG) has introduced a new paradigm for Large Language Models (LLMs), aiding in the resolution of knowledge-intensive tasks.","However, current RAG models position LLMs as passive knowledge receptors, thereby restricting their capacity for learning and comprehending external knowledge.","In this paper, we present ActiveRAG, an innovative RAG framework that shifts from passive knowledge acquisition to an active learning mechanism.","This approach utilizes the Knowledge Construction mechanism to develop a deeper understanding of external knowledge by associating it with previously acquired or memorized knowledge.","Subsequently, it designs the Cognitive Nexus mechanism to incorporate the outcomes from both chains of thought and knowledge construction, thereby calibrating the intrinsic cognition of LLMs.","Our experimental results demonstrate that ActiveRAG surpasses previous RAG models, achieving a 5% improvement on question-answering datasets.","All data and codes are available at https://github.com/OpenMatch/ActiveRAG."],"url":"http://arxiv.org/abs/2402.13547v1","category":"cs.CL"}
{"created":"2024-02-21 05:54:42","title":"A Two-Stage Dual-Path Framework for Text Tampering Detection and Recognition","abstract":"Document tamper detection has always been an important aspect of tamper detection. Before the advent of deep learning, document tamper detection was difficult. We have made some explorations in the field of text tamper detection based on deep learning. Our Ps tamper detection method includes three steps: feature assistance, audit point positioning, and tamper recognition. It involves hierarchical filtering and graded output (tampered/suspected tampered/untampered). By combining artificial tamper data features, we simulate and augment data samples in various scenarios (cropping with noise addition/replacement, single character/space replacement, smearing/splicing, brightness/contrast adjustment, etc.). The auxiliary features include exif/binary stream keyword retrieval/noise, which are used for branch detection based on the results. Audit point positioning uses detection frameworks and controls thresholds for high and low density detection. Tamper recognition employs a dual-path dual-stream recognition network, with RGB and ELA stream feature extraction. After dimensionality reduction through self-correlation percentile pooling, the fused output is processed through vlad, yielding an accuracy of 0.804, recall of 0.659, and precision of 0.913.","sentences":["Document tamper detection has always been an important aspect of tamper detection.","Before the advent of deep learning, document tamper detection was difficult.","We have made some explorations in the field of text tamper detection based on deep learning.","Our Ps tamper detection method includes three steps: feature assistance, audit point positioning, and tamper recognition.","It involves hierarchical filtering and graded output (tampered/suspected tampered/untampered).","By combining artificial tamper data features, we simulate and augment data samples in various scenarios (cropping with noise addition/replacement, single character/space replacement, smearing/splicing, brightness/contrast adjustment, etc.).","The auxiliary features include exif/binary stream keyword retrieval/noise, which are used for branch detection based on the results.","Audit point positioning uses detection frameworks and controls thresholds for high and low density detection.","Tamper recognition employs a dual-path dual-stream recognition network, with RGB and ELA stream feature extraction.","After dimensionality reduction through self-correlation percentile pooling, the fused output is processed through vlad, yielding an accuracy of 0.804, recall of 0.659, and precision of 0.913."],"url":"http://arxiv.org/abs/2402.13545v1","category":"cs.CV"}
{"created":"2024-02-21 05:26:17","title":"EffLoc: Lightweight Vision Transformer for Efficient 6-DOF Camera Relocalization","abstract":"Camera relocalization is pivotal in computer vision, with applications in AR, drones, robotics, and autonomous driving. It estimates 3D camera position and orientation (6-DoF) from images. Unlike traditional methods like SLAM, recent strides use deep learning for direct end-to-end pose estimation. We propose EffLoc, a novel efficient Vision Transformer for single-image camera relocalization. EffLoc's hierarchical layout, memory-bound self-attention, and feed-forward layers boost memory efficiency and inter-channel communication. Our introduced sequential group attention (SGA) module enhances computational efficiency by diversifying input features, reducing redundancy, and expanding model capacity. EffLoc excels in efficiency and accuracy, outperforming prior methods, such as AtLoc and MapNet. It thrives on large-scale outdoor car-driving scenario, ensuring simplicity, end-to-end trainability, and eliminating handcrafted loss functions.","sentences":["Camera relocalization is pivotal in computer vision, with applications in AR, drones, robotics, and autonomous driving.","It estimates 3D camera position and orientation (6-DoF) from images.","Unlike traditional methods like SLAM, recent strides use deep learning for direct end-to-end pose estimation.","We propose EffLoc, a novel efficient Vision Transformer for single-image camera relocalization.","EffLoc's hierarchical layout, memory-bound self-attention, and feed-forward layers boost memory efficiency and inter-channel communication.","Our introduced sequential group attention (SGA) module enhances computational efficiency by diversifying input features, reducing redundancy, and expanding model capacity.","EffLoc excels in efficiency and accuracy, outperforming prior methods, such as AtLoc and MapNet.","It thrives on large-scale outdoor car-driving scenario, ensuring simplicity, end-to-end trainability, and eliminating handcrafted loss functions."],"url":"http://arxiv.org/abs/2402.13537v1","category":"cs.CV"}
{"created":"2024-02-21 02:54:00","title":"Stealthy Adversarial Attacks on Stochastic Multi-Armed Bandits","abstract":"Adversarial attacks against stochastic multi-armed bandit (MAB) algorithms have been extensively studied in the literature. In this work, we focus on reward poisoning attacks and find most existing attacks can be easily detected by our proposed detection method based on the test of homogeneity, due to their aggressive nature in reward manipulations. This motivates us to study the notion of stealthy attack against stochastic MABs and investigate the resulting attackability. Our analysis shows that against two popularly employed MAB algorithms, UCB1 and $\\epsilon$-greedy, the success of a stealthy attack depends on the environmental conditions and the realized reward of the arm pulled in the first round. We also analyze the situation for general MAB algorithms equipped with our attack detection method and find that it is possible to have a stealthy attack that almost always succeeds. This brings new insights into the security risks of MAB algorithms.","sentences":["Adversarial attacks against stochastic multi-armed bandit (MAB) algorithms have been extensively studied in the literature.","In this work, we focus on reward poisoning attacks and find most existing attacks can be easily detected by our proposed detection method based on the test of homogeneity, due to their aggressive nature in reward manipulations.","This motivates us to study the notion of stealthy attack against stochastic MABs and investigate the resulting attackability.","Our analysis shows that against two popularly employed MAB algorithms, UCB1 and $\\epsilon$-greedy, the success of a stealthy attack depends on the environmental conditions and the realized reward of the arm pulled in the first round.","We also analyze the situation for general MAB algorithms equipped with our attack detection method and find that it is possible to have a stealthy attack that almost always succeeds.","This brings new insights into the security risks of MAB algorithms."],"url":"http://arxiv.org/abs/2402.13487v1","category":"cs.LG"}
{"created":"2024-02-21 02:51:07","title":"ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel Decoding","abstract":"Recent advancements in generative large language models (LLMs) have significantly boosted the performance in natural language processing tasks. However, their efficiency is hampered by the inherent limitations in autoregressive token generation. While parallel decoding with token tree verification, e.g., Medusa, has been proposed to improve decoding parallelism and efficiency, it often struggles with maintaining contextual relationships due to its independent token prediction approach and incurs significant verification overhead, especially with large tree sizes and batch processing. In this paper, we propose ProPD, an efficient LLM parallel decoding framework based on dynamic token tree pruning and generation. ProPD features an advanced early pruning mechanism to efficiently eliminate unpromising token sequences to improve verification efficiency. Additionally, it introduces a dynamic token tree generation algorithm to balance the computation and parallelism of the verification phase in real-time and maximize the overall efficiency across different batch sizes, sequence lengths, and tasks, etc. We verify ProPD across a diverse set of datasets, LLMs, and batch sizes and demonstrate ProPD consistently outperforms existing decoding algorithms by 1.1-3.2x.","sentences":["Recent advancements in generative large language models (LLMs) have significantly boosted the performance in natural language processing tasks.","However, their efficiency is hampered by the inherent limitations in autoregressive token generation.","While parallel decoding with token tree verification, e.g., Medusa, has been proposed to improve decoding parallelism and efficiency, it often struggles with maintaining contextual relationships due to its independent token prediction approach and incurs significant verification overhead, especially with large tree sizes and batch processing.","In this paper, we propose ProPD, an efficient LLM parallel decoding framework based on dynamic token tree pruning and generation.","ProPD features an advanced early pruning mechanism to efficiently eliminate unpromising token sequences to improve verification efficiency.","Additionally, it introduces a dynamic token tree generation algorithm to balance the computation and parallelism of the verification phase in real-time and maximize the overall efficiency across different batch sizes, sequence lengths, and tasks, etc.","We verify ProPD across a diverse set of datasets, LLMs, and batch sizes and demonstrate ProPD consistently outperforms existing decoding algorithms by 1.1-3.2x."],"url":"http://arxiv.org/abs/2402.13485v1","category":"cs.LG"}
{"created":"2024-02-21 02:06:45","title":"Learning Highly Dynamic Behaviors for Quadrupedal Robots","abstract":"Learning highly dynamic behaviors for robots has been a longstanding challenge. Traditional approaches have demonstrated robust locomotion, but the exhibited behaviors lack diversity and agility. They employ approximate models, which lead to compromises in performance. Data-driven approaches have been shown to reproduce agile behaviors of animals, but typically have not been able to learn highly dynamic behaviors. In this paper, we propose a learning-based approach to enable robots to learn highly dynamic behaviors from animal motion data. The learned controller is deployed on a quadrupedal robot and the results show that the controller is able to reproduce highly dynamic behaviors including sprinting, jumping and sharp turning. Various behaviors can be activated through human interaction using a stick with markers attached to it. Based on the motion pattern of the stick, the robot exhibits walking, running, sitting and jumping, much like the way humans interact with a pet.","sentences":["Learning highly dynamic behaviors for robots has been a longstanding challenge.","Traditional approaches have demonstrated robust locomotion, but the exhibited behaviors lack diversity and agility.","They employ approximate models, which lead to compromises in performance.","Data-driven approaches have been shown to reproduce agile behaviors of animals, but typically have not been able to learn highly dynamic behaviors.","In this paper, we propose a learning-based approach to enable robots to learn highly dynamic behaviors from animal motion data.","The learned controller is deployed on a quadrupedal robot and the results show that the controller is able to reproduce highly dynamic behaviors including sprinting, jumping and sharp turning.","Various behaviors can be activated through human interaction using a stick with markers attached to it.","Based on the motion pattern of the stick, the robot exhibits walking, running, sitting and jumping, much like the way humans interact with a pet."],"url":"http://arxiv.org/abs/2402.13473v1","category":"cs.RO"}
{"created":"2024-02-21 01:57:58","title":"How Important is Domain Specificity in Language Models and Instruction Finetuning for Biomedical Relation Extraction?","abstract":"Cutting edge techniques developed in the general NLP domain are often subsequently applied to the high-value, data-rich biomedical domain. The past few years have seen generative language models (LMs), instruction finetuning, and few-shot learning become foci of NLP research. As such, generative LMs pretrained on biomedical corpora have proliferated and biomedical instruction finetuning has been attempted as well, all with the hope that domain specificity improves performance on downstream tasks. Given the nontrivial effort in training such models, we investigate what, if any, benefits they have in the key biomedical NLP task of relation extraction. Specifically, we address two questions: (1) Do LMs trained on biomedical corpora outperform those trained on general domain corpora? (2) Do models instruction finetuned on biomedical datasets outperform those finetuned on assorted datasets or those simply pretrained? We tackle these questions using existing LMs, testing across four datasets. In a surprising result, general-domain models typically outperformed biomedical-domain models. However, biomedical instruction finetuning improved performance to a similar degree as general instruction finetuning, despite having orders of magnitude fewer instructions. Our findings suggest it may be more fruitful to focus research effort on larger-scale biomedical instruction finetuning of general LMs over building domain-specific biomedical LMs","sentences":["Cutting edge techniques developed in the general NLP domain are often subsequently applied to the high-value, data-rich biomedical domain.","The past few years have seen generative language models (LMs), instruction finetuning, and few-shot learning become foci of NLP research.","As such, generative LMs pretrained on biomedical corpora have proliferated and biomedical instruction finetuning has been attempted as well, all with the hope that domain specificity improves performance on downstream tasks.","Given the nontrivial effort in training such models, we investigate what, if any, benefits they have in the key biomedical NLP task of relation extraction.","Specifically, we address two questions: (1) Do LMs trained on biomedical corpora outperform those trained on general domain corpora?","(2) Do models instruction finetuned on biomedical datasets outperform those finetuned on assorted datasets or those simply pretrained?","We tackle these questions using existing LMs, testing across four datasets.","In a surprising result, general-domain models typically outperformed biomedical-domain models.","However, biomedical instruction finetuning improved performance to a similar degree as general instruction finetuning, despite having orders of magnitude fewer instructions.","Our findings suggest it may be more fruitful to focus research effort on larger-scale biomedical instruction finetuning of general LMs over building domain-specific biomedical LMs"],"url":"http://arxiv.org/abs/2402.13470v1","category":"cs.CL"}
{"created":"2024-02-21 01:54:58","title":"STENCIL: Submodular Mutual Information Based Weak Supervision for Cold-Start Active Learning","abstract":"As supervised fine-tuning of pre-trained models within NLP applications increases in popularity, larger corpora of annotated data are required, especially with increasing parameter counts in large language models. Active learning, which attempts to mine and annotate unlabeled instances to improve model performance maximally fast, is a common choice for reducing the annotation cost; however, most methods typically ignore class imbalance and either assume access to initial annotated data or require multiple rounds of active learning selection before improving rare classes. We present STENCIL, which utilizes a set of text exemplars and the recently proposed submodular mutual information to select a set of weakly labeled rare-class instances that are then strongly labeled by an annotator. We show that STENCIL improves overall accuracy by $10\\%-24\\%$ and rare-class F-1 score by $17\\%-40\\%$ on multiple text classification datasets over common active learning methods within the class-imbalanced cold-start setting.","sentences":["As supervised fine-tuning of pre-trained models within NLP applications increases in popularity, larger corpora of annotated data are required, especially with increasing parameter counts in large language models.","Active learning, which attempts to mine and annotate unlabeled instances to improve model performance maximally fast, is a common choice for reducing the annotation cost; however, most methods typically ignore class imbalance and either assume access to initial annotated data or require multiple rounds of active learning selection before improving rare classes.","We present STENCIL, which utilizes a set of text exemplars and the recently proposed submodular mutual information to select a set of weakly labeled rare-class instances that are then strongly labeled by an annotator.","We show that STENCIL improves overall accuracy by $10\\%-24\\%$ and rare-class F-1 score by $17\\%-40\\%$ on multiple text classification datasets over common active learning methods within the class-imbalanced cold-start setting."],"url":"http://arxiv.org/abs/2402.13468v1","category":"cs.LG"}
{"created":"2024-02-21 01:30:03","title":"Learning to Poison Large Language Models During Instruction Tuning","abstract":"The advent of Large Language Models (LLMs) has marked significant achievements in language processing and reasoning capabilities. Despite their advancements, LLMs face vulnerabilities to data poisoning attacks, where adversaries insert backdoor triggers into training data to manipulate outputs for malicious purposes. This work further identifies additional security risks in LLMs by designing a new data poisoning attack tailored to exploit the instruction tuning process. We propose a novel gradient-guided backdoor trigger learning approach to identify adversarial triggers efficiently, ensuring an evasion of detection by conventional defenses while maintaining content integrity. Through experimental validation across various LLMs and tasks, our strategy demonstrates a high success rate in compromising model outputs; poisoning only 1\\% of 4,000 instruction tuning samples leads to a Performance Drop Rate (PDR) of around 80\\%. Our work highlights the need for stronger defenses against data poisoning attack, offering insights into safeguarding LLMs against these more sophisticated attacks. The source code can be found on this GitHub repository: https://github.com/RookieZxy/GBTL/blob/main/README.md.","sentences":["The advent of Large Language Models (LLMs) has marked significant achievements in language processing and reasoning capabilities.","Despite their advancements, LLMs face vulnerabilities to data poisoning attacks, where adversaries insert backdoor triggers into training data to manipulate outputs for malicious purposes.","This work further identifies additional security risks in LLMs by designing a new data poisoning attack tailored to exploit the instruction tuning process.","We propose a novel gradient-guided backdoor trigger learning approach to identify adversarial triggers efficiently, ensuring an evasion of detection by conventional defenses while maintaining content integrity.","Through experimental validation across various LLMs and tasks, our strategy demonstrates a high success rate in compromising model outputs; poisoning only 1\\% of 4,000 instruction tuning samples leads to a Performance Drop Rate (PDR) of around 80\\%.","Our work highlights the need for stronger defenses against data poisoning attack, offering insights into safeguarding LLMs against these more sophisticated attacks.","The source code can be found on this GitHub repository: https://github.com/RookieZxy/GBTL/blob/main/README.md."],"url":"http://arxiv.org/abs/2402.13459v1","category":"cs.LG"}
{"created":"2024-02-21 01:18:32","title":"Theoretical Analysis of Submodular Information Measures for Targeted Data Subset Selection","abstract":"With increasing volume of data being used across machine learning tasks, the capability to target specific subsets of data becomes more important. To aid in this capability, the recently proposed Submodular Mutual Information (SMI) has been effectively applied across numerous tasks in literature to perform targeted subset selection with the aid of a exemplar query set. However, all such works are deficient in providing theoretical guarantees for SMI in terms of its sensitivity to a subset's relevance and coverage of the targeted data. For the first time, we provide such guarantees by deriving similarity-based bounds on quantities related to relevance and coverage of the targeted data. With these bounds, we show that the SMI functions, which have empirically shown success in multiple applications, are theoretically sound in achieving good query relevance and query coverage.","sentences":["With increasing volume of data being used across machine learning tasks, the capability to target specific subsets of data becomes more important.","To aid in this capability, the recently proposed Submodular Mutual Information (SMI) has been effectively applied across numerous tasks in literature to perform targeted subset selection with the aid of a exemplar query set.","However, all such works are deficient in providing theoretical guarantees for SMI in terms of its sensitivity to a subset's relevance and coverage of the targeted data.","For the first time, we provide such guarantees by deriving similarity-based bounds on quantities related to relevance and coverage of the targeted data.","With these bounds, we show that the SMI functions, which have empirically shown success in multiple applications, are theoretically sound in achieving good query relevance and query coverage."],"url":"http://arxiv.org/abs/2402.13454v1","category":"cs.LG"}
{"created":"2024-02-21 01:02:48","title":"On-the-fly machine learned force fields for the study of warm dense matter: application to diffusion and viscosity of CH","abstract":"We develop a framework for on-the-fly machine learned force field (MLFF) molecular dynamics (MD) simulations of warm dense matter (WDM). In particular, we employ an MLFF scheme based on the kernel method and Bayesian linear regression, with the training data generated from Kohn-Sham density functional theory (DFT) using the Gauss Spectral Quadrature method, within which we calculate energies, atomic forces, and stresses. We verify the accuracy of the formalism by comparing the predicted properties of warm dense carbon with recent Kohn-Sham DFT results in the literature. In so doing, we demonstrate that ab initio MD simulations of WDM can be accelerated by up to three orders of magnitude, while retaining ab initio accuracy. We apply this framework to calculate the diffusion coefficients and shear viscosity of CH at a density of 1 g/cm$^3$ and temperatures in the range of 75,000 to 750,000 K. We find that the self- and inter-diffusion coefficients as well as the viscosity obey a power law with temperature, and that the diffusion coefficient results suggest a weak coupling between C and H in CH. In addition, we find agreement within standard deviation with previous results for C and CH but disagreement for H, demonstrating the need for ab initio calculations as presented here.","sentences":["We develop a framework for on-the-fly machine learned force field (MLFF) molecular dynamics (MD) simulations of warm dense matter (WDM).","In particular, we employ an MLFF scheme based on the kernel method and Bayesian linear regression, with the training data generated from Kohn-Sham density functional theory (DFT) using the Gauss Spectral Quadrature method, within which we calculate energies, atomic forces, and stresses.","We verify the accuracy of the formalism by comparing the predicted properties of warm dense carbon with recent Kohn-Sham DFT results in the literature.","In so doing, we demonstrate that ab initio MD simulations of WDM can be accelerated by up to three orders of magnitude, while retaining ab initio accuracy.","We apply this framework to calculate the diffusion coefficients and shear viscosity of CH at a density of 1 g/cm$^3$ and temperatures in the range of 75,000 to 750,000 K. We find that the self- and inter-diffusion coefficients as well as the viscosity obey a power law with temperature, and that the diffusion coefficient results suggest a weak coupling between C and H in CH.","In addition, we find agreement within standard deviation with previous results for C and CH but disagreement for H, demonstrating the need for ab initio calculations as presented here."],"url":"http://arxiv.org/abs/2402.13450v1","category":"physics.comp-ph"}
{"created":"2024-02-21 01:00:17","title":"CAMELoT: Towards Large Language Models with Training-Free Consolidated Associative Memory","abstract":"Large Language Models (LLMs) struggle to handle long input sequences due to high memory and runtime costs. Memory-augmented models have emerged as a promising solution to this problem, but current methods are hindered by limited memory capacity and require costly re-training to integrate with a new LLM. In this work, we introduce an associative memory module which can be coupled to any pre-trained (frozen) attention-based LLM without re-training, enabling it to handle arbitrarily long input sequences. Unlike previous methods, our associative memory module consolidates representations of individual tokens into a non-parametric distribution model, dynamically managed by properly balancing the novelty and recency of the incoming data. By retrieving information from this consolidated associative memory, the base LLM can achieve significant (up to 29.7% on Arxiv) perplexity reduction in long-context modeling compared to other baselines evaluated on standard benchmarks. This architecture, which we call CAMELoT (Consolidated Associative Memory Enhanced Long Transformer), demonstrates superior performance even with a tiny context window of 128 tokens, and also enables improved in-context learning with a much larger set of demonstrations.","sentences":["Large Language Models (LLMs) struggle to handle long input sequences due to high memory and runtime costs.","Memory-augmented models have emerged as a promising solution to this problem, but current methods are hindered by limited memory capacity and require costly re-training to integrate with a new LLM.","In this work, we introduce an associative memory module which can be coupled to any pre-trained (frozen) attention-based LLM without re-training, enabling it to handle arbitrarily long input sequences.","Unlike previous methods, our associative memory module consolidates representations of individual tokens into a non-parametric distribution model, dynamically managed by properly balancing the novelty and recency of the incoming data.","By retrieving information from this consolidated associative memory, the base LLM can achieve significant (up to 29.7% on Arxiv) perplexity reduction in long-context modeling compared to other baselines evaluated on standard benchmarks.","This architecture, which we call CAMELoT (Consolidated Associative Memory Enhanced Long Transformer), demonstrates superior performance even with a tiny context window of 128 tokens, and also enables improved in-context learning with a much larger set of demonstrations."],"url":"http://arxiv.org/abs/2402.13449v1","category":"cs.CL"}
{"created":"2024-02-21 00:44:04","title":"Large Language Models for Data Annotation: A Survey","abstract":"Data annotation is the labeling or tagging of raw data with relevant information, essential for improving the efficacy of machine learning models. The process, however, is labor-intensive and expensive. The emergence of advanced Large Language Models (LLMs), exemplified by GPT-4, presents an unprecedented opportunity to revolutionize and automate the intricate process of data annotation. While existing surveys have extensively covered LLM architecture, training, and general applications, this paper uniquely focuses on their specific utility for data annotation. This survey contributes to three core aspects: LLM-Based Data Annotation, Assessing LLM-generated Annotations, and Learning with LLM-generated annotations. Furthermore, the paper includes an in-depth taxonomy of methodologies employing LLMs for data annotation, a comprehensive review of learning strategies for models incorporating LLM-generated annotations, and a detailed discussion on primary challenges and limitations associated with using LLMs for data annotation. As a key guide, this survey aims to direct researchers and practitioners in exploring the potential of the latest LLMs for data annotation, fostering future advancements in this critical domain. We provide a comprehensive papers list at \\url{https://github.com/Zhen-Tan-dmml/LLM4Annotation.git}.","sentences":["Data annotation is the labeling or tagging of raw data with relevant information, essential for improving the efficacy of machine learning models.","The process, however, is labor-intensive and expensive.","The emergence of advanced Large Language Models (LLMs), exemplified by GPT-4, presents an unprecedented opportunity to revolutionize and automate the intricate process of data annotation.","While existing surveys have extensively covered LLM architecture, training, and general applications, this paper uniquely focuses on their specific utility for data annotation.","This survey contributes to three core aspects: LLM-Based Data Annotation, Assessing LLM-generated Annotations, and Learning with LLM-generated annotations.","Furthermore, the paper includes an in-depth taxonomy of methodologies employing LLMs for data annotation, a comprehensive review of learning strategies for models incorporating LLM-generated annotations, and a detailed discussion on primary challenges and limitations associated with using LLMs for data annotation.","As a key guide, this survey aims to direct researchers and practitioners in exploring the potential of the latest LLMs for data annotation, fostering future advancements in this critical domain.","We provide a comprehensive papers list at \\url{https://github.com/Zhen-Tan-dmml/LLM4Annotation.git}."],"url":"http://arxiv.org/abs/2402.13446v1","category":"cs.CL"}
{"created":"2024-02-21 00:35:55","title":"Autonomous Mapless Navigation on Uneven Terrains","abstract":"We propose a new method for autonomous navigation in uneven terrains by utilizing a sparse Gaussian Process (SGP) based local perception model. The SGP local perception model is trained on local ranging observation (pointcloud) to learn the terrain elevation profile and extract the feasible navigation subgoals around the robot. Subsequently, a cost function, which prioritizes the safety of the robot in terms of keeping the robot's roll and pitch angles bounded within a specified range, is used to select a safety-aware subgoal that leads the robot to its final destination. The algorithm is designed to run in real-time and is intensively evaluated in simulation and real world experiments. The results compellingly demonstrate that our proposed algorithm consistently navigates uneven terrains with high efficiency and surpasses the performance of other planners. The code and video can be found here: https://rb.gy/3ov2r8","sentences":["We propose a new method for autonomous navigation in uneven terrains by utilizing a sparse Gaussian Process (SGP) based local perception model.","The SGP local perception model is trained on local ranging observation (pointcloud) to learn the terrain elevation profile and extract the feasible navigation subgoals around the robot.","Subsequently, a cost function, which prioritizes the safety of the robot in terms of keeping the robot's roll and pitch angles bounded within a specified range, is used to select a safety-aware subgoal that leads the robot to its final destination.","The algorithm is designed to run in real-time and is intensively evaluated in simulation and real world experiments.","The results compellingly demonstrate that our proposed algorithm consistently navigates uneven terrains with high efficiency and surpasses the performance of other planners.","The code and video can be found here: https://rb.gy/3ov2r8"],"url":"http://arxiv.org/abs/2402.13443v1","category":"cs.RO"}
{"created":"2024-02-21 00:05:25","title":"Learning to Retrieve for Job Matching","abstract":"Web-scale search systems typically tackle the scalability challenge with a two-step paradigm: retrieval and ranking. The retrieval step, also known as candidate selection, often involves extracting standardized entities, creating an inverted index, and performing term matching for retrieval. Such traditional methods require manual and time-consuming development of query models. In this paper, we discuss applying learning-to-retrieve technology to enhance LinkedIns job search and recommendation systems. In the realm of promoted jobs, the key objective is to improve the quality of applicants, thereby delivering value to recruiter customers. To achieve this, we leverage confirmed hire data to construct a graph that evaluates a seeker's qualification for a job, and utilize learned links for retrieval. Our learned model is easy to explain, debug, and adjust. On the other hand, the focus for organic jobs is to optimize seeker engagement. We accomplished this by training embeddings for personalized retrieval, fortified by a set of rules derived from the categorization of member feedback. In addition to a solution based on a conventional inverted index, we developed an on-GPU solution capable of supporting both KNN and term matching efficiently.","sentences":["Web-scale search systems typically tackle the scalability challenge with a two-step paradigm: retrieval and ranking.","The retrieval step, also known as candidate selection, often involves extracting standardized entities, creating an inverted index, and performing term matching for retrieval.","Such traditional methods require manual and time-consuming development of query models.","In this paper, we discuss applying learning-to-retrieve technology to enhance LinkedIns job search and recommendation systems.","In the realm of promoted jobs, the key objective is to improve the quality of applicants, thereby delivering value to recruiter customers.","To achieve this, we leverage confirmed hire data to construct a graph that evaluates a seeker's qualification for a job, and utilize learned links for retrieval.","Our learned model is easy to explain, debug, and adjust.","On the other hand, the focus for organic jobs is to optimize seeker engagement.","We accomplished this by training embeddings for personalized retrieval, fortified by a set of rules derived from the categorization of member feedback.","In addition to a solution based on a conventional inverted index, we developed an on-GPU solution capable of supporting both KNN and term matching efficiently."],"url":"http://arxiv.org/abs/2402.13435v1","category":"cs.IR"}
{"created":"2024-02-20 23:45:37","title":"Everything You Always Wanted to Know About Storage Compressibility of Pre-Trained ML Models but Were Afraid to Ask","abstract":"As the number of pre-trained machine learning (ML) models is growing exponentially, data reduction tools are not catching up. Existing data reduction techniques are not specifically designed for pre-trained model (PTM) dataset files. This is largely due to a lack of understanding of the patterns and characteristics of these datasets, especially those relevant to data reduction and compressibility.   This paper presents the first, exhaustive analysis to date of PTM datasets on storage compressibility. Our analysis spans different types of data reduction and compression techniques, from hash-based data deduplication, data similarity detection, to dictionary-coding compression. Our analysis explores these techniques at three data granularity levels, from model layers, model chunks, to model parameters. We draw new observations that indicate that modern data reduction tools are not effective when handling PTM datasets. There is a pressing need for new compression methods that take into account PTMs' data characteristics for effective storage reduction.   Motivated by our findings, we design ELF, a simple yet effective, error-bounded, lossy floating-point compression method. ELF transforms floating-point parameters in such a way that the common exponent field of the transformed parameters can be completely eliminated to save storage space. We develop Elves, a compression framework that integrates ELF along with several other data reduction methods. Elves uses the most effective method to compress PTMs that exhibit different patterns. Evaluation shows that Elves achieves an overall compression ratio of $1.52\\times$, which is $1.31\\times$, $1.32\\times$ and $1.29\\times$ higher than a general-purpose compressor (zstd), an error-bounded lossy compressor (SZ3), and the uniform model quantization, respectively, with negligible model accuracy loss.","sentences":["As the number of pre-trained machine learning (ML) models is growing exponentially, data reduction tools are not catching up.","Existing data reduction techniques are not specifically designed for pre-trained model (PTM) dataset files.","This is largely due to a lack of understanding of the patterns and characteristics of these datasets, especially those relevant to data reduction and compressibility.   ","This paper presents the first, exhaustive analysis to date of PTM datasets on storage compressibility.","Our analysis spans different types of data reduction and compression techniques, from hash-based data deduplication, data similarity detection, to dictionary-coding compression.","Our analysis explores these techniques at three data granularity levels, from model layers, model chunks, to model parameters.","We draw new observations that indicate that modern data reduction tools are not effective when handling PTM datasets.","There is a pressing need for new compression methods that take into account PTMs' data characteristics for effective storage reduction.   ","Motivated by our findings, we design ELF, a simple yet effective, error-bounded, lossy floating-point compression method.","ELF transforms floating-point parameters in such a way that the common exponent field of the transformed parameters can be completely eliminated to save storage space.","We develop Elves, a compression framework that integrates ELF along with several other data reduction methods.","Elves uses the most effective method to compress PTMs that exhibit different patterns.","Evaluation shows that Elves achieves an overall compression ratio of $1.52\\times$, which is $1.31\\times$, $1.32\\times$ and $1.29\\times$ higher than a general-purpose compressor (zstd), an error-bounded lossy compressor (SZ3), and the uniform model quantization, respectively, with negligible model accuracy loss."],"url":"http://arxiv.org/abs/2402.13429v1","category":"cs.DB"}
{"created":"2024-02-20 21:49:36","title":"Fairness Risks for Group-conditionally Missing Demographics","abstract":"Fairness-aware classification models have gained increasing attention in recent years as concerns grow on discrimination against some demographic groups. Most existing models require full knowledge of the sensitive features, which can be impractical due to privacy, legal issues, and an individual's fear of discrimination. The key challenge we will address is the group dependency of the unavailability, e.g., people of some age range may be more reluctant to reveal their age. Our solution augments general fairness risks with probabilistic imputations of the sensitive features, while jointly learning the group-conditionally missing probabilities in a variational auto-encoder. Our model is demonstrated effective on both image and tabular datasets, achieving an improved balance between accuracy and fairness.","sentences":["Fairness-aware classification models have gained increasing attention in recent years as concerns grow on discrimination against some demographic groups.","Most existing models require full knowledge of the sensitive features, which can be impractical due to privacy, legal issues, and an individual's fear of discrimination.","The key challenge we will address is the group dependency of the unavailability, e.g., people of some age range may be more reluctant to reveal their age.","Our solution augments general fairness risks with probabilistic imputations of the sensitive features, while jointly learning the group-conditionally missing probabilities in a variational auto-encoder.","Our model is demonstrated effective on both image and tabular datasets, achieving an improved balance between accuracy and fairness."],"url":"http://arxiv.org/abs/2402.13393v1","category":"cs.LG"}
{"created":"2024-02-20 21:34:56","title":"Transformer tricks: Precomputing the first layer","abstract":"This short paper describes a trick to speed up inference of transformers with RoPE (such as LLaMA, Mistral, and PaLM). For these models, a large portion of the first transformer layer can be precomputed, which results in slightly lower latency and lower cost-per-token. Because this trick optimizes only one layer, the relative savings depend on the total number of layers. For example, the maximum savings for a model with only 4 layers (such as Whisper tiny) is limited to 25%, while a 32-layer model (such as Mistral-7B) is limited to 3% savings.","sentences":["This short paper describes a trick to speed up inference of transformers with RoPE (such as LLaMA, Mistral, and PaLM).","For these models, a large portion of the first transformer layer can be precomputed, which results in slightly lower latency and lower cost-per-token.","Because this trick optimizes only one layer, the relative savings depend on the total number of layers.","For example, the maximum savings for a model with only 4 layers (such as Whisper tiny) is limited to 25%, while a 32-layer model (such as Mistral-7B) is limited to 3% savings."],"url":"http://arxiv.org/abs/2402.13388v1","category":"cs.LG"}
{"created":"2024-02-20 20:48:00","title":"Unsupervised Concept Discovery Mitigates Spurious Correlations","abstract":"Models prone to spurious correlations in training data often produce brittle predictions and introduce unintended biases. Addressing this challenge typically involves methods relying on prior knowledge and group annotation to remove spurious correlations, which may not be readily available in many applications. In this paper, we establish a novel connection between unsupervised object-centric learning and mitigation of spurious correlations. Instead of directly inferring sub-groups with varying correlations with labels, our approach focuses on discovering concepts: discrete ideas that are shared across input samples. Leveraging existing object-centric representation learning, we introduce CoBalT: a concept balancing technique that effectively mitigates spurious correlations without requiring human labeling of subgroups. Evaluation across the Waterbirds, CelebA and ImageNet-9 benchmark datasets for subpopulation shifts demonstrate superior or competitive performance compared state-of-the-art baselines, without the need for group annotation.","sentences":["Models prone to spurious correlations in training data often produce brittle predictions and introduce unintended biases.","Addressing this challenge typically involves methods relying on prior knowledge and group annotation to remove spurious correlations, which may not be readily available in many applications.","In this paper, we establish a novel connection between unsupervised object-centric learning and mitigation of spurious correlations.","Instead of directly inferring sub-groups with varying correlations with labels, our approach focuses on discovering concepts: discrete ideas that are shared across input samples.","Leveraging existing object-centric representation learning, we introduce CoBalT: a concept balancing technique that effectively mitigates spurious correlations without requiring human labeling of subgroups.","Evaluation across the Waterbirds, CelebA and ImageNet-9 benchmark datasets for subpopulation shifts demonstrate superior or competitive performance compared state-of-the-art baselines, without the need for group annotation."],"url":"http://arxiv.org/abs/2402.13368v1","category":"cs.LG"}
{"created":"2024-02-21 16:23:14","title":"Quadratic inference with dense functional responses","abstract":"We address the challenge of estimation in the context of constant linear effect models with dense functional responses. In this framework, the conditional expectation of the response curve is represented by a linear combination of functional covariates with constant regression parameters. In this paper, we present an alternative solution by employing the quadratic inference approach, a well-established method for analyzing correlated data, to estimate the regression coefficients. Our approach leverages non-parametrically estimated basis functions, eliminating the need for choosing working correlation structures. Furthermore, we demonstrate that our method achieves a parametric $\\sqrt{n}$-convergence rate, contingent on an appropriate choice of bandwidth. This convergence is observed when the number of repeated measurements per trajectory exceeds a certain threshold, specifically, when it surpasses $n^{a_{0}}$, with $n$ representing the number of trajectories. Additionally, we establish the asymptotic normality of the resulting estimator. The performance of the proposed method is compared with that of existing methods through extensive simulation studies, where our proposed method outperforms. Real data analysis is also conducted to demonstrate the proposed method.","sentences":["We address the challenge of estimation in the context of constant linear effect models with dense functional responses.","In this framework, the conditional expectation of the response curve is represented by a linear combination of functional covariates with constant regression parameters.","In this paper, we present an alternative solution by employing the quadratic inference approach, a well-established method for analyzing correlated data, to estimate the regression coefficients.","Our approach leverages non-parametrically estimated basis functions, eliminating the need for choosing working correlation structures.","Furthermore, we demonstrate that our method achieves a parametric $\\sqrt{n}$-convergence rate, contingent on an appropriate choice of bandwidth.","This convergence is observed when the number of repeated measurements per trajectory exceeds a certain threshold, specifically, when it surpasses $n^{a_{0}}$, with $n$ representing the number of trajectories.","Additionally, we establish the asymptotic normality of the resulting estimator.","The performance of the proposed method is compared with that of existing methods through extensive simulation studies, where our proposed method outperforms.","Real data analysis is also conducted to demonstrate the proposed method."],"url":"http://arxiv.org/abs/2402.13907v1","category":"stat.ME"}
{"created":"2024-02-21 14:43:34","title":"Multi-Agent Online Graph Exploration on Cycles and Tadpole Graphs","abstract":"We study the problem of multi-agent online graph exploration, in which a team of k agents has to explore a given graph, starting and ending on the same node. The graph is initially unknown. Whenever a node is visited by an agent, its neighborhood and adjacent edges are revealed. The agents share a global view of the explored parts of the graph. The cost of the exploration has to be minimized, where cost either describes the time needed for the entire exploration (time model), or the length of the longest path traversed by any agent (energy model). We investigate graph exploration on cycles and tadpole graphs for 2-4 agents, providing optimal results on the competitive ratio in the energy model (1-competitive with two agents on cycles and three agents on tadpole graphs), and for tadpole graphs in the time model (1.5-competitive with four agents). We also show competitive upper bounds of 2 for the exploration of tadpole graphs with three agents, and 2.5 for the exploration of tadpole graphs with two agents in the time model.","sentences":["We study the problem of multi-agent online graph exploration, in which a team of k agents has to explore a given graph, starting and ending on the same node.","The graph is initially unknown.","Whenever a node is visited by an agent, its neighborhood and adjacent edges are revealed.","The agents share a global view of the explored parts of the graph.","The cost of the exploration has to be minimized, where cost either describes the time needed for the entire exploration (time model), or the length of the longest path traversed by any agent (energy model).","We investigate graph exploration on cycles and tadpole graphs for 2-4 agents, providing optimal results on the competitive ratio in the energy model (1-competitive with two agents on cycles and three agents on tadpole graphs), and for tadpole graphs in the time model (1.5-competitive with four agents).","We also show competitive upper bounds of 2 for the exploration of tadpole graphs with three agents, and 2.5 for the exploration of tadpole graphs with two agents in the time model."],"url":"http://arxiv.org/abs/2402.13845v1","category":"cs.DS"}
{"created":"2024-02-21 13:52:20","title":"A quadratically convergent semismooth Newton method for nonlinear semidefinite programming without the subdifferential regularity","abstract":"We introduce a quadratically convergent semismooth Newton method for nonlinear semidefinite programming that eliminates the need for the subdifferential regularity, a common yet stringent requirement in existing approaches. Our strategy revolves around identifying a single nonsingular element within the B(ouligand)-subdifferential, thus avoiding the standard requirement for nonsingularity across the entire subdifferential set, which is often too restrictive for practical applications. The theoretical framework is supported by the introduction of the weak second order condition (W-SOC) and the weak strict Robinson constraint qualification (W-SRCQ). These conditions not only guarantee the existence of a nonsingular element in the subdifferential but also forge a primal-dual connection in linearly constrained convex quadratic programming. The theoretical advancements further lay the foundation for the algorithmic designing of a novel semismooth Newton method, which integrates a correction step to address degenerate issues. Particularly, this correction step ensures the local convergence as well as a superlinear/quadratic convergence rate of the proposed method. Preliminary numerical experiments corroborate our theoretical findings and underscore the practical effectiveness of our method.","sentences":["We introduce a quadratically convergent semismooth Newton method for nonlinear semidefinite programming that eliminates the need for the subdifferential regularity, a common yet stringent requirement in existing approaches.","Our strategy revolves around identifying a single nonsingular element within the B(ouligand)-subdifferential, thus avoiding the standard requirement for nonsingularity across the entire subdifferential set, which is often too restrictive for practical applications.","The theoretical framework is supported by the introduction of the weak second order condition (W-SOC) and the weak strict Robinson constraint qualification (W-SRCQ).","These conditions not only guarantee the existence of a nonsingular element in the subdifferential but also forge a primal-dual connection in linearly constrained convex quadratic programming.","The theoretical advancements further lay the foundation for the algorithmic designing of a novel semismooth Newton method, which integrates a correction step to address degenerate issues.","Particularly, this correction step ensures the local convergence as well as a superlinear/quadratic convergence rate of the proposed method.","Preliminary numerical experiments corroborate our theoretical findings and underscore the practical effectiveness of our method."],"url":"http://arxiv.org/abs/2402.13814v1","category":"math.OC"}
{"created":"2024-02-21 12:30:33","title":"LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens","abstract":"Large context window is a desirable feature in large language models (LLMs). However, due to high fine-tuning costs, scarcity of long texts, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128k tokens. This paper introduces LongRoPE that, for the first time, extends the context window of pre-trained LLMs to an impressive 2048k tokens, with up to only 1k fine-tuning steps at within 256k training lengths, while maintaining performance at the original short context window. This is achieved by three key innovations: (i) we identify and exploit two forms of non-uniformities in positional interpolation through an efficient search, providing a better initialization for fine-tuning and enabling an 8x extension in non-fine-tuning scenarios; (ii) we introduce a progressive extension strategy that first fine-tunes a 256k length LLM and then conducts a second positional interpolation on the fine-tuned extended LLM to achieve a 2048k context window; (iii) we readjust LongRoPE on 8k length to recover the short context window performance. Extensive experiments on LLaMA2 and Mistral across various tasks demonstrate the effectiveness of our method. Models extended via LongRoPE retain the original architecture with minor modifications to the positional embedding, and can reuse most pre-existing optimizations.","sentences":["Large context window is a desirable feature in large language models (LLMs).","However, due to high fine-tuning costs, scarcity of long texts, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128k tokens.","This paper introduces LongRoPE that, for the first time, extends the context window of pre-trained LLMs to an impressive 2048k tokens, with up to only 1k fine-tuning steps at within 256k training lengths, while maintaining performance at the original short context window.","This is achieved by three key innovations: (i) we identify and exploit two forms of non-uniformities in positional interpolation through an efficient search, providing a better initialization for fine-tuning and enabling an 8x extension in non-fine-tuning scenarios; (ii) we introduce a progressive extension strategy that first fine-tunes a 256k length LLM and then conducts a second positional interpolation on the fine-tuned extended LLM to achieve a 2048k context window; (iii) we readjust LongRoPE on 8k length to recover the short context window performance.","Extensive experiments on LLaMA2 and Mistral across various tasks demonstrate the effectiveness of our method.","Models extended via LongRoPE retain the original architecture with minor modifications to the positional embedding, and can reuse most pre-existing optimizations."],"url":"http://arxiv.org/abs/2402.13753v1","category":"cs.CL"}
{"created":"2024-02-21 11:05:14","title":"Robustness analysis and station-keeping control of an interferometer formation flying mission in low Earth orbit","abstract":"The impact of formation flying on interferometry is growing over the years for the potential performance it could offer. However, it is still an open field, and many studies are still required. This article presents the basic principles behind interferometry focusing first on a single array and secondly on a formation of satellites. A sensitivity analysis is carried out to evaluate how the performance of the interferometry is affected by an error in the relative position in the formation geometry. This is estimated by computing the loss of the performance in terms of percentage deviation due to a non-nominal relative trajectory, including two-dimensional errors and defining a payload index. The main goal of this study is to estimate whether some errors in the relative state are more impacting than others. The final objective is to compute the link between a position error and a specific loss of performance, to foresee the origin of the the error. Furthermore, a dynamical model is developed to describe the relative motion in the Low Earth Orbit environment, considering both the unperturbed and the J2 and drag contributions. A Proportional, Integral and Derivative controller is implemented for the position control of a multiple satellite formation flying, considering a low thrust control profile. The Formation Flying L-band Aperture Synthesis study is taken as the case scenario, analysing both nominal and non-nominal configurations. This study serves as a starting point for the development of a combined tool to assess the performance of the interferometry and the control on the relative state for future remote sensing studies involving relative motion.","sentences":["The impact of formation flying on interferometry is growing over the years for the potential performance it could offer.","However, it is still an open field, and many studies are still required.","This article presents the basic principles behind interferometry focusing first on a single array and secondly on a formation of satellites.","A sensitivity analysis is carried out to evaluate how the performance of the interferometry is affected by an error in the relative position in the formation geometry.","This is estimated by computing the loss of the performance in terms of percentage deviation due to a non-nominal relative trajectory, including two-dimensional errors and defining a payload index.","The main goal of this study is to estimate whether some errors in the relative state are more impacting than others.","The final objective is to compute the link between a position error and a specific loss of performance, to foresee the origin of the the error.","Furthermore, a dynamical model is developed to describe the relative motion in the Low Earth Orbit environment, considering both the unperturbed and the J2 and drag contributions.","A Proportional, Integral and Derivative controller is implemented for the position control of a multiple satellite formation flying, considering a low thrust control profile.","The Formation Flying L-band Aperture Synthesis study is taken as the case scenario, analysing both nominal and non-nominal configurations.","This study serves as a starting point for the development of a combined tool to assess the performance of the interferometry and the control on the relative state for future remote sensing studies involving relative motion."],"url":"http://arxiv.org/abs/2402.13702v1","category":"astro-ph.EP"}
{"created":"2024-02-21 08:55:35","title":"Higher-order singular perturbation models for phase transitions","abstract":"Variational models of phase transitions take into account double-well energies singularly perturbed by gradient terms, such as the Cahn-Hilliard free energy. The derivation by $\\Gamma$-convergence of a sharp-interface limit for such energy is a classical result by Modica and Mortola. We consider a singular perturbation of a double-well energy by derivatives of order $k$, and show that we still can describe the limit as in the case $k=1$ with a suitable interfacial energy density, in accord with the case $k=1$ and with the case $k=2$ previously analyzed by Fonseca and Mantegazza. The main isssue is the derivation of an optimal-profile problem on the real line describing the interfacial energy density, which must be conveniently approximated by minimum problems on finite intervals with homogeneous condition on the derivatives at the endpoints up to order $k-1$. To that end a careful study must be carried on of sets where sequences of functions with equibounded energy are ``close to the wells'' and have ``small derivatives'', in terms of interpolation inequalities and energy estimates.","sentences":["Variational models of phase transitions take into account double-well energies singularly perturbed by gradient terms, such as the Cahn-Hilliard free energy.","The derivation by $\\Gamma$-convergence of a sharp-interface limit for such energy is a classical result by Modica and Mortola.","We consider a singular perturbation of a double-well energy by derivatives of order $k$, and show that we still can describe the limit as in the case $k=1$ with a suitable interfacial energy density, in accord with the case $k=1$ and with the case $k=2$ previously analyzed by Fonseca and Mantegazza.","The main isssue is the derivation of an optimal-profile problem on the real line describing the interfacial energy density, which must be conveniently approximated by minimum problems on finite intervals with homogeneous condition on the derivatives at the endpoints up to order $k-1$. To that end a careful study must be carried on of sets where sequences of functions with equibounded energy are ``close to the wells'' and have ``small derivatives'', in terms of interpolation inequalities and energy estimates."],"url":"http://arxiv.org/abs/2402.13626v1","category":"math.AP"}
{"created":"2024-02-21 07:55:31","title":"Two-dimensional Closed-Form Analytical Model of Laterally-Excited Film Bulk Acoustic Wave Resonator Multiferroic Antennas","abstract":"To overcome the physical limitations of electrically small antennas, strain-mediated magnetoelectric antennas have been studied experimentally and theoretically. However, current closed-form analytical models include solely one-dimensional approaches. This paper proposes a two-dimensional closed-form analytical model of a laterally-excited multiferroic antenna. To model thickness-shear vibrations, we propose an extended Mason model to two dimensions. This theory solves unidirectionally-coupled elastodynamics-magnetization dynamics-electrodynamics. Without using the infinitesimal dipole approximation, this model provides a closed-form solution of the radiated electromagnetic field with an explicit dependency of the antenna response on the physical parameters, enabling their optimization. Furthermore, we compare the efficiency-bandwidth product of different multiferroic antennas with Chu's limit in the Akhiezer regime, which delimits the highest achievable mechanical quality factor for acoustic resonators at GHz frequencies. We show that phonon-phonon coupling of Akhiezer damping is most likely to limit multiferroic antenna radiation, not Chu's limit. Moreover, judicious choice of materials can enhance the maximum efficiency by more than an order of magnitude.","sentences":["To overcome the physical limitations of electrically small antennas, strain-mediated magnetoelectric antennas have been studied experimentally and theoretically.","However, current closed-form analytical models include solely one-dimensional approaches.","This paper proposes a two-dimensional closed-form analytical model of a laterally-excited multiferroic antenna.","To model thickness-shear vibrations, we propose an extended Mason model to two dimensions.","This theory solves unidirectionally-coupled elastodynamics-magnetization dynamics-electrodynamics.","Without using the infinitesimal dipole approximation, this model provides a closed-form solution of the radiated electromagnetic field with an explicit dependency of the antenna response on the physical parameters, enabling their optimization.","Furthermore, we compare the efficiency-bandwidth product of different multiferroic antennas with Chu's limit in the Akhiezer regime, which delimits the highest achievable mechanical quality factor for acoustic resonators at GHz frequencies.","We show that phonon-phonon coupling of Akhiezer damping is most likely to limit multiferroic antenna radiation, not Chu's limit.","Moreover, judicious choice of materials can enhance the maximum efficiency by more than an order of magnitude."],"url":"http://arxiv.org/abs/2402.13594v1","category":"physics.app-ph"}
{"created":"2024-02-21 07:16:06","title":"Improving Video Corpus Moment Retrieval with Partial Relevance Enhancement","abstract":"Video corpus moment retrieval~(VCMR) is a new video retrieval task aimed at retrieving a relevant moment from a large corpus of untrimmed videos using a natural language text as query. The relevance between the video and query is partial, mainly evident in two aspects: (1) Scope: The untrimmed video contains information-rich frames, and not all are relevant to the query. Strong correlation is typically observed only within the relevant moment, emphasizing the importance of capturing key content. (2) Modality: The relevance of query to different modalities varies; action descriptions align more with the visual elements, while character conversations are more related to textual information. Recognizing and addressing these modality-specific nuances is crucial for effective retrieval in VCMR. However, existing methods often treat all video contents equally, leading to sub-optimal moment retrieval. We argue that effectively capturing the partial relevance between the query and video is essential for the VCMR task. To this end, we propose a Partial Relevance Enhanced Model~(PREM) to improve VCMR. VCMR involves two sub-tasks: video retrieval and moment localization. To align with their distinct objectives, we implement specialized partial relevance enhancement strategies. For video retrieval, we introduce a multi-modal collaborative video retriever, generating distinct query representations tailored for different modalities by modality-specific pooling, ensuring a more effective match. For moment localization, we propose the focus-then-fuse moment localizer, utilizing modality-specific gates to capture essential content, followed by fusing multi-modal information for moment localization. Experimental results on TVR and DiDeMo datasets show that the proposed model outperforms the baselines, achieving a new state-of-the-art of VCMR.","sentences":["Video corpus moment retrieval~(VCMR) is a new video retrieval task aimed at retrieving a relevant moment from a large corpus of untrimmed videos using a natural language text as query.","The relevance between the video and query is partial, mainly evident in two aspects: (1) Scope:","The untrimmed video contains information-rich frames, and not all are relevant to the query.","Strong correlation is typically observed only within the relevant moment, emphasizing the importance of capturing key content.","(2) Modality: The relevance of query to different modalities varies; action descriptions align more with the visual elements, while character conversations are more related to textual information.","Recognizing and addressing these modality-specific nuances is crucial for effective retrieval in VCMR.","However, existing methods often treat all video contents equally, leading to sub-optimal moment retrieval.","We argue that effectively capturing the partial relevance between the query and video is essential for the VCMR task.","To this end, we propose a Partial Relevance Enhanced Model~(PREM) to improve VCMR.","VCMR involves two sub-tasks: video retrieval and moment localization.","To align with their distinct objectives, we implement specialized partial relevance enhancement strategies.","For video retrieval, we introduce a multi-modal collaborative video retriever, generating distinct query representations tailored for different modalities by modality-specific pooling, ensuring a more effective match.","For moment localization, we propose the focus-then-fuse moment localizer, utilizing modality-specific gates to capture essential content, followed by fusing multi-modal information for moment localization.","Experimental results on TVR and DiDeMo datasets show that the proposed model outperforms the baselines, achieving a new state-of-the-art of VCMR."],"url":"http://arxiv.org/abs/2402.13576v1","category":"cs.CV"}
{"created":"2024-02-21 06:26:40","title":"Nano antenna-assisted quantum dots emission into high-index planar waveguide","abstract":"Integrated quantum photonic circuits require the efficient coupling of photon sources to photonic waveguides. Hybrid plasmonic/photonic platforms are a promising approach, taking advantage of both plasmon modal confinement for efficient coupling to a nearby emitter and photonic circuitry for optical data transfer and processing. In this work, we established directional quantum dot (QD) emission coupling to a planar TiO$_2$ waveguide assisted by a Yagi-Uda antenna. Antenna on waveguide is first designed by scaling radio frequency dimensions to nano-optics, taking into account the hybrid plasmonic/photonic platform. Design is then optimized by full numerical simulations. We fabricate the antenna on a TiO$_2$ planar waveguide and deposit a few QDs close to the Yagi-Uda antenna. The optical characterization shows clear directional coupling originating from antenna effect. We estimate the coupling efficiency and directivity of the light emitted into the waveguide.","sentences":["Integrated quantum photonic circuits require the efficient coupling of photon sources to photonic waveguides.","Hybrid plasmonic/photonic platforms are a promising approach, taking advantage of both plasmon modal confinement for efficient coupling to a nearby emitter and photonic circuitry for optical data transfer and processing.","In this work, we established directional quantum dot (QD) emission coupling to a planar TiO$_2$ waveguide assisted by a Yagi-Uda antenna.","Antenna on waveguide is first designed by scaling radio frequency dimensions to nano-optics, taking into account the hybrid plasmonic/photonic platform.","Design is then optimized by full numerical simulations.","We fabricate the antenna on a TiO$_2$ planar waveguide and deposit a few QDs close to the Yagi-Uda antenna.","The optical characterization shows clear directional coupling originating from antenna effect.","We estimate the coupling efficiency and directivity of the light emitted into the waveguide."],"url":"http://arxiv.org/abs/2402.13557v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-21 04:55:57","title":"Multitier Service Migration Framework Based on Mobility Prediction in Mobile Edge Computing","abstract":"Mobile edge computing (MEC) pushes computing resources to the edge of the network and distributes them at the edge of the mobile network. Offloading computing tasks to the edge instead of the cloud can reduce computing latency and backhaul load simultaneously. However, new challenges incurred by user mobility and limited coverage of MEC server service arise. Services should be dynamically migrated between multiple MEC servers to maintain service performance due to user movement. Tackling this problem is nontrivial because it is arduous to predict user movement, and service migration will generate service interruptions and redundant network traffic. Service interruption time must be minimized, and redundant network traffic should be reduced to ensure service quality. In this paper, the container lives migration technology based on prediction is studied, and an online prediction method based on map data that does not rely on prior knowledge such as user trajectories is proposed to address this challenge in terms of mobility prediction accuracy.","sentences":["Mobile edge computing (MEC) pushes computing resources to the edge of the network and distributes them at the edge of the mobile network.","Offloading computing tasks to the edge instead of the cloud can reduce computing latency and backhaul load simultaneously.","However, new challenges incurred by user mobility and limited coverage of MEC server service arise.","Services should be dynamically migrated between multiple MEC servers to maintain service performance due to user movement.","Tackling this problem is nontrivial because it is arduous to predict user movement, and service migration will generate service interruptions and redundant network traffic.","Service interruption time must be minimized, and redundant network traffic should be reduced to ensure service quality.","In this paper, the container lives migration technology based on prediction is studied, and an online prediction method based on map data that does not rely on prior knowledge such as user trajectories is proposed to address this challenge in terms of mobility prediction accuracy."],"url":"http://arxiv.org/abs/2402.13529v1","category":"cs.NI"}
{"created":"2024-02-21 02:57:27","title":"A Feature Matching Method Based on Multi-Level Refinement Strategy","abstract":"Feature matching is a fundamental and crucial process in visual SLAM, and precision has always been a challenging issue in feature matching. In this paper, based on a multi-level fine matching strategy, we propose a new feature matching method called KTGP-ORB. This method utilizes the similarity of local appearance in the Hamming space generated by feature descriptors to establish initial correspondences. It combines the constraint of local image motion smoothness, uses the GMS algorithm to enhance the accuracy of initial matches, and finally employs the PROSAC algorithm to optimize matches, achieving precise matching based on global grayscale information in Euclidean space. Experimental results demonstrate that the KTGP-ORB method reduces the error by an average of 29.92% compared to the ORB algorithm in complex scenes with illumination variations and blur.","sentences":["Feature matching is a fundamental and crucial process in visual SLAM, and precision has always been a challenging issue in feature matching.","In this paper, based on a multi-level fine matching strategy, we propose a new feature matching method called KTGP-ORB.","This method utilizes the similarity of local appearance in the Hamming space generated by feature descriptors to establish initial correspondences.","It combines the constraint of local image motion smoothness, uses the GMS algorithm to enhance the accuracy of initial matches, and finally employs the PROSAC algorithm to optimize matches, achieving precise matching based on global grayscale information in Euclidean space.","Experimental results demonstrate that the KTGP-ORB method reduces the error by an average of 29.92% compared to the ORB algorithm in complex scenes with illumination variations and blur."],"url":"http://arxiv.org/abs/2402.13488v1","category":"cs.CV"}
{"created":"2024-02-21 00:09:41","title":"Optimisation of design parameters to improve performance of a planar electromagnetic actuator","abstract":"Planar electromagnetic actuators based on the principle of linear motors are widely employed for micro and nano positioning applications. These actuators usually employ a planar magnetic platform driven by a co-planar electromagnetic coil. While these actuators offer a large motion range and high positioning resolution, their actuation bandwidth is limited due to relatively small electromagnetic stiffness. We report optimization of the design parameters of the electromagnetic coil and the magnetic assembly to maximize the electromagnetic force and stiffness. Firstly, we derive closed-form expressions for the electromagnetic forces and stiffness, which enable us to express these quantities in terms of the design parameters of the actuator. Secondly, based on these derived expressions, we estimate the optimum values of the design parameters to maximize force and stiffness. Notably, for the optimum design parameters, the force and stiffness per unit volume can be increased by two and three orders of magnitude, respectively by reducing the pitch of the electromagnetic coil by a factor of 10. Lastly, we develop an electromagnetic actuator and evaluate its performance using a Microelectromechanical system (MEMS) based force sensor. By operating the force sensor in a feedback loop, we precisely measure the generated electromagnetic forces for different design parameters of the actuator. The experimental results obtained align closely with the analytical values, with an error of less than 15%.","sentences":["Planar electromagnetic actuators based on the principle of linear motors are widely employed for micro and nano positioning applications.","These actuators usually employ a planar magnetic platform driven by a co-planar electromagnetic coil.","While these actuators offer a large motion range and high positioning resolution, their actuation bandwidth is limited due to relatively small electromagnetic stiffness.","We report optimization of the design parameters of the electromagnetic coil and the magnetic assembly to maximize the electromagnetic force and stiffness.","Firstly, we derive closed-form expressions for the electromagnetic forces and stiffness, which enable us to express these quantities in terms of the design parameters of the actuator.","Secondly, based on these derived expressions, we estimate the optimum values of the design parameters to maximize force and stiffness.","Notably, for the optimum design parameters, the force and stiffness per unit volume can be increased by two and three orders of magnitude, respectively by reducing the pitch of the electromagnetic coil by a factor of 10.","Lastly, we develop an electromagnetic actuator and evaluate its performance using a Microelectromechanical system (MEMS) based force sensor.","By operating the force sensor in a feedback loop, we precisely measure the generated electromagnetic forces for different design parameters of the actuator.","The experimental results obtained align closely with the analytical values, with an error of less than 15%."],"url":"http://arxiv.org/abs/2402.13436v1","category":"physics.app-ph"}
{"created":"2024-02-20 23:19:30","title":"On binary codes with distances $d$ and $d+2$","abstract":"We consider the problem of finding $A_2(n,\\{d_1,d_2\\})$ defined as the maximal size of a binary (non-linear) code of length $n$ with two distances $d_1$ and $d_2$. Binary codes with distances $d$ and $d+2$ of size $\\sim\\frac{n^2}{\\frac{d}{2}(\\frac{d}{2}+1)}$ can be obtained from $2$-packings of an $n$-element set by blocks of cardinality $\\frac{d}{2}+1$. This value is far from the upper bound $A_2(n,\\{d_1,d_2\\})\\le1+{n\\choose2}$ proved recently by Barg et al.   In this paper we prove that for every fixed $d$ ($d$ even) there exists an integer $N(d)$ such that for every $n\\ge N(d)$ it holds $A_2(n,\\{d,d+2\\})=D(n,\\frac{d}{2}+1,2)$, or, in other words, optimal codes are isomorphic to constant weight codes. We prove also estimates on $N(d)$ for $d=4$ and $d=6$.","sentences":["We consider the problem of finding $A_2(n,\\{d_1,d_2\\})$ defined as the maximal size of a binary (non-linear) code of length $n$ with two distances $d_1$ and $d_2$. Binary codes with distances $d$ and $d+2$ of size $\\sim\\frac{n^2}{\\frac{d}{2}(\\frac{d}{2}+1)}$ can be obtained from $2$-packings of an $n$-element set by blocks of cardinality $\\frac{d}{2}+1$. This value is far from the upper bound $A_2(n,\\{d_1,d_2\\})\\le1+{n\\choose2}$ proved recently by Barg et al.   ","In this paper we prove that for every fixed $d$ ($d$ even) there exists an integer $N(d)$ such that for every $n\\ge N(d)$ it holds $A_2(n,\\{d,d+2\\})=D(n,\\frac{d}{2}+1,2)$, or, in other words, optimal codes are isomorphic to constant weight codes.","We prove also estimates on $N(d)$ for $d=4$ and $d=6$."],"url":"http://arxiv.org/abs/2402.13420v1","category":"math.CO"}
{"created":"2024-02-20 21:29:53","title":"Regular Languages in the Sliding Window Model","abstract":"We study the space complexity of the following problem: For a fixed regular language $L$, test membership of a sliding window of length $n$ to $L$ over a given stream of symbols. For deterministic streaming algorithms we prove a trichotomy theorem, namely that the (optimal) space complexity is either constant, logarithmic or linear, measured in the window length $n$. Additionally, we provide natural language-theoretic characterizations of the space classes. We then extend the results to randomized streaming algorithms and we show that in this setting, the space complexity of any regular language is either constant, doubly logarithmic, logarithmic or linear. Finally, we introduce sliding window testers, which can distinguish whether a window belongs to the language $L$ or has Hamming distance $> \\epsilon n$ to $L$. We prove that every regular language has a deterministic (resp., randomized) sliding window tester that requires only logarithmic (resp., constant) space.","sentences":["We study the space complexity of the following problem: For a fixed regular language $L$, test membership of a sliding window of length $n$ to $L$ over a given stream of symbols.","For deterministic streaming algorithms we prove a trichotomy theorem, namely that the (optimal) space complexity is either constant, logarithmic or linear, measured in the window length $n$. Additionally, we provide natural language-theoretic characterizations of the space classes.","We then extend the results to randomized streaming algorithms and we show that in this setting, the space complexity of any regular language is either constant, doubly logarithmic, logarithmic or linear.","Finally, we introduce sliding window testers, which can distinguish whether a window belongs to the language $L$ or has Hamming distance $> \\epsilon n$ to $L$. We prove that every regular language has a deterministic (resp., randomized) sliding window tester that requires only logarithmic (resp., constant) space."],"url":"http://arxiv.org/abs/2402.13385v1","category":"cs.FL"}
{"created":"2024-02-20 21:07:49","title":"Stable matching as transportation","abstract":"We study matching markets with aligned preferences and establish a connection between common design objectives -- stability, efficiency, and fairness -- and the theory of optimal transport. Optimal transport gives new insights into the structural properties of matchings obtained from pursuing these objectives, and into the trade-offs between different objectives. Matching markets with aligned preferences provide a tractable stylized model capturing supply-demand imbalances in a range of settings such as partnership formation, school choice, organ donor exchange, and markets with transferable utility where bargaining over transfers happens after a match is formed.","sentences":["We study matching markets with aligned preferences and establish a connection between common design objectives -- stability, efficiency, and fairness -- and the theory of optimal transport.","Optimal transport gives new insights into the structural properties of matchings obtained from pursuing these objectives, and into the trade-offs between different objectives.","Matching markets with aligned preferences provide a tractable stylized model capturing supply-demand imbalances in a range of settings such as partnership formation, school choice, organ donor exchange, and markets with transferable utility where bargaining over transfers happens after a match is formed."],"url":"http://arxiv.org/abs/2402.13378v1","category":"econ.TH"}
{"created":"2024-02-20 20:32:13","title":"Low Degree Hardness for Broadcasting on Trees","abstract":"We study the low-degree hardness of broadcasting on trees. Broadcasting on trees has been extensively studied in statistical physics, in computational biology in relation to phylogenetic reconstruction and in statistics and computer science in the context of block model inference, and as a simple data model for algorithms that may require depth for inference.   The inference of the root can be carried by celebrated Belief Propagation (BP) algorithm which achieves Bayes-optimal performance. Despite the fact that this algorithm runs in linear time (using real operations), recent works indicated that this algorithm in fact requires high level of complexity. Moitra, Mossel and Sandon constructed a chain for which estimating the root better than random (for a typical input) is $NC1$ complete. Kohler and Mossel constructed chains such that for trees with $N$ leaves, recovering the root better than random requires a polynomial of degree $N^{\\Omega(1)}$. Both works above asked if such complexity bounds hold in general below the celebrated {\\em Kesten-Stigum} bound.   In this work, we prove that this is indeed the case for low degree polynomials. We show that for the broadcast problem using any Markov chain on trees with $n$ leaves, below the Kesten Stigum bound, any $O(\\log n)$ degree polynomial has vanishing correlation with the root.   Our result is one of the first low-degree lower bound that is proved in a setting that is not based or easily reduced to a product measure.","sentences":["We study the low-degree hardness of broadcasting on trees.","Broadcasting on trees has been extensively studied in statistical physics, in computational biology in relation to phylogenetic reconstruction and in statistics and computer science in the context of block model inference, and as a simple data model for algorithms that may require depth for inference.   ","The inference of the root can be carried by celebrated Belief Propagation (BP) algorithm which achieves Bayes-optimal performance.","Despite the fact that this algorithm runs in linear time (using real operations), recent works indicated that this algorithm in fact requires high level of complexity.","Moitra, Mossel and Sandon constructed a chain for which estimating the root better than random (for a typical input) is $NC1$ complete.","Kohler and Mossel constructed chains such that for trees with $N$ leaves, recovering the root better than random requires a polynomial of degree $N^{\\Omega(1)}$. Both works above asked if such complexity bounds hold in general below the celebrated {\\em Kesten-Stigum} bound.   ","In this work, we prove that this is indeed the case for low degree polynomials.","We show that for the broadcast problem using any Markov chain on trees with $n$ leaves, below the Kesten Stigum bound, any $O(\\log n)$ degree polynomial has vanishing correlation with the root.   ","Our result is one of the first low-degree lower bound that is proved in a setting that is not based or easily reduced to a product measure."],"url":"http://arxiv.org/abs/2402.13359v1","category":"math.PR"}
{"created":"2024-02-20 20:19:33","title":"Minimizing Tardy Processing Time on a Single Machine in Near-Linear Time","abstract":"In this work we revisit the elementary scheduling problem $1||\\sum p_j U_j$. The goal is to select, among $n$ jobs with processing times and due dates, a subset of jobs with maximum total processing time that can be scheduled in sequence without violating their due dates. This problem is NP-hard, but a classical algorithm by Lawler and Moore from the 60s solves this problem in pseudo-polynomial time $O(nP)$, where $P$ is the total processing time of all jobs. With the aim to develop best-possible pseudo-polynomial-time algorithms, a recent wave of results has improved Lawler and Moore's algorithm for $1||\\sum p_j U_j$: First to time $\\tilde O(P^{7/4})$ [Bringmann, Fischer, Hermelin, Shabtay, Wellnitz; ICALP'20], then to time $\\tilde O(P^{5/3})$ [Klein, Polak, Rohwedder; SODA'23], and finally to time $\\tilde O(P^{7/5})$ [Schieber, Sitaraman; WADS'23]. It remained an exciting open question whether these works can be improved further.   In this work we develop an algorithm in near-linear time $\\tilde O(P)$ for the $1||\\sum p_j U_j$ problem. This running time not only significantly improves upon the previous results, but also matches conditional lower bounds based on the Strong Exponential Time Hypothesis or the Set Cover Hypothesis and is therefore likely optimal (up to subpolynomial factors). Our new algorithm also extends to the case of $m$ machines in time $\\tilde O(P^m)$. In contrast to the previous improvements, we take a different, more direct approach inspired by the recent reductions from Modular Subset Sum to dynamic string problems. We thereby arrive at a satisfyingly simple algorithm.","sentences":["In this work we revisit the elementary scheduling problem $1||\\sum p_j","U_j$. The goal is to select, among $n$ jobs with processing times and due dates, a subset of jobs with maximum total processing time that can be scheduled in sequence without violating their due dates.","This problem is NP-hard, but a classical algorithm by Lawler and Moore from the 60s solves this problem in pseudo-polynomial time $O(nP)$, where $P$ is the total processing time of all jobs.","With the aim to develop best-possible pseudo-polynomial-time algorithms, a recent wave of results has improved Lawler and Moore's algorithm for $1||\\sum p_j","U_j$: First to time $\\tilde O(P^{7/4})$","[Bringmann, Fischer, Hermelin, Shabtay, Wellnitz; ICALP'20], then to time $\\tilde O(P^{5/3})$ [Klein, Polak, Rohwedder; SODA'23], and finally to time $\\tilde O(P^{7/5})$","[Schieber, Sitaraman; WADS'23].","It remained an exciting open question whether these works can be improved further.   ","In this work we develop an algorithm in near-linear time $\\tilde O(P)$ for the $1||\\sum p_j","U_j$ problem.","This running time not only significantly improves upon the previous results, but also matches conditional lower bounds based on the Strong Exponential Time Hypothesis or the Set Cover Hypothesis and is therefore likely optimal (up to subpolynomial factors).","Our new algorithm also extends to the case of $m$ machines in time $\\tilde O(P^m)$. In contrast to the previous improvements, we take a different, more direct approach inspired by the recent reductions from Modular Subset Sum to dynamic string problems.","We thereby arrive at a satisfyingly simple algorithm."],"url":"http://arxiv.org/abs/2402.13357v1","category":"cs.DS"}
