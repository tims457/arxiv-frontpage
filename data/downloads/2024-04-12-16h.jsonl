{"created":"2024-04-10 17:59:59","title":"GoodDrag: Towards Good Practices for Drag Editing with Diffusion Models","abstract":"In this paper, we introduce GoodDrag, a novel approach to improve the stability and image quality of drag editing. Unlike existing methods that struggle with accumulated perturbations and often result in distortions, GoodDrag introduces an AlDD framework that alternates between drag and denoising operations within the diffusion process, effectively improving the fidelity of the result. We also propose an information-preserving motion supervision operation that maintains the original features of the starting point for precise manipulation and artifact reduction. In addition, we contribute to the benchmarking of drag editing by introducing a new dataset, Drag100, and developing dedicated quality assessment metrics, Dragging Accuracy Index and Gemini Score, utilizing Large Multimodal Models. Extensive experiments demonstrate that the proposed GoodDrag compares favorably against the state-of-the-art approaches both qualitatively and quantitatively. The project page is https://gooddrag.github.io.","sentences":["In this paper, we introduce GoodDrag, a novel approach to improve the stability and image quality of drag editing.","Unlike existing methods that struggle with accumulated perturbations and often result in distortions, GoodDrag introduces an AlDD framework that alternates between drag and denoising operations within the diffusion process, effectively improving the fidelity of the result.","We also propose an information-preserving motion supervision operation that maintains the original features of the starting point for precise manipulation and artifact reduction.","In addition, we contribute to the benchmarking of drag editing by introducing a new dataset, Drag100, and developing dedicated quality assessment metrics, Dragging Accuracy Index and Gemini Score, utilizing Large Multimodal Models.","Extensive experiments demonstrate that the proposed GoodDrag compares favorably against the state-of-the-art approaches both qualitatively and quantitatively.","The project page is https://gooddrag.github.io."],"url":"http://arxiv.org/abs/2404.07206v1","category":"cs.CV"}
{"created":"2024-04-10 17:59:45","title":"BRAVE: Broadening the visual encoding of vision-language models","abstract":"Vision-language models (VLMs) are typically composed of a vision encoder, e.g. CLIP, and a language model (LM) that interprets the encoded features to solve downstream tasks. Despite remarkable progress, VLMs are subject to several shortcomings due to the limited capabilities of vision encoders, e.g. \"blindness\" to certain image features, visual hallucination, etc. To address these issues, we study broadening the visual encoding capabilities of VLMs. We first comprehensively benchmark several vision encoders with different inductive biases for solving VLM tasks. We observe that there is no single encoding configuration that consistently achieves top performance across different tasks, and encoders with different biases can perform surprisingly similarly. Motivated by this, we introduce a method, named BRAVE, that consolidates features from multiple frozen encoders into a more versatile representation that can be directly fed as the input to a frozen LM. BRAVE achieves state-of-the-art performance on a broad range of captioning and VQA benchmarks and significantly reduces the aforementioned issues of VLMs, while requiring a smaller number of trainable parameters than existing methods and having a more compressed representation. Our results highlight the potential of incorporating different visual biases for a more broad and contextualized visual understanding of VLMs.","sentences":["Vision-language models (VLMs) are typically composed of a vision encoder, e.g. CLIP, and a language model (LM) that interprets the encoded features to solve downstream tasks.","Despite remarkable progress, VLMs are subject to several shortcomings due to the limited capabilities of vision encoders, e.g. \"blindness\" to certain image features, visual hallucination, etc.","To address these issues, we study broadening the visual encoding capabilities of VLMs.","We first comprehensively benchmark several vision encoders with different inductive biases for solving VLM tasks.","We observe that there is no single encoding configuration that consistently achieves top performance across different tasks, and encoders with different biases can perform surprisingly similarly.","Motivated by this, we introduce a method, named BRAVE, that consolidates features from multiple frozen encoders into a more versatile representation that can be directly fed as the input to a frozen LM.","BRAVE achieves state-of-the-art performance on a broad range of captioning and VQA benchmarks and significantly reduces the aforementioned issues of VLMs, while requiring a smaller number of trainable parameters than existing methods and having a more compressed representation.","Our results highlight the potential of incorporating different visual biases for a more broad and contextualized visual understanding of VLMs."],"url":"http://arxiv.org/abs/2404.07204v1","category":"cs.CV"}
{"created":"2024-04-10 17:59:20","title":"UMBRAE: Unified Multimodal Decoding of Brain Signals","abstract":"We address prevailing challenges of the brain-powered research, departing from the observation that the literature hardly recover accurate spatial information and require subject-specific models. To address these challenges, we propose UMBRAE, a unified multimodal decoding of brain signals. First, to extract instance-level conceptual and spatial details from neural signals, we introduce an efficient universal brain encoder for multimodal-brain alignment and recover object descriptions at multiple levels of granularity from subsequent multimodal large language model (MLLM). Second, we introduce a cross-subject training strategy mapping subject-specific features to a common feature space. This allows a model to be trained on multiple subjects without extra resources, even yielding superior results compared to subject-specific models. Further, we demonstrate this supports weakly-supervised adaptation to new subjects, with only a fraction of the total training data. Experiments demonstrate that UMBRAE not only achieves superior results in the newly introduced tasks but also outperforms methods in well established tasks. To assess our method, we construct and share with the community a comprehensive brain understanding benchmark BrainHub. Our code and benchmark are available at https://weihaox.github.io/UMBRAE.","sentences":["We address prevailing challenges of the brain-powered research, departing from the observation that the literature hardly recover accurate spatial information and require subject-specific models.","To address these challenges, we propose UMBRAE, a unified multimodal decoding of brain signals.","First, to extract instance-level conceptual and spatial details from neural signals, we introduce an efficient universal brain encoder for multimodal-brain alignment and recover object descriptions at multiple levels of granularity from subsequent multimodal large language model (MLLM).","Second, we introduce a cross-subject training strategy mapping subject-specific features to a common feature space.","This allows a model to be trained on multiple subjects without extra resources, even yielding superior results compared to subject-specific models.","Further, we demonstrate this supports weakly-supervised adaptation to new subjects, with only a fraction of the total training data.","Experiments demonstrate that UMBRAE not only achieves superior results in the newly introduced tasks but also outperforms methods in well established tasks.","To assess our method, we construct and share with the community a comprehensive brain understanding benchmark BrainHub.","Our code and benchmark are available at https://weihaox.github.io/UMBRAE."],"url":"http://arxiv.org/abs/2404.07202v1","category":"cs.CV"}
{"created":"2024-04-10 17:57:41","title":"RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth Diffusion","abstract":"We introduce RealmDreamer, a technique for generation of general forward-facing 3D scenes from text descriptions. Our technique optimizes a 3D Gaussian Splatting representation to match complex text prompts. We initialize these splats by utilizing the state-of-the-art text-to-image generators, lifting their samples into 3D, and computing the occlusion volume. We then optimize this representation across multiple views as a 3D inpainting task with image-conditional diffusion models. To learn correct geometric structure, we incorporate a depth diffusion model by conditioning on the samples from the inpainting model, giving rich geometric structure. Finally, we finetune the model using sharpened samples from image generators. Notably, our technique does not require video or multi-view data and can synthesize a variety of high-quality 3D scenes in different styles, consisting of multiple objects. Its generality additionally allows 3D synthesis from a single image.","sentences":["We introduce RealmDreamer, a technique for generation of general forward-facing 3D scenes from text descriptions.","Our technique optimizes a 3D Gaussian Splatting representation to match complex text prompts.","We initialize these splats by utilizing the state-of-the-art text-to-image generators, lifting their samples into 3D, and computing the occlusion volume.","We then optimize this representation across multiple views as a 3D inpainting task with image-conditional diffusion models.","To learn correct geometric structure, we incorporate a depth diffusion model by conditioning on the samples from the inpainting model, giving rich geometric structure.","Finally, we finetune the model using sharpened samples from image generators.","Notably, our technique does not require video or multi-view data and can synthesize a variety of high-quality 3D scenes in different styles, consisting of multiple objects.","Its generality additionally allows 3D synthesis from a single image."],"url":"http://arxiv.org/abs/2404.07199v1","category":"cs.CV"}
{"created":"2024-04-10 17:56:07","title":"Zero-shot Logical Query Reasoning on any Knowledge Graph","abstract":"Complex logical query answering (CLQA) in knowledge graphs (KGs) goes beyond simple KG completion and aims at answering compositional queries comprised of multiple projections and logical operations. Existing CLQA methods that learn parameters bound to certain entity or relation vocabularies can only be applied to the graph they are trained on which requires substantial training time before being deployed on a new graph. Here we present UltraQuery, an inductive reasoning model that can zero-shot answer logical queries on any KG. The core idea of UltraQuery is to derive both projections and logical operations as vocabulary-independent functions which generalize to new entities and relations in any KG. With the projection operation initialized from a pre-trained inductive KG reasoning model, UltraQuery can solve CLQA on any KG even if it is only finetuned on a single dataset. Experimenting on 23 datasets, UltraQuery in the zero-shot inference mode shows competitive or better query answering performance than best available baselines and sets a new state of the art on 14 of them.","sentences":["Complex logical query answering (CLQA) in knowledge graphs (KGs) goes beyond simple KG completion and aims at answering compositional queries comprised of multiple projections and logical operations.","Existing CLQA methods that learn parameters bound to certain entity or relation vocabularies can only be applied to the graph they are trained on which requires substantial training time before being deployed on a new graph.","Here we present UltraQuery, an inductive reasoning model that can zero-shot answer logical queries on any KG.","The core idea of UltraQuery is to derive both projections and logical operations as vocabulary-independent functions which generalize to new entities and relations in any KG.","With the projection operation initialized from a pre-trained inductive KG reasoning model, UltraQuery can solve CLQA on any KG even if it is only finetuned on a single dataset.","Experimenting on 23 datasets, UltraQuery in the zero-shot inference mode shows competitive or better query answering performance than best available baselines and sets a new state of the art on 14 of them."],"url":"http://arxiv.org/abs/2404.07198v1","category":"cs.AI"}
{"created":"2024-04-10 17:50:29","title":"VN-EGNN: E(3)-Equivariant Graph Neural Networks with Virtual Nodes Enhance Protein Binding Site Identification","abstract":"Being able to identify regions within or around proteins, to which ligands can potentially bind, is an essential step to develop new drugs. Binding site identification methods can now profit from the availability of large amounts of 3D structures in protein structure databases or from AlphaFold predictions. Current binding site identification methods heavily rely on graph neural networks (GNNs), usually designed to output E(3)-equivariant predictions. Such methods turned out to be very beneficial for physics-related tasks like binding energy or motion trajectory prediction. However, the performance of GNNs at binding site identification is still limited potentially due to the lack of dedicated nodes that model hidden geometric entities, such as binding pockets. In this work, we extend E(n)-Equivariant Graph Neural Networks (EGNNs) by adding virtual nodes and applying an extended message passing scheme. The virtual nodes in these graphs are dedicated quantities to learn representations of binding sites, which leads to improved predictive performance. In our experiments, we show that our proposed method VN-EGNN sets a new state-of-the-art at locating binding site centers on COACH420, HOLO4K and PDBbind2020.","sentences":["Being able to identify regions within or around proteins, to which ligands can potentially bind, is an essential step to develop new drugs.","Binding site identification methods can now profit from the availability of large amounts of 3D structures in protein structure databases or from AlphaFold predictions.","Current binding site identification methods heavily rely on graph neural networks (GNNs), usually designed to output E(3)-equivariant predictions.","Such methods turned out to be very beneficial for physics-related tasks like binding energy or motion trajectory prediction.","However, the performance of GNNs at binding site identification is still limited potentially due to the lack of dedicated nodes that model hidden geometric entities, such as binding pockets.","In this work, we extend E(n)-Equivariant Graph Neural Networks (EGNNs) by adding virtual nodes and applying an extended message passing scheme.","The virtual nodes in these graphs are dedicated quantities to learn representations of binding sites, which leads to improved predictive performance.","In our experiments, we show that our proposed method VN-EGNN sets a new state-of-the-art at locating binding site centers on COACH420, HOLO4K and PDBbind2020."],"url":"http://arxiv.org/abs/2404.07194v1","category":"cs.LG"}
{"created":"2024-04-10 17:40:27","title":"Reward Learning from Suboptimal Demonstrations with Applications in Surgical Electrocautery","abstract":"Automating robotic surgery via learning from demonstration (LfD) techniques is extremely challenging. This is because surgical tasks often involve sequential decision-making processes with complex interactions of physical objects and have low tolerance for mistakes. Prior works assume that all demonstrations are fully observable and optimal, which might not be practical in the real world. This paper introduces a sample-efficient method that learns a robust reward function from a limited amount of ranked suboptimal demonstrations consisting of partial-view point cloud observations. The method then learns a policy by optimizing the learned reward function using reinforcement learning (RL). We show that using a learned reward function to obtain a policy is more robust than pure imitation learning. We apply our approach on a physical surgical electrocautery task and demonstrate that our method can perform well even when the provided demonstrations are suboptimal and the observations are high-dimensional point clouds.","sentences":["Automating robotic surgery via learning from demonstration (LfD) techniques is extremely challenging.","This is because surgical tasks often involve sequential decision-making processes with complex interactions of physical objects and have low tolerance for mistakes.","Prior works assume that all demonstrations are fully observable and optimal, which might not be practical in the real world.","This paper introduces a sample-efficient method that learns a robust reward function from a limited amount of ranked suboptimal demonstrations consisting of partial-view point cloud observations.","The method then learns a policy by optimizing the learned reward function using reinforcement learning (RL).","We show that using a learned reward function to obtain a policy is more robust than pure imitation learning.","We apply our approach on a physical surgical electrocautery task and demonstrate that our method can perform well even when the provided demonstrations are suboptimal and the observations are high-dimensional point clouds."],"url":"http://arxiv.org/abs/2404.07185v1","category":"cs.RO"}
{"created":"2024-04-10 17:05:12","title":"Worst-Case Convergence Time of ML Algorithms via Extreme Value Theory","abstract":"This paper leverages the statistics of extreme values to predict the worst-case convergence times of machine learning algorithms. Timing is a critical non-functional property of ML systems, and providing the worst-case converge times is essential to guarantee the availability of ML and its services. However, timing properties such as worst-case convergence times (WCCT) are difficult to verify since (1) they are not encoded in the syntax or semantics of underlying programming languages of AI, (2) their evaluations depend on both algorithmic implementations and underlying systems, and (3) their measurements involve uncertainty and noise. Therefore, prevalent formal methods and statistical models fail to provide rich information on the amounts and likelihood of WCCT.   Our key observation is that the timing information we seek represents the extreme tail of execution times. Therefore, extreme value theory (EVT), a statistical discipline that focuses on understanding and predicting the distribution of extreme values in the tail of outcomes, provides an ideal framework to model and analyze WCCT in the training and inference phases of ML paradigm. Building upon the mathematical tools from EVT, we propose a practical framework to predict the worst-case timing properties of ML. Over a set of linear ML training algorithms, we show that EVT achieves a better accuracy for predicting WCCTs than relevant statistical methods such as the Bayesian factor. On the set of larger machine learning training algorithms and deep neural network inference, we show the feasibility and usefulness of EVT models to accurately predict WCCTs, their expected return periods, and their likelihood.","sentences":["This paper leverages the statistics of extreme values to predict the worst-case convergence times of machine learning algorithms.","Timing is a critical non-functional property of ML systems, and providing the worst-case converge times is essential to guarantee the availability of ML and its services.","However, timing properties such as worst-case convergence times (WCCT) are difficult to verify since (1) they are not encoded in the syntax or semantics of underlying programming languages of AI, (2) their evaluations depend on both algorithmic implementations and underlying systems, and (3) their measurements involve uncertainty and noise.","Therefore, prevalent formal methods and statistical models fail to provide rich information on the amounts and likelihood of WCCT.   ","Our key observation is that the timing information we seek represents the extreme tail of execution times.","Therefore, extreme value theory (EVT), a statistical discipline that focuses on understanding and predicting the distribution of extreme values in the tail of outcomes, provides an ideal framework to model and analyze WCCT in the training and inference phases of ML paradigm.","Building upon the mathematical tools from EVT, we propose a practical framework to predict the worst-case timing properties of ML.","Over a set of linear ML training algorithms, we show that EVT achieves a better accuracy for predicting WCCTs than relevant statistical methods such as the Bayesian factor.","On the set of larger machine learning training algorithms and deep neural network inference, we show the feasibility and usefulness of EVT models to accurately predict WCCTs, their expected return periods, and their likelihood."],"url":"http://arxiv.org/abs/2404.07170v1","category":"cs.SE"}
{"created":"2024-04-10 17:04:06","title":"Using Neural Networks to Model Hysteretic Kinematics in Tendon-Actuated Continuum Robots","abstract":"The ability to accurately model mechanical hysteretic behavior in tendon-actuated continuum robots using deep learning approaches is a growing area of interest. In this paper, we investigate the hysteretic response of two types of tendon-actuated continuum robots and, ultimately, compare three types of neural network modeling approaches with both forward and inverse kinematic mappings: feedforward neural network (FNN), FNN with a history input buffer, and long short-term memory (LSTM) network. We seek to determine which model best captures temporal dependent behavior. We find that, depending on the robot's design, choosing different kinematic inputs can alter whether hysteresis is exhibited by the system. Furthermore, we present the results of the model fittings, revealing that, in contrast to the standard FNN, both FNN with a history input buffer and the LSTM model exhibit the capacity to model historical dependence with comparable performance in capturing rate-dependent hysteresis.","sentences":["The ability to accurately model mechanical hysteretic behavior in tendon-actuated continuum robots using deep learning approaches is a growing area of interest.","In this paper, we investigate the hysteretic response of two types of tendon-actuated continuum robots and, ultimately, compare three types of neural network modeling approaches with both forward and inverse kinematic mappings: feedforward neural network (FNN), FNN with a history input buffer, and long short-term memory (LSTM) network.","We seek to determine which model best captures temporal dependent behavior.","We find that, depending on the robot's design, choosing different kinematic inputs can alter whether hysteresis is exhibited by the system.","Furthermore, we present the results of the model fittings, revealing that, in contrast to the standard FNN, both FNN with a history input buffer and the LSTM model exhibit the capacity to model historical dependence with comparable performance in capturing rate-dependent hysteresis."],"url":"http://arxiv.org/abs/2404.07168v1","category":"cs.RO"}
{"created":"2024-04-10 17:00:04","title":"Analysis of Distributed Optimization Algorithms on a Real Processing-In-Memory System","abstract":"Machine Learning (ML) training on large-scale datasets is a very expensive and time-consuming workload. Processor-centric architectures (e.g., CPU, GPU) commonly used for modern ML training workloads are limited by the data movement bottleneck, i.e., due to repeatedly accessing the training dataset. As a result, processor-centric systems suffer from performance degradation and high energy consumption. Processing-In-Memory (PIM) is a promising solution to alleviate the data movement bottleneck by placing the computation mechanisms inside or near memory.   Our goal is to understand the capabilities and characteristics of popular distributed optimization algorithms on real-world PIM architectures to accelerate data-intensive ML training workloads. To this end, we 1) implement several representative centralized distributed optimization algorithms on UPMEM's real-world general-purpose PIM system, 2) rigorously evaluate these algorithms for ML training on large-scale datasets in terms of performance, accuracy, and scalability, 3) compare to conventional CPU and GPU baselines, and 4) discuss implications for future PIM hardware and the need to shift to an algorithm-hardware codesign perspective to accommodate decentralized distributed optimization algorithms.   Our results demonstrate three major findings: 1) Modern general-purpose PIM architectures can be a viable alternative to state-of-the-art CPUs and GPUs for many memory-bound ML training workloads, when operations and datatypes are natively supported by PIM hardware, 2) the importance of carefully choosing the optimization algorithm that best fit PIM, and 3) contrary to popular belief, contemporary PIM architectures do not scale approximately linearly with the number of nodes for many data-intensive ML training workloads. To facilitate future research, we aim to open-source our complete codebase.","sentences":["Machine Learning (ML) training on large-scale datasets is a very expensive and time-consuming workload.","Processor-centric architectures (e.g., CPU, GPU) commonly used for modern ML training workloads are limited by the data movement bottleneck, i.e., due to repeatedly accessing the training dataset.","As a result, processor-centric systems suffer from performance degradation and high energy consumption.","Processing-In-Memory (PIM) is a promising solution to alleviate the data movement bottleneck by placing the computation mechanisms inside or near memory.   ","Our goal is to understand the capabilities and characteristics of popular distributed optimization algorithms on real-world PIM architectures to accelerate data-intensive ML training workloads.","To this end, we 1) implement several representative centralized distributed optimization algorithms on UPMEM's real-world general-purpose PIM system, 2) rigorously evaluate these algorithms for ML training on large-scale datasets in terms of performance, accuracy, and scalability, 3) compare to conventional CPU and GPU baselines, and 4) discuss implications for future PIM hardware and the need to shift to an algorithm-hardware codesign perspective to accommodate decentralized distributed optimization algorithms.   ","Our results demonstrate three major findings: 1) Modern general-purpose PIM architectures can be a viable alternative to state-of-the-art CPUs and GPUs for many memory-bound ML training workloads, when operations and datatypes are natively supported by PIM hardware, 2) the importance of carefully choosing the optimization algorithm that best fit PIM, and 3) contrary to popular belief, contemporary PIM architectures do not scale approximately linearly with the number of nodes for many data-intensive ML training workloads.","To facilitate future research, we aim to open-source our complete codebase."],"url":"http://arxiv.org/abs/2404.07164v1","category":"cs.AR"}
{"created":"2024-04-10 16:50:07","title":"Exploring Physiological Responses in Virtual Reality-based Interventions for Autism Spectrum Disorder: A Data-Driven Investigation","abstract":"Virtual Reality (VR) has emerged as a promising tool for enhancing social skills and emotional well-being in individuals with Autism Spectrum Disorder (ASD). Through a technical exploration, this study employs a multiplayer serious gaming environment within VR, engaging 34 individuals diagnosed with ASD and employing high-precision biosensors for a comprehensive view of the participants' arousal and responses during the VR sessions. Participants were subjected to a series of 3 virtual scenarios designed in collaboration with stakeholders and clinical experts to promote socio-cognitive skills and emotional regulation in a controlled and structured virtual environment. We combined the framework with wearable non-invasive sensors for bio-signal acquisition, focusing on the collection of heart rate variability, and respiratory patterns to monitor participants behaviors. Further, behavioral assessments were conducted using observation and semi-structured interviews, with the data analyzed in conjunction with physiological measures to identify correlations and explore digital-intervention efficacy. Preliminary analysis revealed significant correlations between physiological responses and behavioral outcomes, indicating the potential of physiological feedback to enhance VR-based interventions for ASD. The study demonstrated the feasibility of using real-time data to adapt virtual scenarios, suggesting a promising avenue to support personalized therapy. The integration of quantitative physiological feedback into digital platforms represents a forward step in the personalized intervention for ASD. By leveraging real-time data to adjust therapeutic content, this approach promises to enhance the efficacy and engagement of digital-based therapies.","sentences":["Virtual Reality (VR) has emerged as a promising tool for enhancing social skills and emotional well-being in individuals with Autism Spectrum Disorder (ASD).","Through a technical exploration, this study employs a multiplayer serious gaming environment within VR, engaging 34 individuals diagnosed with ASD and employing high-precision biosensors for a comprehensive view of the participants' arousal and responses during the VR sessions.","Participants were subjected to a series of 3 virtual scenarios designed in collaboration with stakeholders and clinical experts to promote socio-cognitive skills and emotional regulation in a controlled and structured virtual environment.","We combined the framework with wearable non-invasive sensors for bio-signal acquisition, focusing on the collection of heart rate variability, and respiratory patterns to monitor participants behaviors.","Further, behavioral assessments were conducted using observation and semi-structured interviews, with the data analyzed in conjunction with physiological measures to identify correlations and explore digital-intervention efficacy.","Preliminary analysis revealed significant correlations between physiological responses and behavioral outcomes, indicating the potential of physiological feedback to enhance VR-based interventions for ASD.","The study demonstrated the feasibility of using real-time data to adapt virtual scenarios, suggesting a promising avenue to support personalized therapy.","The integration of quantitative physiological feedback into digital platforms represents a forward step in the personalized intervention for ASD.","By leveraging real-time data to adjust therapeutic content, this approach promises to enhance the efficacy and engagement of digital-based therapies."],"url":"http://arxiv.org/abs/2404.07159v1","category":"cs.HC"}
{"created":"2024-04-10 16:18:42","title":"Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention","abstract":"This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.","sentences":["This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation.","A key component in our proposed approach is a new attention technique dubbed Infini-attention.","The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block.","We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs.","Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs."],"url":"http://arxiv.org/abs/2404.07143v1","category":"cs.CL"}
{"created":"2024-04-10 16:18:11","title":"Bridging Gaps, Building Futures: Advancing Software Developer Diversity and Inclusion Through Future-Oriented Research","abstract":"Software systems are responsible for nearly all aspects of modern life and society. However, the demographics of software development teams that are tasked with designing and maintaining these software systems rarely match the demographics of users. As the landscape of software engineering (SE) evolves due to technological innovations, such as the rise of automated programming assistants powered by artificial intelligence (AI) and machine learning, more effort is needed to promote software developer diversity and inclusion (SDDI) to ensure inclusive work environments for development teams and usable software for diverse populations. To this end, we present insights from SE researchers and practitioners on challenges and solutions regarding diversity and inclusion in SE. Based on these findings, we share potential utopian and dystopian visions of the future and provide future research directions and implications for academia and industry to promote SDDI in the age of AI-driven SE.","sentences":["Software systems are responsible for nearly all aspects of modern life and society.","However, the demographics of software development teams that are tasked with designing and maintaining these software systems rarely match the demographics of users.","As the landscape of software engineering (SE) evolves due to technological innovations, such as the rise of automated programming assistants powered by artificial intelligence (AI) and machine learning, more effort is needed to promote software developer diversity and inclusion (SDDI) to ensure inclusive work environments for development teams and usable software for diverse populations.","To this end, we present insights from SE researchers and practitioners on challenges and solutions regarding diversity and inclusion in SE.","Based on these findings, we share potential utopian and dystopian visions of the future and provide future research directions and implications for academia and industry to promote SDDI in the age of AI-driven SE."],"url":"http://arxiv.org/abs/2404.07142v1","category":"cs.SE"}
{"created":"2024-04-10 16:14:05","title":"Towards a Game-theoretic Understanding of Explanation-based Membership Inference Attacks","abstract":"Model explanations improve the transparency of black-box machine learning (ML) models and their decisions; however, they can also be exploited to carry out privacy threats such as membership inference attacks (MIA). Existing works have only analyzed MIA in a single \"what if\" interaction scenario between an adversary and the target ML model; thus, it does not discern the factors impacting the capabilities of an adversary in launching MIA in repeated interaction settings. Additionally, these works rely on assumptions about the adversary's knowledge of the target model's structure and, thus, do not guarantee the optimality of the predefined threshold required to distinguish the members from non-members. In this paper, we delve into the domain of explanation-based threshold attacks, where the adversary endeavors to carry out MIA attacks by leveraging the variance of explanations through iterative interactions with the system comprising of the target ML model and its corresponding explanation method. We model such interactions by employing a continuous-time stochastic signaling game framework. In our framework, an adversary plays a stopping game, interacting with the system (having imperfect information about the type of an adversary, i.e., honest or malicious) to obtain explanation variance information and computing an optimal threshold to determine the membership of a datapoint accurately. First, we propose a sound mathematical formulation to prove that such an optimal threshold exists, which can be used to launch MIA. Then, we characterize the conditions under which a unique Markov perfect equilibrium (or steady state) exists in this dynamic system. By means of a comprehensive set of simulations of the proposed game model, we assess different factors that can impact the capability of an adversary to launch MIA in such repeated interaction settings.","sentences":["Model explanations improve the transparency of black-box machine learning (ML) models and their decisions; however, they can also be exploited to carry out privacy threats such as membership inference attacks (MIA).","Existing works have only analyzed MIA in a single \"what if\" interaction scenario between an adversary and the target ML model; thus, it does not discern the factors impacting the capabilities of an adversary in launching MIA in repeated interaction settings.","Additionally, these works rely on assumptions about the adversary's knowledge of the target model's structure and, thus, do not guarantee the optimality of the predefined threshold required to distinguish the members from non-members.","In this paper, we delve into the domain of explanation-based threshold attacks, where the adversary endeavors to carry out MIA attacks by leveraging the variance of explanations through iterative interactions with the system comprising of the target ML model and its corresponding explanation method.","We model such interactions by employing a continuous-time stochastic signaling game framework.","In our framework, an adversary plays a stopping game, interacting with the system (having imperfect information about the type of an adversary, i.e., honest or malicious) to obtain explanation variance information and computing an optimal threshold to determine the membership of a datapoint accurately.","First, we propose a sound mathematical formulation to prove that such an optimal threshold exists, which can be used to launch MIA.","Then, we characterize the conditions under which a unique Markov perfect equilibrium (or steady state) exists in this dynamic system.","By means of a comprehensive set of simulations of the proposed game model, we assess different factors that can impact the capability of an adversary to launch MIA in such repeated interaction settings."],"url":"http://arxiv.org/abs/2404.07139v1","category":"cs.AI"}
{"created":"2024-04-10 16:12:50","title":"Towards Robustness of Text-to-Visualization Translation against Lexical and Phrasal Variability","abstract":"Text-to-Vis is an emerging task in the natural language processing (NLP) area that aims to automatically generate data visualizations from natural language questions (NLQs). Despite their progress, existing text-to-vis models often heavily rely on lexical matching between words in the questions and tokens in data schemas. This overreliance on lexical matching may lead to a diminished level of model robustness against input variations. In this study, we thoroughly examine the robustness of current text-to-vis models, an area that has not previously been explored. In particular, we construct the first robustness dataset nvBench-Rob, which contains diverse lexical and phrasal variations based on the original text-to-vis benchmark nvBench. Then, we found that the performance of existing text-to-vis models on this new dataset dramatically drops, implying that these methods exhibit inadequate robustness overall. Finally, we propose a novel framework based on Retrieval-Augmented Generation (RAG) technique, named GRED, specifically designed to address input perturbations in these two variants. The framework consists of three parts: NLQ-Retrieval Generator, Visualization Query-Retrieval Retuner and Annotation-based Debugger, which are used to tackle the challenges posed by natural language variants, programming style differences and data schema variants, respectively. Extensive experimental evaluations show that, compared to the state-of-the-art model RGVisNet in the Text-to-Vis field, GRED performs better in terms of model robustness, with a 32% increase in accuracy on the proposed nvBench-Rob dataset.","sentences":["Text-to-Vis is an emerging task in the natural language processing (NLP) area that aims to automatically generate data visualizations from natural language questions (NLQs).","Despite their progress, existing text-to-vis models often heavily rely on lexical matching between words in the questions and tokens in data schemas.","This overreliance on lexical matching may lead to a diminished level of model robustness against input variations.","In this study, we thoroughly examine the robustness of current text-to-vis models, an area that has not previously been explored.","In particular, we construct the first robustness dataset nvBench-Rob, which contains diverse lexical and phrasal variations based on the original text-to-vis benchmark nvBench.","Then, we found that the performance of existing text-to-vis models on this new dataset dramatically drops, implying that these methods exhibit inadequate robustness overall.","Finally, we propose a novel framework based on Retrieval-Augmented Generation (RAG) technique, named GRED, specifically designed to address input perturbations in these two variants.","The framework consists of three parts: NLQ-Retrieval Generator, Visualization Query-Retrieval Retuner and Annotation-based Debugger, which are used to tackle the challenges posed by natural language variants, programming style differences and data schema variants, respectively.","Extensive experimental evaluations show that, compared to the state-of-the-art model RGVisNet in the Text-to-Vis field, GRED performs better in terms of model robustness, with a 32% increase in accuracy on the proposed nvBench-Rob dataset."],"url":"http://arxiv.org/abs/2404.07135v2","category":"cs.CL"}
{"created":"2024-04-10 16:04:21","title":"Measuring proximity to standard planes during fetal brain ultrasound scanning","abstract":"This paper introduces a novel pipeline designed to bring ultrasound (US) plane pose estimation closer to clinical use for more effective navigation to the standard planes (SPs) in the fetal brain. We propose a semi-supervised segmentation model utilizing both labeled SPs and unlabeled 3D US volume slices. Our model enables reliable segmentation across a diverse set of fetal brain images. Furthermore, the model incorporates a classification mechanism to identify the fetal brain precisely. Our model not only filters out frames lacking the brain but also generates masks for those containing it, enhancing the relevance of plane pose regression in clinical settings. We focus on fetal brain navigation from 2D ultrasound (US) video analysis and combine this model with a US plane pose regression network to provide sensorless proximity detection to SPs and non-SPs planes; we emphasize the importance of proximity detection to SPs for guiding sonographers, offering a substantial advantage over traditional methods by allowing earlier and more precise adjustments during scanning. We demonstrate the practical applicability of our approach through validation on real fetal scan videos obtained from sonographers of varying expertise levels. Our findings demonstrate the potential of our approach to complement existing fetal US technologies and advance prenatal diagnostic practices.","sentences":["This paper introduces a novel pipeline designed to bring ultrasound (US) plane pose estimation closer to clinical use for more effective navigation to the standard planes (SPs) in the fetal brain.","We propose a semi-supervised segmentation model utilizing both labeled SPs and unlabeled 3D US volume slices.","Our model enables reliable segmentation across a diverse set of fetal brain images.","Furthermore, the model incorporates a classification mechanism to identify the fetal brain precisely.","Our model not only filters out frames lacking the brain but also generates masks for those containing it, enhancing the relevance of plane pose regression in clinical settings.","We focus on fetal brain navigation from 2D ultrasound (US) video analysis and combine this model with a US plane pose regression network to provide sensorless proximity detection to SPs and non-SPs planes; we emphasize the importance of proximity detection to SPs for guiding sonographers, offering a substantial advantage over traditional methods by allowing earlier and more precise adjustments during scanning.","We demonstrate the practical applicability of our approach through validation on real fetal scan videos obtained from sonographers of varying expertise levels.","Our findings demonstrate the potential of our approach to complement existing fetal US technologies and advance prenatal diagnostic practices."],"url":"http://arxiv.org/abs/2404.07124v1","category":"cs.CV"}
{"created":"2024-04-10 16:04:07","title":"Semantically-correlated memories in a dense associative model","abstract":"I introduce a novel associative memory model named Correlated Dense Associative Memory (CDAM), which integrates both auto- and hetero-association in a unified framework for continuous-valued memory patterns. Employing an arbitrary graph structure to semantically link memory patterns, CDAM is theoretically and numerically analysed, revealing four distinct dynamical modes: auto-association, narrow hetero-association, wide hetero-association, and neutral quiescence. Drawing inspiration from inhibitory modulation studies, I employ anti-Hebbian learning rules to control the range of hetero-association, extract multi-scale representations of community structures in graphs, and stabilise the recall of temporal sequences. Experimental demonstrations showcase CDAM's efficacy in handling real-world data, replicating a classical neuroscience experiment, performing image retrieval, and simulating arbitrary finite automata.","sentences":["I introduce a novel associative memory model named Correlated Dense Associative Memory (CDAM), which integrates both auto- and hetero-association in a unified framework for continuous-valued memory patterns.","Employing an arbitrary graph structure to semantically link memory patterns, CDAM is theoretically and numerically analysed, revealing four distinct dynamical modes: auto-association, narrow hetero-association, wide hetero-association, and neutral quiescence.","Drawing inspiration from inhibitory modulation studies, I employ anti-Hebbian learning rules to control the range of hetero-association, extract multi-scale representations of community structures in graphs, and stabilise the recall of temporal sequences.","Experimental demonstrations showcase CDAM's efficacy in handling real-world data, replicating a classical neuroscience experiment, performing image retrieval, and simulating arbitrary finite automata."],"url":"http://arxiv.org/abs/2404.07123v2","category":"cs.NE"}
{"created":"2024-04-10 15:59:48","title":"Digital Over-the-Air Computation: Achieving High Reliability via Bit-Slicing","abstract":"6G mobile networks aim to realize ubiquitous intelligence at the network edge via distributed learning, sensing, and data analytics. Their common operation is to aggregate high-dimensional data, which causes a communication bottleneck that cannot be resolved using traditional orthogonal multi-access schemes. A promising solution, called over-the-air computation (AirComp), exploits channels' waveform superposition property to enable simultaneous access, thereby overcoming the bottleneck. Nevertheless, its reliance on uncoded linear analog modulation exposes data to perturbation by noise and interference. Hence, the traditional analog AirComp falls short of meeting the high-reliability requirement for 6G. Overcoming the limitation of analog AirComp motivates this work, which focuses on developing a framework for digital AirComp. The proposed framework features digital modulation of each data value, integrated with the bit-slicing technique to allocate its bits to multiple symbols, thereby increasing the AirComp reliability. To optimally detect the aggregated digital symbols, we derive the optimal maximum a posteriori detector that is shown to outperform the traditional maximum likelihood detector. Furthermore, a comparative performance analysis of digital AirComp with respect to its analog counterpart with repetition coding is conducted to quantify the practical signal-to-noise ratio (SNR) regime favoring the proposed scheme. On the other hand, digital AirComp is enhanced by further development to feature awareness of heterogeneous bit importance levels and its exploitation in channel adaptation. Lastly, simulation results demonstrate the achivability of substantial reliability improvement of digital AirComp over its analog counterpart given the same channel uses.","sentences":["6G mobile networks aim to realize ubiquitous intelligence at the network edge via distributed learning, sensing, and data analytics.","Their common operation is to aggregate high-dimensional data, which causes a communication bottleneck that cannot be resolved using traditional orthogonal multi-access schemes.","A promising solution, called over-the-air computation (AirComp), exploits channels' waveform superposition property to enable simultaneous access, thereby overcoming the bottleneck.","Nevertheless, its reliance on uncoded linear analog modulation exposes data to perturbation by noise and interference.","Hence, the traditional analog AirComp falls short of meeting the high-reliability requirement for 6G. Overcoming the limitation of analog AirComp motivates this work, which focuses on developing a framework for digital AirComp.","The proposed framework features digital modulation of each data value, integrated with the bit-slicing technique to allocate its bits to multiple symbols, thereby increasing the AirComp reliability.","To optimally detect the aggregated digital symbols, we derive the optimal maximum a posteriori detector that is shown to outperform the traditional maximum likelihood detector.","Furthermore, a comparative performance analysis of digital AirComp with respect to its analog counterpart with repetition coding is conducted to quantify the practical signal-to-noise ratio (SNR) regime favoring the proposed scheme.","On the other hand, digital AirComp is enhanced by further development to feature awareness of heterogeneous bit importance levels and its exploitation in channel adaptation.","Lastly, simulation results demonstrate the achivability of substantial reliability improvement of digital AirComp over its analog counterpart given the same channel uses."],"url":"http://arxiv.org/abs/2404.07121v1","category":"cs.IT"}
{"created":"2024-04-10 15:52:00","title":"\"My toxic trait is thinking I'll remember this\": gaps in the learner experience of video tutorials for feature-rich software","abstract":"Video tutorials are a popular medium for informal and formal learning. However, when learners attempt to view and follow along with these tutorials, they encounter what we call gaps, that is, issues that can prevent learning. We examine the gaps encountered by users of video tutorials for feature-rich software, such as spreadsheets. We develop a theory and taxonomy of such gaps, identifying how they act as barriers to learning, by collecting and analyzing 360 viewer comments from 90 Microsoft Excel video tutorials published by 43 creators across YouTube, TikTok, and Instagram. We conducted contextual interviews with 8 highly influential tutorial creators to investigate the gaps their viewers experience and how they address them. Further, we obtain insights into their creative process and frustrations when creating video tutorials. Finally, we present creators with two designs that aim to address gaps identified in the comment analysis for feedback and alternative design ideas.","sentences":["Video tutorials are a popular medium for informal and formal learning.","However, when learners attempt to view and follow along with these tutorials, they encounter what we call gaps, that is, issues that can prevent learning.","We examine the gaps encountered by users of video tutorials for feature-rich software, such as spreadsheets.","We develop a theory and taxonomy of such gaps, identifying how they act as barriers to learning, by collecting and analyzing 360 viewer comments from 90 Microsoft Excel video tutorials published by 43 creators across YouTube, TikTok, and Instagram.","We conducted contextual interviews with 8 highly influential tutorial creators to investigate the gaps their viewers experience and how they address them.","Further, we obtain insights into their creative process and frustrations when creating video tutorials.","Finally, we present creators with two designs that aim to address gaps identified in the comment analysis for feedback and alternative design ideas."],"url":"http://arxiv.org/abs/2404.07114v1","category":"cs.HC"}
{"created":"2024-04-10 15:39:49","title":"Rethinking Out-of-Distribution Detection for Reinforcement Learning: Advancing Methods for Evaluation and Detection","abstract":"While reinforcement learning (RL) algorithms have been successfully applied across numerous sequential decision-making problems, their generalization to unforeseen testing environments remains a significant concern. In this paper, we study the problem of out-of-distribution (OOD) detection in RL, which focuses on identifying situations at test time that RL agents have not encountered in their training environments. We first propose a clarification of terminology for OOD detection in RL, which aligns it with the literature from other machine learning domains. We then present new benchmark scenarios for OOD detection, which introduce anomalies with temporal autocorrelation into different components of the agent-environment loop. We argue that such scenarios have been understudied in the current literature, despite their relevance to real-world situations. Confirming our theoretical predictions, our experimental results suggest that state-of-the-art OOD detectors are not able to identify such anomalies. To address this problem, we propose a novel method for OOD detection, which we call DEXTER (Detection via Extraction of Time Series Representations). By treating environment observations as time series data, DEXTER extracts salient time series features, and then leverages an ensemble of isolation forest algorithms to detect anomalies. We find that DEXTER can reliably identify anomalies across benchmark scenarios, exhibiting superior performance compared to both state-of-the-art OOD detectors and high-dimensional changepoint detectors adopted from statistics.","sentences":["While reinforcement learning (RL) algorithms have been successfully applied across numerous sequential decision-making problems, their generalization to unforeseen testing environments remains a significant concern.","In this paper, we study the problem of out-of-distribution (OOD) detection in RL, which focuses on identifying situations at test time that RL agents have not encountered in their training environments.","We first propose a clarification of terminology for OOD detection in RL, which aligns it with the literature from other machine learning domains.","We then present new benchmark scenarios for OOD detection, which introduce anomalies with temporal autocorrelation into different components of the agent-environment loop.","We argue that such scenarios have been understudied in the current literature, despite their relevance to real-world situations.","Confirming our theoretical predictions, our experimental results suggest that state-of-the-art OOD detectors are not able to identify such anomalies.","To address this problem, we propose a novel method for OOD detection, which we call DEXTER (Detection via Extraction of Time Series Representations).","By treating environment observations as time series data, DEXTER extracts salient time series features, and then leverages an ensemble of isolation forest algorithms to detect anomalies.","We find that DEXTER can reliably identify anomalies across benchmark scenarios, exhibiting superior performance compared to both state-of-the-art OOD detectors and high-dimensional changepoint detectors adopted from statistics."],"url":"http://arxiv.org/abs/2404.07099v1","category":"cs.LG"}
{"created":"2024-04-10 15:36:59","title":"TransTARec: Time-Adaptive Translating Embedding Model for Next POI Recommendation","abstract":"The rapid growth of location acquisition technologies makes Point-of-Interest(POI) recommendation possible due to redundant user check-in records. In this paper, we focus on next POI recommendation in which next POI is based on previous POI. We observe that time plays an important role in next POI recommendation but is neglected in the recent proposed translating embedding methods. To tackle this shortage, we propose a time-adaptive translating embedding model (TransTARec) for next POI recommendation that naturally incorporates temporal influence, sequential dynamics, and user preference within a single component. Methodologically, we treat a (previous timestamp, user, next timestamp) triplet as a union translation vector and develop a neural-based fusion operation to fuse user preference and temporal influence. The superiority of TransTARec, which is confirmed by extensive experiments on real-world datasets, comes from not only the introduction of temporal influence but also the direct unification with user preference and sequential dynamics.","sentences":["The rapid growth of location acquisition technologies makes Point-of-Interest(POI) recommendation possible due to redundant user check-in records.","In this paper, we focus on next POI recommendation in which next POI is based on previous POI.","We observe that time plays an important role in next POI recommendation but is neglected in the recent proposed translating embedding methods.","To tackle this shortage, we propose a time-adaptive translating embedding model (TransTARec) for next POI recommendation that naturally incorporates temporal influence, sequential dynamics, and user preference within a single component.","Methodologically, we treat a (previous timestamp, user, next timestamp) triplet as a union translation vector and develop a neural-based fusion operation to fuse user preference and temporal influence.","The superiority of TransTARec, which is confirmed by extensive experiments on real-world datasets, comes from not only the introduction of temporal influence but also the direct unification with user preference and sequential dynamics."],"url":"http://arxiv.org/abs/2404.07096v1","category":"cs.IR"}
{"created":"2024-04-10 15:29:29","title":"LaTiM: Longitudinal representation learning in continuous-time models to predict disease progression","abstract":"This work proposes a novel framework for analyzing disease progression using time-aware neural ordinary differential equations (NODE). We introduce a \"time-aware head\" in a framework trained through self-supervised learning (SSL) to leverage temporal information in latent space for data augmentation. This approach effectively integrates NODEs with SSL, offering significant performance improvements compared to traditional methods that lack explicit temporal integration. We demonstrate the effectiveness of our strategy for diabetic retinopathy progression prediction using the OPHDIAT database. Compared to the baseline, all NODE architectures achieve statistically significant improvements in area under the ROC curve (AUC) and Kappa metrics, highlighting the efficacy of pre-training with SSL-inspired approaches. Additionally, our framework promotes stable training for NODEs, a commonly encountered challenge in time-aware modeling.","sentences":["This work proposes a novel framework for analyzing disease progression using time-aware neural ordinary differential equations (NODE).","We introduce a \"time-aware head\" in a framework trained through self-supervised learning (SSL) to leverage temporal information in latent space for data augmentation.","This approach effectively integrates NODEs with SSL, offering significant performance improvements compared to traditional methods that lack explicit temporal integration.","We demonstrate the effectiveness of our strategy for diabetic retinopathy progression prediction using the OPHDIAT database.","Compared to the baseline, all NODE architectures achieve statistically significant improvements in area under the ROC curve (AUC) and Kappa metrics, highlighting the efficacy of pre-training with SSL-inspired approaches.","Additionally, our framework promotes stable training for NODEs, a commonly encountered challenge in time-aware modeling."],"url":"http://arxiv.org/abs/2404.07091v1","category":"cs.LG"}
{"created":"2024-04-10 15:17:17","title":"Dynamic Generation of Personalities with Large Language Models","abstract":"In the realm of mimicking human deliberation, large language models (LLMs) show promising performance, thereby amplifying the importance of this research area. Deliberation is influenced by both logic and personality. However, previous studies predominantly focused on the logic of LLMs, neglecting the exploration of personality aspects. In this work, we introduce Dynamic Personality Generation (DPG), a dynamic personality generation method based on Hypernetworks. Initially, we embed the Big Five personality theory into GPT-4 to form a personality assessment machine, enabling it to evaluate characters' personality traits from dialogues automatically. We propose a new metric to assess personality generation capability based on this evaluation method. Then, we use this personality assessment machine to evaluate dialogues in script data, resulting in a personality-dialogue dataset. Finally, we fine-tune DPG on the personality-dialogue dataset. Experiments prove that DPG's personality generation capability is stronger after fine-tuning on this dataset than traditional fine-tuning methods, surpassing prompt-based GPT-4.","sentences":["In the realm of mimicking human deliberation, large language models (LLMs) show promising performance, thereby amplifying the importance of this research area.","Deliberation is influenced by both logic and personality.","However, previous studies predominantly focused on the logic of LLMs, neglecting the exploration of personality aspects.","In this work, we introduce Dynamic Personality Generation (DPG), a dynamic personality generation method based on Hypernetworks.","Initially, we embed the Big Five personality theory into GPT-4 to form a personality assessment machine, enabling it to evaluate characters' personality traits from dialogues automatically.","We propose a new metric to assess personality generation capability based on this evaluation method.","Then, we use this personality assessment machine to evaluate dialogues in script data, resulting in a personality-dialogue dataset.","Finally, we fine-tune DPG on the personality-dialogue dataset.","Experiments prove that DPG's personality generation capability is stronger after fine-tuning on this dataset than traditional fine-tuning methods, surpassing prompt-based GPT-4."],"url":"http://arxiv.org/abs/2404.07084v1","category":"cs.CL"}
{"created":"2024-04-10 14:56:40","title":"Exploring Concept Depth: How Large Language Models Acquire Knowledge at Different Layers?","abstract":"This paper studies the phenomenon that different concepts are learned in different layers of large language models, i.e. more difficult concepts are fully acquired with deeper layers. We define the difficulty of concepts by the level of abstraction, and here it is crudely categorized by factual, emotional, and inferential. Each category contains a spectrum of tasks, arranged from simple to complex. For example, within the factual dimension, tasks range from lie detection to categorizing mathematical problems. We employ a probing technique to extract representations from different layers of the model and apply these to classification tasks. Our findings reveal that models tend to efficiently classify simpler tasks, indicating that these concepts are learned in shallower layers. Conversely, more complex tasks may only be discernible at deeper layers, if at all. This paper explores the implications of these findings for our understanding of model learning processes and internal representations. Our implementation is available at \\url{https://github.com/Luckfort/CD}.","sentences":["This paper studies the phenomenon that different concepts are learned in different layers of large language models, i.e. more difficult concepts are fully acquired with deeper layers.","We define the difficulty of concepts by the level of abstraction, and here it is crudely categorized by factual, emotional, and inferential.","Each category contains a spectrum of tasks, arranged from simple to complex.","For example, within the factual dimension, tasks range from lie detection to categorizing mathematical problems.","We employ a probing technique to extract representations from different layers of the model and apply these to classification tasks.","Our findings reveal that models tend to efficiently classify simpler tasks, indicating that these concepts are learned in shallower layers.","Conversely, more complex tasks may only be discernible at deeper layers, if at all.","This paper explores the implications of these findings for our understanding of model learning processes and internal representations.","Our implementation is available at \\url{https://github.com/Luckfort/CD}."],"url":"http://arxiv.org/abs/2404.07066v1","category":"cs.CL"}
{"created":"2024-04-10 14:52:35","title":"LaPlaSS: Latent Space Planning for Stochastic Systems","abstract":"Autonomous mobile agents often operate in hazardous environments, necessitating an awareness of safety. These agents can have non-linear, stochastic dynamics that must be considered during planning to guarantee bounded risk. Most state of the art methods require closed-form dynamics to verify plan correctness and safety however modern robotic systems often have dynamics that are learned from data. Thus, there is a need to perform efficient trajectory planning with guarantees on risk for agents without known dynamics models. We propose a \"generate-and-test\" approach to risk-bounded planning in which a planner generates a candidate trajectory using an approximate linear dynamics model and a validator assesses the risk of the trajectory, computing additional safety constraints for the planner if the candidate does not satisfy the desired risk bound. To acquire the approximate model, we use a variational autoencoder to learn a latent linear dynamics model and encode the planning problem into the latent space to generate the candidate trajectory. The VAE also serves to sample trajectories around the candidate to use in the validator. We demonstrate that our algorithm, LaPlaSS, is able to generate trajectory plans with bounded risk for a real-world agent with learned dynamics and is an order of magnitude more efficient than the state of the art.","sentences":["Autonomous mobile agents often operate in hazardous environments, necessitating an awareness of safety.","These agents can have non-linear, stochastic dynamics that must be considered during planning to guarantee bounded risk.","Most state of the art methods require closed-form dynamics to verify plan correctness and safety however modern robotic systems often have dynamics that are learned from data.","Thus, there is a need to perform efficient trajectory planning with guarantees on risk for agents without known dynamics models.","We propose a \"generate-and-test\" approach to risk-bounded planning in which a planner generates a candidate trajectory using an approximate linear dynamics model and a validator assesses the risk of the trajectory, computing additional safety constraints for the planner if the candidate does not satisfy the desired risk bound.","To acquire the approximate model, we use a variational autoencoder to learn a latent linear dynamics model and encode the planning problem into the latent space to generate the candidate trajectory.","The VAE also serves to sample trajectories around the candidate to use in the validator.","We demonstrate that our algorithm, LaPlaSS, is able to generate trajectory plans with bounded risk for a real-world agent with learned dynamics and is an order of magnitude more efficient than the state of the art."],"url":"http://arxiv.org/abs/2404.07063v1","category":"cs.RO"}
{"created":"2024-04-10 14:45:04","title":"Quantum Mechanics of Open Systems in Non-Inertial Motion","abstract":"The study of quantum mechanics in non-inertial reference frames, particularly in the context of open systems, introduces several intriguing phenomena and challenges. This paper presents a comprehensive framework for analyzing the quantum mechanics of open systems undergoing noninertial motion. Our methodology leverages the concept of dissipatons, statistical quasi-particles that capture collective dissipative effects from the environment. We demonstrate that our approach offers a natural understanding of the intricate dynamics among non-inertial effects, decoherence, dissipation, and system-bath entanglement. Specifically, we conduct demonstrations focusing on the Lamb shift phenomenon within a rotating ring cavity. Through theoretical exposition and practical applications, our framework elucidates the profound interplay between open quantum dynamics and non-inertial motion, paving the way for advancements in quantum information processing and sensing technologies.","sentences":["The study of quantum mechanics in non-inertial reference frames, particularly in the context of open systems, introduces several intriguing phenomena and challenges.","This paper presents a comprehensive framework for analyzing the quantum mechanics of open systems undergoing noninertial motion.","Our methodology leverages the concept of dissipatons, statistical quasi-particles that capture collective dissipative effects from the environment.","We demonstrate that our approach offers a natural understanding of the intricate dynamics among non-inertial effects, decoherence, dissipation, and system-bath entanglement.","Specifically, we conduct demonstrations focusing on the Lamb shift phenomenon within a rotating ring cavity.","Through theoretical exposition and practical applications, our framework elucidates the profound interplay between open quantum dynamics and non-inertial motion, paving the way for advancements in quantum information processing and sensing technologies."],"url":"http://arxiv.org/abs/2404.07054v1","category":"quant-ph"}
{"created":"2024-04-10 14:44:48","title":"Meta4XNLI: A Crosslingual Parallel Corpus for Metaphor Detection and Interpretation","abstract":"Metaphors, although occasionally unperceived, are ubiquitous in our everyday language. Thus, it is crucial for Language Models to be able to grasp the underlying meaning of this kind of figurative language. In this work, we present Meta4XNLI, a novel parallel dataset for the tasks of metaphor detection and interpretation that contains metaphor annotations in both Spanish and English. We investigate language models' metaphor identification and understanding abilities through a series of monolingual and cross-lingual experiments by leveraging our proposed corpus. In order to comprehend how these non-literal expressions affect models' performance, we look over the results and perform an error analysis. Additionally, parallel data offers many potential opportunities to investigate metaphor transferability between these languages and the impact of translation on the development of multilingual annotated resources.","sentences":["Metaphors, although occasionally unperceived, are ubiquitous in our everyday language.","Thus, it is crucial for Language Models to be able to grasp the underlying meaning of this kind of figurative language.","In this work, we present Meta4XNLI, a novel parallel dataset for the tasks of metaphor detection and interpretation that contains metaphor annotations in both Spanish and English.","We investigate language models' metaphor identification and understanding abilities through a series of monolingual and cross-lingual experiments by leveraging our proposed corpus.","In order to comprehend how these non-literal expressions affect models' performance, we look over the results and perform an error analysis.","Additionally, parallel data offers many potential opportunities to investigate metaphor transferability between these languages and the impact of translation on the development of multilingual annotated resources."],"url":"http://arxiv.org/abs/2404.07053v1","category":"cs.CL"}
{"created":"2024-04-10 14:36:35","title":"Comparison of decision trees with Local Interpretable Model-Agnostic Explanations (LIME) technique and multi-linear regression for explaining support vector regression model in terms of root mean square error (RMSE) values","abstract":"In this work the decision trees are used for explanation of support vector regression model. The decision trees act as a global technique as well as a local technique. They are compared against the popular technique of LIME which is a local explanatory technique and with multi linear regression. It is observed that decision trees give a lower RMSE value when fitted to support vector regression as compared to LIME in 87% of the runs over 5 datasets. The comparison of results is statistically significant. Multi linear regression also gives a lower RMSE value when fitted to support vector regression model as compared to LIME in 73% of the runs over 5 datasets but the comparison of results is not statistically significant. Also, when used as a local explanatory technique, decision trees give better performance than LIME and the comparison of results is statistically significant.","sentences":["In this work the decision trees are used for explanation of support vector regression model.","The decision trees act as a global technique as well as a local technique.","They are compared against the popular technique of LIME which is a local explanatory technique and with multi linear regression.","It is observed that decision trees give a lower RMSE value when fitted to support vector regression as compared to LIME in 87% of the runs over 5 datasets.","The comparison of results is statistically significant.","Multi linear regression also gives a lower RMSE value when fitted to support vector regression model as compared to LIME in 73% of the runs over 5 datasets but the comparison of results is not statistically significant.","Also, when used as a local explanatory technique, decision trees give better performance than LIME and the comparison of results is statistically significant."],"url":"http://arxiv.org/abs/2404.07046v1","category":"cs.LG"}
{"created":"2024-04-10 14:34:19","title":"On the Performance of IRS-Assisted SSK and RPM over Rician Fading Channels","abstract":"This paper presents the index modulation, that is, the space-shift keying (SSK) and reflection phase modulation (RPM) schemes for intelligent reflecting surface (IRS)-assisted wireless network. IRS simultaneously reflects the incoming information signal from the base station and explicitly encodes the local information bits in the reflection phase shift of IRS elements. The phase shift of the IRS elements is employed according to local data from the RPM constellation. A joint detection using a maximum-likelihood (ML) decoder is performed for the SSK and RPM symbols over a realistic fading scenario modeled as the Rician fading channel. The pairwise error probability over Rician fading channels is derived and utilized to determine the average bit error rate. In addition, the ergodic capacity of the presented system is derived. The derived analytical results are verified and are in exact agreement with Monte-Carlo simulations.","sentences":["This paper presents the index modulation, that is, the space-shift keying (SSK) and reflection phase modulation (RPM) schemes for intelligent reflecting surface (IRS)-assisted wireless network.","IRS simultaneously reflects the incoming information signal from the base station and explicitly encodes the local information bits in the reflection phase shift of IRS elements.","The phase shift of the IRS elements is employed according to local data from the RPM constellation.","A joint detection using a maximum-likelihood (ML) decoder is performed for the SSK and RPM symbols over a realistic fading scenario modeled as the Rician fading channel.","The pairwise error probability over Rician fading channels is derived and utilized to determine the average bit error rate.","In addition, the ergodic capacity of the presented system is derived.","The derived analytical results are verified and are in exact agreement with Monte-Carlo simulations."],"url":"http://arxiv.org/abs/2404.07044v1","category":"cs.IT"}
{"created":"2024-04-10 14:21:22","title":"Propensity of water self-ions at air(oil)-water interfaces revealed by deep potential molecular dynamics with enhanced sampling","abstract":"The preference of water self-ions (hydronium and hydroxide) near air/oil-water interfaces is one of the hottest topics in water research due to its importance for understanding properties, phenomena, and reactions of interfaces. In this work, we performed enhanced-sampling molecular dynamics based on state-of-the-art neural network potentials with M06-2X accuracy to investigate the propensity of hydronium and hydroxide ions at air/oil-water interfaces, which can simultaneously describe well the water autoionization process forming these ions, recombination of ions, and ionic distribution along the normal distance to the interface by employing a set of appropriate Voronoi collective variables. The results support a stable ionic double-layer distribution near the interface for both air-water and oil-water interface systems. Hydronium tends to reside in the topmost layer of the interface, while hydroxide with a slightly stronger interfacial stabilization free energy is enriched in the deeper interfacial layer. This double-layer distribution may help to understand the longstanding controversy about the interfacial acid-base nature.","sentences":["The preference of water self-ions (hydronium and hydroxide) near air/oil-water interfaces is one of the hottest topics in water research due to its importance for understanding properties, phenomena, and reactions of interfaces.","In this work, we performed enhanced-sampling molecular dynamics based on state-of-the-art neural network potentials with M06-2X accuracy to investigate the propensity of hydronium and hydroxide ions at air/oil-water interfaces, which can simultaneously describe well the water autoionization process forming these ions, recombination of ions, and ionic distribution along the normal distance to the interface by employing a set of appropriate Voronoi collective variables.","The results support a stable ionic double-layer distribution near the interface for both air-water and oil-water interface systems.","Hydronium tends to reside in the topmost layer of the interface, while hydroxide with a slightly stronger interfacial stabilization free energy is enriched in the deeper interfacial layer.","This double-layer distribution may help to understand the longstanding controversy about the interfacial acid-base nature."],"url":"http://arxiv.org/abs/2404.07027v1","category":"physics.chem-ph"}
{"created":"2024-04-10 14:05:44","title":"Improving Language Model Reasoning with Self-motivated Learning","abstract":"Large-scale high-quality training data is important for improving the performance of models. After trained with data that has rationales (reasoning steps), models gain reasoning capability. However, the dataset with high-quality rationales is relatively scarce due to the high annotation cost. To address this issue, we propose \\textit{Self-motivated Learning} framework. The framework motivates the model itself to automatically generate rationales on existing datasets. Based on the inherent rank from correctness across multiple rationales, the model learns to generate better rationales, leading to higher reasoning capability. Specifically, we train a reward model with the rank to evaluate the quality of rationales, and improve the performance of reasoning through reinforcement learning. Experiment results of Llama2 7B on multiple reasoning datasets show that our method significantly improves the reasoning ability of models, even outperforming text-davinci-002 in some datasets.","sentences":["Large-scale high-quality training data is important for improving the performance of models.","After trained with data that has rationales (reasoning steps), models gain reasoning capability.","However, the dataset with high-quality rationales is relatively scarce due to the high annotation cost.","To address this issue, we propose \\textit{Self-motivated Learning} framework.","The framework motivates the model itself to automatically generate rationales on existing datasets.","Based on the inherent rank from correctness across multiple rationales, the model learns to generate better rationales, leading to higher reasoning capability.","Specifically, we train a reward model with the rank to evaluate the quality of rationales, and improve the performance of reasoning through reinforcement learning.","Experiment results of Llama2 7B on multiple reasoning datasets show that our method significantly improves the reasoning ability of models, even outperforming text-davinci-002 in some datasets."],"url":"http://arxiv.org/abs/2404.07017v1","category":"cs.CL"}
{"created":"2024-04-10 13:47:22","title":"Knowledge graphs for empirical concept retrieval","abstract":"Concept-based explainable AI is promising as a tool to improve the understanding of complex models at the premises of a given user, viz.\\ as a tool for personalized explainability. An important class of concept-based explainability methods is constructed with empirically defined concepts, indirectly defined through a set of positive and negative examples, as in the TCAV approach (Kim et al., 2018). While it is appealing to the user to avoid formal definitions of concepts and their operationalization, it can be challenging to establish relevant concept datasets. Here, we address this challenge using general knowledge graphs (such as, e.g., Wikidata or WordNet) for comprehensive concept definition and present a workflow for user-driven data collection in both text and image domains. The concepts derived from knowledge graphs are defined interactively, providing an opportunity for personalization and ensuring that the concepts reflect the user's intentions. We test the retrieved concept datasets on two concept-based explainability methods, namely concept activation vectors (CAVs) and concept activation regions (CARs) (Crabbe and van der Schaar, 2022). We show that CAVs and CARs based on these empirical concept datasets provide robust and accurate explanations. Importantly, we also find good alignment between the models' representations of concepts and the structure of knowledge graphs, i.e., human representations. This supports our conclusion that knowledge graph-based concepts are relevant for XAI.","sentences":["Concept-based explainable AI is promising as a tool to improve the understanding of complex models at the premises of a given user, viz.\\ as a tool for personalized explainability.","An important class of concept-based explainability methods is constructed with empirically defined concepts, indirectly defined through a set of positive and negative examples, as in the TCAV approach (Kim et al., 2018).","While it is appealing to the user to avoid formal definitions of concepts and their operationalization, it can be challenging to establish relevant concept datasets.","Here, we address this challenge using general knowledge graphs (such as, e.g., Wikidata or WordNet) for comprehensive concept definition and present a workflow for user-driven data collection in both text and image domains.","The concepts derived from knowledge graphs are defined interactively, providing an opportunity for personalization and ensuring that the concepts reflect the user's intentions.","We test the retrieved concept datasets on two concept-based explainability methods, namely concept activation vectors (CAVs) and concept activation regions (CARs) (Crabbe and van der Schaar, 2022).","We show that CAVs and CARs based on these empirical concept datasets provide robust and accurate explanations.","Importantly, we also find good alignment between the models' representations of concepts and the structure of knowledge graphs, i.e., human representations.","This supports our conclusion that knowledge graph-based concepts are relevant for XAI."],"url":"http://arxiv.org/abs/2404.07008v1","category":"cs.LG"}
{"created":"2024-04-10 13:40:29","title":"WordDecipher: Enhancing Digital Workspace Communication with Explainable AI for Non-native English Speakers","abstract":"Non-native English speakers (NNES) face challenges in digital workspace communication (e.g., emails, Slack messages), often inadvertently translating expressions from their native languages, which can lead to awkward or incorrect usage. Current AI-assisted writing tools are equipped with fluency enhancement and rewriting suggestions; however, NNES may struggle to grasp the subtleties among various expressions, making it challenging to choose the one that accurately reflects their intent. Such challenges are exacerbated in high-stake text-based communications, where the absence of non-verbal cues heightens the risk of misinterpretation. By leveraging the latest advancements in large language models (LLM) and word embeddings, we propose WordDecipher, an explainable AI-assisted writing tool to enhance digital workspace communication for NNES. WordDecipher not only identifies the perceived social intentions detected in users' writing, but also generates rewriting suggestions aligned with users' intended messages, either numerically or by inferring from users' writing in their native language. Then, WordDecipher provides an overview of nuances to help NNES make selections. Through a usage scenario, we demonstrate how WordDecipher can significantly enhance an NNES's ability to communicate her request, showcasing its potential to transform workspace communication for NNES.","sentences":["Non-native English speakers (NNES) face challenges in digital workspace communication (e.g., emails, Slack messages), often inadvertently translating expressions from their native languages, which can lead to awkward or incorrect usage.","Current AI-assisted writing tools are equipped with fluency enhancement and rewriting suggestions; however, NNES may struggle to grasp the subtleties among various expressions, making it challenging to choose the one that accurately reflects their intent.","Such challenges are exacerbated in high-stake text-based communications, where the absence of non-verbal cues heightens the risk of misinterpretation.","By leveraging the latest advancements in large language models (LLM) and word embeddings, we propose WordDecipher, an explainable AI-assisted writing tool to enhance digital workspace communication for NNES.","WordDecipher not only identifies the perceived social intentions detected in users' writing, but also generates rewriting suggestions aligned with users' intended messages, either numerically or by inferring from users' writing in their native language.","Then, WordDecipher provides an overview of nuances to help NNES make selections.","Through a usage scenario, we demonstrate how WordDecipher can significantly enhance an NNES's ability to communicate her request, showcasing its potential to transform workspace communication for NNES."],"url":"http://arxiv.org/abs/2404.07005v1","category":"cs.HC"}
{"created":"2024-04-10 13:40:29","title":"Linked open data per la valorizzazione di collezioni culturali: il dataset mythLOD","abstract":"The formal representation of cultural metadata has always been a challenge, considering both the heterogeneity of cultural objects and the need to document the interpretive act exercised by experts. This article provides an overview of the revalorization of the digital collection Mythologiae in Linked Open Data format. The research aims to explore the data of a collection of artworks (Mythologiae) by promoting the potential of the Semantic Web, focusing particularly on the formal representation of the association of cultural objects with literary sources, as realized by experts, also documenting their interpretations. The workflow consisted of defining the data model, cleaning and disambiguating the data, converting it (from tabular structure to graph), and conducting testing activities (particularly expert domain review of the dataset through competency questions and data visualizations). The result is the mythLOD platform, which presents the dataset and detailed research documentation. Additionally, the platform hosts two data visualization spaces (the online catalogue and a data storytelling experiment on the case study of the Aeneid) that enrich the project documentation as user-friendly test units for the dataset and constitute an additional project documentation tool and exploration of the collection.","sentences":["The formal representation of cultural metadata has always been a challenge, considering both the heterogeneity of cultural objects and the need to document the interpretive act exercised by experts.","This article provides an overview of the revalorization of the digital collection Mythologiae in Linked Open Data format.","The research aims to explore the data of a collection of artworks (Mythologiae) by promoting the potential of the Semantic Web, focusing particularly on the formal representation of the association of cultural objects with literary sources, as realized by experts, also documenting their interpretations.","The workflow consisted of defining the data model, cleaning and disambiguating the data, converting it (from tabular structure to graph), and conducting testing activities (particularly expert domain review of the dataset through competency questions and data visualizations).","The result is the mythLOD platform, which presents the dataset and detailed research documentation.","Additionally, the platform hosts two data visualization spaces (the online catalogue and a data storytelling experiment on the case study of the Aeneid) that enrich the project documentation as user-friendly test units for the dataset and constitute an additional project documentation tool and exploration of the collection."],"url":"http://arxiv.org/abs/2404.07006v1","category":"cs.DL"}
{"created":"2024-04-10 13:31:07","title":"Event Grounded Criminal Court View Generation withCooperative (Large) Language Models","abstract":"With the development of legal intelligence, Criminal Court View Generation has attracted much attention as a crucial task of legal intelligence, which aims to generate concise and coherent texts that summarize case facts and provide explanations for verdicts. Existing researches explore the key information in case facts to yield the court views. Most of them employ a coarse-grained approach that partitions the facts into broad segments (e.g., verdict-related sentences) to make predictions. However, this approach fails to capture the complex details present in the case facts, such as various criminal elements and legal events. To this end, in this paper, we propose an Event Grounded Generation (EGG) method for criminal court view generation with cooperative (Large) Language Models, which introduces the fine-grained event information into the generation. Specifically, we first design a LLMs-based extraction method that can extract events in case facts without massive annotated events. Then, we incorporate the extracted events into court view generation by merging case facts and events. Besides, considering the computational burden posed by the use of LLMs in the extraction phase of EGG, we propose a LLMs-free EGG method that can eliminate the requirement for event extraction using LLMs in the inference phase. Extensive experimental results on a real-world dataset clearly validate the effectiveness of our proposed method.","sentences":["With the development of legal intelligence, Criminal Court View Generation has attracted much attention as a crucial task of legal intelligence, which aims to generate concise and coherent texts that summarize case facts and provide explanations for verdicts.","Existing researches explore the key information in case facts to yield the court views.","Most of them employ a coarse-grained approach that partitions the facts into broad segments (e.g., verdict-related sentences) to make predictions.","However, this approach fails to capture the complex details present in the case facts, such as various criminal elements and legal events.","To this end, in this paper, we propose an Event Grounded Generation (EGG) method for criminal court view generation with cooperative (Large) Language Models, which introduces the fine-grained event information into the generation.","Specifically, we first design a LLMs-based extraction method that can extract events in case facts without massive annotated events.","Then, we incorporate the extracted events into court view generation by merging case facts and events.","Besides, considering the computational burden posed by the use of LLMs in the extraction phase of EGG, we propose a LLMs-free EGG method that can eliminate the requirement for event extraction using LLMs in the inference phase.","Extensive experimental results on a real-world dataset clearly validate the effectiveness of our proposed method."],"url":"http://arxiv.org/abs/2404.07001v1","category":"cs.CL"}
{"created":"2024-04-10 13:24:27","title":"Agent-driven Generative Semantic Communication for Remote Surveillance","abstract":"In the era of 6G, featuring compelling visions of intelligent transportation system, digital twins, remote surveillance is poised to become a ubiquitous practice. The substantial data volume and frequent updates present challenges in wireless networks. To address this, we propose a novel agent-driven generative semantic communication (A-GSC) framework based on reinforcement learning. In contrast to the existing research on semantic communication (SemCom), which mainly focuses on semantic compression or semantic sampling, we seamlessly cascade both together by jointly considering the intrinsic attributes of source information and the contextual information regarding the task. Notably, the introduction of the generative artificial intelligence (GAI) enables the independent design of semantic encoders and decoders. In this work, we develop an agent-assisted semantic encoder leveraging the knowledge based soft actor-critic algorithm, which can track the semantic changes, channel condition, and sampling intervals, so as to perform adaptive semantic sampling. Accordingly, we design a semantic decoder with both predictive and generative capabilities, which consists of two tailored modules. Moreover, the effectiveness of the designed models has been verified based on the dataset generated from CDNet2014, and the performance gain of the overall A-GSC framework in both energy saving and reconstruction accuracy have been demonstrated.","sentences":["In the era of 6G, featuring compelling visions of intelligent transportation system, digital twins, remote surveillance is poised to become a ubiquitous practice.","The substantial data volume and frequent updates present challenges in wireless networks.","To address this, we propose a novel agent-driven generative semantic communication (A-GSC) framework based on reinforcement learning.","In contrast to the existing research on semantic communication (SemCom), which mainly focuses on semantic compression or semantic sampling, we seamlessly cascade both together by jointly considering the intrinsic attributes of source information and the contextual information regarding the task.","Notably, the introduction of the generative artificial intelligence (GAI) enables the independent design of semantic encoders and decoders.","In this work, we develop an agent-assisted semantic encoder leveraging the knowledge based soft actor-critic algorithm, which can track the semantic changes, channel condition, and sampling intervals, so as to perform adaptive semantic sampling.","Accordingly, we design a semantic decoder with both predictive and generative capabilities, which consists of two tailored modules.","Moreover, the effectiveness of the designed models has been verified based on the dataset generated from CDNet2014, and the performance gain of the overall A-GSC framework in both energy saving and reconstruction accuracy have been demonstrated."],"url":"http://arxiv.org/abs/2404.06997v1","category":"cs.NI"}
{"created":"2024-04-10 13:19:56","title":"XNLIeu: a dataset for cross-lingual NLI in Basque","abstract":"XNLI is a popular Natural Language Inference (NLI) benchmark widely used to evaluate cross-lingual Natural Language Understanding (NLU) capabilities across languages. In this paper, we expand XNLI to include Basque, a low-resource language that can greatly benefit from transfer-learning approaches. The new dataset, dubbed XNLIeu, has been developed by first machine-translating the English XNLI corpus into Basque, followed by a manual post-edition step. We have conducted a series of experiments using mono- and multilingual LLMs to assess a) the effect of professional post-edition on the MT system; b) the best cross-lingual strategy for NLI in Basque; and c) whether the choice of the best cross-lingual strategy is influenced by the fact that the dataset is built by translation. The results show that post-edition is necessary and that the translate-train cross-lingual strategy obtains better results overall, although the gain is lower when tested in a dataset that has been built natively from scratch. Our code and datasets are publicly available under open licenses.","sentences":["XNLI is a popular Natural Language Inference (NLI) benchmark widely used to evaluate cross-lingual Natural Language Understanding (NLU) capabilities across languages.","In this paper, we expand XNLI to include Basque, a low-resource language that can greatly benefit from transfer-learning approaches.","The new dataset, dubbed XNLIeu, has been developed by first machine-translating the English XNLI corpus into Basque, followed by a manual post-edition step.","We have conducted a series of experiments using mono- and multilingual LLMs to assess a) the effect of professional post-edition on the MT system; b) the best cross-lingual strategy for NLI in Basque; and c) whether the choice of the best cross-lingual strategy is influenced by the fact that the dataset is built by translation.","The results show that post-edition is necessary and that the translate-train cross-lingual strategy obtains better results overall, although the gain is lower when tested in a dataset that has been built natively from scratch.","Our code and datasets are publicly available under open licenses."],"url":"http://arxiv.org/abs/2404.06996v1","category":"cs.CL"}
{"created":"2024-04-10 12:53:27","title":"Search for a light muon-philic $Z'$ with the NA64-$e$ experiment at CERN","abstract":"The inclusion of an additional U(1) gauge $L_{\\mu} - L_{\\tau}$ symmetry would release the tension between the measured and the predicted value of the anomalous muon magnetic moment: this paradigm assumes the existence of a new, light $Z'$ vector boson, with dominant coupling to ${\\mu}$ and ${\\tau}$ and interacting with electrons via a loop mechanism. The $L_{\\mu} - L_{\\tau}$ model can also explain the Dark Matter relic abundance, by assuming that the $Z'$ boson acts as a \"portal\" to a new Dark Sector of particles in Nature, not charged under known interactions. In this work we present the results of the $Z'$ search performed by the NA64-$e$ experiment at CERN SPS, that collected $ \\sim 9 \\times 10^{11}$ 100-GeV electrons impinging on an active thick target. Despite the suppressed $Z'$ production yield with an electron beam, the limits sets by NA64-$e$ are competitive with other experimental searches, and partially exclude the $g-2$ preferred model parameter values for $Z'$ masses lighter than 2 MeV. This result proves the complementarity of this search with NA64-${\\mu}$, the parallel effort of the NA64 collaboration with a muon beam.","sentences":["The inclusion of an additional U(1) gauge $L_{\\mu} - L_{\\tau}$ symmetry would release the tension between the measured and the predicted value of the anomalous muon magnetic moment: this paradigm assumes the existence of a new, light $Z'$ vector boson, with dominant coupling to ${\\mu}$ and ${\\tau}$ and interacting with electrons via a loop mechanism.","The $L_{\\mu} - L_{\\tau}$ model can also explain the Dark Matter relic abundance, by assuming that the $Z'$ boson acts as a \"portal\" to a new Dark Sector of particles in Nature, not charged under known interactions.","In this work we present the results of the $Z'$ search performed by the NA64-$e$ experiment at CERN SPS, that collected $ \\sim 9 \\times 10^{11}$ 100-GeV electrons impinging on an active thick target.","Despite the suppressed $Z'$ production yield with an electron beam, the limits sets by NA64-$e$ are competitive with other experimental searches, and partially exclude the $g-2$ preferred model parameter values for $Z'$ masses lighter than 2 MeV.","This result proves the complementarity of this search with NA64-${\\mu}$, the parallel effort of the NA64 collaboration with a muon beam."],"url":"http://arxiv.org/abs/2404.06982v1","category":"hep-ex"}
{"created":"2024-04-10 12:45:27","title":"Accurate Tennis Court Line Detection on Amateur Recorded Matches","abstract":"Typically, tennis court line detection is done by running Hough-Line-Detection to find straight lines in the image, and then computing a transformation matrix from the detected lines to create the final court structure. We propose numerous improvements and enhancements to this algorithm, including using pretrained State-of-the-Art shadow-removal and object-detection ML models to make our line-detection more robust. Compared to the original algorithm, our method can accurately detect lines on amateur, dirty courts. When combined with a robust ball-tracking system, our method will enable accurate, automatic refereeing for amateur and professional tennis matches alike.","sentences":["Typically, tennis court line detection is done by running Hough-Line-Detection to find straight lines in the image, and then computing a transformation matrix from the detected lines to create the final court structure.","We propose numerous improvements and enhancements to this algorithm, including using pretrained State-of-the-Art shadow-removal and object-detection ML models to make our line-detection more robust.","Compared to the original algorithm, our method can accurately detect lines on amateur, dirty courts.","When combined with a robust ball-tracking system, our method will enable accurate, automatic refereeing for amateur and professional tennis matches alike."],"url":"http://arxiv.org/abs/2404.06977v1","category":"cs.CV"}
{"created":"2024-04-10 12:42:28","title":"Quati: A Brazilian Portuguese Information Retrieval Dataset from Native Speakers","abstract":"Despite Portuguese being one of the most spoken languages in the world, there is a lack of high-quality information retrieval datasets in that language. We present Quati, a dataset specifically designed for the Brazilian Portuguese language. It comprises a collection of queries formulated by native speakers and a curated set of documents sourced from a selection of high-quality Brazilian Portuguese websites. These websites are frequented more likely by real users compared to those randomly scraped, ensuring a more representative and relevant corpus. To label the query-document pairs, we use a state-of-the-art LLM, which shows inter-annotator agreement levels comparable to human performance in our assessments. We provide a detailed description of our annotation methodology to enable others to create similar datasets for other languages, providing a cost-effective way of creating high-quality IR datasets with an arbitrary number of labeled documents per query. Finally, we evaluate a diverse range of open-source and commercial retrievers to serve as baseline systems. Quati is publicly available at https://huggingface.co/datasets/unicamp-dl/quati and all scripts at https://github.com/unicamp-dl/quati .","sentences":["Despite Portuguese being one of the most spoken languages in the world, there is a lack of high-quality information retrieval datasets in that language.","We present Quati, a dataset specifically designed for the Brazilian Portuguese language.","It comprises a collection of queries formulated by native speakers and a curated set of documents sourced from a selection of high-quality Brazilian Portuguese websites.","These websites are frequented more likely by real users compared to those randomly scraped, ensuring a more representative and relevant corpus.","To label the query-document pairs, we use a state-of-the-art LLM, which shows inter-annotator agreement levels comparable to human performance in our assessments.","We provide a detailed description of our annotation methodology to enable others to create similar datasets for other languages, providing a cost-effective way of creating high-quality IR datasets with an arbitrary number of labeled documents per query.","Finally, we evaluate a diverse range of open-source and commercial retrievers to serve as baseline systems.","Quati is publicly available at https://huggingface.co/datasets/unicamp-dl/quati and all scripts at https://github.com/unicamp-dl/quati ."],"url":"http://arxiv.org/abs/2404.06976v1","category":"cs.IR"}
{"created":"2024-04-10 12:37:53","title":"Embedding Economic Incentives in Social Networks Shape the Diffusion of Digital Technological Innovation","abstract":"The digital innovation accompanied by explicit economic incentives have fundamentally changed the process of innovation diffusion. As a representative of digital innovation, NFTs provide a decentralized and secure way to authenticate and trade digital assets, offering the potential for new revenue streams in the digital space. However, current researches about NFTs mainly focus on their transaction networks and community culture, leaving the interplay among diffusion dynamics, economic dynamics, and social constraints on Twitter. By collecting and analyzing NFTs-related tweet dataset, the motivations of retweeters, the information mechanisms behind emojis, and the networked-based diffusion dynamics is systematically investigated. Results indicate that Retweeting is fueled by Freemint and trading information, with the higher economic incentives as a major motivation and some potential organizational tendencies. The diffusion of NFT is primarily driven by a 'Ringed-layered' information mechanism involving individual promoters and speculators. Both the frequency and presentation of content contribute positively to the growth of the retweet network. This study contributes to the innovation diffusion theory with economic incentives embedded.","sentences":["The digital innovation accompanied by explicit economic incentives have fundamentally changed the process of innovation diffusion.","As a representative of digital innovation, NFTs provide a decentralized and secure way to authenticate and trade digital assets, offering the potential for new revenue streams in the digital space.","However, current researches about NFTs mainly focus on their transaction networks and community culture, leaving the interplay among diffusion dynamics, economic dynamics, and social constraints on Twitter.","By collecting and analyzing NFTs-related tweet dataset, the motivations of retweeters, the information mechanisms behind emojis, and the networked-based diffusion dynamics is systematically investigated.","Results indicate that Retweeting is fueled by Freemint and trading information, with the higher economic incentives as a major motivation and some potential organizational tendencies.","The diffusion of NFT is primarily driven by a 'Ringed-layered' information mechanism involving individual promoters and speculators.","Both the frequency and presentation of content contribute positively to the growth of the retweet network.","This study contributes to the innovation diffusion theory with economic incentives embedded."],"url":"http://arxiv.org/abs/2404.06973v1","category":"cs.SI"}
{"created":"2024-04-10 12:32:18","title":"Toward industrial use of continual learning : new metrics proposal for class incremental learning","abstract":"In this paper, we investigate continual learning performance metrics used in class incremental learning strategies for continual learning (CL) using some high performing methods. We investigate especially mean task accuracy. First, we show that it lacks of expressiveness through some simple experiments to capture performance. We show that monitoring average tasks performance is over optimistic and can lead to misleading conclusions for future real life industrial uses. Then, we propose first a simple metric, Minimal Incremental Class Accuracy (MICA) which gives a fair and more useful evaluation of different continual learning methods. Moreover, in order to provide a simple way to easily compare different methods performance in continual learning, we derive another single scalar metric that take into account the learning performance variation as well as our newly introduced metric.","sentences":["In this paper, we investigate continual learning performance metrics used in class incremental learning strategies for continual learning (CL) using some high performing methods.","We investigate especially mean task accuracy.","First, we show that it lacks of expressiveness through some simple experiments to capture performance.","We show that monitoring average tasks performance is over optimistic and can lead to misleading conclusions for future real life industrial uses.","Then, we propose first a simple metric, Minimal Incremental Class Accuracy (MICA) which gives a fair and more useful evaluation of different continual learning methods.","Moreover, in order to provide a simple way to easily compare different methods performance in continual learning, we derive another single scalar metric that take into account the learning performance variation as well as our newly introduced metric."],"url":"http://arxiv.org/abs/2404.06972v1","category":"cs.LG"}
{"created":"2024-04-10 12:31:43","title":"TrajPRed: Trajectory Prediction with Region-based Relation Learning","abstract":"Forecasting human trajectories in traffic scenes is critical for safety within mixed or fully autonomous systems. Human future trajectories are driven by two major stimuli, social interactions, and stochastic goals. Thus, reliable forecasting needs to capture these two stimuli. Edge-based relation modeling represents social interactions using pairwise correlations from precise individual states. Nevertheless, edge-based relations can be vulnerable under perturbations. To alleviate these issues, we propose a region-based relation learning paradigm that models social interactions via region-wise dynamics of joint states, i.e., the changes in the density of crowds. In particular, region-wise agent joint information is encoded within convolutional feature grids. Social relations are modeled by relating the temporal changes of local joint information from a global perspective. We show that region-based relations are less susceptible to perturbations. In order to account for the stochastic individual goals, we exploit a conditional variational autoencoder to realize multi-goal estimation and diverse future prediction. Specifically, we perform variational inference via the latent distribution, which is conditioned on the correlation between input states and associated target goals. Sampling from the latent distribution enables the framework to reliably capture the stochastic behavior in test data. We integrate multi-goal estimation and region-based relation learning to model the two stimuli, social interactions, and stochastic goals, in a prediction framework. We evaluate our framework on the ETH-UCY dataset and Stanford Drone Dataset (SDD). We show that the diverse prediction better fits the ground truth when incorporating the relation module. Our framework outperforms the state-of-the-art models on SDD by $27.61\\%$/$18.20\\%$ of ADE/FDE metrics.","sentences":["Forecasting human trajectories in traffic scenes is critical for safety within mixed or fully autonomous systems.","Human future trajectories are driven by two major stimuli, social interactions, and stochastic goals.","Thus, reliable forecasting needs to capture these two stimuli.","Edge-based relation modeling represents social interactions using pairwise correlations from precise individual states.","Nevertheless, edge-based relations can be vulnerable under perturbations.","To alleviate these issues, we propose a region-based relation learning paradigm that models social interactions via region-wise dynamics of joint states, i.e., the changes in the density of crowds.","In particular, region-wise agent joint information is encoded within convolutional feature grids.","Social relations are modeled by relating the temporal changes of local joint information from a global perspective.","We show that region-based relations are less susceptible to perturbations.","In order to account for the stochastic individual goals, we exploit a conditional variational autoencoder to realize multi-goal estimation and diverse future prediction.","Specifically, we perform variational inference via the latent distribution, which is conditioned on the correlation between input states and associated target goals.","Sampling from the latent distribution enables the framework to reliably capture the stochastic behavior in test data.","We integrate multi-goal estimation and region-based relation learning to model the two stimuli, social interactions, and stochastic goals, in a prediction framework.","We evaluate our framework on the ETH-UCY dataset and Stanford Drone Dataset (SDD).","We show that the diverse prediction better fits the ground truth when incorporating the relation module.","Our framework outperforms the state-of-the-art models on SDD by $27.61\\%$/$18.20\\%$ of ADE/FDE metrics."],"url":"http://arxiv.org/abs/2404.06971v1","category":"cs.CV"}
{"created":"2024-04-10 12:25:49","title":"Multiple imputation for longitudinal data: A tutorial","abstract":"Longitudinal studies are frequently used in medical research and involve collecting repeated measures on individuals over time. Observations from the same individual are invariably correlated and thus an analytic approach that accounts for this clustering by individual is required. While almost all research suffers from missing data, this can be particularly problematic in longitudinal studies as participation often becomes harder to maintain over time. Multiple imputation (MI) is widely used to handle missing data in such studies. When using MI, it is important that the imputation model is compatible with the proposed analysis model. In a longitudinal analysis, this implies that the clustering considered in the analysis model should be reflected in the imputation process. Several MI approaches have been proposed to impute incomplete longitudinal data, such as treating repeated measurements of the same variable as distinct variables or using generalized linear mixed imputation models. However, the uptake of these methods has been limited, as they require additional data manipulation and use of advanced imputation procedures. In this tutorial, we review the available MI approaches that can be used for handling incomplete longitudinal data, including where individuals are clustered within higher-level clusters. We illustrate implementation with replicable R and Stata code using a case study from the Childhood to Adolescence Transition Study.","sentences":["Longitudinal studies are frequently used in medical research and involve collecting repeated measures on individuals over time.","Observations from the same individual are invariably correlated and thus an analytic approach that accounts for this clustering by individual is required.","While almost all research suffers from missing data, this can be particularly problematic in longitudinal studies as participation often becomes harder to maintain over time.","Multiple imputation (MI) is widely used to handle missing data in such studies.","When using MI, it is important that the imputation model is compatible with the proposed analysis model.","In a longitudinal analysis, this implies that the clustering considered in the analysis model should be reflected in the imputation process.","Several MI approaches have been proposed to impute incomplete longitudinal data, such as treating repeated measurements of the same variable as distinct variables or using generalized linear mixed imputation models.","However, the uptake of these methods has been limited, as they require additional data manipulation and use of advanced imputation procedures.","In this tutorial, we review the available MI approaches that can be used for handling incomplete longitudinal data, including where individuals are clustered within higher-level clusters.","We illustrate implementation with replicable R and Stata code using a case study from the Childhood to Adolescence Transition Study."],"url":"http://arxiv.org/abs/2404.06967v1","category":"stat.ME"}
{"created":"2024-04-10 12:22:32","title":"Charles Translator: A Machine Translation System between Ukrainian and Czech","abstract":"We present Charles Translator, a machine translation system between Ukrainian and Czech, developed as part of a society-wide effort to mitigate the impact of the Russian-Ukrainian war on individuals and society. The system was developed in the spring of 2022 with the help of many language data providers in order to quickly meet the demand for such a service, which was not available at the time in the required quality. The translator was later implemented as an online web interface and as an Android app with speech input, both featuring Cyrillic-Latin script transliteration. The system translates directly, compared to other available systems that use English as a pivot, and thus take advantage of the typological similarity of the two languages. It uses the block back-translation method, which allows for efficient use of monolingual training data. The paper describes the development process, including data collection and implementation, evaluation, mentions several use cases, and outlines possibilities for the further development of the system for educational purposes.","sentences":["We present Charles Translator, a machine translation system between Ukrainian and Czech, developed as part of a society-wide effort to mitigate the impact of the Russian-Ukrainian war on individuals and society.","The system was developed in the spring of 2022 with the help of many language data providers in order to quickly meet the demand for such a service, which was not available at the time in the required quality.","The translator was later implemented as an online web interface and as an Android app with speech input, both featuring Cyrillic-Latin script transliteration.","The system translates directly, compared to other available systems that use English as a pivot, and thus take advantage of the typological similarity of the two languages.","It uses the block back-translation method, which allows for efficient use of monolingual training data.","The paper describes the development process, including data collection and implementation, evaluation, mentions several use cases, and outlines possibilities for the further development of the system for educational purposes."],"url":"http://arxiv.org/abs/2404.06964v1","category":"cs.CL"}
{"created":"2024-04-10 12:22:03","title":"Advancing Real-time Pandemic Forecasting Using Large Language Models: A COVID-19 Case Study","abstract":"Forecasting the short-term spread of an ongoing disease outbreak is a formidable challenge due to the complexity of contributing factors, some of which can be characterized through interlinked, multi-modality variables such as epidemiological time series data, viral biology, population demographics, and the intersection of public policy and human behavior. Existing forecasting model frameworks struggle with the multifaceted nature of relevant data and robust results translation, which hinders their performances and the provision of actionable insights for public health decision-makers. Our work introduces PandemicLLM, a novel framework with multi-modal Large Language Models (LLMs) that reformulates real-time forecasting of disease spread as a text reasoning problem, with the ability to incorporate real-time, complex, non-numerical information that previously unattainable in traditional forecasting models. This approach, through a unique AI-human cooperative prompt design and time series representation learning, encodes multi-modal data for LLMs. The model is applied to the COVID-19 pandemic, and trained to utilize textual public health policies, genomic surveillance, spatial, and epidemiological time series data, and is subsequently tested across all 50 states of the U.S. Empirically, PandemicLLM is shown to be a high-performing pandemic forecasting framework that effectively captures the impact of emerging variants and can provide timely and accurate predictions. The proposed PandemicLLM opens avenues for incorporating various pandemic-related data in heterogeneous formats and exhibits performance benefits over existing models. This study illuminates the potential of adapting LLMs and representation learning to enhance pandemic forecasting, illustrating how AI innovations can strengthen pandemic responses and crisis management in the future.","sentences":["Forecasting the short-term spread of an ongoing disease outbreak is a formidable challenge due to the complexity of contributing factors, some of which can be characterized through interlinked, multi-modality variables such as epidemiological time series data, viral biology, population demographics, and the intersection of public policy and human behavior.","Existing forecasting model frameworks struggle with the multifaceted nature of relevant data and robust results translation, which hinders their performances and the provision of actionable insights for public health decision-makers.","Our work introduces PandemicLLM, a novel framework with multi-modal Large Language Models (LLMs) that reformulates real-time forecasting of disease spread as a text reasoning problem, with the ability to incorporate real-time, complex, non-numerical information that previously unattainable in traditional forecasting models.","This approach, through a unique AI-human cooperative prompt design and time series representation learning, encodes multi-modal data for LLMs.","The model is applied to the COVID-19 pandemic, and trained to utilize textual public health policies, genomic surveillance, spatial, and epidemiological time series data, and is subsequently tested across all 50 states of the U.S. Empirically, PandemicLLM is shown to be a high-performing pandemic forecasting framework that effectively captures the impact of emerging variants and can provide timely and accurate predictions.","The proposed PandemicLLM opens avenues for incorporating various pandemic-related data in heterogeneous formats and exhibits performance benefits over existing models.","This study illuminates the potential of adapting LLMs and representation learning to enhance pandemic forecasting, illustrating how AI innovations can strengthen pandemic responses and crisis management in the future."],"url":"http://arxiv.org/abs/2404.06962v1","category":"cs.LG"}
{"created":"2024-04-10 12:17:25","title":"Adversarial purification for no-reference image-quality metrics: applicability study and new methods","abstract":"Recently, the area of adversarial attacks on image quality metrics has begun to be explored, whereas the area of defences remains under-researched. In this study, we aim to cover that case and check the transferability of adversarial purification defences from image classifiers to IQA methods. In this paper, we apply several widespread attacks on IQA models and examine the success of the defences against them. The purification methodologies covered different preprocessing techniques, including geometrical transformations, compression, denoising, and modern neural network-based methods. Also, we address the challenge of assessing the efficacy of a defensive methodology by proposing ways to estimate output visual quality and the success of neutralizing attacks. Defences were tested against attack on three IQA metrics -- Linearity, MetaIQA and SPAQ. The code for attacks and defences is available at: (link is hidden for a blind review).","sentences":["Recently, the area of adversarial attacks on image quality metrics has begun to be explored, whereas the area of defences remains under-researched.","In this study, we aim to cover that case and check the transferability of adversarial purification defences from image classifiers to IQA methods.","In this paper, we apply several widespread attacks on IQA models and examine the success of the defences against them.","The purification methodologies covered different preprocessing techniques, including geometrical transformations, compression, denoising, and modern neural network-based methods.","Also, we address the challenge of assessing the efficacy of a defensive methodology by proposing ways to estimate output visual quality and the success of neutralizing attacks.","Defences were tested against attack on three IQA metrics -- Linearity, MetaIQA and SPAQ.","The code for attacks and defences is available at: (link is hidden for a blind review)."],"url":"http://arxiv.org/abs/2404.06957v1","category":"cs.CV"}
{"created":"2024-04-10 12:12:50","title":"Untangling Critical Interaction with AI in Students Written Assessment","abstract":"Artificial Intelligence (AI) has become a ubiquitous part of society, but a key challenge exists in ensuring that humans are equipped with the required critical thinking and AI literacy skills to interact with machines effectively by understanding their capabilities and limitations. These skills are particularly important for learners to develop in the age of generative AI where AI tools can demonstrate complex knowledge and ability previously thought to be uniquely human. To activate effective human-AI partnerships in writing, this paper provides a first step toward conceptualizing the notion of critical learner interaction with AI. Using both theoretical models and empirical data, our preliminary findings suggest a general lack of Deep interaction with AI during the writing process. We believe that the outcomes can lead to better task and tool design in the future for learners to develop deep, critical thinking when interacting with AI.","sentences":["Artificial Intelligence (AI) has become a ubiquitous part of society, but a key challenge exists in ensuring that humans are equipped with the required critical thinking and AI literacy skills to interact with machines effectively by understanding their capabilities and limitations.","These skills are particularly important for learners to develop in the age of generative AI where AI tools can demonstrate complex knowledge and ability previously thought to be uniquely human.","To activate effective human-AI partnerships in writing, this paper provides a first step toward conceptualizing the notion of critical learner interaction with AI.","Using both theoretical models and empirical data, our preliminary findings suggest a general lack of Deep interaction with AI during the writing process.","We believe that the outcomes can lead to better task and tool design in the future for learners to develop deep, critical thinking when interacting with AI."],"url":"http://arxiv.org/abs/2404.06955v1","category":"cs.HC"}
{"created":"2024-04-10 11:55:33","title":"A Survey on the Integration of Generative AI for Critical Thinking in Mobile Networks","abstract":"In the near future, mobile networks are expected to broaden their services and coverage to accommodate a larger user base and diverse user needs. Thus, they will increasingly rely on artificial intelligence (AI) to manage network operation and control costs, undertaking complex decision-making roles. This shift will necessitate the application of techniques that incorporate critical thinking abilities, including reasoning and planning. Symbolic AI techniques already facilitate critical thinking based on existing knowledge. Yet, their use in telecommunications is hindered by the high cost of mostly manual curation of this knowledge and high computational complexity of reasoning tasks. At the same time, there is a spurt of innovations in industries such as telecommunications due to Generative AI (GenAI) technologies, operating independently of human-curated knowledge. However, their capacity for critical thinking remains uncertain. This paper aims to address this gap by examining the current status of GenAI algorithms with critical thinking capabilities and investigating their potential applications in telecom networks. Specifically, the aim of this study is to offer an introduction to the potential utilization of GenAI for critical thinking techniques in mobile networks, while also establishing a foundation for future research.","sentences":["In the near future, mobile networks are expected to broaden their services and coverage to accommodate a larger user base and diverse user needs.","Thus, they will increasingly rely on artificial intelligence (AI) to manage network operation and control costs, undertaking complex decision-making roles.","This shift will necessitate the application of techniques that incorporate critical thinking abilities, including reasoning and planning.","Symbolic AI techniques already facilitate critical thinking based on existing knowledge.","Yet, their use in telecommunications is hindered by the high cost of mostly manual curation of this knowledge and high computational complexity of reasoning tasks.","At the same time, there is a spurt of innovations in industries such as telecommunications due to Generative AI (GenAI) technologies, operating independently of human-curated knowledge.","However, their capacity for critical thinking remains uncertain.","This paper aims to address this gap by examining the current status of GenAI algorithms with critical thinking capabilities and investigating their potential applications in telecom networks.","Specifically, the aim of this study is to offer an introduction to the potential utilization of GenAI for critical thinking techniques in mobile networks, while also establishing a foundation for future research."],"url":"http://arxiv.org/abs/2404.06946v1","category":"cs.AI"}
{"created":"2024-04-10 11:45:31","title":"Robotic Learning for Adaptive Informative Path Planning","abstract":"Adaptive informative path planning (AIPP) is important to many robotics applications, enabling mobile robots to efficiently collect useful data about initially unknown environments. In addition, learning-based methods are increasingly used in robotics to enhance adaptability, versatility, and robustness across diverse and complex tasks. Our survey explores research on applying robotic learning to AIPP, bridging the gap between these two research fields. We begin by providing a unified mathematical framework for general AIPP problems. Next, we establish two complementary taxonomies of current work from the perspectives of (i) learning algorithms and (ii) robotic applications. We explore synergies, recent trends, and highlight the benefits of learning-based methods in AIPP frameworks. Finally, we discuss key challenges and promising future directions to enable more generally applicable and robust robotic data-gathering systems through learning. We provide a comprehensive catalogue of papers reviewed in our survey, including publicly available repositories, to facilitate future studies in the field.","sentences":["Adaptive informative path planning (AIPP) is important to many robotics applications, enabling mobile robots to efficiently collect useful data about initially unknown environments.","In addition, learning-based methods are increasingly used in robotics to enhance adaptability, versatility, and robustness across diverse and complex tasks.","Our survey explores research on applying robotic learning to AIPP, bridging the gap between these two research fields.","We begin by providing a unified mathematical framework for general AIPP problems.","Next, we establish two complementary taxonomies of current work from the perspectives of (i) learning algorithms and (ii) robotic applications.","We explore synergies, recent trends, and highlight the benefits of learning-based methods in AIPP frameworks.","Finally, we discuss key challenges and promising future directions to enable more generally applicable and robust robotic data-gathering systems through learning.","We provide a comprehensive catalogue of papers reviewed in our survey, including publicly available repositories, to facilitate future studies in the field."],"url":"http://arxiv.org/abs/2404.06940v1","category":"cs.RO"}
{"created":"2024-04-10 11:43:26","title":"Fast System Technology Co-Optimization Framework for Emerging Technology Based on Graph Neural Networks","abstract":"This paper proposes a fast system technology co-optimization (STCO) framework that optimizes power, performance, and area (PPA) for next-generation IC design, addressing the challenges and opportunities presented by novel materials and device architectures. We focus on accelerating the technology level of STCO using AI techniques, by employing graph neural network (GNN)-based approaches for both TCAD simulation and cell library characterization, which are interconnected through a unified compact model, collectively achieving over a 100X speedup over traditional methods. These advancements enable comprehensive STCO iterations with runtime speedups ranging from 1.9X to 14.1X and supports both emerging and traditional technologies.","sentences":["This paper proposes a fast system technology co-optimization (STCO) framework that optimizes power, performance, and area (PPA) for next-generation IC design, addressing the challenges and opportunities presented by novel materials and device architectures.","We focus on accelerating the technology level of STCO using AI techniques, by employing graph neural network (GNN)-based approaches for both TCAD simulation and cell library characterization, which are interconnected through a unified compact model, collectively achieving over a 100X speedup over traditional methods.","These advancements enable comprehensive STCO iterations with runtime speedups ranging from 1.9X to 14.1X and supports both emerging and traditional technologies."],"url":"http://arxiv.org/abs/2404.06939v1","category":"cs.ET"}
{"created":"2024-04-10 11:17:33","title":"GoEX: Perspectives and Designs Towards a Runtime for Autonomous LLM Applications","abstract":"Large Language Models (LLMs) are evolving beyond their classical role of providing information within dialogue systems to actively engaging with tools and performing actions on real-world applications and services. Today, humans verify the correctness and appropriateness of the LLM-generated outputs (e.g., code, functions, or actions) before putting them into real-world execution. This poses significant challenges as code comprehension is well known to be notoriously difficult. In this paper, we study how humans can efficiently collaborate with, delegate to, and supervise autonomous LLMs in the future. We argue that in many cases, \"post-facto validation\" - verifying the correctness of a proposed action after seeing the output - is much easier than the aforementioned \"pre-facto validation\" setting. The core concept behind enabling a post-facto validation system is the integration of an intuitive undo feature, and establishing a damage confinement for the LLM-generated actions as effective strategies to mitigate the associated risks. Using this, a human can now either revert the effect of an LLM-generated output or be confident that the potential risk is bounded. We believe this is critical to unlock the potential for LLM agents to interact with applications and services with limited (post-facto) human involvement. We describe the design and implementation of our open-source runtime for executing LLM actions, Gorilla Execution Engine (GoEX), and present open research questions towards realizing the goal of LLMs and applications interacting with each other with minimal human supervision. We release GoEX at https://github.com/ShishirPatil/gorilla/.","sentences":["Large Language Models (LLMs) are evolving beyond their classical role of providing information within dialogue systems to actively engaging with tools and performing actions on real-world applications and services.","Today, humans verify the correctness and appropriateness of the LLM-generated outputs (e.g., code, functions, or actions) before putting them into real-world execution.","This poses significant challenges as code comprehension is well known to be notoriously difficult.","In this paper, we study how humans can efficiently collaborate with, delegate to, and supervise autonomous LLMs in the future.","We argue that in many cases, \"post-facto validation\" - verifying the correctness of a proposed action after seeing the output - is much easier than the aforementioned \"pre-facto validation\" setting.","The core concept behind enabling a post-facto validation system is the integration of an intuitive undo feature, and establishing a damage confinement for the LLM-generated actions as effective strategies to mitigate the associated risks.","Using this, a human can now either revert the effect of an LLM-generated output or be confident that the potential risk is bounded.","We believe this is critical to unlock the potential for LLM agents to interact with applications and services with limited (post-facto) human involvement.","We describe the design and implementation of our open-source runtime for executing LLM actions, Gorilla Execution Engine (GoEX), and present open research questions towards realizing the goal of LLMs and applications interacting with each other with minimal human supervision.","We release GoEX at https://github.com/ShishirPatil/gorilla/."],"url":"http://arxiv.org/abs/2404.06921v1","category":"cs.CL"}
{"created":"2024-04-10 11:03:17","title":"Superposition Prompting: Improving and Accelerating Retrieval-Augmented Generation","abstract":"Despite the successes of large language models (LLMs), they exhibit significant drawbacks, particularly when processing long contexts. Their inference cost scales quadratically with respect to sequence length, making it expensive for deployment in some real-world text processing applications, such as retrieval-augmented generation (RAG). Additionally, LLMs also exhibit the \"distraction phenomenon,\" where irrelevant context in the prompt degrades output quality. To address these drawbacks, we propose a novel RAG prompting methodology, superposition prompting, which can be directly applied to pre-trained transformer-based LLMs without the need for fine-tuning. At a high level, superposition prompting allows the LLM to process input documents in parallel prompt paths, discarding paths once they are deemed irrelevant. We demonstrate the capability of our method to simultaneously enhance time efficiency across a variety of question-answering benchmarks using multiple pre-trained LLMs. Furthermore, our technique significantly improves accuracy when the retrieved context is large relative the context the model was trained on. For example, our approach facilitates an 93x reduction in compute time while improving accuracy by 43\\% on the NaturalQuestions-Open dataset with the MPT-7B instruction-tuned model over naive RAG.","sentences":["Despite the successes of large language models (LLMs), they exhibit significant drawbacks, particularly when processing long contexts.","Their inference cost scales quadratically with respect to sequence length, making it expensive for deployment in some real-world text processing applications, such as retrieval-augmented generation (RAG).","Additionally, LLMs also exhibit the \"distraction phenomenon,\" where irrelevant context in the prompt degrades output quality.","To address these drawbacks, we propose a novel RAG prompting methodology, superposition prompting, which can be directly applied to pre-trained transformer-based LLMs without the need for fine-tuning.","At a high level, superposition prompting allows the LLM to process input documents in parallel prompt paths, discarding paths once they are deemed irrelevant.","We demonstrate the capability of our method to simultaneously enhance time efficiency across a variety of question-answering benchmarks using multiple pre-trained LLMs.","Furthermore, our technique significantly improves accuracy when the retrieved context is large relative the context the model was trained on.","For example, our approach facilitates an 93x reduction in compute time while improving accuracy by 43\\% on the NaturalQuestions-Open dataset with the MPT-7B instruction-tuned model over naive RAG."],"url":"http://arxiv.org/abs/2404.06910v1","category":"cs.CL"}
{"created":"2024-04-10 10:46:59","title":"DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic Gaussian Splatting","abstract":"The increasing demand for virtual reality applications has highlighted the significance of crafting immersive 3D assets. We present a text-to-3D 360$^{\\circ}$ scene generation pipeline that facilitates the creation of comprehensive 360$^{\\circ}$ scenes for in-the-wild environments in a matter of minutes. Our approach utilizes the generative power of a 2D diffusion model and prompt self-refinement to create a high-quality and globally coherent panoramic image. This image acts as a preliminary \"flat\" (2D) scene representation. Subsequently, it is lifted into 3D Gaussians, employing splatting techniques to enable real-time exploration. To produce consistent 3D geometry, our pipeline constructs a spatially coherent structure by aligning the 2D monocular depth into a globally optimized point cloud. This point cloud serves as the initial state for the centroids of 3D Gaussians. In order to address invisible issues inherent in single-view inputs, we impose semantic and geometric constraints on both synthesized and input camera views as regularizations. These guide the optimization of Gaussians, aiding in the reconstruction of unseen regions. In summary, our method offers a globally consistent 3D scene within a 360$^{\\circ}$ perspective, providing an enhanced immersive experience over existing techniques. Project website at: http://dreamscene360.github.io/","sentences":["The increasing demand for virtual reality applications has highlighted the significance of crafting immersive 3D assets.","We present a text-to-3D 360$^{\\circ}$ scene generation pipeline that facilitates the creation of comprehensive 360$^{\\circ}$ scenes for in-the-wild environments in a matter of minutes.","Our approach utilizes the generative power of a 2D diffusion model and prompt self-refinement to create a high-quality and globally coherent panoramic image.","This image acts as a preliminary \"flat\" (2D) scene representation.","Subsequently, it is lifted into 3D Gaussians, employing splatting techniques to enable real-time exploration.","To produce consistent 3D geometry, our pipeline constructs a spatially coherent structure by aligning the 2D monocular depth into a globally optimized point cloud.","This point cloud serves as the initial state for the centroids of 3D Gaussians.","In order to address invisible issues inherent in single-view inputs, we impose semantic and geometric constraints on both synthesized and input camera views as regularizations.","These guide the optimization of Gaussians, aiding in the reconstruction of unseen regions.","In summary, our method offers a globally consistent 3D scene within a 360$^{\\circ}$ perspective, providing an enhanced immersive experience over existing techniques.","Project website at: http://dreamscene360.github.io/"],"url":"http://arxiv.org/abs/2404.06903v1","category":"cs.CV"}
{"created":"2024-04-10 10:46:43","title":"Spatiotemporal Analysis of Shared Situation Awareness among Connected Vehicles","abstract":"Shared situation awareness (SSA) has been garnering explosive interest in various applications for intelligent transportation systems (ITS). In addition, the delay-constrained nature of supporting vehicular networks makes it critical to precisely analyze the performance of a SSA procedure. Extending the relevant literature, this paper provides an analysis framework that evaluates the performance of SSA in spatial and temporal aspects simultaneously. Specifically, this paper provides a closed-form probability distribution for the length of time taken for constitution of a SSA among a group of connected vehicles. This paper extends the calculation to investigation of feasibility of SSA in supporting various types of safety messages defined by the SAE J2735.","sentences":["Shared situation awareness (SSA) has been garnering explosive interest in various applications for intelligent transportation systems (ITS).","In addition, the delay-constrained nature of supporting vehicular networks makes it critical to precisely analyze the performance of a SSA procedure.","Extending the relevant literature, this paper provides an analysis framework that evaluates the performance of SSA in spatial and temporal aspects simultaneously.","Specifically, this paper provides a closed-form probability distribution for the length of time taken for constitution of a SSA among a group of connected vehicles.","This paper extends the calculation to investigation of feasibility of SSA in supporting various types of safety messages defined by the SAE J2735."],"url":"http://arxiv.org/abs/2404.06902v1","category":"eess.SY"}
{"created":"2024-04-10 10:13:37","title":"Research on Detection of Floating Objects in River and Lake Based on AI Intelligent Image Recognition","abstract":"With the rapid advancement of artificial intelligence technology, AI-enabled image recognition has emerged as a potent tool for addressing challenges in traditional environmental monitoring. This study focuses on the detection of floating objects in river and lake environments, exploring an innovative approach based on deep learning. By intricately analyzing the technical pathways for detecting static and dynamic features and considering the characteristics of river and lake debris, a comprehensive image acquisition and processing workflow has been developed. The study highlights the application and performance comparison of three mainstream deep learning models -SSD, Faster-RCNN, and YOLOv5- in debris identification. Additionally, a detection system for floating objects has been designed and implemented, encompassing both hardware platform construction and software framework development. Through rigorous experimental validation, the proposed system has demonstrated its ability to significantly enhance the accuracy and efficiency of debris detection, thus offering a new technological avenue for water quality monitoring in rivers and lakes","sentences":["With the rapid advancement of artificial intelligence technology, AI-enabled image recognition has emerged as a potent tool for addressing challenges in traditional environmental monitoring.","This study focuses on the detection of floating objects in river and lake environments, exploring an innovative approach based on deep learning.","By intricately analyzing the technical pathways for detecting static and dynamic features and considering the characteristics of river and lake debris, a comprehensive image acquisition and processing workflow has been developed.","The study highlights the application and performance comparison of three mainstream deep learning models -SSD, Faster-RCNN, and YOLOv5- in debris identification.","Additionally, a detection system for floating objects has been designed and implemented, encompassing both hardware platform construction and software framework development.","Through rigorous experimental validation, the proposed system has demonstrated its ability to significantly enhance the accuracy and efficiency of debris detection, thus offering a new technological avenue for water quality monitoring in rivers and lakes"],"url":"http://arxiv.org/abs/2404.06883v1","category":"cs.CV"}
{"created":"2024-04-10 10:08:42","title":"Joint Active And Passive IRS Aided Wireless Communication: Elements Allocation and Achievable Rate","abstract":"Equipping reflecting elements at the active intelligent reflecting surface (AIRS) enhances signal amplification capability but meanwhile incurs non-negligible amplification noise, which thus challenges the determination of elements allocation for maximizing achievable rate in multi-cooperative AIRS and passive IRS (PIRS) jointly aided wireless communication system. To tackle this issue, we consider the downlink communication from a single-antenna transmitter (Tx) to a single-antenna receiver (Rx), which aided by a pair of AIRS and PIRS with two different deployment orders. Specifically, we target to determine the number of AIRS/PIRS elements over both transmission orders under given deployment budget for the achievable rate maximization. Our analysis illustrates that the PIRS should be allocated more elements than the AIRS for achieving optimized rate and linear signal-to-noise ratio (SNR) scaling orders are attained in both schemes. Simulation results are provided to evaluate the proposed algorithm and compare the rate performance of the AIRS and PIRS jointly aided wireless system with various benchmark systems.","sentences":["Equipping reflecting elements at the active intelligent reflecting surface (AIRS) enhances signal amplification capability but meanwhile incurs non-negligible amplification noise, which thus challenges the determination of elements allocation for maximizing achievable rate in multi-cooperative AIRS and passive IRS (PIRS) jointly aided wireless communication system.","To tackle this issue, we consider the downlink communication from a single-antenna transmitter (Tx) to a single-antenna receiver (Rx), which aided by a pair of AIRS and PIRS with two different deployment orders.","Specifically, we target to determine the number of AIRS/PIRS elements over both transmission orders under given deployment budget for the achievable rate maximization.","Our analysis illustrates that the PIRS should be allocated more elements than the AIRS for achieving optimized rate and linear signal-to-noise ratio (SNR) scaling orders are attained in both schemes.","Simulation results are provided to evaluate the proposed algorithm and compare the rate performance of the AIRS and PIRS jointly aided wireless system with various benchmark systems."],"url":"http://arxiv.org/abs/2404.06880v2","category":"cs.IT"}
{"created":"2024-04-10 09:47:34","title":"SleepPPG-Net2: Deep learning generalization for sleep staging from photoplethysmography","abstract":"Background: Sleep staging is a fundamental component in the diagnosis of sleep disorders and the management of sleep health. Traditionally, this analysis is conducted in clinical settings and involves a time-consuming scoring procedure. Recent data-driven algorithms for sleep staging, using the photoplethysmogram (PPG) time series, have shown high performance on local test sets but lower performance on external datasets due to data drift. Methods: This study aimed to develop a generalizable deep learning model for the task of four class (wake, light, deep, and rapid eye movement (REM)) sleep staging from raw PPG physiological time-series. Six sleep datasets, totaling 2,574 patients recordings, were used. In order to create a more generalizable representation, we developed and evaluated a deep learning model called SleepPPG-Net2, which employs a multi-source domain training approach.SleepPPG-Net2 was benchmarked against two state-of-the-art models. Results: SleepPPG-Net2 showed consistently higher performance over benchmark approaches, with generalization performance (Cohen's kappa) improving by up to 19%. Performance disparities were observed in relation to age, sex, and sleep apnea severity. Conclusion: SleepPPG-Net2 sets a new standard for staging sleep from raw PPG time-series.","sentences":["Background:","Sleep staging is a fundamental component in the diagnosis of sleep disorders and the management of sleep health.","Traditionally, this analysis is conducted in clinical settings and involves a time-consuming scoring procedure.","Recent data-driven algorithms for sleep staging, using the photoplethysmogram (PPG) time series, have shown high performance on local test sets but lower performance on external datasets due to data drift.","Methods:","This study aimed to develop a generalizable deep learning model for the task of four class (wake, light, deep, and rapid eye movement (REM)) sleep staging from raw PPG physiological time-series.","Six sleep datasets, totaling 2,574 patients recordings, were used.","In order to create a more generalizable representation, we developed and evaluated a deep learning model called SleepPPG-Net2, which employs a multi-source domain training approach.","SleepPPG-Net2 was benchmarked against two state-of-the-art models.","Results: SleepPPG-Net2 showed consistently higher performance over benchmark approaches, with generalization performance (Cohen's kappa) improving by up to 19%.","Performance disparities were observed in relation to age, sex, and sleep apnea severity.","Conclusion: SleepPPG-Net2 sets a new standard for staging sleep from raw PPG time-series."],"url":"http://arxiv.org/abs/2404.06869v1","category":"cs.LG"}
{"created":"2024-04-10 09:35:36","title":"Multi-Label Continual Learning for the Medical Domain: A Novel Benchmark","abstract":"Multi-label image classification in dynamic environments is a problem that poses significant challenges. Previous studies have primarily focused on scenarios such as Domain Incremental Learning and Class Incremental Learning, which do not fully capture the complexity of real-world applications. In this paper, we study the problem of classification of medical imaging in the scenario termed New Instances and New Classes, which combines the challenges of both new class arrivals and domain shifts in a single framework. Unlike traditional scenarios, it reflects the realistic nature of CL in domains such as medical imaging, where updates may introduce both new classes and changes in domain characteristics. To address the unique challenges posed by this complex scenario, we introduce a novel approach called Pseudo-Label Replay. This method aims to mitigate forgetting while adapting to new classes and domain shifts by combining the advantages of the Replay and Pseudo-Label methods and solving their limitations in the proposed scenario. We evaluate our proposed approach on a challenging benchmark consisting of two datasets, seven tasks, and nineteen classes, modeling a realistic Continual Learning scenario. Our experimental findings demonstrate the effectiveness of Pseudo-Label Replay in addressing the challenges posed by the complex scenario proposed. Our method surpasses existing approaches, exhibiting superior performance while showing minimal forgetting.","sentences":["Multi-label image classification in dynamic environments is a problem that poses significant challenges.","Previous studies have primarily focused on scenarios such as Domain Incremental Learning and Class Incremental Learning, which do not fully capture the complexity of real-world applications.","In this paper, we study the problem of classification of medical imaging in the scenario termed New Instances and New Classes, which combines the challenges of both new class arrivals and domain shifts in a single framework.","Unlike traditional scenarios, it reflects the realistic nature of CL in domains such as medical imaging, where updates may introduce both new classes and changes in domain characteristics.","To address the unique challenges posed by this complex scenario, we introduce a novel approach called Pseudo-Label Replay.","This method aims to mitigate forgetting while adapting to new classes and domain shifts by combining the advantages of the Replay and Pseudo-Label methods and solving their limitations in the proposed scenario.","We evaluate our proposed approach on a challenging benchmark consisting of two datasets, seven tasks, and nineteen classes, modeling a realistic Continual Learning scenario.","Our experimental findings demonstrate the effectiveness of Pseudo-Label Replay in addressing the challenges posed by the complex scenario proposed.","Our method surpasses existing approaches, exhibiting superior performance while showing minimal forgetting."],"url":"http://arxiv.org/abs/2404.06859v2","category":"cs.CV"}
{"created":"2024-04-10 09:25:18","title":"Research Artifacts in Software Engineering Publications: Status and Trends","abstract":"The Software Engineering (SE) community has been embracing the open science policy and encouraging researchers to disclose artifacts in their publications. However, the status and trends of artifact practice and quality remain unclear, lacking insights on further improvement. In this paper, we present an empirical study to characterize the research artifacts in SE publications. Specifically, we manually collect 1,487 artifacts from all 2,196 papers published in top-tier SE conferences (ASE, FSE, ICSE, and ISSTA) from 2017 to 2022. We investigate the common practices (e.g., URL location and format, storage websites), maintenance activities (e.g., last update time and URL validity), popularity (e.g., the number of stars on GitHub and characteristics), and quality (e.g., documentation and code smell) of these artifacts. Based on our analysis, we reveal a rise in publications providing artifacts. The usage of Zenodo for sharing artifacts has significantly increased. However, artifacts stored in GitHub tend to receive few stars, indicating a limited influence on real-world SE applications. We summarize the results and provide suggestions to different stakeholders in conjunction with current guidelines.","sentences":["The Software Engineering (SE) community has been embracing the open science policy and encouraging researchers to disclose artifacts in their publications.","However, the status and trends of artifact practice and quality remain unclear, lacking insights on further improvement.","In this paper, we present an empirical study to characterize the research artifacts in SE publications.","Specifically, we manually collect 1,487 artifacts from all 2,196 papers published in top-tier SE conferences (ASE, FSE, ICSE, and ISSTA) from 2017 to 2022.","We investigate the common practices (e.g., URL location and format, storage websites), maintenance activities (e.g., last update time and URL validity), popularity (e.g., the number of stars on GitHub and characteristics), and quality (e.g., documentation and code smell) of these artifacts.","Based on our analysis, we reveal a rise in publications providing artifacts.","The usage of Zenodo for sharing artifacts has significantly increased.","However, artifacts stored in GitHub tend to receive few stars, indicating a limited influence on real-world SE applications.","We summarize the results and provide suggestions to different stakeholders in conjunction with current guidelines."],"url":"http://arxiv.org/abs/2404.06852v1","category":"cs.SE"}
{"created":"2024-04-10 09:09:03","title":"Projection method for quasiperiodic elliptic equations and application to quasiperiodic homogenization","abstract":"In this study, our main objective is to address the challenge of solving elliptic equations with quasiperiodic coefficients. To achieve accurate and efficient computation, we introduce the projection method, which enables the embedding of quasiperiodic systems into higher-dimensional periodic systems. To enhance the computational efficiency, we propose a compressed storage strategy for the stiffness matrix, reducing memory requirements while preserving accuracy. Furthermore, we design a diagonal preconditioner to efficiently solve the resulting high-dimensional linear system by reducing the condition number of the stiffness matrix. These techniques collectively contribute to the computational effectiveness of our proposed approach. We demonstrate the effectiveness and accuracy of our approach through a series of numerical examples. Moreover, we apply our method to achieve a highly accurate computation of the homogenized coefficients for a quasiperiodic multiscale elliptic equation.","sentences":["In this study, our main objective is to address the challenge of solving elliptic equations with quasiperiodic coefficients.","To achieve accurate and efficient computation, we introduce the projection method, which enables the embedding of quasiperiodic systems into higher-dimensional periodic systems.","To enhance the computational efficiency, we propose a compressed storage strategy for the stiffness matrix, reducing memory requirements while preserving accuracy.","Furthermore, we design a diagonal preconditioner to efficiently solve the resulting high-dimensional linear system by reducing the condition number of the stiffness matrix.","These techniques collectively contribute to the computational effectiveness of our proposed approach.","We demonstrate the effectiveness and accuracy of our approach through a series of numerical examples.","Moreover, we apply our method to achieve a highly accurate computation of the homogenized coefficients for a quasiperiodic multiscale elliptic equation."],"url":"http://arxiv.org/abs/2404.06841v1","category":"math.NA"}
{"created":"2024-04-10 08:32:29","title":"Proposed modified computational model for the amoeba-inspired combinatorial optimization machine","abstract":"A single-celled amoeba can solve the traveling salesman problem through its shape-changing dynamics. In this paper, we examine roles of several elements in a previously proposed computational model of the solution-search process of amoeba and three modifications towards enhancing the solution-search preformance. We find that appropriate modifications can indeed significantly improve the quality of solutions. It is also found that a condition associated with the volume conservation can also be modified in contrast to the naive belief that it is indispensable for the solution-search ability of amoeba. A proposed modified model shows much better performance.","sentences":["A single-celled amoeba can solve the traveling salesman problem through its shape-changing dynamics.","In this paper, we examine roles of several elements in a previously proposed computational model of the solution-search process of amoeba and three modifications towards enhancing the solution-search preformance.","We find that appropriate modifications can indeed significantly improve the quality of solutions.","It is also found that a condition associated with the volume conservation can also be modified in contrast to the naive belief that it is indispensable for the solution-search ability of amoeba.","A proposed modified model shows much better performance."],"url":"http://arxiv.org/abs/2404.06828v1","category":"cs.NE"}
{"created":"2024-04-10 08:31:40","title":"Impact of Extensions on Browser Performance: An Empirical Study on Google Chrome","abstract":"Web browsers have been used widely by users to conduct various online activities, such as information seeking or online shopping. To improve user experience and extend the functionality of browsers, practitioners provide mechanisms to allow users to install third-party-provided plugins (i.e., extensions) on their browsers. However, little is known about the performance implications caused by such extensions. In this paper, we conduct an empirical study to understand the impact of extensions on the user-perceived performance (i.e., energy consumption and page load time) of Google Chrome, the most popular browser. We study a total of 72 representative extensions from 11 categories (e.g., Developer Tools and Sports). We observe that browser performance can be negatively impacted by the use of extensions, even when the extensions are used in unintended circumstances (e.g., when logging into an extension is not granted but required, or when an extension is not used for designated websites). We also identify a set of factors that significantly influence the performance impact of extensions, such as code complexity and privacy practices (i.e., collection of user data) adopted by the extensions. Based on our empirical observations, we provide recommendations for developers and users to mitigate the performance impact of browser extensions, such as conducting performance testing and optimization for unintended usage scenarios of extensions, or adhering to proper usage practices of extensions (e.g., logging into an extension when required).","sentences":["Web browsers have been used widely by users to conduct various online activities, such as information seeking or online shopping.","To improve user experience and extend the functionality of browsers, practitioners provide mechanisms to allow users to install third-party-provided plugins (i.e., extensions) on their browsers.","However, little is known about the performance implications caused by such extensions.","In this paper, we conduct an empirical study to understand the impact of extensions on the user-perceived performance (i.e., energy consumption and page load time) of Google Chrome, the most popular browser.","We study a total of 72 representative extensions from 11 categories (e.g., Developer Tools and Sports).","We observe that browser performance can be negatively impacted by the use of extensions, even when the extensions are used in unintended circumstances (e.g., when logging into an extension is not granted but required, or when an extension is not used for designated websites).","We also identify a set of factors that significantly influence the performance impact of extensions, such as code complexity and privacy practices (i.e., collection of user data) adopted by the extensions.","Based on our empirical observations, we provide recommendations for developers and users to mitigate the performance impact of browser extensions, such as conducting performance testing and optimization for unintended usage scenarios of extensions, or adhering to proper usage practices of extensions (e.g., logging into an extension when required)."],"url":"http://arxiv.org/abs/2404.06827v1","category":"cs.PF"}
{"created":"2024-04-10 08:13:21","title":"A proposal for a revised meta-architecture of intelligent tutoring systems to foster explainability and transparency for educators","abstract":"This contribution draws attention to implications connected with meta-architectural design decisions for intelligent tutoring systems in the context of formative assessments. As a first result of addressing this issue, this contribution presents a meta-architectural system design that includes the role of educators.","sentences":["This contribution draws attention to implications connected with meta-architectural design decisions for intelligent tutoring systems in the context of formative assessments.","As a first result of addressing this issue, this contribution presents a meta-architectural system design that includes the role of educators."],"url":"http://arxiv.org/abs/2404.06820v1","category":"cs.HC"}
{"created":"2024-04-10 06:58:58","title":"Private Wasserstein Distance with Random Noises","abstract":"Wasserstein distance is a principle measure of data divergence from a distributional standpoint. However, its application becomes challenging in the context of data privacy, where sharing raw data is restricted. Prior attempts have employed techniques like Differential Privacy or Federated optimization to approximate Wasserstein distance. Nevertheless, these approaches often lack accuracy and robustness against potential attack. In this study, we investigate the underlying triangular properties within the Wasserstein space, leading to a straightforward solution named TriangleWad. This approach enables the computation of Wasserstein distance between datasets stored across different entities. Notably, TriangleWad is 20 times faster, making raw data information truly invisible, enhancing resilience against attacks, and without sacrificing estimation accuracy. Through comprehensive experimentation across various tasks involving both image and text data, we demonstrate its superior performance and generalizations.","sentences":["Wasserstein distance is a principle measure of data divergence from a distributional standpoint.","However, its application becomes challenging in the context of data privacy, where sharing raw data is restricted.","Prior attempts have employed techniques like Differential Privacy or Federated optimization to approximate Wasserstein distance.","Nevertheless, these approaches often lack accuracy and robustness against potential attack.","In this study, we investigate the underlying triangular properties within the Wasserstein space, leading to a straightforward solution named TriangleWad.","This approach enables the computation of Wasserstein distance between datasets stored across different entities.","Notably, TriangleWad is 20 times faster, making raw data information truly invisible, enhancing resilience against attacks, and without sacrificing estimation accuracy.","Through comprehensive experimentation across various tasks involving both image and text data, we demonstrate its superior performance and generalizations."],"url":"http://arxiv.org/abs/2404.06787v1","category":"cs.LG"}
{"created":"2024-04-10 06:45:02","title":"On the effect of a large cloud of rigid particles on the motion of an incompressible non--Newtonian fluid","abstract":"We show that the collective effect of $N$ rigid bodies $(\\mathcal{S}_{n,N})_{n=1}^N$ of diameters $(r_{n,N})_{n=1}^N$ immersed in an incompressible non--Newtonian fluid is negligible in the asymptotic limit $N \\to \\infty$ as long as their total packing volume $\\sum_{n=1}^N r_{n,N}^d$, $d=2,3$ tends to zero exponentially -- $\\sum_{n=1}^N r_{n,N}^d \\approx A^{-N}$ -- for a certain constant $A > 1$. The result is rather surprising and in a sharp contrast with the associated homogenization problem, where the same number of obstacles can completely stop the fluid motion in the case of shear thickening viscosity. A large class of non--Newtonian fluids is included, for which the viscous stress is a subdifferential of a convex potential.","sentences":["We show that the collective effect of $N$ rigid bodies $(\\mathcal{S}_{n,N})_{n=1}^N$ of diameters $(r_{n,N})_{n=1}^N$ immersed in an incompressible non--Newtonian fluid is negligible in the asymptotic limit $N \\to \\infty$ as long as their total packing volume $\\sum_{n=1}^N r_{n,N}^d$, $d=2,3$ tends to zero exponentially -- $\\sum_{n=1}^N r_{n,N}^d \\approx A^{-N}$ -- for a certain constant $A > 1$.","The result is rather surprising and in a sharp contrast with the associated homogenization problem, where the same number of obstacles can completely stop the fluid motion in the case of shear thickening viscosity.","A large class of non--Newtonian fluids is included, for which the viscous stress is a subdifferential of a convex potential."],"url":"http://arxiv.org/abs/2404.06782v1","category":"math.AP"}
{"created":"2024-04-10 06:39:18","title":"Efficient and Scalable Chinese Vector Font Generation via Component Composition","abstract":"Chinese vector font generation is challenging due to the complex structure and huge amount of Chinese characters. Recent advances remain limited to generating a small set of characters with simple structure. In this work, we first observe that most Chinese characters can be disassembled into frequently-reused components. Therefore, we introduce the first efficient and scalable Chinese vector font generation approach via component composition, allowing generating numerous vector characters from a small set of components. To achieve this, we collect a large-scale dataset that contains over \\textit{90K} Chinese characters with their components and layout information. Upon the dataset, we propose a simple yet effective framework based on spatial transformer networks (STN) and multiple losses tailored to font characteristics to learn the affine transformation of the components, which can be directly applied to the B\\'ezier curves, resulting in Chinese characters in vector format. Our qualitative and quantitative experiments have demonstrated that our method significantly surpasses the state-of-the-art vector font generation methods in generating large-scale complex Chinese characters in both font generation and zero-shot font extension.","sentences":["Chinese vector font generation is challenging due to the complex structure and huge amount of Chinese characters.","Recent advances remain limited to generating a small set of characters with simple structure.","In this work, we first observe that most Chinese characters can be disassembled into frequently-reused components.","Therefore, we introduce the first efficient and scalable Chinese vector font generation approach via component composition, allowing generating numerous vector characters from a small set of components.","To achieve this, we collect a large-scale dataset that contains over \\textit{90K} Chinese characters with their components and layout information.","Upon the dataset, we propose a simple yet effective framework based on spatial transformer networks (STN) and multiple losses tailored to font characteristics to learn the affine transformation of the components, which can be directly applied to the B\\'ezier curves, resulting in Chinese characters in vector format.","Our qualitative and quantitative experiments have demonstrated that our method significantly surpasses the state-of-the-art vector font generation methods in generating large-scale complex Chinese characters in both font generation and zero-shot font extension."],"url":"http://arxiv.org/abs/2404.06779v1","category":"cs.CV"}
{"created":"2024-04-10 06:36:48","title":"Responsible Federated Learning in Smart Transportation: Outlooks and Challenges","abstract":"Integrating artificial intelligence (AI) and federated learning (FL) in smart transportation has raised critical issues regarding their responsible use. Ensuring responsible AI is paramount for the stability and sustainability of intelligent transportation systems. Despite its importance, research on the responsible application of AI and FL in this domain remains nascent, with a paucity of in-depth investigations into their confluence. Our study analyzes the roles of FL in smart transportation, as well as the promoting effect of responsible AI on distributed smart transportation. Lastly, we discuss the challenges of developing and implementing responsible FL in smart transportation and propose potential solutions. By integrating responsible AI and federated learning, intelligent transportation systems are expected to achieve a higher degree of intelligence, personalization, safety, and transparency.","sentences":["Integrating artificial intelligence (AI) and federated learning (FL) in smart transportation has raised critical issues regarding their responsible use.","Ensuring responsible AI is paramount for the stability and sustainability of intelligent transportation systems.","Despite its importance, research on the responsible application of AI and FL in this domain remains nascent, with a paucity of in-depth investigations into their confluence.","Our study analyzes the roles of FL in smart transportation, as well as the promoting effect of responsible AI on distributed smart transportation.","Lastly, we discuss the challenges of developing and implementing responsible FL in smart transportation and propose potential solutions.","By integrating responsible AI and federated learning, intelligent transportation systems are expected to achieve a higher degree of intelligence, personalization, safety, and transparency."],"url":"http://arxiv.org/abs/2404.06777v1","category":"cs.NI"}
{"created":"2024-04-10 06:35:25","title":"Logit Calibration and Feature Contrast for Robust Federated Learning on Non-IID Data","abstract":"Federated learning (FL) is a privacy-preserving distributed framework for collaborative model training on devices in edge networks. However, challenges arise due to vulnerability to adversarial examples (AEs) and the non-independent and identically distributed (non-IID) nature of data distribution among devices, hindering the deployment of adversarially robust and accurate learning models at the edge. While adversarial training (AT) is commonly acknowledged as an effective defense strategy against adversarial attacks in centralized training, we shed light on the adverse effects of directly applying AT in FL that can severely compromise accuracy, especially in non-IID challenges. Given this limitation, this paper proposes FatCC, which incorporates local logit \\underline{C}alibration and global feature \\underline{C}ontrast into the vanilla federated adversarial training (\\underline{FAT}) process from both logit and feature perspectives. This approach can effectively enhance the federated system's robust accuracy (RA) and clean accuracy (CA). First, we propose logit calibration, where the logits are calibrated during local adversarial updates, thereby improving adversarial robustness. Second, FatCC introduces feature contrast, which involves a global alignment term that aligns each local representation with unbiased global features, thus further enhancing robustness and accuracy in federated adversarial environments. Extensive experiments across multiple datasets demonstrate that FatCC achieves comparable or superior performance gains in both CA and RA compared to other baselines.","sentences":["Federated learning (FL) is a privacy-preserving distributed framework for collaborative model training on devices in edge networks.","However, challenges arise due to vulnerability to adversarial examples (AEs) and the non-independent and identically distributed (non-IID) nature of data distribution among devices, hindering the deployment of adversarially robust and accurate learning models at the edge.","While adversarial training (AT) is commonly acknowledged as an effective defense strategy against adversarial attacks in centralized training, we shed light on the adverse effects of directly applying AT in FL that can severely compromise accuracy, especially in non-IID challenges.","Given this limitation, this paper proposes FatCC, which incorporates local logit \\underline{C}alibration and global feature \\underline{C}ontrast into the vanilla federated adversarial training (\\underline{FAT}) process from both logit and feature perspectives.","This approach can effectively enhance the federated system's robust accuracy (RA) and clean accuracy (CA).","First, we propose logit calibration, where the logits are calibrated during local adversarial updates, thereby improving adversarial robustness.","Second, FatCC introduces feature contrast, which involves a global alignment term that aligns each local representation with unbiased global features, thus further enhancing robustness and accuracy in federated adversarial environments.","Extensive experiments across multiple datasets demonstrate that FatCC achieves comparable or superior performance gains in both CA and RA compared to other baselines."],"url":"http://arxiv.org/abs/2404.06776v1","category":"cs.LG"}
{"created":"2024-04-10 06:19:19","title":"Solving the Food-Energy-Water Nexus Problem via Intelligent Optimization Algorithms","abstract":"The application of evolutionary algorithms (EAs) to multi-objective optimization problems has been widespread. However, the EA research community has not paid much attention to large-scale multi-objective optimization problems arising from real-world applications. Especially, Food-Energy-Water systems are intricately linked among food, energy and water that impact each other. They usually involve a huge number of decision variables and many conflicting objectives to be optimized. Solving their related optimization problems is essentially important to sustain the high-quality life of human beings. Their solution space size expands exponentially with the number of decision variables. Searching in such a vast space is challenging because of such large numbers of decision variables and objective functions. In recent years, a number of large-scale many-objectives optimization evolutionary algorithms have been proposed. In this paper, we solve a Food-Energy-Water optimization problem by using the state-of-art intelligent optimization methods and compare their performance. Our results conclude that the algorithm based on an inverse model outperforms the others. This work should be highly useful for practitioners to select the most suitable method for their particular large-scale engineering optimization problems.","sentences":["The application of evolutionary algorithms (EAs) to multi-objective optimization problems has been widespread.","However, the EA research community has not paid much attention to large-scale multi-objective optimization problems arising from real-world applications.","Especially, Food-Energy-Water systems are intricately linked among food, energy and water that impact each other.","They usually involve a huge number of decision variables and many conflicting objectives to be optimized.","Solving their related optimization problems is essentially important to sustain the high-quality life of human beings.","Their solution space size expands exponentially with the number of decision variables.","Searching in such a vast space is challenging because of such large numbers of decision variables and objective functions.","In recent years, a number of large-scale many-objectives optimization evolutionary algorithms have been proposed.","In this paper, we solve a Food-Energy-Water optimization problem by using the state-of-art intelligent optimization methods and compare their performance.","Our results conclude that the algorithm based on an inverse model outperforms the others.","This work should be highly useful for practitioners to select the most suitable method for their particular large-scale engineering optimization problems."],"url":"http://arxiv.org/abs/2404.06769v1","category":"cs.NE"}
{"created":"2024-04-10 17:53:33","title":"Laser driven melt pool resonances through dynamically oscillating energy inputs","abstract":"Spatially selective melting of metal materials by laser irradiation allows for the precise welding as well as the 3D printing of complex metal parts. However, the simple scanning of a conventional Gaussian beam typically results in a melt track with randomly distributed surface features due to the complex and dynamic behavior of the melt pool. In this study, the implications of utilizing a dynamically oscillating energy input on driving melt track fluctuations is investigated. Specifically, the laser intensity and/or intensity distribution is sinusoidally modulated at different scan speeds, and the effect of modulation frequency on the resulting surface features of the melt track is examined. The formation of periodically oriented surface features indicates an evident frequency coupling between the melt pool and the modulation frequency. Moreover, such a frequency coupling becomes most prominent under a specific modulation frequency, suggesting resonant behavior. The insights provided in this study will enable the development of novel methods, allowing for the control and/or mitigation of inherent fluctuations in the melt pool through laser-driven resonances.","sentences":["Spatially selective melting of metal materials by laser irradiation allows for the precise welding as well as the 3D printing of complex metal parts.","However, the simple scanning of a conventional Gaussian beam typically results in a melt track with randomly distributed surface features due to the complex and dynamic behavior of the melt pool.","In this study, the implications of utilizing a dynamically oscillating energy input on driving melt track fluctuations is investigated.","Specifically, the laser intensity and/or intensity distribution is sinusoidally modulated at different scan speeds, and the effect of modulation frequency on the resulting surface features of the melt track is examined.","The formation of periodically oriented surface features indicates an evident frequency coupling between the melt pool and the modulation frequency.","Moreover, such a frequency coupling becomes most prominent under a specific modulation frequency, suggesting resonant behavior.","The insights provided in this study will enable the development of novel methods, allowing for the control and/or mitigation of inherent fluctuations in the melt pool through laser-driven resonances."],"url":"http://arxiv.org/abs/2404.07195v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-10 17:48:37","title":"InstantMesh: Efficient 3D Mesh Generation from a Single Image with Sparse-view Large Reconstruction Models","abstract":"We present InstantMesh, a feed-forward framework for instant 3D mesh generation from a single image, featuring state-of-the-art generation quality and significant training scalability. By synergizing the strengths of an off-the-shelf multiview diffusion model and a sparse-view reconstruction model based on the LRM architecture, InstantMesh is able to create diverse 3D assets within 10 seconds. To enhance the training efficiency and exploit more geometric supervisions, e.g, depths and normals, we integrate a differentiable iso-surface extraction module into our framework and directly optimize on the mesh representation. Experimental results on public datasets demonstrate that InstantMesh significantly outperforms other latest image-to-3D baselines, both qualitatively and quantitatively. We release all the code, weights, and demo of InstantMesh, with the intention that it can make substantial contributions to the community of 3D generative AI and empower both researchers and content creators.","sentences":["We present InstantMesh, a feed-forward framework for instant 3D mesh generation from a single image, featuring state-of-the-art generation quality and significant training scalability.","By synergizing the strengths of an off-the-shelf multiview diffusion model and a sparse-view reconstruction model based on the LRM architecture, InstantMesh is able to create diverse 3D assets within 10 seconds.","To enhance the training efficiency and exploit more geometric supervisions, e.g, depths and normals, we integrate a differentiable iso-surface extraction module into our framework and directly optimize on the mesh representation.","Experimental results on public datasets demonstrate that InstantMesh significantly outperforms other latest image-to-3D baselines, both qualitatively and quantitatively.","We release all the code, weights, and demo of InstantMesh, with the intention that it can make substantial contributions to the community of 3D generative AI and empower both researchers and content creators."],"url":"http://arxiv.org/abs/2404.07191v1","category":"cs.CV"}
{"created":"2024-04-10 17:29:12","title":"Machine learning-based similarity measure to forecast M&A from patent data","abstract":"Defining and finalizing Mergers and Acquisitions (M&A) requires complex human skills, which makes it very hard to automatically find the best partner or predict which firms will make a deal. In this work, we propose the MASS algorithm, a specifically designed measure of similarity between companies and we apply it to patenting activity data to forecast M&A deals. MASS is based on an extreme simplification of tree-based machine learning algorithms and naturally incorporates intuitive criteria for deals; as such, it is fully interpretable and explainable. By applying MASS to the Zephyr and Crunchbase datasets, we show that it outperforms LightGCN, a \"black box\" graph convolutional network algorithm. When similar companies have disjoint patenting activities, on the contrary, LightGCN turns out to be the most effective algorithm. This study provides a simple and powerful tool to model and predict M&A deals, offering valuable insights to managers and practitioners for informed decision-making.","sentences":["Defining and finalizing Mergers and Acquisitions (M&A) requires complex human skills, which makes it very hard to automatically find the best partner or predict which firms will make a deal.","In this work, we propose the MASS algorithm, a specifically designed measure of similarity between companies and we apply it to patenting activity data to forecast M&A deals.","MASS is based on an extreme simplification of tree-based machine learning algorithms and naturally incorporates intuitive criteria for deals; as such, it is fully interpretable and explainable.","By applying MASS to the Zephyr and Crunchbase datasets, we show that it outperforms LightGCN, a \"black box\" graph convolutional network algorithm.","When similar companies have disjoint patenting activities, on the contrary, LightGCN turns out to be the most effective algorithm.","This study provides a simple and powerful tool to model and predict M&A deals, offering valuable insights to managers and practitioners for informed decision-making."],"url":"http://arxiv.org/abs/2404.07179v1","category":"physics.soc-ph"}
{"created":"2024-04-10 17:27:54","title":"Scaling Laws for Data Filtering -- Data Curation cannot be Compute Agnostic","abstract":"Vision-language models (VLMs) are trained for thousands of GPU hours on carefully curated web datasets. In recent times, data curation has gained prominence with several works developing strategies to retain 'high-quality' subsets of 'raw' scraped data. For instance, the LAION public dataset retained only 10% of the total crawled data. However, these strategies are typically developed agnostic of the available compute for training. In this paper, we first demonstrate that making filtering decisions independent of training compute is often suboptimal: the limited high-quality data rapidly loses its utility when repeated, eventually requiring the inclusion of 'unseen' but 'lower-quality' data. To address this quality-quantity tradeoff ($\\texttt{QQT}$), we introduce neural scaling laws that account for the non-homogeneous nature of web data, an angle ignored in existing literature. Our scaling laws (i) characterize the $\\textit{differing}$ 'utility' of various quality subsets of web data; (ii) account for how utility diminishes for a data point at its 'nth' repetition; and (iii) formulate the mutual interaction of various data pools when combined, enabling the estimation of model performance on a combination of multiple data pools without ever jointly training on them. Our key message is that data curation $\\textit{cannot}$ be agnostic of the total compute that a model will be trained for. Our scaling laws allow us to curate the best possible pool for achieving top performance on Datacomp at various compute budgets, carving out a pareto-frontier for data curation. Code is available at https://github.com/locuslab/scaling_laws_data_filtering.","sentences":["Vision-language models (VLMs) are trained for thousands of GPU hours on carefully curated web datasets.","In recent times, data curation has gained prominence with several works developing strategies to retain 'high-quality' subsets of 'raw' scraped data.","For instance, the LAION public dataset retained only 10% of the total crawled data.","However, these strategies are typically developed agnostic of the available compute for training.","In this paper, we first demonstrate that making filtering decisions independent of training compute is often suboptimal: the limited high-quality data rapidly loses its utility when repeated, eventually requiring the inclusion of 'unseen' but 'lower-quality' data.","To address this quality-quantity tradeoff ($\\texttt{QQT}$), we introduce neural scaling laws that account for the non-homogeneous nature of web data, an angle ignored in existing literature.","Our scaling laws (i) characterize the $\\textit{differing}$ 'utility' of various quality subsets of web data; (ii) account for how utility diminishes for a data point at its 'nth' repetition; and (iii) formulate the mutual interaction of various data pools when combined, enabling the estimation of model performance on a combination of multiple data pools without ever jointly training on them.","Our key message is that data curation $\\textit{cannot}$ be agnostic of the total compute that a model will be trained for.","Our scaling laws allow us to curate the best possible pool for achieving top performance on Datacomp at various compute budgets, carving out a pareto-frontier for data curation.","Code is available at https://github.com/locuslab/scaling_laws_data_filtering."],"url":"http://arxiv.org/abs/2404.07177v1","category":"cs.LG"}
{"created":"2024-04-10 17:19:41","title":"Temperature Prediction for Stored Grain: A Multi-model Fusion Strategy Based on Machine Learning","abstract":"Temperature fluctuations significantly affect microorganism growth and pest activity in grain stacks. Thus, precise monitoring and forecasting of grain stack temperature are essential for maintaining the quality and safety of grain storage. This paper proposes a multi-model fusion approach to predict grain temperature using historical temperature data of stored grains and meteorological data from the region. Based on the proposed approaches, four distinct machine learning models, namely Adaboost, decision tree, extra trees, and random forest, are first developed. These models are then fine-tuned through parameter optimization to enhance their predictive capabilities. Subsequently, the optimized models are combined to form different ensemble models. In essence, the fusion process integrates the predictions of each individual model as new feature inputs into the prediction model. Furthermore, the study utilizes the random forest to identify the key factors influencing grain temperature, providing insights into the importance of different influencing factors. The experimental results demonstrate that the fusion models proposed in this paper have achieved higher prediction accuracy and robustness compared with the traditional prediction methods (i.e., single-model prediction). Additionally, the analysis of feature importance also offers empirical evidence for understanding the factors influencing grain temperature.","sentences":["Temperature fluctuations significantly affect microorganism growth and pest activity in grain stacks.","Thus, precise monitoring and forecasting of grain stack temperature are essential for maintaining the quality and safety of grain storage.","This paper proposes a multi-model fusion approach to predict grain temperature using historical temperature data of stored grains and meteorological data from the region.","Based on the proposed approaches, four distinct machine learning models, namely Adaboost, decision tree, extra trees, and random forest, are first developed.","These models are then fine-tuned through parameter optimization to enhance their predictive capabilities.","Subsequently, the optimized models are combined to form different ensemble models.","In essence, the fusion process integrates the predictions of each individual model as new feature inputs into the prediction model.","Furthermore, the study utilizes the random forest to identify the key factors influencing grain temperature, providing insights into the importance of different influencing factors.","The experimental results demonstrate that the fusion models proposed in this paper have achieved higher prediction accuracy and robustness compared with the traditional prediction methods (i.e., single-model prediction).","Additionally, the analysis of feature importance also offers empirical evidence for understanding the factors influencing grain temperature."],"url":"http://arxiv.org/abs/2404.07175v1","category":"cs.CE"}
{"created":"2024-04-10 17:02:34","title":"Adinkras and Pure Spinors","abstract":"The nilpotence variety for extended supersymmetric quantum mechanics is a cone over a quadric in projective space. The pure spinor correspondence, which relates the description of off-shell supermultiplets to the classification of modules over the corresponding hypersurface ring, reduces to a classical problem of linear algebra. Spinor bundles, which correspond to maximal Cohen-Macaulay modules, serve as basic building blocks. Koszul duality appears as a deformed version of the Bernstein-Gel'fand-Gel'fand correspondence that we make fully concrete. We illustrate in numerous examples the close relationship between these connections and the powerful graphical technology of Adinkras. We emphasize the role of R-symmetry for recovering higher-dimensional gauge and gravity multiplets.","sentences":["The nilpotence variety for extended supersymmetric quantum mechanics is a cone over a quadric in projective space.","The pure spinor correspondence, which relates the description of off-shell supermultiplets to the classification of modules over the corresponding hypersurface ring, reduces to a classical problem of linear algebra.","Spinor bundles, which correspond to maximal Cohen-Macaulay modules, serve as basic building blocks.","Koszul duality appears as a deformed version of the Bernstein-Gel'fand-Gel'fand correspondence that we make fully concrete.","We illustrate in numerous examples the close relationship between these connections and the powerful graphical technology of Adinkras.","We emphasize the role of R-symmetry for recovering higher-dimensional gauge and gravity multiplets."],"url":"http://arxiv.org/abs/2404.07167v1","category":"math-ph"}
{"created":"2024-04-10 17:02:00","title":"Pressure-tuned many-body phases through $\u0393$-K valleytronics in moir\u00e9 bilayer WSe$_2$","abstract":"Recent experiments in twisted bilayer transition-metal dichalcogenides have revealed a variety of strongly correlated phenomena. To theoretically explore their origin, we combine here ab initio calculations with correlated model approaches to describe and study many-body effects in twisted bilayer WSe$_2$ under pressure. We find that the interlayer distance is a key factor for the electronic structure, as it tunes the relative energetic positions between the K and the $\\Gamma$ valleys of the valence band maximum of the untwisted bilayer. As a result, applying uniaxial pressure to a twisted bilayer induces a charge-transfer from the K valley to the flat bands in the $\\Gamma$ valley. Upon Wannierizing moir\\'e bands from both valleys, we establish the relevant tight-binding model parameters and calculate the effective interaction strengths using the constrained random phase approximation. With this, we approximate the interacting pressure-doping phase diagram of WSe$_2$ moir\\'e bilayers using self-consistent mean field theory. Our results establish twisted bilayer WSe$_2$ as a platform that allows the direct pressure-tuning of different correlated phases, ranging from Mott insulators, charge-valley-transfer insulators to Kondo lattice-like systems.","sentences":["Recent experiments in twisted bilayer transition-metal dichalcogenides have revealed a variety of strongly correlated phenomena.","To theoretically explore their origin, we combine here ab initio calculations with correlated model approaches to describe and study many-body effects in twisted bilayer WSe$_2$ under pressure.","We find that the interlayer distance is a key factor for the electronic structure, as it tunes the relative energetic positions between the K and the $\\Gamma$ valleys of the valence band maximum of the untwisted bilayer.","As a result, applying uniaxial pressure to a twisted bilayer induces a charge-transfer from the K valley to the flat bands in the $\\Gamma$ valley.","Upon Wannierizing moir\\'e bands from both valleys, we establish the relevant tight-binding model parameters and calculate the effective interaction strengths using the constrained random phase approximation.","With this, we approximate the interacting pressure-doping phase diagram of WSe$_2$ moir\\'e bilayers using self-consistent mean field theory.","Our results establish twisted bilayer WSe$_2$ as a platform that allows the direct pressure-tuning of different correlated phases, ranging from Mott insulators, charge-valley-transfer insulators to Kondo lattice-like systems."],"url":"http://arxiv.org/abs/2404.07165v1","category":"cond-mat.str-el"}
{"created":"2024-04-10 16:49:39","title":"CBFKIT: A Control Barrier Function Toolbox for Robotics Applications","abstract":"This paper introduces CBFKit, a Python/ROS toolbox for safe robotics planning and control under uncertainty. The toolbox provides a general framework for designing control barrier functions for mobility systems within both deterministic and stochastic environments. It can be connected to the ROS open-source robotics middleware, allowing for the setup of multi-robot applications, encoding of environments and maps, and integrations with predictive motion planning algorithms. Additionally, it offers multiple CBF variations and algorithms for robot control. The CBFKit is demonstrated on the Toyota Human Support Robot (HSR) in both simulation and in physical experiments.","sentences":["This paper introduces CBFKit, a Python/ROS toolbox for safe robotics planning and control under uncertainty.","The toolbox provides a general framework for designing control barrier functions for mobility systems within both deterministic and stochastic environments.","It can be connected to the ROS open-source robotics middleware, allowing for the setup of multi-robot applications, encoding of environments and maps, and integrations with predictive motion planning algorithms.","Additionally, it offers multiple CBF variations and algorithms for robot control.","The CBFKit is demonstrated on the Toyota Human Support Robot (HSR) in both simulation and in physical experiments."],"url":"http://arxiv.org/abs/2404.07158v1","category":"cs.RO"}
{"created":"2024-04-10 16:18:07","title":"High-dimensional copula-based Wasserstein dependence","abstract":"We generalize 2-Wasserstein dependence coefficients to measure dependence between a finite number of random vectors. This generalization includes theoretical properties, and in particular focuses on an interpretation of maximal dependence and an asymptotic normality result for a proposed semi-parametric estimator under a Gaussian copula assumption. In addition, we discuss general axioms for dependence measures between multiple random vectors, other plausible normalizations, and various examples. Afterwards, we look into plug-in estimators based on penalized empirical covariance matrices in order to deal with high dimensionality issues and take possible marginal independencies into account by inducing (block) sparsity. The latter ideas are investigated via a simulation study, considering other dependence coefficients as well. We illustrate the use of the developed methods in two real data applications.","sentences":["We generalize 2-Wasserstein dependence coefficients to measure dependence between a finite number of random vectors.","This generalization includes theoretical properties, and in particular focuses on an interpretation of maximal dependence and an asymptotic normality result for a proposed semi-parametric estimator under a Gaussian copula assumption.","In addition, we discuss general axioms for dependence measures between multiple random vectors, other plausible normalizations, and various examples.","Afterwards, we look into plug-in estimators based on penalized empirical covariance matrices in order to deal with high dimensionality issues and take possible marginal independencies into account by inducing (block) sparsity.","The latter ideas are investigated via a simulation study, considering other dependence coefficients as well.","We illustrate the use of the developed methods in two real data applications."],"url":"http://arxiv.org/abs/2404.07141v1","category":"stat.ME"}
{"created":"2024-04-10 16:17:41","title":"Characterising directed and undirected metrics of high-order interdependence","abstract":"Systems of interest for theoretical or experimental work often exhibit high-order interactions, corresponding to statistical interdependencies in groups of variables that cannot be reduced to dependencies in subsets of them. While still under active development, the framework of partial information decomposition (PID) has emerged as the dominant approach to conceptualise and calculate high-order interdependencies. PID approaches can be grouped in two types: directed approaches that divide variables into sources and targets, and undirected approaches that treat all variables equally. Directed and undirected approaches are usually employed to investigate different scenarios, and hence little is known about how these two types of approaches may relate to each other, or if their corresponding quantities are linked in some way. In this paper we investigate the relationship between the redundancy-synergy index (RSI) and the O-information, which are practical metrics of directed and undirected high-order interdependencies, respectively. Our results reveal tight links between these two quantities, and provide interpretations of them in terms of likelihood ratios in a hypothesis testing setting, as well as in terms of projections in information geometry.","sentences":["Systems of interest for theoretical or experimental work often exhibit high-order interactions, corresponding to statistical interdependencies in groups of variables that cannot be reduced to dependencies in subsets of them.","While still under active development, the framework of partial information decomposition (PID) has emerged as the dominant approach to conceptualise and calculate high-order interdependencies.","PID approaches can be grouped in two types: directed approaches that divide variables into sources and targets, and undirected approaches that treat all variables equally.","Directed and undirected approaches are usually employed to investigate different scenarios, and hence little is known about how these two types of approaches may relate to each other, or if their corresponding quantities are linked in some way.","In this paper we investigate the relationship between the redundancy-synergy index (RSI) and the O-information, which are practical metrics of directed and undirected high-order interdependencies, respectively.","Our results reveal tight links between these two quantities, and provide interpretations of them in terms of likelihood ratios in a hypothesis testing setting, as well as in terms of projections in information geometry."],"url":"http://arxiv.org/abs/2404.07140v1","category":"cs.IT"}
{"created":"2024-04-10 16:13:53","title":"Microbial iron reduction under oxic conditions: implications for subsurface biogeochemistry","abstract":"Iron (Fe) reduction is one of Earth's most ancient microbial metabolisms, but after atmosphere-ocean oxygenation, this anaerobic process was relegated to niche anoxic environments below the water and soil surface. However, new technologies to monitor redox processes at the microscale relevant to microbial cells have recently revealed that the oxygen (O2) concentrations controlling the distribution of aerobic and anaerobic metabolisms are more heterogeneous than previously believed. To explore how O2 levels regulate microbial Fe reduction, we cultivated a facultative Fe-reducing bacterium using a cutting-edge microfluidic reactor integrated with transparent planar O2 sensors. Contrary to expectations, microbial growth induced Fe(III)-oxide (ferrihydrite) reduction under fully oxygenated conditions without forming O2-depleted microsites. Batch incubations highlighted the importance of the process at a larger scale, fundamentally changing our understanding of Fe cycling from the conceptualization of metal and nutrient mobility in the subsurface to our interpretation of Fe mineralogy in the rock record.","sentences":["Iron (Fe) reduction is one of Earth's most ancient microbial metabolisms, but after atmosphere-ocean oxygenation, this anaerobic process was relegated to niche anoxic environments below the water and soil surface.","However, new technologies to monitor redox processes at the microscale relevant to microbial cells have recently revealed that the oxygen (O2) concentrations controlling the distribution of aerobic and anaerobic metabolisms are more heterogeneous than previously believed.","To explore how O2 levels regulate microbial Fe reduction, we cultivated a facultative Fe-reducing bacterium using a cutting-edge microfluidic reactor integrated with transparent planar O2 sensors.","Contrary to expectations, microbial growth induced Fe(III)-oxide (ferrihydrite) reduction under fully oxygenated conditions without forming O2-depleted microsites.","Batch incubations highlighted the importance of the process at a larger scale, fundamentally changing our understanding of Fe cycling from the conceptualization of metal and nutrient mobility in the subsurface to our interpretation of Fe mineralogy in the rock record."],"url":"http://arxiv.org/abs/2404.07137v1","category":"physics.bio-ph"}
{"created":"2024-04-10 16:13:16","title":"To impute or not to? Testing multivariate normality on incomplete dataset: Revisiting the BHEP test","abstract":"In this paper, we focus on testing multivariate normality using the BHEP test with data that are missing completely at random. Our objective is twofold: first, to gain insight into the asymptotic behavior of BHEP test statistics under two widely used approaches for handling missing data, namely complete-case analysis and imputation, and second, to compare the power performance of test statistic under these approaches. It is observed that under the imputation approach, the affine invariance of test statistics is not preserved. To address this issue, we propose an appropriate bootstrap algorithm for approximating p-values. Extensive simulation studies demonstrate that both mean and median approaches exhibit greater power compared to testing with complete-case analysis, and open some questions for further research.","sentences":["In this paper, we focus on testing multivariate normality using the BHEP test with data that are missing completely at random.","Our objective is twofold: first, to gain insight into the asymptotic behavior of BHEP test statistics under two widely used approaches for handling missing data, namely complete-case analysis and imputation, and second, to compare the power performance of test statistic under these approaches.","It is observed that under the imputation approach, the affine invariance of test statistics is not preserved.","To address this issue, we propose an appropriate bootstrap algorithm for approximating p-values.","Extensive simulation studies demonstrate that both mean and median approaches exhibit greater power compared to testing with complete-case analysis, and open some questions for further research."],"url":"http://arxiv.org/abs/2404.07136v1","category":"stat.ME"}
{"created":"2024-04-10 16:10:59","title":"Investigating Ocean Circulation Dynamics Through Data Assimilation: A Mathematical Study Using the Stommel Box Model with Rapid Oscillatory Forcings","abstract":"We investigate ocean circulation changes through the lens of data assimilation using a reduced-order model. Our primary interest lies in the Stommel box model which reveals itself to be one of the most practicable models that has the ability of reproducing the meridional overturning circulation. The Stommel box model has at most two regimes: TH (temperature driven circulation with sinking near the north pole) and SA (salinity driven with sinking near the equator). Currently, the meridional overturning is in the TH regime. Using box-averaged Met Office EN4 ocean temperature and salinity data, our goal is to provide a probability that a future regime change occurs and establish how this probability depends on the uncertainties in initial conditions, parameters and forcings. We will achieve this using data assimilation tools and DAPPER within the Stommel box model with fast oscillatory regimes.","sentences":["We investigate ocean circulation changes through the lens of data assimilation using a reduced-order model.","Our primary interest lies in the Stommel box model which reveals itself to be one of the most practicable models that has the ability of reproducing the meridional overturning circulation.","The Stommel box model has at most two regimes: TH (temperature driven circulation with sinking near the north pole) and SA (salinity driven with sinking near the equator).","Currently, the meridional overturning is in the TH regime.","Using box-averaged Met Office EN4 ocean temperature and salinity data, our goal is to provide a probability that a future regime change occurs and establish how this probability depends on the uncertainties in initial conditions, parameters and forcings.","We will achieve this using data assimilation tools and DAPPER within the Stommel box model with fast oscillatory regimes."],"url":"http://arxiv.org/abs/2404.07134v1","category":"math.DS"}
{"created":"2024-04-10 16:08:13","title":"A conservative Eulerian finite element method for transport and diffusion in moving domains","abstract":"The paper introduces a finite element method for an Eulerian formulation of partial differential equations governing the transport and diffusion of a scalar quantity in a time-dependent domain. The method follows the idea from Lehrenfeld & Olshanskii [ESAIM: M2AN, 53(2): 585-614, 2019] of a solution extension to realise the Eulearian time-stepping scheme. However, a reformulation of the partial differential equation is suggested to derive a scheme which conserves the quantity under consideration exactly on the discrete level. For the spatial discretisation, the paper considers an unfitted finite element method. Ghost-penalty stabilisation is used to release the discrete solution extension and gives a scheme robust against arbitrary intersections between the mesh and geometry interface. The stability is analysed for both first- and second-order backward differentiation formula versions of the scheme. Several numerical examples in two and three spatial dimensions are included to illustrate the potential of this method.","sentences":["The paper introduces a finite element method for an Eulerian formulation of partial differential equations governing the transport and diffusion of a scalar quantity in a time-dependent domain.","The method follows the idea from Lehrenfeld & Olshanskii","[ESAIM: M2AN, 53(2): 585-614, 2019] of a solution extension to realise the Eulearian time-stepping scheme.","However, a reformulation of the partial differential equation is suggested to derive a scheme which conserves the quantity under consideration exactly on the discrete level.","For the spatial discretisation, the paper considers an unfitted finite element method.","Ghost-penalty stabilisation is used to release the discrete solution extension and gives a scheme robust against arbitrary intersections between the mesh and geometry interface.","The stability is analysed for both first- and second-order backward differentiation formula versions of the scheme.","Several numerical examples in two and three spatial dimensions are included to illustrate the potential of this method."],"url":"http://arxiv.org/abs/2404.07130v1","category":"math.NA"}
{"created":"2024-04-10 15:55:07","title":"Continuous Language Model Interpolation for Dynamic and Controllable Text Generation","abstract":"As large language models (LLMs) have gained popularity for a variety of use cases, making them adaptable and controllable has become increasingly important, especially for user-facing applications. While the existing literature on LLM adaptation primarily focuses on finding a model (or models) that optimizes a single predefined objective, here we focus on the challenging case where the model must dynamically adapt to diverse -- and often changing -- user preferences. For this, we leverage adaptation methods based on linear weight interpolation, casting them as continuous multi-domain interpolators that produce models with specific prescribed generation characteristics on-the-fly. Specifically, we use low-rank updates to fine-tune a base model to various different domains, yielding a set of anchor models with distinct generation profiles. Then, we use the weight updates of these anchor models to parametrize the entire (infinite) class of models contained within their convex hull. We empirically show that varying the interpolation weights yields predictable and consistent change in the model outputs with respect to all of the controlled attributes. We find that there is little entanglement between most attributes and identify and discuss the pairs of attributes for which this is not the case. Our results suggest that linearly interpolating between the weights of fine-tuned models facilitates predictable, fine-grained control of model outputs with respect to multiple stylistic characteristics simultaneously.","sentences":["As large language models (LLMs) have gained popularity for a variety of use cases, making them adaptable and controllable has become increasingly important, especially for user-facing applications.","While the existing literature on LLM adaptation primarily focuses on finding a model (or models) that optimizes a single predefined objective, here we focus on the challenging case where the model must dynamically adapt to diverse -- and often changing -- user preferences.","For this, we leverage adaptation methods based on linear weight interpolation, casting them as continuous multi-domain interpolators that produce models with specific prescribed generation characteristics on-the-fly.","Specifically, we use low-rank updates to fine-tune a base model to various different domains, yielding a set of anchor models with distinct generation profiles.","Then, we use the weight updates of these anchor models to parametrize the entire (infinite) class of models contained within their convex hull.","We empirically show that varying the interpolation weights yields predictable and consistent change in the model outputs with respect to all of the controlled attributes.","We find that there is little entanglement between most attributes and identify and discuss the pairs of attributes for which this is not the case.","Our results suggest that linearly interpolating between the weights of fine-tuned models facilitates predictable, fine-grained control of model outputs with respect to multiple stylistic characteristics simultaneously."],"url":"http://arxiv.org/abs/2404.07117v1","category":"cs.CL"}
{"created":"2024-04-10 15:46:21","title":"Coupling Molecular Density Functional Theory with Converged Selected Configuration Interaction Methods to Study Excited states in Aqueous Solution","abstract":"This paper presents the first implementation of a coupling between advanced wave function theories and molecular density functional theory (MDFT). This method enables the modeling of solvent effect into quantum mechanical (QM) calculations by incorporating an electrostatic potential generated by solvent charges into the electronic Hamiltonian. Solvent charges are deduced from the spatially and angularly dependent solvent particle density. Such density is obtained through the minimization of the functional associated to the molecular mechanics (MM) Hamiltonian describing the interaction between the fluid particles. The introduced QM/MDFT framework belongs to QM/MM family of methods but its originality lies in the use of MDFT as the MM solver, offering two main advantages. Firstly, its functional formulation makes it competitive with respect to sampling-based molecular mechanics. Secondly, it preserves a molecular-level description lost in macroscopic continuum approaches. Excited states properties of water and formaldehyde molecules solvated into water have been computed at the selected configuration interaction (SCI) level. Excitation energies and dipole moment have been compared with experimental data and previous theoretical work. A key finding is that using the Hartree-Fock method to describe the solute allows for predicting the solvent charge around the ground-state with sufficient precision for the subsequent SCI calculations of excited-states. This significantly reduces the computational cost of the described procedure, paving the way for the study of more complex molecules.","sentences":["This paper presents the first implementation of a coupling between advanced wave function theories and molecular density functional theory (MDFT).","This method enables the modeling of solvent effect into quantum mechanical (QM) calculations by incorporating an electrostatic potential generated by solvent charges into the electronic Hamiltonian.","Solvent charges are deduced from the spatially and angularly dependent solvent particle density.","Such density is obtained through the minimization of the functional associated to the molecular mechanics (MM) Hamiltonian describing the interaction between the fluid particles.","The introduced QM/MDFT framework belongs to QM/MM family of methods but its originality lies in the use of MDFT as the MM solver, offering two main advantages.","Firstly, its functional formulation makes it competitive with respect to sampling-based molecular mechanics.","Secondly, it preserves a molecular-level description lost in macroscopic continuum approaches.","Excited states properties of water and formaldehyde molecules solvated into water have been computed at the selected configuration interaction (SCI) level.","Excitation energies and dipole moment have been compared with experimental data and previous theoretical work.","A key finding is that using the Hartree-Fock method to describe the solute allows for predicting the solvent charge around the ground-state with sufficient precision for the subsequent SCI calculations of excited-states.","This significantly reduces the computational cost of the described procedure, paving the way for the study of more complex molecules."],"url":"http://arxiv.org/abs/2404.07109v2","category":"physics.chem-ph"}
{"created":"2024-04-10 17:25:42","title":"Self-supervised Monocular Depth Estimation on Water Scenes via Specular Reflection Prior","abstract":"Monocular depth estimation from a single image is an ill-posed problem for computer vision due to insufficient reliable cues as the prior knowledge. Besides the inter-frame supervision, namely stereo and adjacent frames, extensive prior information is available in the same frame. Reflections from specular surfaces, informative intra-frame priors, enable us to reformulate the ill-posed depth estimation task as a multi-view synthesis. This paper proposes the first self-supervision for deep-learning depth estimation on water scenes via intra-frame priors, known as reflection supervision and geometrical constraints. In the first stage, a water segmentation network is performed to separate the reflection components from the entire image. Next, we construct a self-supervised framework to predict the target appearance from reflections, perceived as other perspectives. The photometric re-projection error, incorporating SmoothL1 and a novel photometric adaptive SSIM, is formulated to optimize pose and depth estimation by aligning the transformed virtual depths and source ones. As a supplement, the water surface is determined from real and virtual camera positions, which complement the depth of the water area. Furthermore, to alleviate these laborious ground truth annotations, we introduce a large-scale water reflection scene (WRS) dataset rendered from Unreal Engine 4. Extensive experiments on the WRS dataset prove the feasibility of the proposed method compared to state-of-the-art depth estimation techniques.","sentences":["Monocular depth estimation from a single image is an ill-posed problem for computer vision due to insufficient reliable cues as the prior knowledge.","Besides the inter-frame supervision, namely stereo and adjacent frames, extensive prior information is available in the same frame.","Reflections from specular surfaces, informative intra-frame priors, enable us to reformulate the ill-posed depth estimation task as a multi-view synthesis.","This paper proposes the first self-supervision for deep-learning depth estimation on water scenes via intra-frame priors, known as reflection supervision and geometrical constraints.","In the first stage, a water segmentation network is performed to separate the reflection components from the entire image.","Next, we construct a self-supervised framework to predict the target appearance from reflections, perceived as other perspectives.","The photometric re-projection error, incorporating SmoothL1 and a novel photometric adaptive SSIM, is formulated to optimize pose and depth estimation by aligning the transformed virtual depths and source ones.","As a supplement, the water surface is determined from real and virtual camera positions, which complement the depth of the water area.","Furthermore, to alleviate these laborious ground truth annotations, we introduce a large-scale water reflection scene (WRS) dataset rendered from Unreal Engine 4.","Extensive experiments on the WRS dataset prove the feasibility of the proposed method compared to state-of-the-art depth estimation techniques."],"url":"http://arxiv.org/abs/2404.07176v1","category":"cs.CV"}
{"created":"2024-04-10 16:54:07","title":"Evaluating Navigation and Comparison Performance of Computational Notebooks on Desktop and in Virtual Reality","abstract":"The computational notebook serves as a versatile tool for data analysis. However, its conventional user interface falls short of keeping pace with the ever-growing data-related tasks, signaling the need for novel approaches. With the rapid development of interaction techniques and computing environments, there is a growing interest in integrating emerging technologies in data-driven workflows. Virtual reality, in particular, has demonstrated its potential in interactive data visualizations. In this work, we aimed to experiment with adapting computational notebooks into VR and verify the potential benefits VR can bring. We focus on the navigation and comparison aspects as they are primitive components in analysts' workflow. To further improve comparison, we have designed and implemented a Branching&Merging functionality. We tested computational notebooks on the desktop and in VR, both with and without the added Branching&Merging capability. We found VR significantly facilitated navigation compared to desktop, and the ability to create branches enhanced comparison.","sentences":["The computational notebook serves as a versatile tool for data analysis.","However, its conventional user interface falls short of keeping pace with the ever-growing data-related tasks, signaling the need for novel approaches.","With the rapid development of interaction techniques and computing environments, there is a growing interest in integrating emerging technologies in data-driven workflows.","Virtual reality, in particular, has demonstrated its potential in interactive data visualizations.","In this work, we aimed to experiment with adapting computational notebooks into VR and verify the potential benefits VR can bring.","We focus on the navigation and comparison aspects as they are primitive components in analysts' workflow.","To further improve comparison, we have designed and implemented a Branching&Merging functionality.","We tested computational notebooks on the desktop and in VR, both with and without the added Branching&Merging capability.","We found VR significantly facilitated navigation compared to desktop, and the ability to create branches enhanced comparison."],"url":"http://arxiv.org/abs/2404.07161v1","category":"cs.HC"}
{"created":"2024-04-10 16:44:11","title":"Unified Language-driven Zero-shot Domain Adaptation","abstract":"This paper introduces Unified Language-driven Zero-shot Domain Adaptation (ULDA), a novel task setting that enables a single model to adapt to diverse target domains without explicit domain-ID knowledge. We identify the constraints in the existing language-driven zero-shot domain adaptation task, particularly the requirement for domain IDs and domain-specific models, which may restrict flexibility and scalability. To overcome these issues, we propose a new framework for ULDA, consisting of Hierarchical Context Alignment (HCA), Domain Consistent Representation Learning (DCRL), and Text-Driven Rectifier (TDR). These components work synergistically to align simulated features with target text across multiple visual levels, retain semantic correlations between different regional representations, and rectify biases between simulated and real target visual features, respectively. Our extensive empirical evaluations demonstrate that this framework achieves competitive performance in both settings, surpassing even the model that requires domain-ID, showcasing its superiority and generalization ability. The proposed method is not only effective but also maintains practicality and efficiency, as it does not introduce additional computational costs during inference. Our project page is https://senqiaoyang.com/project/ULDA .","sentences":["This paper introduces Unified Language-driven Zero-shot Domain Adaptation (ULDA), a novel task setting that enables a single model to adapt to diverse target domains without explicit domain-ID knowledge.","We identify the constraints in the existing language-driven zero-shot domain adaptation task, particularly the requirement for domain IDs and domain-specific models, which may restrict flexibility and scalability.","To overcome these issues, we propose a new framework for ULDA, consisting of Hierarchical Context Alignment (HCA), Domain Consistent Representation Learning (DCRL), and Text-Driven Rectifier (TDR).","These components work synergistically to align simulated features with target text across multiple visual levels, retain semantic correlations between different regional representations, and rectify biases between simulated and real target visual features, respectively.","Our extensive empirical evaluations demonstrate that this framework achieves competitive performance in both settings, surpassing even the model that requires domain-ID, showcasing its superiority and generalization ability.","The proposed method is not only effective but also maintains practicality and efficiency, as it does not introduce additional computational costs during inference.","Our project page is https://senqiaoyang.com/project/ULDA ."],"url":"http://arxiv.org/abs/2404.07155v1","category":"cs.CV"}
{"created":"2024-04-10 16:33:55","title":"Adaptive behavior with stable synapses","abstract":"Behavioral changes in animals and humans, as a consequence of an error or a verbal instruction, can be extremely rapid. Improvement in behavioral performances are usually associated in machine learning and reinforcement learning to synaptic plasticity, and, in general, to changes and optimization of network parameters. However, such rapid changes are not coherent with the timescales of synaptic plasticity, suggesting that the mechanism responsible for that could be a dynamical network reconfiguration. In the last few years, similar capabilities have been observed in transformers, foundational architecture in the field of machine learning that are widely used in applications such as natural language and image processing. Transformers are capable of in-context learning, the ability to adapt and acquire new information dynamically within the context of the task or environment they are currently engaged in, without the need for significant changes to their underlying parameters. Building upon the notion of something unique within transformers enabling the emergence of this property, we claim that it could also be supported by input segregation and dendritic amplification, features extensively observed in biological networks. We propose an architecture composed of gain-modulated recurrent networks that excels at in-context learning, showing abilities inaccessible to standard networks. We argue that such a framework can describe the psychometry of context-dependent tasks on humans and other species, solving the incoherence of plasticity timescales. When the context is changed, the network is dynamically reconfigured, and the predicted output undergoes dynamic updates until it aligns with the information embedded in the context.","sentences":["Behavioral changes in animals and humans, as a consequence of an error or a verbal instruction, can be extremely rapid.","Improvement in behavioral performances are usually associated in machine learning and reinforcement learning to synaptic plasticity, and, in general, to changes and optimization of network parameters.","However, such rapid changes are not coherent with the timescales of synaptic plasticity, suggesting that the mechanism responsible for that could be a dynamical network reconfiguration.","In the last few years, similar capabilities have been observed in transformers, foundational architecture in the field of machine learning that are widely used in applications such as natural language and image processing.","Transformers are capable of in-context learning, the ability to adapt and acquire new information dynamically within the context of the task or environment they are currently engaged in, without the need for significant changes to their underlying parameters.","Building upon the notion of something unique within transformers enabling the emergence of this property, we claim that it could also be supported by input segregation and dendritic amplification, features extensively observed in biological networks.","We propose an architecture composed of gain-modulated recurrent networks that excels at in-context learning, showing abilities inaccessible to standard networks.","We argue that such a framework can describe the psychometry of context-dependent tasks on humans and other species, solving the incoherence of plasticity timescales.","When the context is changed, the network is dynamically reconfigured, and the predicted output undergoes dynamic updates until it aligns with the information embedded in the context."],"url":"http://arxiv.org/abs/2404.07150v1","category":"q-bio.NC"}
{"created":"2024-04-10 16:05:37","title":"Iterative solvers in adaptive FEM","abstract":"This chapter provides an overview of state-of-the-art adaptive finite element methods (AFEMs) for the numerical solution of second-order elliptic partial differential equations (PDEs), where the primary focus is on the optimal interplay of local mesh refinement and iterative solution of the arising discrete systems. Particular emphasis is placed on the thorough description of the essential ingredients necessary to design adaptive algorithms of optimal complexity, i.e., algorithms that mathematically guarantee the optimal rate of convergence with respect to the overall computational cost and, hence, time. Crucially, adaptivity induces reliability of the computed numerical approximations by means of a-posteriori error control. This ensures that the error committed by the numerical scheme is bounded from above by computable quantities. The analysis of the adaptive algorithms is based on the study of appropriate quasi-error quantities that include and balance different components of the overall error. Importantly, the quasi-errors stemming from an adaptive algorithm with contractive iterative solver satisfy a centerpiece concept, namely, full R-linear convergence. This guarantees that the adaptive algorithm is essentially contracting this quasi-error at each step and it turns out to be the cornerstone for the optimal complexity of AFEM. The unified analysis of the adaptive algorithms is presented in the context of symmetric linear PDEs. Extensions to goal-oriented, non-symmetric, as well as non-linear PDEs are presented with suitable nested iterative solvers fitting into the general analytical framework of the linear symmetric case. Numerical experiments highlight the theoretical results and emphasize the practical relevance and gain of adaptivity with iterative solvers for numerical simulations with optimal complexity.","sentences":["This chapter provides an overview of state-of-the-art adaptive finite element methods (AFEMs) for the numerical solution of second-order elliptic partial differential equations (PDEs), where the primary focus is on the optimal interplay of local mesh refinement and iterative solution of the arising discrete systems.","Particular emphasis is placed on the thorough description of the essential ingredients necessary to design adaptive algorithms of optimal complexity, i.e., algorithms that mathematically guarantee the optimal rate of convergence with respect to the overall computational cost and, hence, time.","Crucially, adaptivity induces reliability of the computed numerical approximations by means of a-posteriori error control.","This ensures that the error committed by the numerical scheme is bounded from above by computable quantities.","The analysis of the adaptive algorithms is based on the study of appropriate quasi-error quantities that include and balance different components of the overall error.","Importantly, the quasi-errors stemming from an adaptive algorithm with contractive iterative solver satisfy a centerpiece concept, namely, full R-linear convergence.","This guarantees that the adaptive algorithm is essentially contracting this quasi-error at each step and it turns out to be the cornerstone for the optimal complexity of AFEM.","The unified analysis of the adaptive algorithms is presented in the context of symmetric linear PDEs.","Extensions to goal-oriented, non-symmetric, as well as non-linear PDEs are presented with suitable nested iterative solvers fitting into the general analytical framework of the linear symmetric case.","Numerical experiments highlight the theoretical results and emphasize the practical relevance and gain of adaptivity with iterative solvers for numerical simulations with optimal complexity."],"url":"http://arxiv.org/abs/2404.07126v1","category":"math.NA"}
{"created":"2024-04-10 15:51:46","title":"Unfolding ADMM for Enhanced Subspace Clustering of Hyperspectral Images","abstract":"Deep subspace clustering methods are now prominent in clustering, typically using fully connected networks and a self-representation loss function. However, these methods often struggle with overfitting and lack interpretability. In this paper, we explore an alternative clustering approach based on deep unfolding. By unfolding iterative optimization methods into neural networks, this approach offers enhanced interpretability and reliability compared to data-driven deep learning methods, and greater adaptability and generalization than model-based approaches. Hence, unfolding has become widely used in inverse imaging problems, such as image restoration, reconstruction, and super-resolution, but has not been sufficiently explored yet in the context of clustering. In this work, we introduce an innovative clustering architecture for hyperspectral images (HSI) by unfolding an iterative solver based on the Alternating Direction Method of Multipliers (ADMM) for sparse subspace clustering. To our knowledge, this is the first attempt to apply unfolding ADMM for computing the self-representation matrix in subspace clustering. Moreover, our approach captures well the structural characteristics of HSI data by employing the K nearest neighbors algorithm as part of a structure preservation module. Experimental evaluation of three established HSI datasets shows clearly the potential of the unfolding approach in HSI clustering and even demonstrates superior performance compared to state-of-the-art techniques.","sentences":["Deep subspace clustering methods are now prominent in clustering, typically using fully connected networks and a self-representation loss function.","However, these methods often struggle with overfitting and lack interpretability.","In this paper, we explore an alternative clustering approach based on deep unfolding.","By unfolding iterative optimization methods into neural networks, this approach offers enhanced interpretability and reliability compared to data-driven deep learning methods, and greater adaptability and generalization than model-based approaches.","Hence, unfolding has become widely used in inverse imaging problems, such as image restoration, reconstruction, and super-resolution, but has not been sufficiently explored yet in the context of clustering.","In this work, we introduce an innovative clustering architecture for hyperspectral images (HSI) by unfolding an iterative solver based on the Alternating Direction Method of Multipliers (ADMM) for sparse subspace clustering.","To our knowledge, this is the first attempt to apply unfolding ADMM for computing the self-representation matrix in subspace clustering.","Moreover, our approach captures well the structural characteristics of HSI data by employing the K nearest neighbors algorithm as part of a structure preservation module.","Experimental evaluation of three established HSI datasets shows clearly the potential of the unfolding approach in HSI clustering and even demonstrates superior performance compared to state-of-the-art techniques."],"url":"http://arxiv.org/abs/2404.07112v1","category":"cs.CV"}
{"created":"2024-04-10 15:47:35","title":"Wild Visual Navigation: Fast Traversability Learning via Pre-Trained Models and Online Self-Supervision","abstract":"Natural environments such as forests and grasslands are challenging for robotic navigation because of the false perception of rigid obstacles from high grass, twigs, or bushes. In this work, we present Wild Visual Navigation (WVN), an online self-supervised learning system for visual traversability estimation. The system is able to continuously adapt from a short human demonstration in the field, only using onboard sensing and computing. One of the key ideas to achieve this is the use of high-dimensional features from pre-trained self-supervised models, which implicitly encode semantic information that massively simplifies the learning task. Further, the development of an online scheme for supervision generator enables concurrent training and inference of the learned model in the wild. We demonstrate our approach through diverse real-world deployments in forests, parks, and grasslands. Our system is able to bootstrap the traversable terrain segmentation in less than 5 min of in-field training time, enabling the robot to navigate in complex, previously unseen outdoor terrains. Code: https://bit.ly/498b0CV - Project page:https://bit.ly/3M6nMHH","sentences":["Natural environments such as forests and grasslands are challenging for robotic navigation because of the false perception of rigid obstacles from high grass, twigs, or bushes.","In this work, we present Wild Visual Navigation (WVN), an online self-supervised learning system for visual traversability estimation.","The system is able to continuously adapt from a short human demonstration in the field, only using onboard sensing and computing.","One of the key ideas to achieve this is the use of high-dimensional features from pre-trained self-supervised models, which implicitly encode semantic information that massively simplifies the learning task.","Further, the development of an online scheme for supervision generator enables concurrent training and inference of the learned model in the wild.","We demonstrate our approach through diverse real-world deployments in forests, parks, and grasslands.","Our system is able to bootstrap the traversable terrain segmentation in less than 5 min of in-field training time, enabling the robot to navigate in complex, previously unseen outdoor terrains.","Code: https://bit.ly/498b0CV - Project page:https://bit.ly/3M6nMHH"],"url":"http://arxiv.org/abs/2404.07110v1","category":"cs.RO"}
{"created":"2024-04-10 15:45:50","title":"Entanglement distribution through separable states via a zero-added-loss photon multiplexing inspired protocol","abstract":"The recently proposed zero-added-loss multiplexing (ZALM) source of entangled photons enables higher efficiency in entanglement distribution than SPDC sources and can be carried out using both space-to-ground and ground-to-ground links. We demonstrate the flexibility of ZALM architectures to be adapted to alternative entanglement distribution protocols. Focusing on the counter-intuitive result that entanglement can be generated between distant parties without using any entanglement as a resource, we analyze two protocols for entanglement distribution to memories via separable states. Modelling them in a ZALM setup, we consider the effects of noise both in the communication channels and in the memories. We thereby identify the optimal protocol to use, with respect to the highest entanglement generated, given the noise conditions of the network.","sentences":["The recently proposed zero-added-loss multiplexing (ZALM) source of entangled photons enables higher efficiency in entanglement distribution than SPDC sources and can be carried out using both space-to-ground and ground-to-ground links.","We demonstrate the flexibility of ZALM architectures to be adapted to alternative entanglement distribution protocols.","Focusing on the counter-intuitive result that entanglement can be generated between distant parties without using any entanglement as a resource, we analyze two protocols for entanglement distribution to memories via separable states.","Modelling them in a ZALM setup, we consider the effects of noise both in the communication channels and in the memories.","We thereby identify the optimal protocol to use, with respect to the highest entanglement generated, given the noise conditions of the network."],"url":"http://arxiv.org/abs/2404.07107v1","category":"quant-ph"}
{"created":"2024-04-10 15:38:02","title":"Unraveling Consumer Purchase Journey Using Neural Network Models","abstract":"This study utilizes an ensemble of feedforward neural network models to analyze large-volume and high-dimensional consumer touchpoints and their impact on purchase decisions. When applied to a proprietary dataset of consumer touchpoints and purchases from a global software service provider, the proposed approach demonstrates better predictive accuracy than both traditional models, such as logistic regression, naive Bayes, and k-nearest neighbors, as well as ensemble tree-based classifiers, such as bagging, random forest, AdaBoost, and gradient boosting. By calculating the Shapley values within this network, we provide nuanced insights into touchpoint effectiveness, as we not only assess the marginal impact of diverse touchpoint types but also offer a granular view of the impact distribution within a touchpoint type. Additionally, our model shows excellent adaptability and resilience with limited data resources. When the historical data is reduced from 40 to 1 month, our model shows only a modest 19% decrease in accuracy. This modeling framework can enable managers to more accurately and comprehensively evaluate consumer touchpoints, thereby enhancing the effectiveness and efficiency of their marketing campaigns.","sentences":["This study utilizes an ensemble of feedforward neural network models to analyze large-volume and high-dimensional consumer touchpoints and their impact on purchase decisions.","When applied to a proprietary dataset of consumer touchpoints and purchases from a global software service provider, the proposed approach demonstrates better predictive accuracy than both traditional models, such as logistic regression, naive Bayes, and k-nearest neighbors, as well as ensemble tree-based classifiers, such as bagging, random forest, AdaBoost, and gradient boosting.","By calculating the Shapley values within this network, we provide nuanced insights into touchpoint effectiveness, as we not only assess the marginal impact of diverse touchpoint types but also offer a granular view of the impact distribution within a touchpoint type.","Additionally, our model shows excellent adaptability and resilience with limited data resources.","When the historical data is reduced from 40 to 1 month, our model shows only a modest 19% decrease in accuracy.","This modeling framework can enable managers to more accurately and comprehensively evaluate consumer touchpoints, thereby enhancing the effectiveness and efficiency of their marketing campaigns."],"url":"http://arxiv.org/abs/2404.07098v1","category":"stat.AP"}
{"created":"2024-04-10 15:34:10","title":"MoCap-to-Visual Domain Adaptation for Efficient Human Mesh Estimation from 2D Keypoints","abstract":"This paper presents Key2Mesh, a model that takes a set of 2D human pose keypoints as input and estimates the corresponding body mesh. Since this process does not involve any visual (i.e. RGB image) data, the model can be trained on large-scale motion capture (MoCap) datasets, thereby overcoming the scarcity of image datasets with 3D labels. To enable the model's application on RGB images, we first run an off-the-shelf 2D pose estimator to obtain the 2D keypoints, and then feed these 2D keypoints to Key2Mesh. To improve the performance of our model on RGB images, we apply an adversarial domain adaptation (DA) method to bridge the gap between the MoCap and visual domains. Crucially, our DA method does not require 3D labels for visual data, which enables adaptation to target sets without the need for costly labels. We evaluate Key2Mesh for the task of estimating 3D human meshes from 2D keypoints, in the absence of RGB and mesh label pairs. Our results on widely used H3.6M and 3DPW datasets show that Key2Mesh sets the new state-of-the-art by outperforming other models in PA-MPJPE for both datasets, and in MPJPE and PVE for the 3DPW dataset. Thanks to our model's simple architecture, it operates at least 12x faster than the prior state-of-the-art model, LGD. Additional qualitative samples and code are available on the project website: https://key2mesh.github.io/.","sentences":["This paper presents Key2Mesh, a model that takes a set of 2D human pose keypoints as input and estimates the corresponding body mesh.","Since this process does not involve any visual (i.e. RGB image) data, the model can be trained on large-scale motion capture (MoCap) datasets, thereby overcoming the scarcity of image datasets with 3D labels.","To enable the model's application on RGB images, we first run an off-the-shelf 2D pose estimator to obtain the 2D keypoints, and then feed these 2D keypoints to Key2Mesh.","To improve the performance of our model on RGB images, we apply an adversarial domain adaptation (DA) method to bridge the gap between the MoCap and visual domains.","Crucially, our DA method does not require 3D labels for visual data, which enables adaptation to target sets without the need for costly labels.","We evaluate Key2Mesh for the task of estimating 3D human meshes from 2D keypoints, in the absence of RGB and mesh label pairs.","Our results on widely used H3.6M and 3DPW datasets show that Key2Mesh sets the new state-of-the-art by outperforming other models in PA-MPJPE for both datasets, and in MPJPE and PVE for the 3DPW dataset.","Thanks to our model's simple architecture, it operates at least 12x faster than the prior state-of-the-art model, LGD.","Additional qualitative samples and code are available on the project website: https://key2mesh.github.io/."],"url":"http://arxiv.org/abs/2404.07094v1","category":"cs.CV"}
{"created":"2024-04-10 15:18:13","title":"An efficient tidal dissipation mechanism via stellar magnetic fields","abstract":"Recent work suggests that inwardly propagating internal gravity waves (IGWs) within a star can be fully converted to outward magnetic waves (MWs) if they encounter a sufficiently strong magnetic field. The resulting magnetic waves dissipate as they propagate outward to regions with lower Alfv\\'{e}n velocity. While tidal forcing is known to excite IGWs, this conversion and subsequent damping of magnetic waves has not been explored as a tidal dissipation mechanism. In particular, stars with sufficiently strong magnetic fields could fully dissipate tidally excited waves, yielding the same tidal evolution as the previously-studied ``travelling wave regime''. Here, we evaluate the viability of this mechanism using stellar models of stars with convective cores (F-type stars in the mass range of $1.2$-$1.6M_\\odot$) which were previously thought to be weakly tidally dissipative (due to the absence of nonlinear gravity wave breaking). The criterion for wave conversion to operate is evaluated for each stellar mass using the properties of each star's interior along with estimates of the magnetic field produced by a convective core dynamo under the assumption of equipartition between kinetic (convective) and magnetic energies. Our main result is that this previously unexplored source of efficient tidal dissipation can operate in stars within this mass range for significant fractions of their lifetimes. This tidal dissipation mechanism appears to be consistent with the observed inspiral of WASP-12b, and more generally could play an important role in the orbital evolution of hot Jupiters -- and to lower mass ultra-short period planets -- orbiting F-type stars.","sentences":["Recent work suggests that inwardly propagating internal gravity waves (IGWs) within a star can be fully converted to outward magnetic waves (MWs) if they encounter a sufficiently strong magnetic field.","The resulting magnetic waves dissipate as they propagate outward to regions with lower Alfv\\'{e}n velocity.","While tidal forcing is known to excite IGWs, this conversion and subsequent damping of magnetic waves has not been explored as a tidal dissipation mechanism.","In particular, stars with sufficiently strong magnetic fields could fully dissipate tidally excited waves, yielding the same tidal evolution as the previously-studied ``travelling wave regime''.","Here, we evaluate the viability of this mechanism using stellar models of stars with convective cores (F-type stars in the mass range of $1.2$-$1.6M_\\odot$) which were previously thought to be weakly tidally dissipative (due to the absence of nonlinear gravity wave breaking).","The criterion for wave conversion to operate is evaluated for each stellar mass using the properties of each star's interior along with estimates of the magnetic field produced by a convective core dynamo under the assumption of equipartition between kinetic (convective) and magnetic energies.","Our main result is that this previously unexplored source of efficient tidal dissipation can operate in stars within this mass range for significant fractions of their lifetimes.","This tidal dissipation mechanism appears to be consistent with the observed inspiral of WASP-12b, and more generally could play an important role in the orbital evolution of hot Jupiters -- and to lower mass ultra-short period planets -- orbiting F-type stars."],"url":"http://arxiv.org/abs/2404.07085v1","category":"astro-ph.SR"}
{"created":"2024-04-10 15:02:26","title":"Implicit Multi-Spectral Transformer: An Lightweight and Effective Visible to Infrared Image Translation Model","abstract":"In the field of computer vision, visible light images often exhibit low contrast in low-light conditions, presenting a significant challenge. While infrared imagery provides a potential solution, its utilization entails high costs and practical limitations. Recent advancements in deep learning, particularly the deployment of Generative Adversarial Networks (GANs), have facilitated the transformation of visible light images to infrared images. However, these methods often experience unstable training phases and may produce suboptimal outputs. To address these issues, we propose a novel end-to-end Transformer-based model that efficiently converts visible light images into high-fidelity infrared images. Initially, the Texture Mapping Module and Color Perception Adapter collaborate to extract texture and color features from the visible light image. The Dynamic Fusion Aggregation Module subsequently integrates these features. Finally, the transformation into an infrared image is refined through the synergistic action of the Color Perception Adapter and the Enhanced Perception Attention mechanism. Comprehensive benchmarking experiments confirm that our model outperforms existing methods, producing infrared images of markedly superior quality, both qualitatively and quantitatively. Furthermore, the proposed model enables more effective downstream applications for infrared images than other methods.","sentences":["In the field of computer vision, visible light images often exhibit low contrast in low-light conditions, presenting a significant challenge.","While infrared imagery provides a potential solution, its utilization entails high costs and practical limitations.","Recent advancements in deep learning, particularly the deployment of Generative Adversarial Networks (GANs), have facilitated the transformation of visible light images to infrared images.","However, these methods often experience unstable training phases and may produce suboptimal outputs.","To address these issues, we propose a novel end-to-end Transformer-based model that efficiently converts visible light images into high-fidelity infrared images.","Initially, the Texture Mapping Module and Color Perception Adapter collaborate to extract texture and color features from the visible light image.","The Dynamic Fusion Aggregation Module subsequently integrates these features.","Finally, the transformation into an infrared image is refined through the synergistic action of the Color Perception Adapter and the Enhanced Perception Attention mechanism.","Comprehensive benchmarking experiments confirm that our model outperforms existing methods, producing infrared images of markedly superior quality, both qualitatively and quantitatively.","Furthermore, the proposed model enables more effective downstream applications for infrared images than other methods."],"url":"http://arxiv.org/abs/2404.07072v1","category":"cs.CV"}
{"created":"2024-04-10 14:28:09","title":"A Computational Analysis of the Dehumanisation of Migrants from Syria and Ukraine in Slovene News Media","abstract":"Dehumanisation involves the perception and or treatment of a social group's members as less than human. This phenomenon is rarely addressed with computational linguistic techniques. We adapt a recently proposed approach for English, making it easier to transfer to other languages and to evaluate, introducing a new sentiment resource, the use of zero-shot cross-lingual valence and arousal detection, and a new method for statistical significance testing. We then apply it to study attitudes to migration expressed in Slovene newspapers, to examine changes in the Slovene discourse on migration between the 2015-16 migration crisis following the war in Syria and the 2022-23 period following the war in Ukraine. We find that while this discourse became more negative and more intense over time, it is less dehumanising when specifically addressing Ukrainian migrants compared to others.","sentences":["Dehumanisation involves the perception and or treatment of a social group's members as less than human.","This phenomenon is rarely addressed with computational linguistic techniques.","We adapt a recently proposed approach for English, making it easier to transfer to other languages and to evaluate, introducing a new sentiment resource, the use of zero-shot cross-lingual valence and arousal detection, and a new method for statistical significance testing.","We then apply it to study attitudes to migration expressed in Slovene newspapers, to examine changes in the Slovene discourse on migration between the 2015-16 migration crisis following the war in Syria and the 2022-23 period following the war in Ukraine.","We find that while this discourse became more negative and more intense over time, it is less dehumanising when specifically addressing Ukrainian migrants compared to others."],"url":"http://arxiv.org/abs/2404.07036v1","category":"cs.CL"}
{"created":"2024-04-10 14:24:10","title":"ORacle: Large Vision-Language Models for Knowledge-Guided Holistic OR Domain Modeling","abstract":"Every day, countless surgeries are performed worldwide, each within the distinct settings of operating rooms (ORs) that vary not only in their setups but also in the personnel, tools, and equipment used. This inherent diversity poses a substantial challenge for achieving a holistic understanding of the OR, as it requires models to generalize beyond their initial training datasets. To reduce this gap, we introduce ORacle, an advanced vision-language model designed for holistic OR domain modeling, which incorporates multi-view and temporal capabilities and can leverage external knowledge during inference, enabling it to adapt to previously unseen surgical scenarios. This capability is further enhanced by our novel data augmentation framework, which significantly diversifies the training dataset, ensuring ORacle's proficiency in applying the provided knowledge effectively. In rigorous testing, in scene graph generation, and downstream tasks on the 4D-OR dataset, ORacle not only demonstrates state-of-the-art performance but does so requiring less data than existing models. Furthermore, its adaptability is displayed through its ability to interpret unseen views, actions, and appearances of tools and equipment. This demonstrates ORacle's potential to significantly enhance the scalability and affordability of OR domain modeling and opens a pathway for future advancements in surgical data science. We will release our code and data upon acceptance.","sentences":["Every day, countless surgeries are performed worldwide, each within the distinct settings of operating rooms (ORs) that vary not only in their setups but also in the personnel, tools, and equipment used.","This inherent diversity poses a substantial challenge for achieving a holistic understanding of the OR, as it requires models to generalize beyond their initial training datasets.","To reduce this gap, we introduce ORacle, an advanced vision-language model designed for holistic OR domain modeling, which incorporates multi-view and temporal capabilities and can leverage external knowledge during inference, enabling it to adapt to previously unseen surgical scenarios.","This capability is further enhanced by our novel data augmentation framework, which significantly diversifies the training dataset, ensuring ORacle's proficiency in applying the provided knowledge effectively.","In rigorous testing, in scene graph generation, and downstream tasks on the 4D-OR dataset, ORacle not only demonstrates state-of-the-art performance but does so requiring less data than existing models.","Furthermore, its adaptability is displayed through its ability to interpret unseen views, actions, and appearances of tools and equipment.","This demonstrates ORacle's potential to significantly enhance the scalability and affordability of OR domain modeling and opens a pathway for future advancements in surgical data science.","We will release our code and data upon acceptance."],"url":"http://arxiv.org/abs/2404.07031v1","category":"cs.CV"}
{"created":"2024-04-10 13:49:20","title":"Multi-Agent Soft Actor-Critic with Global Loss for Autonomous Mobility-on-Demand Fleet Control","abstract":"We study a sequential decision-making problem for a profit-maximizing operator of an Autonomous Mobility-on-Demand system. Optimizing a central operator's vehicle-to-request dispatching policy requires efficient and effective fleet control strategies. To this end, we employ a multi-agent Soft Actor-Critic algorithm combined with weighted bipartite matching. We propose a novel vehicle-based algorithm architecture and adapt the critic's loss function to appropriately consider global actions. Furthermore, we extend our algorithm to incorporate rebalancing capabilities. Through numerical experiments, we show that our approach outperforms state-of-the-art benchmarks by up to 12.9% for dispatching and up to 38.9% with integrated rebalancing.","sentences":["We study a sequential decision-making problem for a profit-maximizing operator of an Autonomous Mobility-on-Demand system.","Optimizing a central operator's vehicle-to-request dispatching policy requires efficient and effective fleet control strategies.","To this end, we employ a multi-agent Soft Actor-Critic algorithm combined with weighted bipartite matching.","We propose a novel vehicle-based algorithm architecture and adapt the critic's loss function to appropriately consider global actions.","Furthermore, we extend our algorithm to incorporate rebalancing capabilities.","Through numerical experiments, we show that our approach outperforms state-of-the-art benchmarks by up to 12.9% for dispatching and up to 38.9% with integrated rebalancing."],"url":"http://arxiv.org/abs/2404.06975v1","category":"eess.SY"}
{"created":"2024-04-10 13:35:48","title":"An adaptive acceleration scheme for phase-field fatigue computations","abstract":"Phase-field models of fatigue are capable of reproducing the main phenomenology of fatigue behavior. However, phase-field computations in the high-cycle fatigue regime are prohibitively expensive, due to the need to resolve spatially the small length scale inherent to phase-field models and temporally the loading history for several millions of cycles. As a remedy, we propose a fully adaptive acceleration scheme based on the cycle jump technique, where the cycle-by-cycle resolution of an appropriately determined number of cycles is skipped while predicting the local system evolution during the jump. The novelty of our approach is a cycle-jump criterion to determine the appropriate cycle-jump size based on a target increment of a global variable which monitors the advancement of fatigue. We propose the definition and meaning of this variable for three general stages of the fatigue life. In comparison to existing acceleration techniques, our approach needs no parameters and bounds for the cycle-jump size, and it works independently of the material, specimen or loading conditions. Since one of the monitoring variables is the fatigue crack length, we introduce an accurate, flexible and efficient method for its computation, which overcomes the issues of conventional crack tip tracking algorithms and enables the consideration of several cracks evolving at the same time. The performance of the proposed acceleration scheme is demonstrated with representative numerical examples, which show a speedup reaching four orders of magnitude in the high-cycle fatigue regime with consistently high accuracy.","sentences":["Phase-field models of fatigue are capable of reproducing the main phenomenology of fatigue behavior.","However, phase-field computations in the high-cycle fatigue regime are prohibitively expensive, due to the need to resolve spatially the small length scale inherent to phase-field models and temporally the loading history for several millions of cycles.","As a remedy, we propose a fully adaptive acceleration scheme based on the cycle jump technique, where the cycle-by-cycle resolution of an appropriately determined number of cycles is skipped while predicting the local system evolution during the jump.","The novelty of our approach is a cycle-jump criterion to determine the appropriate cycle-jump size based on a target increment of a global variable which monitors the advancement of fatigue.","We propose the definition and meaning of this variable for three general stages of the fatigue life.","In comparison to existing acceleration techniques, our approach needs no parameters and bounds for the cycle-jump size, and it works independently of the material, specimen or loading conditions.","Since one of the monitoring variables is the fatigue crack length, we introduce an accurate, flexible and efficient method for its computation, which overcomes the issues of conventional crack tip tracking algorithms and enables the consideration of several cracks evolving at the same time.","The performance of the proposed acceleration scheme is demonstrated with representative numerical examples, which show a speedup reaching four orders of magnitude in the high-cycle fatigue regime with consistently high accuracy."],"url":"http://arxiv.org/abs/2404.07003v1","category":"cs.CE"}
{"created":"2024-04-10 12:58:23","title":"Adaptive Strategy of Testing Alphas in High Dimensional Linear Factor Pricing Models","abstract":"In recent years, there has been considerable research on testing alphas in high-dimensional linear factor pricing models. In our study, we introduce a novel max-type test procedure that performs well under sparse alternatives. Furthermore, we demonstrate that this new max-type test procedure is asymptotically independent from the sum-type test procedure proposed by Pesaran and Yamagata (2017). Building on this, we propose a Fisher combination test procedure that exhibits good performance for both dense and sparse alternatives.","sentences":["In recent years, there has been considerable research on testing alphas in high-dimensional linear factor pricing models.","In our study, we introduce a novel max-type test procedure that performs well under sparse alternatives.","Furthermore, we demonstrate that this new max-type test procedure is asymptotically independent from the sum-type test procedure proposed by Pesaran and Yamagata (2017).","Building on this, we propose a Fisher combination test procedure that exhibits good performance for both dense and sparse alternatives."],"url":"http://arxiv.org/abs/2404.06984v1","category":"stat.ME"}
{"created":"2024-04-10 11:55:55","title":"The fluid dynamics of a viscoelastic fluid dripping onto a substrate","abstract":"Extensional flows of complex fluids are pivotal in industrial applications like spraying, atomisation, and microfluidic drop deposition. The Dripping-on-Substrate (DoS) technique is a conceptually simple, but dynamically-complex, probe of the extensional rheology of low-viscosity, non-Newtonian fluids. DoS involves capillary-driven thinning of a liquid bridge formed by a slowly dispensed drop onto a partially-wetting solid substrate. By following the filament thinning and pinch-off, the extensional viscosity and relaxation time can be determined. Importantly, DoS enables measurements for lower viscosity solutions than commercially available capillary break-up extensional rheometers. To understand DoS operation, we employ a computational rheology approach via adaptively-refined, time-dependent axisymmetric simulations using the open-source Eulerian code, \\textit{Basilisk}. The volume-of-fluid technique is used to capture the moving interface, and the log-conformation transformation enables a stable viscoelastic solution. We focus on understanding the roles of surface tension, elasticity, and finite chain extensibility in the Elasto-Capillary (EC) regime. Additionally, we explore perturbative effects of gravity and substrate wettability in setting the evolution of the self-similar thinning and pinch-off dynamics. To illustrate the interplay of these different forces, we construct a simple one-dimensional model capturing the initial thinning rates, balancing inertia and capillarity. This model also describes the structure of the transition region to the nonlinear EC regime, where elastic stresses counteract capillary pressure in the thread as the filament thins toward breakup. Finally, we propose a fitting methodology based on the analytical solutions for FENE-P fluids to enhance accuracy in determining the effective relaxation time for unknown fluids.","sentences":["Extensional flows of complex fluids are pivotal in industrial applications like spraying, atomisation, and microfluidic drop deposition.","The Dripping-on-Substrate (DoS) technique is a conceptually simple, but dynamically-complex, probe of the extensional rheology of low-viscosity, non-Newtonian fluids.","DoS involves capillary-driven thinning of a liquid bridge formed by a slowly dispensed drop onto a partially-wetting solid substrate.","By following the filament thinning and pinch-off, the extensional viscosity and relaxation time can be determined.","Importantly, DoS enables measurements for lower viscosity solutions than commercially available capillary break-up extensional rheometers.","To understand DoS operation, we employ a computational rheology approach via adaptively-refined, time-dependent axisymmetric simulations using the open-source Eulerian code, \\textit{Basilisk}.","The volume-of-fluid technique is used to capture the moving interface, and the log-conformation transformation enables a stable viscoelastic solution.","We focus on understanding the roles of surface tension, elasticity, and finite chain extensibility in the Elasto-Capillary (EC) regime.","Additionally, we explore perturbative effects of gravity and substrate wettability in setting the evolution of the self-similar thinning and pinch-off dynamics.","To illustrate the interplay of these different forces, we construct a simple one-dimensional model capturing the initial thinning rates, balancing inertia and capillarity.","This model also describes the structure of the transition region to the nonlinear EC regime, where elastic stresses counteract capillary pressure in the thread as the filament thins toward breakup.","Finally, we propose a fitting methodology based on the analytical solutions for FENE-P fluids to enhance accuracy in determining the effective relaxation time for unknown fluids."],"url":"http://arxiv.org/abs/2404.06947v1","category":"physics.flu-dyn"}
{"created":"2024-04-10 11:27:06","title":"Efficient Sound Field Reconstruction with Conditional Invertible Neural Networks","abstract":"In this study, we introduce a method for estimating sound fields in reverberant environments using a conditional invertible neural network (CINN). Sound field reconstruction can be hindered by experimental errors, limited spatial data, model mismatches, and long inference times, leading to potentially flawed and prolonged characterizations. Further, the complexity of managing inherent uncertainties often escalates computational demands or is neglected in models. Our approach seeks to balance accuracy and computational efficiency, while incorporating uncertainty estimates to tailor reconstructions to specific needs. By training a CINN with Monte Carlo simulations of random wave fields, our method reduces the dependency on extensive datasets and enables inference from sparse experimental data. The CINN proves versatile at reconstructing Room Impulse Responses (RIRs), by acting either as a likelihood model for maximum a posteriori estimation or as an approximate posterior distribution through amortized Bayesian inference. Compared to traditional Bayesian methods, the CINN achieves similar accuracy with greater efficiency and without requiring its adaptation to distinct sound field conditions.","sentences":["In this study, we introduce a method for estimating sound fields in reverberant environments using a conditional invertible neural network (CINN).","Sound field reconstruction can be hindered by experimental errors, limited spatial data, model mismatches, and long inference times, leading to potentially flawed and prolonged characterizations.","Further, the complexity of managing inherent uncertainties often escalates computational demands or is neglected in models.","Our approach seeks to balance accuracy and computational efficiency, while incorporating uncertainty estimates to tailor reconstructions to specific needs.","By training a CINN with Monte Carlo simulations of random wave fields, our method reduces the dependency on extensive datasets and enables inference from sparse experimental data.","The CINN proves versatile at reconstructing Room Impulse Responses (RIRs), by acting either as a likelihood model for maximum a posteriori estimation or as an approximate posterior distribution through amortized Bayesian inference.","Compared to traditional Bayesian methods, the CINN achieves similar accuracy with greater efficiency and without requiring its adaptation to distinct sound field conditions."],"url":"http://arxiv.org/abs/2404.06928v1","category":"eess.AS"}
{"created":"2024-04-10 11:24:34","title":"Gaussian-LIC: Photo-realistic LiDAR-Inertial-Camera SLAM with 3D Gaussian Splatting","abstract":"We present a real-time LiDAR-Inertial-Camera SLAM system with 3D Gaussian Splatting as the mapping backend. Leveraging robust pose estimates from our LiDAR-Inertial-Camera odometry, Coco-LIC, an incremental photo-realistic mapping system is proposed in this paper. We initialize 3D Gaussians from colorized LiDAR points and optimize them using differentiable rendering powered by 3D Gaussian Splatting. Meticulously designed strategies are employed to incrementally expand the Gaussian map and adaptively control its density, ensuring high-quality mapping with real-time capability. Experiments conducted in diverse scenarios demonstrate the superior performance of our method compared to existing radiance-field-based SLAM systems.","sentences":["We present a real-time LiDAR-Inertial-Camera SLAM system with 3D Gaussian Splatting as the mapping backend.","Leveraging robust pose estimates from our LiDAR-Inertial-Camera odometry, Coco-LIC, an incremental photo-realistic mapping system is proposed in this paper.","We initialize 3D Gaussians from colorized LiDAR points and optimize them using differentiable rendering powered by 3D Gaussian Splatting.","Meticulously designed strategies are employed to incrementally expand the Gaussian map and adaptively control its density, ensuring high-quality mapping with real-time capability.","Experiments conducted in diverse scenarios demonstrate the superior performance of our method compared to existing radiance-field-based SLAM systems."],"url":"http://arxiv.org/abs/2404.06926v1","category":"cs.RO"}
{"created":"2024-04-10 11:06:29","title":"Sparse Global Matching for Video Frame Interpolation with Large Motion","abstract":"Large motion poses a critical challenge in Video Frame Interpolation (VFI) task. Existing methods are often constrained by limited receptive fields, resulting in sub-optimal performance when handling scenarios with large motion. In this paper, we introduce a new pipeline for VFI, which can effectively integrate global-level information to alleviate issues associated with large motion. Specifically, we first estimate a pair of initial intermediate flows using a high-resolution feature map for extracting local details. Then, we incorporate a sparse global matching branch to compensate for flow estimation, which consists of identifying flaws in initial flows and generating sparse flow compensation with a global receptive field. Finally, we adaptively merge the initial flow estimation with global flow compensation, yielding a more accurate intermediate flow. To evaluate the effectiveness of our method in handling large motion, we carefully curate a more challenging subset from commonly used benchmarks. Our method demonstrates the state-of-the-art performance on these VFI subsets with large motion.","sentences":["Large motion poses a critical challenge in Video Frame Interpolation (VFI) task.","Existing methods are often constrained by limited receptive fields, resulting in sub-optimal performance when handling scenarios with large motion.","In this paper, we introduce a new pipeline for VFI, which can effectively integrate global-level information to alleviate issues associated with large motion.","Specifically, we first estimate a pair of initial intermediate flows using a high-resolution feature map for extracting local details.","Then, we incorporate a sparse global matching branch to compensate for flow estimation, which consists of identifying flaws in initial flows and generating sparse flow compensation with a global receptive field.","Finally, we adaptively merge the initial flow estimation with global flow compensation, yielding a more accurate intermediate flow.","To evaluate the effectiveness of our method in handling large motion, we carefully curate a more challenging subset from commonly used benchmarks.","Our method demonstrates the state-of-the-art performance on these VFI subsets with large motion."],"url":"http://arxiv.org/abs/2404.06913v1","category":"cs.CV"}
{"created":"2024-04-10 11:00:48","title":"Twisted Adiabatic Limit for Complex Structures","abstract":"Given a complex manifold $X$ and a smooth positive function $\\eta$ thereon, we perturb the standard differential operator $d=\\partial + \\bar\\partial$ acting on differential forms to a first-order differential operator $D_\\eta$ whose principal part is $\\eta\\partial + \\bar\\partial$. The role of the zero-th order part is to force the integrability property $D_\\eta^2=0$ that leads to a cohomology isomorphic to the de Rham cohomology of $X$, while the components of types $(0,\\,1)$ and $(1,\\,0)$ of $D_\\eta$ induce cohomologies isomorphic to the Dolbeault and conjugate-Dolbeault cohomologies. We compute Bochner-Kodaira-Nakano-type formulae for the Laplacians induced by these operators and a given Hermitian metric on $X$. The computations throw up curvature-like operators of order one that can be made (semi-)positive under appropriate assumptions on the function $\\eta$. As applications, we obtain vanishing results for certain harmonic spaces on complete, non-compact, manifolds and for the Dolbeault cohomology of compact complex manifolds that carry certain types of functions $\\eta$. This study continues and generalises the one of the operators $d_h=h\\partial + \\bar\\partial$ that we introduced and investigated recently for a positive constant $h$ that was then let to converge to $0$ and, more generally, for constants $h\\in\\C$. The operators $d_h$ had, in turn, been adapted to complex structures from the well-known adiabatic limit construction for Riemannian foliations. Allowing now for possibly non-constant functions $\\eta$ creates positivity in the curvature-like operator that stands one in good stead for various kinds of applications.","sentences":["Given a complex manifold $X$ and a smooth positive function $\\eta$ thereon, we perturb the standard differential operator $d=\\partial + \\bar\\partial$ acting on differential forms to a first-order differential operator $D_\\eta$ whose principal part is $\\eta\\partial + \\bar\\partial$.","The role of the zero-th order part is to force the integrability property $D_\\eta^2=0$ that leads to a cohomology isomorphic to the de Rham cohomology of $X$, while the components of types $(0,\\,1)$ and $(1,\\,0)$ of $D_\\eta$ induce cohomologies isomorphic to the Dolbeault and conjugate-Dolbeault cohomologies.","We compute Bochner-Kodaira-Nakano-type formulae for the Laplacians induced by these operators and a given Hermitian metric on $X$. The computations throw up curvature-like operators of order one that can be made (semi-)positive under appropriate assumptions on the function $\\eta$. As applications, we obtain vanishing results for certain harmonic spaces on complete, non-compact, manifolds and for the Dolbeault cohomology of compact complex manifolds that carry certain types of functions $\\eta$.","This study continues and generalises the one of the operators $d_h=h\\partial + \\bar\\partial$ that we introduced and investigated recently for a positive constant $h$ that was then let to converge to $0$ and, more generally, for constants $h\\in\\C$. The operators $d_h$ had, in turn, been adapted to complex structures from the well-known adiabatic limit construction for Riemannian foliations.","Allowing now for possibly non-constant functions $\\eta$ creates positivity in the curvature-like operator that stands one in good stead for various kinds of applications."],"url":"http://arxiv.org/abs/2404.06908v1","category":"math.DG"}
{"created":"2024-04-10 10:57:44","title":"A nano vacuum gauge based on second-order coherence in optical levitation","abstract":"Accurate measurement of pressure with a wide dynamic range holds significant importance for various applications. This issue can be realized with a mechanical nano-oscillator, where the pressure-related collisions with surrounding molecules induce its energy dissipation. However, this energy dissipation of the nano-oscillator may be overshadowed by other processes. Here, we apply the second-order coherence analysis to accurately characterize those distinct dissipation processes. Based on an optically levitated nano-oscillator, we successfully obtain precise measurements of the air pressure surrounding the particles from atmosphere to 7E-6 mbar, over 8 orders of magnitude. It proves that the mechanical nano-oscillator is an extremely promising candidate for precision pressure sensing applications. Moreover, the second-order coherence analysis method on a classical system can pave the way to characterize the dynamic properties of an oscillator, which will benefit microscopic thermodynamics, precision measurement, and macroscopic quantum research.","sentences":["Accurate measurement of pressure with a wide dynamic range holds significant importance for various applications.","This issue can be realized with a mechanical nano-oscillator, where the pressure-related collisions with surrounding molecules induce its energy dissipation.","However, this energy dissipation of the nano-oscillator may be overshadowed by other processes.","Here, we apply the second-order coherence analysis to accurately characterize those distinct dissipation processes.","Based on an optically levitated nano-oscillator, we successfully obtain precise measurements of the air pressure surrounding the particles from atmosphere to 7E-6 mbar, over 8 orders of magnitude.","It proves that the mechanical nano-oscillator is an extremely promising candidate for precision pressure sensing applications.","Moreover, the second-order coherence analysis method on a classical system can pave the way to characterize the dynamic properties of an oscillator, which will benefit microscopic thermodynamics, precision measurement, and macroscopic quantum research."],"url":"http://arxiv.org/abs/2404.06907v1","category":"physics.optics"}
{"created":"2024-04-10 10:32:29","title":"PACP: Priority-Aware Collaborative Perception for Connected and Autonomous Vehicles","abstract":"Surrounding perceptions are quintessential for safe driving for connected and autonomous vehicles (CAVs), where the Bird's Eye View has been employed to accurately capture spatial relationships among vehicles. However, severe inherent limitations of BEV, like blind spots, have been identified. Collaborative perception has emerged as an effective solution to overcoming these limitations through data fusion from multiple views of surrounding vehicles. While most existing collaborative perception strategies adopt a fully connected graph predicated on fairness in transmissions, they often neglect the varying importance of individual vehicles due to channel variations and perception redundancy. To address these challenges, we propose a novel Priority-Aware Collaborative Perception (PACP) framework to employ a BEV-match mechanism to determine the priority levels based on the correlation between nearby CAVs and the ego vehicle for perception. By leveraging submodular optimization, we find near-optimal transmission rates, link connectivity, and compression metrics. Moreover, we deploy a deep learning-based adaptive autoencoder to modulate the image reconstruction quality under dynamic channel conditions. Finally, we conduct extensive studies and demonstrate that our scheme significantly outperforms the state-of-the-art schemes by 8.27% and 13.60%, respectively, in terms of utility and precision of the Intersection over Union.","sentences":["Surrounding perceptions are quintessential for safe driving for connected and autonomous vehicles (CAVs), where the Bird's Eye View has been employed to accurately capture spatial relationships among vehicles.","However, severe inherent limitations of BEV, like blind spots, have been identified.","Collaborative perception has emerged as an effective solution to overcoming these limitations through data fusion from multiple views of surrounding vehicles.","While most existing collaborative perception strategies adopt a fully connected graph predicated on fairness in transmissions, they often neglect the varying importance of individual vehicles due to channel variations and perception redundancy.","To address these challenges, we propose a novel Priority-Aware Collaborative Perception (PACP) framework to employ a BEV-match mechanism to determine the priority levels based on the correlation between nearby CAVs and the ego vehicle for perception.","By leveraging submodular optimization, we find near-optimal transmission rates, link connectivity, and compression metrics.","Moreover, we deploy a deep learning-based adaptive autoencoder to modulate the image reconstruction quality under dynamic channel conditions.","Finally, we conduct extensive studies and demonstrate that our scheme significantly outperforms the state-of-the-art schemes by 8.27% and 13.60%, respectively, in terms of utility and precision of the Intersection over Union."],"url":"http://arxiv.org/abs/2404.06891v1","category":"cs.NI"}
{"created":"2024-04-10 10:31:11","title":"Discrete time crystals in the presence of non-Markovian dynamics","abstract":"We study discrete time crystals (DTCs) in periodically driven quantum systems, in the presence of non-Markovian dissipation. In contrast to DTCs observed in earlier works in the presence of Markovian dynamics, using the open Dicke model in presence of Jaynes-Cummings-like dissipation, we show that non-Markovian regime can be highly beneficial for stabilizing DTCs over a wide range of parameter values. This may be attributed to periodically varying dissipation rates even at long times in the case of non-Markovian dynamics. Further the Markovian and non-Markovian regimes show sharp distinctions for intermediate strengths of the dissipator coefficient, with a time-independent steady-state in the Markovian regime being replaced by varied dynamical phases, including DTC order, in the non-Markovian regime. We also verify the robustness of the DTC phase in the non-Markovian regime by introducing errors both in the Hamiltonian as well as in the dissipation. Our study shows the possibility of using DTC as a probe for non-Markovian dynamics in periodically modulated open quantum systems, at long times.","sentences":["We study discrete time crystals (DTCs) in periodically driven quantum systems, in the presence of non-Markovian dissipation.","In contrast to DTCs observed in earlier works in the presence of Markovian dynamics, using the open Dicke model in presence of Jaynes-Cummings-like dissipation, we show that non-Markovian regime can be highly beneficial for stabilizing DTCs over a wide range of parameter values.","This may be attributed to periodically varying dissipation rates even at long times in the case of non-Markovian dynamics.","Further the Markovian and non-Markovian regimes show sharp distinctions for intermediate strengths of the dissipator coefficient, with a time-independent steady-state in the Markovian regime being replaced by varied dynamical phases, including DTC order, in the non-Markovian regime.","We also verify the robustness of the DTC phase in the non-Markovian regime by introducing errors both in the Hamiltonian as well as in the dissipation.","Our study shows the possibility of using DTC as a probe for non-Markovian dynamics in periodically modulated open quantum systems, at long times."],"url":"http://arxiv.org/abs/2404.06890v1","category":"quant-ph"}
{"created":"2024-04-10 09:43:03","title":"Hydrodynamic simulations of WD-WD mergers and the origin of RCB stars","abstract":"We study the properties of double white dwarf (DWD) mergers by performing hydrodynamic simulations using the new and improved adaptive mesh refinement code Octo-Tiger. We follow the orbital evolution of DWD systems of mass ratio q=0.7 for tens of orbits until and after the merger to investigate them as a possible origin for R Coronae Borealis (RCB) type stars. We reproduce previous results, finding that during the merger, the Helium WD donor star is tidally disrupted within 20-80 minutes since the beginning of the simulation onto the accretor Carbon-Oxygen WD, creating a high temperature shell around the accretor. We investigate the possible Helium burning in this shell and the merged object's general structure. Specifically, we are interested in the amount of Oxygen-16 dredged-up to the hot shell and the amount of Oxygen-18 produced. This is critical as the discovery of very low Oxygen-16 to Oxygen-18 ratios in RCB stars pointed out the merger scenario as a favorable explanation for their origin. A small amount of hydrogen in the donor may help keep the Oxygen-16 to Oxygen-18 ratios within observational bounds, even if moderate dredge-up from the accretor occurs. In addition, we perform a resolution study to reconcile the difference found in the amount of Oxygen-16 dredge-up between smoothed-particle hydrodynamics and grid-based simulations.","sentences":["We study the properties of double white dwarf (DWD) mergers by performing hydrodynamic simulations using the new and improved adaptive mesh refinement code Octo-Tiger.","We follow the orbital evolution of DWD systems of mass ratio q=0.7 for tens of orbits until and after the merger to investigate them as a possible origin for R Coronae Borealis (RCB) type stars.","We reproduce previous results, finding that during the merger, the Helium WD donor star is tidally disrupted within 20-80 minutes since the beginning of the simulation onto the accretor Carbon-Oxygen WD, creating a high temperature shell around the accretor.","We investigate the possible Helium burning in this shell and the merged object's general structure.","Specifically, we are interested in the amount of Oxygen-16 dredged-up to the hot shell and the amount of Oxygen-18 produced.","This is critical as the discovery of very low Oxygen-16 to Oxygen-18 ratios in RCB stars pointed out the merger scenario as a favorable explanation for their origin.","A small amount of hydrogen in the donor may help keep the Oxygen-16 to Oxygen-18 ratios within observational bounds, even if moderate dredge-up from the accretor occurs.","In addition, we perform a resolution study to reconcile the difference found in the amount of Oxygen-16 dredge-up between smoothed-particle hydrodynamics and grid-based simulations."],"url":"http://arxiv.org/abs/2404.06864v1","category":"astro-ph.SR"}
{"created":"2024-04-10 09:05:59","title":"On the radially deformed Fourier transform","abstract":"In this paper we consider the kernel of the radially deformed Fourier transform introduced in the context of Clifford analysis in [10]. By adapting the Laplace transform method from [4], we obtain the Laplace domain expressions of the kernel for the cases of $m=2$ and $m > 2$ when $1+c=\\frac{1}{n}, n\\in \\mathbb{N}_0\\backslash\\{1\\}$ with $n$ odd. Moreover, we show that the expressions can be simplified using the Poisson kernel and the generating function of the Gegenbauer polynomials. As a consequence, the inverse formulas are used to get the integral expressions of the kernel in terms of Mittag-Leffler functions.","sentences":["In this paper we consider the kernel of the radially deformed Fourier transform introduced in the context of Clifford analysis in [10].","By adapting the Laplace transform method from [4], we obtain the Laplace domain expressions of the kernel for the cases of $m=2$ and $m > 2$ when $1+c=\\frac{1}{n}, n\\in \\mathbb{N}_0\\backslash\\{1\\}$ with $n$ odd.","Moreover, we show that the expressions can be simplified using the Poisson kernel and the generating function of the Gegenbauer polynomials.","As a consequence, the inverse formulas are used to get the integral expressions of the kernel in terms of Mittag-Leffler functions."],"url":"http://arxiv.org/abs/2404.06839v1","category":"math.CA"}
{"created":"2024-04-10 08:54:43","title":"O2V-Mapping: Online Open-Vocabulary Mapping with Neural Implicit Representation","abstract":"Online construction of open-ended language scenes is crucial for robotic applications, where open-vocabulary interactive scene understanding is required. Recently, neural implicit representation has provided a promising direction for online interactive mapping. However, implementing open-vocabulary scene understanding capability into online neural implicit mapping still faces three challenges: lack of local scene updating ability, blurry spatial hierarchical semantic segmentation and difficulty in maintaining multi-view consistency. To this end, we proposed O2V-mapping, which utilizes voxel-based language and geometric features to create an open-vocabulary field, thus allowing for local updates during online training process. Additionally, we leverage a foundational model for image segmentation to extract language features on object-level entities, achieving clear segmentation boundaries and hierarchical semantic features. For the purpose of preserving consistency in 3D object properties across different viewpoints, we propose a spatial adaptive voxel adjustment mechanism and a multi-view weight selection method. Extensive experiments on open-vocabulary object localization and semantic segmentation demonstrate that O2V-mapping achieves online construction of language scenes while enhancing accuracy, outperforming the previous SOTA method.","sentences":["Online construction of open-ended language scenes is crucial for robotic applications, where open-vocabulary interactive scene understanding is required.","Recently, neural implicit representation has provided a promising direction for online interactive mapping.","However, implementing open-vocabulary scene understanding capability into online neural implicit mapping still faces three challenges: lack of local scene updating ability, blurry spatial hierarchical semantic segmentation and difficulty in maintaining multi-view consistency.","To this end, we proposed O2V-mapping, which utilizes voxel-based language and geometric features to create an open-vocabulary field, thus allowing for local updates during online training process.","Additionally, we leverage a foundational model for image segmentation to extract language features on object-level entities, achieving clear segmentation boundaries and hierarchical semantic features.","For the purpose of preserving consistency in 3D object properties across different viewpoints, we propose a spatial adaptive voxel adjustment mechanism and a multi-view weight selection method.","Extensive experiments on open-vocabulary object localization and semantic segmentation demonstrate that O2V-mapping achieves online construction of language scenes while enhancing accuracy, outperforming the previous SOTA method."],"url":"http://arxiv.org/abs/2404.06836v1","category":"cs.CV"}
{"created":"2024-04-10 08:54:00","title":"Tuning-Free Adaptive Style Incorporation for Structure-Consistent Text-Driven Style Transfer","abstract":"In this work, we target the task of text-driven style transfer in the context of text-to-image (T2I) diffusion models. The main challenge is consistent structure preservation while enabling effective style transfer effects. The past approaches in this field directly concatenate the content and style prompts for a prompt-level style injection, leading to unavoidable structure distortions. In this work, we propose a novel solution to the text-driven style transfer task, namely, Adaptive Style Incorporation~(ASI), to achieve fine-grained feature-level style incorporation. It consists of the Siamese Cross-Attention~(SiCA) to decouple the single-track cross-attention to a dual-track structure to obtain separate content and style features, and the Adaptive Content-Style Blending (AdaBlending) module to couple the content and style information from a structure-consistent manner. Experimentally, our method exhibits much better performance in both structure preservation and stylized effects.","sentences":["In this work, we target the task of text-driven style transfer in the context of text-to-image (T2I) diffusion models.","The main challenge is consistent structure preservation while enabling effective style transfer effects.","The past approaches in this field directly concatenate the content and style prompts for a prompt-level style injection, leading to unavoidable structure distortions.","In this work, we propose a novel solution to the text-driven style transfer task, namely, Adaptive Style Incorporation~(ASI), to achieve fine-grained feature-level style incorporation.","It consists of the Siamese Cross-Attention~(SiCA) to decouple the single-track cross-attention to a dual-track structure to obtain separate content and style features, and the Adaptive Content-Style Blending (AdaBlending) module to couple the content and style information from a structure-consistent manner.","Experimentally, our method exhibits much better performance in both structure preservation and stylized effects."],"url":"http://arxiv.org/abs/2404.06835v1","category":"cs.CV"}
{"created":"2024-04-10 08:47:57","title":"Optimal Regret with Limited Adaptivity for Generalized Linear Contextual Bandits","abstract":"We study the generalized linear contextual bandit problem within the requirements of limited adaptivity. In this paper, we present two algorithms, B-GLinCB and RS-GLinCB, that address, respectively, two prevalent limited adaptivity models: batch learning with stochastic contexts and rare policy switches with adversarial contexts. For both these models, we establish essentially tight regret bounds. Notably, in the obtained bounds, we manage to eliminate a dependence on a key parameter $\\kappa$, which captures the non-linearity of the underlying reward model. For our batch learning algorithm B-GLinCB, with $\\Omega\\left( \\log{\\log T} \\right)$ batches, the regret scales as $\\tilde{O}(\\sqrt{T})$. Further, we establish that our rarely switching algorithm RS-GLinCB updates its policy at most $\\tilde{O}(\\log^2 T)$ times and achieves a regret of $\\tilde{O}(\\sqrt{T})$. Our approach for removing the dependence on $\\kappa$ for generalized linear contextual bandits might be of independent interest.","sentences":["We study the generalized linear contextual bandit problem within the requirements of limited adaptivity.","In this paper, we present two algorithms, B-GLinCB and RS-GLinCB, that address, respectively, two prevalent limited adaptivity models: batch learning with stochastic contexts and rare policy switches with adversarial contexts.","For both these models, we establish essentially tight regret bounds.","Notably, in the obtained bounds, we manage to eliminate a dependence on a key parameter $\\kappa$, which captures the non-linearity of the underlying reward model.","For our batch learning algorithm B-GLinCB, with $\\Omega\\left( \\log{\\log T} \\right)$ batches, the regret scales as $\\tilde{O}(\\sqrt{T})$. Further, we establish that our rarely switching algorithm RS-GLinCB updates its policy at most $\\tilde{O}(\\log^2 T)$ times and achieves a regret of $\\tilde{O}(\\sqrt{T})$. Our approach for removing the dependence on $\\kappa$ for generalized linear contextual bandits might be of independent interest."],"url":"http://arxiv.org/abs/2404.06831v2","category":"cs.LG"}
{"created":"2024-04-10 08:11:12","title":"Enc2DB: A Hybrid and Adaptive Encrypted Query Processing Framework","abstract":"As cloud computing gains traction, data owners are outsourcing their data to cloud service providers (CSPs) for Database Service (DBaaS), bringing in a deviation of data ownership and usage, and intensifying privacy concerns, especially with potential breaches by hackers or CSP insiders. To address that, encrypted database services propose encrypting every tuple and query statement before submitting to the CSP, ensuring data confidentiality when the CSP is honest-but-curious, or even compromised. Existing solutions either employ property preserving cryptography schemes, which can perform certain operations over ciphertext without decrypting the data over the CSP, or utilize trusted execution environment (TEE) to safeguard data and computations from the CSP. Based on these efforts, we introduce Enc2DB, a novel secure database system, following a hybrid strategy on PostgreSQL and openGauss. We present a micro-benchmarking test and self-adaptive mode switch strategy that can dynamically choose the best execution path (cryptography or TEE) to answer a given query. Besides, we also design and implement a ciphertext index compatible with native cost model and query optimizers to accelerate query processing. Empirical study over TPC-C test justifies that Enc2DB outperforms pure TEE and cryptography solutions, and our ciphertext index implementation also outperforms the state-of-the-art cryptographic-based system.","sentences":["As cloud computing gains traction, data owners are outsourcing their data to cloud service providers (CSPs) for Database Service (DBaaS), bringing in a deviation of data ownership and usage, and intensifying privacy concerns, especially with potential breaches by hackers or CSP insiders.","To address that, encrypted database services propose encrypting every tuple and query statement before submitting to the CSP, ensuring data confidentiality when the CSP is honest-but-curious, or even compromised.","Existing solutions either employ property preserving cryptography schemes, which can perform certain operations over ciphertext without decrypting the data over the CSP, or utilize trusted execution environment (TEE) to safeguard data and computations from the CSP.","Based on these efforts, we introduce Enc2DB, a novel secure database system, following a hybrid strategy on PostgreSQL and openGauss.","We present a micro-benchmarking test and self-adaptive mode switch strategy that can dynamically choose the best execution path (cryptography or TEE) to answer a given query.","Besides, we also design and implement a ciphertext index compatible with native cost model and query optimizers to accelerate query processing.","Empirical study over TPC-C test justifies that Enc2DB outperforms pure TEE and cryptography solutions, and our ciphertext index implementation also outperforms the state-of-the-art cryptographic-based system."],"url":"http://arxiv.org/abs/2404.06819v1","category":"cs.CR"}
{"created":"2024-04-10 08:06:15","title":"Towards Efficient and Real-Time Piano Transcription Using Neural Autoregressive Models","abstract":"In recent years, advancements in neural network designs and the availability of large-scale labeled datasets have led to significant improvements in the accuracy of piano transcription models. However, most previous work focused on high-performance offline transcription, neglecting deliberate consideration of model size. The goal of this work is to implement real-time inference for piano transcription while ensuring both high performance and lightweight. To this end, we propose novel architectures for convolutional recurrent neural networks, redesigning an existing autoregressive piano transcription model. First, we extend the acoustic module by adding a frequency-conditioned FiLM layer to the CNN module to adapt the convolutional filters on the frequency axis. Second, we improve note-state sequence modeling by using a pitchwise LSTM that focuses on note-state transitions within a note. In addition, we augment the autoregressive connection with an enhanced recursive context. Using these components, we propose two types of models; one for high performance and the other for high compactness. Through extensive experiments, we show that the proposed models are comparable to state-of-the-art models in terms of note accuracy on the MAESTRO dataset. We also investigate the effective model size and real-time inference latency by gradually streamlining the architecture. Finally, we conduct cross-data evaluation on unseen piano datasets and in-depth analysis to elucidate the effect of the proposed components in the view of note length and pitch range.","sentences":["In recent years, advancements in neural network designs and the availability of large-scale labeled datasets have led to significant improvements in the accuracy of piano transcription models.","However, most previous work focused on high-performance offline transcription, neglecting deliberate consideration of model size.","The goal of this work is to implement real-time inference for piano transcription while ensuring both high performance and lightweight.","To this end, we propose novel architectures for convolutional recurrent neural networks, redesigning an existing autoregressive piano transcription model.","First, we extend the acoustic module by adding a frequency-conditioned FiLM layer to the CNN module to adapt the convolutional filters on the frequency axis.","Second, we improve note-state sequence modeling by using a pitchwise LSTM that focuses on note-state transitions within a note.","In addition, we augment the autoregressive connection with an enhanced recursive context.","Using these components, we propose two types of models; one for high performance and the other for high compactness.","Through extensive experiments, we show that the proposed models are comparable to state-of-the-art models in terms of note accuracy on the MAESTRO dataset.","We also investigate the effective model size and real-time inference latency by gradually streamlining the architecture.","Finally, we conduct cross-data evaluation on unseen piano datasets and in-depth analysis to elucidate the effect of the proposed components in the view of note length and pitch range."],"url":"http://arxiv.org/abs/2404.06818v1","category":"eess.AS"}
{"created":"2024-04-10 07:04:21","title":"Modeling of antenna-coupled Si MOSFETs in the Terahertz Frequency Range","abstract":"We report on the modeling and experimental characterization of Si CMOS detectors of terahertz radiation based on antenna-coupled field-effect transistors (TeraFETs). The detectors are manufactured using TSMC's 65-nm technology. We apply two models -- the TSMC RF foundry model and our own ADS-HDM -- to simulate the Si CMOS TeraFET performance and compare their predictions with respective experimental data. Both models are implemented in the commercial circuit simulation software Keysight Advanced Design System (ADS). We find that the compact model TSMC RF is capable to predict the detector responsivity and its dependence on frequency and gate voltage with good accuracy up to the highest frequency of 1.2 THz covered in this study. This frequency is well beyond the tool's intended operation range for 5G communications and 110-GHz millimeter wave applications. We demonstrate that our self-developed physics-based ADS-HDM tool, which relies on an extended one-dimensional hydrodynamic transport model and can be adapted readily to other material technologies, has high predictive qualities comparable to those of the foundry model. We use the ADS-HDM to discuss the contribution of diffusive and plasmonic effects to the THz response of Si CMOS TeraFETs, finding that these effects, while becoming more significant with rising frequency, are never dominant. Finally, we estimate that the electrical NEP (perfect power coupling conditions) is on the order of 5 pW/$\\sqrt{\\rm{Hz}}$ at room-temperature.","sentences":["We report on the modeling and experimental characterization of Si CMOS detectors of terahertz radiation based on antenna-coupled field-effect transistors (TeraFETs).","The detectors are manufactured using TSMC's 65-nm technology.","We apply two models -- the TSMC RF foundry model and our own ADS-HDM -- to simulate the Si CMOS TeraFET performance and compare their predictions with respective experimental data.","Both models are implemented in the commercial circuit simulation software Keysight Advanced Design System (ADS).","We find that the compact model TSMC RF is capable to predict the detector responsivity and its dependence on frequency and gate voltage with good accuracy up to the highest frequency of 1.2 THz covered in this study.","This frequency is well beyond the tool's intended operation range for 5G communications and 110-GHz millimeter wave applications.","We demonstrate that our self-developed physics-based ADS-HDM tool, which relies on an extended one-dimensional hydrodynamic transport model and can be adapted readily to other material technologies, has high predictive qualities comparable to those of the foundry model.","We use the ADS-HDM to discuss the contribution of diffusive and plasmonic effects to the THz response of Si CMOS TeraFETs, finding that these effects, while becoming more significant with rising frequency, are never dominant.","Finally, we estimate that the electrical NEP (perfect power coupling conditions) is on the order of 5 pW/$\\sqrt{\\rm{Hz}}$ at room-temperature."],"url":"http://arxiv.org/abs/2404.06790v2","category":"physics.app-ph"}
{"created":"2024-04-10 06:56:04","title":"Fluid Simulation for a Finite Size Plasma","abstract":"Studies on finite-size plasma have attracted a lot of attention lately. They can form by ionizing liquid droplets by lasers. The dynamical behavior of such plasma droplets is, therefore, a topic of significant interest. In particular, questions related to the linear and nonlinear characteristics (associated with the inhomogeneous density typically at the edge of the droplet), the behavior of plasma expansion, etc., are of interest. A one-dimensional fluid simulation study has been carried out to investigate this behavior. It is observed that a slight imbalance in the charge density leads to oscillations that are concentrated and keep acquiring higher amplitude and sharper profile at the inhomogeneous edge region. Such oscillations lead to the expansion of the droplet. Though the fluid description breaks when the sharpness of these structures becomes comparable to the grid size, it provides a reasonable estimate of wave-breaking time. The presence of dissipative effects like diffusion is shown to arrest the sharpness of these structures. The dynamics of these structures in the presence of an externally applied oscillating electric field corresponding to a long wavelength radiation has also been studied.","sentences":["Studies on finite-size plasma have attracted a lot of attention lately.","They can form by ionizing liquid droplets by lasers.","The dynamical behavior of such plasma droplets is, therefore, a topic of significant interest.","In particular, questions related to the linear and nonlinear characteristics (associated with the inhomogeneous density typically at the edge of the droplet), the behavior of plasma expansion, etc., are of interest.","A one-dimensional fluid simulation study has been carried out to investigate this behavior.","It is observed that a slight imbalance in the charge density leads to oscillations that are concentrated and keep acquiring higher amplitude and sharper profile at the inhomogeneous edge region.","Such oscillations lead to the expansion of the droplet.","Though the fluid description breaks when the sharpness of these structures becomes comparable to the grid size, it provides a reasonable estimate of wave-breaking time.","The presence of dissipative effects like diffusion is shown to arrest the sharpness of these structures.","The dynamics of these structures in the presence of an externally applied oscillating electric field corresponding to a long wavelength radiation has also been studied."],"url":"http://arxiv.org/abs/2404.06786v1","category":"physics.plasm-ph"}
{"created":"2024-04-10 06:41:30","title":"Urban Architect: Steerable 3D Urban Scene Generation with Layout Prior","abstract":"Text-to-3D generation has achieved remarkable success via large-scale text-to-image diffusion models. Nevertheless, there is no paradigm for scaling up the methodology to urban scale. Urban scenes, characterized by numerous elements, intricate arrangement relationships, and vast scale, present a formidable barrier to the interpretability of ambiguous textual descriptions for effective model optimization. In this work, we surmount the limitations by introducing a compositional 3D layout representation into text-to-3D paradigm, serving as an additional prior. It comprises a set of semantic primitives with simple geometric structures and explicit arrangement relationships, complementing textual descriptions and enabling steerable generation. Upon this, we propose two modifications -- (1) We introduce Layout-Guided Variational Score Distillation to address model optimization inadequacies. It conditions the score distillation sampling process with geometric and semantic constraints of 3D layouts. (2) To handle the unbounded nature of urban scenes, we represent 3D scene with a Scalable Hash Grid structure, incrementally adapting to the growing scale of urban scenes. Extensive experiments substantiate the capability of our framework to scale text-to-3D generation to large-scale urban scenes that cover over 1000m driving distance for the first time. We also present various scene editing demonstrations, showing the powers of steerable urban scene generation. Website: https://urbanarchitect.github.io.","sentences":["Text-to-3D generation has achieved remarkable success via large-scale text-to-image diffusion models.","Nevertheless, there is no paradigm for scaling up the methodology to urban scale.","Urban scenes, characterized by numerous elements, intricate arrangement relationships, and vast scale, present a formidable barrier to the interpretability of ambiguous textual descriptions for effective model optimization.","In this work, we surmount the limitations by introducing a compositional 3D layout representation into text-to-3D paradigm, serving as an additional prior.","It comprises a set of semantic primitives with simple geometric structures and explicit arrangement relationships, complementing textual descriptions and enabling steerable generation.","Upon this, we propose two modifications -- (1) We introduce Layout-Guided Variational Score Distillation to address model optimization inadequacies.","It conditions the score distillation sampling process with geometric and semantic constraints of 3D layouts.","(2) To handle the unbounded nature of urban scenes, we represent 3D scene with a Scalable Hash Grid structure, incrementally adapting to the growing scale of urban scenes.","Extensive experiments substantiate the capability of our framework to scale text-to-3D generation to large-scale urban scenes that cover over 1000m driving distance for the first time.","We also present various scene editing demonstrations, showing the powers of steerable urban scene generation.","Website: https://urbanarchitect.github.io."],"url":"http://arxiv.org/abs/2404.06780v1","category":"cs.CV"}
{"created":"2024-04-10 06:30:08","title":"Adapting LLaMA Decoder to Vision Transformer","abstract":"This work examines whether decoder-only Transformers such as LLaMA, which were originally designed for large language models (LLMs), can be adapted to the computer vision field. We first \"LLaMAfy\" a standard ViT step-by-step to align with LLaMA's architecture, and find that directly applying a casual mask to the self-attention brings an attention collapse issue, resulting in the failure to the network training. We suggest to reposition the class token behind the image tokens with a post-sequence class token technique to overcome this challenge, enabling causal self-attention to efficiently capture the entire image's information. Additionally, we develop a soft mask strategy that gradually introduces a casual mask to the self-attention at the onset of training to facilitate the optimization behavior. The tailored model, dubbed as image LLaMA (iLLaMA), is akin to LLaMA in architecture and enables direct supervised learning. Its causal self-attention boosts computational efficiency and learns complex representation by elevating attention map ranks. iLLaMA rivals the performance with its encoder-only counterparts, achieving 75.1% ImageNet top-1 accuracy with only 5.7M parameters. Scaling the model to ~310M and pre-training on ImageNet-21K further enhances the accuracy to 86.0%. Extensive experiments demonstrate iLLaMA's reliable properties: calibration, shape-texture bias, quantization compatibility, ADE20K segmentation and CIFAR transfer learning. We hope our study can kindle fresh views to visual model design in the wave of LLMs. Pre-trained models and codes are available here.","sentences":["This work examines whether decoder-only Transformers such as LLaMA, which were originally designed for large language models (LLMs), can be adapted to the computer vision field.","We first \"LLaMAfy\" a standard ViT step-by-step to align with LLaMA's architecture, and find that directly applying a casual mask to the self-attention brings an attention collapse issue, resulting in the failure to the network training.","We suggest to reposition the class token behind the image tokens with a post-sequence class token technique to overcome this challenge, enabling causal self-attention to efficiently capture the entire image's information.","Additionally, we develop a soft mask strategy that gradually introduces a casual mask to the self-attention at the onset of training to facilitate the optimization behavior.","The tailored model, dubbed as image LLaMA (iLLaMA), is akin to LLaMA in architecture and enables direct supervised learning.","Its causal self-attention boosts computational efficiency and learns complex representation by elevating attention map ranks.","iLLaMA rivals the performance with its encoder-only counterparts, achieving 75.1% ImageNet top-1 accuracy with only 5.7M parameters.","Scaling the model to ~310M and pre-training on ImageNet-21K further enhances the accuracy to 86.0%.","Extensive experiments demonstrate iLLaMA's reliable properties: calibration, shape-texture bias, quantization compatibility, ADE20K segmentation and CIFAR transfer learning.","We hope our study can kindle fresh views to visual model design in the wave of LLMs.","Pre-trained models and codes are available here."],"url":"http://arxiv.org/abs/2404.06773v1","category":"cs.CV"}
{"created":"2024-04-10 06:28:19","title":"Beyond Gait: Learning Knee Angle for Seamless Prosthesis Control in Multiple Scenarios","abstract":"Deep learning models have become a powerful tool in knee angle estimation for lower limb prostheses, owing to their adaptability across various gait phases and locomotion modes. Current methods utilize Multi-Layer Perceptrons (MLP), Long-Short Term Memory Networks (LSTM), and Convolutional Neural Networks (CNN), predominantly analyzing motion information from the thigh. Contrary to these approaches, our study introduces a holistic perspective by integrating whole-body movements as inputs. We propose a transformer-based probabilistic framework, termed the Angle Estimation Probabilistic Model (AEPM), that offers precise angle estimations across extensive scenarios beyond walking. AEPM achieves an overall RMSE of 6.70 degrees, with an RMSE of 3.45 degrees in walking scenarios. Compared to the state of the art, AEPM has improved the prediction accuracy for walking by 11.31%. Our method can achieve seamless adaptation between different locomotion modes. Also, this model can be utilized to analyze the synergy between the knee and other joints. We reveal that the whole body movement has valuable information for knee movement, which can provide insights into designing sensors for prostheses. The code is available at https://github.com/penway/Beyond-Gait-AEPM.","sentences":["Deep learning models have become a powerful tool in knee angle estimation for lower limb prostheses, owing to their adaptability across various gait phases and locomotion modes.","Current methods utilize Multi-Layer Perceptrons (MLP), Long-Short Term Memory Networks (LSTM), and Convolutional Neural Networks (CNN), predominantly analyzing motion information from the thigh.","Contrary to these approaches, our study introduces a holistic perspective by integrating whole-body movements as inputs.","We propose a transformer-based probabilistic framework, termed the Angle Estimation Probabilistic Model (AEPM), that offers precise angle estimations across extensive scenarios beyond walking.","AEPM achieves an overall RMSE of 6.70 degrees, with an RMSE of 3.45 degrees in walking scenarios.","Compared to the state of the art, AEPM has improved the prediction accuracy for walking by 11.31%.","Our method can achieve seamless adaptation between different locomotion modes.","Also, this model can be utilized to analyze the synergy between the knee and other joints.","We reveal that the whole body movement has valuable information for knee movement, which can provide insights into designing sensors for prostheses.","The code is available at https://github.com/penway/Beyond-Gait-AEPM."],"url":"http://arxiv.org/abs/2404.06772v1","category":"cs.RO"}
{"created":"2024-04-10 06:24:07","title":"Vibrational ADAPT-VQE: Critical points leads to problematic convergence","abstract":"Quantum chemistry is one of the most promising applications for which quantum computing is expected to have significant impact. Despite considerable research in the field of electronic structure, calculating the vibrational properties of molecules on quantum computers remain a relatively unexplored field. In this work, we develop a vibrational ADAPT-VQE (vADAPT-VQE) formalism based on an infinite product representation (IPR) of anti-Hermitian excitation operators of the Full Vibrational Configuration Interaction (FVCI) wavefunction which allows for preparing eigenstates of vibrational Hamiltonians on quantum computers. In order to establish the vADAPT- VQE algorithm using the IPR, we study the exactness of disentangled Unitary Vibrational Coupled Cluster (dUVCC) theory and show that dUVCC can formally represent the FVCI wavefunction in an infinite expansion. To investigate the performance of the vADAPT-VQE algorithm, we numerically study whether the vADAPT-VQE algorithm generates a sequence of operators which may represent the FVCI wavefunction. Our numerical results indicate frequent appearance of critical points in the wavefunction preparation using vADAPT-VQE. These results imply that one may encounter diminishing usefulness when preparing vibrational wavefunctions on quantum computers using vADAPT-VQE and that additional studies are required to find methods that can circumvent this behavior.","sentences":["Quantum chemistry is one of the most promising applications for which quantum computing is expected to have significant impact.","Despite considerable research in the field of electronic structure, calculating the vibrational properties of molecules on quantum computers remain a relatively unexplored field.","In this work, we develop a vibrational ADAPT-VQE (vADAPT-VQE) formalism based on an infinite product representation (IPR) of anti-Hermitian excitation operators of the Full Vibrational Configuration Interaction (FVCI) wavefunction which allows for preparing eigenstates of vibrational Hamiltonians on quantum computers.","In order to establish the vADAPT- VQE algorithm using the IPR, we study the exactness of disentangled Unitary Vibrational Coupled Cluster (dUVCC) theory and show that dUVCC can formally represent the FVCI wavefunction in an infinite expansion.","To investigate the performance of the vADAPT-VQE algorithm, we numerically study whether the vADAPT-VQE algorithm generates a sequence of operators which may represent the FVCI wavefunction.","Our numerical results indicate frequent appearance of critical points in the wavefunction preparation using vADAPT-VQE.","These results imply that one may encounter diminishing usefulness when preparing vibrational wavefunctions on quantum computers using vADAPT-VQE and that additional studies are required to find methods that can circumvent this behavior."],"url":"http://arxiv.org/abs/2404.06770v1","category":"quant-ph"}
{"created":"2024-04-10 06:03:13","title":"Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems","abstract":"Intelligent Tutoring Systems (ITSs) can provide personalized and self-paced learning experience. The emergence of large language models (LLMs) further enables better human-machine interaction, and facilitates the development of conversational ITSs in various disciplines such as math and language learning. In dialogic teaching, recognizing and adapting to individual characteristics can significantly enhance student engagement and learning efficiency. However, characterizing and simulating student's persona remain challenging in training and evaluating conversational ITSs. In this work, we propose a framework to construct profiles of different student groups by refining and integrating both cognitive and noncognitive aspects, and leverage LLMs for personality-aware student simulation in a language learning scenario. We further enhance the framework with multi-aspect validation, and conduct extensive analysis from both teacher and student perspectives. Our experimental results show that state-of-the-art LLMs can produce diverse student responses according to the given language ability and personality traits, and trigger teacher's adaptive scaffolding strategies.","sentences":["Intelligent Tutoring Systems (ITSs) can provide personalized and self-paced learning experience.","The emergence of large language models (LLMs) further enables better human-machine interaction, and facilitates the development of conversational ITSs in various disciplines such as math and language learning.","In dialogic teaching, recognizing and adapting to individual characteristics can significantly enhance student engagement and learning efficiency.","However, characterizing and simulating student's persona remain challenging in training and evaluating conversational ITSs.","In this work, we propose a framework to construct profiles of different student groups by refining and integrating both cognitive and noncognitive aspects, and leverage LLMs for personality-aware student simulation in a language learning scenario.","We further enhance the framework with multi-aspect validation, and conduct extensive analysis from both teacher and student perspectives.","Our experimental results show that state-of-the-art LLMs can produce diverse student responses according to the given language ability and personality traits, and trigger teacher's adaptive scaffolding strategies."],"url":"http://arxiv.org/abs/2404.06762v1","category":"cs.CL"}
{"created":"2024-04-10 05:27:03","title":"Combination of Site-Wide and Real-Time Optimization for the Control of Systems of Flexible Energy Resources","abstract":"The rapid expansion of renewable energy sources introduces significant volatility and unpredictability to the energy supply chain, challenging the stability and reliability of the power grid. This work presents a method that enhances existing static optimization models by converting them into dynamic models suitable for the real-time optimization of flexible energy resources. By adapting static models for real-time application, the proposed two-stage optimization strategy allows for flexible adjustments of operational plans, facilitating the seamless integration of renewable energy sources. This approach not only ensures grid reliability but also improves economic efficiency by optimizing resource utilization. The effectiveness of this method is demonstrated through a case study involving a system of electrolyzers, showcasing significant advantages over traditional static optimization methods in aligning energy consumption with renewable energy generation.","sentences":["The rapid expansion of renewable energy sources introduces significant volatility and unpredictability to the energy supply chain, challenging the stability and reliability of the power grid.","This work presents a method that enhances existing static optimization models by converting them into dynamic models suitable for the real-time optimization of flexible energy resources.","By adapting static models for real-time application, the proposed two-stage optimization strategy allows for flexible adjustments of operational plans, facilitating the seamless integration of renewable energy sources.","This approach not only ensures grid reliability but also improves economic efficiency by optimizing resource utilization.","The effectiveness of this method is demonstrated through a case study involving a system of electrolyzers, showcasing significant advantages over traditional static optimization methods in aligning energy consumption with renewable energy generation."],"url":"http://arxiv.org/abs/2404.06748v1","category":"math.OC"}
{"created":"2024-04-10 04:24:09","title":"UAV-Assisted Enhanced Coverage and Capacity in Dynamic MU-mMIMO IoT Systems: A Deep Reinforcement Learning Approach","abstract":"This study focuses on a multi-user massive multiple-input multiple-output (MU-mMIMO) system by incorporating an unmanned aerial vehicle (UAV) as a decode-and-forward (DF) relay between the base station (BS) and multiple Internet-of-Things (IoT) devices. Our primary objective is to maximize the overall achievable rate (AR) by introducing a novel framework that integrates joint hybrid beamforming (HBF) and UAV localization in dynamic MU-mMIMO IoT systems. Particularly, HBF stages for BS and UAV are designed by leveraging slow time-varying angular information, whereas a deep reinforcement learning (RL) algorithm, namely deep deterministic policy gradient (DDPG) with continuous action space, is developed to train the UAV for its deployment. By using a customized reward function, the RL agent learns an optimal UAV deployment policy capable of adapting to both static and dynamic environments. The illustrative results show that the proposed DDPG-based UAV deployment (DDPG-UD) can achieve approximately 99.5% of the sum-rate capacity achieved by particle swarm optimization (PSO)-based UAV deployment (PSO-UD), while requiring a significantly reduced runtime at approximately 68.50% of that needed by PSO-UD, offering an efficient solution in dynamic MU-mMIMO environments.","sentences":["This study focuses on a multi-user massive multiple-input multiple-output (MU-mMIMO) system by incorporating an unmanned aerial vehicle (UAV) as a decode-and-forward (DF) relay between the base station (BS) and multiple Internet-of-Things (IoT) devices.","Our primary objective is to maximize the overall achievable rate (AR) by introducing a novel framework that integrates joint hybrid beamforming (HBF) and UAV localization in dynamic MU-mMIMO IoT systems.","Particularly, HBF stages for BS and UAV are designed by leveraging slow time-varying angular information, whereas a deep reinforcement learning (RL) algorithm, namely deep deterministic policy gradient (DDPG) with continuous action space, is developed to train the UAV for its deployment.","By using a customized reward function, the RL agent learns an optimal UAV deployment policy capable of adapting to both static and dynamic environments.","The illustrative results show that the proposed DDPG-based UAV deployment (DDPG-UD) can achieve approximately 99.5% of the sum-rate capacity achieved by particle swarm optimization (PSO)-based UAV deployment (PSO-UD), while requiring a significantly reduced runtime at approximately 68.50% of that needed by PSO-UD, offering an efficient solution in dynamic MU-mMIMO environments."],"url":"http://arxiv.org/abs/2404.06726v1","category":"eess.SP"}
{"created":"2024-04-10 03:45:38","title":"A Reexamination of the COnfLUX 2.5D LU Factorization Algorithm","abstract":"This article conducts a reexamination of the research conducted by Kwasniewski et al., focusing on their adaptation of the 2.5D LU factorization algorithm with tournament pivoting, known as \\func{COnfLUX}. Our reexamination reveals potential concerns regarding the upper bound, empirical investigation methods, and lower bound, despite the original study providing a theoretical foundation and an instantiation of the proposed algorithm. This paper offers a reexamination of these matters, highlighting probable shortcomings in the original investigation. Our observations are intended to enhance the development and comprehension of parallel matrix factorization algorithms.","sentences":["This article conducts a reexamination of the research conducted by Kwasniewski et al., focusing on their adaptation of the 2.5D LU factorization algorithm with tournament pivoting, known as \\func{COnfLUX}.","Our reexamination reveals potential concerns regarding the upper bound, empirical investigation methods, and lower bound, despite the original study providing a theoretical foundation and an instantiation of the proposed algorithm.","This paper offers a reexamination of these matters, highlighting probable shortcomings in the original investigation.","Our observations are intended to enhance the development and comprehension of parallel matrix factorization algorithms."],"url":"http://arxiv.org/abs/2404.06713v1","category":"cs.DC"}
{"created":"2024-04-10 03:19:03","title":"What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions","abstract":"There is increasing interest in the use of the LEArnable Front-end (LEAF) in a variety of speech processing systems. However, there is a dearth of analyses of what is actually learnt and the relative importance of training the different components of the front-end. In this paper, we investigate this question on keyword spotting, speech-based emotion recognition and language identification tasks and find that the filters for spectral decomposition and the low pass filter used to estimate spectral energy variations exhibit no learning and the per-channel energy normalisation (PCEN) is the key component that is learnt. Following this, we explore the potential of adapting only the PCEN layer with a small amount of noisy data to enable it to learn appropriate dynamic range compression that better suits the noise conditions. This in turn enables a system trained on clean speech to work more accurately on noisy test data as demonstrated by the experimental results reported in this paper.","sentences":["There is increasing interest in the use of the LEArnable Front-end (LEAF) in a variety of speech processing systems.","However, there is a dearth of analyses of what is actually learnt and the relative importance of training the different components of the front-end.","In this paper, we investigate this question on keyword spotting, speech-based emotion recognition and language identification tasks and find that the filters for spectral decomposition and the low pass filter used to estimate spectral energy variations exhibit no learning and the per-channel energy normalisation (PCEN) is the key component that is learnt.","Following this, we explore the potential of adapting only the PCEN layer with a small amount of noisy data to enable it to learn appropriate dynamic range compression that better suits the noise conditions.","This in turn enables a system trained on clean speech to work more accurately on noisy test data as demonstrated by the experimental results reported in this paper."],"url":"http://arxiv.org/abs/2404.06702v1","category":"eess.AS"}
{"created":"2024-04-10 02:37:24","title":"Latent Chemical Space Searching for Plug-in Multi-objective Molecule Generation","abstract":"Molecular generation, an essential method for identifying new drug structures, has been supported by advancements in machine learning and computational technology. However, challenges remain in multi-objective generation, model adaptability, and practical application in drug discovery. In this study, we developed a versatile 'plug-in' molecular generation model that incorporates multiple objectives related to target affinity, drug-likeness, and synthesizability, facilitating its application in various drug development contexts. We improved the Particle Swarm Optimization (PSO) in the context of drug discoveries, and identified PSO-ENP as the optimal variant for multi-objective molecular generation and optimization through comparative experiments. The model also incorporates a novel target-ligand affinity predictor, enhancing the model's utility by supporting three-dimensional information and improving synthetic feasibility. Case studies focused on generating and optimizing drug-like big marine natural products were performed, underscoring PSO-ENP's effectiveness and demonstrating its considerable potential for practical drug discovery applications.","sentences":["Molecular generation, an essential method for identifying new drug structures, has been supported by advancements in machine learning and computational technology.","However, challenges remain in multi-objective generation, model adaptability, and practical application in drug discovery.","In this study, we developed a versatile 'plug-in' molecular generation model that incorporates multiple objectives related to target affinity, drug-likeness, and synthesizability, facilitating its application in various drug development contexts.","We improved the Particle Swarm Optimization (PSO) in the context of drug discoveries, and identified PSO-ENP as the optimal variant for multi-objective molecular generation and optimization through comparative experiments.","The model also incorporates a novel target-ligand affinity predictor, enhancing the model's utility by supporting three-dimensional information and improving synthetic feasibility.","Case studies focused on generating and optimizing drug-like big marine natural products were performed, underscoring PSO-ENP's effectiveness and demonstrating its considerable potential for practical drug discovery applications."],"url":"http://arxiv.org/abs/2404.06691v1","category":"q-bio.BM"}
{"created":"2024-04-10 00:11:03","title":"Multi-modal Document Presentation Attack Detection With Forensics Trace Disentanglement","abstract":"Document Presentation Attack Detection (DPAD) is an important measure in protecting the authenticity of a document image. However, recent DPAD methods demand additional resources, such as manual effort in collecting additional data or knowing the parameters of acquisition devices. This work proposes a DPAD method based on multi-modal disentangled traces (MMDT) without the above drawbacks. We first disentangle the recaptured traces by a self-supervised disentanglement and synthesis network to enhance the generalization capacity in document images with different contents and layouts. Then, unlike the existing DPAD approaches that rely only on data in the RGB domain, we propose to explicitly employ the disentangled recaptured traces as new modalities in the transformer backbone through adaptive multi-modal adapters to fuse RGB/trace features efficiently. Visualization of the disentangled traces confirms the effectiveness of the proposed method in different document contents. Extensive experiments on three benchmark datasets demonstrate the superiority of our MMDT method on representing forensic traces of recapturing distortion.","sentences":["Document Presentation Attack Detection (DPAD) is an important measure in protecting the authenticity of a document image.","However, recent DPAD methods demand additional resources, such as manual effort in collecting additional data or knowing the parameters of acquisition devices.","This work proposes a DPAD method based on multi-modal disentangled traces (MMDT) without the above drawbacks.","We first disentangle the recaptured traces by a self-supervised disentanglement and synthesis network to enhance the generalization capacity in document images with different contents and layouts.","Then, unlike the existing DPAD approaches that rely only on data in the RGB domain, we propose to explicitly employ the disentangled recaptured traces as new modalities in the transformer backbone through adaptive multi-modal adapters to fuse RGB/trace features efficiently.","Visualization of the disentangled traces confirms the effectiveness of the proposed method in different document contents.","Extensive experiments on three benchmark datasets demonstrate the superiority of our MMDT method on representing forensic traces of recapturing distortion."],"url":"http://arxiv.org/abs/2404.06663v1","category":"cs.CV"}
{"created":"2024-04-09 22:57:42","title":"Efficiently Cooling Quantum Systems with Finite Resources: Insights from Thermodynamic Geometry","abstract":"Landauer's universal limit on heat dissipation during information erasure becomes increasingly crucial as computing devices shrink: minimising heat-induced errors demands optimal pure-state preparation. For this, however, Nernst's third law posits an infinite-resource requirement: either energy, time, or control complexity must diverge. Here, we address the practical challenge of efficiently cooling quantum systems using finite resources. We investigate the ensuing resource trade-offs and present efficient protocols for finite distinct energy gaps in settings pertaining to coherent or incoherent control, corresponding to quantum batteries and heat engines, respectively. Expressing energy bounds through thermodynamic length, our findings illuminate the optimal distribution of energy gaps, detailing the resource limitations of preparing pure states in practical settings.","sentences":["Landauer's universal limit on heat dissipation during information erasure becomes increasingly crucial as computing devices shrink: minimising heat-induced errors demands optimal pure-state preparation.","For this, however, Nernst's third law posits an infinite-resource requirement: either energy, time, or control complexity must diverge.","Here, we address the practical challenge of efficiently cooling quantum systems using finite resources.","We investigate the ensuing resource trade-offs and present efficient protocols for finite distinct energy gaps in settings pertaining to coherent or incoherent control, corresponding to quantum batteries and heat engines, respectively.","Expressing energy bounds through thermodynamic length, our findings illuminate the optimal distribution of energy gaps, detailing the resource limitations of preparing pure states in practical settings."],"url":"http://arxiv.org/abs/2404.06649v1","category":"quant-ph"}
{"created":"2024-04-09 22:17:20","title":"SAM-I-Am: Semantic Boosting for Zero-shot Atomic-Scale Electron Micrograph Segmentation","abstract":"Image segmentation is a critical enabler for tasks ranging from medical diagnostics to autonomous driving. However, the correct segmentation semantics - where are boundaries located? what segments are logically similar? - change depending on the domain, such that state-of-the-art foundation models can generate meaningless and incorrect results. Moreover, in certain domains, fine-tuning and retraining techniques are infeasible: obtaining labels is costly and time-consuming; domain images (micrographs) can be exponentially diverse; and data sharing (for third-party retraining) is restricted. To enable rapid adaptation of the best segmentation technology, we propose the concept of semantic boosting: given a zero-shot foundation model, guide its segmentation and adjust results to match domain expectations. We apply semantic boosting to the Segment Anything Model (SAM) to obtain microstructure segmentation for transmission electron microscopy. Our booster, SAM-I-Am, extracts geometric and textural features of various intermediate masks to perform mask removal and mask merging operations. We demonstrate a zero-shot performance increase of (absolute) +21.35%, +12.6%, +5.27% in mean IoU, and a -9.91%, -18.42%, -4.06% drop in mean false positive masks across images of three difficulty classes over vanilla SAM (ViT-L).","sentences":["Image segmentation is a critical enabler for tasks ranging from medical diagnostics to autonomous driving.","However, the correct segmentation semantics - where are boundaries located?","what segments are logically similar?","- change depending on the domain, such that state-of-the-art foundation models can generate meaningless and incorrect results.","Moreover, in certain domains, fine-tuning and retraining techniques are infeasible: obtaining labels is costly and time-consuming; domain images (micrographs) can be exponentially diverse; and data sharing (for third-party retraining) is restricted.","To enable rapid adaptation of the best segmentation technology, we propose the concept of semantic boosting: given a zero-shot foundation model, guide its segmentation and adjust results to match domain expectations.","We apply semantic boosting to the Segment Anything Model (SAM) to obtain microstructure segmentation for transmission electron microscopy.","Our booster, SAM-I-Am, extracts geometric and textural features of various intermediate masks to perform mask removal and mask merging operations.","We demonstrate a zero-shot performance increase of (absolute) +21.35%, +12.6%, +5.27% in mean IoU, and a -9.91%, -18.42%, -4.06% drop in mean false positive masks across images of three difficulty classes over vanilla SAM (ViT-L)."],"url":"http://arxiv.org/abs/2404.06638v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-09 21:21:36","title":"Adapted optimal transport between Gaussian processes in discrete time","abstract":"We derive explicitly the adapted $2$-Wasserstein distance between arbitrary non-degenerate Gaussian distributions on $\\mathbb{R}^N$ and characterize the optimal bicausal coupling(s). This leads to an adapted version of the Bures-Wasserstein distance on the space of positive definite matrices.","sentences":["We derive explicitly the adapted $2$-Wasserstein distance between arbitrary non-degenerate Gaussian distributions on $\\mathbb{R}^N$ and characterize the optimal bicausal coupling(s).","This leads to an adapted version of the Bures-Wasserstein distance on the space of positive definite matrices."],"url":"http://arxiv.org/abs/2404.06625v1","category":"math.PR"}
{"created":"2024-04-09 21:12:31","title":"Calibrating Higher-Order Statistics for Few-Shot Class-Incremental Learning with Pre-trained Vision Transformers","abstract":"Few-shot class-incremental learning (FSCIL) aims to adapt the model to new classes from very few data (5 samples) without forgetting the previously learned classes. Recent works in many-shot CIL (MSCIL) (using all available training data) exploited pre-trained models to reduce forgetting and achieve better plasticity. In a similar fashion, we use ViT models pre-trained on large-scale datasets for few-shot settings, which face the critical issue of low plasticity. FSCIL methods start with a many-shot first task to learn a very good feature extractor and then move to the few-shot setting from the second task onwards. While the focus of most recent studies is on how to learn the many-shot first task so that the model generalizes to all future few-shot tasks, we explore in this work how to better model the few-shot data using pre-trained models, irrespective of how the first task is trained. Inspired by recent works in MSCIL, we explore how using higher-order feature statistics can influence the classification of few-shot classes. We identify the main challenge of obtaining a good covariance matrix from few-shot data and propose to calibrate the covariance matrix for new classes based on semantic similarity to the many-shot base classes. Using the calibrated feature statistics in combination with existing methods significantly improves few-shot continual classification on several FSCIL benchmarks. Code is available at https://github.com/dipamgoswami/FSCIL-Calibration.","sentences":["Few-shot class-incremental learning (FSCIL) aims to adapt the model to new classes from very few data (5 samples) without forgetting the previously learned classes.","Recent works in many-shot CIL (MSCIL) (using all available training data) exploited pre-trained models to reduce forgetting and achieve better plasticity.","In a similar fashion, we use ViT models pre-trained on large-scale datasets for few-shot settings, which face the critical issue of low plasticity.","FSCIL methods start with a many-shot first task to learn a very good feature extractor and then move to the few-shot setting from the second task onwards.","While the focus of most recent studies is on how to learn the many-shot first task so that the model generalizes to all future few-shot tasks, we explore in this work how to better model the few-shot data using pre-trained models, irrespective of how the first task is trained.","Inspired by recent works in MSCIL, we explore how using higher-order feature statistics can influence the classification of few-shot classes.","We identify the main challenge of obtaining a good covariance matrix from few-shot data and propose to calibrate the covariance matrix for new classes based on semantic similarity to the many-shot base classes.","Using the calibrated feature statistics in combination with existing methods significantly improves few-shot continual classification on several FSCIL benchmarks.","Code is available at https://github.com/dipamgoswami/FSCIL-Calibration."],"url":"http://arxiv.org/abs/2404.06622v1","category":"cs.CV"}
{"created":"2024-04-09 21:10:17","title":"Encoder-Quantization-Motion-based Video Quality Metrics","abstract":"In an adaptive bitrate streaming application, the efficiency of video compression and the encoded video quality depend on both the video codec and the quality metric used to perform encoding optimization. The development of such a quality metric need large scale subjective datasets. In this work we merge several datasets into one to support the creation of a metric tailored for video compression and scaling. We proposed a set of HEVC lightweight features to boost performance of the metrics. Our metrics can be computed from tightly coupled encoding process with 4% compute overhead or from the decoding process in real-time. The proposed method can achieve better correlation than VMAF and P.1204.3. It can extrapolate to different dynamic ranges, and is suitable for real-time video quality metrics delivery in the bitstream. The performance is verified by in-distribution and cross-dataset tests. This work paves the way for adaptive client-side heuristics, real-time segment optimization, dynamic bitrate capping, and quality-dependent post-processing neural network switching, etc.","sentences":["In an adaptive bitrate streaming application, the efficiency of video compression and the encoded video quality depend on both the video codec and the quality metric used to perform encoding optimization.","The development of such a quality metric need large scale subjective datasets.","In this work we merge several datasets into one to support the creation of a metric tailored for video compression and scaling.","We proposed a set of HEVC lightweight features to boost performance of the metrics.","Our metrics can be computed from tightly coupled encoding process with 4% compute overhead or from the decoding process in real-time.","The proposed method can achieve better correlation than VMAF and P.1204.3.","It can extrapolate to different dynamic ranges, and is suitable for real-time video quality metrics delivery in the bitstream.","The performance is verified by in-distribution and cross-dataset tests.","This work paves the way for adaptive client-side heuristics, real-time segment optimization, dynamic bitrate capping, and quality-dependent post-processing neural network switching, etc."],"url":"http://arxiv.org/abs/2404.06620v1","category":"eess.IV"}
{"created":"2024-04-09 20:06:25","title":"FMDA-OT: Federated Multi-source Domain Adaptation Through Optimal Transport","abstract":"Multi-source Domain Adaptation (MDA) aims to adapt models trained on multiple labeled source domains to an unlabeled target domain. In this paper, we introduce our approach as a collaborative MDA framework, which comprises two adaptation phases. Firstly, we conduct domain adaptation for each source individually with the target, utilizing optimal transport. Then, in the second phase, which constitutes the final part of the framework, we design the architecture of centralized federated learning to collaborate the N models representing the N sources. This architecture offers the advantage of using the sources without accessing their data, thus resolving data privacy issues inherent in domain adaptation. Additionally, during this phase, the server guides and fine-tunes the adaptation using a small number of pseudo-labeled samples available in the target domain, referred to as the target validation subset of the dataset.","sentences":["Multi-source Domain Adaptation (MDA) aims to adapt models trained on multiple labeled source domains to an unlabeled target domain.","In this paper, we introduce our approach as a collaborative MDA framework, which comprises two adaptation phases.","Firstly, we conduct domain adaptation for each source individually with the target, utilizing optimal transport.","Then, in the second phase, which constitutes the final part of the framework, we design the architecture of centralized federated learning to collaborate the N models representing the N sources.","This architecture offers the advantage of using the sources without accessing their data, thus resolving data privacy issues inherent in domain adaptation.","Additionally, during this phase, the server guides and fine-tunes the adaptation using a small number of pseudo-labeled samples available in the target domain, referred to as the target validation subset of the dataset."],"url":"http://arxiv.org/abs/2404.06599v1","category":"cs.LG"}
{"created":"2024-04-09 18:52:30","title":"The Tensor-Train Stochastic Finite Volume Method for Uncertainty Quantification","abstract":"The stochastic finite volume method offers an efficient one-pass approach for assessing uncertainty in hyperbolic conservation laws. Still, it struggles with the curse of dimensionality when dealing with multiple stochastic variables. We introduce the stochastic finite volume method within the tensor-train framework to counteract this limitation. This integration, however, comes with its own set of difficulties, mainly due to the propensity for shock formation in hyperbolic systems. To overcome these issues, we have developed a tensor-train-adapted stochastic finite volume method that employs a global WENO reconstruction, making it suitable for such complex systems. This approach represents the first step in designing tensor-train techniques for hyperbolic systems and conservation laws involving shocks.","sentences":["The stochastic finite volume method offers an efficient one-pass approach for assessing uncertainty in hyperbolic conservation laws.","Still, it struggles with the curse of dimensionality when dealing with multiple stochastic variables.","We introduce the stochastic finite volume method within the tensor-train framework to counteract this limitation.","This integration, however, comes with its own set of difficulties, mainly due to the propensity for shock formation in hyperbolic systems.","To overcome these issues, we have developed a tensor-train-adapted stochastic finite volume method that employs a global WENO reconstruction, making it suitable for such complex systems.","This approach represents the first step in designing tensor-train techniques for hyperbolic systems and conservation laws involving shocks."],"url":"http://arxiv.org/abs/2404.06574v1","category":"math.NA"}
{"created":"2024-04-09 18:21:28","title":"PSF quality metrics in the problem of revealing Intermediate-Mass Black Holes using MICADO@ELT","abstract":"Nowadays, astronomers perform point spread function (PSF) fitting for most types of observational data. Interpolation of the PSF is often an intermediate step in such algorithms. In the case of the Multi-AO Imaging Camera for Deep Observations (MICADO) at the Extremely Large Telescope (ELT), PSF interpolation will play a crucial role in high-precision astrometry for stellar clusters and confirmation of the Intermediate-Mass Black Holes (IMBHs) presence. Significant PSF variations across the field of view invalidate the approach of deconvolution with a mean PSF or on-axis PSF. The ignoring of PSF variations can be especially unsatisfactory in the case of Single Conjugate Adaptive Optics (SCAO) observations, as these sophisticated and expensive systems are designed to achieve high resolution with ground-based telescopes by correcting for atmospheric turbulence in the direction of one reference star. In plenty of tasks, you face the question: How can I establish the quality of PSF fitting or interpolation? Our study aims to demonstrate the variety of PSF quality metrics, including the problem of revealing IMBHs in stellar clusters.","sentences":["Nowadays, astronomers perform point spread function (PSF) fitting for most types of observational data.","Interpolation of the PSF is often an intermediate step in such algorithms.","In the case of the Multi-AO Imaging Camera for Deep Observations (MICADO) at the Extremely Large Telescope (ELT), PSF interpolation will play a crucial role in high-precision astrometry for stellar clusters and confirmation of the Intermediate-Mass Black Holes (IMBHs) presence.","Significant PSF variations across the field of view invalidate the approach of deconvolution with a mean PSF or on-axis PSF.","The ignoring of PSF variations can be especially unsatisfactory in the case of Single Conjugate Adaptive Optics (SCAO) observations, as these sophisticated and expensive systems are designed to achieve high resolution with ground-based telescopes by correcting for atmospheric turbulence in the direction of one reference star.","In plenty of tasks, you face the question: How can I establish the quality of PSF fitting or interpolation?","Our study aims to demonstrate the variety of PSF quality metrics, including the problem of revealing IMBHs in stellar clusters."],"url":"http://arxiv.org/abs/2404.06558v1","category":"astro-ph.IM"}
{"created":"2024-04-09 18:02:01","title":"Variational Stochastic Gradient Descent for Deep Neural Networks","abstract":"Optimizing deep neural networks is one of the main tasks in successful deep learning. Current state-of-the-art optimizers are adaptive gradient-based optimization methods such as Adam. Recently, there has been an increasing interest in formulating gradient-based optimizers in a probabilistic framework for better estimation of gradients and modeling uncertainties. Here, we propose to combine both approaches, resulting in the Variational Stochastic Gradient Descent (VSGD) optimizer. We model gradient updates as a probabilistic model and utilize stochastic variational inference (SVI) to derive an efficient and effective update rule. Further, we show how our VSGD method relates to other adaptive gradient-based optimizers like Adam. Lastly, we carry out experiments on two image classification datasets and four deep neural network architectures, where we show that VSGD outperforms Adam and SGD.","sentences":["Optimizing deep neural networks is one of the main tasks in successful deep learning.","Current state-of-the-art optimizers are adaptive gradient-based optimization methods such as Adam.","Recently, there has been an increasing interest in formulating gradient-based optimizers in a probabilistic framework for better estimation of gradients and modeling uncertainties.","Here, we propose to combine both approaches, resulting in the Variational Stochastic Gradient Descent (VSGD) optimizer.","We model gradient updates as a probabilistic model and utilize stochastic variational inference (SVI) to derive an efficient and effective update rule.","Further, we show how our VSGD method relates to other adaptive gradient-based optimizers like Adam.","Lastly, we carry out experiments on two image classification datasets and four deep neural network architectures, where we show that VSGD outperforms Adam and SGD."],"url":"http://arxiv.org/abs/2404.06549v1","category":"cs.LG"}
{"created":"2024-04-09 17:49:07","title":"Existence of Mexican-hat dispersion and symmetry group of a layer","abstract":"Increased interest in physics of graphene and other two-dimensional materials boosted investigations of band structure near nodal points and lines. In contrast, group theoretical explanation of simple bands (that do not touch other bands), is sporadically present in the literature. This paper presents electronic dispersions up to forth order in momentum, near Brillouin zone (BZ) high symmetry points of all eighty layer groups. The method applies to non magnetic materials both with or without spin-orbit coupling. Particular attention is devoted to Mexican-hat dispersion, showing that it can appear only at BZ center of hexagonal layer groups. Presented symmetry adapted Taylor expansion of bands can be used to fit ab-initio or experimental band structures, or for analytical calculation of crystal properties. The results presented here might serve also as a guiding tool for design of new two-dimensional materials.","sentences":["Increased interest in physics of graphene and other two-dimensional materials boosted investigations of band structure near nodal points and lines.","In contrast, group theoretical explanation of simple bands (that do not touch other bands), is sporadically present in the literature.","This paper presents electronic dispersions up to forth order in momentum, near Brillouin zone (BZ) high symmetry points of all eighty layer groups.","The method applies to non magnetic materials both with or without spin-orbit coupling.","Particular attention is devoted to Mexican-hat dispersion, showing that it can appear only at BZ center of hexagonal layer groups.","Presented symmetry adapted Taylor expansion of bands can be used to fit ab-initio or experimental band structures, or for analytical calculation of crystal properties.","The results presented here might serve also as a guiding tool for design of new two-dimensional materials."],"url":"http://arxiv.org/abs/2404.06494v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-09 17:37:08","title":"GO4Align: Group Optimization for Multi-Task Alignment","abstract":"This paper proposes \\textit{GO4Align}, a multi-task optimization approach that tackles task imbalance by explicitly aligning the optimization across tasks. To achieve this, we design an adaptive group risk minimization strategy, compromising two crucial techniques in implementation: (i) dynamical group assignment, which clusters similar tasks based on task interactions; (ii) risk-guided group indicators, which exploit consistent task correlations with risk information from previous iterations. Comprehensive experimental results on diverse typical benchmarks demonstrate our method's performance superiority with even lower computational costs.","sentences":["This paper proposes \\textit{GO4Align}, a multi-task optimization approach that tackles task imbalance by explicitly aligning the optimization across tasks.","To achieve this, we design an adaptive group risk minimization strategy, compromising two crucial techniques in implementation: (i) dynamical group assignment, which clusters similar tasks based on task interactions; (ii) risk-guided group indicators, which exploit consistent task correlations with risk information from previous iterations.","Comprehensive experimental results on diverse typical benchmarks demonstrate our method's performance superiority with even lower computational costs."],"url":"http://arxiv.org/abs/2404.06486v1","category":"cs.LG"}
{"created":"2024-04-09 17:30:48","title":"Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks","abstract":"Recently, the large language model (LLM) community has shown increasing interest in enhancing LLMs' capability to handle extremely long documents. As various long-text techniques and model architectures emerge, the precise and detailed evaluation of models' long-text capabilities has become increasingly important. Existing long-text evaluation benchmarks, such as L-Eval and LongBench, construct long-text test sets based on open-source datasets, focusing mainly on QA and summarization tasks. These datasets include test samples of varying lengths (from 2k to 32k+) entangled together, making it challenging to assess model capabilities across different length ranges. Moreover, they do not cover the ultralong settings (100k+ tokens) that the latest LLMs claim to achieve. In this paper, we introduce Ada-LEval, a length-adaptable benchmark for evaluating the long-context understanding of LLMs. Ada-LEval includes two challenging subsets, TSort and BestAnswer, which enable a more reliable evaluation of LLMs' long context capabilities. These benchmarks support intricate manipulation of the length of test cases, and can easily produce text samples up to 128k tokens. We evaluate 4 state-of-the-art closed-source API models and 6 open-source models with Ada-LEval. The evaluation results demonstrate the limitations of current LLMs, especially in ultra-long-context settings. Our code is available at https://github.com/open-compass/Ada-LEval.","sentences":["Recently, the large language model (LLM) community has shown increasing interest in enhancing LLMs' capability to handle extremely long documents.","As various long-text techniques and model architectures emerge, the precise and detailed evaluation of models' long-text capabilities has become increasingly important.","Existing long-text evaluation benchmarks, such as L-Eval and LongBench, construct long-text test sets based on open-source datasets, focusing mainly on QA and summarization tasks.","These datasets include test samples of varying lengths (from 2k to 32k+) entangled together, making it challenging to assess model capabilities across different length ranges.","Moreover, they do not cover the ultralong settings (100k+ tokens) that the latest LLMs claim to achieve.","In this paper, we introduce Ada-LEval, a length-adaptable benchmark for evaluating the long-context understanding of LLMs.","Ada-LEval includes two challenging subsets, TSort and BestAnswer, which enable a more reliable evaluation of LLMs' long context capabilities.","These benchmarks support intricate manipulation of the length of test cases, and can easily produce text samples up to 128k tokens.","We evaluate 4 state-of-the-art closed-source API models and 6 open-source models with Ada-LEval.","The evaluation results demonstrate the limitations of current LLMs, especially in ultra-long-context settings.","Our code is available at https://github.com/open-compass/Ada-LEval."],"url":"http://arxiv.org/abs/2404.06480v2","category":"cs.CL"}
{"created":"2024-04-09 17:26:13","title":"Two-dimensional turbulence above topography: condensation transition and selection of minimum enstrophy solutions","abstract":"We consider two-dimensional flows above topography, revisiting the selective decay (or minimum-enstrophy) hypothesis of Bretherton and Haidvogel. We derive a `condensed branch' of solutions to the variational problem where a domain-scale condensate coexists with a flow at the (smaller) scale of the topography. The condensate arises through a supercritical bifurcation as the conserved energy of the initial condition exceeds a threshold value, a prediction that we quantitatively validate using Direct Numerical Simulations (DNS). We then consider the forced-dissipative case, showing how weak forcing and dissipation select a single dissipative state out of the continuum of solutions to the energy-conserving system predicted by selective decay. As the forcing strength increases, the condensate arises through a supercritical bifurcation for topographic-scale forcing and through a subcritical bifurcation for domain-scale forcing, both predictions being quantitatively validated by DNS. This method provides a way of determining the equilibrated state of forced-dissipative flows based on variational approaches to the associated energy-conserving system, such as the statistical mechanics of 2D flows or selective decay.","sentences":["We consider two-dimensional flows above topography, revisiting the selective decay (or minimum-enstrophy) hypothesis of Bretherton and Haidvogel.","We derive a `condensed branch' of solutions to the variational problem where a domain-scale condensate coexists with a flow at the (smaller) scale of the topography.","The condensate arises through a supercritical bifurcation as the conserved energy of the initial condition exceeds a threshold value, a prediction that we quantitatively validate using Direct Numerical Simulations (DNS).","We then consider the forced-dissipative case, showing how weak forcing and dissipation select a single dissipative state out of the continuum of solutions to the energy-conserving system predicted by selective decay.","As the forcing strength increases, the condensate arises through a supercritical bifurcation for topographic-scale forcing and through a subcritical bifurcation for domain-scale forcing, both predictions being quantitatively validated by DNS.","This method provides a way of determining the equilibrated state of forced-dissipative flows based on variational approaches to the associated energy-conserving system, such as the statistical mechanics of 2D flows or selective decay."],"url":"http://arxiv.org/abs/2404.06475v1","category":"physics.flu-dyn"}
{"created":"2024-04-09 17:17:23","title":"Neuromorphic In-Context Learning for Energy-Efficient MIMO Symbol Detection","abstract":"In-context learning (ICL), a property demonstrated by transformer-based sequence models, refers to the automatic inference of an input-output mapping based on examples of the mapping provided as context. ICL requires no explicit learning, i.e., no explicit updates of model weights, directly mapping context and new input to the new output. Prior work has proved the usefulness of ICL for detection in MIMO channels. In this setting, the context is given by pilot symbols, and ICL automatically adapts a detector, or equalizer, to apply to newly received signals. However, the implementation tested in prior art was based on conventional artificial neural networks (ANNs), which may prove too energy-demanding to be run on mobile devices. This paper evaluates a neuromorphic implementation of the transformer for ICL-based MIMO detection. This approach replaces ANNs with spiking neural networks (SNNs), and implements the attention mechanism via stochastic computing, requiring no multiplications, but only logical AND operations and counting. When using conventional digital CMOS hardware, the proposed implementation is shown to preserve accuracy, with a reduction in power consumption ranging from $5.4\\times$ to $26.8\\times$, depending on the model sizes, as compared to ANN-based implementations.","sentences":["In-context learning (ICL), a property demonstrated by transformer-based sequence models, refers to the automatic inference of an input-output mapping based on examples of the mapping provided as context.","ICL requires no explicit learning, i.e., no explicit updates of model weights, directly mapping context and new input to the new output.","Prior work has proved the usefulness of ICL for detection in MIMO channels.","In this setting, the context is given by pilot symbols, and ICL automatically adapts a detector, or equalizer, to apply to newly received signals.","However, the implementation tested in prior art was based on conventional artificial neural networks (ANNs), which may prove too energy-demanding to be run on mobile devices.","This paper evaluates a neuromorphic implementation of the transformer for ICL-based MIMO detection.","This approach replaces ANNs with spiking neural networks (SNNs), and implements the attention mechanism via stochastic computing, requiring no multiplications, but only logical AND operations and counting.","When using conventional digital CMOS hardware, the proposed implementation is shown to preserve accuracy, with a reduction in power consumption ranging from $5.4\\times$ to $26.8\\times$, depending on the model sizes, as compared to ANN-based implementations."],"url":"http://arxiv.org/abs/2404.06469v1","category":"eess.SP"}
{"created":"2024-04-09 17:12:38","title":"Phase space contraction of degenerately damped random splittings","abstract":"When studying out-of-equilibrium systems, one often excites the dynamics in some degrees of freedom while removing the excitation in others through damping. In order for the system to converge to a statistical steady state, the dynamics must transfer the energy from the excited modes to the dissipative directions. The precise mechanisms underlying this transfer are of particular interest and are the topic of this paper. We explore a class of randomly switched models introduced in [2,3] and provide some of the first results showing that minimal damping is sufficient to stabilize the system in a fluids model.","sentences":["When studying out-of-equilibrium systems, one often excites the dynamics in some degrees of freedom while removing the excitation in others through damping.","In order for the system to converge to a statistical steady state, the dynamics must transfer the energy from the excited modes to the dissipative directions.","The precise mechanisms underlying this transfer are of particular interest and are the topic of this paper.","We explore a class of randomly switched models introduced in [2,3] and provide some of the first results showing that minimal damping is sufficient to stabilize the system in a fluids model."],"url":"http://arxiv.org/abs/2404.06465v1","category":"math.PR"}
{"created":"2024-04-09 16:53:43","title":"SmartControl: Enhancing ControlNet for Handling Rough Visual Conditions","abstract":"Human visual imagination usually begins with analogies or rough sketches. For example, given an image with a girl playing guitar before a building, one may analogously imagine how it seems like if Iron Man playing guitar before Pyramid in Egypt. Nonetheless, visual condition may not be precisely aligned with the imaginary result indicated by text prompt, and existing layout-controllable text-to-image (T2I) generation models is prone to producing degraded generated results with obvious artifacts. To address this issue, we present a novel T2I generation method dubbed SmartControl, which is designed to modify the rough visual conditions for adapting to text prompt. The key idea of our SmartControl is to relax the visual condition on the areas that are conflicted with text prompts. In specific, a Control Scale Predictor (CSP) is designed to identify the conflict regions and predict the local control scales, while a dataset with text prompts and rough visual conditions is constructed for training CSP. It is worth noting that, even with a limited number (e.g., 1,000~2,000) of training samples, our SmartControl can generalize well to unseen objects. Extensive experiments on four typical visual condition types clearly show the efficacy of our SmartControl against state-of-the-arts. Source code, pre-trained models, and datasets are available at https://github.com/liuxiaoyu1104/SmartControl.","sentences":["Human visual imagination usually begins with analogies or rough sketches.","For example, given an image with a girl playing guitar before a building, one may analogously imagine how it seems like if Iron Man playing guitar before Pyramid in Egypt.","Nonetheless, visual condition may not be precisely aligned with the imaginary result indicated by text prompt, and existing layout-controllable text-to-image (T2I) generation models is prone to producing degraded generated results with obvious artifacts.","To address this issue, we present a novel T2I generation method dubbed SmartControl, which is designed to modify the rough visual conditions for adapting to text prompt.","The key idea of our SmartControl is to relax the visual condition on the areas that are conflicted with text prompts.","In specific, a Control Scale Predictor (CSP) is designed to identify the conflict regions and predict the local control scales, while a dataset with text prompts and rough visual conditions is constructed for training CSP.","It is worth noting that, even with a limited number (e.g., 1,000~2,000) of training samples, our SmartControl can generalize well to unseen objects.","Extensive experiments on four typical visual condition types clearly show the efficacy of our SmartControl against state-of-the-arts.","Source code, pre-trained models, and datasets are available at https://github.com/liuxiaoyu1104/SmartControl."],"url":"http://arxiv.org/abs/2404.06451v1","category":"cs.CV"}
{"created":"2024-04-09 16:50:30","title":"Automated Federated Pipeline for Parameter-Efficient Fine-Tuning of Large Language Models","abstract":"Recently, there has been a surge in the development of advanced intelligent generative content (AIGC), especially large language models (LLMs). However, for many downstream tasks, it is necessary to fine-tune LLMs using private data. While federated learning offers a promising privacy-preserving solution to LLM fine-tuning, the substantial size of an LLM, combined with high computational and communication demands, makes it hard to apply to downstream tasks. More importantly, private edge servers often possess varying computing and network resources in real-world scenarios, introducing additional complexities to LLM fine-tuning. To tackle these problems, we design and implement an automated federated pipeline, named FedPipe, to fine-tune LLMs with minimal training cost but without adding any inference latency. FedPipe firstly identifies the weights to be fine-tuned based on their contributions to the LLM training. It then configures a low-rank adapter for each selected weight to train local low-rank adapters on an edge server, and aggregate local adapters of all edge servers to fine-tune the whole LLM. Finally, it appropriately quantizes the parameters of LLM to reduce memory space according to the requirements of edge servers. Extensive experiments demonstrate that FedPipe expedites the model training and achieves higher accuracy than state-of-the-art benchmarks.","sentences":["Recently, there has been a surge in the development of advanced intelligent generative content (AIGC), especially large language models (LLMs).","However, for many downstream tasks, it is necessary to fine-tune LLMs using private data.","While federated learning offers a promising privacy-preserving solution to LLM fine-tuning, the substantial size of an LLM, combined with high computational and communication demands, makes it hard to apply to downstream tasks.","More importantly, private edge servers often possess varying computing and network resources in real-world scenarios, introducing additional complexities to LLM fine-tuning.","To tackle these problems, we design and implement an automated federated pipeline, named FedPipe, to fine-tune LLMs with minimal training cost but without adding any inference latency.","FedPipe firstly identifies the weights to be fine-tuned based on their contributions to the LLM training.","It then configures a low-rank adapter for each selected weight to train local low-rank adapters on an edge server, and aggregate local adapters of all edge servers to fine-tune the whole LLM.","Finally, it appropriately quantizes the parameters of LLM to reduce memory space according to the requirements of edge servers.","Extensive experiments demonstrate that FedPipe expedites the model training and achieves higher accuracy than state-of-the-art benchmarks."],"url":"http://arxiv.org/abs/2404.06448v1","category":"cs.LG"}
{"created":"2024-04-09 16:45:34","title":"Multi-scale Dynamic and Hierarchical Relationship Modeling for Facial Action Units Recognition","abstract":"Human facial action units (AUs) are mutually related in a hierarchical manner, as not only they are associated with each other in both spatial and temporal domains but also AUs located in the same/close facial regions show stronger relationships than those of different facial regions. While none of existing approach thoroughly model such hierarchical inter-dependencies among AUs, this paper proposes to comprehensively model multi-scale AU-related dynamic and hierarchical spatio-temporal relationship among AUs for their occurrences recognition. Specifically, we first propose a novel multi-scale temporal differencing network with an adaptive weighting block to explicitly capture facial dynamics across frames at different spatial scales, which specifically considers the heterogeneity of range and magnitude in different AUs' activation. Then, a two-stage strategy is introduced to hierarchically model the relationship among AUs based on their spatial distribution (i.e., local and cross-region AU relationship modelling). Experimental results achieved on BP4D and DISFA show that our approach is the new state-of-the-art in the field of AU occurrence recognition. Our code is publicly available at https://github.com/CVI-SZU/MDHR.","sentences":["Human facial action units (AUs) are mutually related in a hierarchical manner, as not only they are associated with each other in both spatial and temporal domains but also AUs located in the same/close facial regions show stronger relationships than those of different facial regions.","While none of existing approach thoroughly model such hierarchical inter-dependencies among AUs, this paper proposes to comprehensively model multi-scale AU-related dynamic and hierarchical spatio-temporal relationship among AUs for their occurrences recognition.","Specifically, we first propose a novel multi-scale temporal differencing network with an adaptive weighting block to explicitly capture facial dynamics across frames at different spatial scales, which specifically considers the heterogeneity of range and magnitude in different AUs' activation.","Then, a two-stage strategy is introduced to hierarchically model the relationship among AUs based on their spatial distribution (i.e., local and cross-region AU relationship modelling).","Experimental results achieved on BP4D and DISFA show that our approach is the new state-of-the-art in the field of AU occurrence recognition.","Our code is publicly available at https://github.com/CVI-SZU/MDHR."],"url":"http://arxiv.org/abs/2404.06443v1","category":"cs.CV"}
{"created":"2024-04-10 17:58:04","title":"Toward a Better Understanding of Fourier Neural Operators: Analysis and Improvement from a Spectral Perspective","abstract":"In solving partial differential equations (PDEs), Fourier Neural Operators (FNOs) have exhibited notable effectiveness compared to Convolutional Neural Networks (CNNs). This paper presents clear empirical evidence through spectral analysis to elucidate the superiority of FNO over CNNs: FNO is significantly more capable of learning low-frequencies. This empirical evidence also unveils FNO's distinct low-frequency bias, which limits FNO's effectiveness in learning high-frequency information from PDE data. To tackle this challenge, we introduce SpecBoost, an ensemble learning framework that employs multiple FNOs to better capture high-frequency information. Specifically, a secondary FNO is utilized to learn the overlooked high-frequency information from the prediction residual of the initial FNO. Experiments demonstrate that SpecBoost noticeably enhances FNO's prediction accuracy on diverse PDE applications, achieving an up to 71% improvement.","sentences":["In solving partial differential equations (PDEs), Fourier Neural Operators (FNOs) have exhibited notable effectiveness compared to Convolutional Neural Networks (CNNs).","This paper presents clear empirical evidence through spectral analysis to elucidate the superiority of FNO over CNNs: FNO is significantly more capable of learning low-frequencies.","This empirical evidence also unveils FNO's distinct low-frequency bias, which limits FNO's effectiveness in learning high-frequency information from PDE data.","To tackle this challenge, we introduce SpecBoost, an ensemble learning framework that employs multiple FNOs to better capture high-frequency information.","Specifically, a secondary FNO is utilized to learn the overlooked high-frequency information from the prediction residual of the initial FNO.","Experiments demonstrate that SpecBoost noticeably enhances FNO's prediction accuracy on diverse PDE applications, achieving an up to 71% improvement."],"url":"http://arxiv.org/abs/2404.07200v1","category":"cs.LG"}
{"created":"2024-04-10 17:41:41","title":"GCV-Turbo: End-to-end Acceleration of GNN-based Computer Vision Tasks on FPGA","abstract":"Graph neural networks (GNNs) have recently empowered various novel computer vision (CV) tasks. In GNN-based CV tasks, a combination of CNN layers and GNN layers or only GNN layers are employed. This paper introduces GCV-Turbo, a domain-specific accelerator on FPGA for end-to-end acceleration of GNN-based CV tasks. GCV-Turbo consists of two key components: (1) a \\emph{novel} hardware architecture optimized for the computation kernels in both CNNs and GNNs using the same set of computation resources. (2) a PyTorch-compatible compiler that takes a user-defined model as input, performs end-to-end optimization for the computation graph of a given GNN-based CV task, and produces optimized code for hardware execution. The hardware architecture and the compiler work synergistically to support a variety of GNN-based CV tasks. We implement GCV-Turbo on a state-of-the-art FPGA and evaluate its performance across six representative GNN-based CV tasks with diverse input data modalities (e.g., image, human skeleton, point cloud). Compared with state-of-the-art CPU (GPU) implementations, GCV-Turbo achieves an average latency reduction of $68.4\\times$ ($4.1\\times$) on these six GNN-based CV tasks. Moreover, GCV-Turbo supports the execution of the standalone CNNs or GNNs, achieving performance comparable to that of state-of-the-art CNN (GNN) accelerators for widely used CNN-only (GNN-only) models.","sentences":["Graph neural networks (GNNs) have recently empowered various novel computer vision (CV) tasks.","In GNN-based CV tasks, a combination of CNN layers and GNN layers or only GNN layers are employed.","This paper introduces GCV-Turbo, a domain-specific accelerator on FPGA for end-to-end acceleration of GNN-based CV tasks.","GCV-Turbo consists of two key components: (1) a \\emph{novel} hardware architecture optimized for the computation kernels in both CNNs and GNNs using the same set of computation resources.","(2) a PyTorch-compatible compiler that takes a user-defined model as input, performs end-to-end optimization for the computation graph of a given GNN-based CV task, and produces optimized code for hardware execution.","The hardware architecture and the compiler work synergistically to support a variety of GNN-based CV tasks.","We implement GCV-Turbo on a state-of-the-art FPGA and evaluate its performance across six representative GNN-based CV tasks with diverse input data modalities (e.g., image, human skeleton, point cloud).","Compared with state-of-the-art CPU (GPU) implementations, GCV-Turbo achieves an average latency reduction of $68.4\\times$ ($4.1\\times$) on these six GNN-based CV tasks.","Moreover, GCV-Turbo supports the execution of the standalone CNNs or GNNs, achieving performance comparable to that of state-of-the-art CNN (GNN) accelerators for widely used CNN-only (GNN-only) models."],"url":"http://arxiv.org/abs/2404.07188v1","category":"cs.DC"}
{"created":"2024-04-10 17:33:11","title":"Studying the Supernova Absolute Magnitude Constancy with Baryonic Acoustic Oscillations","abstract":"In this proceeding we review and expand on our recent work investigating the constancy of the absolute magnitude $M_B$ of Type Ia supernovae. In it, we used baryonic acoustic oscillations (BAO) to calibrate the supernova data and to check whether the resulting $M_B$ is constant. We used non-parametric methods like Gaussian processes and artificial neural networks to reconstruct $M_B(z)$. Here we elaborate on the results by putting them in the context of other studies investigating possible non-constant $M_B$ and the impact of the distance-duality relation. We also present some numerical details on the calculations in the original paper and new non-parametric reconstructions, including a conservative model-independent fit, confirming its main results. Notably, we see that $M_B$ remains constant within $1\\sigma$, with a possible jump around $z = 0.01 - 0.15$. Furthermore, the observed distribution of $M_B(z)$ cannot be described by a single Gaussian, displaying multiple peaks and tails. The choice of the only remaining parameter -- the sound horizon $r_d$ leads to a tension in the $M_B-r_d$ plane. Fitting different non-constant $M_B(z)$ models does not significantly improve the fit and there is no preference for any of the models by the statistical measures we employ.","sentences":["In this proceeding we review and expand on our recent work investigating the constancy of the absolute magnitude $M_B$ of Type Ia supernovae.","In it, we used baryonic acoustic oscillations (BAO) to calibrate the supernova data and to check whether the resulting $M_B$ is constant.","We used non-parametric methods like Gaussian processes and artificial neural networks to reconstruct $M_B(z)$. Here we elaborate on the results by putting them in the context of other studies investigating possible non-constant $M_B$ and the impact of the distance-duality relation.","We also present some numerical details on the calculations in the original paper and new non-parametric reconstructions, including a conservative model-independent fit, confirming its main results.","Notably, we see that $M_B$ remains constant within $1\\sigma$, with a possible jump around $z = 0.01 - 0.15$.","Furthermore, the observed distribution of $M_B(z)$ cannot be described by a single Gaussian, displaying multiple peaks and tails.","The choice of the only remaining parameter -- the sound horizon $r_d$ leads to a tension in the $M_B-r_d$ plane.","Fitting different non-constant $M_B(z)$ models does not significantly improve the fit and there is no preference for any of the models by the statistical measures we employ."],"url":"http://arxiv.org/abs/2404.07182v1","category":"astro-ph.CO"}
{"created":"2024-04-10 17:17:05","title":"Ground state-based quantum feature maps","abstract":"We introduce a quantum data embedding protocol based on the preparation of a ground state of a parameterized Hamiltonian. We analyze the corresponding quantum feature map, recasting it as an adiabatic state preparation procedure with Trotterized evolution. We compare the properties of underlying quantum models with ubiquitous Fourier-type quantum models, and show that ground state embeddings can be described effectively by a spectrum with degree that grows rapidly with the number of qubits, corresponding to a large model capacity. We observe that the spectrum contains massive frequency degeneracies, and the weighting coefficients for the modes are highly structured, thus limiting model expressivity. Our results provide a step towards understanding models based on quantum data, and contribute to fundamental knowledge needed for building efficient quantum machine learning (QML) protocols. As non-trivial embeddings are crucial for designing QML protocols that cannot be simulated classically, our findings guide the search for high-capacity quantum models that can largely outperform classical models.","sentences":["We introduce a quantum data embedding protocol based on the preparation of a ground state of a parameterized Hamiltonian.","We analyze the corresponding quantum feature map, recasting it as an adiabatic state preparation procedure with Trotterized evolution.","We compare the properties of underlying quantum models with ubiquitous Fourier-type quantum models, and show that ground state embeddings can be described effectively by a spectrum with degree that grows rapidly with the number of qubits, corresponding to a large model capacity.","We observe that the spectrum contains massive frequency degeneracies, and the weighting coefficients for the modes are highly structured, thus limiting model expressivity.","Our results provide a step towards understanding models based on quantum data, and contribute to fundamental knowledge needed for building efficient quantum machine learning (QML) protocols.","As non-trivial embeddings are crucial for designing QML protocols that cannot be simulated classically, our findings guide the search for high-capacity quantum models that can largely outperform classical models."],"url":"http://arxiv.org/abs/2404.07174v1","category":"quant-ph"}
{"created":"2024-04-10 17:02:09","title":"Defect-engineering hexagonal boron nitride using low-energy Ar+ irradiation","abstract":"Monolayer hexagonal boron nitride (hBN) has recently become the focus of intense research as a material to host quantum emitters. Although it is well known that such emission is associated with point defects, so far no conclusive correlation between the spectra and specific defects has been demonstrated. Here, we prepare atomically clean suspended hBN samples and subject them to low-energy ion irradiation. The samples are characterized before and after irradiation via automated scanning transmission electron microscopy imaging to assess the defect concentrations and distributions. We find an intrinsic defect concentration of ca. 0.03/nm2 (with ca. 55% boron and 8% nitrogen single vacancies, 20% double vacancies and 16% more complex vacancy structures). To be able to differentiate between these and irradiation-induced defects, we create a significantly higher (but still moderate) concentration of defects with the ions (0.30/nm2), and now find ca. 55% boron and 12% nitrogen single vacancies, 14% double vacancies, and 18% more complex vacancy structures. The results demonstrate that already the simplest irradiation provides selectivity for the defect types, and open the way for future experiments to explore changing the selectivity by modifying the irradiation parameters.","sentences":["Monolayer hexagonal boron nitride (hBN) has recently become the focus of intense research as a material to host quantum emitters.","Although it is well known that such emission is associated with point defects, so far no conclusive correlation between the spectra and specific defects has been demonstrated.","Here, we prepare atomically clean suspended hBN samples and subject them to low-energy ion irradiation.","The samples are characterized before and after irradiation via automated scanning transmission electron microscopy imaging to assess the defect concentrations and distributions.","We find an intrinsic defect concentration of ca.","0.03/nm2 (with ca.","55% boron and 8% nitrogen single vacancies, 20% double vacancies and 16% more complex vacancy structures).","To be able to differentiate between these and irradiation-induced defects, we create a significantly higher (but still moderate) concentration of defects with the ions (0.30/nm2), and now find ca.","55% boron and 12% nitrogen single vacancies, 14% double vacancies, and 18% more complex vacancy structures.","The results demonstrate that already the simplest irradiation provides selectivity for the defect types, and open the way for future experiments to explore changing the selectivity by modifying the irradiation parameters."],"url":"http://arxiv.org/abs/2404.07166v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-10 16:39:50","title":"Lost in Translation: Modern Neural Networks Still Struggle With Small Realistic Image Transformations","abstract":"Deep neural networks that achieve remarkable performance in image classification have previously been shown to be easily fooled by tiny transformations such as a one pixel translation of the input image. In order to address this problem, two approaches have been proposed in recent years. The first approach suggests using huge datasets together with data augmentation in the hope that a highly varied training set will teach the network to learn to be invariant. The second approach suggests using architectural modifications based on sampling theory to deal explicitly with image translations. In this paper, we show that these approaches still fall short in robustly handling 'natural' image translations that simulate a subtle change in camera orientation. Our findings reveal that a mere one-pixel translation can result in a significant change in the predicted image representation for approximately 40% of the test images in state-of-the-art models (e.g. open-CLIP trained on LAION-2B or DINO-v2) , while models that are explicitly constructed to be robust to cyclic translations can still be fooled with 1 pixel realistic (non-cyclic) translations 11% of the time. We present Robust Inference by Crop Selection: a simple method that can be proven to achieve any desired level of consistency, although with a modest tradeoff with the model's accuracy. Importantly, we demonstrate how employing this method reduces the ability to fool state-of-the-art models with a 1 pixel translation to less than 5% while suffering from only a 1% drop in classification accuracy. Additionally, we show that our method can be easy adjusted to deal with circular shifts as well. In such case we achieve 100% robustness to integer shifts with state-of-the-art accuracy, and with no need for any further training.","sentences":["Deep neural networks that achieve remarkable performance in image classification have previously been shown to be easily fooled by tiny transformations such as a one pixel translation of the input image.","In order to address this problem, two approaches have been proposed in recent years.","The first approach suggests using huge datasets together with data augmentation in the hope that a highly varied training set will teach the network to learn to be invariant.","The second approach suggests using architectural modifications based on sampling theory to deal explicitly with image translations.","In this paper, we show that these approaches still fall short in robustly handling 'natural' image translations that simulate a subtle change in camera orientation.","Our findings reveal that a mere one-pixel translation can result in a significant change in the predicted image representation for approximately 40% of the test images in state-of-the-art models (e.g. open-CLIP trained on LAION-2B or DINO-v2) , while models that are explicitly constructed to be robust to cyclic translations can still be fooled with 1 pixel realistic (non-cyclic) translations 11% of the time.","We present Robust Inference by Crop Selection: a simple method that can be proven to achieve any desired level of consistency, although with a modest tradeoff with the model's accuracy.","Importantly, we demonstrate how employing this method reduces the ability to fool state-of-the-art models with a 1 pixel translation to less than 5% while suffering from only a 1% drop in classification accuracy.","Additionally, we show that our method can be easy adjusted to deal with circular shifts as well.","In such case we achieve 100% robustness to integer shifts with state-of-the-art accuracy, and with no need for any further training."],"url":"http://arxiv.org/abs/2404.07153v1","category":"cs.CV"}
{"created":"2024-04-10 16:14:02","title":"Nonexistence of Courant-type nodal domain bounds for eigenfunctions of the Dirichlet-to-Neumann operator","abstract":"Given a compact manifold $\\mathcal M$ with boundary of dimension $n\\geq 3$ and any integers $K$ and $N$, we show that there exists a metric on $\\mathcal M$ for which the first $K$ nonconstant eigenfunctions of the Dirichlet-to-Neumann map on $\\partial\\mathcal M$ have at least $N$ nodal components. This provides a negative answer to the question of whether the number of nodal domains of Dirichlet-to-Neumann eigenfunctions satisfies a Courant-type bound, which has been featured in recent surveys by Girouard and Polterovich [21, Open problem 9] and by Colbois, Girouard, Gordon and Sher [9, Open question 10.14].","sentences":["Given a compact manifold $\\mathcal M$ with boundary of dimension $n\\geq 3$ and any integers $K$ and $N$, we show that there exists a metric on $\\mathcal M$ for which the first $K$ nonconstant eigenfunctions of the Dirichlet-to-Neumann map on $\\partial\\mathcal M$ have at least $N$ nodal components.","This provides a negative answer to the question of whether the number of nodal domains of Dirichlet-to-Neumann eigenfunctions satisfies a Courant-type bound, which has been featured in recent surveys by Girouard and Polterovich [21, Open problem 9] and by Colbois, Girouard, Gordon and Sher [9, Open question 10.14]."],"url":"http://arxiv.org/abs/2404.07138v1","category":"math.SP"}
{"created":"2024-04-10 16:07:29","title":"Learning of deep convolutional network image classifiers via stochastic gradient descent and over-parametrization","abstract":"Image classification from independent and identically distributed random variables is considered. Image classifiers are defined which are based on a linear combination of deep convolutional networks with max-pooling layer. Here all the weights are learned by stochastic gradient descent. A general result is presented which shows that the image classifiers are able to approximate the best possible deep convolutional network. In case that the a posteriori probability satisfies a suitable hierarchical composition model it is shown that the corresponding deep convolutional neural network image classifier achieves a rate of convergence which is independent of the dimension of the images.","sentences":["Image classification from independent and identically distributed random variables is considered.","Image classifiers are defined which are based on a linear combination of deep convolutional networks with max-pooling layer.","Here all the weights are learned by stochastic gradient descent.","A general result is presented which shows that the image classifiers are able to approximate the best possible deep convolutional network.","In case that the a posteriori probability satisfies a suitable hierarchical composition model it is shown that the corresponding deep convolutional neural network image classifier achieves a rate of convergence which is independent of the dimension of the images."],"url":"http://arxiv.org/abs/2404.07128v1","category":"math.ST"}
{"created":"2024-04-10 16:01:37","title":"Driver Attention Tracking and Analysis","abstract":"We propose a novel method to estimate a driver's points-of-gaze using a pair of ordinary cameras mounted on the windshield and dashboard of a car. This is a challenging problem due to the dynamics of traffic environments with 3D scenes of unknown depths. This problem is further complicated by the volatile distance between the driver and the camera system. To tackle these challenges, we develop a novel convolutional network that simultaneously analyzes the image of the scene and the image of the driver's face. This network has a camera calibration module that can compute an embedding vector that represents the spatial configuration between the driver and the camera system. This calibration module improves the overall network's performance, which can be jointly trained end to end.   We also address the lack of annotated data for training and evaluation by introducing a large-scale driving dataset with point-of-gaze annotations. This is an in situ dataset of real driving sessions in an urban city, containing synchronized images of the driving scene as well as the face and gaze of the driver. Experiments on this dataset show that the proposed method outperforms various baseline methods, having the mean prediction error of 29.69 pixels, which is relatively small compared to the $1280{\\times}720$ resolution of the scene camera.","sentences":["We propose a novel method to estimate a driver's points-of-gaze using a pair of ordinary cameras mounted on the windshield and dashboard of a car.","This is a challenging problem due to the dynamics of traffic environments with 3D scenes of unknown depths.","This problem is further complicated by the volatile distance between the driver and the camera system.","To tackle these challenges, we develop a novel convolutional network that simultaneously analyzes the image of the scene and the image of the driver's face.","This network has a camera calibration module that can compute an embedding vector that represents the spatial configuration between the driver and the camera system.","This calibration module improves the overall network's performance, which can be jointly trained end to end.   ","We also address the lack of annotated data for training and evaluation by introducing a large-scale driving dataset with point-of-gaze annotations.","This is an in situ dataset of real driving sessions in an urban city, containing synchronized images of the driving scene as well as the face and gaze of the driver.","Experiments on this dataset show that the proposed method outperforms various baseline methods, having the mean prediction error of 29.69 pixels, which is relatively small compared to the $1280{\\times}720$ resolution of the scene camera."],"url":"http://arxiv.org/abs/2404.07122v2","category":"cs.CV"}
{"created":"2024-04-10 15:42:25","title":"Fabrication Tolerant Multi-Layer Integrated Photonic Topology Optimization","abstract":"Optimal multi-layer device design requires consideration of fabrication uncertainties associated with inter-layer alignment and conformal layering. We present layer-restricted topology optimization (TO), a novel technique which mitigates the effects of unwanted conformal layering for multi-layer structures and enables TO in multi-etch material platforms. We explore several approaches to achieve this result compatible with density-based TO projection techniques and geometric constraints. Then, we present a robust TO formulation to design devices resilient to inter-layer misalignment. The novel constraint and robust formulation are demonstrated in 2D grating couplers and a 3D polarization rotator.","sentences":["Optimal multi-layer device design requires consideration of fabrication uncertainties associated with inter-layer alignment and conformal layering.","We present layer-restricted topology optimization (TO), a novel technique which mitigates the effects of unwanted conformal layering for multi-layer structures and enables TO in multi-etch material platforms.","We explore several approaches to achieve this result compatible with density-based TO projection techniques and geometric constraints.","Then, we present a robust TO formulation to design devices resilient to inter-layer misalignment.","The novel constraint and robust formulation are demonstrated in 2D grating couplers and a 3D polarization rotator."],"url":"http://arxiv.org/abs/2404.07104v1","category":"physics.optics"}
{"created":"2024-04-10 15:41:53","title":"Empowering AlphaFold2 for protein conformation selective drug discovery with AlphaFold2-RAVE","abstract":"Small molecule drug design hinges on obtaining co-crystallized ligand-protein structures. Despite AlphaFold2's strides in protein native structure prediction, its focus on apo structures overlooks ligands and associated holo structures. Moreover, designing selective drugs often benefits from the targeting of diverse metastable conformations. Therefore, direct application of AlphaFold2 models in virtual screening and drug discovery remains tentative. Here, we demonstrate an AlphaFold2 based framework combined with all-atom enhanced sampling molecular dynamics and induced fit docking, named AF2RAVE-Glide, to conduct computational model based small molecule binding of metastable protein kinase conformations, initiated from protein sequences. We demonstrate the AF2RAVE-Glide workflow on protein kinases and their inhibitors, with special emphasis on binding of known type II kinase inhibitors which target the metastable classical DFG-out state. These states are not easy to sample from AlphaFold2. Here we demonstrate how with AF2RAVE these metastable conformations can be sampled for different kinases with high enough accuracy to enable subsequent docking of known type II kinase inhibitors with more than 50% success rates across docking calculations. We believe the protocol should be deployable for other kinases and more proteins generally.","sentences":["Small molecule drug design hinges on obtaining co-crystallized ligand-protein structures.","Despite AlphaFold2's strides in protein native structure prediction, its focus on apo structures overlooks ligands and associated holo structures.","Moreover, designing selective drugs often benefits from the targeting of diverse metastable conformations.","Therefore, direct application of AlphaFold2 models in virtual screening and drug discovery remains tentative.","Here, we demonstrate an AlphaFold2 based framework combined with all-atom enhanced sampling molecular dynamics and induced fit docking, named AF2RAVE-Glide, to conduct computational model based small molecule binding of metastable protein kinase conformations, initiated from protein sequences.","We demonstrate the AF2RAVE-Glide workflow on protein kinases and their inhibitors, with special emphasis on binding of known type II kinase inhibitors which target the metastable classical DFG-out state.","These states are not easy to sample from AlphaFold2.","Here we demonstrate how with AF2RAVE these metastable conformations can be sampled for different kinases with high enough accuracy to enable subsequent docking of known type II kinase inhibitors with more than 50% success rates across docking calculations.","We believe the protocol should be deployable for other kinases and more proteins generally."],"url":"http://arxiv.org/abs/2404.07102v1","category":"physics.bio-ph"}
{"created":"2024-04-10 15:37:00","title":"Learning Priors for Non Rigid SfM from Casual Videos","abstract":"We tackle the long-standing challenge of reconstructing 3D structures and camera positions from videos. The problem is particularly hard when objects are transformed in a non-rigid way. Current approaches to this problem make unrealistic assumptions or require a long optimization time.   We present TracksTo4D, a novel deep learning-based approach that enables inferring 3D structure and camera positions from dynamic content originating from in-the-wild videos using a single feed-forward pass on a sparse point track matrix. To achieve this, we leverage recent advances in 2D point tracking and design an equivariant neural architecture tailored for directly processing 2D point tracks by leveraging their symmetries. TracksTo4D is trained on a dataset of in-the-wild videos utilizing only the 2D point tracks extracted from the videos, without any 3D supervision. Our experiments demonstrate that TracksTo4D generalizes well to unseen videos of unseen semantic categories at inference time, producing equivalent results to state-of-the-art methods while significantly reducing the runtime compared to other baselines.","sentences":["We tackle the long-standing challenge of reconstructing 3D structures and camera positions from videos.","The problem is particularly hard when objects are transformed in a non-rigid way.","Current approaches to this problem make unrealistic assumptions or require a long optimization time.   ","We present TracksTo4D, a novel deep learning-based approach that enables inferring 3D structure and camera positions from dynamic content originating from in-the-wild videos using a single feed-forward pass on a sparse point track matrix.","To achieve this, we leverage recent advances in 2D point tracking and design an equivariant neural architecture tailored for directly processing 2D point tracks by leveraging their symmetries.","TracksTo4D is trained on a dataset of in-the-wild videos utilizing only the 2D point tracks extracted from the videos, without any 3D supervision.","Our experiments demonstrate that TracksTo4D generalizes well to unseen videos of unseen semantic categories at inference time, producing equivalent results to state-of-the-art methods while significantly reducing the runtime compared to other baselines."],"url":"http://arxiv.org/abs/2404.07097v1","category":"cs.CV"}
{"created":"2024-04-10 15:35:27","title":"Particle Scattering and Fusion for the Ablowitz-Ladik Chain","abstract":"The Ablowitz-Ladik chain is an integrable discretized version of the nonlinear Schr\\\"{o}dinger equation. We report on a novel underlying Hamiltonian particle system with properties similar to the ones known for the classical Toda chain and Calogero fluid with $1/\\sinh^2$ pair interaction. Boundary conditions are imposed such that, both in the distant past and future, particles have a constant velocity. We establish the many-particle scattering for the Ablowitz-Ladik chain and obtain properties known for generic integrable many-body systems. For a specific choice of the chain, real initial data remain real in the course of time. Then, asymptotically, particles move in pairs with a velocity-dependent size and scattering shifts are governed by the fusion rule.","sentences":["The Ablowitz-Ladik chain is an integrable discretized version of the nonlinear Schr\\\"{o}dinger equation.","We report on a novel underlying Hamiltonian particle system with properties similar to the ones known for the classical Toda chain and Calogero fluid with $1/\\sinh^2$ pair interaction.","Boundary conditions are imposed such that, both in the distant past and future, particles have a constant velocity.","We establish the many-particle scattering for the Ablowitz-Ladik chain and obtain properties known for generic integrable many-body systems.","For a specific choice of the chain, real initial data remain real in the course of time.","Then, asymptotically, particles move in pairs with a velocity-dependent size and scattering shifts are governed by the fusion rule."],"url":"http://arxiv.org/abs/2404.07095v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-10 15:18:29","title":"Mid-Infrared Spectrum of the Disk around the Forming Companion GQ Lup B Revealed by JWST/MIRI","abstract":"GQ Lup B is a forming brown dwarf companion ($M\\sim10-30~M_J$) showing evidence for an infrared excess associated with a disk surronding the companion itself. Here we present mid-infrared (MIR) observations of GQ Lup B with the Medium Resolution Spectrograph (MRS) on JWST, spanning $4.8-11.7~\\mu$m. We remove the stellar contamination using reference differential imaging based on principal component analysis (PCA), demonstrating that the MRS can perform high-contrast science. Our observations provide a sensitive probe of the disk surrounding GQ Lup B. We find no sign of a silicate feature, similar to other disk surrounding very low mass objects, which likely implies significant grain growth ($a_{\\mathrm{min}}\\gtrsim5~\\mu$m), and potentially dust settling. Additionally, we find that if the emission is dominated by an inner wall, the disk around the companion might have an inner cavity larger than the one set by sublimation. Conversely, if our data probe the emission from a thin flat disk, we find the disk to be very compact. More observations are required to confirm this finding and assess the vertical structure of the disk. This approach paves the path to the future study of circumplanetary disks and their physical properties. Our results demonstrate that MIR spectroscopic observations can reveal the physical characteristics of disks around forming companions, providing unique insights into the formation of giant planets, brown dwarfs and their satellites.","sentences":["GQ Lup B is a forming brown dwarf companion ($M\\sim10-30~M_J$) showing evidence for an infrared excess associated with a disk surronding the companion itself.","Here we present mid-infrared (MIR) observations of GQ Lup B with the Medium Resolution Spectrograph (MRS) on JWST, spanning $4.8-11.7~\\mu$m.","We remove the stellar contamination using reference differential imaging based on principal component analysis (PCA), demonstrating that the MRS can perform high-contrast science.","Our observations provide a sensitive probe of the disk surrounding GQ Lup B. We find no sign of a silicate feature, similar to other disk surrounding very low mass objects, which likely implies significant grain growth ($a_{\\mathrm{min}}\\gtrsim5~\\mu$m), and potentially dust settling.","Additionally, we find that if the emission is dominated by an inner wall, the disk around the companion might have an inner cavity larger than the one set by sublimation.","Conversely, if our data probe the emission from a thin flat disk, we find the disk to be very compact.","More observations are required to confirm this finding and assess the vertical structure of the disk.","This approach paves the path to the future study of circumplanetary disks and their physical properties.","Our results demonstrate that MIR spectroscopic observations can reveal the physical characteristics of disks around forming companions, providing unique insights into the formation of giant planets, brown dwarfs and their satellites."],"url":"http://arxiv.org/abs/2404.07086v1","category":"astro-ph.EP"}
{"created":"2024-04-10 15:16:04","title":"Minimizing Chebyshev Prototype Risk Magically Mitigates the Perils of Overfitting","abstract":"Overparameterized deep neural networks (DNNs), if not sufficiently regularized, are susceptible to overfitting their training examples and not generalizing well to test data. To discourage overfitting, researchers have developed multicomponent loss functions that reduce intra-class feature correlation and maximize inter-class feature distance in one or more layers of the network. By analyzing the penultimate feature layer activations output by a DNN's feature extraction section prior to the linear classifier, we find that modified forms of the intra-class feature covariance and inter-class prototype separation are key components of a fundamental Chebyshev upper bound on the probability of misclassification, which we designate the Chebyshev Prototype Risk (CPR). While previous approaches' covariance loss terms scale quadratically with the number of network features, our CPR bound indicates that an approximate covariance loss in log-linear time is sufficient to reduce the bound and is scalable to large architectures. We implement the terms of the CPR bound into our Explicit CPR (exCPR) loss function and observe from empirical results on multiple datasets and network architectures that our training algorithm reduces overfitting and improves upon previous approaches in many settings. Our code is available at https://github.com/Deano1718/Regularization_exCPR .","sentences":["Overparameterized deep neural networks (DNNs), if not sufficiently regularized, are susceptible to overfitting their training examples and not generalizing well to test data.","To discourage overfitting, researchers have developed multicomponent loss functions that reduce intra-class feature correlation and maximize inter-class feature distance in one or more layers of the network.","By analyzing the penultimate feature layer activations output by a DNN's feature extraction section prior to the linear classifier, we find that modified forms of the intra-class feature covariance and inter-class prototype separation are key components of a fundamental Chebyshev upper bound on the probability of misclassification, which we designate the Chebyshev Prototype Risk (CPR).","While previous approaches' covariance loss terms scale quadratically with the number of network features, our CPR bound indicates that an approximate covariance loss in log-linear time is sufficient to reduce the bound and is scalable to large architectures.","We implement the terms of the CPR bound into our Explicit CPR (exCPR) loss function and observe from empirical results on multiple datasets and network architectures that our training algorithm reduces overfitting and improves upon previous approaches in many settings.","Our code is available at https://github.com/Deano1718/Regularization_exCPR ."],"url":"http://arxiv.org/abs/2404.07083v2","category":"cs.LG"}
{"created":"2024-04-10 14:52:54","title":"A random matrix model for the density of states of jammed soft spheres with applied stress","abstract":"We investigate the addition of applied stress to a random block matrix model introduced by Parisi to study the Hessian matrix of soft spheres near the jamming point. In the infinite dimensional limit the applied stress translates the spectral distribution to the left, leading to a stability constraint. With negative stress, as in the case of a random network of stretched elastic springs, the spectral distribution is translated to the right, and the density of states has a peak before the plateau.","sentences":["We investigate the addition of applied stress to a random block matrix model introduced by Parisi to study the Hessian matrix of soft spheres near the jamming point.","In the infinite dimensional limit the applied stress translates the spectral distribution to the left, leading to a stability constraint.","With negative stress, as in the case of a random network of stretched elastic springs, the spectral distribution is translated to the right, and the density of states has a peak before the plateau."],"url":"http://arxiv.org/abs/2404.07064v1","category":"cond-mat.dis-nn"}
{"created":"2024-04-10 14:50:43","title":"A Tight $O(4^k/p_c)$ Runtime Bound for a ($\u03bc$+1) GA on Jump$_k$ for Realistic Crossover Probabilities","abstract":"The Jump$_k$ benchmark was the first problem for which crossover was proven to give a speedup over mutation-only evolutionary algorithms. Jansen and Wegener (2002) proved an upper bound of $O({\\rm poly}(n) + 4^k/p_c)$ for the ($\\mu$+1)~Genetic Algorithm ($(\\mu+1)$ GA), but only for unrealistically small crossover probabilities $p_c$. To this date, it remains an open problem to prove similar upper bounds for realistic~$p_c$; the best known runtime bound for $p_c = \\Omega(1)$ is $O((n/\\chi)^{k-1})$, $\\chi$ a positive constant. Using recently developed techniques, we analyse the evolution of the population diversity, measured as sum of pairwise Hamming distances, for a variant of the \\muga on Jump$_k$. We show that population diversity converges to an equilibrium of near-perfect diversity. This yields an improved and tight time bound of $O(\\mu n \\log(k) + 4^k/p_c)$ for a range of~$k$ under the mild assumptions $p_c = O(1/k)$ and $\\mu \\in \\Omega(kn)$. For all constant~$k$ the restriction is satisfied for some $p_c = \\Omega(1)$. Our work partially solves a problem that has been open for more than 20 years.","sentences":["The Jump$_k$ benchmark was the first problem for which crossover was proven to give a speedup over mutation-only evolutionary algorithms.","Jansen and Wegener (2002) proved an upper bound of $O({\\rm poly}(n)","+ 4^k/p_c)$ for the ($\\mu$+1)~Genetic Algorithm ($(\\mu+1)$ GA), but only for unrealistically small crossover probabilities $p_c$. To this date, it remains an open problem to prove similar upper bounds for realistic~$p_c$; the best known runtime bound for $p_c = \\Omega(1)$ is $O((n/\\chi)^{k-1})$, $\\chi$ a positive constant.","Using recently developed techniques, we analyse the evolution of the population diversity, measured as sum of pairwise Hamming distances, for a variant of the \\muga on Jump$_k$. We show that population diversity converges to an equilibrium of near-perfect diversity.","This yields an improved and tight time bound of $O(\\mu n \\log(k) + 4^k/p_c)$ for a range of~$k$ under the mild assumptions $p_c","= O(1/k)$ and $\\mu \\in \\Omega(kn)$. For all constant~$k$ the restriction is satisfied for some $p_c = \\Omega(1)$. Our work partially solves a problem that has been open for more than 20 years."],"url":"http://arxiv.org/abs/2404.07061v1","category":"cs.NE"}
{"created":"2024-04-10 14:46:40","title":"Inner-extremal regular black holes from pure gravity","abstract":"Recently it was shown that essentially all regular black hole models constructed so far can be obtained as solutions of vacuum gravity equations, upon considering an infinite series of quasi-topological higher curvature corrections. Here we show that such a construction can be upgraded to yield regular black holes with vanishing inner horizon surface gravity. In four dimensions, such a condition is necessary for the absence of classical instabilities associated with mass inflation on the inner horizon.","sentences":["Recently it was shown that essentially all regular black hole models constructed so far can be obtained as solutions of vacuum gravity equations, upon considering an infinite series of quasi-topological higher curvature corrections.","Here we show that such a construction can be upgraded to yield regular black holes with vanishing inner horizon surface gravity.","In four dimensions, such a condition is necessary for the absence of classical instabilities associated with mass inflation on the inner horizon."],"url":"http://arxiv.org/abs/2404.07058v1","category":"gr-qc"}
{"created":"2024-04-10 14:45:56","title":"Quantum Isotropic Universe in RQM Analogy: the Cosmological Horizon","abstract":"We investigate the quantum dynamics of the isotropic Universe in the presence of a free massless scalar field, playing the role of a physical clock. The Hilbert space is constructed via a direct analogy between the Wheeler-DeWitt equation in the minisuperspace and a relativistic scalar one in physical space. In particular, we show how the introduction of a \"turning point\" in the Universe evolution allows to overcome an intrinsic ambiguity in representing the expanding and collapsing Universe. In this way, the positive and negative frequencies are simply identified with time reversed states. The main subject of the present analysis is the construction of a horizon operator, whose quantum behavior is investigated when Polymer Quantum Mechanics is implemented to describe the asymptotic evolution near the initial singularity. The reason of this choice is motivated by the intrinsic spreading of localized wavepackets when the polymer dispersion relation governs the quantum dynamics. The evidence that the mean value of the quantum horizon operator follows its semiclassical behavior (corrected for polymerization) is a clear indication that a concept of causality can be restored also in the quantum cosmological picture.","sentences":["We investigate the quantum dynamics of the isotropic Universe in the presence of a free massless scalar field, playing the role of a physical clock.","The Hilbert space is constructed via a direct analogy between the Wheeler-DeWitt equation in the minisuperspace and a relativistic scalar one in physical space.","In particular, we show how the introduction of a \"turning point\" in the Universe evolution allows to overcome an intrinsic ambiguity in representing the expanding and collapsing Universe.","In this way, the positive and negative frequencies are simply identified with time reversed states.","The main subject of the present analysis is the construction of a horizon operator, whose quantum behavior is investigated when Polymer Quantum Mechanics is implemented to describe the asymptotic evolution near the initial singularity.","The reason of this choice is motivated by the intrinsic spreading of localized wavepackets when the polymer dispersion relation governs the quantum dynamics.","The evidence that the mean value of the quantum horizon operator follows its semiclassical behavior (corrected for polymerization) is a clear indication that a concept of causality can be restored also in the quantum cosmological picture."],"url":"http://arxiv.org/abs/2404.07056v1","category":"gr-qc"}
{"created":"2024-04-10 14:38:58","title":"Towards Learning Stochastic Population Models by Gradient Descent","abstract":"Increasing effort is put into the development of methods for learning mechanistic models from data. This task entails not only the accurate estimation of parameters, but also a suitable model structure. Recent work on the discovery of dynamical systems formulates this problem as a linear equation system. Here, we explore several simulation-based optimization approaches, which allow much greater freedom in the objective formulation and weaker conditions on the available data. We show that even for relatively small stochastic population models, simultaneous estimation of parameters and structure poses major challenges for optimization procedures. Particularly, we investigate the application of the local stochastic gradient descent method, commonly used for training machine learning models. We demonstrate accurate estimation of models but find that enforcing the inference of parsimonious, interpretable models drastically increases the difficulty. We give an outlook on how this challenge can be overcome.","sentences":["Increasing effort is put into the development of methods for learning mechanistic models from data.","This task entails not only the accurate estimation of parameters, but also a suitable model structure.","Recent work on the discovery of dynamical systems formulates this problem as a linear equation system.","Here, we explore several simulation-based optimization approaches, which allow much greater freedom in the objective formulation and weaker conditions on the available data.","We show that even for relatively small stochastic population models, simultaneous estimation of parameters and structure poses major challenges for optimization procedures.","Particularly, we investigate the application of the local stochastic gradient descent method, commonly used for training machine learning models.","We demonstrate accurate estimation of models but find that enforcing the inference of parsimonious, interpretable models drastically increases the difficulty.","We give an outlook on how this challenge can be overcome."],"url":"http://arxiv.org/abs/2404.07049v1","category":"cs.LG"}
{"created":"2024-04-10 14:37:31","title":"Four-fifths laws in electron and Hall magnetohydrodynamic fluids: Energy, Magnetic helicity and Generalized helicity","abstract":"This paper examines the Kolmogorov type laws of conserved quantities in the electron and Hall magnetohydrodynamic fluids. Inspired by Eyink's longitudinal structure functions and recent progress in classical MHD equations, we derive four-fifths laws for energy, magnetic helicity and generalized helicity in these systems.","sentences":["This paper examines the Kolmogorov type laws of conserved quantities in the electron and Hall magnetohydrodynamic fluids.","Inspired by Eyink's longitudinal structure functions and recent progress in classical MHD equations, we derive four-fifths laws for energy, magnetic helicity and generalized helicity in these systems."],"url":"http://arxiv.org/abs/2404.07047v1","category":"math.AP"}
{"created":"2024-04-10 14:32:01","title":"A note on spectral theory of integral-functional Volterra operators","abstract":"A concise overview of the spectral theory of integral-functional operators is provided. In the context of analysis, a technique is described for deriving solutions to equations involving operators in a closed form. A constructive theorem has been established, outlining a procedure for determining the eigenvalues and eigenfunctions of these operators. Based on this foundation, an analytical approach for generating solutions to a Volterra-type integro-functional inhomogeneous equation is proposed.","sentences":["A concise overview of the spectral theory of integral-functional operators is provided.","In the context of analysis, a technique is described for deriving solutions to equations involving operators in a closed form.","A constructive theorem has been established, outlining a procedure for determining the eigenvalues and eigenfunctions of these operators.","Based on this foundation, an analytical approach for generating solutions to a Volterra-type integro-functional inhomogeneous equation is proposed."],"url":"http://arxiv.org/abs/2404.07041v1","category":"math.DS"}
{"created":"2024-04-10 14:18:20","title":"Anisotropy ansatz for the axisymmetric Jeans equations","abstract":"The Jeans equations do not form a closed system, and to solve them a parametrization relating the velocity moments is often adopted. For axisymmetric models, a phenomenological choice (the \"$b$-ansatz\") is widely used for the relation between the vertical ($\\sigma_z^2$) and radial ($\\sigma_R^2$) components of the velocity dispersion tensor, thus breaking their identity present in two-integral systems. However, the way in which the ansatz affects the resulting kinematical fields can be quite complicated, so that the analysis of these fields is usually performed only after numerically computing them. We present here a general procedure to study the properties of the ansatz-dependent fields $\\overline{v_{\\varphi}^2}$, $\\Delta = \\overline{v_{\\varphi}^2} - \\sigma_z^2$ and $\\Delta_R = \\overline{v_{\\varphi}^2} - \\sigma_R^2$. Specifically, the effects of the $b$-ansatz can be determined before solving the Jeans equations once the behaviour over the ($R,z$)-plane of three easy-to-build ansatz-independent functions is known. The procedure also constrains the ansatz to exclude unphysical results (as a negative $\\overline{v_{\\varphi}^2}$). The method is illustrated by discussing the cases of three well-known galaxy models: the Miyamoto & Nagai and Satoh disks, and the Binney logarithmic halo, for which the regions and the constraints on the ansatz values can be determined analytically; a two-component (Miyamoto & Nagai plus logarithmic halo) model is also discussed.","sentences":["The Jeans equations do not form a closed system, and to solve them a parametrization relating the velocity moments is often adopted.","For axisymmetric models, a phenomenological choice (the \"$b$-ansatz\") is widely used for the relation between the vertical ($\\sigma_z^2$) and radial ($\\sigma_R^2$) components of the velocity dispersion tensor, thus breaking their identity present in two-integral systems.","However, the way in which the ansatz affects the resulting kinematical fields can be quite complicated, so that the analysis of these fields is usually performed only after numerically computing them.","We present here a general procedure to study the properties of the ansatz-dependent fields $\\overline{v_{\\varphi}^2}$, $\\Delta = \\overline{v_{\\varphi}^2} - \\sigma_z^2$ and $\\Delta_R = \\overline{v_{\\varphi}^2} - \\sigma_R^2$. Specifically, the effects of the $b$-ansatz can be determined before solving the Jeans equations once the behaviour over the ($R,z$)-plane of three easy-to-build ansatz-independent functions is known.","The procedure also constrains the ansatz to exclude unphysical results (as a negative $\\overline{v_{\\varphi}^2}$).","The method is illustrated by discussing the cases of three well-known galaxy models: the Miyamoto & Nagai and Satoh disks, and the Binney logarithmic halo, for which the regions and the constraints on the ansatz values can be determined analytically; a two-component (Miyamoto & Nagai plus logarithmic halo) model is also discussed."],"url":"http://arxiv.org/abs/2404.07023v1","category":"astro-ph.GA"}
{"created":"2024-04-10 14:04:53","title":"Variational Quantum Crank-Nicolson and Method of Lines for the Solution of Initial Value Problems","abstract":"In this paper we use a Variational Quantum Algorithm to solve Initial Value Problems with the Implicit Crank-Nicolson and the Method of Lines evolution schemes. The unknown functions use a spectral decomposition with the Fourier basis. The examples developed to illustrate the implementation are the advection equation, the wave equation written as a first order system of coupled equations and the viscous Burgers equation as a non-linear case. The problems are solved using: i) standard Finite Differences as the solution to compare with, ii) the State Vector Formalism (SVF), and iii) the Sampling Error Formalism (SEF). Our results for these equations show that the SVF provides convergent solutions whereas those constructed with the SEF are not consistent with the increase of resolution. Byproducts of our implementation include the construction of cost functions for the two evolution schemes and an efficient method to simulate the SVF and SEF in classical computers.","sentences":["In this paper we use a Variational Quantum Algorithm to solve Initial Value Problems with the Implicit Crank-Nicolson and the Method of Lines evolution schemes.","The unknown functions use a spectral decomposition with the Fourier basis.","The examples developed to illustrate the implementation are the advection equation, the wave equation written as a first order system of coupled equations and the viscous Burgers equation as a non-linear case.","The problems are solved using: i) standard Finite Differences as the solution to compare with, ii) the State Vector Formalism (SVF), and iii) the Sampling Error Formalism (SEF).","Our results for these equations show that the SVF provides convergent solutions whereas those constructed with the SEF are not consistent with the increase of resolution.","Byproducts of our implementation include the construction of cost functions for the two evolution schemes and an efficient method to simulate the SVF and SEF in classical computers."],"url":"http://arxiv.org/abs/2404.07016v1","category":"quant-ph"}
{"created":"2024-04-10 13:58:43","title":"Numerical approximation of SDEs driven by fractional Brownian motion for all $H\\in(0,1)$ using WIS integration","abstract":"We examine the numerical approximation of a quasilinear stochastic differential equation (SDE) with multiplicative fractional Brownian motion. The stochastic integral is interpreted in the Wick-It\\^o-Skorohod (WIS) sense that is well defined and centered for all $H\\in(0,1)$. We give an introduction to the theory of WIS integration before we examine existence and uniqueness of a solution to the SDE. We then introduce our numerical method which is based on the theoretical results in \\cite{Mishura2008article, Mishura2008} for $H\\geq \\frac{1}{2}$. We construct explicitly a translation operator required for the practical implementation of the method and are not aware of any other implementation of a numerical method for the WIS SDE. We then prove a strong convergence result that gives, in the non-autonomous case, an error of $O(\\Delta t^H)$ and in the non-autonomous case $O(\\Delta t^{\\min(H,\\zeta)})$, where $\\zeta$ is a H\\\"older continuity parameter. We present some numerical experiments and conjecture that the theoretical results may not be optimal since we observe numerically a rate of $\\min(H+\\frac{1}{2},1)$ in the autonomous case. This work opens up the possibility to efficiently simulate SDEs for all $H$ values, including small values of $H$ when the stochastic integral is interpreted in the WIS sense.","sentences":["We examine the numerical approximation of a quasilinear stochastic differential equation (SDE) with multiplicative fractional Brownian motion.","The stochastic integral is interpreted in the Wick-It\\^o-Skorohod (WIS) sense that is well defined and centered for all $H\\in(0,1)$.","We give an introduction to the theory of WIS integration before we examine existence and uniqueness of a solution to the SDE.","We then introduce our numerical method which is based on the theoretical results in \\cite{Mishura2008article, Mishura2008} for $H\\geq \\frac{1}{2}$. We construct explicitly a translation operator required for the practical implementation of the method and are not aware of any other implementation of a numerical method for the WIS SDE.","We then prove a strong convergence result that gives, in the non-autonomous case, an error of $O(\\Delta t^H)$ and in the non-autonomous case $O(\\Delta t^{\\min(H,\\zeta)})$, where $\\zeta$ is a H\\\"older continuity parameter.","We present some numerical experiments and conjecture that the theoretical results may not be optimal since we observe numerically a rate of $\\min(H+\\frac{1}{2},1)$ in the autonomous case.","This work opens up the possibility to efficiently simulate SDEs for all $H$ values, including small values of $H$ when the stochastic integral is interpreted in the WIS sense."],"url":"http://arxiv.org/abs/2404.07013v1","category":"math.NA"}
{"created":"2024-04-10 13:32:09","title":"Deformations of the scalar curvature of a partially integrable pseudohermitian manifold","abstract":"We consider deformations of the scalar curvature of a partially integrable pseudohermitian manifold, in analogy with the work of Fischer and Marsden on Riemannian manifolds. In particular, we introduce and discuss $R$-singular spaces, give sufficient conditions for the stability of the scalar curvature, and give a partial infinitesimal rigidity result for the scalar curvature of a compact, torsion-free, scalar-flat, integrable pseudohermitian manifold.","sentences":["We consider deformations of the scalar curvature of a partially integrable pseudohermitian manifold, in analogy with the work of Fischer and Marsden on Riemannian manifolds.","In particular, we introduce and discuss $R$-singular spaces, give sufficient conditions for the stability of the scalar curvature, and give a partial infinitesimal rigidity result for the scalar curvature of a compact, torsion-free, scalar-flat, integrable pseudohermitian manifold."],"url":"http://arxiv.org/abs/2404.07002v1","category":"math.DG"}
{"created":"2024-04-10 13:30:09","title":"On quantum Floquet theorem","abstract":"We consider the Schr\\\"odinger equation $ih\\partial_t\\psi = H\\psi$, $\\psi=\\psi(\\cdot,t)\\in L^2({\\mathbb T})$. The operator $H = -\\partial^2_x + V(x,t)$ includes smooth potential $V$, which is assumed to be time $T$-periodic. Let $W=W(t)$ be the fundamental solution of this linear ODE system on $L^2({\\mathbb T})$. Then according to terminology from Lyapunov-Floquet theory, ${\\cal M}=W(T)$ is the monodromy operator. We prove that ${\\cal M}$ is unitarily conjugated to $\\exp\\big(-\\frac{T}{ih} \\partial^2_x\\big) + {\\cal C}$, where ${\\cal C}$ is a compact operator with an arbitrarily small norm.","sentences":["We consider the Schr\\\"odinger equation $ih\\partial_t\\psi = H\\psi$, $\\psi=\\psi(\\cdot,t)\\in L^2({\\mathbb T})$.","The operator $H = -\\partial^2_x + V(x,t)$ includes smooth potential $V$, which is assumed to be time $T$-periodic.","Let $W=W(t)$ be the fundamental solution of this linear ODE system on $L^2({\\mathbb T})$.","Then according to terminology from Lyapunov-Floquet theory, ${\\cal M}=W(T)$ is the monodromy operator.","We prove that ${\\cal M}$ is unitarily conjugated to $\\exp\\big(-\\frac{T}{ih} \\partial^2_x\\big)","+ {\\cal C}$, where ${\\cal C}$ is a compact operator with an arbitrarily small norm."],"url":"http://arxiv.org/abs/2404.06999v1","category":"math.DS"}
{"created":"2024-04-10 13:10:52","title":"Ray-driven Spectral CT Reconstruction Based on Neural Base-Material Fields","abstract":"In spectral CT reconstruction, the basis materials decomposition involves solving a large-scale nonlinear system of integral equations, which is highly ill-posed mathematically. This paper proposes a model that parameterizes the attenuation coefficients of the object using a neural field representation, thereby avoiding the complex calculations of pixel-driven projection coefficient matrices during the discretization process of line integrals. It introduces a lightweight discretization method for line integrals based on a ray-driven neural field, enhancing the accuracy of the integral approximation during the discretization process. The basis materials are represented as continuous vector-valued implicit functions to establish a neural field parameterization model for the basis materials. The auto-differentiation framework of deep learning is then used to solve the implicit continuous function of the neural base-material fields. This method is not limited by the spatial resolution of reconstructed images, and the network has compact and regular properties. Experimental validation shows that our method performs exceptionally well in addressing the spectral CT reconstruction. Additionally, it fulfils the requirements for the generation of high-resolution reconstruction images.","sentences":["In spectral CT reconstruction, the basis materials decomposition involves solving a large-scale nonlinear system of integral equations, which is highly ill-posed mathematically.","This paper proposes a model that parameterizes the attenuation coefficients of the object using a neural field representation, thereby avoiding the complex calculations of pixel-driven projection coefficient matrices during the discretization process of line integrals.","It introduces a lightweight discretization method for line integrals based on a ray-driven neural field, enhancing the accuracy of the integral approximation during the discretization process.","The basis materials are represented as continuous vector-valued implicit functions to establish a neural field parameterization model for the basis materials.","The auto-differentiation framework of deep learning is then used to solve the implicit continuous function of the neural base-material fields.","This method is not limited by the spatial resolution of reconstructed images, and the network has compact and regular properties.","Experimental validation shows that our method performs exceptionally well in addressing the spectral CT reconstruction.","Additionally, it fulfils the requirements for the generation of high-resolution reconstruction images."],"url":"http://arxiv.org/abs/2404.06991v1","category":"eess.IV"}
{"created":"2024-04-10 13:01:52","title":"Friedmann equations of the fractal apparent horizon","abstract":"From a fractal perspective, the entropy bound of gravitational systems undergoes changes. Furthermore, in the cosmological setting, the conservation law of a perfect fluid is also altered in such systems, affecting spatial elements like volume, area, and radius. By applying the first law of thermodynamics and deriving the Friedmann equations, we can gain insight into the evolution of such a fractal cosmos. However, observations continue to necessitate the existence of a dark energy source. To address this, in this article, we have created a novel fractal $\\Lambda$CDM cosmological model and determined the fractal cosmological observables. We show that the spatial fractal dimension is two, and the age of the Universe is 13.91 Gyr, by fitting the model's parameters to cosmological data.","sentences":["From a fractal perspective, the entropy bound of gravitational systems undergoes changes.","Furthermore, in the cosmological setting, the conservation law of a perfect fluid is also altered in such systems, affecting spatial elements like volume, area, and radius.","By applying the first law of thermodynamics and deriving the Friedmann equations, we can gain insight into the evolution of such a fractal cosmos.","However, observations continue to necessitate the existence of a dark energy source.","To address this, in this article, we have created a novel fractal $\\Lambda$CDM cosmological model and determined the fractal cosmological observables.","We show that the spatial fractal dimension is two, and the age of the Universe is 13.91 Gyr, by fitting the model's parameters to cosmological data."],"url":"http://arxiv.org/abs/2404.06986v1","category":"gr-qc"}
{"created":"2024-04-10 12:52:09","title":"A priori regularity estimates for equations degenerating on nodal sets","abstract":"We prove $\\textit{a priori}$ and $\\textit{a posteriori}$ H\\\"older bounds and Schauder $C^{1,\\alpha}$ estimates for continuous solutions to singular/degenerate equations with variable coefficients of type $$ \\mathrm{div}\\left(|u|^a A\\nabla w\\right)=0\\qquad\\mathrm{in \\ }\\Omega\\subset\\mathbb{R}^n, $$ where the weight $u$ solves an elliptic equation of type $\\mathrm{div}\\left(A\\nabla u\\right)=0$ with a Lipschitz-continuous and uniformly elliptic matrix $A$ and has a nontrivial, possibly singular, nodal set. Such estimates are uniform with respect to $u$ in a class of normalized solutions having bounded Almgren's frequency. More precisely, we provide $\\textit{a priori}$ H\\\"{o}lder bounds in any space dimension, and Schauder estimates when $n=2$. When $a=2$, the results apply to the ratios of two solutions to the same PDE sharing their zero sets. Then, one can infer higher order boundary Harnack principles on nodal domains by applying the Schauder estimates for solutions to the auxiliary degenerate equation. The results are based upon a fine blow-up argument, Liouville theorems and quasiconformal maps.","sentences":["We prove $\\textit{a priori}$ and $\\textit{a posteriori}$ H\\\"older bounds and Schauder $C^{1,\\alpha}$ estimates for continuous solutions to singular/degenerate equations with variable coefficients of type $$ \\mathrm{div}\\left(|u|^a A\\nabla w\\right)=0\\qquad\\mathrm{in","\\ }\\Omega\\subset\\mathbb{R}^n, $$ where the weight $u$ solves an elliptic equation of type $\\mathrm{div}\\left(A\\nabla u\\right)=0$ with a Lipschitz-continuous and uniformly elliptic matrix $A$ and has a nontrivial, possibly singular, nodal set.","Such estimates are uniform with respect to $u$ in a class of normalized solutions having bounded Almgren's frequency.","More precisely, we provide $\\textit{a priori}$ H\\\"{o}lder bounds in any space dimension, and Schauder estimates when $n=2$. When $a=2$, the results apply to the ratios of two solutions to the same PDE sharing their zero sets.","Then, one can infer higher order boundary Harnack principles on nodal domains by applying the Schauder estimates for solutions to the auxiliary degenerate equation.","The results are based upon a fine blow-up argument, Liouville theorems and quasiconformal maps."],"url":"http://arxiv.org/abs/2404.06980v1","category":"math.AP"}
{"created":"2024-04-10 12:49:31","title":"Tidal Love numbers and approximate universal relations for fermion soliton stars","abstract":"Fermion soliton stars are a consistent model of exotic compact objects which involve a nonlinear interaction between a real scalar field and fermions through a Yukawa term. This interaction results in an effective fermion mass that depends upon the vacuum structure in the scalar potential. In this work we investigate the tidal deformations of fermion soliton stars and compute the corresponding tidal Love numbers for different model parameters. Furthermore, we discuss the existence of approximate universal relations for the electric and magnetic tidal deformabilities of these stars, and compare them with other solutions of general relativity, such as neutron stars or boson stars. These relations for fermion soliton stars are less universal than for neutron stars, but they are sufficiently different from the ordinary neutron star case that a measurement of the electric and magnetic tidal Love numbers (as potentially achievable by next-generation gravitational wave detectors) can be used to disentangle these families of compact objects. Finally, we discuss the conditions for tidal disruption of fermion soliton stars in a binary system and estimate the detectability of the electromagnetic signal associated with such tidal disruption events.","sentences":["Fermion soliton stars are a consistent model of exotic compact objects which involve a nonlinear interaction between a real scalar field and fermions through a Yukawa term.","This interaction results in an effective fermion mass that depends upon the vacuum structure in the scalar potential.","In this work we investigate the tidal deformations of fermion soliton stars and compute the corresponding tidal Love numbers for different model parameters.","Furthermore, we discuss the existence of approximate universal relations for the electric and magnetic tidal deformabilities of these stars, and compare them with other solutions of general relativity, such as neutron stars or boson stars.","These relations for fermion soliton stars are less universal than for neutron stars, but they are sufficiently different from the ordinary neutron star case that a measurement of the electric and magnetic tidal Love numbers (as potentially achievable by next-generation gravitational wave detectors) can be used to disentangle these families of compact objects.","Finally, we discuss the conditions for tidal disruption of fermion soliton stars in a binary system and estimate the detectability of the electromagnetic signal associated with such tidal disruption events."],"url":"http://arxiv.org/abs/2404.06979v1","category":"gr-qc"}
{"created":"2024-04-10 12:26:35","title":"Transcendence properties of the Artin-Hasse exponential modulo $p$","abstract":"Let $E_p(x)$ denote the Artin-Hasse exponential and let $\\overline{E}_p(x)$ denote its reduction modulo $p$ in $\\mathbb{F}_p[[x]]$. In this article we study transcendence properties of $\\overline{E}_p(x)$ over $\\mathbb{F}_p[x]$. We give two proofs that $\\overline{E}_p(x)$ is transcendental, affirmatively answering a question of Thakur. We also prove algebraic independence results: i) for $f_1,\\dots,f_r \\in x\\mathbb{F}_p[x]$ satisfying certain linear independence properties, we show that the $\\overline{E}_p(f_1), \\dots, \\overline{E}_p(f_r)$ are algebraically independent over $\\mathbb{F}_p[x]$ and ii) we determine the algebraic relations between $\\overline{E}_p(cx)$, where $c \\in \\mathbb{F}_p^\\times$. Our proof studies the higher derivatives of $\\overline{E}_p(x)$ and makes use of iterative differential Galois theory.","sentences":["Let $E_p(x)$ denote the Artin-Hasse exponential and let $\\overline{E}_p(x)$ denote its reduction modulo $p$ in $\\mathbb{F}_p[[x]]$. In this article we study transcendence properties of $\\overline{E}_p(x)$ over $\\mathbb{F}_p[x]$. We give two proofs that $\\overline{E}_p(x)$ is transcendental, affirmatively answering a question of Thakur.","We also prove algebraic independence results: i) for $f_1,\\dots,f_r \\in x\\mathbb{F}_p[x]$ satisfying certain linear independence properties, we show that the $\\overline{E}_p(f_1), \\dots, \\overline{E}_p(f_r)$ are algebraically independent over $\\mathbb{F}_p[x]$ and ii) we determine the algebraic relations between $\\overline{E}_p(cx)$, where $c \\in \\mathbb{F}_p^\\times$.","Our proof studies the higher derivatives of $\\overline{E}_p(x)$ and makes use of iterative differential Galois theory."],"url":"http://arxiv.org/abs/2404.06968v1","category":"math.NT"}
{"created":"2024-04-10 12:19:06","title":"Brownian particles controlled by their occupation measure","abstract":"In this article, we study a finite horizon linear-quadratic stochastic control problem for Brownian particles, where the cost functions depend on the state and the occupation measure of the particles. To address this problem, we develop an It\\^o formula for the flow of occupation measure, which enables us to derive the associated Hamilton-Jacobi-Bellman equation. Then, thanks to a Feynman-Kac formula and the Bou\\'e-Dupuis formula, we construct an optimal strategy and an optimal trajectory. Finally, we illustrate our result when the cost-function is the volume of the sausage associated to the particles.","sentences":["In this article, we study a finite horizon linear-quadratic stochastic control problem for Brownian particles, where the cost functions depend on the state and the occupation measure of the particles.","To address this problem, we develop an It\\^o formula for the flow of occupation measure, which enables us to derive the associated Hamilton-Jacobi-Bellman equation.","Then, thanks to a Feynman-Kac formula and the Bou\\'e-Dupuis formula, we construct an optimal strategy and an optimal trajectory.","Finally, we illustrate our result when the cost-function is the volume of the sausage associated to the particles."],"url":"http://arxiv.org/abs/2404.06960v1","category":"math.PR"}
{"created":"2024-04-10 12:10:48","title":"Blow-up of stochastic semilinear parabolic equations driven by L\u00e9vy noise","abstract":"The blow-up phenomena of stochastic semilinear parabolic equations with additive as well as linear multiplicative L\\'evy noises are investigated in this work. By suitably modifying the concavity method in the stochastic context, we establish the blow-up phenomena of such systems defined on bounded domains.","sentences":["The blow-up phenomena of stochastic semilinear parabolic equations with additive as well as linear multiplicative L\\'evy noises are investigated in this work.","By suitably modifying the concavity method in the stochastic context, we establish the blow-up phenomena of such systems defined on bounded domains."],"url":"http://arxiv.org/abs/2404.06953v1","category":"math.PR"}
{"created":"2024-04-10 11:50:37","title":"A negative result on regularity estimates on finite radial Morse index solutions to elliptic problems","abstract":"In the regularity theory of solutions to elliptic partial differential equations often the concept of stability plays the role of a sufficient condition for smoothness. It is a natural question to ask if this holds true for nonstable but finite Morse index solutions. We provide a negative answer showing the existence of sequences of solutions with radial Morse index equal to 1 for which regularity estimates can not be satisfied.","sentences":["In the regularity theory of solutions to elliptic partial differential equations often the concept of stability plays the role of a sufficient condition for smoothness.","It is a natural question to ask if this holds true for nonstable but finite Morse index solutions.","We provide a negative answer showing the existence of sequences of solutions with radial Morse index equal to 1 for which regularity estimates can not be satisfied."],"url":"http://arxiv.org/abs/2404.06944v1","category":"math.AP"}
{"created":"2024-04-10 11:40:02","title":"Efficient and Generic Point Model for Lossless Point Cloud Attribute Compression","abstract":"The past several years have witnessed the emergence of learned point cloud compression (PCC) techniques. However, current learning-based lossless point cloud attribute compression (PCAC) methods either suffer from high computational complexity or deteriorated compression performance. Moreover, the significant variations in point cloud scale and sparsity encountered in real-world applications make developing an all-in-one neural model a challenging task. In this paper, we propose PoLoPCAC, an efficient and generic lossless PCAC method that achieves high compression efficiency and strong generalizability simultaneously. We formulate lossless PCAC as the task of inferring explicit distributions of attributes from group-wise autoregressive priors. A progressive random grouping strategy is first devised to efficiently resolve the point cloud into groups, and then the attributes of each group are modeled sequentially from accumulated antecedents. A locality-aware attention mechanism is utilized to exploit prior knowledge from context windows in parallel. Since our method directly operates on points, it can naturally avoids distortion caused by voxelization, and can be executed on point clouds with arbitrary scale and density. Experiments show that our method can be instantly deployed once trained on a Synthetic 2k-ShapeNet dataset while enjoying continuous bit-rate reduction over the latest G-PCCv23 on various datasets (ShapeNet, ScanNet, MVUB, 8iVFB). Meanwhile, our method reports shorter coding time than G-PCCv23 on the majority of sequences with a lightweight model size (2.6MB), which is highly attractive for practical applications. Dataset, code and trained model are available at https://github.com/I2-Multimedia-Lab/PoLoPCAC.","sentences":["The past several years have witnessed the emergence of learned point cloud compression (PCC) techniques.","However, current learning-based lossless point cloud attribute compression (PCAC) methods either suffer from high computational complexity or deteriorated compression performance.","Moreover, the significant variations in point cloud scale and sparsity encountered in real-world applications make developing an all-in-one neural model a challenging task.","In this paper, we propose PoLoPCAC, an efficient and generic lossless PCAC method that achieves high compression efficiency and strong generalizability simultaneously.","We formulate lossless PCAC as the task of inferring explicit distributions of attributes from group-wise autoregressive priors.","A progressive random grouping strategy is first devised to efficiently resolve the point cloud into groups, and then the attributes of each group are modeled sequentially from accumulated antecedents.","A locality-aware attention mechanism is utilized to exploit prior knowledge from context windows in parallel.","Since our method directly operates on points, it can naturally avoids distortion caused by voxelization, and can be executed on point clouds with arbitrary scale and density.","Experiments show that our method can be instantly deployed once trained on a Synthetic 2k-ShapeNet dataset while enjoying continuous bit-rate reduction over the latest G-PCCv23 on various datasets (ShapeNet, ScanNet, MVUB, 8iVFB).","Meanwhile, our method reports shorter coding time than G-PCCv23 on the majority of sequences with a lightweight model size (2.6MB), which is highly attractive for practical applications.","Dataset, code and trained model are available at https://github.com/I2-Multimedia-Lab/PoLoPCAC."],"url":"http://arxiv.org/abs/2404.06936v1","category":"cs.CV"}
{"created":"2024-04-10 11:35:39","title":"Homogeneous spacetime with shear viscosity","abstract":"We study the homogeneous and anisotropic evolution of spacetime driven by perfect fluid with shear viscosity. We obtain exact solutions by considering the simplest form of the equation of state wherein the pressure and the shear stress are proportional to the energy density individually. The solutions represent Bianchi type-I cosmology of which the special case becomes Bianchi type-VII. We find that the initial singularity can be removed only for the Bianchi type-VII.","sentences":["We study the homogeneous and anisotropic evolution of spacetime driven by perfect fluid with shear viscosity.","We obtain exact solutions by considering the simplest form of the equation of state wherein the pressure and the shear stress are proportional to the energy density individually.","The solutions represent Bianchi type-I cosmology of which the special case becomes Bianchi type-VII.","We find that the initial singularity can be removed only for the Bianchi type-VII."],"url":"http://arxiv.org/abs/2404.06934v1","category":"gr-qc"}
{"created":"2024-04-10 11:30:07","title":"Multifractal phase in the weighted adjacency matrices of random Erd\u00f6s-R\u00e9nyi graphs","abstract":"We study the spectral properties of the adjacency matrix in the giant connected component of Erd\\\"os-R\\'enyi random graphs, with average connectivity $p$ and randomly distributed hopping amplitudes. By solving the self-consistent cavity equations satisfied by the matrix elements of the resolvent, we compute the probability distribution of the local density of states, which governs the scaling with the system size of the moments of the eigenvectors' amplitudes, as well as several other observables related to the spectral statistics. For small values of $p>1$ above the percolation threshold, we unveil the presence of an exotic delocalized but (weakly) multifractal phase in a broad region of the parameter space, which separates the localized phase found for $p\\le1$ from the fully-delocalized GOE-like phase expected for $p\\to \\infty$. We explore the fundamental physical mechanism underlying the emergence of delocalized multifractal states, rooted in the pronounced heterogeneity in the topology of the graph. This heterogeneity arises from the interplay between strong fluctuations in local degrees and hopping amplitudes, and leads to an effective fragmentation of the graph. We further support our findings by characterizing the level statistics and the two-point spatial correlations within the multifractal phase, and address the ensuing anomalous transport and relaxation properties affecting the quantum dynamical evolution.","sentences":["We study the spectral properties of the adjacency matrix in the giant connected component of Erd\\\"os-R\\'enyi random graphs, with average connectivity $p$ and randomly distributed hopping amplitudes.","By solving the self-consistent cavity equations satisfied by the matrix elements of the resolvent, we compute the probability distribution of the local density of states, which governs the scaling with the system size of the moments of the eigenvectors' amplitudes, as well as several other observables related to the spectral statistics.","For small values of $p>1$ above the percolation threshold, we unveil the presence of an exotic delocalized but (weakly) multifractal phase in a broad region of the parameter space, which separates the localized phase found for $p\\le1$ from the fully-delocalized GOE-like phase expected for $p\\to \\infty$. We explore the fundamental physical mechanism underlying the emergence of delocalized multifractal states, rooted in the pronounced heterogeneity in the topology of the graph.","This heterogeneity arises from the interplay between strong fluctuations in local degrees and hopping amplitudes, and leads to an effective fragmentation of the graph.","We further support our findings by characterizing the level statistics and the two-point spatial correlations within the multifractal phase, and address the ensuing anomalous transport and relaxation properties affecting the quantum dynamical evolution."],"url":"http://arxiv.org/abs/2404.06931v1","category":"cond-mat.dis-nn"}
{"created":"2024-04-10 11:28:09","title":"Exact solution of a two-parameter extended Bariev model","abstract":"An exactly solvable strongly correlated electron model with two parameters is constructed in the frame work of the quantum inverse scattering method. Through the nested Bethe ansatz prodedure, the Bethe ansatz equations are obtained and the exact ground state energy in the thermodynamic limit is derived.","sentences":["An exactly solvable strongly correlated electron model with two parameters is constructed in the frame work of the quantum inverse scattering method.","Through the nested Bethe ansatz prodedure, the Bethe ansatz equations are obtained and the exact ground state energy in the thermodynamic limit is derived."],"url":"http://arxiv.org/abs/2404.06929v1","category":"cond-mat.str-el"}
{"created":"2024-04-10 11:03:57","title":"GraSAME: Injecting Token-Level Structural Information to Pretrained Language Models via Graph-guided Self-Attention Mechanism","abstract":"Pretrained Language Models (PLMs) benefit from external knowledge stored in graph structures for various downstream tasks. However, bridging the modality gap between graph structures and text remains a significant challenge. Traditional methods like linearizing graphs for PLMs lose vital graph connectivity, whereas Graph Neural Networks (GNNs) require cumbersome processes for integration into PLMs. In this work, we propose a novel graph-guided self-attention mechanism, GraSAME. GraSAME seamlessly incorporates token-level structural information into PLMs without necessitating additional alignment or concatenation efforts. As an end-to-end, lightweight multimodal module, GraSAME follows a multi-task learning strategy and effectively bridges the gap between graph and textual modalities, facilitating dynamic interactions between GNNs and PLMs. Our experiments on the graph-to-text generation task demonstrate that GraSAME outperforms baseline models and achieves results comparable to state-of-the-art (SOTA) models on WebNLG datasets. Furthermore, compared to SOTA models, GraSAME eliminates the need for extra pre-training tasks to adjust graph inputs and reduces the number of trainable parameters by over 100 million.","sentences":["Pretrained Language Models (PLMs) benefit from external knowledge stored in graph structures for various downstream tasks.","However, bridging the modality gap between graph structures and text remains a significant challenge.","Traditional methods like linearizing graphs for PLMs lose vital graph connectivity, whereas Graph Neural Networks (GNNs) require cumbersome processes for integration into PLMs.","In this work, we propose a novel graph-guided self-attention mechanism, GraSAME.","GraSAME seamlessly incorporates token-level structural information into PLMs without necessitating additional alignment or concatenation efforts.","As an end-to-end, lightweight multimodal module, GraSAME follows a multi-task learning strategy and effectively bridges the gap between graph and textual modalities, facilitating dynamic interactions between GNNs and PLMs.","Our experiments on the graph-to-text generation task demonstrate that GraSAME outperforms baseline models and achieves results comparable to state-of-the-art (SOTA) models on WebNLG datasets.","Furthermore, compared to SOTA models, GraSAME eliminates the need for extra pre-training tasks to adjust graph inputs and reduces the number of trainable parameters by over 100 million."],"url":"http://arxiv.org/abs/2404.06911v1","category":"cs.CL"}
{"created":"2024-04-10 10:45:30","title":"NFARec: A Negative Feedback-Aware Recommender Model","abstract":"Graph neural network (GNN)-based models have been extensively studied for recommendations, as they can extract high-order collaborative signals accurately which is required for high-quality recommender systems. However, they neglect the valuable information gained through negative feedback in two aspects: (1) different users might hold opposite feedback on the same item, which hampers optimal information propagation in GNNs, and (2) even when an item vastly deviates from users' preferences, they might still choose it and provide a negative rating. In this paper, we propose a negative feedback-aware recommender model (NFARec) that maximizes the leverage of negative feedback. To transfer information to multi-hop neighbors along an optimal path effectively, NFARec adopts a feedback-aware correlation that guides hypergraph convolutions (HGCs) to learn users' structural representations. Moreover, NFARec incorporates an auxiliary task - predicting the feedback sentiment polarity (i.e., positive or negative) of the next interaction - based on the Transformer Hawkes Process. The task is beneficial for understanding users by learning the sentiment expressed in their previous sequential feedback patterns and predicting future interactions. Extensive experiments demonstrate that NFARec outperforms competitive baselines. Our source code and data are released at https://github.com/WangXFng/NFARec.","sentences":["Graph neural network (GNN)-based models have been extensively studied for recommendations, as they can extract high-order collaborative signals accurately which is required for high-quality recommender systems.","However, they neglect the valuable information gained through negative feedback in two aspects: (1) different users might hold opposite feedback on the same item, which hampers optimal information propagation in GNNs, and (2) even when an item vastly deviates from users' preferences, they might still choose it and provide a negative rating.","In this paper, we propose a negative feedback-aware recommender model (NFARec) that maximizes the leverage of negative feedback.","To transfer information to multi-hop neighbors along an optimal path effectively, NFARec adopts a feedback-aware correlation that guides hypergraph convolutions (HGCs) to learn users' structural representations.","Moreover, NFARec incorporates an auxiliary task - predicting the feedback sentiment polarity (i.e., positive or negative) of the next interaction - based on the Transformer Hawkes Process.","The task is beneficial for understanding users by learning the sentiment expressed in their previous sequential feedback patterns and predicting future interactions.","Extensive experiments demonstrate that NFARec outperforms competitive baselines.","Our source code and data are released at https://github.com/WangXFng/NFARec."],"url":"http://arxiv.org/abs/2404.06900v1","category":"cs.IR"}
{"created":"2024-04-10 10:38:24","title":"CaDRec: Contextualized and Debiased Recommender Model","abstract":"Recommender models aimed at mining users' behavioral patterns have raised great attention as one of the essential applications in daily life. Recent work on graph neural networks (GNNs) or debiasing methods has attained remarkable gains. However, they still suffer from (1) over-smoothing node embeddings caused by recursive convolutions with GNNs, and (2) the skewed distribution of interactions due to popularity and user-individual biases. This paper proposes a contextualized and debiased recommender model (CaDRec). To overcome the over-smoothing issue, we explore a novel hypergraph convolution operator that can select effective neighbors during convolution by introducing both structural context and sequential context. To tackle the skewed distribution, we propose two strategies for disentangling interactions: (1) modeling individual biases to learn unbiased item embeddings, and (2) incorporating item popularity with positional encoding. Moreover, we mathematically show that the imbalance of the gradients to update item embeddings exacerbates the popularity bias, thus adopting regularization and weighting schemes as solutions. Extensive experiments on four datasets demonstrate the superiority of the CaDRec against state-of-the-art (SOTA) methods. Our source code and data are released at https://github.com/WangXFng/CaDRec.","sentences":["Recommender models aimed at mining users' behavioral patterns have raised great attention as one of the essential applications in daily life.","Recent work on graph neural networks (GNNs) or debiasing methods has attained remarkable gains.","However, they still suffer from (1) over-smoothing node embeddings caused by recursive convolutions with GNNs, and (2) the skewed distribution of interactions due to popularity and user-individual biases.","This paper proposes a contextualized and debiased recommender model (CaDRec).","To overcome the over-smoothing issue, we explore a novel hypergraph convolution operator that can select effective neighbors during convolution by introducing both structural context and sequential context.","To tackle the skewed distribution, we propose two strategies for disentangling interactions: (1) modeling individual biases to learn unbiased item embeddings, and (2) incorporating item popularity with positional encoding.","Moreover, we mathematically show that the imbalance of the gradients to update item embeddings exacerbates the popularity bias, thus adopting regularization and weighting schemes as solutions.","Extensive experiments on four datasets demonstrate the superiority of the CaDRec against state-of-the-art (SOTA) methods.","Our source code and data are released at https://github.com/WangXFng/CaDRec."],"url":"http://arxiv.org/abs/2404.06895v1","category":"cs.IR"}
{"created":"2024-04-10 17:31:49","title":"BAMBOO: a predictive and transferable machine learning force field framework for liquid electrolyte development","abstract":"Despite the widespread applications of machine learning force field (MLFF) on solids and small molecules, there is a notable gap in applying MLFF to complex liquid electrolytes. In this work, we introduce BAMBOO (ByteDance AI Molecular Simulation Booster), a novel framework for molecular dynamics (MD) simulations, with a demonstration of its capabilities in the context of liquid electrolytes for lithium batteries. We design a physics-inspired graph equivariant transformer architecture as the backbone of BAMBOO to learn from quantum mechanical simulations. Additionally, we pioneer an ensemble knowledge distillation approach and apply it on MLFFs to improve the stability of MD simulations. Finally, we propose the density alignment algorithm to align BAMBOO with experimental measurements. BAMBOO demonstrates state-of-the-art accuracy in predicting key electrolyte properties such as density, viscosity, and ionic conductivity across various solvents and salt combinations. Our current model, trained on more than 15 chemical species, achieves the average density error of 0.01 g/cm$^3$ on various compositions compared with experimental data. Moreover, our model demonstrates transferability to molecules not included in the quantum mechanical dataset. We envision this work as paving the way to a \"universal MLFF\" capable of simulating properties of common organic liquids.","sentences":["Despite the widespread applications of machine learning force field (MLFF) on solids and small molecules, there is a notable gap in applying MLFF to complex liquid electrolytes.","In this work, we introduce BAMBOO (ByteDance AI Molecular Simulation Booster), a novel framework for molecular dynamics (MD) simulations, with a demonstration of its capabilities in the context of liquid electrolytes for lithium batteries.","We design a physics-inspired graph equivariant transformer architecture as the backbone of BAMBOO to learn from quantum mechanical simulations.","Additionally, we pioneer an ensemble knowledge distillation approach and apply it on MLFFs to improve the stability of MD simulations.","Finally, we propose the density alignment algorithm to align BAMBOO with experimental measurements.","BAMBOO demonstrates state-of-the-art accuracy in predicting key electrolyte properties such as density, viscosity, and ionic conductivity across various solvents and salt combinations.","Our current model, trained on more than 15 chemical species, achieves the average density error of 0.01 g/cm$^3$ on various compositions compared with experimental data.","Moreover, our model demonstrates transferability to molecules not included in the quantum mechanical dataset.","We envision this work as paving the way to a \"universal MLFF\" capable of simulating properties of common organic liquids."],"url":"http://arxiv.org/abs/2404.07181v2","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-10 17:28:16","title":"Move Anything with Layered Scene Diffusion","abstract":"Diffusion models generate images with an unprecedented level of quality, but how can we freely rearrange image layouts? Recent works generate controllable scenes via learning spatially disentangled latent codes, but these methods do not apply to diffusion models due to their fixed forward process. In this work, we propose SceneDiffusion to optimize a layered scene representation during the diffusion sampling process. Our key insight is that spatial disentanglement can be obtained by jointly denoising scene renderings at different spatial layouts. Our generated scenes support a wide range of spatial editing operations, including moving, resizing, cloning, and layer-wise appearance editing operations, including object restyling and replacing. Moreover, a scene can be generated conditioned on a reference image, thus enabling object moving for in-the-wild images. Notably, this approach is training-free, compatible with general text-to-image diffusion models, and responsive in less than a second.","sentences":["Diffusion models generate images with an unprecedented level of quality, but how can we freely rearrange image layouts?","Recent works generate controllable scenes via learning spatially disentangled latent codes, but these methods do not apply to diffusion models due to their fixed forward process.","In this work, we propose SceneDiffusion to optimize a layered scene representation during the diffusion sampling process.","Our key insight is that spatial disentanglement can be obtained by jointly denoising scene renderings at different spatial layouts.","Our generated scenes support a wide range of spatial editing operations, including moving, resizing, cloning, and layer-wise appearance editing operations, including object restyling and replacing.","Moreover, a scene can be generated conditioned on a reference image, thus enabling object moving for in-the-wild images.","Notably, this approach is training-free, compatible with general text-to-image diffusion models, and responsive in less than a second."],"url":"http://arxiv.org/abs/2404.07178v1","category":"cs.CV"}
{"created":"2024-04-10 17:49:34","title":"Optimal Reeb graphs on two- and three-connected planar polygon","abstract":"To investigate the topological structure of planar polygon decomposition on trapezoids, which is formed by height functions. We use the oriented Reeb graph of the function with a marked vertex. We describe all possible optimal Reeb graphs in the case of polygon in general position with one local minimum and one local maximum. By optimal Reeb graph we mean Reeb graph, which cann't be obtaned from other Reeb graph by subdivision of an edge or a leaf attaching. In this case polygon is a triangle with triangle holes. Constructed Reeb graphs give topological structures of trapezoid maps on two-connected polygon with six vertexes and three-connected polygon with nine vertexes.","sentences":["To investigate the topological structure of planar polygon decomposition on trapezoids, which is formed by height functions.","We use the oriented Reeb graph of the function with a marked vertex.","We describe all possible optimal Reeb graphs in the case of polygon in general position with one local minimum and one local maximum.","By optimal Reeb graph we mean Reeb graph, which cann't be obtaned from other Reeb graph by subdivision of an edge or a leaf attaching.","In this case polygon is a triangle with triangle holes.","Constructed Reeb graphs give topological structures of trapezoid maps on two-connected polygon with six vertexes and three-connected polygon with nine vertexes."],"url":"http://arxiv.org/abs/2404.07193v1","category":"math.GT"}
{"created":"2024-04-10 17:08:46","title":"A Gauss-Newton Approach for Min-Max Optimization in Generative Adversarial Networks","abstract":"A novel first-order method is proposed for training generative adversarial networks (GANs). It modifies the Gauss-Newton method to approximate the min-max Hessian and uses the Sherman-Morrison inversion formula to calculate the inverse. The method corresponds to a fixed-point method that ensures necessary contraction. To evaluate its effectiveness, numerical experiments are conducted on various datasets commonly used in image generation tasks, such as MNIST, Fashion MNIST, CIFAR10, FFHQ, and LSUN. Our method is capable of generating high-fidelity images with greater diversity across multiple datasets. It also achieves the highest inception score for CIFAR10 among all compared methods, including state-of-the-art second-order methods. Additionally, its execution time is comparable to that of first-order min-max methods.","sentences":["A novel first-order method is proposed for training generative adversarial networks (GANs).","It modifies the Gauss-Newton method to approximate the min-max Hessian and uses the Sherman-Morrison inversion formula to calculate the inverse.","The method corresponds to a fixed-point method that ensures necessary contraction.","To evaluate its effectiveness, numerical experiments are conducted on various datasets commonly used in image generation tasks, such as MNIST, Fashion MNIST, CIFAR10, FFHQ, and LSUN.","Our method is capable of generating high-fidelity images with greater diversity across multiple datasets.","It also achieves the highest inception score for CIFAR10 among all compared methods, including state-of-the-art second-order methods.","Additionally, its execution time is comparable to that of first-order min-max methods."],"url":"http://arxiv.org/abs/2404.07172v1","category":"cs.LG"}
{"created":"2024-04-10 17:08:07","title":"Unlocking Quantum Optimization: A Use Case Study on NISQ Systems","abstract":"The major advances in quantum computing over the last few decades have sparked great interest in applying it to solve the most challenging computational problems in a wide variety of areas. One of the most pronounced domains here are optimization problems and a number of algorithmic approaches have been proposed for their solution. For the current noisy intermediate-scale quantum (NISQ) computers the quantum approximate optimization algorithm (QAOA), the variational quantum eigensolver (VQE), and quantum annealing (QA) are the central algorithms for this problem class. The two former can be executed on digital gate-model quantum computers, whereas the latter requires a quantum annealer. Across all hardware architectures and manufactures, the quantum computers available today share the property of being too error-prone to reliably execute involved quantum circuits as they typically arise from quantum optimization algorithms. In order to characterize the limits of existing quantum computers, many component and system level benchmarks have been proposed. However, owing to the complex nature of the errors in quantum systems these benchmark fail to provide predictive power beyond simple quantum circuits and small examples. Application oriented benchmarks have been proposed to remedy this problem, but both, results from real quantum systems as well as use cases beyond constructed academic examples, remain very rare. This paper addresses precisely this gap by considering two industrial relevant use cases: one in the realm of optimizing charging schedules for electric vehicles, the other concerned with the optimization of truck routes. Our central contribution are systematic series of examples derived from these uses cases that we execute on different processors of the gate-based quantum computers of IBM as well as on the quantum annealer of D-Wave.","sentences":["The major advances in quantum computing over the last few decades have sparked great interest in applying it to solve the most challenging computational problems in a wide variety of areas.","One of the most pronounced domains here are optimization problems and a number of algorithmic approaches have been proposed for their solution.","For the current noisy intermediate-scale quantum (NISQ) computers the quantum approximate optimization algorithm (QAOA), the variational quantum eigensolver (VQE), and quantum annealing (QA) are the central algorithms for this problem class.","The two former can be executed on digital gate-model quantum computers, whereas the latter requires a quantum annealer.","Across all hardware architectures and manufactures, the quantum computers available today share the property of being too error-prone to reliably execute involved quantum circuits as they typically arise from quantum optimization algorithms.","In order to characterize the limits of existing quantum computers, many component and system level benchmarks have been proposed.","However, owing to the complex nature of the errors in quantum systems these benchmark fail to provide predictive power beyond simple quantum circuits and small examples.","Application oriented benchmarks have been proposed to remedy this problem, but both, results from real quantum systems as well as use cases beyond constructed academic examples, remain very rare.","This paper addresses precisely this gap by considering two industrial relevant use cases: one in the realm of optimizing charging schedules for electric vehicles, the other concerned with the optimization of truck routes.","Our central contribution are systematic series of examples derived from these uses cases that we execute on different processors of the gate-based quantum computers of IBM as well as on the quantum annealer of D-Wave."],"url":"http://arxiv.org/abs/2404.07171v1","category":"quant-ph"}
{"created":"2024-04-10 16:44:43","title":"Understanding Dynamics in Coarse-Grained Models: IV. Connection of Fine-Grained and Coarse-Grained Dynamics with the Stokes-Einstein and Stokes-Einstein-Debye Relations","abstract":"Applying an excess entropy scaling formalism to the coarse-grained (CG) dynamics of liquids, we discovered that missing rotational motions during the CG process are responsible for artificially accelerated CG dynamics. In the context of the dynamic representability between the fine-grained (FG) and CG dynamics, this work introduces the well-known Stokes-Einstein and Stokes-Einstein-Debye relations to unravel the rotational dynamics underlying FG trajectories, thereby allowing for an indirect evaluation of the effective rotations based only on the translational information at the reduced CG resolution. Since the representability issue in CG modeling limits a direct evaluation of the shear stress appearing in the Stokes-Einstein and Stokes-Einstein-Debye relations, we introduce a translational relaxation time as a proxy to employ these relations, and we demonstrate that these relations hold for the ambient conditions studied in our series of work. Additional theoretical links to our previous work are also established. First, we demonstrate that the effective hard sphere radius determined by the classical perturbation theory can approximate the complex hydrodynamic radius value reasonably well. Also, we present a simple derivation of an excess entropy scaling relationship for viscosity by estimating the elliptical integral of molecules. In turn, since the translational and rotational motions at the FG level are correlated to each other, we conclude that the \"entropy-free\" CG diffusion only depends on the shape of the reference molecule. Our results and analyses impart an alternative way of recovering the FG diffusion from the CG description by coupling the translational and rotational motions at the hydrodynamic level.","sentences":["Applying an excess entropy scaling formalism to the coarse-grained (CG) dynamics of liquids, we discovered that missing rotational motions during the CG process are responsible for artificially accelerated CG dynamics.","In the context of the dynamic representability between the fine-grained (FG) and CG dynamics, this work introduces the well-known Stokes-Einstein and Stokes-Einstein-Debye relations to unravel the rotational dynamics underlying FG trajectories, thereby allowing for an indirect evaluation of the effective rotations based only on the translational information at the reduced CG resolution.","Since the representability issue in CG modeling limits a direct evaluation of the shear stress appearing in the Stokes-Einstein and Stokes-Einstein-Debye relations, we introduce a translational relaxation time as a proxy to employ these relations, and we demonstrate that these relations hold for the ambient conditions studied in our series of work.","Additional theoretical links to our previous work are also established.","First, we demonstrate that the effective hard sphere radius determined by the classical perturbation theory can approximate the complex hydrodynamic radius value reasonably well.","Also, we present a simple derivation of an excess entropy scaling relationship for viscosity by estimating the elliptical integral of molecules.","In turn, since the translational and rotational motions at the FG level are correlated to each other, we conclude that the \"entropy-free\" CG diffusion only depends on the shape of the reference molecule.","Our results and analyses impart an alternative way of recovering the FG diffusion from the CG description by coupling the translational and rotational motions at the hydrodynamic level."],"url":"http://arxiv.org/abs/2404.07156v1","category":"physics.chem-ph"}
{"created":"2024-04-10 16:35:36","title":"Logarithmic-Depth Quantum Circuits for Hamming Weight Projections","abstract":"A pure state of fixed Hamming weight is a superposition of computational basis states such that each bitstring in the superposition has the same number of ones. Given a Hilbert space of the form $\\mathcal{H} = (\\mathbb{C}_2)^{\\otimes n}$, or an $n$-qubit system, the identity operator can be decomposed as a sum of projectors onto subspaces of fixed Hamming weight. In this work, we propose several quantum algorithms that realize a coherent Hamming weight projective measurement on an input pure state, meaning that the post-measurement state of the algorithm is the projection of the input state onto the corresponding subspace of fixed Hamming weight. We analyze a depth-width trade-off for the corresponding quantum circuits, allowing for a depth reduction of the circuits at the cost of more control qubits. For an $n$-qubit input, the depth-optimal algorithm uses $O(n)$ control qubits and the corresponding circuit has depth $O(\\log (n))$, assuming that we have the ability to perform qubit resets. Furthermore, the proposed algorithm construction uses only one- and two-qubit gates.","sentences":["A pure state of fixed Hamming weight is a superposition of computational basis states such that each bitstring in the superposition has the same number of ones.","Given a Hilbert space of the form $\\mathcal{H} = (\\mathbb{C}_2)^{\\otimes n}$, or an $n$-qubit system, the identity operator can be decomposed as a sum of projectors onto subspaces of fixed Hamming weight.","In this work, we propose several quantum algorithms that realize a coherent Hamming weight projective measurement on an input pure state, meaning that the post-measurement state of the algorithm is the projection of the input state onto the corresponding subspace of fixed Hamming weight.","We analyze a depth-width trade-off for the corresponding quantum circuits, allowing for a depth reduction of the circuits at the cost of more control qubits.","For an $n$-qubit input, the depth-optimal algorithm uses $O(n)$ control qubits and the corresponding circuit has depth $O(\\log (n))$, assuming that we have the ability to perform qubit resets.","Furthermore, the proposed algorithm construction uses only one-","and two-qubit gates."],"url":"http://arxiv.org/abs/2404.07151v1","category":"quant-ph"}
{"created":"2024-04-10 16:29:21","title":"How Consistent are Clinicians? Evaluating the Predictability of Sepsis Disease Progression with Dynamics Models","abstract":"Reinforcement learning (RL) is a promising approach to generate treatment policies for sepsis patients in intensive care. While retrospective evaluation metrics show decreased mortality when these policies are followed, studies with clinicians suggest their recommendations are often spurious. We propose that these shortcomings may be due to lack of diversity in observed actions and outcomes in the training data, and we construct experiments to investigate the feasibility of predicting sepsis disease severity changes due to clinician actions. Preliminary results suggest incorporating action information does not significantly improve model performance, indicating that clinician actions may not be sufficiently variable to yield measurable effects on disease progression. We discuss the implications of these findings for optimizing sepsis treatment.","sentences":["Reinforcement learning (RL) is a promising approach to generate treatment policies for sepsis patients in intensive care.","While retrospective evaluation metrics show decreased mortality when these policies are followed, studies with clinicians suggest their recommendations are often spurious.","We propose that these shortcomings may be due to lack of diversity in observed actions and outcomes in the training data, and we construct experiments to investigate the feasibility of predicting sepsis disease severity changes due to clinician actions.","Preliminary results suggest incorporating action information does not significantly improve model performance, indicating that clinician actions may not be sufficiently variable to yield measurable effects on disease progression.","We discuss the implications of these findings for optimizing sepsis treatment."],"url":"http://arxiv.org/abs/2404.07148v1","category":"cs.LG"}
{"created":"2024-04-10 16:24:51","title":"On noise in swap ASAP repeater chains: exact analytics, distributions and tight approximations","abstract":"Losses are one of the main bottlenecks for the distribution of entanglement in quantum networks, which can be overcome by the implementation of quantum repeaters. The most basic form of a quantum repeater chain is the swap ASAP repeater chain. In such a repeater chain, elementary links are probabilistically generated and deterministically swapped as soon as two adjacent links have been generated. As each entangled state is waiting to be swapped, decoherence is experienced, turning the fidelity of the entangled state between the end nodes of the chain into a random variable. Fully characterizing the (average) fidelity as the repeater chain grows is still an open problem. Here, we analytically investigate the case of equally-spaced repeaters, where we find exact analytic formulae for all moments of the fidelity up to 25 segments. We obtain these formulae by providing a general solution in terms of a generating function; a function whose n'th term in its Maclaurin series yields the moments of the fidelity for n segments. We generalize this approaches as well to a global cut-off policy -- a method for increasing fidelity at the cost of longer entanglement delivery times -- allowing for fast optimization of the cut-off parameter by eliminating the need for Monte Carlo simulation. We furthermore find simple approximations of the average fidelity that are exponentially tight, and, for up to 10 segments, the full distribution of the delivered fidelity. We use this to analytically calculate the secret-key rate when the distributed entanglement is used for quantum-key distribution, both with and without binning methods. In follow-up work we exploit a connection to a model in statistical physics to numerically calculate quantities of interest for the inhomogeneous multipartite case.","sentences":["Losses are one of the main bottlenecks for the distribution of entanglement in quantum networks, which can be overcome by the implementation of quantum repeaters.","The most basic form of a quantum repeater chain is the swap ASAP repeater chain.","In such a repeater chain, elementary links are probabilistically generated and deterministically swapped as soon as two adjacent links have been generated.","As each entangled state is waiting to be swapped, decoherence is experienced, turning the fidelity of the entangled state between the end nodes of the chain into a random variable.","Fully characterizing the (average) fidelity as the repeater chain grows is still an open problem.","Here, we analytically investigate the case of equally-spaced repeaters, where we find exact analytic formulae for all moments of the fidelity up to 25 segments.","We obtain these formulae by providing a general solution in terms of a generating function; a function whose n'th term in its Maclaurin series yields the moments of the fidelity for n segments.","We generalize this approaches as well to a global cut-off policy -- a method for increasing fidelity at the cost of longer entanglement delivery times -- allowing for fast optimization of the cut-off parameter by eliminating the need for Monte Carlo simulation.","We furthermore find simple approximations of the average fidelity that are exponentially tight, and, for up to 10 segments, the full distribution of the delivered fidelity.","We use this to analytically calculate the secret-key rate when the distributed entanglement is used for quantum-key distribution, both with and without binning methods.","In follow-up work we exploit a connection to a model in statistical physics to numerically calculate quantities of interest for the inhomogeneous multipartite case."],"url":"http://arxiv.org/abs/2404.07146v1","category":"quant-ph"}
{"created":"2024-04-10 16:04:49","title":"Strengthening Lasserre's Hierarchy in Real and Complex Polynomial Optimization","abstract":"We establish a connection between multiplication operators and shift operators. Moreover, we derive positive semidefinite conditions of finite rank moment sequences and use these conditions to strengthen Lasserre's hierarchy for real and complex polynomial optimization. Integration of the strengthening technique with sparsity is considered. Extensive numerical experiments show that our strengthening technique can significantly improve the bound (especially for complex polynomial optimization) and allows to achieve global optimality at lower relaxation orders, thus providing substantial computational savings.","sentences":["We establish a connection between multiplication operators and shift operators.","Moreover, we derive positive semidefinite conditions of finite rank moment sequences and use these conditions to strengthen Lasserre's hierarchy for real and complex polynomial optimization.","Integration of the strengthening technique with sparsity is considered.","Extensive numerical experiments show that our strengthening technique can significantly improve the bound (especially for complex polynomial optimization) and allows to achieve global optimality at lower relaxation orders, thus providing substantial computational savings."],"url":"http://arxiv.org/abs/2404.07125v1","category":"math.OC"}
{"created":"2024-04-10 15:53:54","title":"Photonic next-generation reservoir computer based on distributed feedback in optical fiber","abstract":"Reservoir computing (RC) is a machine learning paradigm that excels at dynamical systems analysis. Photonic RCs, which perform implicit computation through optical interactions, have attracted increasing attention due to their potential for low latency predictions. However, most existing photonic RCs rely on a nonlinear physical cavity to implement system memory, limiting control over the memory structure and requiring long warm-up times to eliminate transients. In this work, we resolve these issues by demonstrating a photonic next-generation reservoir computer (NG-RC) using a fiber optic platform. Our photonic NG-RC eliminates the need for a cavity by generating feature vectors directly from nonlinear combinations of the input data with varying delays. Our approach uses Rayleigh backscattering to produce output feature vectors by an unconventional nonlinearity resulting from coherent, interferometric mixing followed by a quadratic readout. Performing linear optimization on these feature vectors, our photonic NG-RC demonstrates state-of-the-art performance for the observer (cross-prediction) task applied to the R\\\"ossler, Lorenz, and Kuramoto-Sivashinsky systems. In contrast to digital NG-RC implementations, this scheme is easily scalable to high-dimensional systems while maintaining low latency and low power consumption.","sentences":["Reservoir computing (RC) is a machine learning paradigm that excels at dynamical systems analysis.","Photonic RCs, which perform implicit computation through optical interactions, have attracted increasing attention due to their potential for low latency predictions.","However, most existing photonic RCs rely on a nonlinear physical cavity to implement system memory, limiting control over the memory structure and requiring long warm-up times to eliminate transients.","In this work, we resolve these issues by demonstrating a photonic next-generation reservoir computer (NG-RC) using a fiber optic platform.","Our photonic NG-RC eliminates the need for a cavity by generating feature vectors directly from nonlinear combinations of the input data with varying delays.","Our approach uses Rayleigh backscattering to produce output feature vectors by an unconventional nonlinearity resulting from coherent, interferometric mixing followed by a quadratic readout.","Performing linear optimization on these feature vectors, our photonic NG-RC demonstrates state-of-the-art performance for the observer (cross-prediction) task applied to the R\\\"ossler, Lorenz, and Kuramoto-Sivashinsky systems.","In contrast to digital NG-RC implementations, this scheme is easily scalable to high-dimensional systems while maintaining low latency and low power consumption."],"url":"http://arxiv.org/abs/2404.07116v1","category":"physics.optics"}
{"created":"2024-04-10 15:53:41","title":"Classical simulation and quantum resource theory of non-Gaussian optics","abstract":"We propose efficient algorithms for classically simulating Gaussian unitaries and measurements applied to non-Gaussian initial states. The constructions are based on decomposing the non-Gaussian states into linear combinations of Gaussian states. We use an extension of the covariance matrix formalism to efficiently track relative phases in the superpositions of Gaussian states. We get an exact simulation that scales quadratically with the number of Gaussian states required to represent the initial state and an approximate simulation algorithm that scales linearly with the degree. We define measures of non-Gaussianty quantifying this simulation cost, which we call the Gaussian rank and the Gaussian extent. From the perspective of quantum resource theories, we investigate the properties of this type of non-Gaussianity measure and compute optimal decomposition for states relevant to continuous-variable quantum computing.","sentences":["We propose efficient algorithms for classically simulating Gaussian unitaries and measurements applied to non-Gaussian initial states.","The constructions are based on decomposing the non-Gaussian states into linear combinations of Gaussian states.","We use an extension of the covariance matrix formalism to efficiently track relative phases in the superpositions of Gaussian states.","We get an exact simulation that scales quadratically with the number of Gaussian states required to represent the initial state and an approximate simulation algorithm that scales linearly with the degree.","We define measures of non-Gaussianty quantifying this simulation cost, which we call the Gaussian rank and the Gaussian extent.","From the perspective of quantum resource theories, we investigate the properties of this type of non-Gaussianity measure and compute optimal decomposition for states relevant to continuous-variable quantum computing."],"url":"http://arxiv.org/abs/2404.07115v1","category":"quant-ph"}
{"created":"2024-04-10 15:03:48","title":"Multiscale structure-property discovery via active learning in scanning tunneling microscopy","abstract":"Atomic arrangements and local sub-structures fundamentally influence emergent material functionalities. The local structures are conventionally probed using spatially resolved studies and the property correlations are usually deciphered by a researcher based on sequential explorations and auxiliary information, thus limiting the throughput efficiency. Here we demonstrate a Bayesian deep learning based framework that automatically correlates material structure with its electronic properties using scanning tunneling microscopy (STM) measurements in real-time. Its predictions are used to autonomously direct exploration toward regions of the sample that optimize a given material property. This autonomous method is deployed on the low-temperature ultra-high vacuum STM to understand the structure-property relationship in a europium-based semimetal, EuZn2As2, one of the promising candidates for studying the magnetism-driven topological properties. The framework employs a sparse sampling approach to efficiently construct the scalar-property space using a minimal number of measurements, about 1 - 10 % of the data required in standard hyperspectral imaging methods. We further demonstrate a target-property-guided active learning of structures within a multiscale framework. This is implemented across length scales in a hierarchical fashion for the autonomous discovery of structural origins for an observed material property. This framework offers the choice to select and derive a suitable scalar property from the spectroscopic data to steer exploration across the sample space. Our findings reveal correlations of the electronic properties unique to surface terminations, local defect density, and point defects.","sentences":["Atomic arrangements and local sub-structures fundamentally influence emergent material functionalities.","The local structures are conventionally probed using spatially resolved studies and the property correlations are usually deciphered by a researcher based on sequential explorations and auxiliary information, thus limiting the throughput efficiency.","Here we demonstrate a Bayesian deep learning based framework that automatically correlates material structure with its electronic properties using scanning tunneling microscopy (STM) measurements in real-time.","Its predictions are used to autonomously direct exploration toward regions of the sample that optimize a given material property.","This autonomous method is deployed on the low-temperature ultra-high vacuum STM to understand the structure-property relationship in a europium-based semimetal, EuZn2As2, one of the promising candidates for studying the magnetism-driven topological properties.","The framework employs a sparse sampling approach to efficiently construct the scalar-property space using a minimal number of measurements, about 1 - 10 % of the data required in standard hyperspectral imaging methods.","We further demonstrate a target-property-guided active learning of structures within a multiscale framework.","This is implemented across length scales in a hierarchical fashion for the autonomous discovery of structural origins for an observed material property.","This framework offers the choice to select and derive a suitable scalar property from the spectroscopic data to steer exploration across the sample space.","Our findings reveal correlations of the electronic properties unique to surface terminations, local defect density, and point defects."],"url":"http://arxiv.org/abs/2404.07074v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-10 15:03:30","title":"Data-driven quasiconformal morphodynamic flows","abstract":"Temporal imaging of biological epithelial structures yields shape data at discrete time points, leading to a natural question: how can we reconstruct the most likely path of growth patterns consistent with these discrete observations? We present a physically plausible framework to solve this inverse problem by creating a framework that generalizes quasiconformal maps to quasiconformal flows. By allowing for the spatio-temporal variation of the shear and dilatation fields during the growth process, subject to regulatory mechanisms, we are led to a type of generalized Ricci flow. When guided by observational data associated with surface shape as a function of time, this leads to a constrained optimization problem. Deploying our data-driven algorithmic approach to the shape of insect wings, leaves and even sculpted faces, we show how optimal quasiconformal flows allow us to characterize the morphogenesis of a range of surfaces.","sentences":["Temporal imaging of biological epithelial structures yields shape data at discrete time points, leading to a natural question: how can we reconstruct the most likely path of growth patterns consistent with these discrete observations?","We present a physically plausible framework to solve this inverse problem by creating a framework that generalizes quasiconformal maps to quasiconformal flows.","By allowing for the spatio-temporal variation of the shear and dilatation fields during the growth process, subject to regulatory mechanisms, we are led to a type of generalized Ricci flow.","When guided by observational data associated with surface shape as a function of time, this leads to a constrained optimization problem.","Deploying our data-driven algorithmic approach to the shape of insect wings, leaves and even sculpted faces, we show how optimal quasiconformal flows allow us to characterize the morphogenesis of a range of surfaces."],"url":"http://arxiv.org/abs/2404.07073v1","category":"cs.CG"}
{"created":"2024-04-10 14:46:14","title":"Generalized Straight-Line Programs","abstract":"It was recently proved that any Straight-Line Program (SLP) generating a given string can be transformed in linear time into an equivalent balanced SLP of the same asymptotic size. We generalize this proof to a general class of grammars we call Generalized SLPs (GSLPs), which allow rules of the form $A \\rightarrow x$ where $x$ is any Turing-complete representation (of size $|x|$) of a sequence of symbols (potentially much longer than $|x|$). We then specialize GSLPs to so-called Iterated SLPs (ISLPs), which allow rules of the form $A \\rightarrow \\Pi_{i=k_1}^{k_2} B_1^{i^{c_1}}\\cdots B_t^{i^{c_t}}$ of size $2t+2$. We prove that ISLPs break, for some text families, the measure $\\delta$ based on substring complexity, a lower bound for most measures and compressors exploiting repetitiveness. Further, ISLPs can extract any substring of length $\\lambda$, from the represented text $T[1.. n]$, in time $O(\\lambda + \\log^2 n\\log\\log n)$. This is the first compressed representation for repetitive texts breaking $\\delta$ while, at the same time, supporting direct access to arbitrary text symbols in polylogarithmic time. We also show how to compute some substring queries, like range minima and next/previous smaller value, in time $O(\\log^2 n \\log\\log n)$. Finally, we further specialize the grammars to Run-Length SLPs (RLSLPs), which restrict the rules allowed by ISLPs to the form $A \\rightarrow B^t$. Apart from inheriting all the previous results with the term $\\log^2 n \\log\\log n$ reduced to the near-optimal $\\log n$, we show that RLSLPs can exploit balance to efficiently compute a wide class of substring queries we call ``composable'' -- i.e., $f(X \\cdot Y)$ can be obtained from $f(X)$ and $f(Y)$...","sentences":["It was recently proved that any Straight-Line Program (SLP) generating a given string can be transformed in linear time into an equivalent balanced SLP of the same asymptotic size.","We generalize this proof to a general class of grammars we call Generalized SLPs (GSLPs), which allow rules of the form $A \\rightarrow x$ where $x$ is any Turing-complete representation (of size $|x|$) of a sequence of symbols (potentially much longer than $|x|$).","We then specialize GSLPs to so-called Iterated SLPs (ISLPs), which allow rules of the form $A \\rightarrow \\Pi_{i=k_1}^{k_2} B_1^{i^{c_1}}\\cdots B_t^{i^{c_t}}$ of size $2t+2$. We prove that ISLPs break, for some text families, the measure $\\delta$ based on substring complexity, a lower bound for most measures and compressors exploiting repetitiveness.","Further, ISLPs can extract any substring of length $\\lambda$, from the represented text $T[1.. n]$, in time $O(\\lambda + \\log^2 n\\log\\log","n)$. This is the first compressed representation for repetitive texts breaking $\\delta$ while, at the same time, supporting direct access to arbitrary text symbols in polylogarithmic time.","We also show how to compute some substring queries, like range minima and next/previous smaller value, in time $O(\\log^2 n \\log\\log n)$.","Finally, we further specialize the grammars to Run-Length SLPs (RLSLPs), which restrict the rules allowed by ISLPs to the form $A \\rightarrow B^t$. Apart from inheriting all the previous results with the term $\\log^2 n \\log\\log n$ reduced to the near-optimal $\\log n$, we show that RLSLPs can exploit balance to efficiently compute a wide class of substring queries we call ``composable'' -- i.e., $f(X \\cdot Y)$ can be obtained from $f(X)$ and $f(Y)$..."],"url":"http://arxiv.org/abs/2404.07057v1","category":"cs.DS"}
{"created":"2024-04-10 14:45:21","title":"Observational features of reflection asymmetric black holes","abstract":"The Kerr spacetime is symmetric with respect to a well-defined equatorial plane. When testing the equatorial reflection symmetry of an isolated black hole, one is at the same time testing the Kerr hypothesis in General Relativity. In this work, we investigate the possible observational features when a Keplerian disk is surrounding a rotating black hole without reflection symmetry. When such symmetry is broken, generically, the photon trajectories around the black hole and the Keplerian orbits on the accretion disk are distorted vertically away from the equatorial plane by an amount that depends on their distance to the black hole. In the reflection asymmetric spacetime we are considering, these two kinds of orbits are distorted in opposite directions. Interestingly, while the size and shape of black hole shadows closely resemble those of Kerr black holes, distinct observational characteristics can emerge in the disk image and emission line profiles. When observing the disk edge-on, a pronounced concave shape may appear along its innermost edge on the incoming side. Furthermore, distinctive horn-like features might be observed on the spectral line profile at the blue-shifted side. These special features can serve as compelling indicators of the reflection asymmetry present in rotating black holes.","sentences":["The Kerr spacetime is symmetric with respect to a well-defined equatorial plane.","When testing the equatorial reflection symmetry of an isolated black hole, one is at the same time testing the Kerr hypothesis in General Relativity.","In this work, we investigate the possible observational features when a Keplerian disk is surrounding a rotating black hole without reflection symmetry.","When such symmetry is broken, generically, the photon trajectories around the black hole and the Keplerian orbits on the accretion disk are distorted vertically away from the equatorial plane by an amount that depends on their distance to the black hole.","In the reflection asymmetric spacetime we are considering, these two kinds of orbits are distorted in opposite directions.","Interestingly, while the size and shape of black hole shadows closely resemble those of Kerr black holes, distinct observational characteristics can emerge in the disk image and emission line profiles.","When observing the disk edge-on, a pronounced concave shape may appear along its innermost edge on the incoming side.","Furthermore, distinctive horn-like features might be observed on the spectral line profile at the blue-shifted side.","These special features can serve as compelling indicators of the reflection asymmetry present in rotating black holes."],"url":"http://arxiv.org/abs/2404.07055v1","category":"gr-qc"}
{"created":"2024-04-10 14:22:16","title":"Diffusion-based inpainting of incomplete Euclidean distance matrices of trajectories generated by a fractional Brownian motion","abstract":"Fractional Brownian trajectories (fBm) feature both randomness and strong scale-free correlations, challenging generative models to reproduce the intrinsic memory characterizing the underlying process. Here we test a diffusion probabilistic model on a specific dataset of corrupted images corresponding to incomplete Euclidean distance matrices of fBm at various memory exponents $H$. Our dataset implies uniqueness of the data imputation in the regime of low missing ratio, where the remaining partial graph is rigid, providing the ground truth for the inpainting. We find that the conditional diffusion generation stably reproduces the statistics of missing fBm-distributed distances for different values of $H$ exponent. Furthermore, while diffusion models have been recently shown to remember samples from the training database, we show that diffusion-based inpainting behaves qualitatively different from the database search with the increasing database size. Finally, we apply our fBm-trained diffusion model with $H=1/3$ for completion of chromosome distance matrices obtained in single-cell microscopy experiments, showing its superiority over the standard bioinformatics algorithms. Our source code is available on GitHub at https://github.com/alobashev/diffusion_fbm.","sentences":["Fractional Brownian trajectories (fBm) feature both randomness and strong scale-free correlations, challenging generative models to reproduce the intrinsic memory characterizing the underlying process.","Here we test a diffusion probabilistic model on a specific dataset of corrupted images corresponding to incomplete Euclidean distance matrices of fBm at various memory exponents $H$. Our dataset implies uniqueness of the data imputation in the regime of low missing ratio, where the remaining partial graph is rigid, providing the ground truth for the inpainting.","We find that the conditional diffusion generation stably reproduces the statistics of missing fBm-distributed distances for different values of $H$ exponent.","Furthermore, while diffusion models have been recently shown to remember samples from the training database, we show that diffusion-based inpainting behaves qualitatively different from the database search with the increasing database size.","Finally, we apply our fBm-trained diffusion model with $H=1/3$ for completion of chromosome distance matrices obtained in single-cell microscopy experiments, showing its superiority over the standard bioinformatics algorithms.","Our source code is available on GitHub at https://github.com/alobashev/diffusion_fbm."],"url":"http://arxiv.org/abs/2404.07029v1","category":"cs.CV"}
{"created":"2024-04-10 14:20:52","title":"Optimal Communication Complexity of Chained Index","abstract":"We study the CHAIN communication problem introduced by Cormode et al. [ICALP 2019]. It is a generalization of the well-studied INDEX problem. For $k\\geq 1$, in CHAIN$_{n,k}$, there are $k$ instances of INDEX, all with the same answer. They are shared between $k+1$ players as follows. Player 1 has the first string $X^1 \\in \\{0,1\\}^n$, player 2 has the first index $\\sigma^1 \\in [n]$ and the second string $X^2 \\in \\{0,1\\}^n$, player 3 has the second index $\\sigma^2 \\in [n]$ along with the third string $X^3 \\in \\{0,1\\}^n$, and so on. Player $k+1$ has the last index $\\sigma^k \\in [n]$. The communication is one way from each player to the next, starting from player 1 to player 2, then from player 2 to player 3 and so on. Player $k+1$, after receiving the message from player $k$, has to output a single bit which is the answer to all $k$ instances of INDEX.   It was proved that the CHAIN$_{n,k}$ problem requires $\\Omega(n/k^2)$ communication by Cormode et al., and they used it to prove streaming lower bounds for approximation of maximum independent sets. Subsequently, it was used by Feldman et al. [STOC 2020] to prove lower bounds for streaming submodular maximization. However, these works do not get optimal bounds on the communication complexity of CHAIN$_{n,k}$, and in fact, it was conjectured by Cormode et al. that $\\Omega(n)$ bits are necessary, for any $k$.   As our main result, we prove the optimal lower bound of $\\Omega(n)$ for CHAIN$_{n,k}$. This settles the open conjecture of Cormode et al. in the affirmative. The key technique is to use information theoretic tools to analyze protocols over the Jensen-Shannon divergence measure, as opposed to total variation distance. As a corollary, we get an improved lower bound for approximation of maximum independent set in vertex arrival streams through a reduction from CHAIN directly.","sentences":["We study the CHAIN communication problem introduced by Cormode et al.","[ICALP 2019].","It is a generalization of the well-studied INDEX problem.","For $k\\geq 1$, in CHAIN$_{n,k}$, there are $k$ instances of INDEX, all with the same answer.","They are shared between $k+1$ players as follows.","Player 1 has the first string $X^1 \\in \\{0,1\\}^n$, player 2 has the first index $\\sigma^1","\\in [n]$ and the second string $X^2 \\in \\{0,1\\}^n$, player 3 has the second index $\\sigma^2 \\in","[n]$ along with the third string $X^3 \\in \\{0,1\\}^n$, and so on.","Player $k+1$ has the last index $\\sigma^k \\in","[n]$. The communication is one way from each player to the next, starting from player 1 to player 2, then from player 2 to player 3 and so on.","Player $k+1$, after receiving the message from player $k$, has to output a single bit which is the answer to all $k$ instances of INDEX.   ","It was proved that the CHAIN$_{n,k}$ problem requires $\\Omega(n/k^2)$ communication by Cormode et al., and they used it to prove streaming lower bounds for approximation of maximum independent sets.","Subsequently, it was used by Feldman et al.","[STOC 2020] to prove lower bounds for streaming submodular maximization.","However, these works do not get optimal bounds on the communication complexity of CHAIN$_{n,k}$, and in fact, it was conjectured by Cormode et al. that $\\Omega(n)$ bits are necessary, for any $k$.   ","As our main result, we prove the optimal lower bound of $\\Omega(n)$ for CHAIN$_{n,k}$. This settles the open conjecture of Cormode et al. in the affirmative.","The key technique is to use information theoretic tools to analyze protocols over the Jensen-Shannon divergence measure, as opposed to total variation distance.","As a corollary, we get an improved lower bound for approximation of maximum independent set in vertex arrival streams through a reduction from CHAIN directly."],"url":"http://arxiv.org/abs/2404.07026v1","category":"cs.CC"}
{"created":"2024-04-10 14:18:47","title":"Secrecy Enhancement for UAV-enabled Integrated Sensing and Communication System","abstract":"In this correspondence, we propose an unmanned aerial vehicle (UAV)-enabled integrated sensing and communication (ISAC) system, where a full-duplex UAV equipped with uniform planar array (UPA) is adopted as a base station for the multiuser downlink communications, while sensing and jamming a passive ground eavesdropper. The goal of this work is to maximize the sum secrecy rate of ground users subject to the constraints of sensing accuracy and UAV's operational capability by jointly optimizing the transceiver beamforming and UAV's trajectory. To this end, we develop the algorithmic solution based on block coordinate descent (BCD) and semidefinite programming (SDP) relaxation techniques, whose performance is verified via simulations indicating its efficacy in improving communication security with the sufficient mission period.","sentences":["In this correspondence, we propose an unmanned aerial vehicle (UAV)-enabled integrated sensing and communication (ISAC) system, where a full-duplex UAV equipped with uniform planar array (UPA) is adopted as a base station for the multiuser downlink communications, while sensing and jamming a passive ground eavesdropper.","The goal of this work is to maximize the sum secrecy rate of ground users subject to the constraints of sensing accuracy and UAV's operational capability by jointly optimizing the transceiver beamforming and UAV's trajectory.","To this end, we develop the algorithmic solution based on block coordinate descent (BCD) and semidefinite programming (SDP) relaxation techniques, whose performance is verified via simulations indicating its efficacy in improving communication security with the sufficient mission period."],"url":"http://arxiv.org/abs/2404.07024v1","category":"eess.SP"}
{"created":"2024-04-10 14:14:42","title":"A 4x32Gb/s 1.8pJ/bit Collaborative Baud-Rate CDR with Background Eye-Climbing Algorithm and Low-Power Global Clock Distribution","abstract":"This paper presents design techniques for an energy-efficient multi-lane receiver (RX) with baud-rate clock and data recovery (CDR), which is essential for high-throughput low-latency communication in high-performance computing systems. The proposed low-power global clock distribution not only significantly reduces power consumption across multi-lane RXs but is capable of compensating for the frequency offset without any phase interpolators. To this end, a fractional divider controlled by CDR is placed close to the global phase locked loop. Moreover, in order to address the sub-optimal lock point of conventional baud-rate phase detectors, the proposed CDR employs a background eye-climbing algorithm, which optimizes the sampling phase and maximizes the vertical eye margin (VEM). Fabricated in a 28nm CMOS process, the proposed 4x32Gb/s RX shows a low integrated fractional spur of -40.4dBc at a 2500ppm frequency offset. Furthermore, it improves bit-error-rate performance by increasing the VEM by 17%. The entire RX achieves the energy efficiency of 1.8pJ/bit with the aggregate data rate of 128Gb/s.","sentences":["This paper presents design techniques for an energy-efficient multi-lane receiver (RX) with baud-rate clock and data recovery (CDR), which is essential for high-throughput low-latency communication in high-performance computing systems.","The proposed low-power global clock distribution not only significantly reduces power consumption across multi-lane RXs but is capable of compensating for the frequency offset without any phase interpolators.","To this end, a fractional divider controlled by CDR is placed close to the global phase locked loop.","Moreover, in order to address the sub-optimal lock point of conventional baud-rate phase detectors, the proposed CDR employs a background eye-climbing algorithm, which optimizes the sampling phase and maximizes the vertical eye margin (VEM).","Fabricated in a 28nm CMOS process, the proposed 4x32Gb/s RX shows a low integrated fractional spur of -40.4dBc at a 2500ppm frequency offset.","Furthermore, it improves bit-error-rate performance by increasing the VEM by 17%.","The entire RX achieves the energy efficiency of 1.8pJ/bit with the aggregate data rate of 128Gb/s."],"url":"http://arxiv.org/abs/2404.07021v1","category":"eess.SP"}
{"created":"2024-04-10 14:09:46","title":"Learned Finite-Time Consensus for Distributed Optimization","abstract":"Most algorithms for decentralized learning employ a consensus or diffusion mechanism to drive agents to a common solution of a global optimization problem. Generally this takes the form of linear averaging, at a rate of contraction determined by the mixing rate of the underlying network topology. For very sparse graphs this can yield a bottleneck, slowing down the convergence of the learning algorithm. We show that a sequence of matrices achieving finite-time consensus can be learned for unknown graph topologies in a decentralized manner by solving a constrained matrix factorization problem. We demonstrate numerically the benefit of the resulting scheme in both structured and unstructured graphs.","sentences":["Most algorithms for decentralized learning employ a consensus or diffusion mechanism to drive agents to a common solution of a global optimization problem.","Generally this takes the form of linear averaging, at a rate of contraction determined by the mixing rate of the underlying network topology.","For very sparse graphs this can yield a bottleneck, slowing down the convergence of the learning algorithm.","We show that a sequence of matrices achieving finite-time consensus can be learned for unknown graph topologies in a decentralized manner by solving a constrained matrix factorization problem.","We demonstrate numerically the benefit of the resulting scheme in both structured and unstructured graphs."],"url":"http://arxiv.org/abs/2404.07018v1","category":"math.OC"}
{"created":"2024-04-10 14:04:06","title":"POD Suboptimal Control of Evolution Problems: Theory and Applications","abstract":"The work is organized as follows. First an introduction is given in Chapter 1. In Chapter 2 we introduce the POD method in finite and infinite-dimensional Hilbert spaces and discuss various applications. Chapter 3 is devoted to to POD-based Galerkin schemes for evolution problems. Mainly, we study linear problems taking different discretization methods into account. We provide a certified a-priori and a-posteriori error analysis. Furthermore, the numerical realizations are explained and illustrated by test examples. Quadratic programming problems governed by liner evolution problems are investigated in Chapter 4. As in Chapter 3 we present a certified a-priori and a-posteriori error analysis. Moreover, we discuss basis update strategies. In Chapter 5 we give an outlook to further directions in reduced-order modeling in optimal control and optimization. More precisely, a nonlinear optimal control problem is studied. Moreover, state-constrained optimization problems are solved by a tailored combination of primal-dual active set methods and POD-aesed reduced-order modeling. Furthermore, POD Galerkin methods for multiobjective optimal control problems are investigated. Finally, some required theoretical foundations are summarized in the appendix.","sentences":["The work is organized as follows.","First an introduction is given in Chapter 1.","In Chapter 2 we introduce the POD method in finite and infinite-dimensional Hilbert spaces and discuss various applications.","Chapter 3 is devoted to to POD-based Galerkin schemes for evolution problems.","Mainly, we study linear problems taking different discretization methods into account.","We provide a certified a-priori and a-posteriori error analysis.","Furthermore, the numerical realizations are explained and illustrated by test examples.","Quadratic programming problems governed by liner evolution problems are investigated in Chapter 4.","As in Chapter 3 we present a certified a-priori and a-posteriori error analysis.","Moreover, we discuss basis update strategies.","In Chapter 5 we give an outlook to further directions in reduced-order modeling in optimal control and optimization.","More precisely, a nonlinear optimal control problem is studied.","Moreover, state-constrained optimization problems are solved by a tailored combination of primal-dual active set methods and POD-aesed reduced-order modeling.","Furthermore, POD Galerkin methods for multiobjective optimal control problems are investigated.","Finally, some required theoretical foundations are summarized in the appendix."],"url":"http://arxiv.org/abs/2404.07015v1","category":"math.OC"}
{"created":"2024-04-10 13:58:29","title":"Zero-one Laws for a Control Problem with Random Action Sets","abstract":"In many control problems there is only limited information about the actions that will be available at future stages. We introduce a framework where the Controller chooses actions $a_{0}, a_{1}, \\ldots$, one at a time. Her goal is to maximize the probability that the infinite sequence $(a_{0}, a_{1}, \\ldots)$ is an element of a given subset $G$ of $\\mathbb{N}^{\\mathbb{N}}$. The set $G$, called the goal, is assumed to be a Borel tail set. The Controller's choices are restricted: having taken a sequence $h_{t} = (a_{0}, \\ldots, a_{t-1})$ of actions prior to stage $t \\in \\mathbb{N}$, she must choose an action $a_{t}$ at stage $t$ from a non-empty, finite subset $A(h_{t})$ of $\\mathbb{N}$. The set $A(h_{t})$ is chosen from a distribution $p_{t}$, independently over all $t \\in \\mathbb{N}$ and all $h_{t} \\in \\mathbb{N}^{t}$. We consider several information structures defined by how far ahead into the future the Controller knows what actions will be available.   In the special case where all the action sets are singletons (and thus the Controller is a dummy), Kolmogorov's 0-1 law says that the probability for the goal to be reached is 0 or 1. We construct a number of counterexamples to show that in general the value of the control problem can be strictly between 0 and 1, and derive several sufficient conditions for the 0-1 ``law\" to hold.","sentences":["In many control problems there is only limited information about the actions that will be available at future stages.","We introduce a framework where the Controller chooses actions $a_{0}, a_{1}, \\ldots$, one at a time.","Her goal is to maximize the probability that the infinite sequence $(a_{0}, a_{1}, \\ldots)$ is an element of a given subset $G$ of $\\mathbb{N}^{\\mathbb{N}}$. The set $G$, called the goal, is assumed to be a Borel tail set.","The Controller's choices are restricted: having taken a sequence $h_{t} = (a_{0}, \\ldots, a_{t-1})$ of actions prior to stage $t \\in \\mathbb{N}$, she must choose an action $a_{t}$ at stage $t$ from a non-empty, finite subset $A(h_{t})$ of $\\mathbb{N}$. The set $A(h_{t})$ is chosen from a distribution $p_{t}$, independently over all $t \\in \\mathbb{N}$ and","all $h_{t} \\in \\mathbb{N}^{t}$.","We consider several information structures defined by how far ahead into the future the Controller knows what actions will be available.   ","In the special case where all the action sets are singletons (and thus the Controller is a dummy), Kolmogorov's 0-1 law says that the probability for the goal to be reached is 0 or 1.","We construct a number of counterexamples to show that in general the value of the control problem can be strictly between 0 and 1, and derive several sufficient conditions for the 0-1 ``law\" to hold."],"url":"http://arxiv.org/abs/2404.07012v1","category":"math.OC"}
{"created":"2024-04-10 13:53:33","title":"An asymptotically optimal algorithm for generating bin cardinalities","abstract":"In the balls-into-bins setting, $n$ balls are thrown uniformly at random into $n$ bins. The na\\\"{i}ve way to generate the final load vector takes $\\Theta(n)$ time. However, it is well-known that this load vector has with high probability bin cardinalities of size $\\Theta(\\frac{\\log n}{\\log \\log n})$. Here, we present an algorithm in the RAM model that generates the bin cardinalities of the final load vector in the optimal $\\Theta(\\frac{\\log n}{\\log \\log n})$ time in expectation and with high probability.   Further, the algorithm that we present is still optimal for any $m \\in [n, n \\log n]$ balls and can also be used as a building block to efficiently simulate more involved load balancing algorithms. In particular, for the Two-Choice algorithm, which samples two bins in each step and allocates to the least-loaded of the two, we obtain roughly a quadratic speed-up over the na\\\"{i}ve simulation.","sentences":["In the balls-into-bins setting, $n$ balls are thrown uniformly at random into $n$ bins.","The na\\\"{i}ve way to generate the final load vector takes $\\Theta(n)$ time.","However, it is well-known that this load vector has with high probability bin cardinalities of size $\\Theta(\\frac{\\log n}{\\log \\log n})$.","Here, we present an algorithm in the RAM model that generates the bin cardinalities of the final load vector in the optimal $\\Theta(\\frac{\\log n}{\\log \\log n})$ time in expectation and with high probability.   ","Further, the algorithm that we present is still optimal for any $m \\in [n, n \\log n]$ balls and can also be used as a building block to efficiently simulate more involved load balancing algorithms.","In particular, for the Two-Choice algorithm, which samples two bins in each step and allocates to the least-loaded of the two, we obtain roughly a quadratic speed-up over the na\\\"{i}ve simulation."],"url":"http://arxiv.org/abs/2404.07011v1","category":"cs.DS"}
{"created":"2024-04-10 13:52:45","title":"Gaining or losing perspective for convex multivariate functions on box domains","abstract":"MINLO (mixed-integer nonlinear optimization) formulations of the disjunction between the origin and a polytope via a binary indicator variable is broadly used in nonlinear combinatorial optimization for modeling a fixed cost associated with carrying out a group of activities and a convex cost function associated with the levels of the activities. The perspective relaxation of such models is often used to solve to global optimality in a branch-and-bound context, but it typically requires suitable conic solvers and is not compatible with general-purpose NLP software in the presence of other classes of constraints. This motivates the investigation of when simpler but weaker relaxations may be adequate. Comparing the volume (i.e., Lebesgue measure) of the relaxations as a measure of tightness, we lift some of the results related to the simplex case to the box case. In order to compare the volumes of different relaxations in the box case, it is necessary to find an appropriate concave upper bound that preserves the convexity and is minimal, which is more difficult than in the simplex case. To address the challenge beyond the simplex case, the triangulation approach is used.","sentences":["MINLO (mixed-integer nonlinear optimization) formulations of the disjunction between the origin and a polytope via a binary indicator variable is broadly used in nonlinear combinatorial optimization for modeling a fixed cost associated with carrying out a group of activities and a convex cost function associated with the levels of the activities.","The perspective relaxation of such models is often used to solve to global optimality in a branch-and-bound context, but it typically requires suitable conic solvers and is not compatible with general-purpose NLP software in the presence of other classes of constraints.","This motivates the investigation of when simpler but weaker relaxations may be adequate.","Comparing the volume (i.e., Lebesgue measure) of the relaxations as a measure of tightness, we lift some of the results related to the simplex case to the box case.","In order to compare the volumes of different relaxations in the box case, it is necessary to find an appropriate concave upper bound that preserves the convexity and is minimal, which is more difficult than in the simplex case.","To address the challenge beyond the simplex case, the triangulation approach is used."],"url":"http://arxiv.org/abs/2404.07010v1","category":"math.OC"}
{"created":"2024-04-10 13:45:52","title":"Opinion dynamics of two populations with time-delayed coupling","abstract":"We study a Hegselmann-Krause type opinion formation model for a system of two populations. The two groups interact with each other via subsets of individuals, namely the leaders, and natural time delay effects are considered. By using careful estimates of the system's trajectories, we are able to prove an asymptotic convergence to consensus result. Some numerical tests illustrate the theoretical result and point out some possible applications.","sentences":["We study a Hegselmann-Krause type opinion formation model for a system of two populations.","The two groups interact with each other via subsets of individuals, namely the leaders, and natural time delay effects are considered.","By using careful estimates of the system's trajectories, we are able to prove an asymptotic convergence to consensus result.","Some numerical tests illustrate the theoretical result and point out some possible applications."],"url":"http://arxiv.org/abs/2404.07007v1","category":"math.OC"}
{"created":"2024-04-10 13:05:39","title":"Quantum Network Tomography via Learning Isometries on Stiefel Manifold","abstract":"Explicit mathematical reconstructions of quantum networks play a significant role in developing quantum information science. However, tremendous parameter requirements and physical constraint implementations have become computationally non-ignorable encumbrances. In this work, we propose an efficient method for quantum network tomography by learning isometries on the Stiefel manifold. Tasks of reconstructing quantum networks are tackled by solving a series of unconstrained optimization problems with significantly less parameters. The step-wise isometry estimation shows the capability for providing information of the truncated quantum comb while processing the tomography. Remarkably, this method enables the compressive QCT by specifying the dimensions of isometries. As a result, our proposed method exhibits high accuracy and efficiency.","sentences":["Explicit mathematical reconstructions of quantum networks play a significant role in developing quantum information science.","However, tremendous parameter requirements and physical constraint implementations have become computationally non-ignorable encumbrances.","In this work, we propose an efficient method for quantum network tomography by learning isometries on the Stiefel manifold.","Tasks of reconstructing quantum networks are tackled by solving a series of unconstrained optimization problems with significantly less parameters.","The step-wise isometry estimation shows the capability for providing information of the truncated quantum comb while processing the tomography.","Remarkably, this method enables the compressive QCT by specifying the dimensions of isometries.","As a result, our proposed method exhibits high accuracy and efficiency."],"url":"http://arxiv.org/abs/2404.06988v1","category":"quant-ph"}
{"created":"2024-04-10 13:00:46","title":"Algebraic Proofs of Path Disconnectedness using Time-Dependent Barrier Functions","abstract":"Two subsets of a given set are path-disconnected if they lie in different connected components of the larger set. Verification of path-disconnectedness is essential in proving the infeasibility of motion planning and trajectory optimization algorithms. We formulate path-disconnectedness as the infeasibility of a single-integrator control task to move between an initial set and a target set in a sufficiently long time horizon. This control-infeasibility task is certified through the generation of a time-dependent barrier function that separates the initial and final sets. The existence of a time-dependent barrier function is a necessary and sufficient condition for path-disconnectedness under compactness conditions. Numerically, the search for a polynomial barrier function is formulated using the moment-sum-of-squares hierarchy of semidefinite programs. The barrier function proves path-disconnectedness at a sufficiently large polynomial degree. The computational complexity of these semidefinite programs can be reduced by elimination of the control variables. Disconnectedness proofs are synthesized for example systems.","sentences":["Two subsets of a given set are path-disconnected if they lie in different connected components of the larger set.","Verification of path-disconnectedness is essential in proving the infeasibility of motion planning and trajectory optimization algorithms.","We formulate path-disconnectedness as the infeasibility of a single-integrator control task to move between an initial set and a target set in a sufficiently long time horizon.","This control-infeasibility task is certified through the generation of a time-dependent barrier function that separates the initial and final sets.","The existence of a time-dependent barrier function is a necessary and sufficient condition for path-disconnectedness under compactness conditions.","Numerically, the search for a polynomial barrier function is formulated using the moment-sum-of-squares hierarchy of semidefinite programs.","The barrier function proves path-disconnectedness at a sufficiently large polynomial degree.","The computational complexity of these semidefinite programs can be reduced by elimination of the control variables.","Disconnectedness proofs are synthesized for example systems."],"url":"http://arxiv.org/abs/2404.06985v1","category":"math.OC"}
{"created":"2024-04-10 12:38:38","title":"Deep Reinforcement Learning for Mobile Robot Path Planning","abstract":"Path planning is an important problem with the the applications in many aspects, such as video games, robotics etc. This paper proposes a novel method to address the problem of Deep Reinforcement Learning (DRL) based path planning for a mobile robot. We design DRL-based algorithms, including reward functions, and parameter optimization, to avoid time-consuming work in a 2D environment. We also designed an Two-way search hybrid A* algorithm to improve the quality of local path planning. We transferred the designed algorithm to a simple embedded environment to test the computational load of the algorithm when running on a mobile robot. Experiments show that when deployed on a robot platform, the DRL-based algorithm in this article can achieve better planning results and consume less computing resources.","sentences":["Path planning is an important problem with the the applications in many aspects, such as video games, robotics etc.","This paper proposes a novel method to address the problem of Deep Reinforcement Learning (DRL) based path planning for a mobile robot.","We design DRL-based algorithms, including reward functions, and parameter optimization, to avoid time-consuming work in a 2D environment.","We also designed an Two-way search hybrid A* algorithm to improve the quality of local path planning.","We transferred the designed algorithm to a simple embedded environment to test the computational load of the algorithm when running on a mobile robot.","Experiments show that when deployed on a robot platform, the DRL-based algorithm in this article can achieve better planning results and consume less computing resources."],"url":"http://arxiv.org/abs/2404.06974v1","category":"cs.RO"}
{"created":"2024-04-10 12:21:03","title":"Peak Time-Windowed Risk Estimation of Stochastic Processes","abstract":"This paper develops a method to upper-bound extreme-values of time-windowed risks for stochastic processes. Examples of such risks include the maximum average or 90% quantile of the current along a transmission line in any 5-minute window. This work casts the time-windowed risk analysis problem as an infinite-dimensional linear program in occupation measures. In particular, we employ the coherent risk measures of the mean and the expected shortfall (conditional value at risk) to define the maximal time-windowed risk along trajectories. The infinite-dimensional linear program must then be truncated into finite-dimensional optimization problems, such as by using the moment-sum of squares hierarchy of semidefinite programs. The infinite-dimensional linear program will have the same optimal value as the original nonconvex risk estimation task under compactness and regularity assumptions, and the sequence of semidefinite programs will converge to the true value under additional properties of algebraic characterization. The scheme is demonstrated for risk analysis of example stochastic processes.","sentences":["This paper develops a method to upper-bound extreme-values of time-windowed risks for stochastic processes.","Examples of such risks include the maximum average or 90% quantile of the current along a transmission line in any 5-minute window.","This work casts the time-windowed risk analysis problem as an infinite-dimensional linear program in occupation measures.","In particular, we employ the coherent risk measures of the mean and the expected shortfall (conditional value at risk) to define the maximal time-windowed risk along trajectories.","The infinite-dimensional linear program must then be truncated into finite-dimensional optimization problems, such as by using the moment-sum of squares hierarchy of semidefinite programs.","The infinite-dimensional linear program will have the same optimal value as the original nonconvex risk estimation task under compactness and regularity assumptions, and the sequence of semidefinite programs will converge to the true value under additional properties of algebraic characterization.","The scheme is demonstrated for risk analysis of example stochastic processes."],"url":"http://arxiv.org/abs/2404.06961v2","category":"math.OC"}
{"created":"2024-04-10 11:43:01","title":"Optimal Matching of Thermal Vibrations into Carbon Nanotubes","abstract":"Carbon nanotubes (CNTs) are promising candidates to improve the thermal conductivity of nano-composites. The main obstacle to these applications is the extremely high thermal boundary (Kapitza) resistance between the CNTs and their matrix. In this theoretical work our goal is to maximize the heat flux through the CNT by functionalizing the CNT ends. We use a Landauer approach to calculate and optimize the energy flux from a soft to a hard material in one dimension through a connecting continuous medium of varying elasticity and density. The transmission probability of phonons through the system is calculated both numerically and analytically. We find that over 90% of the maximum heat flux into CNT is possible for 1nm length of the intermediate material at room temperature (300K).","sentences":["Carbon nanotubes (CNTs) are promising candidates to improve the thermal conductivity of nano-composites.","The main obstacle to these applications is the extremely high thermal boundary (Kapitza) resistance between the CNTs and their matrix.","In this theoretical work our goal is to maximize the heat flux through the CNT by functionalizing the CNT ends.","We use a Landauer approach to calculate and optimize the energy flux from a soft to a hard material in one dimension through a connecting continuous medium of varying elasticity and density.","The transmission probability of phonons through the system is calculated both numerically and analytically.","We find that over 90% of the maximum heat flux into CNT is possible for 1nm length of the intermediate material at room temperature (300K)."],"url":"http://arxiv.org/abs/2404.06938v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-10 11:42:48","title":"Phenomenon of a stronger trapping behaviour in $\u039b$-type quantum systems with symmetry","abstract":"$\\Lambda$, $V$, $\\Xi$ (ladder), and other three-level quantum systems with one forbidden transition ($\\Lambda$-type systems) play an important role in quantum physics. Various applications require manipulation by such systems using as control shaped laser field. In this work, we study how degeneracy in energy states and Bohr frequencies of these systems affects the efficiency or difficulty of finding optimal shape of the control field. For this, we adopt the notion of higher order traps which was introduced in [A.N. Pechen and D.J. Tannor, Are there traps in quantum control landscapes? Phys. Rev. Lett. {\\bf 106}, 120402 (2011)], where second/third order traps were discovered for $\\Lambda$-type systems with one forbidden transition and with non-degenerate energy levels. We study control of such systems with and without denegeracy in their eigenstates and Bohr frequencies, and investigate how these degeneracies influence on the efficiency of optimizing the control laser field. We find that the degeneracy of Bohr frequencies in the $\\Xi$ system leads to the appearance of seventh order trap with a more significant attracting domain resulting in a more difficult optimization, while degeneracy in energy states of $\\Lambda$-type systems does not lead to increase of the order of the zero control trap compared to the non-degenerate case.","sentences":["$\\Lambda$, $V$, $\\Xi$ (ladder), and other three-level quantum systems with one forbidden transition ($\\Lambda$-type systems) play an important role in quantum physics.","Various applications require manipulation by such systems using as control shaped laser field.","In this work, we study how degeneracy in energy states and Bohr frequencies of these systems affects the efficiency or difficulty of finding optimal shape of the control field.","For this, we adopt the notion of higher order traps which was introduced in [A.N. Pechen and D.J. Tannor, Are there traps in quantum control landscapes?","Phys. Rev. Lett.","{\\bf 106}, 120402 (2011)], where second/third order traps were discovered for $\\Lambda$-type systems with one forbidden transition and with non-degenerate energy levels.","We study control of such systems with and without denegeracy in their eigenstates and Bohr frequencies, and investigate how these degeneracies influence on the efficiency of optimizing the control laser field.","We find that the degeneracy of Bohr frequencies in the $\\Xi$ system leads to the appearance of seventh order trap with a more significant attracting domain resulting in a more difficult optimization, while degeneracy in energy states of $\\Lambda$-type systems does not lead to increase of the order of the zero control trap compared to the non-degenerate case."],"url":"http://arxiv.org/abs/2404.06937v1","category":"quant-ph"}
{"created":"2024-04-10 11:32:06","title":"Improving prediction accuracy by choosing resampling distribution via cross-validation","abstract":"In a regression model, prediction is typically performed after model selection. The large variability in the model selection makes the prediction unstable. Thus, it is essential to reduce the variability in model selection and improve prediction accuracy. To achieve this goal, a parametric bootstrap smoothing can be applied. In this method, model selection is performed for each resampling from a parametric distribution, and these models are then averaged such that the distribution of the selected models is considered. Here, the prediction accuracy is highly dependent on the choice of a distribution for resampling. In particular, an experimental study shows that the choice of error variance significantly changes the distribution of the selected model and thus plays a key role in improving the prediction accuracy. We also observed that the true error variance does not always provide optimal prediction accuracy. Therefore, it would not always be appropriate to use unbiased estimators of the true parameters or standard estimators of the parameters for the resampling distribution. In this study, we propose employing cross validation to choose a suitable resampling distribution rather than unbiased estimators of parameters. Our proposed method was applied to electricity demand data. The results indicate that the proposed method provides a better prediction accuracy than the existing method.","sentences":["In a regression model, prediction is typically performed after model selection.","The large variability in the model selection makes the prediction unstable.","Thus, it is essential to reduce the variability in model selection and improve prediction accuracy.","To achieve this goal, a parametric bootstrap smoothing can be applied.","In this method, model selection is performed for each resampling from a parametric distribution, and these models are then averaged such that the distribution of the selected models is considered.","Here, the prediction accuracy is highly dependent on the choice of a distribution for resampling.","In particular, an experimental study shows that the choice of error variance significantly changes the distribution of the selected model and thus plays a key role in improving the prediction accuracy.","We also observed that the true error variance does not always provide optimal prediction accuracy.","Therefore, it would not always be appropriate to use unbiased estimators of the true parameters or standard estimators of the parameters for the resampling distribution.","In this study, we propose employing cross validation to choose a suitable resampling distribution rather than unbiased estimators of parameters.","Our proposed method was applied to electricity demand data.","The results indicate that the proposed method provides a better prediction accuracy than the existing method."],"url":"http://arxiv.org/abs/2404.06932v1","category":"stat.CO"}
{"created":"2024-04-10 11:20:27","title":"Restoring the topological edge states in a finite optical superlattice","abstract":"We consider the emergence of edge states in a finite optical lattice and show that the boundaries of the lattice play a decisive role for their location in the corresponding energy spectrum. We introduce a simple parametrisation of the boundaries of the optical lattice and demonstrate the existence of an optimal choice of the values of the parameters which lead to an approximate restoration of chiral symmetry. A crucial property of this optimization is the suppression of tunneling between next-nearest neighboring wells of the lattice. This in turn allows the mapping of the optical lattice set-up to a finite SSH model. The topological character of the emerging edge states is discussed.","sentences":["We consider the emergence of edge states in a finite optical lattice and show that the boundaries of the lattice play a decisive role for their location in the corresponding energy spectrum.","We introduce a simple parametrisation of the boundaries of the optical lattice and demonstrate the existence of an optimal choice of the values of the parameters which lead to an approximate restoration of chiral symmetry.","A crucial property of this optimization is the suppression of tunneling between next-nearest neighboring wells of the lattice.","This in turn allows the mapping of the optical lattice set-up to a finite SSH model.","The topological character of the emerging edge states is discussed."],"url":"http://arxiv.org/abs/2404.06924v1","category":"quant-ph"}
{"created":"2024-04-10 11:04:24","title":"Set-Encoder: Permutation-Invariant Inter-Passage Attention for Listwise Passage Re-Ranking with Cross-Encoders","abstract":"Cross-encoders are effective passage re-rankers. But when re-ranking multiple passages at once, existing cross-encoders inefficiently optimize the output ranking over several input permutations, as their passage interactions are not permutation-invariant. Moreover, their high memory footprint constrains the number of passages during listwise training. To tackle these issues, we propose the Set-Encoder, a new cross-encoder architecture that (1) introduces inter-passage attention with parallel passage processing to ensure permutation invariance between input passages, and that (2) uses fused-attention kernels to enable training with more passages at a time. In experiments on TREC Deep Learning and TIREx, the Set-Encoder is more effective than previous cross-encoders with a similar number of parameters. Compared to larger models, the Set-Encoder is more efficient and either on par or even more effective.","sentences":["Cross-encoders are effective passage re-rankers.","But when re-ranking multiple passages at once, existing cross-encoders inefficiently optimize the output ranking over several input permutations, as their passage interactions are not permutation-invariant.","Moreover, their high memory footprint constrains the number of passages during listwise training.","To tackle these issues, we propose the Set-Encoder, a new cross-encoder architecture that (1) introduces inter-passage attention with parallel passage processing to ensure permutation invariance between input passages, and that (2) uses fused-attention kernels to enable training with more passages at a time.","In experiments on TREC Deep Learning and TIREx, the Set-Encoder is more effective than previous cross-encoders with a similar number of parameters.","Compared to larger models, the Set-Encoder is more efficient and either on par or even more effective."],"url":"http://arxiv.org/abs/2404.06912v2","category":"cs.IR"}
{"created":"2024-04-10 09:45:32","title":"The G\u00f6del Universe as the Lie Group with left-invariant Lorentz metric","abstract":"The author studies the G\\\"odel Universe as the Lie group with left-invariant Lorentz metric. The expressions for timelike and isotropic geodesics in elementary functions are found by methods of geometric theory of optimal control for the search of geodesics on Lie groups with left-invariant (sub-)Lorentz metrics. It is proved that the G\\\"odel Universe has no closed timelike or isotropic geodesics.","sentences":["The author studies the G\\\"odel Universe as the Lie group with left-invariant Lorentz metric.","The expressions for timelike and isotropic geodesics in elementary functions are found by methods of geometric theory of optimal control for the search of geodesics on Lie groups with left-invariant (sub-)Lorentz metrics.","It is proved that the G\\\"odel Universe has no closed timelike or isotropic geodesics."],"url":"http://arxiv.org/abs/2404.06866v1","category":"math.DG"}
{"created":"2024-04-10 09:38:28","title":"Electron acceleration and X-ray generation from near-critical-density carbon nanotube foams driven by moderately relativistic lasers","abstract":"Direct laser acceleration of electrons in near-critical-density (NCD) carbon nanotube foams (CNFs) has its advantages in the high-efficiency generation of relativistic electrons and broadband X-rays. Here, we report the first simultaneous measurement on the spectra of laser-driven electrons and X-rays from CNFs at moderately relativistic intensities of around 5\\times{10}^{19}\\ W/cm^2.\\ The density and thickness of the CNFs were scanned in the experiments, indicating the optimized electrons temperature of 5.5 MeV and X-ray critical energy of 5 keV. Two-dimensional (2D) particle-in-cell (PIC) simulations confirm that the electrons, with a temperature significantly higher than the pondermotive scale, are directly accelerated by the laser along the NCD plasma channel, while the bright X-rays are emitted by these electrons through betatron radiation or Thomson backscattering inside the channel. The simultaneously generated electrons and X-rays, automatically synchronized with the femtosecond laser driver, are suitable for applications such as bi-modal radiography.","sentences":["Direct laser acceleration of electrons in near-critical-density (NCD) carbon nanotube foams (CNFs) has its advantages in the high-efficiency generation of relativistic electrons and broadband X-rays.","Here, we report the first simultaneous measurement on the spectra of laser-driven electrons and X-rays from CNFs at moderately relativistic intensities of around 5\\times{10}^{19}\\ W/cm^2.\\ The density and thickness of the CNFs were scanned in the experiments, indicating the optimized electrons temperature of 5.5 MeV and X-ray critical energy of 5 keV. Two-dimensional (2D) particle-in-cell (PIC) simulations confirm that the electrons, with a temperature significantly higher than the pondermotive scale, are directly accelerated by the laser along the NCD plasma channel, while the bright X-rays are emitted by these electrons through betatron radiation or Thomson backscattering inside the channel.","The simultaneously generated electrons and X-rays, automatically synchronized with the femtosecond laser driver, are suitable for applications such as bi-modal radiography."],"url":"http://arxiv.org/abs/2404.06862v1","category":"physics.plasm-ph"}
{"created":"2024-04-10 09:38:13","title":"A scalable 2-local architecture for quantum annealing of all-to-all Ising models","abstract":"Achieving dense connectivities is a challenge for most quantum computing platforms today, and a particularly crucial one for the case of quantum annealing applications. In this context, we present a scalable architecture for quantum annealers defined on a graph of degree $d=3$ and containing exclusively 2-local interactions to realize an all-to-all connected Ising model. This amounts to an efficient braiding of logical chains of qubits which can be derived from a description of the problem in terms of triangles. We also devise strategies to address the challenges of scalable architectures, such as the faster shrinking of the gap due to the larger physical Hilbert space, based on driver Hamiltonians more suited to the symmetries of the logical solution space. We thus show an alternative route to scale up devices dedicated to classical optimization tasks within the quantum annealing paradigm.","sentences":["Achieving dense connectivities is a challenge for most quantum computing platforms today, and a particularly crucial one for the case of quantum annealing applications.","In this context, we present a scalable architecture for quantum annealers defined on a graph of degree $d=3$ and containing exclusively 2-local interactions to realize an all-to-all connected Ising model.","This amounts to an efficient braiding of logical chains of qubits which can be derived from a description of the problem in terms of triangles.","We also devise strategies to address the challenges of scalable architectures, such as the faster shrinking of the gap due to the larger physical Hilbert space, based on driver Hamiltonians more suited to the symmetries of the logical solution space.","We thus show an alternative route to scale up devices dedicated to classical optimization tasks within the quantum annealing paradigm."],"url":"http://arxiv.org/abs/2404.06861v1","category":"quant-ph"}
{"created":"2024-04-10 09:25:56","title":"Revealing mechanism of pore defect formation in laser directed energy deposition of aluminum alloy via in-situ synchrotron X-ray imaging","abstract":"Laser metal additive manufacturing technology is capable of producing components with complex geometries and compositions that cannot be realized by conventional manufacturing methods. However, a large number of pores generated during the additive manufacturing process greatly affect the mechanical properties of the additively manufactured parts, and the mechanism of such pore generation has not been revealed by direct observation clearly. Here, we report the mechanism of pore generation in the laser direct energy deposition process as revealed by {\\it in-situ} high-speed high-resolution synchrotron X-ray imaging. We found that dissolution and re-precipitation of external gases and precipitation of metal vapors are the two main mechanisms of pore formation. We further explored the effects of different process parameters on the generation of pores and optimized the process to suppress pore generation. This work provides important insights into the formation of porosity defects during laser metal additive manufacturing, and can provide guidance for related process optimization.","sentences":["Laser metal additive manufacturing technology is capable of producing components with complex geometries and compositions that cannot be realized by conventional manufacturing methods.","However, a large number of pores generated during the additive manufacturing process greatly affect the mechanical properties of the additively manufactured parts, and the mechanism of such pore generation has not been revealed by direct observation clearly.","Here, we report the mechanism of pore generation in the laser direct energy deposition process as revealed by {\\it in-situ} high-speed high-resolution synchrotron X-ray imaging.","We found that dissolution and re-precipitation of external gases and precipitation of metal vapors are the two main mechanisms of pore formation.","We further explored the effects of different process parameters on the generation of pores and optimized the process to suppress pore generation.","This work provides important insights into the formation of porosity defects during laser metal additive manufacturing, and can provide guidance for related process optimization."],"url":"http://arxiv.org/abs/2404.06853v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-10 09:24:54","title":"UDiFF: Generating Conditional Unsigned Distance Fields with Optimal Wavelet Diffusion","abstract":"Diffusion models have shown remarkable results for image generation, editing and inpainting. Recent works explore diffusion models for 3D shape generation with neural implicit functions, i.e., signed distance function and occupancy function. However, they are limited to shapes with closed surfaces, which prevents them from generating diverse 3D real-world contents containing open surfaces. In this work, we present UDiFF, a 3D diffusion model for unsigned distance fields (UDFs) which is capable to generate textured 3D shapes with open surfaces from text conditions or unconditionally. Our key idea is to generate UDFs in spatial-frequency domain with an optimal wavelet transformation, which produces a compact representation space for UDF generation. Specifically, instead of selecting an appropriate wavelet transformation which requires expensive manual efforts and still leads to large information loss, we propose a data-driven approach to learn the optimal wavelet transformation for UDFs. We evaluate UDiFF to show our advantages by numerical and visual comparisons with the latest methods on widely used benchmarks. Page: https://weiqi-zhang.github.io/UDiFF.","sentences":["Diffusion models have shown remarkable results for image generation, editing and inpainting.","Recent works explore diffusion models for 3D shape generation with neural implicit functions, i.e., signed distance function and occupancy function.","However, they are limited to shapes with closed surfaces, which prevents them from generating diverse 3D real-world contents containing open surfaces.","In this work, we present UDiFF, a 3D diffusion model for unsigned distance fields (UDFs) which is capable to generate textured 3D shapes with open surfaces from text conditions or unconditionally.","Our key idea is to generate UDFs in spatial-frequency domain with an optimal wavelet transformation, which produces a compact representation space for UDF generation.","Specifically, instead of selecting an appropriate wavelet transformation which requires expensive manual efforts and still leads to large information loss, we propose a data-driven approach to learn the optimal wavelet transformation for UDFs.","We evaluate UDiFF to show our advantages by numerical and visual comparisons with the latest methods on widely used benchmarks.","Page: https://weiqi-zhang.github.io/UDiFF."],"url":"http://arxiv.org/abs/2404.06851v1","category":"cs.CV"}
{"created":"2024-04-10 09:18:01","title":"Quadratically Regularized Optimal Transport: Existence and Multiplicity of Potentials","abstract":"The optimal transport problem with quadratic regularization is useful when sparse couplings are desired. The density of the optimal coupling is described by two functions called potentials; equivalently, potentials can be defined as a solution of the dual problem. We prove the existence of potentials for a general square-integrable cost. Potentials are not necessarily unique, a phenomenon directly related to sparsity of the optimal support. For discrete problems, we describe the family of all potentials based on the connected components of the support, for a graph-theoretic notion of connectedness. On the other hand, we show that continuous problems have unique potentials under standard regularity assumptions, regardless of sparsity. Using potentials, we prove that the optimal support is indeed sparse for small regularization parameter in a continuous setting with quadratic cost, which seems to be the first theoretical guarantee for sparsity in this context.","sentences":["The optimal transport problem with quadratic regularization is useful when sparse couplings are desired.","The density of the optimal coupling is described by two functions called potentials; equivalently, potentials can be defined as a solution of the dual problem.","We prove the existence of potentials for a general square-integrable cost.","Potentials are not necessarily unique, a phenomenon directly related to sparsity of the optimal support.","For discrete problems, we describe the family of all potentials based on the connected components of the support, for a graph-theoretic notion of connectedness.","On the other hand, we show that continuous problems have unique potentials under standard regularity assumptions, regardless of sparsity.","Using potentials, we prove that the optimal support is indeed sparse for small regularization parameter in a continuous setting with quadratic cost, which seems to be the first theoretical guarantee for sparsity in this context."],"url":"http://arxiv.org/abs/2404.06847v1","category":"math.OC"}
{"created":"2024-04-10 09:17:22","title":"Register Your Forests: Decision Tree Ensemble Optimization by Explicit CPU Register Allocation","abstract":"Bringing high-level machine learning models to efficient and well-suited machine implementations often invokes a bunch of tools, e.g.~code generators, compilers, and optimizers. Along such tool chains, abstractions have to be applied. This leads to not optimally used CPU registers. This is a shortcoming, especially in resource constrained embedded setups. In this work, we present a code generation approach for decision tree ensembles, which produces machine assembly code within a single conversion step directly from the high-level model representation. Specifically, we develop various approaches to effectively allocate registers for the inference of decision tree ensembles. Extensive evaluations of the proposed method are conducted in comparison to the basic realization of C code from the high-level machine learning model and succeeding compilation. The results show that the performance of decision tree ensemble inference can be significantly improved (by up to $\\approx1.6\\times$), if the methods are applied carefully to the appropriate scenario.","sentences":["Bringing high-level machine learning models to efficient and well-suited machine implementations often invokes a bunch of tools, e.g.~code generators, compilers, and optimizers.","Along such tool chains, abstractions have to be applied.","This leads to not optimally used CPU registers.","This is a shortcoming, especially in resource constrained embedded setups.","In this work, we present a code generation approach for decision tree ensembles, which produces machine assembly code within a single conversion step directly from the high-level model representation.","Specifically, we develop various approaches to effectively allocate registers for the inference of decision tree ensembles.","Extensive evaluations of the proposed method are conducted in comparison to the basic realization of C code from the high-level machine learning model and succeeding compilation.","The results show that the performance of decision tree ensemble inference can be significantly improved (by up to $\\approx1.6\\times$), if the methods are applied carefully to the appropriate scenario."],"url":"http://arxiv.org/abs/2404.06846v1","category":"cs.LG"}
{"created":"2024-04-10 08:38:09","title":"EMF Mitigation via 5G and 6G MAC Scheduling","abstract":"High antenna directivity allows for high throughput transmission but also increases the exposure to electromagnetic field (EMF) of the end-users. Health regulations impose limitations on the incident power density, that generate a negative impact on network performance. In this work we focus at the slot-by-slot operations of a cellular Medium Access Control (MAC) scheduler to constrain the short-term EMF exposure upon real-time resource allocation, minimizing the impacts on network performance. We assume that the long-term EMF exposure is controlled by a proper outer-loop technique, that is not the object of this paper. Due to the minimal computational complexity allowed in MAC scheduling, existing solutions allowing practical implementation are few and focused at sub-optimal approaches curbing radio resource allocation. Our contribution is the derivation of a computationally efficient water-filling solution to allocate power and - then - resources, with a feasible integration of the necessary algorithms in the operations of a 5G MAC scheduler. We finally evaluate our proposal versus the prior art approaches with system level simulations with realistic modeling of physical and MAC level cellular procedures. We conclude that our proposal can control EMF with considerable less impact on network performance, making it a standout candidate for 5G and future 6G MAC scheduler implementations.","sentences":["High antenna directivity allows for high throughput transmission but also increases the exposure to electromagnetic field (EMF) of the end-users.","Health regulations impose limitations on the incident power density, that generate a negative impact on network performance.","In this work we focus at the slot-by-slot operations of a cellular Medium Access Control (MAC) scheduler to constrain the short-term EMF exposure upon real-time resource allocation, minimizing the impacts on network performance.","We assume that the long-term EMF exposure is controlled by a proper outer-loop technique, that is not the object of this paper.","Due to the minimal computational complexity allowed in MAC scheduling, existing solutions allowing practical implementation are few and focused at sub-optimal approaches curbing radio resource allocation.","Our contribution is the derivation of a computationally efficient water-filling solution to allocate power and - then - resources, with a feasible integration of the necessary algorithms in the operations of a 5G MAC scheduler.","We finally evaluate our proposal versus the prior art approaches with system level simulations with realistic modeling of physical and MAC level cellular procedures.","We conclude that our proposal can control EMF with considerable less impact on network performance, making it a standout candidate for 5G and future 6G MAC scheduler implementations."],"url":"http://arxiv.org/abs/2404.06830v1","category":"cs.NI"}
{"created":"2024-04-10 08:27:46","title":"A Global Stochastic Maximum Principle for Mean-Field Forward-Backward Stochastic Control Systems with Quadratic Generators","abstract":"Our paper is devoted to the study of Peng's stochastic maximum principle (SMP) for a stochastic control problem composed of a controlled forward stochastic differential equation (SDE) as dynamics and a controlled backward SDE which defines the cost functional. Our studies combine the difficulties which come, on one hand, from the fact that the coefficients of both the SDE and the backward SDE are of mean-field type (i.e., they do not only depend on the control process and the solution processes but also on their law), and on the other hand, from the fact that the coefficient of the BSDE is of quadratic growth in $Z$. Our SMP is novel, it extends in a by far non trivial way existing results on SMP.","sentences":["Our paper is devoted to the study of Peng's stochastic maximum principle (SMP) for a stochastic control problem composed of a controlled forward stochastic differential equation (SDE) as dynamics and a controlled backward SDE which defines the cost functional.","Our studies combine the difficulties which come, on one hand, from the fact that the coefficients of both the SDE and the backward SDE are of mean-field type (i.e., they do not only depend on the control process and the solution processes but also on their law), and on the other hand, from the fact that the coefficient of the BSDE is of quadratic growth in $Z$. Our SMP is novel, it extends in a by far non trivial way existing results on SMP."],"url":"http://arxiv.org/abs/2404.06826v1","category":"math.OC"}
{"created":"2024-04-10 08:19:46","title":"Origins of Fine Structure in DNA Melting Curves","abstract":"With the help of one-dimensional random Potts-like model we study the origins of fine structure observed on differential melting profiles of double-stranded DNA. We assess the effects of sequence arrangement on DNA melting curves through the comparison of results for random, correlated, and block sequences. Our results re-confirm the smearing out the fine structure with the increase of chain length for all types of sequence arrangements and suggest fine structure to be a finite-size effect. We have found, that the fine structure in chains comprised of blocks with the correlation in sequence is more persistent, probably, because of increased sequence disorder the blocks introduce. Many natural DNAs show a well-expressed fine structure of melting profiles. In view of our results it might mean the existence of blocks in such DNAs. The very observation of fine structure may also mean, that there exists an optimal length for natural DNAs \\emph{in vivo}.","sentences":["With the help of one-dimensional random Potts-like model we study the origins of fine structure observed on differential melting profiles of double-stranded DNA.","We assess the effects of sequence arrangement on DNA melting curves through the comparison of results for random, correlated, and block sequences.","Our results re-confirm the smearing out the fine structure with the increase of chain length for all types of sequence arrangements and suggest fine structure to be a finite-size effect.","We have found, that the fine structure in chains comprised of blocks with the correlation in sequence is more persistent, probably, because of increased sequence disorder the blocks introduce.","Many natural DNAs show a well-expressed fine structure of melting profiles.","In view of our results it might mean the existence of blocks in such DNAs.","The very observation of fine structure may also mean, that there exists an optimal length for natural DNAs \\emph{in vivo}."],"url":"http://arxiv.org/abs/2404.06822v1","category":"cond-mat.soft"}
{"created":"2024-04-10 08:04:15","title":"Machine learning assisted optical diagnostics on a cylindrical atmospheric pressure surface dielectric barrier discharge","abstract":"The present study explores combining machine learning (ML) algorithms with standard optical diagnostics (such as time--integrated emission spectroscopy and imaging) to accurately predict operating conditions and assess the emission uniformity of a cylindrical surface Dielectric Barrier Discharge (SDBD). It is demonstrated that ML can be complementary with these optical diagnostics and identify peculiarities associated with the discharge emission pattern at different high voltage waveforms (AC and pulsed) and amplitudes. By employing unsupervised (Principal Component Analysis (PCA)) and supervised (Multilayer Perceptron (MLP) neural networks) algorithms, the applied voltage waveform and amplitude are categorised and predicted based on correlations/differences identified within large amounts of corresponding data. PCA allowed us to effectively classify the voltage waveforms and amplitudes applied to the SDBD through a transformation of the spectroscopic/imaging data into principal components (PCs) and their projection to a two-dimensional PC space. Furthermore, an accurate prediction of the voltage amplitude is achieved using the MLP which is trained with PCA--preprocessed data. A particularly interesting aspect of this concept involves examining the uniformity of the emission pattern of the discharge. This is achieved by analysing spectroscopic data recorded at four different regions around the SDBD surface using the two ML--based techniques. These discoveries are instrumental in enhancing plasma--induced processes. They open up new avenues for real--time control, monitoring, and optimization of plasma--based applications across diverse fields such as flow control for the present SDBD.","sentences":["The present study explores combining machine learning (ML) algorithms with standard optical diagnostics (such as time--integrated emission spectroscopy and imaging) to accurately predict operating conditions and assess the emission uniformity of a cylindrical surface Dielectric Barrier Discharge (SDBD).","It is demonstrated that ML can be complementary with these optical diagnostics and identify peculiarities associated with the discharge emission pattern at different high voltage waveforms (AC and pulsed) and amplitudes.","By employing unsupervised (Principal Component Analysis (PCA)) and supervised (Multilayer Perceptron (MLP) neural networks) algorithms, the applied voltage waveform and amplitude are categorised and predicted based on correlations/differences identified within large amounts of corresponding data.","PCA allowed us to effectively classify the voltage waveforms and amplitudes applied to the SDBD through a transformation of the spectroscopic/imaging data into principal components (PCs) and their projection to a two-dimensional PC space.","Furthermore, an accurate prediction of the voltage amplitude is achieved using the MLP which is trained with PCA--preprocessed data.","A particularly interesting aspect of this concept involves examining the uniformity of the emission pattern of the discharge.","This is achieved by analysing spectroscopic data recorded at four different regions around the SDBD surface using the two ML--based techniques.","These discoveries are instrumental in enhancing plasma--induced processes.","They open up new avenues for real--time control, monitoring, and optimization of plasma--based applications across diverse fields such as flow control for the present SDBD."],"url":"http://arxiv.org/abs/2404.06817v1","category":"physics.plasm-ph"}
{"created":"2024-04-10 07:52:41","title":"Near-Optimal Channel Estimation for Dense Array Systems","abstract":"By deploying a large number of antennas with sub-half-wavelength spacing in a compact space, dense array systems(DASs) can fully unleash the multiplexing-and-diversity gains of limited apertures. To acquire these gains, accurate channel state information acquisition is necessary but challenging due to the large antenna numbers. To overcome this obstacle, this paper reveals that exploiting the high spatial correlation of DAS channels is crucial while designing the observation matrix for optimal/near-optimal channel estimation. Firstly, we prove that the observation matrix design is equivalent to a time-domain duality of multiple-input multiple-output precoding, which can be ideally addressed by the water-filling principle. For practical realizations, a novel ice-filling algorithm is proposed to design amplitude-and-phase controllable observation matrices, and a majorization-minimization algorithm is proposed to address the phase-only controllable case. Particularly, we prove that the ice-filling algorithm can be viewed as a ``quantized\" water-filling algorithm. To support the sub-optimality of the proposed designs, we provide comprehensive analyses on the achievable mean square errors and their asymptotic expressions. Finally, numerical simulations verify that our proposed channel estimation designs can achieve the near-optimal performance and outperform existing approaches significantly.","sentences":["By deploying a large number of antennas with sub-half-wavelength spacing in a compact space, dense array systems(DASs) can fully unleash the multiplexing-and-diversity gains of limited apertures.","To acquire these gains, accurate channel state information acquisition is necessary but challenging due to the large antenna numbers.","To overcome this obstacle, this paper reveals that exploiting the high spatial correlation of DAS channels is crucial while designing the observation matrix for optimal/near-optimal channel estimation.","Firstly, we prove that the observation matrix design is equivalent to a time-domain duality of multiple-input multiple-output precoding, which can be ideally addressed by the water-filling principle.","For practical realizations, a novel ice-filling algorithm is proposed to design amplitude-and-phase controllable observation matrices, and a majorization-minimization algorithm is proposed to address the phase-only controllable case.","Particularly, we prove that the ice-filling algorithm can be viewed as a ``quantized\" water-filling algorithm.","To support the sub-optimality of the proposed designs, we provide comprehensive analyses on the achievable mean square errors and their asymptotic expressions.","Finally, numerical simulations verify that our proposed channel estimation designs can achieve the near-optimal performance and outperform existing approaches significantly."],"url":"http://arxiv.org/abs/2404.06806v1","category":"cs.IT"}
{"created":"2024-04-10 07:34:37","title":"Extracting Clean and Balanced Subset for Noisy Long-tailed Classification","abstract":"Real-world datasets usually are class-imbalanced and corrupted by label noise. To solve the joint issue of long-tailed distribution and label noise, most previous works usually aim to design a noise detector to distinguish the noisy and clean samples. Despite their effectiveness, they may be limited in handling the joint issue effectively in a unified way. In this work, we develop a novel pseudo labeling method using class prototypes from the perspective of distribution matching, which can be solved with optimal transport (OT). By setting a manually-specific probability measure and using a learned transport plan to pseudo-label the training samples, the proposed method can reduce the side-effects of noisy and long-tailed data simultaneously. Then we introduce a simple yet effective filter criteria by combining the observed labels and pseudo labels to obtain a more balanced and less noisy subset for a robust model training. Extensive experiments demonstrate that our method can extract this class-balanced subset with clean labels, which brings effective performance gains for long-tailed classification with label noise.","sentences":["Real-world datasets usually are class-imbalanced and corrupted by label noise.","To solve the joint issue of long-tailed distribution and label noise, most previous works usually aim to design a noise detector to distinguish the noisy and clean samples.","Despite their effectiveness, they may be limited in handling the joint issue effectively in a unified way.","In this work, we develop a novel pseudo labeling method using class prototypes from the perspective of distribution matching, which can be solved with optimal transport (OT).","By setting a manually-specific probability measure and using a learned transport plan to pseudo-label the training samples, the proposed method can reduce the side-effects of noisy and long-tailed data simultaneously.","Then we introduce a simple yet effective filter criteria by combining the observed labels and pseudo labels to obtain a more balanced and less noisy subset for a robust model training.","Extensive experiments demonstrate that our method can extract this class-balanced subset with clean labels, which brings effective performance gains for long-tailed classification with label noise."],"url":"http://arxiv.org/abs/2404.06795v1","category":"cs.LG"}
{"created":"2024-04-10 07:06:51","title":"Data-driven modeling of unsteady flow based on deep operator network","abstract":"Time-dependent flow fields are typically generated by a computational fluid dynamics (CFD) method, which is an extremely time-consuming process. However, the latent relationship between the flow fields is governed by the Navier-Stokes equations and can be described by an operator. We therefore train a deep operator network, or simply DeepONet, to learn the temporal evolution between flow snapshots. Once properly trained, given a few consecutive snapshots as input, the network has a great potential to generate the next snapshot accurately and quickly. Using the output as a new input, the network iterates the process, generating a series of successive snapshots with little wall time. Specifically, we consider 2D flow around a circular cylinder at Reynolds number 1000, and prepare a set of high-fidelity data using a high-order spectral/hp element method as ground truth. Although the flow fields are periodic, there are many small-scale features in the wake flow that are difficult to generate accurately. Furthermore, any discrepancy between the prediction and the ground truth for the first snapshots can easily accumulate during the iterative process, which eventually amplifies the overall deviations. Therefore, we propose two alternative techniques to improve the training of DeepONet. The first one enhances the feature extraction of the network by harnessing the \"multi-head non-local block\". The second one refines the network parameters by leveraging the local smooth optimization technique. Both techniques prove to be highly effective in reducing the cumulative errors and our results outperform those of the dynamic mode decomposition method.","sentences":["Time-dependent flow fields are typically generated by a computational fluid dynamics (CFD) method, which is an extremely time-consuming process.","However, the latent relationship between the flow fields is governed by the Navier-Stokes equations and can be described by an operator.","We therefore train a deep operator network, or simply DeepONet, to learn the temporal evolution between flow snapshots.","Once properly trained, given a few consecutive snapshots as input, the network has a great potential to generate the next snapshot accurately and quickly.","Using the output as a new input, the network iterates the process, generating a series of successive snapshots with little wall time.","Specifically, we consider 2D flow around a circular cylinder at Reynolds number 1000, and prepare a set of high-fidelity data using a high-order spectral/hp element method as ground truth.","Although the flow fields are periodic, there are many small-scale features in the wake flow that are difficult to generate accurately.","Furthermore, any discrepancy between the prediction and the ground truth for the first snapshots can easily accumulate during the iterative process, which eventually amplifies the overall deviations.","Therefore, we propose two alternative techniques to improve the training of DeepONet.","The first one enhances the feature extraction of the network by harnessing the \"multi-head non-local block\".","The second one refines the network parameters by leveraging the local smooth optimization technique.","Both techniques prove to be highly effective in reducing the cumulative errors and our results outperform those of the dynamic mode decomposition method."],"url":"http://arxiv.org/abs/2404.06791v1","category":"physics.flu-dyn"}
