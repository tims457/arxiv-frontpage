{"created":"2024-04-15 12:53:48","title":"Can We Break Free from Strong Data Augmentations in Self-Supervised Learning?","abstract":"Self-supervised learning (SSL) has emerged as a promising solution for addressing the challenge of limited labeled data in deep neural networks (DNNs), offering scalability potential. However, the impact of design dependencies within the SSL framework remains insufficiently investigated. In this study, we comprehensively explore SSL behavior across a spectrum of augmentations, revealing their crucial role in shaping SSL model performance and learning mechanisms. Leveraging these insights, we propose a novel learning approach that integrates prior knowledge, with the aim of curtailing the need for extensive data augmentations and thereby amplifying the efficacy of learned representations. Notably, our findings underscore that SSL models imbued with prior knowledge exhibit reduced texture bias, diminished reliance on shortcuts and augmentations, and improved robustness against both natural and adversarial corruptions. These findings not only illuminate a new direction in SSL research, but also pave the way for enhancing DNN performance while concurrently alleviating the imperative for intensive data augmentation, thereby enhancing scalability and real-world problem-solving capabilities.","sentences":["Self-supervised learning (SSL) has emerged as a promising solution for addressing the challenge of limited labeled data in deep neural networks (DNNs), offering scalability potential.","However, the impact of design dependencies within the SSL framework remains insufficiently investigated.","In this study, we comprehensively explore SSL behavior across a spectrum of augmentations, revealing their crucial role in shaping SSL model performance and learning mechanisms.","Leveraging these insights, we propose a novel learning approach that integrates prior knowledge, with the aim of curtailing the need for extensive data augmentations and thereby amplifying the efficacy of learned representations.","Notably, our findings underscore that SSL models imbued with prior knowledge exhibit reduced texture bias, diminished reliance on shortcuts and augmentations, and improved robustness against both natural and adversarial corruptions.","These findings not only illuminate a new direction in SSL research, but also pave the way for enhancing DNN performance while concurrently alleviating the imperative for intensive data augmentation, thereby enhancing scalability and real-world problem-solving capabilities."],"url":"http://arxiv.org/abs/2404.09752v1","category":"cs.CV"}
{"created":"2024-04-15 12:47:23","title":"Gradient descent for unbounded convex functions on Hadamard manifolds and its applications to scaling problems","abstract":"In this paper, we study asymptotic behaviors of continuous-time and discrete-time gradient flows of a ``lower-unbounded\" convex function $f$ on a Hadamard manifold $M$, particularly, their convergence properties to the boundary $M^{\\infty}$ at infinity of $M$. We establish a duality theorem that the infimum of the gradient-norm $\\|\\nabla f(x)\\|$ of $f$ over $M$ is equal to the supremum of the negative of the recession function $f^{\\infty}$ of $f$ over the boundary $M^{\\infty}$, provided the infimum is positive. Further, the infimum and the supremum are obtained by the limits of the gradient flows of $f$, Our results feature convex-optimization ingredients of the moment-weight inequality for reductive group actions by Georgoulas, Robbin, and Salamon,and are applied to noncommutative optimization by B\\\"urgisser et al. FOCS 2019. We show that the gradient descent of the Kempf-Ness function for an unstable orbit converges to a 1-parameter subgroup in the Hilbert-Mumford criterion, and the associated moment-map sequence converges to the mimimum-norm point of the moment polytope. We show further refinements for operator scaling -- the left-right action on a matrix tuple $A= (A_1,A_2,\\ldots,A_N)$. We characterize the gradient-flow limit of operator scaling via a vector-space generalization of the classical Dulmage-Mendelsohn decomposition of a bipartite graph. Also, for a special case of $N = 2$, we reveal that this limit determines the Kronecker canonical form of matrix pencils $s A_1+A_2$.","sentences":["In this paper, we study asymptotic behaviors of continuous-time and discrete-time gradient flows of a ``lower-unbounded\" convex function $f$ on a Hadamard manifold $M$, particularly, their convergence properties to the boundary $M^{\\infty}$ at infinity of $M$. We establish a duality theorem that the infimum of the gradient-norm $\\|\\nabla f(x)\\|$ of $f$ over $M$ is equal to the supremum of the negative of the recession function $f^{\\infty}$ of $f$ over the boundary $M^{\\infty}$, provided the infimum is positive.","Further, the infimum and the supremum are obtained by the limits of the gradient flows of $f$, Our results feature convex-optimization ingredients of the moment-weight inequality for reductive group actions by Georgoulas, Robbin, and Salamon,and are applied to noncommutative optimization by B\\\"urgisser et al. FOCS 2019.","We show that the gradient descent of the Kempf-Ness function for an unstable orbit converges to a 1-parameter subgroup in the Hilbert-Mumford criterion, and the associated moment-map sequence converges to the mimimum-norm point of the moment polytope.","We show further refinements for operator scaling -- the left-right action on a matrix tuple $A= (A_1,A_2,\\ldots,A_N)$. We characterize the gradient-flow limit of operator scaling via a vector-space generalization of the classical Dulmage-Mendelsohn decomposition of a bipartite graph.","Also, for a special case of $N = 2$, we reveal that this limit determines the Kronecker canonical form of matrix pencils $s A_1+A_2$."],"url":"http://arxiv.org/abs/2404.09746v1","category":"math.OC"}
{"created":"2024-04-15 12:44:22","title":"On-chip fluid information detection based on micro-ring optical frequency comb technology and machine learning","abstract":"The research on sensing the sensitivity of the light field in the whispering gallery mode (WGM) to the micro-cavity environment has already appeared, which uses the frequency shift of the light field in the WGM or the sensitivity of the resonance peak frequency shift. Multi-mode comb teeth of optical frequency comb(OFC) generated by nonlinear micro-cavity have excellent sensitivity to micro-cavity environment, and they have more sensitivity degrees of freedom compared with WGM light field (the strength of each comb tooth can be influenced by micro-cavity environment). The influence of different substances on the environmental parameters of micro-cavity is complex and nonlinear, so we use machine learning method to automatically extract the spectrum characteristics, the average accuracy of single-parameter identification attains to 99.5%, and the average accuracy of double parameter identification attains to 97.0%. Based on the integration of micro-cavity OFC and wave-guide coupling structure, we propose an set of fluid characteristics detection integrated device in theoretically.","sentences":["The research on sensing the sensitivity of the light field in the whispering gallery mode (WGM) to the micro-cavity environment has already appeared, which uses the frequency shift of the light field in the WGM or the sensitivity of the resonance peak frequency shift.","Multi-mode comb teeth of optical frequency comb(OFC) generated by nonlinear micro-cavity have excellent sensitivity to micro-cavity environment, and they have more sensitivity degrees of freedom compared with WGM light field (the strength of each comb tooth can be influenced by micro-cavity environment).","The influence of different substances on the environmental parameters of micro-cavity is complex and nonlinear, so we use machine learning method to automatically extract the spectrum characteristics, the average accuracy of single-parameter identification attains to 99.5%, and the average accuracy of double parameter identification attains to 97.0%.","Based on the integration of micro-cavity OFC and wave-guide coupling structure, we propose an set of fluid characteristics detection integrated device in theoretically."],"url":"http://arxiv.org/abs/2404.09742v1","category":"physics.optics"}
{"created":"2024-04-15 12:43:00","title":"Data Models With Two Manifestations of Imprecision","abstract":"Motivated by recently emerging problems in machine learning and statistics, we propose data models which relax the familiar i.i.d. assumption. In essence, we seek to understand what it means for data to come from a set of probability measures. We show that our frequentist data models, parameterized by such sets, manifest two aspects of imprecision. We characterize the intricate interplay of these manifestations, aggregate (ir)regularity and local (ir)regularity, where a much richer set of behaviours compared to an i.i.d. model is possible. In doing so we shed new light on the relationship between non-stationary, locally precise and stationary, locally imprecise data models. We discuss possible applications of these data models in machine learning and how the set of probabilities can be estimated. For the estimation of aggregate irregularity, we provide a negative result but argue that it does not warrant pessimism. Understanding these frequentist aspects of imprecise probabilities paves the way for deriving generalization of proper scoring rules and calibration to the imprecise case, which can then contribute to tackling practical problems.","sentences":["Motivated by recently emerging problems in machine learning and statistics, we propose data models which relax the familiar i.i.d. assumption.","In essence, we seek to understand what it means for data to come from a set of probability measures.","We show that our frequentist data models, parameterized by such sets, manifest two aspects of imprecision.","We characterize the intricate interplay of these manifestations, aggregate (ir)regularity and local (ir)regularity, where a much richer set of behaviours compared to an i.i.d. model is possible.","In doing so we shed new light on the relationship between non-stationary, locally precise and stationary, locally imprecise data models.","We discuss possible applications of these data models in machine learning and how the set of probabilities can be estimated.","For the estimation of aggregate irregularity, we provide a negative result but argue that it does not warrant pessimism.","Understanding these frequentist aspects of imprecise probabilities paves the way for deriving generalization of proper scoring rules and calibration to the imprecise case, which can then contribute to tackling practical problems."],"url":"http://arxiv.org/abs/2404.09741v1","category":"math.ST"}
{"created":"2024-04-15 12:42:29","title":"Leveraging Zero-Level Distillation to Generate High-Fidelity Magic States","abstract":"Magic state distillation plays an important role in universal fault-tolerant quantum computing, and its overhead is one of the major obstacles to realizing fault-tolerant quantum computers. Hence, many studies have been conducted to reduce this overhead. Among these, Litinski has provided a concrete assessment of resource-efficient distillation protocol implementations on the rotated surface code. On the other hand, recently, Itogawa et al. have proposed zero-level distillation, a distillation protocol offering very small spatial and temporal overhead to generate relatively low-fidelity magic states. While zero-level distillation offers preferable spatial and temporal overhead, it cannot directly generate high-fidelity magic states since it only reduces the logical error rate of the magic state quadratically. In this study, we evaluate the spatial and temporal overhead of two-level distillation implementations generating relatively high-fidelity magic states, including ones incorporating zero-level distillation. To this end, we introduce (0+1)-level distillation, a two-level distillation protocol which combines zero-level distillation and the 15-to-1 distillation protocol. We refine the second-level 15-to-1 implementation in it to capitalize on the small footprint of zero-level distillation. Under conditions of a physical error probability of $p_{\\mathrm{phys}} = 10^{-4}$ ($10^{-3}$) and targeting an error rate for the magic state within $[5 \\times 10^{-17}, 10^{-11}]$ ($[5 \\times 10^{-11}, 10^{-8}]$), (0+1)-level distillation reduces the spatiotemporal overhead by more than 63% (61%) compared to the (15-to-1)$\\times$(15-to-1) protocol and more than 43% (44%) compared to the (15-to-1)$\\times$(20-to-4) protocol, offering a substantial efficiency gain over the traditional protocols.","sentences":["Magic state distillation plays an important role in universal fault-tolerant quantum computing, and its overhead is one of the major obstacles to realizing fault-tolerant quantum computers.","Hence, many studies have been conducted to reduce this overhead.","Among these, Litinski has provided a concrete assessment of resource-efficient distillation protocol implementations on the rotated surface code.","On the other hand, recently, Itogawa et al. have proposed zero-level distillation, a distillation protocol offering very small spatial and temporal overhead to generate relatively low-fidelity magic states.","While zero-level distillation offers preferable spatial and temporal overhead, it cannot directly generate high-fidelity magic states since it only reduces the logical error rate of the magic state quadratically.","In this study, we evaluate the spatial and temporal overhead of two-level distillation implementations generating relatively high-fidelity magic states, including ones incorporating zero-level distillation.","To this end, we introduce (0+1)-level distillation, a two-level distillation protocol which combines zero-level distillation and the 15-to-1 distillation protocol.","We refine the second-level 15-to-1 implementation in it to capitalize on the small footprint of zero-level distillation.","Under conditions of a physical error probability of $p_{\\mathrm{phys}} = 10^{-4}$ ($10^{-3}$) and targeting an error rate for the magic state within $[5 \\times 10^{-17}, 10^{-11}]$ ($[5 \\times 10^{-11}, 10^{-8}]$), (0+1)-level distillation reduces the spatiotemporal overhead by more than 63% (61%) compared to the (15-to-1)$\\times$(15-to-1) protocol and more than 43% (44%) compared to the (15-to-1)$\\times$(20-to-4) protocol, offering a substantial efficiency gain over the traditional protocols."],"url":"http://arxiv.org/abs/2404.09740v1","category":"quant-ph"}
{"created":"2024-04-15 12:40:12","title":"AMPCliff: quantitative definition and benchmarking of activity cliffs in antimicrobial peptides","abstract":"Activity cliff (AC) is a phenomenon that a pair of similar molecules differ by a small structural alternation but exhibit a large difference in their biochemical activities. The AC of small molecules has been extensively investigated but limited knowledge is accumulated about the AC phenomenon in peptides with canonical amino acids. This study introduces a quantitative definition and benchmarking framework AMPCliff for the AC phenomenon in antimicrobial peptides (AMPs) composed by canonical amino acids. A comprehensive analysis of the existing AMP dataset reveals a significant prevalence of AC within AMPs. AMPCliff quantifies the activities of AMPs by the metric minimum inhibitory concentration (MIC), and defines 0.9 as the minimum threshold for the normalized BLOSUM62 similarity score between a pair of aligned peptides with at least two-fold MIC changes. This study establishes a benchmark dataset of paired AMPs in Staphylococcus aureus from the publicly available AMP dataset GRAMPA, and conducts a rigorous procedure to evaluate various AMP AC prediction models, including nine machine learning, four deep learning algorithms, four masked language models, and four generative language models. Our analysis reveals that these models are capable of detecting AMP AC events and the pre-trained protein language ESM2 model demonstrates superior performance across the evaluations. The predictive performance of AMP activity cliffs remains to be further improved, considering that ESM2 with 33 layers only achieves the Spearman correlation coefficient=0.50 for the regression task of the MIC values on the benchmark dataset. Source code and additional resources are available at https://www.healthinformaticslab.org/supp/ or https://github.com/Kewei2023/AMPCliff-generation.","sentences":["Activity cliff (AC) is a phenomenon that a pair of similar molecules differ by a small structural alternation but exhibit a large difference in their biochemical activities.","The AC of small molecules has been extensively investigated but limited knowledge is accumulated about the AC phenomenon in peptides with canonical amino acids.","This study introduces a quantitative definition and benchmarking framework AMPCliff for the AC phenomenon in antimicrobial peptides (AMPs) composed by canonical amino acids.","A comprehensive analysis of the existing AMP dataset reveals a significant prevalence of AC within AMPs.","AMPCliff quantifies the activities of AMPs by the metric minimum inhibitory concentration (MIC), and defines 0.9 as the minimum threshold for the normalized BLOSUM62 similarity score between a pair of aligned peptides with at least two-fold MIC changes.","This study establishes a benchmark dataset of paired AMPs in Staphylococcus aureus from the publicly available AMP dataset GRAMPA, and conducts a rigorous procedure to evaluate various AMP AC prediction models, including nine machine learning, four deep learning algorithms, four masked language models, and four generative language models.","Our analysis reveals that these models are capable of detecting AMP AC events and the pre-trained protein language ESM2 model demonstrates superior performance across the evaluations.","The predictive performance of AMP activity cliffs remains to be further improved, considering that ESM2 with 33 layers only achieves the Spearman correlation coefficient=0.50 for the regression task of the MIC values on the benchmark dataset.","Source code and additional resources are available at https://www.healthinformaticslab.org/supp/ or https://github.com/Kewei2023/AMPCliff-generation."],"url":"http://arxiv.org/abs/2404.09738v1","category":"q-bio.BM"}
{"created":"2024-04-15 12:34:21","title":"Photo-Realistic Image Restoration in the Wild with Controlled Vision-Language Models","abstract":"Though diffusion models have been successfully applied to various image restoration (IR) tasks, their performance is sensitive to the choice of training datasets. Typically, diffusion models trained in specific datasets fail to recover images that have out-of-distribution degradations. To address this problem, this work leverages a capable vision-language model and a synthetic degradation pipeline to learn image restoration in the wild (wild IR). More specifically, all low-quality images are simulated with a synthetic degradation pipeline that contains multiple common degradations such as blur, resize, noise, and JPEG compression. Then we introduce robust training for a degradation-aware CLIP model to extract enriched image content features to assist high-quality image restoration. Our base diffusion model is the image restoration SDE (IR-SDE). Built upon it, we further present a posterior sampling strategy for fast noise-free image generation. We evaluate our model on both synthetic and real-world degradation datasets. Moreover, experiments on the unified image restoration task illustrate that the proposed posterior sampling improves image generation quality for various degradations.","sentences":["Though diffusion models have been successfully applied to various image restoration (IR) tasks, their performance is sensitive to the choice of training datasets.","Typically, diffusion models trained in specific datasets fail to recover images that have out-of-distribution degradations.","To address this problem, this work leverages a capable vision-language model and a synthetic degradation pipeline to learn image restoration in the wild (wild IR).","More specifically, all low-quality images are simulated with a synthetic degradation pipeline that contains multiple common degradations such as blur, resize, noise, and JPEG compression.","Then we introduce robust training for a degradation-aware CLIP model to extract enriched image content features to assist high-quality image restoration.","Our base diffusion model is the image restoration SDE (IR-SDE).","Built upon it, we further present a posterior sampling strategy for fast noise-free image generation.","We evaluate our model on both synthetic and real-world degradation datasets.","Moreover, experiments on the unified image restoration task illustrate that the proposed posterior sampling improves image generation quality for various degradations."],"url":"http://arxiv.org/abs/2404.09732v1","category":"cs.CV"}
{"created":"2024-04-15 12:29:28","title":"Convergence Analysis of Probability Flow ODE for Score-based Generative Models","abstract":"Score-based generative models have emerged as a powerful approach for sampling high-dimensional probability distributions. Despite their effectiveness, their theoretical underpinnings remain relatively underdeveloped. In this work, we study the convergence properties of deterministic samplers based on probability flow ODEs from both theoretical and numerical perspectives. Assuming access to $L^2$-accurate estimates of the score function, we prove the total variation between the target and the generated data distributions can be bounded above by $\\mathcal{O}(d\\sqrt{\\delta})$ in the continuous time level, where $d$ denotes the data dimension and $\\delta$ represents the $L^2$-score matching error. For practical implementations using a $p$-th order Runge-Kutta integrator with step size $h$, we establish error bounds of $\\mathcal{O}(d(\\sqrt{\\delta} + (dh)^p))$ at the discrete level. Finally, we present numerical studies on problems up to $128$ dimensions to verify our theory, which indicate a better score matching error and dimension dependence.","sentences":["Score-based generative models have emerged as a powerful approach for sampling high-dimensional probability distributions.","Despite their effectiveness, their theoretical underpinnings remain relatively underdeveloped.","In this work, we study the convergence properties of deterministic samplers based on probability flow ODEs from both theoretical and numerical perspectives.","Assuming access to $L^2$-accurate estimates of the score function, we prove the total variation between the target and the generated data distributions can be bounded above by $\\mathcal{O}(d\\sqrt{\\delta})$ in the continuous time level, where $d$ denotes the data dimension and $\\delta$ represents the $L^2$-score matching error.","For practical implementations using a $p$-th order Runge-Kutta integrator with step size $h$, we establish error bounds of $\\mathcal{O}(d(\\sqrt{\\delta} + (dh)^p))$ at the discrete level.","Finally, we present numerical studies on problems up to $128$ dimensions to verify our theory, which indicate a better score matching error and dimension dependence."],"url":"http://arxiv.org/abs/2404.09730v1","category":"cs.LG"}
{"created":"2024-04-15 12:25:41","title":"VFLGAN: Vertical Federated Learning-based Generative Adversarial Network for Vertically Partitioned Data Publication","abstract":"In the current artificial intelligence (AI) era, the scale and quality of the dataset play a crucial role in training a high-quality AI model. However, good data is not a free lunch and is always hard to access due to privacy regulations like the General Data Protection Regulation (GDPR). A potential solution is to release a synthetic dataset with a similar distribution to that of the private dataset. Nevertheless, in some scenarios, it has been found that the attributes needed to train an AI model belong to different parties, and they cannot share the raw data for synthetic data publication due to privacy regulations. In PETS 2023, Xue et al. proposed the first generative adversary network-based model, VertiGAN, for vertically partitioned data publication. However, after thoroughly investigating, we found that VertiGAN is less effective in preserving the correlation among the attributes of different parties. This article proposes a Vertical Federated Learning-based Generative Adversarial Network, VFLGAN, for vertically partitioned data publication to address the above issues. Our experimental results show that compared with VertiGAN, VFLGAN significantly improves the quality of synthetic data. Taking the MNIST dataset as an example, the quality of the synthetic dataset generated by VFLGAN is 3.2 times better than that generated by VertiGAN w.r.t. the Fr\\'echet Distance. We also designed a more efficient and effective Gaussian mechanism for the proposed VFLGAN to provide the synthetic dataset with a differential privacy guarantee. On the other hand, differential privacy only gives the upper bound of the worst-case privacy guarantee. This article also proposes a practical auditing scheme that applies membership inference attacks to estimate privacy leakage through the synthetic dataset.","sentences":["In the current artificial intelligence (AI) era, the scale and quality of the dataset play a crucial role in training a high-quality AI model.","However, good data is not a free lunch and is always hard to access due to privacy regulations like the General Data Protection Regulation (GDPR).","A potential solution is to release a synthetic dataset with a similar distribution to that of the private dataset.","Nevertheless, in some scenarios, it has been found that the attributes needed to train an AI model belong to different parties, and they cannot share the raw data for synthetic data publication due to privacy regulations.","In PETS 2023, Xue et al. proposed the first generative adversary network-based model, VertiGAN, for vertically partitioned data publication.","However, after thoroughly investigating, we found that VertiGAN is less effective in preserving the correlation among the attributes of different parties.","This article proposes a Vertical Federated Learning-based Generative Adversarial Network, VFLGAN, for vertically partitioned data publication to address the above issues.","Our experimental results show that compared with VertiGAN, VFLGAN significantly improves the quality of synthetic data.","Taking the MNIST dataset as an example, the quality of the synthetic dataset generated by VFLGAN is 3.2 times better than that generated by VertiGAN w.r.t.","the Fr\\'echet Distance.","We also designed a more efficient and effective Gaussian mechanism for the proposed VFLGAN to provide the synthetic dataset with a differential privacy guarantee.","On the other hand, differential privacy only gives the upper bound of the worst-case privacy guarantee.","This article also proposes a practical auditing scheme that applies membership inference attacks to estimate privacy leakage through the synthetic dataset."],"url":"http://arxiv.org/abs/2404.09722v1","category":"cs.LG"}
{"created":"2024-04-15 12:25:11","title":"Offshore power and hydrogen networks for Europe's North Sea","abstract":"The European North Sea has a vast renewable energy potential and can be a powerhouse for Europe's energy transition. However, currently there is uncertainty about how much offshore wind energy can be integrated, whether offshore grids should be meshed and to what extent offshore hydrogen should play a role. To address these questions, we use the open-source energy system optimization model PyPSA-Eur to model a European carbon-neutral sector-coupled energy system in high spatial and temporal resolution. We let the model endogenously decide how much offshore wind is deployed and which infrastructure is used to integrate the offshore wind. We find that with point-to-point connections like we have today, 310 GW offshore wind can be integrated in the North Sea. However, if we allow meshed networks and hydrogen, we find that this can be raised to 420 GW with cost savings up to 15 billion euros per year. Furthermore, we only observe significant amounts of up to 75 GW of floating wind turbines in the North Sea if we have offshore hydrogen production. Generally, the model opts for offshore wind integration through a mix of both electricity and hydrogen infrastructure. However, the bulk of the offshore energy is transported as hydrogen, which is twice as much as the amount transported as electricity. Moreover, we find that the offshore power network is mainly used for offshore wind integration, with only a small portion used for inter-country transmission.","sentences":["The European North Sea has a vast renewable energy potential and can be a powerhouse for Europe's energy transition.","However, currently there is uncertainty about how much offshore wind energy can be integrated, whether offshore grids should be meshed and to what extent offshore hydrogen should play a role.","To address these questions, we use the open-source energy system optimization model PyPSA-Eur to model a European carbon-neutral sector-coupled energy system in high spatial and temporal resolution.","We let the model endogenously decide how much offshore wind is deployed and which infrastructure is used to integrate the offshore wind.","We find that with point-to-point connections like we have today, 310 GW offshore wind can be integrated in the North Sea.","However, if we allow meshed networks and hydrogen, we find that this can be raised to 420 GW with cost savings up to 15 billion euros per year.","Furthermore, we only observe significant amounts of up to 75 GW of floating wind turbines in the North Sea if we have offshore hydrogen production.","Generally, the model opts for offshore wind integration through a mix of both electricity and hydrogen infrastructure.","However, the bulk of the offshore energy is transported as hydrogen, which is twice as much as the amount transported as electricity.","Moreover, we find that the offshore power network is mainly used for offshore wind integration, with only a small portion used for inter-country transmission."],"url":"http://arxiv.org/abs/2404.09721v1","category":"physics.soc-ph"}
{"created":"2024-04-15 12:20:09","title":"Unveiling Imitation Learning: Exploring the Impact of Data Falsity to Large Language Model","abstract":"Many recent studies endeavor to improve open-source language models through imitation learning, and re-training on the synthetic instruction data from state-of-the-art proprietary models like ChatGPT and GPT-4. However, the innate nature of synthetic data inherently contains noisy data, giving rise to a substantial presence of low-quality data replete with erroneous responses, and flawed reasoning. Although we intuitively grasp the potential harm of noisy data, we lack a quantitative understanding of its impact. To this end, this paper explores the correlation between the degree of noise and its impact on language models through instruction tuning. We first introduce the Falsity-Controllable (FACO) dataset, which comprises pairs of true answers with corresponding reasoning, as well as false pairs to manually control the falsity ratio of the dataset.Through our extensive experiments, we found multiple intriguing findings of the correlation between the factuality of the dataset and instruction tuning: Specifically, we verified falsity of the instruction is highly relevant to various benchmark scores. Moreover, when LLMs are trained with false instructions, they learn to lie and generate fake unfaithful answers, even though they know the correct answer for the user request. Additionally, we noted that once the language model is trained with a dataset contaminated by noise, restoring its original performance is possible, but it failed to reach full performance.","sentences":["Many recent studies endeavor to improve open-source language models through imitation learning, and re-training on the synthetic instruction data from state-of-the-art proprietary models like ChatGPT and GPT-4.","However, the innate nature of synthetic data inherently contains noisy data, giving rise to a substantial presence of low-quality data replete with erroneous responses, and flawed reasoning.","Although we intuitively grasp the potential harm of noisy data, we lack a quantitative understanding of its impact.","To this end, this paper explores the correlation between the degree of noise and its impact on language models through instruction tuning.","We first introduce the Falsity-Controllable (FACO) dataset, which comprises pairs of true answers with corresponding reasoning, as well as false pairs to manually control the falsity ratio of the dataset.","Through our extensive experiments, we found multiple intriguing findings of the correlation between the factuality of the dataset and instruction tuning: Specifically, we verified falsity of the instruction is highly relevant to various benchmark scores.","Moreover, when LLMs are trained with false instructions, they learn to lie and generate fake unfaithful answers, even though they know the correct answer for the user request.","Additionally, we noted that once the language model is trained with a dataset contaminated by noise, restoring its original performance is possible, but it failed to reach full performance."],"url":"http://arxiv.org/abs/2404.09717v1","category":"cs.CL"}
{"created":"2024-04-15 12:18:09","title":"Higher Replay Ratio Empowers Sample-Efficient Multi-Agent Reinforcement Learning","abstract":"One of the notorious issues for Reinforcement Learning (RL) is poor sample efficiency. Compared to single agent RL, the sample efficiency for Multi-Agent Reinforcement Learning (MARL) is more challenging because of its inherent partial observability, non-stationary training, and enormous strategy space. Although much effort has been devoted to developing new methods and enhancing sample efficiency, we look at the widely used episodic training mechanism. In each training step, tens of frames are collected, but only one gradient step is made. We argue that this episodic training could be a source of poor sample efficiency. To better exploit the data already collected, we propose to increase the frequency of the gradient updates per environment interaction (a.k.a. Replay Ratio or Update-To-Data ratio). To show its generality, we evaluate $3$ MARL methods on $6$ SMAC tasks. The empirical results validate that a higher replay ratio significantly improves the sample efficiency for MARL algorithms. The codes to reimplement the results presented in this paper are open-sourced at https://anonymous.4open.science/r/rr_for_MARL-0D83/.","sentences":["One of the notorious issues for Reinforcement Learning (RL) is poor sample efficiency.","Compared to single agent RL, the sample efficiency for Multi-Agent Reinforcement Learning (MARL) is more challenging because of its inherent partial observability, non-stationary training, and enormous strategy space.","Although much effort has been devoted to developing new methods and enhancing sample efficiency, we look at the widely used episodic training mechanism.","In each training step, tens of frames are collected, but only one gradient step is made.","We argue that this episodic training could be a source of poor sample efficiency.","To better exploit the data already collected, we propose to increase the frequency of the gradient updates per environment interaction (a.k.a. Replay Ratio or Update-To-Data ratio).","To show its generality, we evaluate $3$ MARL methods on $6$ SMAC tasks.","The empirical results validate that a higher replay ratio significantly improves the sample efficiency for MARL algorithms.","The codes to reimplement the results presented in this paper are open-sourced at https://anonymous.4open.science/r/rr_for_MARL-0D83/."],"url":"http://arxiv.org/abs/2404.09715v1","category":"cs.LG"}
{"created":"2024-04-15 12:18:09","title":"Optimal Cut-Point Estimation for functional digital biomarkers: Application to Continuous Glucose Monitoring","abstract":"Establish optimal cut points plays a crucial role in epidemiology and biomarker discovery, enabling the development of effective and practical clinical decision criteria. While there is extensive literature to define optimal cut off over scalar biomarkers, there is a notable lack of general methodologies for analyzing statistical objects in more complex spaces of functions and graphs, which are increasingly relevant in digital health applications. This paper proposes a new general methodology to define optimal cut points for random objects in separable Hilbert spaces. The paper is motivated by the need for creating new clinical rules for diabetes mellitus disease, exploiting the functional information of a continuous diabetes monitor (CGM) as a digital biomarker. More specifically, we provide the functional cut off to identify diabetes cases with CGM information based on glucose distributional functional representations.","sentences":["Establish optimal cut points plays a crucial role in epidemiology and biomarker discovery, enabling the development of effective and practical clinical decision criteria.","While there is extensive literature to define optimal cut off over scalar biomarkers, there is a notable lack of general methodologies for analyzing statistical objects in more complex spaces of functions and graphs, which are increasingly relevant in digital health applications.","This paper proposes a new general methodology to define optimal cut points for random objects in separable Hilbert spaces.","The paper is motivated by the need for creating new clinical rules for diabetes mellitus disease, exploiting the functional information of a continuous diabetes monitor (CGM) as a digital biomarker.","More specifically, we provide the functional cut off to identify diabetes cases with CGM information based on glucose distributional functional representations."],"url":"http://arxiv.org/abs/2404.09716v1","category":"stat.ME"}
{"created":"2024-04-15 12:17:04","title":"Classification of finite type fusion quivers","abstract":"In recent work, the second author introduced the concept of Coxeter quivers, generalizing several previous notions of a quiver representation. Finite type Coxeter quivers were classified, and their indecomposable objects were shown to be in bijection with positive roots, generalizing a classical theorem of Gabriel. In this paper we define fusion quivers, a natural generalization of Coxeter quivers. We classify the finite type fusion quivers, and prove the analogue of Gabriel's theorem. As a special case, this proves a generalised quantum McKay correspondence for fusion categories, an analogue of Auslander--Reiten's result for finite groups in the fusion categorical setting.","sentences":["In recent work, the second author introduced the concept of Coxeter quivers, generalizing several previous notions of a quiver representation.","Finite type Coxeter quivers were classified, and their indecomposable objects were shown to be in bijection with positive roots, generalizing a classical theorem of Gabriel.","In this paper we define fusion quivers, a natural generalization of Coxeter quivers.","We classify the finite type fusion quivers, and prove the analogue of Gabriel's theorem.","As a special case, this proves a generalised quantum McKay correspondence for fusion categories, an analogue of Auslander--Reiten's result for finite groups in the fusion categorical setting."],"url":"http://arxiv.org/abs/2404.09714v1","category":"math.RT"}
{"created":"2024-04-15 12:16:20","title":"Patterson-Sullivan theory for coarse cocycles","abstract":"In this paper we develop a theory of Patterson-Sullivan measures associated to coarse cocycles of convergence groups. This framework includes Patterson-Sullivan measures associated to the Busemann cocycle on the geodesic boundary of a Gromov hyperbolic metric spaces and Patterson-Sullivan measures on flag manifolds associated to Anosov (or more general transverse) subgroups of semisimple Lie groups, as well as more examples. Under some natural geometric assumptions on the coarse cocycle, we prove existence, uniqueness, and ergodicity results.","sentences":["In this paper we develop a theory of Patterson-Sullivan measures associated to coarse cocycles of convergence groups.","This framework includes Patterson-Sullivan measures associated to the Busemann cocycle on the geodesic boundary of a Gromov hyperbolic metric spaces and Patterson-Sullivan measures on flag manifolds associated to Anosov (or more general transverse) subgroups of semisimple Lie groups, as well as more examples.","Under some natural geometric assumptions on the coarse cocycle, we prove existence, uniqueness, and ergodicity results."],"url":"http://arxiv.org/abs/2404.09713v1","category":"math.DS"}
{"created":"2024-04-15 12:12:25","title":"Online Multi-level Aggregation with Delays and Stochastic Arrivals","abstract":"This paper presents a new research direction for online Multi-Level Aggregation (MLA) with delays. In this problem, we are given an edge-weighted rooted tree $T$, and we have to serve a sequence of requests arriving at its vertices in an online manner. Each request $r$ is characterized by two parameters: its arrival time $t(r)$ and location $l(r)$ (a vertex). Once a request $r$ arrives, we can either serve it immediately or postpone this action until any time $t > t(r)$. We can serve several pending requests at the same time, and the service cost of a service corresponds to the weight of the subtree that contains all the requests served and the root of $T$. Postponing the service of a request $r$ to time $t > t(r)$ generates an additional delay cost of $t - t(r)$. The goal is to serve all requests in an online manner such that the total cost (i.e., the total sum of service and delay costs) is minimized. The current best algorithm for this problem achieves a competitive ratio of $O(d^2)$ (Azar and Touitou, FOCS'19), where $d$ denotes the depth of the tree.   Here, we consider a stochastic version of MLA where the requests follow a Poisson arrival process. We present a deterministic online algorithm which achieves a constant ratio of expectations, meaning that the ratio between the expected costs of the solution generated by our algorithm and the optimal offline solution is bounded by a constant. Our algorithm is obtained by carefully combining two strategies. In the first one, we plan periodic oblivious visits to the subset of frequent vertices, whereas in the second one, we greedily serve the pending requests in the remaining vertices. This problem is complex enough to demonstrate a very rare phenomenon that ``single-minded\" or ``sample-average\" strategies are not enough in stochastic optimization.","sentences":["This paper presents a new research direction for online Multi-Level Aggregation (MLA) with delays.","In this problem, we are given an edge-weighted rooted tree $T$, and we have to serve a sequence of requests arriving at its vertices in an online manner.","Each request $r$ is characterized by two parameters: its arrival time $t(r)$ and location $l(r)$ (a vertex).","Once a request $r$ arrives, we can either serve it immediately or postpone this action until any time $t > t(r)$. We can serve several pending requests at the same time, and the service cost of a service corresponds to the weight of the subtree that contains all the requests served and the root of $T$. Postponing the service of a request $r$ to time $t >","t(r)$ generates an additional delay cost of $t - t(r)$. The goal is to serve all requests in an online manner such that the total cost (i.e., the total sum of service and delay costs) is minimized.","The current best algorithm for this problem achieves a competitive ratio of $O(d^2)$ (Azar and Touitou, FOCS'19), where $d$ denotes the depth of the tree.   ","Here, we consider a stochastic version of MLA where the requests follow a Poisson arrival process.","We present a deterministic online algorithm which achieves a constant ratio of expectations, meaning that the ratio between the expected costs of the solution generated by our algorithm and the optimal offline solution is bounded by a constant.","Our algorithm is obtained by carefully combining two strategies.","In the first one, we plan periodic oblivious visits to the subset of frequent vertices, whereas in the second one, we greedily serve the pending requests in the remaining vertices.","This problem is complex enough to demonstrate a very rare phenomenon that ``single-minded\" or ``sample-average\" strategies are not enough in stochastic optimization."],"url":"http://arxiv.org/abs/2404.09711v1","category":"cs.DS"}
{"created":"2024-04-15 12:06:00","title":"Adaptive Patching for High-resolution Image Segmentation with Transformers","abstract":"Attention-based models are proliferating in the space of image analytics, including segmentation. The standard method of feeding images to transformer encoders is to divide the images into patches and then feed the patches to the model as a linear sequence of tokens. For high-resolution images, e.g. microscopic pathology images, the quadratic compute and memory cost prohibits the use of an attention-based model, if we are to use smaller patch sizes that are favorable in segmentation. The solution is to either use custom complex multi-resolution models or approximate attention schemes. We take inspiration from Adapative Mesh Refinement (AMR) methods in HPC by adaptively patching the images, as a pre-processing step, based on the image details to reduce the number of patches being fed to the model, by orders of magnitude. This method has a negligible overhead, and works seamlessly with any attention-based model, i.e. it is a pre-processing step that can be adopted by any attention-based model without friction. We demonstrate superior segmentation quality over SoTA segmentation models for real-world pathology datasets while gaining a geomean speedup of $6.9\\times$ for resolutions up to $64K^2$, on up to $2,048$ GPUs.","sentences":["Attention-based models are proliferating in the space of image analytics, including segmentation.","The standard method of feeding images to transformer encoders is to divide the images into patches and then feed the patches to the model as a linear sequence of tokens.","For high-resolution images, e.g. microscopic pathology images, the quadratic compute and memory cost prohibits the use of an attention-based model, if we are to use smaller patch sizes that are favorable in segmentation.","The solution is to either use custom complex multi-resolution models or approximate attention schemes.","We take inspiration from Adapative Mesh Refinement (AMR) methods in HPC by adaptively patching the images, as a pre-processing step, based on the image details to reduce the number of patches being fed to the model, by orders of magnitude.","This method has a negligible overhead, and works seamlessly with any attention-based model, i.e. it is a pre-processing step that can be adopted by any attention-based model without friction.","We demonstrate superior segmentation quality over SoTA segmentation models for real-world pathology datasets while gaining a geomean speedup of $6.9\\times$ for resolutions up to $64K^2$, on up to $2,048$ GPUs."],"url":"http://arxiv.org/abs/2404.09707v1","category":"cs.CV"}
{"created":"2024-04-15 12:02:59","title":"Enhancing Robot Explanation Capabilities through Vision-Language Models: a Preliminary Study by Interpreting Visual Inputs for Improved Human-Robot Interaction","abstract":"This paper presents an improved system based on our prior work, designed to create explanations for autonomous robot actions during Human-Robot Interaction (HRI). Previously, we developed a system that used Large Language Models (LLMs) to interpret logs and produce natural language explanations. In this study, we expand our approach by incorporating Vision-Language Models (VLMs), enabling the system to analyze textual logs with the added context of visual input. This method allows for generating explanations that combine data from the robot's logs and the images it captures. We tested this enhanced system on a basic navigation task where the robot needs to avoid a human obstacle. The findings from this preliminary study indicate that adding visual interpretation improves our system's explanations by precisely identifying obstacles and increasing the accuracy of the explanations provided.","sentences":["This paper presents an improved system based on our prior work, designed to create explanations for autonomous robot actions during Human-Robot Interaction (HRI).","Previously, we developed a system that used Large Language Models (LLMs) to interpret logs and produce natural language explanations.","In this study, we expand our approach by incorporating Vision-Language Models (VLMs), enabling the system to analyze textual logs with the added context of visual input.","This method allows for generating explanations that combine data from the robot's logs and the images it captures.","We tested this enhanced system on a basic navigation task where the robot needs to avoid a human obstacle.","The findings from this preliminary study indicate that adding visual interpretation improves our system's explanations by precisely identifying obstacles and increasing the accuracy of the explanations provided."],"url":"http://arxiv.org/abs/2404.09705v1","category":"cs.RO"}
{"created":"2024-04-15 12:01:42","title":"AI Competitions and Benchmarks: Dataset Development","abstract":"Machine learning is now used in many applications thanks to its ability to predict, generate, or discover patterns from large quantities of data. However, the process of collecting and transforming data for practical use is intricate. Even in today's digital era, where substantial data is generated daily, it is uncommon for it to be readily usable; most often, it necessitates meticulous manual data preparation. The haste in developing new models can frequently result in various shortcomings, potentially posing risks when deployed in real-world scenarios (eg social discrimination, critical failures), leading to the failure or substantial escalation of costs in AI-based projects. This chapter provides a comprehensive overview of established methodological tools, enriched by our practical experience, in the development of datasets for machine learning. Initially, we develop the tasks involved in dataset development and offer insights into their effective management (including requirements, design, implementation, evaluation, distribution, and maintenance). Then, we provide more details about the implementation process which includes data collection, transformation, and quality evaluation. Finally, we address practical considerations regarding dataset distribution and maintenance.","sentences":["Machine learning is now used in many applications thanks to its ability to predict, generate, or discover patterns from large quantities of data.","However, the process of collecting and transforming data for practical use is intricate.","Even in today's digital era, where substantial data is generated daily, it is uncommon for it to be readily usable; most often, it necessitates meticulous manual data preparation.","The haste in developing new models can frequently result in various shortcomings, potentially posing risks when deployed in real-world scenarios (eg social discrimination, critical failures), leading to the failure or substantial escalation of costs in AI-based projects.","This chapter provides a comprehensive overview of established methodological tools, enriched by our practical experience, in the development of datasets for machine learning.","Initially, we develop the tasks involved in dataset development and offer insights into their effective management (including requirements, design, implementation, evaluation, distribution, and maintenance).","Then, we provide more details about the implementation process which includes data collection, transformation, and quality evaluation.","Finally, we address practical considerations regarding dataset distribution and maintenance."],"url":"http://arxiv.org/abs/2404.09703v1","category":"cs.LG"}
{"created":"2024-04-15 12:01:35","title":"Higher-order Sobolev embeddings into spaces of Campanato and Morrey type","abstract":"Necessary and sufficient conditions are offered for Sobolev type spaces built on rearrangement-invariant spaces to be continuously embedded into (generalized) Campanato and Morrey spaces on open subsets of the $n$-dimensional Euclidean space. As a consequence, the optimal target and domain spaces in the relevant embeddings are identified. Our general criteria are implemented to derive sharp embeddings in the class of Orlicz-Sobolev spaces.","sentences":["Necessary and sufficient conditions are offered for Sobolev type spaces built on rearrangement-invariant spaces to be continuously embedded into (generalized)","Campanato and Morrey spaces on open subsets of the $n$-dimensional Euclidean space.","As a consequence, the optimal target and domain spaces in the relevant embeddings are identified.","Our general criteria are implemented to derive sharp embeddings in the class of Orlicz-Sobolev spaces."],"url":"http://arxiv.org/abs/2404.09702v1","category":"math.FA"}
{"created":"2024-04-15 12:00:48","title":"Four-loop splitting functions in QCD -- The quark-to-gluon case","abstract":"We present the even-N moments N =< 20 of the fourth-order (N^3LO) contribution P_{gq}^(3)(x) to the quark-to-gluon splitting function in perturbative QCD. These moments, obtained by analytically computing off-shell operator matrix elements for a general gauge group, agree with all known results, in particular with the moments N =< 10 derived before from structure functions in deep-inelastic scattering. Using the new moments and the available endpoint constraints, we construct approximations for P_{gq}^(3)(x) which improve upon those obtained from the lowest five even moments. The remaining uncertainties of this function are now practically irrelevant at momentum fractions x > 0.1. The resulting errors of the convolution of P_{gq} at N^3LO with a typical quark distribution are small at x >~ 10^{-3} and exceed 1% only at x ~< 10^{-4} for a strong coupling alpha_s = 0.2. The present results for P_{gq}^(3)(x) should thus be sufficient for most collider-physics applications.","sentences":["We present the even-N moments N =< 20 of the fourth-order (N^3LO) contribution P_{gq}^(3)(x) to the quark-to-gluon splitting function in perturbative QCD.","These moments, obtained by analytically computing off-shell operator matrix elements for a general gauge group, agree with all known results, in particular with the moments N =< 10 derived before from structure functions in deep-inelastic scattering.","Using the new moments and the available endpoint constraints, we construct approximations for P_{gq}^(3)(x) which improve upon those obtained from the lowest five even moments.","The remaining uncertainties of this function are now practically irrelevant at momentum fractions x > 0.1.","The resulting errors of the convolution of P_{gq} at N^3LO with a typical quark distribution are small at x >~ 10^{-3} and exceed 1% only at x ~< 10^{-4} for a strong coupling alpha_s = 0.2.","The present results for P_{gq}^(3)(x) should thus be sufficient for most collider-physics applications."],"url":"http://arxiv.org/abs/2404.09701v1","category":"hep-ph"}
{"created":"2024-04-15 11:59:45","title":"Generative AI for Game Theory-based Mobile Networking","abstract":"With the continuous advancement of network technology, various emerging complex networking optimization problems opened up a wide range of applications utilizating of game theory. However, since game theory is a mathematical framework, game theory-based solutions often require the experience and knowledge of human experts. Recently, the remarkable advantages exhibited by generative artificial intelligence (GAI) have gained widespread attention. In this article, we propose a novel GAI-enabled game theory solution that combines the powerful reasoning and generation capabilities of GAI to the design and optimization of mobile networking. Specifically, we first outline the game theory and key technologies of GAI, and then explore the advantages of combining GAI with game theory. Then, we briefly review the advantages and limitations of existing research and demonstrate the potential application values of GAI applied to game theory in mobile networking. Subsequently, we develop a game theory framework enabled by large language models (LLMs) to realize this combination, and demonstrate the effectiveness of the proposed framework through a case study in secured UAV networks. Finally, we provide several directions for future extensions.","sentences":["With the continuous advancement of network technology, various emerging complex networking optimization problems opened up a wide range of applications utilizating of game theory.","However, since game theory is a mathematical framework, game theory-based solutions often require the experience and knowledge of human experts.","Recently, the remarkable advantages exhibited by generative artificial intelligence (GAI) have gained widespread attention.","In this article, we propose a novel GAI-enabled game theory solution that combines the powerful reasoning and generation capabilities of GAI to the design and optimization of mobile networking.","Specifically, we first outline the game theory and key technologies of GAI, and then explore the advantages of combining GAI with game theory.","Then, we briefly review the advantages and limitations of existing research and demonstrate the potential application values of GAI applied to game theory in mobile networking.","Subsequently, we develop a game theory framework enabled by large language models (LLMs) to realize this combination, and demonstrate the effectiveness of the proposed framework through a case study in secured UAV networks.","Finally, we provide several directions for future extensions."],"url":"http://arxiv.org/abs/2404.09699v1","category":"cs.GT"}
{"created":"2024-04-15 11:59:36","title":"Myrzakulov $F(T,Q)$ gravity: cosmological implications and constraints","abstract":"In this paper, we investigate some exact cosmological models in Myrzakulov $F(T,Q)$ gravity or the Myrzakulov gravity-III (MG-III) proposed in [arXiv:1205.5266], with observational constraints. The MG-III gravity is some kind of unification of two known gravity theories, namely, the $F(T)$ gravity and the $F(Q)$ gravity. The field equations of the MG-III theory are obtained by regarding the metric tensor and the general affine connection as independent variables. We then focus on the particular case in which the $F(T,Q)$ function characterizing the aforementioned metric-affine models is linear that is $F(T,Q)=\\lambda T+\\mu Q$. We investigate this linear case and consider a Friedmann-Lema\\^{i}tre-Robertson-Walker background to study cosmological aspects and applications. We have obtained three exact solutions of the modified field equations in different cases $T$ and $Q$, in the form of Hubble function $H(t)$ and scale factor $a(t)$ and placed observational constraints on it through the Hubble $H(z)$ datasets on it using the MCMC analysis. We have investigated the deceleration parameter $q(z)$, effective EoS parameters and a comparative study of all three models with $\\Lambda$CDM model has been carried out.","sentences":["In this paper, we investigate some exact cosmological models in Myrzakulov $F(T,Q)$ gravity or the Myrzakulov gravity-III (MG-III) proposed in [arXiv:1205.5266], with observational constraints.","The MG-III gravity is some kind of unification of two known gravity theories, namely, the $F(T)$ gravity and the $F(Q)$ gravity.","The field equations of the MG-III theory are obtained by regarding the metric tensor and the general affine connection as independent variables.","We then focus on the particular case in which the $F(T,Q)$ function characterizing the aforementioned metric-affine models is linear that is $F(T,Q)=\\lambda T+\\mu","Q$.","We investigate this linear case and consider a Friedmann-Lema\\^{i}tre-Robertson-Walker background to study cosmological aspects and applications.","We have obtained three exact solutions of the modified field equations in different cases $T$ and $Q$, in the form of Hubble function $H(t)$ and scale factor $a(t)$ and placed observational constraints on it through the Hubble $H(z)$ datasets on it using the MCMC analysis.","We have investigated the deceleration parameter $q(z)$, effective EoS parameters and a comparative study of all three models with $\\Lambda$CDM model has been carried out."],"url":"http://arxiv.org/abs/2404.09698v1","category":"gr-qc"}
{"created":"2024-04-15 11:54:27","title":"Are Large Language Models Reliable Argument Quality Annotators?","abstract":"Evaluating the quality of arguments is a crucial aspect of any system leveraging argument mining. However, it is a challenge to obtain reliable and consistent annotations regarding argument quality, as this usually requires domain-specific expertise of the annotators. Even among experts, the assessment of argument quality is often inconsistent due to the inherent subjectivity of this task. In this paper, we study the potential of using state-of-the-art large language models (LLMs) as proxies for argument quality annotators. To assess the capability of LLMs in this regard, we analyze the agreement between model, human expert, and human novice annotators based on an established taxonomy of argument quality dimensions. Our findings highlight that LLMs can produce consistent annotations, with a moderately high agreement with human experts across most of the quality dimensions. Moreover, we show that using LLMs as additional annotators can significantly improve the agreement between annotators. These results suggest that LLMs can serve as a valuable tool for automated argument quality assessment, thus streamlining and accelerating the evaluation of large argument datasets.","sentences":["Evaluating the quality of arguments is a crucial aspect of any system leveraging argument mining.","However, it is a challenge to obtain reliable and consistent annotations regarding argument quality, as this usually requires domain-specific expertise of the annotators.","Even among experts, the assessment of argument quality is often inconsistent due to the inherent subjectivity of this task.","In this paper, we study the potential of using state-of-the-art large language models (LLMs) as proxies for argument quality annotators.","To assess the capability of LLMs in this regard, we analyze the agreement between model, human expert, and human novice annotators based on an established taxonomy of argument quality dimensions.","Our findings highlight that LLMs can produce consistent annotations, with a moderately high agreement with human experts across most of the quality dimensions.","Moreover, we show that using LLMs as additional annotators can significantly improve the agreement between annotators.","These results suggest that LLMs can serve as a valuable tool for automated argument quality assessment, thus streamlining and accelerating the evaluation of large argument datasets."],"url":"http://arxiv.org/abs/2404.09696v1","category":"cs.CL"}
{"created":"2024-04-15 11:53:22","title":"LoRAP: Transformer Sub-Layers Deserve Differentiated Structured Compression for Large Language Models","abstract":"Large language models (LLMs) show excellent performance in difficult tasks, but they often require massive memories and computational resources. How to reduce the parameter scale of LLMs has become research hotspots. In this study, we make an important observation that the multi-head self-attention (MHA) sub-layer of Transformer exhibits noticeable low-rank structure, while the feed-forward network (FFN) sub-layer does not. With this regard, we design a mixed compression model, which organically combines Low-Rank matrix approximation And structured Pruning (LoRAP). For the MHA sub-layer, we propose an input activation weighted singular value decomposition method to strengthen the low-rank characteristic. Furthermore, we discover that the weight matrices in MHA sub-layer have different low-rank degrees. Thus, a novel parameter allocation scheme according to the discrepancy of low-rank degrees is devised. For the FFN sub-layer, we propose a gradient-free structured channel pruning method. During the pruning, we get an interesting finding that the least important 1% of parameter actually play a vital role in model performance. Extensive evaluations on zero-shot perplexity and zero-shot task classification indicate that our proposal is superior to previous structured compression rivals under multiple compression ratios.","sentences":["Large language models (LLMs) show excellent performance in difficult tasks, but they often require massive memories and computational resources.","How to reduce the parameter scale of LLMs has become research hotspots.","In this study, we make an important observation that the multi-head self-attention (MHA) sub-layer of Transformer exhibits noticeable low-rank structure, while the feed-forward network (FFN) sub-layer does not.","With this regard, we design a mixed compression model, which organically combines Low-Rank matrix approximation And structured Pruning (LoRAP).","For the MHA sub-layer, we propose an input activation weighted singular value decomposition method to strengthen the low-rank characteristic.","Furthermore, we discover that the weight matrices in MHA sub-layer have different low-rank degrees.","Thus, a novel parameter allocation scheme according to the discrepancy of low-rank degrees is devised.","For the FFN sub-layer, we propose a gradient-free structured channel pruning method.","During the pruning, we get an interesting finding that the least important 1% of parameter actually play a vital role in model performance.","Extensive evaluations on zero-shot perplexity and zero-shot task classification indicate that our proposal is superior to previous structured compression rivals under multiple compression ratios."],"url":"http://arxiv.org/abs/2404.09695v1","category":"cs.LG"}
{"created":"2024-04-15 11:45:30","title":"Harnessing GPT-4V(ision) for Insurance: A Preliminary Exploration","abstract":"The emergence of Large Multimodal Models (LMMs) marks a significant milestone in the development of artificial intelligence. Insurance, as a vast and complex discipline, involves a wide variety of data forms in its operational processes, including text, images, and videos, thereby giving rise to diverse multimodal tasks. Despite this, there has been limited systematic exploration of multimodal tasks specific to insurance, nor a thorough investigation into how LMMs can address these challenges. In this paper, we explore GPT-4V's capabilities in the insurance domain. We categorize multimodal tasks by focusing primarily on visual aspects based on types of insurance (e.g., auto, household/commercial property, health, and agricultural insurance) and insurance stages (e.g., risk assessment, risk monitoring, and claims processing). Our experiment reveals that GPT-4V exhibits remarkable abilities in insurance-related tasks, demonstrating not only a robust understanding of multimodal content in the insurance domain but also a comprehensive knowledge of insurance scenarios. However, there are notable shortcomings: GPT-4V struggles with detailed risk rating and loss assessment, suffers from hallucination in image understanding, and shows variable support for different languages. Through this work, we aim to bridge the insurance domain with cutting-edge LMM technology, facilitate interdisciplinary exchange and development, and provide a foundation for the continued advancement and evolution of future research endeavors.","sentences":["The emergence of Large Multimodal Models (LMMs) marks a significant milestone in the development of artificial intelligence.","Insurance, as a vast and complex discipline, involves a wide variety of data forms in its operational processes, including text, images, and videos, thereby giving rise to diverse multimodal tasks.","Despite this, there has been limited systematic exploration of multimodal tasks specific to insurance, nor a thorough investigation into how LMMs can address these challenges.","In this paper, we explore GPT-4V's capabilities in the insurance domain.","We categorize multimodal tasks by focusing primarily on visual aspects based on types of insurance (e.g., auto, household/commercial property, health, and agricultural insurance) and insurance stages (e.g., risk assessment, risk monitoring, and claims processing).","Our experiment reveals that GPT-4V exhibits remarkable abilities in insurance-related tasks, demonstrating not only a robust understanding of multimodal content in the insurance domain but also a comprehensive knowledge of insurance scenarios.","However, there are notable shortcomings: GPT-4V struggles with detailed risk rating and loss assessment, suffers from hallucination in image understanding, and shows variable support for different languages.","Through this work, we aim to bridge the insurance domain with cutting-edge LMM technology, facilitate interdisciplinary exchange and development, and provide a foundation for the continued advancement and evolution of future research endeavors."],"url":"http://arxiv.org/abs/2404.09690v1","category":"cs.CV"}
{"created":"2024-04-15 11:37:47","title":"Plus Strategies are Exponentially Slower for Planted Optima of Random Height","abstract":"We compare the $(1,\\lambda)$-EA and the $(1 + \\lambda)$-EA on the recently introduced benchmark DisOM, which is the OneMax function with randomly planted local optima. Previous work showed that if all local optima have the same relative height, then the plus strategy never loses more than a factor $O(n\\log n)$ compared to the comma strategy. Here we show that even small random fluctuations in the heights of the local optima have a devastating effect for the plus strategy and lead to super-polynomial runtimes. On the other hand, due to their ability to escape local optima, comma strategies are unaffected by the height of the local optima and remain efficient. Our results hold for a broad class of possible distortions and show that the plus strategy, but not the comma strategy, is generally deceived by sparse unstructured fluctuations of a smooth landscape.","sentences":["We compare the $(1,\\lambda)$-EA and the $(1 + \\lambda)$-EA on the recently introduced benchmark DisOM, which is the OneMax function with randomly planted local optima.","Previous work showed that if all local optima have the same relative height, then the plus strategy never loses more than a factor $O(n\\log n)$ compared to the comma strategy.","Here we show that even small random fluctuations in the heights of the local optima have a devastating effect for the plus strategy and lead to super-polynomial runtimes.","On the other hand, due to their ability to escape local optima, comma strategies are unaffected by the height of the local optima and remain efficient.","Our results hold for a broad class of possible distortions and show that the plus strategy, but not the comma strategy, is generally deceived by sparse unstructured fluctuations of a smooth landscape."],"url":"http://arxiv.org/abs/2404.09687v1","category":"cs.NE"}
{"created":"2024-04-15 11:36:58","title":"Bridging the Gap: Advancements in Technology to Support Dementia Care -- A Scoping Review","abstract":"Dementia has serious consequences for the daily life of the person affected due to the decline in the their cognitive, behavioral and functional abilities. Caring for people living with dementia can be challenging and distressing. Innovative solutions are becoming essential to enrich the lives of those impacted and alleviate caregiver burdens. This scoping review, spanning literature from 2010 to July 2023 in the field of Human-Computer Interaction (HCI), offers a comprehensive look at how interactive technology contributes to dementia care. Emphasizing technology's role in addressing the unique needs of people with dementia (PwD) and their caregivers, this review encompasses assistive devices, mobile applications, sensors, and GPS tracking. Delving into challenges encountered in clinical and home-care settings, it succinctly outlines the influence of cutting-edge technologies, such as wearables, virtual reality, robots, and artificial intelligence, in supporting individuals with dementia and their caregivers. We categorize current dementia-related technologies into six groups based on their intended use and function: 1) daily life monitoring, 2) daily life support, 3) social interaction and communication, 4) well-being enhancement, 5) cognitive support, and 6) caregiver support.","sentences":["Dementia has serious consequences for the daily life of the person affected due to the decline in the their cognitive, behavioral and functional abilities.","Caring for people living with dementia can be challenging and distressing.","Innovative solutions are becoming essential to enrich the lives of those impacted and alleviate caregiver burdens.","This scoping review, spanning literature from 2010 to July 2023 in the field of Human-Computer Interaction (HCI), offers a comprehensive look at how interactive technology contributes to dementia care.","Emphasizing technology's role in addressing the unique needs of people with dementia (PwD) and their caregivers, this review encompasses assistive devices, mobile applications, sensors, and GPS tracking.","Delving into challenges encountered in clinical and home-care settings, it succinctly outlines the influence of cutting-edge technologies, such as wearables, virtual reality, robots, and artificial intelligence, in supporting individuals with dementia and their caregivers.","We categorize current dementia-related technologies into six groups based on their intended use and function: 1) daily life monitoring, 2) daily life support, 3) social interaction and communication, 4) well-being enhancement, 5) cognitive support, and 6) caregiver support."],"url":"http://arxiv.org/abs/2404.09685v1","category":"cs.HC"}
{"created":"2024-04-15 11:36:38","title":"Testing trajectory-based determinism via time probability distributions","abstract":"It is notorious that quantum mechanics (QM) cannot predict well-defined values for all physical quantities. Less well-known, however, is the fact that QM is unable to furnish probabilistic predictions even in emblematic scenarios such as the double-slit experiment. In contrast, equipped with postulate trajectories, Bohmian mechanics (BM) has inherited more predictive power. It follows that, contrary to common belief, QM and BM are not just different interpretations but distinct theories. This work formalizes the aforementioned assertions and illustrates them through three case studies: (i) free particle, (ii) free fall under a uniform gravitational field, and (iii) the double-slit experiment. Specifically, we introduce a prescription for constructing a flight-time probability distribution within generic trajectory-equipped theories. We then apply our formalism to BM and derive probability distributions that are unreachable by QM. Our results can, in principle, be tested against real experiments, thereby assessing the validity of Bohmian trajectories.","sentences":["It is notorious that quantum mechanics (QM) cannot predict well-defined values for all physical quantities.","Less well-known, however, is the fact that QM is unable to furnish probabilistic predictions even in emblematic scenarios such as the double-slit experiment.","In contrast, equipped with postulate trajectories, Bohmian mechanics (BM) has inherited more predictive power.","It follows that, contrary to common belief, QM and BM are not just different interpretations but distinct theories.","This work formalizes the aforementioned assertions and illustrates them through three case studies: (i) free particle, (ii) free fall under a uniform gravitational field, and (iii) the double-slit experiment.","Specifically, we introduce a prescription for constructing a flight-time probability distribution within generic trajectory-equipped theories.","We then apply our formalism to BM and derive probability distributions that are unreachable by QM.","Our results can, in principle, be tested against real experiments, thereby assessing the validity of Bohmian trajectories."],"url":"http://arxiv.org/abs/2404.09684v1","category":"quant-ph"}
{"created":"2024-04-15 11:36:10","title":"Multi-News+: Cost-efficient Dataset Cleansing via LLM-based Data Annotation","abstract":"The quality of the dataset is crucial for ensuring optimal performance and reliability of downstream task models. However, datasets often contain noisy data inadvertently included during the construction process. Numerous attempts have been made to correct this issue through human annotators. However, hiring and managing human annotators is expensive and time-consuming. As an alternative, recent studies are exploring the use of large language models (LLMs) for data annotation.   In this study, we present a case study that extends the application of LLM-based data annotation to enhance the quality of existing datasets through a cleansing strategy. Specifically, we leverage approaches such as chain-of-thought (CoT) and majority voting to imitate human annotation and classify unrelated documents from the Multi-News dataset, which is widely used for the multi-document summarization task. Through our proposed cleansing method, we introduce an enhanced Multi-News+. By employing LLMs for data cleansing, we demonstrate an efficient and effective approach to improving dataset quality without relying on expensive human annotation efforts.","sentences":["The quality of the dataset is crucial for ensuring optimal performance and reliability of downstream task models.","However, datasets often contain noisy data inadvertently included during the construction process.","Numerous attempts have been made to correct this issue through human annotators.","However, hiring and managing human annotators is expensive and time-consuming.","As an alternative, recent studies are exploring the use of large language models (LLMs) for data annotation.   ","In this study, we present a case study that extends the application of LLM-based data annotation to enhance the quality of existing datasets through a cleansing strategy.","Specifically, we leverage approaches such as chain-of-thought (CoT) and majority voting to imitate human annotation and classify unrelated documents from the Multi-News dataset, which is widely used for the multi-document summarization task.","Through our proposed cleansing method, we introduce an enhanced Multi-News+.","By employing LLMs for data cleansing, we demonstrate an efficient and effective approach to improving dataset quality without relying on expensive human annotation efforts."],"url":"http://arxiv.org/abs/2404.09682v1","category":"cs.CL"}
{"created":"2024-04-15 11:24:16","title":"On the geometry of exponential random graphs and applications","abstract":"In a seminal paper in 2009, Borcea, Br\\\"and\\'en, and Liggett described the connection between probability distributions and the geometry of their generating polynomials. Namely, they characterized that stable generating polynomials correspond to distributions with the strongest form of negative dependence. This motivates us to investigate other distributions that can have this property, and our focus is on random graph models. In this article, we will lay the groundwork to investigate Markov random graphs, and more generally exponential random graph models (ERGMs), from this geometric perspective. In particular, by determining when their corresponding generating polynomials are either stable and/or Lorentzian. The Lorentzian property was first described in 2020 by Br\\\"and\\'en and Huh and independently by Anari, Oveis-Gharan, and Vinzant where the latter group called it the completely log-concave property. The theory of stable polynomials predates this, and is commonly thought of as the multivariate notion of real-rootedness. Br\\\"and\\'en and Huh proved that stable polynomials are always Lorentzian. Although it is a strong condition, verifying stability is not always feasible. We will characterize when certain classes of Markov random graphs are stable and when they are only Lorentzian. We then shift our attention to applications of these properties to real-world networks.","sentences":["In a seminal paper in 2009, Borcea, Br\\\"and\\'en, and Liggett described the connection between probability distributions and the geometry of their generating polynomials.","Namely, they characterized that stable generating polynomials correspond to distributions with the strongest form of negative dependence.","This motivates us to investigate other distributions that can have this property, and our focus is on random graph models.","In this article, we will lay the groundwork to investigate Markov random graphs, and more generally exponential random graph models (ERGMs), from this geometric perspective.","In particular, by determining when their corresponding generating polynomials are either stable and/or Lorentzian.","The Lorentzian property was first described in 2020 by Br\\\"and\\'en and Huh and independently by Anari, Oveis-Gharan, and Vinzant where the latter group called it the completely log-concave property.","The theory of stable polynomials predates this, and is commonly thought of as the multivariate notion of real-rootedness.","Br\\\"and\\'en and Huh proved that stable polynomials are always Lorentzian.","Although it is a strong condition, verifying stability is not always feasible.","We will characterize when certain classes of Markov random graphs are stable and when they are only Lorentzian.","We then shift our attention to applications of these properties to real-world networks."],"url":"http://arxiv.org/abs/2404.09680v1","category":"math.CO"}
{"created":"2024-04-15 11:14:25","title":"A Generic Trajectory Planning Method for Constrained All-Wheel-Steering Robots","abstract":"This paper presents a trajectory planning method for wheeled robots with fixed steering axes while the steering angle of each wheel is constrained. In the past, All-Wheel-Steering(AWS) robots, incorporating modes such as rotation-free translation maneuvers, in-situ rotational maneuvers, and proportional steering, exhibited inefficient performance due to time-consuming mode switches. This inefficiency arises from wheel rotation constraints and inter-wheel cooperation requirements. The direct application of a holonomic moving strategy can lead to significant slip angles or even structural failure. Additionally, the limited steering range of AWS wheeled robots exacerbates nonlinearity issues, thereby complicating control processes. To address these challenges, we developed a novel planning method termed Constrained AWS(C-AWS), which integrates second-order discrete search with predictive control techniques. Experimental results demonstrate that our method adeptly generates feasible and smooth trajectories for C-AWS while adhering to steering angle constraints.","sentences":["This paper presents a trajectory planning method for wheeled robots with fixed steering axes while the steering angle of each wheel is constrained.","In the past, All-Wheel-Steering(AWS) robots, incorporating modes such as rotation-free translation maneuvers, in-situ rotational maneuvers, and proportional steering, exhibited inefficient performance due to time-consuming mode switches.","This inefficiency arises from wheel rotation constraints and inter-wheel cooperation requirements.","The direct application of a holonomic moving strategy can lead to significant slip angles or even structural failure.","Additionally, the limited steering range of AWS wheeled robots exacerbates nonlinearity issues, thereby complicating control processes.","To address these challenges, we developed a novel planning method termed Constrained AWS(C-AWS), which integrates second-order discrete search with predictive control techniques.","Experimental results demonstrate that our method adeptly generates feasible and smooth trajectories for C-AWS while adhering to steering angle constraints."],"url":"http://arxiv.org/abs/2404.09677v1","category":"cs.RO"}
{"created":"2024-04-15 11:08:37","title":"Parametrized Black Hole Quasinormal Ringdown Formalism for Higher Overtones","abstract":"We investigate the parametrized black hole quasinormal ringdown formalism, which is a robust framework used to analyze quasinormal modes in systems that closely resemble general relativity, paying particular attention to the higher overtones. We find that larger deviations from the general relativity case typically appear in the quasinormal frequencies for the higher overtones. This growing tendency for higher overtones can be understood using an analytical method, and its relations to previous works are briefly discussed. Our findings indicate that we can impose a strong constraint on gravity theories by considering the overtones of quasinormal modes.","sentences":["We investigate the parametrized black hole quasinormal ringdown formalism, which is a robust framework used to analyze quasinormal modes in systems that closely resemble general relativity, paying particular attention to the higher overtones.","We find that larger deviations from the general relativity case typically appear in the quasinormal frequencies for the higher overtones.","This growing tendency for higher overtones can be understood using an analytical method, and its relations to previous works are briefly discussed.","Our findings indicate that we can impose a strong constraint on gravity theories by considering the overtones of quasinormal modes."],"url":"http://arxiv.org/abs/2404.09672v1","category":"gr-qc"}
{"created":"2024-04-15 11:03:31","title":"Localized Resonant Phonon Polaritons in Biaxial Nanoparticles","abstract":"The discovery of localized plasmon polariton resonances has been pivotal in enabling tunability of the optical resonance. Recently, extensive research efforts have aimed to expand these achievements to other polaritonic states that exhibit less loss and in other spectral regions. However, these efforts were limited to isotropic or uniaxial structures, and an eigenmode theory was derived only for isotropic particles. Here, we present a breakthrough in synthesizing biaxial nanostructures that exhibit localized hyperbolic phonon resonances with high Q-factors in the mid-infrared. Furthermore, we develop a theory that predicts high-order resonances in anisotropic particles with coupling between the axial permittivites. Finally, we confirm the theoretical predictions through near-field measurements, which demonstrate the existence of both the first and higher-order resonant modes. Our findings provide the foundation for designing a new generation of anisotropic resonators with various applications in the mid-IR range. Our analysis applies to other fields, such as quasi-magnetostatics and heat conduction.","sentences":["The discovery of localized plasmon polariton resonances has been pivotal in enabling tunability of the optical resonance.","Recently, extensive research efforts have aimed to expand these achievements to other polaritonic states that exhibit less loss and in other spectral regions.","However, these efforts were limited to isotropic or uniaxial structures, and an eigenmode theory was derived only for isotropic particles.","Here, we present a breakthrough in synthesizing biaxial nanostructures that exhibit localized hyperbolic phonon resonances with high Q-factors in the mid-infrared.","Furthermore, we develop a theory that predicts high-order resonances in anisotropic particles with coupling between the axial permittivites.","Finally, we confirm the theoretical predictions through near-field measurements, which demonstrate the existence of both the first and higher-order resonant modes.","Our findings provide the foundation for designing a new generation of anisotropic resonators with various applications in the mid-IR range.","Our analysis applies to other fields, such as quasi-magnetostatics and heat conduction."],"url":"http://arxiv.org/abs/2404.09669v1","category":"physics.optics"}
{"created":"2024-04-15 10:51:03","title":"Microwave Hall measurements using a circularly polarized dielectric cavity","abstract":"We have developed a circularly polarized dielectric rutile (TiO$_2$) cavity with a high quality-factor that can generate circularly polarized microwaves from two orthogonal linearly polarized microwaves with a phase difference of $\\pm\\pi/2$ using a hybrid coupler. Using this cavity, we have established a new methodology to measure the microwave Hall conductivity of a small single crystal of metals in the skin-depth region. Based on the cavity perturbation technique, we have shown that all components of the surface impedance tensor can be extracted under the application of a magnetic field by comparing the right- and left-handed circularly polarization modes. To verify the validity of the developed method, we performed test measurements on tiny Bi single crystals at low temperatures. As a result, we have successfully obtained the surface impedance tensor components and confirmed that the characteristic field dependence of the ac Hall angle in the microwave region is consistent with the expectation from the dc transport measurements. These results demonstrate a significant improvement in sensitivity compared to previous methods. Thus, our developed technique allows more accurate microwave Hall measurements, opening the way for new approaches to explore novel topological quantum materials, such as time-reversal symmetry-breaking superconductors.","sentences":["We have developed a circularly polarized dielectric rutile (TiO$_2$) cavity with a high quality-factor that can generate circularly polarized microwaves from two orthogonal linearly polarized microwaves with a phase difference of $\\pm\\pi/2$ using a hybrid coupler.","Using this cavity, we have established a new methodology to measure the microwave Hall conductivity of a small single crystal of metals in the skin-depth region.","Based on the cavity perturbation technique, we have shown that all components of the surface impedance tensor can be extracted under the application of a magnetic field by comparing the right- and left-handed circularly polarization modes.","To verify the validity of the developed method, we performed test measurements on tiny Bi single crystals at low temperatures.","As a result, we have successfully obtained the surface impedance tensor components and confirmed that the characteristic field dependence of the ac Hall angle in the microwave region is consistent with the expectation from the dc transport measurements.","These results demonstrate a significant improvement in sensitivity compared to previous methods.","Thus, our developed technique allows more accurate microwave Hall measurements, opening the way for new approaches to explore novel topological quantum materials, such as time-reversal symmetry-breaking superconductors."],"url":"http://arxiv.org/abs/2404.09662v1","category":"cond-mat.str-el"}
{"created":"2024-04-15 10:45:12","title":"Sampling for Model Predictive Trajectory Planning in Autonomous Driving using Normalizing Flows","abstract":"Alongside optimization-based planners, sampling-based approaches are often used in trajectory planning for autonomous driving due to their simplicity. Model predictive path integral control is a framework that builds upon optimization principles while incorporating stochastic sampling of input trajectories. This paper investigates several sampling approaches for trajectory generation. In this context, normalizing flows originating from the field of variational inference are considered for the generation of sampling distributions, as they model transformations of simple to more complex distributions. Accordingly, learning-based normalizing flow models are trained for a more efficient exploration of the input domain for the task at hand. The developed algorithm and the proposed sampling distributions are evaluated in two simulation scenarios.","sentences":["Alongside optimization-based planners, sampling-based approaches are often used in trajectory planning for autonomous driving due to their simplicity.","Model predictive path integral control is a framework that builds upon optimization principles while incorporating stochastic sampling of input trajectories.","This paper investigates several sampling approaches for trajectory generation.","In this context, normalizing flows originating from the field of variational inference are considered for the generation of sampling distributions, as they model transformations of simple to more complex distributions.","Accordingly, learning-based normalizing flow models are trained for a more efficient exploration of the input domain for the task at hand.","The developed algorithm and the proposed sampling distributions are evaluated in two simulation scenarios."],"url":"http://arxiv.org/abs/2404.09657v1","category":"cs.RO"}
{"created":"2024-04-15 10:44:31","title":"Learn Your Reference Model for Real Good Alignment","abstract":"The complexity of the alignment problem stems from the fact that existing methods are unstable. Researchers continuously invent various tricks to address this shortcoming. For instance, in the fundamental Reinforcement Learning From Human Feedback (RLHF) technique of Language Model alignment, in addition to reward maximization, the Kullback-Leibler divergence between the trainable policy and the SFT policy is minimized. This addition prevents the model from being overfitted to the Reward Model (RM) and generating texts that are out-of-domain for the RM. The Direct Preference Optimization (DPO) method reformulates the optimization task of RLHF and eliminates the Reward Model while tacitly maintaining the requirement for the policy to be close to the SFT policy. In our paper, we argue that this implicit limitation in the DPO method leads to sub-optimal results. We propose a new method called Trust Region DPO (TR-DPO), which updates the reference policy during training. With such a straightforward update, we demonstrate the effectiveness of TR-DPO against DPO on the Anthropic HH and TLDR datasets. We show that TR-DPO outperforms DPO by up to 19%, measured by automatic evaluation with GPT-4. The new alignment approach that we propose allows us to improve the quality of models across several parameters at once, such as coherence, correctness, level of detail, helpfulness, and harmlessness.","sentences":["The complexity of the alignment problem stems from the fact that existing methods are unstable.","Researchers continuously invent various tricks to address this shortcoming.","For instance, in the fundamental Reinforcement Learning From Human Feedback (RLHF) technique of Language Model alignment, in addition to reward maximization, the Kullback-Leibler divergence between the trainable policy and the SFT policy is minimized.","This addition prevents the model from being overfitted to the Reward Model (RM) and generating texts that are out-of-domain for the RM.","The Direct Preference Optimization (DPO) method reformulates the optimization task of RLHF and eliminates the Reward Model while tacitly maintaining the requirement for the policy to be close to the SFT policy.","In our paper, we argue that this implicit limitation in the DPO method leads to sub-optimal results.","We propose a new method called Trust Region DPO (TR-DPO), which updates the reference policy during training.","With such a straightforward update, we demonstrate the effectiveness of TR-DPO against DPO on the Anthropic HH and TLDR datasets.","We show that TR-DPO outperforms DPO by up to 19%, measured by automatic evaluation with GPT-4.","The new alignment approach that we propose allows us to improve the quality of models across several parameters at once, such as coherence, correctness, level of detail, helpfulness, and harmlessness."],"url":"http://arxiv.org/abs/2404.09656v1","category":"cs.LG"}
{"created":"2024-04-15 10:42:22","title":"Do LLMs Understand Visual Anomalies? Uncovering LLM Capabilities in Zero-shot Anomaly Detection","abstract":"Large vision-language models (LVLMs) are markedly proficient in deriving visual representations guided by natural language. Recent explorations have utilized LVLMs to tackle zero-shot visual anomaly detection (VAD) challenges by pairing images with textual descriptions indicative of normal and abnormal conditions, referred to as anomaly prompts. However, existing approaches depend on static anomaly prompts that are prone to cross-semantic ambiguity, and prioritize global image-level representations over crucial local pixel-level image-to-text alignment that is necessary for accurate anomaly localization. In this paper, we present ALFA, a training-free approach designed to address these challenges via a unified model. We propose a run-time prompt adaptation strategy, which first generates informative anomaly prompts to leverage the capabilities of a large language model (LLM). This strategy is enhanced by a contextual scoring mechanism for per-image anomaly prompt adaptation and cross-semantic ambiguity mitigation. We further introduce a novel fine-grained aligner to fuse local pixel-level semantics for precise anomaly localization, by projecting the image-text alignment from global to local semantic spaces. Extensive evaluations on the challenging MVTec and VisA datasets confirm ALFA's effectiveness in harnessing the language potential for zero-shot VAD, achieving significant PRO improvements of 12.1% on MVTec AD and 8.9% on VisA compared to state-of-the-art zero-shot VAD approaches.","sentences":["Large vision-language models (LVLMs) are markedly proficient in deriving visual representations guided by natural language.","Recent explorations have utilized LVLMs to tackle zero-shot visual anomaly detection (VAD) challenges by pairing images with textual descriptions indicative of normal and abnormal conditions, referred to as anomaly prompts.","However, existing approaches depend on static anomaly prompts that are prone to cross-semantic ambiguity, and prioritize global image-level representations over crucial local pixel-level image-to-text alignment that is necessary for accurate anomaly localization.","In this paper, we present ALFA, a training-free approach designed to address these challenges via a unified model.","We propose a run-time prompt adaptation strategy, which first generates informative anomaly prompts to leverage the capabilities of a large language model (LLM).","This strategy is enhanced by a contextual scoring mechanism for per-image anomaly prompt adaptation and cross-semantic ambiguity mitigation.","We further introduce a novel fine-grained aligner to fuse local pixel-level semantics for precise anomaly localization, by projecting the image-text alignment from global to local semantic spaces.","Extensive evaluations on the challenging MVTec and VisA datasets confirm ALFA's effectiveness in harnessing the language potential for zero-shot VAD, achieving significant PRO improvements of 12.1% on MVTec AD and 8.9% on VisA compared to state-of-the-art zero-shot VAD approaches."],"url":"http://arxiv.org/abs/2404.09654v1","category":"cs.CV"}
{"created":"2024-04-15 10:33:39","title":"Monitoring Second-Order Hyperproperties","abstract":"Hyperproperties express the relationship between multiple executions of a system. This is needed in many AI-related fields, such as knowledge representation and planning, to capture system properties related to knowledge, information flow, and privacy. In this paper, we study the monitoring of complex hyperproperties at runtime. Previous work in this area has either focused on the simpler problem of monitoring trace properties (which are sets of traces, while hyperproperties are sets of sets of traces) or on monitoring first-order hyperproperties, which are expressible in temporal logics with first-order quantification over traces, such as HyperLTL. We present the first monitoring algorithm for the much more expressive class of second-order hyperproperties. Second-order hyperproperties include system properties like common knowledge, which cannot be expressed in first-order logics like HyperLTL.   We introduce Hyper$^2$LTL$_f$, a temporal logic over finite traces that allows for second-order quantification over sets of traces. We study the monitoring problem in two fundamental execution models: (1) the parallel model, where a fixed number of traces is monitored in parallel, and (2) the sequential model, where an unbounded number of traces is observed sequentially, one trace after the other. For the parallel model, we show that the monitoring of the second-order hyperproperties of Hyper$^2$LTL$_f$ can be reduced to monitoring first-order hyperproperties. For the sequential model, we present a monitoring algorithm that handles second-order quantification efficiently, exploiting optimizations based on the monotonicity of subformulas, graph-based storing of executions, and fixpoint hashing. We present experimental results from a range of benchmarks, including examples from common knowledge and planning.","sentences":["Hyperproperties express the relationship between multiple executions of a system.","This is needed in many AI-related fields, such as knowledge representation and planning, to capture system properties related to knowledge, information flow, and privacy.","In this paper, we study the monitoring of complex hyperproperties at runtime.","Previous work in this area has either focused on the simpler problem of monitoring trace properties (which are sets of traces, while hyperproperties are sets of sets of traces) or on monitoring first-order hyperproperties, which are expressible in temporal logics with first-order quantification over traces, such as HyperLTL.","We present the first monitoring algorithm for the much more expressive class of second-order hyperproperties.","Second-order hyperproperties include system properties like common knowledge, which cannot be expressed in first-order logics like HyperLTL.   ","We introduce Hyper$^2$LTL$_f$, a temporal logic over finite traces that allows for second-order quantification over sets of traces.","We study the monitoring problem in two fundamental execution models: (1) the parallel model, where a fixed number of traces is monitored in parallel, and (2) the sequential model, where an unbounded number of traces is observed sequentially, one trace after the other.","For the parallel model, we show that the monitoring of the second-order hyperproperties of Hyper$^2$LTL$_f$ can be reduced to monitoring first-order hyperproperties.","For the sequential model, we present a monitoring algorithm that handles second-order quantification efficiently, exploiting optimizations based on the monotonicity of subformulas, graph-based storing of executions, and fixpoint hashing.","We present experimental results from a range of benchmarks, including examples from common knowledge and planning."],"url":"http://arxiv.org/abs/2404.09652v1","category":"cs.LO"}
{"created":"2024-04-15 10:23:11","title":"An Origami-Inspired Variable Friction Surface for Increasing the Dexterity of Robotic Grippers","abstract":"While the grasping capability of robotic grippers has shown significant development, the ability to manipulate objects within the hand is still limited. One explanation for this limitation is the lack of controlled contact variation between the grasped object and the gripper. For instance, human hands have the ability to firmly grip object surfaces, as well as slide over object faces, an aspect that aids the enhanced manipulation of objects within the hand without losing contact. In this letter, we present a parametric, origami-inspired thin surface capable of transitioning between a high friction and a low friction state, suitable for implementation as an epidermis in robotic fingers. A numerical analysis of the proposed surface based on its design parameters, force analysis, and performance in in-hand manipulation tasks is presented. Through the development of a simple two-fingered two-degree-of-freedom gripper utilizing the proposed variable-friction surfaces with different parameters, we experimentally demonstrate the improved manipulation capabilities of the hand when compared to the same gripper without changeable friction. Results show that the pattern density and valley gap are the main parameters that effect the in-hand manipulation performance. The origami-inspired thin surface with a higher pattern density generated a smaller valley gap and smaller height change, producing a more stable improvement of the manipulation capabilities of the hand.","sentences":["While the grasping capability of robotic grippers has shown significant development, the ability to manipulate objects within the hand is still limited.","One explanation for this limitation is the lack of controlled contact variation between the grasped object and the gripper.","For instance, human hands have the ability to firmly grip object surfaces, as well as slide over object faces, an aspect that aids the enhanced manipulation of objects within the hand without losing contact.","In this letter, we present a parametric, origami-inspired thin surface capable of transitioning between a high friction and a low friction state, suitable for implementation as an epidermis in robotic fingers.","A numerical analysis of the proposed surface based on its design parameters, force analysis, and performance in in-hand manipulation tasks is presented.","Through the development of a simple two-fingered two-degree-of-freedom gripper utilizing the proposed variable-friction surfaces with different parameters, we experimentally demonstrate the improved manipulation capabilities of the hand when compared to the same gripper without changeable friction.","Results show that the pattern density and valley gap are the main parameters that effect the in-hand manipulation performance.","The origami-inspired thin surface with a higher pattern density generated a smaller valley gap and smaller height change, producing a more stable improvement of the manipulation capabilities of the hand."],"url":"http://arxiv.org/abs/2404.09644v1","category":"cs.RO"}
{"created":"2024-04-15 10:21:28","title":"Indirect constraints on third generation baryon number violation","abstract":"The non-observation of baryon number violation suggests that the scale of baryon-number violating interactions at zero temperature is comparable to the GUT scale. However, the pertinent measurements involve hadrons made of the first-generation quarks, such as protons and neutrons. One may therefore entertain the idea that new flavour physics breaks baryon number at a much lower scale, but only in the coupling to a third generation quark, leading to observable baryon-number violating $b$-hadron decay rates. In this paper we show that indirect constraints on the new physics scale $\\Lambda_{\\rm BNV}$ from the existing bounds on the proton lifetime do not allow for this possibility. For this purpose we consider the three dominant proton decay channels $p \\to \\ell^+ \\nu_\\ell \\bar{\\nu}$, $p \\to \\pi^+ \\bar{\\nu}$ and $p \\to \\pi^0 \\ell^+$ mediated by a virtual bottom quark.","sentences":["The non-observation of baryon number violation suggests that the scale of baryon-number violating interactions at zero temperature is comparable to the GUT scale.","However, the pertinent measurements involve hadrons made of the first-generation quarks, such as protons and neutrons.","One may therefore entertain the idea that new flavour physics breaks baryon number at a much lower scale, but only in the coupling to a third generation quark, leading to observable baryon-number violating $b$-hadron decay rates.","In this paper we show that indirect constraints on the new physics scale $\\Lambda_{\\rm BNV}$ from the existing bounds on the proton lifetime do not allow for this possibility.","For this purpose we consider the three dominant proton decay channels $p \\to \\ell^+ \\nu_\\ell \\bar{\\nu}$, $p \\to \\pi^+ \\bar{\\nu}$ and $p \\to \\pi^0 \\ell^+$ mediated by a virtual bottom quark."],"url":"http://arxiv.org/abs/2404.09642v1","category":"hep-ph"}
{"created":"2024-04-15 10:20:51","title":"Multiple single-photon generations in three-level atoms coupled to cavity with non-Markovian effects","abstract":"In this paper, we show how to generate the multiple single-photon wavepackets of arbitrary temporal shape from an optical cavity coupled with $N$ three-level atoms driven by a driving field in the non-Markovian regime. We derive an exact analytical expression of the optimal driving field for generating such wavepackets, which depends on two detunings of the cavity and driving field with respect to the three-level atoms. The cavity we used consists of two mirrors facing each other, where one is perfect and the other exists the dissipation (one-sided cavity), which couples with the corresponding non-Markovian input-output fields. If the first single-photon wavepacket generated by the Markovian system is the same as the non-Markovian case, the Markovian system cannot generate the same multiple single-photon wavepackets as the non-Markovian one when the spectral widths of the other environments taking values different from the spectral width of the first environment, while setting the equal spectral widths for the different environments can generate this. The generated multiple different single-photon wavepackets are not independent of each other, which satisfy certain relations with non-Markovian spectral parameters. We analyse the transition from Markovian to non-Markovian regimes and compare the differences between them, where the cavity interacts simultaneously with the multiple non-Markovian environments. Finally, we extend the above results to a general non-Markovian quantum network involving many cavities coupled with driven three-level atoms.","sentences":["In this paper, we show how to generate the multiple single-photon wavepackets of arbitrary temporal shape from an optical cavity coupled with $N$ three-level atoms driven by a driving field in the non-Markovian regime.","We derive an exact analytical expression of the optimal driving field for generating such wavepackets, which depends on two detunings of the cavity and driving field with respect to the three-level atoms.","The cavity we used consists of two mirrors facing each other, where one is perfect and the other exists the dissipation (one-sided cavity), which couples with the corresponding non-Markovian input-output fields.","If the first single-photon wavepacket generated by the Markovian system is the same as the non-Markovian case, the Markovian system cannot generate the same multiple single-photon wavepackets as the non-Markovian one when the spectral widths of the other environments taking values different from the spectral width of the first environment, while setting the equal spectral widths for the different environments can generate this.","The generated multiple different single-photon wavepackets are not independent of each other, which satisfy certain relations with non-Markovian spectral parameters.","We analyse the transition from Markovian to non-Markovian regimes and compare the differences between them, where the cavity interacts simultaneously with the multiple non-Markovian environments.","Finally, we extend the above results to a general non-Markovian quantum network involving many cavities coupled with driven three-level atoms."],"url":"http://arxiv.org/abs/2404.09641v1","category":"quant-ph"}
{"created":"2024-04-15 10:13:05","title":"climber++: Pivot-Based Approximate Similarity Search over Big Data Series","abstract":"The generation and collection of big data series are becoming an integral part of many emerging applications in sciences, IoT, finance, and web applications among several others. The terabyte-scale of data series has motivated recent efforts to design fully distributed techniques for supporting operations such as approximate kNN similarity search, which is a building block operation in most analytics services on data series. Unfortunately, these techniques are heavily geared towards achieving scalability at the cost of sacrificing the results' accuracy. State-of-the-art systems report accuracy below 10% and 40%, respectively, which is not practical for many real-world applications. In this paper, we investigate the root problems in these existing techniques that limit their ability to achieve better a trade-off between scalability and accuracy. Then, we propose a framework, called CLIMBER, that encompasses a novel feature extraction mechanism, indexing scheme, and query processing algorithms for supporting approximate similarity search in big data series. For CLIMBER, we propose a new loss-resistant dual representation composed of rank-sensitive and ranking-insensitive signatures capturing data series objects. Based on this representation, we devise a distributed two-level index structure supported by an efficient data partitioning scheme. Our similarity metrics tailored for this dual representation enables meaningful comparison and distance evaluation between the rank-sensitive and ranking-insensitive signatures. Finally, we propose two efficient query processing algorithms, CLIMBER-kNN and CLIMBER-kNN-Adaptive, for answering approximate kNN similarity queries. Our experimental study on real-world and benchmark datasets demonstrates that CLIMBER, unlike existing techniques, features results' accuracy above 80% while retaining the desired scalability to terabytes of data.","sentences":["The generation and collection of big data series are becoming an integral part of many emerging applications in sciences, IoT, finance, and web applications among several others.","The terabyte-scale of data series has motivated recent efforts to design fully distributed techniques for supporting operations such as approximate kNN similarity search, which is a building block operation in most analytics services on data series.","Unfortunately, these techniques are heavily geared towards achieving scalability at the cost of sacrificing the results' accuracy.","State-of-the-art systems report accuracy below 10% and 40%, respectively, which is not practical for many real-world applications.","In this paper, we investigate the root problems in these existing techniques that limit their ability to achieve better a trade-off between scalability and accuracy.","Then, we propose a framework, called CLIMBER, that encompasses a novel feature extraction mechanism, indexing scheme, and query processing algorithms for supporting approximate similarity search in big data series.","For CLIMBER, we propose a new loss-resistant dual representation composed of rank-sensitive and ranking-insensitive signatures capturing data series objects.","Based on this representation, we devise a distributed two-level index structure supported by an efficient data partitioning scheme.","Our similarity metrics tailored for this dual representation enables meaningful comparison and distance evaluation between the rank-sensitive and ranking-insensitive signatures.","Finally, we propose two efficient query processing algorithms, CLIMBER-kNN and CLIMBER-kNN-Adaptive, for answering approximate kNN similarity queries.","Our experimental study on real-world and benchmark datasets demonstrates that CLIMBER, unlike existing techniques, features results' accuracy above 80% while retaining the desired scalability to terabytes of data."],"url":"http://arxiv.org/abs/2404.09637v1","category":"cs.DB"}
{"created":"2024-04-15 10:12:33","title":"All-in-one simulation-based inference","abstract":"Amortized Bayesian inference trains neural networks to solve stochastic inference problems using model simulations, thereby making it possible to rapidly perform Bayesian inference for any newly observed data. However, current simulation-based amortized inference methods are simulation-hungry and inflexible: They require the specification of a fixed parametric prior, simulator, and inference tasks ahead of time. Here, we present a new amortized inference method -- the Simformer -- which overcomes these limitations. By training a probabilistic diffusion model with transformer architectures, the Simformer outperforms current state-of-the-art amortized inference approaches on benchmark tasks and is substantially more flexible: It can be applied to models with function-valued parameters, it can handle inference scenarios with missing or unstructured data, and it can sample arbitrary conditionals of the joint distribution of parameters and data, including both posterior and likelihood. We showcase the performance and flexibility of the Simformer on simulators from ecology, epidemiology, and neuroscience, and demonstrate that it opens up new possibilities and application domains for amortized Bayesian inference on simulation-based models.","sentences":["Amortized Bayesian inference trains neural networks to solve stochastic inference problems using model simulations, thereby making it possible to rapidly perform Bayesian inference for any newly observed data.","However, current simulation-based amortized inference methods are simulation-hungry and inflexible: They require the specification of a fixed parametric prior, simulator, and inference tasks ahead of time.","Here, we present a new amortized inference method -- the Simformer -- which overcomes these limitations.","By training a probabilistic diffusion model with transformer architectures, the Simformer outperforms current state-of-the-art amortized inference approaches on benchmark tasks and is substantially more flexible: It can be applied to models with function-valued parameters, it can handle inference scenarios with missing or unstructured data, and it can sample arbitrary conditionals of the joint distribution of parameters and data, including both posterior and likelihood.","We showcase the performance and flexibility of the Simformer on simulators from ecology, epidemiology, and neuroscience, and demonstrate that it opens up new possibilities and application domains for amortized Bayesian inference on simulation-based models."],"url":"http://arxiv.org/abs/2404.09636v1","category":"cs.LG"}
{"created":"2024-04-15 10:10:24","title":"Pair statistics of anisotropic particles settling in a turbulent flow","abstract":"We perform direct numerical simulations of sub-Kolmogorov, inertial spheroids settling under gravity in homogeneous, isotropic turbulence and find that small-scale clustering, measured via the correlation dimension, depends sensitively on their aspect ratios. In particular, such particles are shown to cluster more as their anisotropy increases. Further, the approach rate for pairs of spheroids are calculated and found to deviate significantly from the spherical-particle limit. Our study, spanning a range of Stokes numbers and aspect ratios, provides critical inputs for developing collision models to understand the dynamics of sedimenting, anisotropic particles in general and ice crystals in clouds in particular.","sentences":["We perform direct numerical simulations of sub-Kolmogorov, inertial spheroids settling under gravity in homogeneous, isotropic turbulence and find that small-scale clustering, measured via the correlation dimension, depends sensitively on their aspect ratios.","In particular, such particles are shown to cluster more as their anisotropy increases.","Further, the approach rate for pairs of spheroids are calculated and found to deviate significantly from the spherical-particle limit.","Our study, spanning a range of Stokes numbers and aspect ratios, provides critical inputs for developing collision models to understand the dynamics of sedimenting, anisotropic particles in general and ice crystals in clouds in particular."],"url":"http://arxiv.org/abs/2404.09635v1","category":"physics.flu-dyn"}
{"created":"2024-04-15 10:07:33","title":"A Weitzenb\u00f6ck formula on Sasakian holomorphic bundles","abstract":"This work seeks to advance the understanding of the smooth structure of the moduli space of self-dual contact instantons (SDCI) on Sasakian 7-manifolds M. A neighborhood of a smooth point of M is locally modeled on the first cohomological group of an elliptic complex (1.4). There is a cohomological obstruction to the smoothness for the moduli space, in terms of a second basic cohomological group, in this paper we study conditions under which this obstruction disappears, by computing a Weitzenb\\\"ock formula and using a Bochner-type method to obtain a vanishing theorem. Given an SDCI on a Sasakian bundle E, we find sufficient conditions for the vanishing of the obstruction in the positivity of a couple of operators R and F depending on the curvatures of the connection and the Riemann curvature of the Sasakian metric g. In particular, we find that if M is transversely Ricci positive and F positive, the moduli space of SDCI must be smooth. However, in general, the operator F is not positive definite and we describe bundles over the Stiefel manifold for which it is the case. Finally, we show that when the energy of the curvature is less than the first non-zero eigenvalue of RicT the obstruction vanishes.","sentences":["This work seeks to advance the understanding of the smooth structure of the moduli space of self-dual contact instantons (SDCI) on Sasakian 7-manifolds M. A neighborhood of a smooth point of M is locally modeled on the first cohomological group of an elliptic complex (1.4).","There is a cohomological obstruction to the smoothness for the moduli space, in terms of a second basic cohomological group, in this paper we study conditions under which this obstruction disappears, by computing a Weitzenb\\\"ock formula and using a Bochner-type method to obtain a vanishing theorem.","Given an SDCI on a Sasakian bundle E, we find sufficient conditions for the vanishing of the obstruction in the positivity of a couple of operators R and F depending on the curvatures of the connection and the Riemann curvature of the Sasakian metric g.","In particular, we find that if M is transversely Ricci positive and F positive, the moduli space of SDCI must be smooth.","However, in general, the operator F is not positive definite and we describe bundles over the Stiefel manifold for which it is the case.","Finally, we show that when the energy of the curvature is less than the first non-zero eigenvalue of RicT the obstruction vanishes."],"url":"http://arxiv.org/abs/2404.09634v1","category":"math.DG"}
{"created":"2024-04-15 10:05:36","title":"In-Context Translation: Towards Unifying Image Recognition, Processing, and Generation","abstract":"We propose In-Context Translation (ICT), a general learning framework to unify visual recognition (e.g., semantic segmentation), low-level image processing (e.g., denoising), and conditional image generation (e.g., edge-to-image synthesis). Thanks to unification, ICT significantly reduces the inherent inductive bias that comes with designing models for specific tasks, and it maximizes mutual enhancement across similar tasks. However, the unification across a large number of tasks is non-trivial due to various data formats and training pipelines. To this end, ICT introduces two designs. Firstly, it standardizes input-output data of different tasks into RGB image pairs, e.g., semantic segmentation data pairs an RGB image with its segmentation mask in the same RGB format. This turns different tasks into a general translation task between two RGB images. Secondly, it standardizes the training of different tasks into a general in-context learning, where \"in-context\" means the input comprises an example input-output pair of the target task and a query image. The learning objective is to generate the \"missing\" data paired with the query. The implicit translation process is thus between the query and the generated image. In experiments, ICT unifies ten vision tasks and showcases impressive performance on their respective benchmarks. Notably, compared to its competitors, e.g., Painter and PromptDiffusion, ICT trained on only 4 RTX 3090 GPUs is shown to be more efficient and less costly in training.","sentences":["We propose In-Context Translation (ICT), a general learning framework to unify visual recognition (e.g., semantic segmentation), low-level image processing (e.g., denoising), and conditional image generation (e.g., edge-to-image synthesis).","Thanks to unification, ICT significantly reduces the inherent inductive bias that comes with designing models for specific tasks, and it maximizes mutual enhancement across similar tasks.","However, the unification across a large number of tasks is non-trivial due to various data formats and training pipelines.","To this end, ICT introduces two designs.","Firstly, it standardizes input-output data of different tasks into RGB image pairs, e.g., semantic segmentation data pairs an RGB image with its segmentation mask in the same RGB format.","This turns different tasks into a general translation task between two RGB images.","Secondly, it standardizes the training of different tasks into a general in-context learning, where \"in-context\" means the input comprises an example input-output pair of the target task and a query image.","The learning objective is to generate the \"missing\" data paired with the query.","The implicit translation process is thus between the query and the generated image.","In experiments, ICT unifies ten vision tasks and showcases impressive performance on their respective benchmarks.","Notably, compared to its competitors, e.g., Painter and PromptDiffusion, ICT trained on only 4 RTX 3090 GPUs is shown to be more efficient and less costly in training."],"url":"http://arxiv.org/abs/2404.09633v1","category":"cs.CV"}
{"created":"2024-04-15 10:04:15","title":"Bridging Vision and Language Spaces with Assignment Prediction","abstract":"This paper introduces VLAP, a novel approach that bridges pretrained vision models and large language models (LLMs) to make frozen LLMs understand the visual world. VLAP transforms the embedding space of pretrained vision models into the LLMs' word embedding space using a single linear layer for efficient and general-purpose visual and language understanding. Specifically, we harness well-established word embeddings to bridge two modality embedding spaces. The visual and text representations are simultaneously assigned to a set of word embeddings within pretrained LLMs by formulating the assigning procedure as an optimal transport problem. We predict the assignment of one modality from the representation of another modality data, enforcing consistent assignments for paired multimodal data. This allows vision and language representations to contain the same information, grounding the frozen LLMs' word embedding space in visual data. Moreover, a robust semantic taxonomy of LLMs can be preserved with visual data since the LLMs interpret and reason linguistic information from correlations between word embeddings. Experimental results show that VLAP achieves substantial improvements over the previous linear transformation-based approaches across a range of vision-language tasks, including image captioning, visual question answering, and cross-modal retrieval. We also demonstrate the learned visual representations hold a semantic taxonomy of LLMs, making visual semantic arithmetic possible.","sentences":["This paper introduces VLAP, a novel approach that bridges pretrained vision models and large language models (LLMs) to make frozen LLMs understand the visual world.","VLAP transforms the embedding space of pretrained vision models into the LLMs' word embedding space using a single linear layer for efficient and general-purpose visual and language understanding.","Specifically, we harness well-established word embeddings to bridge two modality embedding spaces.","The visual and text representations are simultaneously assigned to a set of word embeddings within pretrained LLMs by formulating the assigning procedure as an optimal transport problem.","We predict the assignment of one modality from the representation of another modality data, enforcing consistent assignments for paired multimodal data.","This allows vision and language representations to contain the same information, grounding the frozen LLMs' word embedding space in visual data.","Moreover, a robust semantic taxonomy of LLMs can be preserved with visual data since the LLMs interpret and reason linguistic information from correlations between word embeddings.","Experimental results show that VLAP achieves substantial improvements over the previous linear transformation-based approaches across a range of vision-language tasks, including image captioning, visual question answering, and cross-modal retrieval.","We also demonstrate the learned visual representations hold a semantic taxonomy of LLMs, making visual semantic arithmetic possible."],"url":"http://arxiv.org/abs/2404.09632v1","category":"cs.CV"}
{"created":"2024-04-15 10:01:43","title":"Action Model Learning with Guarantees","abstract":"This paper studies the problem of action model learning with full observability. Following the learning by search paradigm by Mitchell, we develop a theory for action model learning based on version spaces that interprets the task as search for hypothesis that are consistent with the learning examples. Our theoretical findings are instantiated in an online algorithm that maintains a compact representation of all solutions of the problem. Among these range of solutions, we bring attention to actions models approximating the actual transition system from below (sound models) and from above (complete models). We show how to manipulate the output of our learning algorithm to build deterministic and non-deterministic formulations of the sound and complete models and prove that, given enough examples, both formulations converge into the very same true model. Our experiments reveal their usefulness over a range of planning domains.","sentences":["This paper studies the problem of action model learning with full observability.","Following the learning by search paradigm by Mitchell, we develop a theory for action model learning based on version spaces that interprets the task as search for hypothesis that are consistent with the learning examples.","Our theoretical findings are instantiated in an online algorithm that maintains a compact representation of all solutions of the problem.","Among these range of solutions, we bring attention to actions models approximating the actual transition system from below (sound models) and from above (complete models).","We show how to manipulate the output of our learning algorithm to build deterministic and non-deterministic formulations of the sound and complete models and prove that, given enough examples, both formulations converge into the very same true model.","Our experiments reveal their usefulness over a range of planning domains."],"url":"http://arxiv.org/abs/2404.09631v1","category":"cs.AI"}
{"created":"2024-04-15 10:00:35","title":"Quantifying fair income distribution in Thailand","abstract":"Given a vast concern about high income inequality in Thailand as opposed to empirical findings around the world showing people's preference for fair income inequality over unfair income equality, it is therefore important to examine whether inequality in income distribution in Thailand over the past three decades is fair, and what fair inequality in income distribution in Thailand should be. To quantitatively measure fair income distribution, this study employs the fairness benchmarks that are derived from the distributions of athletes' salaries in professional sports which satisfy the concepts of distributive justice and procedural justice, the no-envy principle of fair allocation, and the general consensus or the international norm criterion of a meaningful benchmark. By using the data on quintile income shares and the income Gini index of Thailand from the National Social and Economic Development Council, this study finds that, throughout the period from 1988 to 2021, the Thai income earners in the bottom 20%, the second 20%, and the top 20% receive income shares more than the fair shares whereas those in the third 20% and the fourth 20% receive income shares less than the fair shares. Provided that there are infinite combinations of quintile income shares that can have the same value of income Gini index but only one of them is regarded as fair, this study demonstrates the use of fairness benchmarks as a practical guideline for designing policies with an aim to achieve fair income distribution in Thailand. Moreover, a comparative analysis is conducted by employing the method for estimating optimal (fair) income distribution representing feasible income equality in order to provide an alternative recommendation on what optimal (fair) income distribution characterizing feasible income equality in Thailand should be.","sentences":["Given a vast concern about high income inequality in Thailand as opposed to empirical findings around the world showing people's preference for fair income inequality over unfair income equality, it is therefore important to examine whether inequality in income distribution in Thailand over the past three decades is fair, and what fair inequality in income distribution in Thailand should be.","To quantitatively measure fair income distribution, this study employs the fairness benchmarks that are derived from the distributions of athletes' salaries in professional sports which satisfy the concepts of distributive justice and procedural justice, the no-envy principle of fair allocation, and the general consensus or the international norm criterion of a meaningful benchmark.","By using the data on quintile income shares and the income Gini index of Thailand from the National Social and Economic Development Council, this study finds that, throughout the period from 1988 to 2021, the Thai income earners in the bottom 20%, the second 20%, and the top 20% receive income shares more than the fair shares whereas those in the third 20% and the fourth 20% receive income shares less than the fair shares.","Provided that there are infinite combinations of quintile income shares that can have the same value of income Gini index but only one of them is regarded as fair, this study demonstrates the use of fairness benchmarks as a practical guideline for designing policies with an aim to achieve fair income distribution in Thailand.","Moreover, a comparative analysis is conducted by employing the method for estimating optimal (fair) income distribution representing feasible income equality in order to provide an alternative recommendation on what optimal (fair) income distribution characterizing feasible income equality in Thailand should be."],"url":"http://arxiv.org/abs/2404.09629v1","category":"econ.GN"}
{"created":"2024-04-15 09:59:55","title":"Coerciveness and Morrey Inequalities for Elliptic Operators with Natural Boundary Conditions via Weitzenb\u00f6ck Identities","abstract":"We prove a Weitzenb\\\"ock identity for general pairs of constant coefficient homogeneous first order partial differential operators, and deduce from it sufficient algebraic conditions for coerciveness and Morrey estimates under the natural 1/2 boundary conditions. Our proof of the $W^{1,2}$ elliptic estimate relies on the Aronszajn-Necas-Smith coercive estimate. For generalized strongly pseudoconvex domains, we improve the Morrey estimate to a weighted $W^{1,2}$ square function estimate, using a generalized Cauchy-Pompieu reproducing formula and the T1 theorem for singular integrals. We use Van Schaftingen's notion of cocanceling to study the generalized Levi forms appearing.","sentences":["We prove a Weitzenb\\\"ock identity for general pairs of constant coefficient homogeneous first order partial differential operators, and deduce from it sufficient algebraic conditions for coerciveness and Morrey estimates under the natural 1/2 boundary conditions.","Our proof of the $W^{1,2}$ elliptic estimate relies on the Aronszajn-Necas-Smith coercive estimate.","For generalized strongly pseudoconvex domains, we improve the Morrey estimate to a weighted $W^{1,2}$ square function estimate, using a generalized Cauchy-Pompieu reproducing formula and the T1 theorem for singular integrals.","We use Van Schaftingen's notion of cocanceling to study the generalized Levi forms appearing."],"url":"http://arxiv.org/abs/2404.09628v1","category":"math.AP"}
{"created":"2024-04-15 09:56:36","title":"Privacy-Preserving Intrusion Detection using Convolutional Neural Networks","abstract":"Privacy-preserving analytics is designed to protect valuable assets. A common service provision involves the input data from the client and the model on the analyst's side. The importance of the privacy preservation is fuelled by legal obligations and intellectual property concerns. We explore the use case of a model owner providing an analytic service on customer's private data. No information about the data shall be revealed to the analyst and no information about the model shall be leaked to the customer. Current methods involve costs: accuracy deterioration and computational complexity. The complexity, in turn, results in a longer processing time, increased requirement on computing resources, and involves data communication between the client and the server. In order to deploy such service architecture, we need to evaluate the optimal setting that fits the constraints. And that is what this paper addresses. In this work, we enhance an attack detection system based on Convolutional Neural Networks with privacy-preserving technology based on PriMIA framework that is initially designed for medical data.","sentences":["Privacy-preserving analytics is designed to protect valuable assets.","A common service provision involves the input data from the client and the model on the analyst's side.","The importance of the privacy preservation is fuelled by legal obligations and intellectual property concerns.","We explore the use case of a model owner providing an analytic service on customer's private data.","No information about the data shall be revealed to the analyst and no information about the model shall be leaked to the customer.","Current methods involve costs: accuracy deterioration and computational complexity.","The complexity, in turn, results in a longer processing time, increased requirement on computing resources, and involves data communication between the client and the server.","In order to deploy such service architecture, we need to evaluate the optimal setting that fits the constraints.","And that is what this paper addresses.","In this work, we enhance an attack detection system based on Convolutional Neural Networks with privacy-preserving technology based on PriMIA framework that is initially designed for medical data."],"url":"http://arxiv.org/abs/2404.09625v1","category":"cs.CR"}
{"created":"2024-04-15 09:56:20","title":"AesExpert: Towards Multi-modality Foundation Model for Image Aesthetics Perception","abstract":"The highly abstract nature of image aesthetics perception (IAP) poses significant challenge for current multimodal large language models (MLLMs). The lack of human-annotated multi-modality aesthetic data further exacerbates this dilemma, resulting in MLLMs falling short of aesthetics perception capabilities. To address the above challenge, we first introduce a comprehensively annotated Aesthetic Multi-Modality Instruction Tuning (AesMMIT) dataset, which serves as the footstone for building multi-modality aesthetics foundation models. Specifically, to align MLLMs with human aesthetics perception, we construct a corpus-rich aesthetic critique database with 21,904 diverse-sourced images and 88K human natural language feedbacks, which are collected via progressive questions, ranging from coarse-grained aesthetic grades to fine-grained aesthetic descriptions. To ensure that MLLMs can handle diverse queries, we further prompt GPT to refine the aesthetic critiques and assemble the large-scale aesthetic instruction tuning dataset, i.e. AesMMIT, which consists of 409K multi-typed instructions to activate stronger aesthetic capabilities. Based on the AesMMIT database, we fine-tune the open-sourced general foundation models, achieving multi-modality Aesthetic Expert models, dubbed AesExpert. Extensive experiments demonstrate that the proposed AesExpert models deliver significantly better aesthetic perception performances than the state-of-the-art MLLMs, including the most advanced GPT-4V and Gemini-Pro-Vision. Source data will be available at https://github.com/yipoh/AesExpert.","sentences":["The highly abstract nature of image aesthetics perception (IAP) poses significant challenge for current multimodal large language models (MLLMs).","The lack of human-annotated multi-modality aesthetic data further exacerbates this dilemma, resulting in MLLMs falling short of aesthetics perception capabilities.","To address the above challenge, we first introduce a comprehensively annotated Aesthetic Multi-Modality Instruction Tuning (AesMMIT) dataset, which serves as the footstone for building multi-modality aesthetics foundation models.","Specifically, to align MLLMs with human aesthetics perception, we construct a corpus-rich aesthetic critique database with 21,904 diverse-sourced images and 88K human natural language feedbacks, which are collected via progressive questions, ranging from coarse-grained aesthetic grades to fine-grained aesthetic descriptions.","To ensure that MLLMs can handle diverse queries, we further prompt GPT to refine the aesthetic critiques and assemble the large-scale aesthetic instruction tuning dataset, i.e. AesMMIT, which consists of 409K multi-typed instructions to activate stronger aesthetic capabilities.","Based on the AesMMIT database, we fine-tune the open-sourced general foundation models, achieving multi-modality Aesthetic Expert models, dubbed AesExpert.","Extensive experiments demonstrate that the proposed AesExpert models deliver significantly better aesthetic perception performances than the state-of-the-art MLLMs, including the most advanced GPT-4V and Gemini-Pro-Vision.","Source data will be available at https://github.com/yipoh/AesExpert."],"url":"http://arxiv.org/abs/2404.09624v1","category":"cs.CV"}
{"created":"2024-04-15 09:53:47","title":"Skew two-sided bracoids","abstract":"Isabel Martin-Lyons and Paul J.Truman generalized the definition of a skew brace to give a new algebraic object, which they termed a skew bracoid. Their construction involves two groups interacting in a manner analogous to the compatibility condition found in the definition of a skew brace. They formulated tools for characterizing and classifying skew bracoids, and studied substructures and quotients of skew bracoids. In this paper we study two-sided bracoids. In \\cite{WR07} Rump showed that if a left brace $(B, \\star ,\\cdot )$ is a two-sided brace and the operation $\\ast : B \\times B \\longrightarrow B$ is defined by $a \\ast b = a\\cdot b \\star \\overline{a} \\star \\overline{b}$ for all $a, b \\in B$ then $(B, \\star ,\\ast )$ is a Jacobson radical ring. Lau showed that if $(B, \\star ,\\cdot )$ is a left brace and the operation is asssociative, then $B$ is a two-sided brace. We will prove bracoid versions of this results.","sentences":["Isabel Martin-Lyons and Paul J.Truman generalized the definition of a skew brace to give a new algebraic object, which they termed a skew bracoid.","Their construction involves two groups interacting in a manner analogous to the compatibility condition found in the definition of a skew brace.","They formulated tools for characterizing and classifying skew bracoids, and studied substructures and quotients of skew bracoids.","In this paper we study two-sided bracoids.","In \\cite{WR07} Rump showed that if a left brace $(B, \\star ,\\cdot )$ is a two-sided brace and the operation $\\ast : B \\times B \\longrightarrow B$ is defined by $a \\ast b = a\\cdot b \\star \\overline{a} \\star \\overline{b}$ for all $a, b \\in B$ then $(B, \\star ,\\ast )$ is a Jacobson radical ring.","Lau showed that if $(B, \\star ,\\cdot )$ is a left brace and the operation is asssociative, then $B$ is a two-sided brace.","We will prove bracoid versions of this results."],"url":"http://arxiv.org/abs/2404.09623v1","category":"math.RA"}
{"created":"2024-04-15 09:49:33","title":"DIDLM:A Comprehensive Multi-Sensor Dataset with Infrared Cameras, Depth Cameras, LiDAR, and 4D Millimeter-Wave Radar in Challenging Scenarios for 3D Mapping","abstract":"This study presents a comprehensive multi-sensor dataset designed for 3D mapping in challenging indoor and outdoor environments. The dataset comprises data from infrared cameras, depth cameras, LiDAR, and 4D millimeter-wave radar, facilitating exploration of advanced perception and mapping techniques. Integration of diverse sensor data enhances perceptual capabilities in extreme conditions such as rain, snow, and uneven road surfaces. The dataset also includes interactive robot data at different speeds indoors and outdoors, providing a realistic background environment. Slam comparisons between similar routes are conducted, analyzing the influence of different complex scenes on various sensors. Various SLAM algorithms are employed to process the dataset, revealing performance differences among algorithms in different scenarios. In summary, this dataset addresses the problem of data scarcity in special environments, fostering the development of perception and mapping algorithms for extreme conditions. Leveraging multi-sensor data including infrared, depth cameras, LiDAR, 4D millimeter-wave radar, and robot interactions, the dataset advances intelligent mapping and perception capabilities.Our dataset is available at https://github.com/GongWeiSheng/DIDLM.","sentences":["This study presents a comprehensive multi-sensor dataset designed for 3D mapping in challenging indoor and outdoor environments.","The dataset comprises data from infrared cameras, depth cameras, LiDAR, and 4D millimeter-wave radar, facilitating exploration of advanced perception and mapping techniques.","Integration of diverse sensor data enhances perceptual capabilities in extreme conditions such as rain, snow, and uneven road surfaces.","The dataset also includes interactive robot data at different speeds indoors and outdoors, providing a realistic background environment.","Slam comparisons between similar routes are conducted, analyzing the influence of different complex scenes on various sensors.","Various SLAM algorithms are employed to process the dataset, revealing performance differences among algorithms in different scenarios.","In summary, this dataset addresses the problem of data scarcity in special environments, fostering the development of perception and mapping algorithms for extreme conditions.","Leveraging multi-sensor data including infrared, depth cameras, LiDAR, 4D millimeter-wave radar, and robot interactions, the dataset advances intelligent mapping and perception capabilities.","Our dataset is available at https://github.com/GongWeiSheng/DIDLM."],"url":"http://arxiv.org/abs/2404.09622v1","category":"cs.RO"}
{"created":"2024-04-15 09:47:48","title":"UNIAA: A Unified Multi-modal Image Aesthetic Assessment Baseline and Benchmark","abstract":"As an alternative to expensive expert evaluation, Image Aesthetic Assessment (IAA) stands out as a crucial task in computer vision. However, traditional IAA methods are typically constrained to a single data source or task, restricting the universality and broader application. In this work, to better align with human aesthetics, we propose a Unified Multi-modal Image Aesthetic Assessment (UNIAA) framework, including a Multi-modal Large Language Model (MLLM) named UNIAA-LLaVA and a comprehensive benchmark named UNIAA-Bench. We choose MLLMs with both visual perception and language ability for IAA and establish a low-cost paradigm for transforming the existing datasets into unified and high-quality visual instruction tuning data, from which the UNIAA-LLaVA is trained. To further evaluate the IAA capability of MLLMs, we construct the UNIAA-Bench, which consists of three aesthetic levels: Perception, Description, and Assessment. Extensive experiments validate the effectiveness and rationality of UNIAA. UNIAA-LLaVA achieves competitive performance on all levels of UNIAA-Bench, compared with existing MLLMs. Specifically, our model performs better than GPT-4V in aesthetic perception and even approaches the junior-level human. We find MLLMs have great potential in IAA, yet there remains plenty of room for further improvement. The UNIAA-LLaVA and UNIAA-Bench will be released.","sentences":["As an alternative to expensive expert evaluation, Image Aesthetic Assessment (IAA) stands out as a crucial task in computer vision.","However, traditional IAA methods are typically constrained to a single data source or task, restricting the universality and broader application.","In this work, to better align with human aesthetics, we propose a Unified Multi-modal Image Aesthetic Assessment (UNIAA) framework, including a Multi-modal Large Language Model (MLLM) named UNIAA-LLaVA and a comprehensive benchmark named UNIAA-Bench.","We choose MLLMs with both visual perception and language ability for IAA and establish a low-cost paradigm for transforming the existing datasets into unified and high-quality visual instruction tuning data, from which the UNIAA-LLaVA is trained.","To further evaluate the IAA capability of MLLMs, we construct the UNIAA-Bench, which consists of three aesthetic levels: Perception, Description, and Assessment.","Extensive experiments validate the effectiveness and rationality of UNIAA.","UNIAA-LLaVA achieves competitive performance on all levels of UNIAA-Bench, compared with existing MLLMs.","Specifically, our model performs better than GPT-4V in aesthetic perception and even approaches the junior-level human.","We find MLLMs have great potential in IAA, yet there remains plenty of room for further improvement.","The UNIAA-LLaVA and UNIAA-Bench will be released."],"url":"http://arxiv.org/abs/2404.09619v1","category":"cs.CV"}
{"created":"2024-04-15 09:40:44","title":"A Review and Efficient Implementation of Scene Graph Generation Metrics","abstract":"Scene graph generation has emerged as a prominent research field in computer vision, witnessing significant advancements in the recent years. However, despite these strides, precise and thorough definitions for the metrics used to evaluate scene graph generation models are lacking. In this paper, we address this gap in the literature by providing a review and precise definition of commonly used metrics in scene graph generation. Our comprehensive examination clarifies the underlying principles of these metrics and can serve as a reference or introduction to scene graph metrics.   Furthermore, to facilitate the usage of these metrics, we introduce a standalone Python package called SGBench that efficiently implements all defined metrics, ensuring their accessibility to the research community. Additionally, we present a scene graph benchmarking web service, that enables researchers to compare scene graph generation methods and increase visibility of new methods in a central place.   All of our code can be found at https://lorjul.github.io/sgbench/.","sentences":["Scene graph generation has emerged as a prominent research field in computer vision, witnessing significant advancements in the recent years.","However, despite these strides, precise and thorough definitions for the metrics used to evaluate scene graph generation models are lacking.","In this paper, we address this gap in the literature by providing a review and precise definition of commonly used metrics in scene graph generation.","Our comprehensive examination clarifies the underlying principles of these metrics and can serve as a reference or introduction to scene graph metrics.   ","Furthermore, to facilitate the usage of these metrics, we introduce a standalone Python package called SGBench that efficiently implements all defined metrics, ensuring their accessibility to the research community.","Additionally, we present a scene graph benchmarking web service, that enables researchers to compare scene graph generation methods and increase visibility of new methods in a central place.   ","All of our code can be found at https://lorjul.github.io/sgbench/."],"url":"http://arxiv.org/abs/2404.09616v1","category":"cs.CV"}
{"created":"2024-04-15 09:34:14","title":"Distinguishing subsampled power laws from other heavy-tailed distributions","abstract":"Distinguishing power-law distributions from other heavy-tailed distributions is challenging, and this task is often further complicated by subsampling effects. In this work, we evaluate the performance of two commonly used methods for detecting power-law distributions - the maximum likelihood method of Clauset et al. and the extreme value method of Voitalov et al. - in distinguishing subsampled power laws from two other heavy-tailed distributions, the lognormal and the stretched exponential distributions. We focus on a random subsampling method commonly applied in network science and biological sciences. In this subsampling scheme, we are ultimately interested in the frequency distribution of elements with a certain number of constituent parts, and each part is selected to the subsample with an equal probability. We investigate how well the results obtained from subsamples generalize to the original distribution. Our results show that the power-law exponent of the original distribution can be estimated fairly accurately from subsamples, but classifying the distribution correctly is more challenging. The maximum likelihood method falsely rejects the power-law hypothesis for a large fraction of subsamples from power-law distributions. While the extreme value method correctly recognizes subsampled power-law distributions with all tested subsampling depths, its capacity to distinguish power laws from the heavy-tailed alternatives is limited. However, these false positives tend to result not from the subsampling itself but from the estimators' inability to classify the original sample correctly. In fact, we show that the extreme value method can sometimes be expected to perform better on subsamples than on the original samples from the lognormal and the stretched exponential distributions, while the contrary is true for the main tests included in the maximum likelihood method.","sentences":["Distinguishing power-law distributions from other heavy-tailed distributions is challenging, and this task is often further complicated by subsampling effects.","In this work, we evaluate the performance of two commonly used methods for detecting power-law distributions - the maximum likelihood method of Clauset et al. and the extreme value method of Voitalov et al. - in distinguishing subsampled power laws from two other heavy-tailed distributions, the lognormal and the stretched exponential distributions.","We focus on a random subsampling method commonly applied in network science and biological sciences.","In this subsampling scheme, we are ultimately interested in the frequency distribution of elements with a certain number of constituent parts, and each part is selected to the subsample with an equal probability.","We investigate how well the results obtained from subsamples generalize to the original distribution.","Our results show that the power-law exponent of the original distribution can be estimated fairly accurately from subsamples, but classifying the distribution correctly is more challenging.","The maximum likelihood method falsely rejects the power-law hypothesis for a large fraction of subsamples from power-law distributions.","While the extreme value method correctly recognizes subsampled power-law distributions with all tested subsampling depths, its capacity to distinguish power laws from the heavy-tailed alternatives is limited.","However, these false positives tend to result not from the subsampling itself but from the estimators' inability to classify the original sample correctly.","In fact, we show that the extreme value method can sometimes be expected to perform better on subsamples than on the original samples from the lognormal and the stretched exponential distributions, while the contrary is true for the main tests included in the maximum likelihood method."],"url":"http://arxiv.org/abs/2404.09614v1","category":"physics.data-an"}
{"created":"2024-04-15 09:33:09","title":"Efficient and accurate neural field reconstruction using resistive memory","abstract":"Human beings construct perception of space by integrating sparse observations into massively interconnected synapses and neurons, offering a superior parallelism and efficiency. Replicating this capability in AI finds wide applications in medical imaging, AR/VR, and embodied AI, where input data is often sparse and computing resources are limited. However, traditional signal reconstruction methods on digital computers face both software and hardware challenges. On the software front, difficulties arise from storage inefficiencies in conventional explicit signal representation. Hardware obstacles include the von Neumann bottleneck, which limits data transfer between the CPU and memory, and the limitations of CMOS circuits in supporting parallel processing. We propose a systematic approach with software-hardware co-optimizations for signal reconstruction from sparse inputs. Software-wise, we employ neural field to implicitly represent signals via neural networks, which is further compressed using low-rank decomposition and structured pruning. Hardware-wise, we design a resistive memory-based computing-in-memory (CIM) platform, featuring a Gaussian Encoder (GE) and an MLP Processing Engine (PE). The GE harnesses the intrinsic stochasticity of resistive memory for efficient input encoding, while the PE achieves precise weight mapping through a Hardware-Aware Quantization (HAQ) circuit. We demonstrate the system's efficacy on a 40nm 256Kb resistive memory-based in-memory computing macro, achieving huge energy efficiency and parallelism improvements without compromising reconstruction quality in tasks like 3D CT sparse reconstruction, novel view synthesis, and novel view synthesis for dynamic scenes. This work advances the AI-driven signal restoration technology and paves the way for future efficient and robust medical AI and 3D vision applications.","sentences":["Human beings construct perception of space by integrating sparse observations into massively interconnected synapses and neurons, offering a superior parallelism and efficiency.","Replicating this capability in AI finds wide applications in medical imaging, AR/VR, and embodied AI, where input data is often sparse and computing resources are limited.","However, traditional signal reconstruction methods on digital computers face both software and hardware challenges.","On the software front, difficulties arise from storage inefficiencies in conventional explicit signal representation.","Hardware obstacles include the von Neumann bottleneck, which limits data transfer between the CPU and memory, and the limitations of CMOS circuits in supporting parallel processing.","We propose a systematic approach with software-hardware co-optimizations for signal reconstruction from sparse inputs.","Software-wise, we employ neural field to implicitly represent signals via neural networks, which is further compressed using low-rank decomposition and structured pruning.","Hardware-wise, we design a resistive memory-based computing-in-memory (CIM) platform, featuring a Gaussian Encoder (GE) and an MLP Processing Engine (PE).","The GE harnesses the intrinsic stochasticity of resistive memory for efficient input encoding, while the PE achieves precise weight mapping through a Hardware-Aware Quantization (HAQ) circuit.","We demonstrate the system's efficacy on a 40nm 256Kb resistive memory-based in-memory computing macro, achieving huge energy efficiency and parallelism improvements without compromising reconstruction quality in tasks like 3D CT sparse reconstruction, novel view synthesis, and novel view synthesis for dynamic scenes.","This work advances the AI-driven signal restoration technology and paves the way for future efficient and robust medical AI and 3D vision applications."],"url":"http://arxiv.org/abs/2404.09613v1","category":"cs.ET"}
{"created":"2024-04-15 09:33:06","title":"A new class of separable Lagrangian systems generalizing Sawada-Kotera","abstract":"Some characteristics of the Sawada-Kotera Lagrangian system lend themselves to generalization, producing a large class of separable Lagrangian systems with two degrees of freedom.","sentences":["Some characteristics of the Sawada-Kotera Lagrangian system lend themselves to generalization, producing a large class of separable Lagrangian systems with two degrees of freedom."],"url":"http://arxiv.org/abs/2404.09612v1","category":"math.DS"}
{"created":"2024-04-15 09:32:12","title":"LoRA Dropout as a Sparsity Regularizer for Overfitting Control","abstract":"Parameter-efficient fine-tuning methods, represented by LoRA, play an essential role in adapting large-scale pre-trained models to downstream tasks. However, fine-tuning LoRA-series models also faces the risk of overfitting on the training dataset, and yet there's still a lack of theoretical guidance and practical mechanism to control overfitting on LoRA-based PEFT methods. In this paper, we propose a LoRA Dropout mechanism for the LoRA-based methods by introducing random noises to the learnable low-rank matrices and increasing parameter sparsity. We then demonstrate the theoretical mechanism of our LoRA Dropout mechanism from the perspective of sparsity regularization by providing a generalization error bound under this framework. Theoretical results show that appropriate sparsity would help tighten the gap between empirical and generalization risks and thereby control overfitting. Furthermore, based on the LoRA Dropout framework, we introduce a test-time ensemble strategy and provide theoretical evidence demonstrating that the ensemble method can further compress the error bound, and lead to better performance during inference time. Extensive experiments on various NLP tasks provide practical validations of the effectiveness of our LoRA Dropout framework in improving model accuracy and calibration.","sentences":["Parameter-efficient fine-tuning methods, represented by LoRA, play an essential role in adapting large-scale pre-trained models to downstream tasks.","However, fine-tuning LoRA-series models also faces the risk of overfitting on the training dataset, and yet there's still a lack of theoretical guidance and practical mechanism to control overfitting on LoRA-based PEFT methods.","In this paper, we propose a LoRA Dropout mechanism for the LoRA-based methods by introducing random noises to the learnable low-rank matrices and increasing parameter sparsity.","We then demonstrate the theoretical mechanism of our LoRA Dropout mechanism from the perspective of sparsity regularization by providing a generalization error bound under this framework.","Theoretical results show that appropriate sparsity would help tighten the gap between empirical and generalization risks and thereby control overfitting.","Furthermore, based on the LoRA Dropout framework, we introduce a test-time ensemble strategy and provide theoretical evidence demonstrating that the ensemble method can further compress the error bound, and lead to better performance during inference time.","Extensive experiments on various NLP tasks provide practical validations of the effectiveness of our LoRA Dropout framework in improving model accuracy and calibration."],"url":"http://arxiv.org/abs/2404.09610v1","category":"cs.LG"}
{"created":"2024-04-15 09:29:58","title":"Quantum Principle of Least Action in Dynamic Theories With Higher Derivatives","abstract":"A generalized canonical form of action of dynamic theories with higher derivatives is proposed, which does not require the introduction of additional dynamic variables. This form is the initial point for the construction of quantum theory, in which the state of motion of the system is described by the wave functional on its trajectories in the configuration space, and the functional itself is an eigenvector of the action operator. The Pais-Uhlenbeck oscillator is considered as the simplest model. For this case, the correspondence between the new form of quantum theory and \"ordinary\" quantum mechanics has been established in the local limit, when the Pais-Uhlenbeck oscillator is reduced to the \"ordinary\" harmonic oscillator.","sentences":["A generalized canonical form of action of dynamic theories with higher derivatives is proposed, which does not require the introduction of additional dynamic variables.","This form is the initial point for the construction of quantum theory, in which the state of motion of the system is described by the wave functional on its trajectories in the configuration space, and the functional itself is an eigenvector of the action operator.","The Pais-Uhlenbeck oscillator is considered as the simplest model.","For this case, the correspondence between the new form of quantum theory and \"ordinary\" quantum mechanics has been established in the local limit, when the Pais-Uhlenbeck oscillator is reduced to the \"ordinary\" harmonic oscillator."],"url":"http://arxiv.org/abs/2404.09608v1","category":"quant-ph"}
{"created":"2024-04-15 09:26:33","title":"A Self-feedback Knowledge Elicitation Approach for Chemical Reaction Predictions","abstract":"The task of chemical reaction predictions (CRPs) plays a pivotal role in advancing drug discovery and material science. However, its effectiveness is constrained by the vast and uncertain chemical reaction space and challenges in capturing reaction selectivity, particularly due to existing methods' limitations in exploiting the data's inherent knowledge. To address these challenges, we introduce a data-curated self-feedback knowledge elicitation approach. This method starts from iterative optimization of molecular representations and facilitates the extraction of knowledge on chemical reaction types (RTs). Then, we employ adaptive prompt learning to infuse the prior knowledge into the large language model (LLM). As a result, we achieve significant enhancements: a 14.2% increase in retrosynthesis prediction accuracy, a 74.2% rise in reagent prediction accuracy, and an expansion in the model's capability for handling multi-task chemical reactions. This research offers a novel paradigm for knowledge elicitation in scientific research and showcases the untapped potential of LLMs in CRPs.","sentences":["The task of chemical reaction predictions (CRPs) plays a pivotal role in advancing drug discovery and material science.","However, its effectiveness is constrained by the vast and uncertain chemical reaction space and challenges in capturing reaction selectivity, particularly due to existing methods' limitations in exploiting the data's inherent knowledge.","To address these challenges, we introduce a data-curated self-feedback knowledge elicitation approach.","This method starts from iterative optimization of molecular representations and facilitates the extraction of knowledge on chemical reaction types (RTs).","Then, we employ adaptive prompt learning to infuse the prior knowledge into the large language model (LLM).","As a result, we achieve significant enhancements: a 14.2% increase in retrosynthesis prediction accuracy, a 74.2% rise in reagent prediction accuracy, and an expansion in the model's capability for handling multi-task chemical reactions.","This research offers a novel paradigm for knowledge elicitation in scientific research and showcases the untapped potential of LLMs in CRPs."],"url":"http://arxiv.org/abs/2404.09606v1","category":"cs.LG"}
{"created":"2024-04-15 09:16:49","title":"Reactive Model Correction: Mitigating Harm to Task-Relevant Features via Conditional Bias Suppression","abstract":"Deep Neural Networks are prone to learning and relying on spurious correlations in the training data, which, for high-risk applications, can have fatal consequences. Various approaches to suppress model reliance on harmful features have been proposed that can be applied post-hoc without additional training. Whereas those methods can be applied with efficiency, they also tend to harm model performance by globally shifting the distribution of latent features. To mitigate unintended overcorrection of model behavior, we propose a reactive approach conditioned on model-derived knowledge and eXplainable Artificial Intelligence (XAI) insights. While the reactive approach can be applied to many post-hoc methods, we demonstrate the incorporation of reactivity in particular for P-ClArC (Projective Class Artifact Compensation), introducing a new method called R-ClArC (Reactive Class Artifact Compensation). Through rigorous experiments in controlled settings (FunnyBirds) and with a real-world dataset (ISIC2019), we show that introducing reactivity can minimize the detrimental effect of the applied correction while simultaneously ensuring low reliance on spurious features.","sentences":["Deep Neural Networks are prone to learning and relying on spurious correlations in the training data, which, for high-risk applications, can have fatal consequences.","Various approaches to suppress model reliance on harmful features have been proposed that can be applied post-hoc without additional training.","Whereas those methods can be applied with efficiency, they also tend to harm model performance by globally shifting the distribution of latent features.","To mitigate unintended overcorrection of model behavior, we propose a reactive approach conditioned on model-derived knowledge and eXplainable Artificial Intelligence (XAI) insights.","While the reactive approach can be applied to many post-hoc methods, we demonstrate the incorporation of reactivity in particular for P-ClArC (Projective Class Artifact Compensation), introducing a new method called R-ClArC (Reactive Class Artifact Compensation).","Through rigorous experiments in controlled settings (FunnyBirds) and with a real-world dataset (ISIC2019), we show that introducing reactivity can minimize the detrimental effect of the applied correction while simultaneously ensuring low reliance on spurious features."],"url":"http://arxiv.org/abs/2404.09601v1","category":"cs.LG"}
{"created":"2024-04-15 09:10:52","title":"Enhancing Code Vulnerability Detection via Vulnerability-Preserving Data Augmentation","abstract":"Source code vulnerability detection aims to identify inherent vulnerabilities to safeguard software systems from potential attacks. Many prior studies overlook diverse vulnerability characteristics, simplifying the problem into a binary (0-1) classification task for example determining whether it is vulnerable or not. This poses a challenge for a single deep learning-based model to effectively learn the wide array of vulnerability characteristics. Furthermore, due to the challenges associated with collecting large-scale vulnerability data, these detectors often overfit limited training datasets, resulting in lower model generalization performance.   To address the aforementioned challenges, in this work, we introduce a fine-grained vulnerability detector namely FGVulDet. Unlike previous approaches, FGVulDet employs multiple classifiers to discern characteristics of various vulnerability types and combines their outputs to identify the specific type of vulnerability. Each classifier is designed to learn type-specific vulnerability semantics. Additionally, to address the scarcity of data for some vulnerability types and enhance data diversity for learning better vulnerability semantics, we propose a novel vulnerability-preserving data augmentation technique to augment the number of vulnerabilities. Taking inspiration from recent advancements in graph neural networks for learning program semantics, we incorporate a Gated Graph Neural Network (GGNN) and extend it to an edge-aware GGNN to capture edge-type information. FGVulDet is trained on a large-scale dataset from GitHub, encompassing five different types of vulnerabilities. Extensive experiments compared with static-analysis-based approaches and learning-based approaches have demonstrated the effectiveness of FGVulDet.","sentences":["Source code vulnerability detection aims to identify inherent vulnerabilities to safeguard software systems from potential attacks.","Many prior studies overlook diverse vulnerability characteristics, simplifying the problem into a binary (0-1) classification task for example determining whether it is vulnerable or not.","This poses a challenge for a single deep learning-based model to effectively learn the wide array of vulnerability characteristics.","Furthermore, due to the challenges associated with collecting large-scale vulnerability data, these detectors often overfit limited training datasets, resulting in lower model generalization performance.   ","To address the aforementioned challenges, in this work, we introduce a fine-grained vulnerability detector namely FGVulDet.","Unlike previous approaches, FGVulDet employs multiple classifiers to discern characteristics of various vulnerability types and combines their outputs to identify the specific type of vulnerability.","Each classifier is designed to learn type-specific vulnerability semantics.","Additionally, to address the scarcity of data for some vulnerability types and enhance data diversity for learning better vulnerability semantics, we propose a novel vulnerability-preserving data augmentation technique to augment the number of vulnerabilities.","Taking inspiration from recent advancements in graph neural networks for learning program semantics, we incorporate a Gated Graph Neural Network (GGNN) and extend it to an edge-aware GGNN to capture edge-type information.","FGVulDet is trained on a large-scale dataset from GitHub, encompassing five different types of vulnerabilities.","Extensive experiments compared with static-analysis-based approaches and learning-based approaches have demonstrated the effectiveness of FGVulDet."],"url":"http://arxiv.org/abs/2404.09599v1","category":"cs.CR"}
{"created":"2024-04-15 09:06:07","title":"Building Semantic Communication System via Molecules: An End-to-End Training Approach","abstract":"The concept of semantic communication provides a novel approach for applications in scenarios with limited communication resources. In this paper, we propose an end-to-end (E2E) semantic molecular communication system, aiming to enhance the efficiency of molecular communication systems by reducing the transmitted information. Specifically, following the joint source channel coding paradigm, the network is designed to encode the task-relevant information into the concentration of the information molecules, which is robust to the degradation of the molecular communication channel. Furthermore, we propose a channel network to enable the E2E learning over the non-differentiable molecular channel. Experimental results demonstrate the superior performance of the semantic molecular communication system over the conventional methods in classification tasks.","sentences":["The concept of semantic communication provides a novel approach for applications in scenarios with limited communication resources.","In this paper, we propose an end-to-end (E2E) semantic molecular communication system, aiming to enhance the efficiency of molecular communication systems by reducing the transmitted information.","Specifically, following the joint source channel coding paradigm, the network is designed to encode the task-relevant information into the concentration of the information molecules, which is robust to the degradation of the molecular communication channel.","Furthermore, we propose a channel network to enable the E2E learning over the non-differentiable molecular channel.","Experimental results demonstrate the superior performance of the semantic molecular communication system over the conventional methods in classification tasks."],"url":"http://arxiv.org/abs/2404.09595v1","category":"eess.SP"}
{"created":"2024-04-15 09:01:47","title":"3D Gaussian Splatting as Markov Chain Monte Carlo","abstract":"While 3D Gaussian Splatting has recently become popular for neural rendering, current methods rely on carefully engineered cloning and splitting strategies for placing Gaussians, which does not always generalize and may lead to poor-quality renderings. In addition, for real-world scenes, they rely on a good initial point cloud to perform well. In this work, we rethink 3D Gaussians as random samples drawn from an underlying probability distribution describing the physical representation of the scene -- in other words, Markov Chain Monte Carlo (MCMC) samples. Under this view, we show that the 3D Gaussian updates are strikingly similar to a Stochastic Langevin Gradient Descent (SGLD) update. As with MCMC, samples are nothing but past visit locations, adding new Gaussians under our framework can simply be realized without heuristics as placing Gaussians at existing Gaussian locations. To encourage using fewer Gaussians for efficiency, we introduce an L1-regularizer on the Gaussians. On various standard evaluation scenes, we show that our method provides improved rendering quality, easy control over the number of Gaussians, and robustness to initialization.","sentences":["While 3D Gaussian Splatting has recently become popular for neural rendering, current methods rely on carefully engineered cloning and splitting strategies for placing Gaussians, which does not always generalize and may lead to poor-quality renderings.","In addition, for real-world scenes, they rely on a good initial point cloud to perform well.","In this work, we rethink 3D Gaussians as random samples drawn from an underlying probability distribution describing the physical representation of the scene -- in other words, Markov Chain Monte Carlo (MCMC) samples.","Under this view, we show that the 3D Gaussian updates are strikingly similar to a Stochastic Langevin Gradient Descent (SGLD) update.","As with MCMC, samples are nothing but past visit locations, adding new Gaussians under our framework can simply be realized without heuristics as placing Gaussians at existing Gaussian locations.","To encourage using fewer Gaussians for efficiency, we introduce an L1-regularizer on the Gaussians.","On various standard evaluation scenes, we show that our method provides improved rendering quality, easy control over the number of Gaussians, and robustness to initialization."],"url":"http://arxiv.org/abs/2404.09591v1","category":"cs.CV"}
{"created":"2024-04-15 08:58:00","title":"On variable Lebesgue spaces and generalized nonlinear heat equations","abstract":"In this work we address some questions concerning the Cauchy problem for a generalized nonlinear heat equations considering as functional framework the variable Lebesgue spaces $L^{p(\\cdot)}(\\mathbb{R}^n)$. More precisely, by mixing some structural properties of these spaces with decay estimates of the fractional heat kernel, we were able to prove two well-posedness results for these equations. In a first theorem, we prove the existence and uniqueness of global-in-time mild solutions in the mixed-space $\\mathcal{L}^{p(\\cdot)}_{ \\frac{nb}{2\\alpha - \\langle 1 \\rangle_\\gamma} } (\\mathbb{R}^n,L^\\infty([0,T[ ))$. On the other hand, by introducing a new class of variable exponents, we demonstrate the existence of an unique local-in-time mild solution in the space $L^{p(\\cdot)} \\left( [0,T], L^{q} (\\mathbb{R}^3) \\right)$.","sentences":["In this work we address some questions concerning the Cauchy problem for a generalized nonlinear heat equations considering as functional framework the variable Lebesgue spaces $L^{p(\\cdot)}(\\mathbb{R}^n)$. More precisely, by mixing some structural properties of these spaces with decay estimates of the fractional heat kernel, we were able to prove two well-posedness results for these equations.","In a first theorem, we prove the existence and uniqueness of global-in-time mild solutions in the mixed-space $\\mathcal{L}^{p(\\cdot)}_{ \\frac{nb}{2\\alpha - \\langle 1 \\rangle_\\gamma} } (\\mathbb{R}^n,L^\\infty([0,T[ ))$.","On the other hand, by introducing a new class of variable exponents, we demonstrate the existence of an unique local-in-time mild solution in the space $L^{p(\\cdot)} \\left( [0,T], L^{q} (\\mathbb{R}^3) \\right)$."],"url":"http://arxiv.org/abs/2404.09588v1","category":"math.AP"}
{"created":"2024-04-15 08:56:53","title":"German Tourism Knowledge Graph","abstract":"Tourism is one of the most critical sectors of the global economy. Due to its heterogeneous and fragmented nature, it provides one of the most suitable use cases for knowledge graphs. In this poster, we introduce the German Tourism Knowledge Graph that integrates tourism-related data from 16 federal states of Germany and various other sources to provide a curated knowledge source for various applications. It is publicly available through GUIs and an API.","sentences":["Tourism is one of the most critical sectors of the global economy.","Due to its heterogeneous and fragmented nature, it provides one of the most suitable use cases for knowledge graphs.","In this poster, we introduce the German Tourism Knowledge Graph that integrates tourism-related data from 16 federal states of Germany and various other sources to provide a curated knowledge source for various applications.","It is publicly available through GUIs and an API."],"url":"http://arxiv.org/abs/2404.09587v1","category":"cs.AI"}
{"created":"2024-04-15 08:40:01","title":"Modelling Language","abstract":"This paper argues that large language models have a valuable scientific role to play in serving as scientific models of a language. Linguistic study should not only be concerned with the cognitive processes behind linguistic competence, but also with language understood as an external, social entity. Once this is recognized, the value of large language models as scientific models becomes clear. This paper defends this position against a number of arguments to the effect that language models provide no linguistic insight. It also draws upon recent work in philosophy of science to show how large language models could serve as scientific models.","sentences":["This paper argues that large language models have a valuable scientific role to play in serving as scientific models of a language.","Linguistic study should not only be concerned with the cognitive processes behind linguistic competence, but also with language understood as an external, social entity.","Once this is recognized, the value of large language models as scientific models becomes clear.","This paper defends this position against a number of arguments to the effect that language models provide no linguistic insight.","It also draws upon recent work in philosophy of science to show how large language models could serve as scientific models."],"url":"http://arxiv.org/abs/2404.09579v1","category":"cs.CL"}
{"created":"2024-04-15 08:38:43","title":"Transformers, Contextualism, and Polysemy","abstract":"The transformer architecture, introduced by Vaswani et al. (2017), is at the heart of the remarkable recent progress in the development of language models, including famous chatbots such as Chat-gpt and Bard. In this paper, I argue that we an extract from the way the transformer architecture works a picture of the relationship between context and meaning. I call this the transformer picture, and I argue that it is a novel with regard to two related philosophical debates: the contextualism debate regarding the extent of context-sensitivity across natural language, and the polysemy debate regarding how polysemy should be captured within an account of word meaning. Although much of the paper merely tries to position the transformer picture with respect to these two debates, I will also begin to make the case for the transformer picture.","sentences":["The transformer architecture, introduced by Vaswani et al. (2017), is at the heart of the remarkable recent progress in the development of language models, including famous chatbots such as Chat-gpt and Bard.","In this paper, I argue that we an extract from the way the transformer architecture works a picture of the relationship between context and meaning.","I call this the transformer picture, and I argue that it is a novel with regard to two related philosophical debates: the contextualism debate regarding the extent of context-sensitivity across natural language, and the polysemy debate regarding how polysemy should be captured within an account of word meaning.","Although much of the paper merely tries to position the transformer picture with respect to these two debates, I will also begin to make the case for the transformer picture."],"url":"http://arxiv.org/abs/2404.09577v1","category":"cs.CL"}
{"created":"2024-04-15 08:37:26","title":"Large language models and linguistic intentionality","abstract":"Do large language models like Chat-GPT or LLaMa meaningfully use the words they produce? Or are they merely clever prediction machines, simulating language use by producing statistically plausible text? There have already been some initial attempts to answer this question by showing that these models meet the criteria for entering meaningful states according to metasemantic theories of mental content. In this paper, I will argue for a different approach - that we should instead consider whether language models meet the criteria given by our best metasemantic theories of linguistic content. In that vein, I will illustrate how this can be done by applying two such theories to the case of language models: Gareth Evans' (1982) account of naming practices and Ruth Millikan's (1984, 2004, 2005) teleosemantics. In doing so, I will argue that it is a mistake to think that the failure of LLMs to meet plausible conditions for mental intentionality thereby renders their outputs meaningless, and that a distinguishing feature of linguistic intentionality - dependency on a pre-existing linguistic system - allows for the plausible result LLM outputs are meaningful.","sentences":["Do large language models like Chat-GPT or LLaMa meaningfully use the words they produce?","Or are they merely clever prediction machines, simulating language use by producing statistically plausible text?","There have already been some initial attempts to answer this question by showing that these models meet the criteria for entering meaningful states according to metasemantic theories of mental content.","In this paper, I will argue for a different approach - that we should instead consider whether language models meet the criteria given by our best metasemantic theories of linguistic content.","In that vein, I will illustrate how this can be done by applying two such theories to the case of language models: Gareth Evans' (1982) account of naming practices and Ruth Millikan's (1984, 2004, 2005) teleosemantics.","In doing so, I will argue that it is a mistake to think that the failure of LLMs to meet plausible conditions for mental intentionality thereby renders their outputs meaningless, and that a distinguishing feature of linguistic intentionality - dependency on a pre-existing linguistic system - allows for the plausible result LLM outputs are meaningful."],"url":"http://arxiv.org/abs/2404.09576v1","category":"cs.CL"}
{"created":"2024-04-15 08:36:40","title":"Predicting and Analyzing Pedestrian Crossing Behavior at Unsignalized Crossings","abstract":"Understanding and predicting pedestrian crossing behavior is essential for enhancing automated driving and improving driving safety. Predicting gap selection behavior and the use of zebra crossing enables driving systems to proactively respond and prevent potential conflicts. This task is particularly challenging at unsignalized crossings due to the ambiguous right of way, requiring pedestrians to constantly interact with vehicles and other pedestrians. This study addresses these challenges by utilizing simulator data to investigate scenarios involving multiple vehicles and pedestrians. We propose and evaluate machine learning models to predict gap selection in non-zebra scenarios and zebra crossing usage in zebra scenarios. We investigate and discuss how pedestrians' behaviors are influenced by various factors, including pedestrian waiting time, walking speed, the number of unused gaps, the largest missed gap, and the influence of other pedestrians. This research contributes to the evolution of intelligent vehicles by providing predictive models and valuable insights into pedestrian crossing behavior.","sentences":["Understanding and predicting pedestrian crossing behavior is essential for enhancing automated driving and improving driving safety.","Predicting gap selection behavior and the use of zebra crossing enables driving systems to proactively respond and prevent potential conflicts.","This task is particularly challenging at unsignalized crossings due to the ambiguous right of way, requiring pedestrians to constantly interact with vehicles and other pedestrians.","This study addresses these challenges by utilizing simulator data to investigate scenarios involving multiple vehicles and pedestrians.","We propose and evaluate machine learning models to predict gap selection in non-zebra scenarios and zebra crossing usage in zebra scenarios.","We investigate and discuss how pedestrians' behaviors are influenced by various factors, including pedestrian waiting time, walking speed, the number of unused gaps, the largest missed gap, and the influence of other pedestrians.","This research contributes to the evolution of intelligent vehicles by providing predictive models and valuable insights into pedestrian crossing behavior."],"url":"http://arxiv.org/abs/2404.09574v1","category":"cs.LG"}
{"created":"2024-04-15 08:32:44","title":"Swarm dynamics for global optimisation on finite sets","abstract":"Consider the global optimisation of a function $U$ defined on a finite set $V$ endowed with an irreducible and reversible Markov generator.By integration, we extend $U$ to the set $\\mathcal{P}(V)$ of probability distributions on $V$ and we penalise it with a time-dependent generalised entropy functional.Endowing $\\mathcal{P}(V)$ with a Maas' Wasserstein-type Riemannian structure, enables us to consider an associated time-inhomogeneous gradient descent algorithm.There are several ways to interpret this $\\cP(V)$-valued dynamical system as the time-marginal laws of a time-inhomogeneous non-linear Markov process taking values in $V$, each of them allowing for interacting particle approximations.This procedure extends to the discrete framework the continuous state space swarm algorithm approach of Bolte, Miclo and Villeneuve \\cite{Bolte}, but here we go further by considering more general generalised entropy functionals for which functional inequalities can be proven.Thus in the full generality of the above finite framework, we give conditions on the underlying time dependence ensuring the convergence of the algorithm toward laws supported by the set of global minima of $U$.Numerical simulations illustrate that one has to be careful about the choice of the time-inhomogeneous non-linear Markov process interpretation.","sentences":["Consider the global optimisation of a function $U$ defined on a finite set $V$ endowed with an irreducible and reversible Markov generator.","By integration, we extend $U$ to the set $\\mathcal{P}(V)$ of probability distributions on $V$ and we penalise it with a time-dependent generalised entropy functional.","Endowing $\\mathcal{P}(V)$ with a Maas' Wasserstein-type Riemannian structure, enables us to consider an associated time-inhomogeneous gradient descent algorithm.","There are several ways to interpret this $\\cP(V)$-valued dynamical system as the time-marginal laws of a time-inhomogeneous non-linear Markov process taking values in $V$, each of them allowing for interacting particle approximations.","This procedure extends to the discrete framework the continuous state space swarm algorithm approach of Bolte, Miclo and Villeneuve \\cite{Bolte}, but here we go further by considering more general generalised entropy functionals for which functional inequalities can be proven.","Thus in the full generality of the above finite framework, we give conditions on the underlying time dependence ensuring the convergence of the algorithm toward laws supported by the set of global minima of $U$.Numerical simulations illustrate that one has to be careful about the choice of the time-inhomogeneous non-linear Markov process interpretation."],"url":"http://arxiv.org/abs/2404.09572v1","category":"math.FA"}
{"created":"2024-04-15 08:30:37","title":"Branching diffusion processes and spectral properties of Feynman-Kac semigroup","abstract":"In this article we study the long time behavior of linear functionals of branching diffusion processesas well as the time reversal of the spinal process by means of spectral properties of the Feynman-Kacsemigroup. We generalize for this non Markovian semigroup the theory of quasi-stationary distribution(q.s.d.) and Q-process. The most amazing result is the identification of the law of the reversal time spinalprocess issued from q.s.d. with the Q-process of the Feynman-Kac semigroup.","sentences":["In this article we study the long time behavior of linear functionals of branching diffusion processesas well as the time reversal of the spinal process by means of spectral properties of the Feynman-Kacsemigroup.","We generalize for this non Markovian semigroup the theory of quasi-stationary distribution(q.s.d.)","and","Q-process.","The most amazing result is the identification of the law of the reversal time spinalprocess issued from q.s.d.","with the Q-process of the Feynman-Kac semigroup."],"url":"http://arxiv.org/abs/2404.09568v1","category":"math.PR"}
{"created":"2024-04-15 08:27:47","title":"Reliability Estimation of News Media Sources: Birds of a Feather Flock Together","abstract":"Evaluating the reliability of news sources is a routine task for journalists and organizations committed to acquiring and disseminating accurate information. Recent research has shown that predicting sources' reliability represents an important first-prior step in addressing additional challenges such as fake news detection and fact-checking. In this paper, we introduce a novel approach for source reliability estimation that leverages reinforcement learning strategies for estimating the reliability degree of news sources. Contrary to previous research, our proposed approach models the problem as the estimation of a reliability degree, and not a reliability label, based on how all the news media sources interact with each other on the Web. We validated the effectiveness of our method on a news media reliability dataset that is an order of magnitude larger than comparable existing datasets. Results show that the estimated reliability degrees strongly correlates with journalists-provided scores (Spearman=0.80) and can effectively predict reliability labels (macro-avg. F$_1$ score=81.05). We release our implementation and dataset, aiming to provide a valuable resource for the NLP community working on information verification.","sentences":["Evaluating the reliability of news sources is a routine task for journalists and organizations committed to acquiring and disseminating accurate information.","Recent research has shown that predicting sources' reliability represents an important first-prior step in addressing additional challenges such as fake news detection and fact-checking.","In this paper, we introduce a novel approach for source reliability estimation that leverages reinforcement learning strategies for estimating the reliability degree of news sources.","Contrary to previous research, our proposed approach models the problem as the estimation of a reliability degree, and not a reliability label, based on how all the news media sources interact with each other on the Web.","We validated the effectiveness of our method on a news media reliability dataset that is an order of magnitude larger than comparable existing datasets.","Results show that the estimated reliability degrees strongly correlates with journalists-provided scores (Spearman=0.80) and can effectively predict reliability labels (macro-avg.","F$_1$ score=81.05).","We release our implementation and dataset, aiming to provide a valuable resource for the NLP community working on information verification."],"url":"http://arxiv.org/abs/2404.09565v1","category":"cs.CL"}
{"created":"2024-04-15 08:22:47","title":"\u03c3-GPTs: A New Approach to Autoregressive Models","abstract":"Autoregressive models, such as the GPT family, use a fixed order, usually left-to-right, to generate sequences. However, this is not a necessity. In this paper, we challenge this assumption and show that by simply adding a positional encoding for the output, this order can be modulated on-the-fly per-sample which offers key advantageous properties. It allows for the sampling of and conditioning on arbitrary subsets of tokens, and it also allows sampling in one shot multiple tokens dynamically according to a rejection strategy, leading to a sub-linear number of model evaluations. We evaluate our method across various domains, including language modeling, path-solving, and aircraft vertical rate prediction, decreasing the number of steps required for generation by an order of magnitude.","sentences":["Autoregressive models, such as the GPT family, use a fixed order, usually left-to-right, to generate sequences.","However, this is not a necessity.","In this paper, we challenge this assumption and show that by simply adding a positional encoding for the output, this order can be modulated on-the-fly per-sample which offers key advantageous properties.","It allows for the sampling of and conditioning on arbitrary subsets of tokens, and it also allows sampling in one shot multiple tokens dynamically according to a rejection strategy, leading to a sub-linear number of model evaluations.","We evaluate our method across various domains, including language modeling, path-solving, and aircraft vertical rate prediction, decreasing the number of steps required for generation by an order of magnitude."],"url":"http://arxiv.org/abs/2404.09562v1","category":"cs.LG"}
{"created":"2024-04-15 08:22:12","title":"Generalization the parameters of minimal linear codes over the ring $\\mathbb{Z}_{p^l}$","abstract":"In this article, We introduce a condition that is both necessary and sufficient for a linear code to achieve minimality when analyzed over the ring $\\mathbb{Z}_{p^l}$. The fundamental inquiry in minimal linear codes is the existence of a $[n,k]$ minimal linear code where $k$ is less than or equal to $n$. W. Lu et al. ( see \\cite{nine}) showed that there exists a positive integer $n(k;q)$ such that for $n\\geq n(k;q)$ a minimal linear code of length $n$ and dimension $k$ over a finite field $\\mathbb{F}_q$ must exist. They give the upper and lower bound of $n(k;q)$. In this manuscript, we establish both an upper and lower bound for $n(k;p^l)$ within the ring $\\mathbb{Z}_{p^l}$.","sentences":["In this article, We introduce a condition that is both necessary and sufficient for a linear code to achieve minimality when analyzed over the ring $\\mathbb{Z}_{p^l}$. The fundamental inquiry in minimal linear codes is the existence of a $[n,k]$ minimal linear code where $k$ is less than or equal to $n$. W. Lu et al.","( see \\cite{nine}) showed that there exists a positive integer $n(k;q)$ such that for $n\\geq n(k;q)$ a minimal linear code of length $n$ and dimension $k$ over a finite field $\\mathbb{F}_q$ must exist.","They give the upper and lower bound of $n(k;q)$. In this manuscript, we establish both an upper and lower bound for $n(k;p^l)$ within the ring $\\mathbb{Z}_{p^l}$."],"url":"http://arxiv.org/abs/2404.09561v1","category":"cs.IT"}
{"created":"2024-04-15 08:21:17","title":"Joint Contrastive Learning with Feature Alignment for Cross-Corpus EEG-based Emotion Recognition","abstract":"The integration of human emotions into multimedia applications shows great potential for enriching user experiences and enhancing engagement across various digital platforms. Unlike traditional methods such as questionnaires, facial expressions, and voice analysis, brain signals offer a more direct and objective understanding of emotional states. However, in the field of electroencephalography (EEG)-based emotion recognition, previous studies have primarily concentrated on training and testing EEG models within a single dataset, overlooking the variability across different datasets. This oversight leads to significant performance degradation when applying EEG models to cross-corpus scenarios. In this study, we propose a novel Joint Contrastive learning framework with Feature Alignment (JCFA) to address cross-corpus EEG-based emotion recognition. The JCFA model operates in two main stages. In the pre-training stage, a joint domain contrastive learning strategy is introduced to characterize generalizable time-frequency representations of EEG signals, without the use of labeled data. It extracts robust time-based and frequency-based embeddings for each EEG sample, and then aligns them within a shared latent time-frequency space. In the fine-tuning stage, JCFA is refined in conjunction with downstream tasks, where the structural connections among brain electrodes are considered. The model capability could be further enhanced for the application in emotion detection and interpretation. Extensive experimental results on two well-recognized emotional datasets show that the proposed JCFA model achieves state-of-the-art (SOTA) performance, outperforming the second-best method by an average accuracy increase of 4.09% in cross-corpus EEG-based emotion recognition tasks.","sentences":["The integration of human emotions into multimedia applications shows great potential for enriching user experiences and enhancing engagement across various digital platforms.","Unlike traditional methods such as questionnaires, facial expressions, and voice analysis, brain signals offer a more direct and objective understanding of emotional states.","However, in the field of electroencephalography (EEG)-based emotion recognition, previous studies have primarily concentrated on training and testing EEG models within a single dataset, overlooking the variability across different datasets.","This oversight leads to significant performance degradation when applying EEG models to cross-corpus scenarios.","In this study, we propose a novel Joint Contrastive learning framework with Feature Alignment (JCFA) to address cross-corpus EEG-based emotion recognition.","The JCFA model operates in two main stages.","In the pre-training stage, a joint domain contrastive learning strategy is introduced to characterize generalizable time-frequency representations of EEG signals, without the use of labeled data.","It extracts robust time-based and frequency-based embeddings for each EEG sample, and then aligns them within a shared latent time-frequency space.","In the fine-tuning stage, JCFA is refined in conjunction with downstream tasks, where the structural connections among brain electrodes are considered.","The model capability could be further enhanced for the application in emotion detection and interpretation.","Extensive experimental results on two well-recognized emotional datasets show that the proposed JCFA model achieves state-of-the-art (SOTA) performance, outperforming the second-best method by an average accuracy increase of 4.09% in cross-corpus EEG-based emotion recognition tasks."],"url":"http://arxiv.org/abs/2404.09559v1","category":"cs.HC"}
{"created":"2024-04-15 08:20:16","title":"Global prediction of nuclear charge density distributions using deep neural network","abstract":"A deep neural network (DNN) has been developed to generate the distributions of nuclear charge density, utilizing the training data from the relativistic density functional theory and incorporating available experimental charge radii of 1014 nuclei into the loss function. The DNN achieved a root-mean-square (rms) deviation of 0.0193 fm for charge radii on its validation set. Furthermore, the DNN can improve the description in both the tail and central regions of the charge density, enhancing agreement with experimental findings. The model's predictive capability has been further validated by its agreement with recent experimental data on charge radii. Finally, this refined model is employed to predict the charge density distributions in a wider range of nuclide chart, and the parameterized charge densities, charge radii, and higher-order moments of charge density distributions are given, providing a robust reference for future experimental investigations.","sentences":["A deep neural network (DNN) has been developed to generate the distributions of nuclear charge density, utilizing the training data from the relativistic density functional theory and incorporating available experimental charge radii of 1014 nuclei into the loss function.","The DNN achieved a root-mean-square (rms) deviation of 0.0193 fm for charge radii on its validation set.","Furthermore, the DNN can improve the description in both the tail and central regions of the charge density, enhancing agreement with experimental findings.","The model's predictive capability has been further validated by its agreement with recent experimental data on charge radii.","Finally, this refined model is employed to predict the charge density distributions in a wider range of nuclide chart, and the parameterized charge densities, charge radii, and higher-order moments of charge density distributions are given, providing a robust reference for future experimental investigations."],"url":"http://arxiv.org/abs/2404.09558v1","category":"nucl-th"}
{"created":"2024-04-15 08:19:13","title":"Characterization and Mitigation of Insufficiencies in Automated Driving Systems","abstract":"Automated Driving (AD) systems have the potential to increase safety, comfort and energy efficiency. Recently, major automotive companies have started testing and validating AD systems (ADS) on public roads. Nevertheless, the commercial deployment and wide adoption of ADS have been moderate, partially due to system functional insufficiencies (FI) that undermine passenger safety and lead to hazardous situations on the road. FIs are defined in ISO 21448 Safety Of The Intended Functionality (SOTIF). FIs are insufficiencies in sensors, actuators and algorithm implementations, including neural networks and probabilistic calculations. Examples of FIs in ADS include inaccurate ego-vehicle localization on the road, incorrect prediction of a cyclist maneuver, unreliable detection of a pedestrian, etc.   The main goal of our study is to formulate a generic architectural design pattern, which is compatible with existing methods and ADS, to improve FI mitigation and enable faster commercial deployment of ADS. First, we studied the 2021 autonomous vehicles disengagement reports published by the California Department of Motor Vehicles (DMV). The data clearly show that disengagements are five times more often caused by FIs rather than by system faults. We then made a comprehensive list of insufficiencies and their characteristics by analyzing over 10 hours of publicly available road test videos. In particular, we identified insufficiency types in four major categories: world model, motion plan, traffic rule, and operational design domain. The insufficiency characterization helps making the SOTIF analyses of triggering conditions more systematic and comprehensive.   Based on our FI characterization, simulation experiments and literature survey, we define a novel generic architectural design pattern Daruma to dynamically select the channel that is least likely to have a FI at the moment.","sentences":["Automated Driving (AD) systems have the potential to increase safety, comfort and energy efficiency.","Recently, major automotive companies have started testing and validating AD systems (ADS) on public roads.","Nevertheless, the commercial deployment and wide adoption of ADS have been moderate, partially due to system functional insufficiencies (FI) that undermine passenger safety and lead to hazardous situations on the road.","FIs are defined in ISO 21448 Safety Of The Intended Functionality (SOTIF).","FIs are insufficiencies in sensors, actuators and algorithm implementations, including neural networks and probabilistic calculations.","Examples of FIs in ADS include inaccurate ego-vehicle localization on the road, incorrect prediction of a cyclist maneuver, unreliable detection of a pedestrian, etc.   ","The main goal of our study is to formulate a generic architectural design pattern, which is compatible with existing methods and ADS, to improve FI mitigation and enable faster commercial deployment of ADS.","First, we studied the 2021 autonomous vehicles disengagement reports published by the California Department of Motor Vehicles (DMV).","The data clearly show that disengagements are five times more often caused by FIs rather than by system faults.","We then made a comprehensive list of insufficiencies and their characteristics by analyzing over 10 hours of publicly available road test videos.","In particular, we identified insufficiency types in four major categories: world model, motion plan, traffic rule, and operational design domain.","The insufficiency characterization helps making the SOTIF analyses of triggering conditions more systematic and comprehensive.   ","Based on our FI characterization, simulation experiments and literature survey, we define a novel generic architectural design pattern Daruma to dynamically select the channel that is least likely to have a FI at the moment."],"url":"http://arxiv.org/abs/2404.09557v1","category":"cs.RO"}
{"created":"2024-04-15 08:18:16","title":"Explainable Generative AI (GenXAI): A Survey, Conceptualization, and Research Agenda","abstract":"Generative AI (GenAI) marked a shift from AI being able to recognize to AI being able to generate solutions for a wide variety of tasks. As the generated solutions and applications become increasingly more complex and multi-faceted, novel needs, objectives, and possibilities have emerged for explainability (XAI). In this work, we elaborate on why XAI has gained importance with the rise of GenAI and its challenges for explainability research. We also unveil novel and emerging desiderata that explanations should fulfill, covering aspects such as verifiability, interactivity, security, and cost. To this end, we focus on surveying existing works. Furthermore, we provide a taxonomy of relevant dimensions that allows us to better characterize existing XAI mechanisms and methods for GenAI. We discuss different avenues to ensure XAI, from training data to prompting. Our paper offers a short but concise technical background of GenAI for non-technical readers, focusing on text and images to better understand novel or adapted XAI techniques for GenAI. However, due to the vast array of works on GenAI, we decided to forego detailed aspects of XAI related to evaluation and usage of explanations. As such, the manuscript interests both technically oriented people and other disciplines, such as social scientists and information systems researchers. Our research roadmap provides more than ten directions for future investigation.","sentences":["Generative AI (GenAI) marked a shift from AI being able to recognize to AI being able to generate solutions for a wide variety of tasks.","As the generated solutions and applications become increasingly more complex and multi-faceted, novel needs, objectives, and possibilities have emerged for explainability (XAI).","In this work, we elaborate on why XAI has gained importance with the rise of GenAI and its challenges for explainability research.","We also unveil novel and emerging desiderata that explanations should fulfill, covering aspects such as verifiability, interactivity, security, and cost.","To this end, we focus on surveying existing works.","Furthermore, we provide a taxonomy of relevant dimensions that allows us to better characterize existing XAI mechanisms and methods for GenAI.","We discuss different avenues to ensure XAI, from training data to prompting.","Our paper offers a short but concise technical background of GenAI for non-technical readers, focusing on text and images to better understand novel or adapted XAI techniques for GenAI.","However, due to the vast array of works on GenAI, we decided to forego detailed aspects of XAI related to evaluation and usage of explanations.","As such, the manuscript interests both technically oriented people and other disciplines, such as social scientists and information systems researchers.","Our research roadmap provides more than ten directions for future investigation."],"url":"http://arxiv.org/abs/2404.09554v1","category":"cs.AI"}
{"created":"2024-04-15 08:17:43","title":"Has Anti-corruption Efforts lowered Enterprises Innovation Efficiency? -An Empirical Analysis from China","abstract":"This study adopts the fixed effects panel model and provincial panel data on anticorruption and the innovation efficiency of high-level technology and new technology enterprises in China from 2005 to 2014, to estimate the effects of the anticorruption movement on the innovation efficiency of enterprises at different corruption levels. The empirical results show that anticorruption is positively correlated with the innovation efficiency of enterprises; however, the correlation is differentiated according to different corruption levels and business natures. At a high level of corruption, anticorruption has positive impacts on enterprises' innovation; at a low level of corruption, it negatively affects innovation efficiency. However, anticorruption has negative effects on the innovation efficiency of state-owned enterprises at both high and low corruption levels; for nonstate-owned enterprises, its effects are positive at a high corruption level and negative at a low corruption level. The effects remain the same across different regions.","sentences":["This study adopts the fixed effects panel model and provincial panel data on anticorruption and the innovation efficiency of high-level technology and new technology enterprises in China from 2005 to 2014, to estimate the effects of the anticorruption movement on the innovation efficiency of enterprises at different corruption levels.","The empirical results show that anticorruption is positively correlated with the innovation efficiency of enterprises; however, the correlation is differentiated according to different corruption levels and business natures.","At a high level of corruption, anticorruption has positive impacts on enterprises' innovation; at a low level of corruption, it negatively affects innovation efficiency.","However, anticorruption has negative effects on the innovation efficiency of state-owned enterprises at both high and low corruption levels; for nonstate-owned enterprises, its effects are positive at a high corruption level and negative at a low corruption level.","The effects remain the same across different regions."],"url":"http://arxiv.org/abs/2404.09553v1","category":"econ.GN"}
{"created":"2024-04-15 08:15:54","title":"On the Wasserstein distance between a hyperuniform point process and its mean","abstract":"We study the average $p-$Wasserstein distance between a finite sample of an infinite hyperuniform point process on $\\mathbb{R}^2$ and its mean for any $p\\geq 1$. The average Wasserstein transport cost is shown to be bounded from above and from below by some multiples of the number of points. More generally, we give a control on the $p-$Wasserstein distance in function of a control on the $L^p$ norm of the difference of the point process and its mean.","sentences":["We study the average $p-$Wasserstein distance between a finite sample of an infinite hyperuniform point process on $\\mathbb{R}^2$ and its mean for any $p\\geq 1$.","The average Wasserstein transport cost is shown to be bounded from above and from below by some multiples of the number of points.","More generally, we give a control on the $p-$Wasserstein distance in function of a control on the $L^p$ norm of the difference of the point process and its mean."],"url":"http://arxiv.org/abs/2404.09549v1","category":"math.PR"}
{"created":"2024-04-15 08:13:55","title":"Cosmological Correlators with Double Massive Exchanges: Bootstrap Equation and Phenomenology","abstract":"Using the recently developed cosmological bootstrap method, we compute the exact analytical solution for the seed integral appearing in cosmological correlators with double massive scalar exchanges. The result is explicit, valid in any kinematic configuration, and free from spurious divergences. It is applicable to any number of fields' species with any masses. With an appropriate choice of variables, the results contain only single-layer summations. We also propose simple approximate formulas valid in different limits, enabling direct and instantaneous evaluation.Supported by exact numerical results using CosmoFlow, we explore the phenomenology of double massive exchange diagrams. Contrary to single-exchange diagrams with ubiquitous Lorentz-covariant interactions, the size of the cubic coupling constant can be large while respecting perturbativity bounds. Because of this property, the primordial bispectrum from double-exchange diagrams can be as large as, coincidentally, current observational constraints. In addition to being sizable on equilateral configurations, we show that the primordial bispectrum exhibits a large cosmological collider signal in the squeezed limit, making the double massive exchanges interesting channels for the detection of massive primordial fields. We propose to decisively disentangle double-exchange channels from single-exchange ones with cosmological observations by exploiting the phase information of the cosmological collider signal, the inflationary flavor oscillations from multiple fields' species exchanges and the double soft limit in the primordial trispectrum.","sentences":["Using the recently developed cosmological bootstrap method, we compute the exact analytical solution for the seed integral appearing in cosmological correlators with double massive scalar exchanges.","The result is explicit, valid in any kinematic configuration, and free from spurious divergences.","It is applicable to any number of fields' species with any masses.","With an appropriate choice of variables, the results contain only single-layer summations.","We also propose simple approximate formulas valid in different limits, enabling direct and instantaneous evaluation.","Supported by exact numerical results using CosmoFlow, we explore the phenomenology of double massive exchange diagrams.","Contrary to single-exchange diagrams with ubiquitous Lorentz-covariant interactions, the size of the cubic coupling constant can be large while respecting perturbativity bounds.","Because of this property, the primordial bispectrum from double-exchange diagrams can be as large as, coincidentally, current observational constraints.","In addition to being sizable on equilateral configurations, we show that the primordial bispectrum exhibits a large cosmological collider signal in the squeezed limit, making the double massive exchanges interesting channels for the detection of massive primordial fields.","We propose to decisively disentangle double-exchange channels from single-exchange ones with cosmological observations by exploiting the phase information of the cosmological collider signal, the inflationary flavor oscillations from multiple fields' species exchanges and the double soft limit in the primordial trispectrum."],"url":"http://arxiv.org/abs/2404.09547v1","category":"hep-th"}
{"created":"2024-04-15 08:11:21","title":"GNNavigator: Towards Adaptive Training of Graph Neural Networks via Automatic Guideline Exploration","abstract":"Graph Neural Networks (GNNs) succeed significantly in many applications recently. However, balancing GNNs training runtime cost, memory consumption, and attainable accuracy for various applications is non-trivial. Previous training methodologies suffer from inferior adaptability and lack a unified training optimization solution. To address the problem, this work proposes GNNavigator, an adaptive GNN training configuration optimization framework. GNNavigator meets diverse GNN application requirements due to our unified software-hardware co-abstraction, proposed GNNs training performance model, and practical design space exploration solution. Experimental results show that GNNavigator can achieve up to 3.1x speedup and 44.9% peak memory reduction with comparable accuracy to state-of-the-art approaches.","sentences":["Graph Neural Networks (GNNs) succeed significantly in many applications recently.","However, balancing GNNs training runtime cost, memory consumption, and attainable accuracy for various applications is non-trivial.","Previous training methodologies suffer from inferior adaptability and lack a unified training optimization solution.","To address the problem, this work proposes GNNavigator, an adaptive GNN training configuration optimization framework.","GNNavigator meets diverse GNN application requirements due to our unified software-hardware co-abstraction, proposed GNNs training performance model, and practical design space exploration solution.","Experimental results show that GNNavigator can achieve up to 3.1x speedup and 44.9% peak memory reduction with comparable accuracy to state-of-the-art approaches."],"url":"http://arxiv.org/abs/2404.09544v1","category":"cs.LG"}
{"created":"2024-04-15 08:06:54","title":"Application of the representative measure approach to assess the reliability of decision trees in dealing with unseen vehicle collision data","abstract":"Machine learning algorithms are fundamental components of novel data-informed Artificial Intelligence architecture. In this domain, the imperative role of representative datasets is a cornerstone in shaping the trajectory of artificial intelligence (AI) development. Representative datasets are needed to train machine learning components properly. Proper training has multiple impacts: it reduces the final model's complexity, power, and uncertainties. In this paper, we investigate the reliability of the $\\varepsilon$-representativeness method to assess the dataset similarity from a theoretical perspective for decision trees. We decided to focus on the family of decision trees because it includes a wide variety of models known to be explainable. Thus, in this paper, we provide a result guaranteeing that if two datasets are related by $\\varepsilon$-representativeness, i.e., both of them have points closer than $\\varepsilon$, then the predictions by the classic decision tree are similar. Experimentally, we have also tested that $\\varepsilon$-representativeness presents a significant correlation with the ordering of the feature importance. Moreover, we extend the results experimentally in the context of unseen vehicle collision data for XGboost, a machine-learning component widely adopted for dealing with tabular data.","sentences":["Machine learning algorithms are fundamental components of novel data-informed Artificial Intelligence architecture.","In this domain, the imperative role of representative datasets is a cornerstone in shaping the trajectory of artificial intelligence (AI) development.","Representative datasets are needed to train machine learning components properly.","Proper training has multiple impacts: it reduces the final model's complexity, power, and uncertainties.","In this paper, we investigate the reliability of the $\\varepsilon$-representativeness method to assess the dataset similarity from a theoretical perspective for decision trees.","We decided to focus on the family of decision trees because it includes a wide variety of models known to be explainable.","Thus, in this paper, we provide a result guaranteeing that if two datasets are related by $\\varepsilon$-representativeness, i.e., both of them have points closer than $\\varepsilon$, then the predictions by the classic decision tree are similar.","Experimentally, we have also tested that $\\varepsilon$-representativeness presents a significant correlation with the ordering of the feature importance.","Moreover, we extend the results experimentally in the context of unseen vehicle collision data for XGboost, a machine-learning component widely adopted for dealing with tabular data."],"url":"http://arxiv.org/abs/2404.09541v1","category":"cs.LG"}
{"created":"2024-04-15 08:04:44","title":"Text-Driven Diverse Facial Texture Generation via Progressive Latent-Space Refinement","abstract":"Automatic 3D facial texture generation has gained significant interest recently. Existing approaches may not support the traditional physically based rendering pipeline or rely on 3D data captured by Light Stage. Our key contribution is a progressive latent space refinement approach that can bootstrap from 3D Morphable Models (3DMMs)-based texture maps generated from facial images to generate high-quality and diverse PBR textures, including albedo, normal, and roughness. It starts with enhancing Generative Adversarial Networks (GANs) for text-guided and diverse texture generation. To this end, we design a self-supervised paradigm to overcome the reliance on ground truth 3D textures and train the generative model with only entangled texture maps. Besides, we foster mutual enhancement between GANs and Score Distillation Sampling (SDS). SDS boosts GANs with more generative modes, while GANs promote more efficient optimization of SDS. Furthermore, we introduce an edge-aware SDS for multi-view consistent facial structure. Experiments demonstrate that our method outperforms existing 3D texture generation methods regarding photo-realistic quality, diversity, and efficiency.","sentences":["Automatic 3D facial texture generation has gained significant interest recently.","Existing approaches may not support the traditional physically based rendering pipeline or rely on 3D data captured by Light Stage.","Our key contribution is a progressive latent space refinement approach that can bootstrap from 3D Morphable Models (3DMMs)-based texture maps generated from facial images to generate high-quality and diverse PBR textures, including albedo, normal, and roughness.","It starts with enhancing Generative Adversarial Networks (GANs) for text-guided and diverse texture generation.","To this end, we design a self-supervised paradigm to overcome the reliance on ground truth 3D textures and train the generative model with only entangled texture maps.","Besides, we foster mutual enhancement between GANs and Score Distillation Sampling (SDS).","SDS boosts GANs with more generative modes, while GANs promote more efficient optimization of SDS.","Furthermore, we introduce an edge-aware SDS for multi-view consistent facial structure.","Experiments demonstrate that our method outperforms existing 3D texture generation methods regarding photo-realistic quality, diversity, and efficiency."],"url":"http://arxiv.org/abs/2404.09540v1","category":"cs.CV"}
{"created":"2024-04-15 08:04:32","title":"LR-FHSS-Sim: A Discrete-Event Simulator for LR-FHSS Networks","abstract":"This work presents the LR-FHSS-Sim, a free and open-source discrete-event simulator for LR-FHSS networks. We highlight the importance of network modeling for IoT coverage, especially when it is needed to capture dynamic network behaviors. Written in Python, we present the LR-FHSS-Sim main structure, procedures, and extensions. We discuss the importance of a modular code, which facilitates the creation of algorithmic strategies and signal-processing techniques for LR-FHSS networks. Moreover, we showcase how to achieve results when considering different packet generation traffic patterns and with a previously published extension. Finally, we discuss our thoughts on future implementations and what can be achieved with them.","sentences":["This work presents the LR-FHSS-Sim, a free and open-source discrete-event simulator for LR-FHSS networks.","We highlight the importance of network modeling for IoT coverage, especially when it is needed to capture dynamic network behaviors.","Written in Python, we present the LR-FHSS-Sim main structure, procedures, and extensions.","We discuss the importance of a modular code, which facilitates the creation of algorithmic strategies and signal-processing techniques for LR-FHSS networks.","Moreover, we showcase how to achieve results when considering different packet generation traffic patterns and with a previously published extension.","Finally, we discuss our thoughts on future implementations and what can be achieved with them."],"url":"http://arxiv.org/abs/2404.09539v1","category":"cs.NI"}
{"created":"2024-04-15 08:01:02","title":"Machine Learning Techniques for Python Source Code Vulnerability Detection","abstract":"Software vulnerabilities are a fundamental reason for the prevalence of cyber attacks and their identification is a crucial yet challenging problem in cyber security. In this paper, we apply and compare different machine learning algorithms for source code vulnerability detection specifically for Python programming language. Our experimental evaluation demonstrates that our Bidirectional Long Short-Term Memory (BiLSTM) model achieves a remarkable performance (average Accuracy = 98.6%, average F-Score = 94.7%, average Precision = 96.2%, average Recall = 93.3%, average ROC = 99.3%), thereby, establishing a new benchmark for vulnerability detection in Python source code.","sentences":["Software vulnerabilities are a fundamental reason for the prevalence of cyber attacks and their identification is a crucial yet challenging problem in cyber security.","In this paper, we apply and compare different machine learning algorithms for source code vulnerability detection specifically for Python programming language.","Our experimental evaluation demonstrates that our Bidirectional Long Short-Term Memory (BiLSTM) model achieves a remarkable performance (average Accuracy = 98.6%, average F-Score = 94.7%, average Precision = 96.2%, average Recall = 93.3%, average ROC = 99.3%), thereby, establishing a new benchmark for vulnerability detection in Python source code."],"url":"http://arxiv.org/abs/2404.09537v1","category":"cs.SE"}
{"created":"2024-04-15 07:59:11","title":"Beyond Noise: Privacy-Preserving Decentralized Learning with Virtual Nodes","abstract":"Decentralized learning (DL) enables collaborative learning without a server and without training data leaving the users' devices. However, the models shared in DL can still be used to infer training data. Conventional privacy defenses such as differential privacy and secure aggregation fall short in effectively safeguarding user privacy in DL. We introduce Shatter, a novel DL approach in which nodes create virtual nodes (VNs) to disseminate chunks of their full model on their behalf. This enhances privacy by (i) preventing attackers from collecting full models from other nodes, and (ii) hiding the identity of the original node that produced a given model chunk. We theoretically prove the convergence of Shatter and provide a formal analysis demonstrating how Shatter reduces the efficacy of attacks compared to when exchanging full models between participating nodes. We evaluate the convergence and attack resilience of Shatter with existing DL algorithms, with heterogeneous datasets, and against three standard privacy attacks, including gradient inversion. Our evaluation shows that Shatter not only renders these privacy attacks infeasible when each node operates 16 VNs but also exhibits a positive impact on model convergence compared to standard DL. This enhanced privacy comes with a manageable increase in communication volume.","sentences":["Decentralized learning (DL) enables collaborative learning without a server and without training data leaving the users' devices.","However, the models shared in DL can still be used to infer training data.","Conventional privacy defenses such as differential privacy and secure aggregation fall short in effectively safeguarding user privacy in DL.","We introduce Shatter, a novel DL approach in which nodes create virtual nodes (VNs) to disseminate chunks of their full model on their behalf.","This enhances privacy by (i) preventing attackers from collecting full models from other nodes, and (ii) hiding the identity of the original node that produced a given model chunk.","We theoretically prove the convergence of Shatter and provide a formal analysis demonstrating how Shatter reduces the efficacy of attacks compared to when exchanging full models between participating nodes.","We evaluate the convergence and attack resilience of Shatter with existing DL algorithms, with heterogeneous datasets, and against three standard privacy attacks, including gradient inversion.","Our evaluation shows that Shatter not only renders these privacy attacks infeasible when each node operates 16 VNs but also exhibits a positive impact on model convergence compared to standard DL.","This enhanced privacy comes with a manageable increase in communication volume."],"url":"http://arxiv.org/abs/2404.09536v1","category":"cs.DC"}
{"created":"2024-04-15 07:53:07","title":"WiTUnet: A U-Shaped Architecture Integrating CNN and Transformer for Improved Feature Alignment and Local Information Fusion","abstract":"Low-dose computed tomography (LDCT) has become the technology of choice for diagnostic medical imaging, given its lower radiation dose compared to standard CT, despite increasing image noise and potentially affecting diagnostic accuracy. To address this, advanced deep learning-based LDCT denoising algorithms have been developed, primarily using Convolutional Neural Networks (CNNs) or Transformer Networks with the Unet architecture. This architecture enhances image detail by integrating feature maps from the encoder and decoder via skip connections. However, current methods often overlook enhancements to the Unet architecture itself, focusing instead on optimizing encoder and decoder structures. This approach can be problematic due to the significant differences in feature map characteristics between the encoder and decoder, where simple fusion strategies may not effectively reconstruct images.In this paper, we introduce WiTUnet, a novel LDCT image denoising method that utilizes nested, dense skip pathways instead of traditional skip connections to improve feature integration. WiTUnet also incorporates a windowed Transformer structure to process images in smaller, non-overlapping segments, reducing computational load. Additionally, the integration of a Local Image Perception Enhancement (LiPe) module in both the encoder and decoder replaces the standard multi-layer perceptron (MLP) in Transformers, enhancing local feature capture and representation. Through extensive experimental comparisons, WiTUnet has demonstrated superior performance over existing methods in key metrics such as Peak Signal-to-Noise Ratio (PSNR), Structural Similarity (SSIM), and Root Mean Square Error (RMSE), significantly improving noise removal and image quality.","sentences":["Low-dose computed tomography (LDCT) has become the technology of choice for diagnostic medical imaging, given its lower radiation dose compared to standard CT, despite increasing image noise and potentially affecting diagnostic accuracy.","To address this, advanced deep learning-based LDCT denoising algorithms have been developed, primarily using Convolutional Neural Networks (CNNs) or Transformer Networks with the Unet architecture.","This architecture enhances image detail by integrating feature maps from the encoder and decoder via skip connections.","However, current methods often overlook enhancements to the Unet architecture itself, focusing instead on optimizing encoder and decoder structures.","This approach can be problematic due to the significant differences in feature map characteristics between the encoder and decoder, where simple fusion strategies may not effectively reconstruct images.","In this paper, we introduce WiTUnet, a novel LDCT image denoising method that utilizes nested, dense skip pathways instead of traditional skip connections to improve feature integration.","WiTUnet also incorporates a windowed Transformer structure to process images in smaller, non-overlapping segments, reducing computational load.","Additionally, the integration of a Local Image Perception Enhancement (LiPe) module in both the encoder and decoder replaces the standard multi-layer perceptron (MLP) in Transformers, enhancing local feature capture and representation.","Through extensive experimental comparisons, WiTUnet has demonstrated superior performance over existing methods in key metrics such as Peak Signal-to-Noise Ratio (PSNR), Structural Similarity (SSIM), and Root Mean Square Error (RMSE), significantly improving noise removal and image quality."],"url":"http://arxiv.org/abs/2404.09533v1","category":"cs.CV"}
{"created":"2024-04-15 07:51:40","title":"TMPQ-DM: Joint Timestep Reduction and Quantization Precision Selection for Efficient Diffusion Models","abstract":"Diffusion models have emerged as preeminent contenders in the realm of generative models. Distinguished by their distinctive sequential generative processes, characterized by hundreds or even thousands of timesteps, diffusion models progressively reconstruct images from pure Gaussian noise, with each timestep necessitating full inference of the entire model. However, the substantial computational demands inherent to these models present challenges for deployment, quantization is thus widely used to lower the bit-width for reducing the storage and computing overheads. Current quantization methodologies primarily focus on model-side optimization, disregarding the temporal dimension, such as the length of the timestep sequence, thereby allowing redundant timesteps to continue consuming computational resources, leaving substantial scope for accelerating the generative process. In this paper, we introduce TMPQ-DM, which jointly optimizes timestep reduction and quantization to achieve a superior performance-efficiency trade-off, addressing both temporal and model optimization aspects. For timestep reduction, we devise a non-uniform grouping scheme tailored to the non-uniform nature of the denoising process, thereby mitigating the explosive combinations of timesteps. In terms of quantization, we adopt a fine-grained layer-wise approach to allocate varying bit-widths to different layers based on their respective contributions to the final generative performance, thus rectifying performance degradation observed in prior studies. To expedite the evaluation of fine-grained quantization, we further devise a super-network to serve as a precision solver by leveraging shared quantization results. These two design components are seamlessly integrated within our framework, enabling rapid joint exploration of the exponentially large decision space via a gradient-free evolutionary search algorithm.","sentences":["Diffusion models have emerged as preeminent contenders in the realm of generative models.","Distinguished by their distinctive sequential generative processes, characterized by hundreds or even thousands of timesteps, diffusion models progressively reconstruct images from pure Gaussian noise, with each timestep necessitating full inference of the entire model.","However, the substantial computational demands inherent to these models present challenges for deployment, quantization is thus widely used to lower the bit-width for reducing the storage and computing overheads.","Current quantization methodologies primarily focus on model-side optimization, disregarding the temporal dimension, such as the length of the timestep sequence, thereby allowing redundant timesteps to continue consuming computational resources, leaving substantial scope for accelerating the generative process.","In this paper, we introduce TMPQ-DM, which jointly optimizes timestep reduction and quantization to achieve a superior performance-efficiency trade-off, addressing both temporal and model optimization aspects.","For timestep reduction, we devise a non-uniform grouping scheme tailored to the non-uniform nature of the denoising process, thereby mitigating the explosive combinations of timesteps.","In terms of quantization, we adopt a fine-grained layer-wise approach to allocate varying bit-widths to different layers based on their respective contributions to the final generative performance, thus rectifying performance degradation observed in prior studies.","To expedite the evaluation of fine-grained quantization, we further devise a super-network to serve as a precision solver by leveraging shared quantization results.","These two design components are seamlessly integrated within our framework, enabling rapid joint exploration of the exponentially large decision space via a gradient-free evolutionary search algorithm."],"url":"http://arxiv.org/abs/2404.09532v1","category":"cs.CV"}
{"created":"2024-04-15 07:50:15","title":"RanLayNet: A Dataset for Document Layout Detection used for Domain Adaptation and Generalization","abstract":"Large ground-truth datasets and recent advances in deep learning techniques have been useful for layout detection. However, because of the restricted layout diversity of these datasets, training on them requires a sizable number of annotated instances, which is both expensive and time-consuming. As a result, differences between the source and target domains may significantly impact how well these models function. To solve this problem, domain adaptation approaches have been developed that use a small quantity of labeled data to adjust the model to the target domain. In this research, we introduced a synthetic document dataset called RanLayNet, enriched with automatically assigned labels denoting spatial positions, ranges, and types of layout elements. The primary aim of this endeavor is to develop a versatile dataset capable of training models with robustness and adaptability to diverse document formats. Through empirical experimentation, we demonstrate that a deep layout identification model trained on our dataset exhibits enhanced performance compared to a model trained solely on actual documents. Moreover, we conduct a comparative analysis by fine-tuning inference models using both PubLayNet and IIIT-AR-13K datasets on the Doclaynet dataset. Our findings emphasize that models enriched with our dataset are optimal for tasks such as achieving 0.398 and 0.588 mAP95 score in the scientific document domain for the TABLE class.","sentences":["Large ground-truth datasets and recent advances in deep learning techniques have been useful for layout detection.","However, because of the restricted layout diversity of these datasets, training on them requires a sizable number of annotated instances, which is both expensive and time-consuming.","As a result, differences between the source and target domains may significantly impact how well these models function.","To solve this problem, domain adaptation approaches have been developed that use a small quantity of labeled data to adjust the model to the target domain.","In this research, we introduced a synthetic document dataset called RanLayNet, enriched with automatically assigned labels denoting spatial positions, ranges, and types of layout elements.","The primary aim of this endeavor is to develop a versatile dataset capable of training models with robustness and adaptability to diverse document formats.","Through empirical experimentation, we demonstrate that a deep layout identification model trained on our dataset exhibits enhanced performance compared to a model trained solely on actual documents.","Moreover, we conduct a comparative analysis by fine-tuning inference models using both PubLayNet and IIIT-AR-13K datasets on the Doclaynet dataset.","Our findings emphasize that models enriched with our dataset are optimal for tasks such as achieving 0.398 and 0.588 mAP95 score in the scientific document domain for the TABLE class."],"url":"http://arxiv.org/abs/2404.09530v1","category":"cs.CV"}
{"created":"2024-04-15 07:49:10","title":"Prepacking: A Simple Method for Fast Prefilling and Increased Throughput in Large Language Models","abstract":"During inference for transformer-based large language models (LLM), prefilling is the computation of the key-value (KV) cache for input tokens in the prompt prior to autoregressive generation. For longer input prompt lengths, prefilling will incur a significant overhead on decoding time. In this work, we highlight the following pitfall of prefilling: for batches containing high-varying prompt lengths, significant computation is wasted by the standard practice of padding sequences to the maximum length. As LLMs increasingly support longer context lengths, potentially up to 10 million tokens, variations in prompt lengths within a batch become more pronounced. To address this, we propose Prepacking, a simple yet effective method to optimize prefilling computation. To avoid redundant computation on pad tokens, prepacking combines prompts of varying lengths into a sequence and packs multiple sequences into a compact batch using a bin-packing algorithm. It then modifies the attention mask and positional encoding to compute multiple prefilled KV-caches for multiple prompts within a single sequence. On standard curated dataset containing prompts with varying lengths, we obtain a significant speed and memory efficiency improvements as compared to the default padding-based prefilling computation within Huggingface across a range of base model configurations and inference serving scenarios.","sentences":["During inference for transformer-based large language models (LLM), prefilling is the computation of the key-value (KV) cache for input tokens in the prompt prior to autoregressive generation.","For longer input prompt lengths, prefilling will incur a significant overhead on decoding time.","In this work, we highlight the following pitfall of prefilling: for batches containing high-varying prompt lengths, significant computation is wasted by the standard practice of padding sequences to the maximum length.","As LLMs increasingly support longer context lengths, potentially up to 10 million tokens, variations in prompt lengths within a batch become more pronounced.","To address this, we propose Prepacking, a simple yet effective method to optimize prefilling computation.","To avoid redundant computation on pad tokens, prepacking combines prompts of varying lengths into a sequence and packs multiple sequences into a compact batch using a bin-packing algorithm.","It then modifies the attention mask and positional encoding to compute multiple prefilled KV-caches for multiple prompts within a single sequence.","On standard curated dataset containing prompts with varying lengths, we obtain a significant speed and memory efficiency improvements as compared to the default padding-based prefilling computation within Huggingface across a range of base model configurations and inference serving scenarios."],"url":"http://arxiv.org/abs/2404.09529v1","category":"cs.LG"}
{"created":"2024-04-15 07:42:14","title":"Coupling results and Markovian structures for number representations of continuous random variables","abstract":"A general setting for nested subdivisions of a bounded real set into intervals defining the digits $X_1,X_2,...$ of a random variable $X$ with a probability density function $f$ is considered. Under the weak condition that $f$ is almost everywhere lower semi-continuous, a coupling between $X$ and a non-negative integer-valued random variable $N$ is established so that $X_1,...,X_N$ have an interpretation as the ``sufficient digits'', since the distribution of $R=(X_{N+1},X_{N+2},...)$ conditioned on $S=(X_1,...,X_N)$ does not depend on $f$. Adding a condition about a Markovian structure of the lengths of the intervals in the nested subdivisions, $R\\,|\\,S$ becomes a Markov chain of a certain order $s\\ge0$. If $s=0$ then $X_{N+1},X_{N+2},...$ are IID with a known distribution. When $s>0$ and the Markov chain is uniformly geometric ergodic, a coupling is established between $(X,N)$ and a random time $M$ so that the chain after time $\\max\\{N,s\\}+M-s$ is stationary and $M$ follows a simple known distribution. The results are related to several examples of number representations generated by a dynamical system, including base-$q$ expansions, generalized L\\\"uroth series, $\\beta$-expansions, and continued fraction representations. The importance of the results and some suggestions and open problems for future research are discussed.","sentences":["A general setting for nested subdivisions of a bounded real set into intervals defining the digits $X_1,X_2,...$ of a random variable $X$ with a probability density function $f$ is considered.","Under the weak condition that $f$ is almost everywhere lower semi-continuous, a coupling between $X$ and a non-negative integer-valued random variable $N$ is established so that $X_1,...,X_N$ have an interpretation as the ``sufficient digits'', since the distribution of $R=(X_{N+1},X_{N+2},...)$ conditioned on $S=(X_1,...,X_N)$ does not depend on $f$. Adding a condition about a Markovian structure of the lengths of the intervals in the nested subdivisions, $R\\,|\\,S$ becomes a Markov chain of a certain order $s\\ge0$. If $s=0$ then $X_{N+1},X_{N+2},...$ are IID with a known distribution.","When $s>0$ and the Markov chain is uniformly geometric ergodic, a coupling is established between $(X,N)$ and a random time $M$ so that the chain after time $\\max\\{N,s\\}+M-s$ is stationary and $M$ follows a simple known distribution.","The results are related to several examples of number representations generated by a dynamical system, including base-$q$ expansions, generalized L\\\"uroth series, $\\beta$-expansions, and continued fraction representations.","The importance of the results and some suggestions and open problems for future research are discussed."],"url":"http://arxiv.org/abs/2404.09525v1","category":"math.PR"}
{"created":"2024-04-15 07:31:48","title":"Inferring Behavior-Specific Context Improves Zero-Shot Generalization in Reinforcement Learning","abstract":"In this work, we address the challenge of zero-shot generalization (ZSG) in Reinforcement Learning (RL), where agents must adapt to entirely novel environments without additional training. We argue that understanding and utilizing contextual cues, such as the gravity level of the environment, is critical for robust generalization, and we propose to integrate the learning of context representations directly with policy learning. Our algorithm demonstrates improved generalization on various simulated domains, outperforming prior context-learning techniques in zero-shot settings. By jointly learning policy and context, our method acquires behavior-specific context representations, enabling adaptation to unseen environments and marks progress towards reinforcement learning systems that generalize across diverse real-world tasks. Our code and experiments are available at https://github.com/tidiane-camaret/contextual_rl_zero_shot.","sentences":["In this work, we address the challenge of zero-shot generalization (ZSG) in Reinforcement Learning (RL), where agents must adapt to entirely novel environments without additional training.","We argue that understanding and utilizing contextual cues, such as the gravity level of the environment, is critical for robust generalization, and we propose to integrate the learning of context representations directly with policy learning.","Our algorithm demonstrates improved generalization on various simulated domains, outperforming prior context-learning techniques in zero-shot settings.","By jointly learning policy and context, our method acquires behavior-specific context representations, enabling adaptation to unseen environments and marks progress towards reinforcement learning systems that generalize across diverse real-world tasks.","Our code and experiments are available at https://github.com/tidiane-camaret/contextual_rl_zero_shot."],"url":"http://arxiv.org/abs/2404.09521v1","category":"cs.LG"}
{"created":"2024-04-15 07:24:45","title":"State Space Model for New-Generation Network Alternative to Transformers: A Survey","abstract":"In the post-deep learning era, the Transformer architecture has demonstrated its powerful performance across pre-trained big models and various downstream tasks. However, the enormous computational demands of this architecture have deterred many researchers. To further reduce the complexity of attention models, numerous efforts have been made to design more efficient methods. Among them, the State Space Model (SSM), as a possible replacement for the self-attention based Transformer model, has drawn more and more attention in recent years. In this paper, we give the first comprehensive review of these works and also provide experimental comparisons and analysis to better demonstrate the features and advantages of SSM. Specifically, we first give a detailed description of principles to help the readers quickly capture the key ideas of SSM. After that, we dive into the reviews of existing SSMs and their various applications, including natural language processing, computer vision, graph, multi-modal and multi-media, point cloud/event stream, time series data, and other domains. In addition, we give statistical comparisons and analysis of these models and hope it helps the readers to understand the effectiveness of different structures on various tasks. Then, we propose possible research points in this direction to better promote the development of the theoretical model and application of SSM. More related works will be continuously updated on the following GitHub: https://github.com/Event-AHU/Mamba_State_Space_Model_Paper_List.","sentences":["In the post-deep learning era, the Transformer architecture has demonstrated its powerful performance across pre-trained big models and various downstream tasks.","However, the enormous computational demands of this architecture have deterred many researchers.","To further reduce the complexity of attention models, numerous efforts have been made to design more efficient methods.","Among them, the State Space Model (SSM), as a possible replacement for the self-attention based Transformer model, has drawn more and more attention in recent years.","In this paper, we give the first comprehensive review of these works and also provide experimental comparisons and analysis to better demonstrate the features and advantages of SSM.","Specifically, we first give a detailed description of principles to help the readers quickly capture the key ideas of SSM.","After that, we dive into the reviews of existing SSMs and their various applications, including natural language processing, computer vision, graph, multi-modal and multi-media, point cloud/event stream, time series data, and other domains.","In addition, we give statistical comparisons and analysis of these models and hope it helps the readers to understand the effectiveness of different structures on various tasks.","Then, we propose possible research points in this direction to better promote the development of the theoretical model and application of SSM.","More related works will be continuously updated on the following GitHub: https://github.com/Event-AHU/Mamba_State_Space_Model_Paper_List."],"url":"http://arxiv.org/abs/2404.09516v1","category":"cs.LG"}
{"created":"2024-04-15 07:20:09","title":"Deep image learning of quantitative structure-property relationships of cooper alloys via feature augmentation on Geodesic curve in shape space","abstract":"Understanding how the structure of materials affects their properties is a cornerstone of materials science and engineering. However, traditional methods have struggled to accurately describe the quantitative structure-property relationships for complex structures. In our study, we bridge this gap by leveraging machine learning to analyze images of materials' microstructures, thus offering a novel way to understand and predict the properties of materials based on their microstructures. We introduce a method known as FAGC (Feature Augmentation on Geodesic Curves), specifically demonstrated for Cu-Cr-Zr alloys. This approach utilizes machine learning to examine the shapes within images of the alloys' microstructures and predict their mechanical and electronic properties. This generative FAGC approach can effectively expand the relatively small training datasets due to the limited availability of materials images labeled with quantitative properties. The process begins with extracting features from the images using neural networks. These features are then mapped onto the Pre-shape space to construct the Geodesic curves. Along these curves, new features are generated, effectively increasing the dataset. Moreover, we design a pseudo-labeling mechanism for these newly generated features to further enhance the training dataset. Our FAGC method has shown remarkable results, significantly improving the accuracy of predicting the electronic conductivity and hardness of Cu-Cr-Zr alloys, with R-squared values of 0.978 and 0.998, respectively. These outcomes underscore the potential of FAGC to address the challenge of limited image data in materials science, providing a powerful tool for establishing detailed and quantitative relationships between complex microstructures and material properties.","sentences":["Understanding how the structure of materials affects their properties is a cornerstone of materials science and engineering.","However, traditional methods have struggled to accurately describe the quantitative structure-property relationships for complex structures.","In our study, we bridge this gap by leveraging machine learning to analyze images of materials' microstructures, thus offering a novel way to understand and predict the properties of materials based on their microstructures.","We introduce a method known as FAGC (Feature Augmentation on Geodesic Curves), specifically demonstrated for Cu-Cr-Zr alloys.","This approach utilizes machine learning to examine the shapes within images of the alloys' microstructures and predict their mechanical and electronic properties.","This generative FAGC approach can effectively expand the relatively small training datasets due to the limited availability of materials images labeled with quantitative properties.","The process begins with extracting features from the images using neural networks.","These features are then mapped onto the Pre-shape space to construct the Geodesic curves.","Along these curves, new features are generated, effectively increasing the dataset.","Moreover, we design a pseudo-labeling mechanism for these newly generated features to further enhance the training dataset.","Our FAGC method has shown remarkable results, significantly improving the accuracy of predicting the electronic conductivity and hardness of Cu-Cr-Zr alloys, with R-squared values of 0.978 and 0.998, respectively.","These outcomes underscore the potential of FAGC to address the challenge of limited image data in materials science, providing a powerful tool for establishing detailed and quantitative relationships between complex microstructures and material properties."],"url":"http://arxiv.org/abs/2404.09515v1","category":"cs.CV"}
{"created":"2024-04-15 07:16:52","title":"Asymptotics in infinite monoidal categories","abstract":"We discuss formulas for the asymptotic growth rate of the number of summands in tensor powers in certain (finite or infinite) monoidal categories. Our focus is on monoidal categories with infinitely many indecomposable objects, with our main tools being generalized Perron-Frobenius theory alongside techniques from random walks.","sentences":["We discuss formulas for the asymptotic growth rate of the number of summands in tensor powers in certain (finite or infinite) monoidal categories.","Our focus is on monoidal categories with infinitely many indecomposable objects, with our main tools being generalized Perron-Frobenius theory alongside techniques from random walks."],"url":"http://arxiv.org/abs/2404.09513v1","category":"math.CT"}
{"created":"2024-04-15 07:15:39","title":"Magic Clothing: Controllable Garment-Driven Image Synthesis","abstract":"We propose Magic Clothing, a latent diffusion model (LDM)-based network architecture for an unexplored garment-driven image synthesis task. Aiming at generating customized characters wearing the target garments with diverse text prompts, the image controllability is the most critical issue, i.e., to preserve the garment details and maintain faithfulness to the text prompts. To this end, we introduce a garment extractor to capture the detailed garment features, and employ self-attention fusion to incorporate them into the pretrained LDMs, ensuring that the garment details remain unchanged on the target character. Then, we leverage the joint classifier-free guidance to balance the control of garment features and text prompts over the generated results. Meanwhile, the proposed garment extractor is a plug-in module applicable to various finetuned LDMs, and it can be combined with other extensions like ControlNet and IP-Adapter to enhance the diversity and controllability of the generated characters. Furthermore, we design Matched-Points-LPIPS (MP-LPIPS), a robust metric for evaluating the consistency of the target image to the source garment. Extensive experiments demonstrate that our Magic Clothing achieves state-of-the-art results under various conditional controls for garment-driven image synthesis. Our source code is available at https://github.com/ShineChen1024/MagicClothing.","sentences":["We propose Magic Clothing, a latent diffusion model (LDM)-based network architecture for an unexplored garment-driven image synthesis task.","Aiming at generating customized characters wearing the target garments with diverse text prompts, the image controllability is the most critical issue, i.e., to preserve the garment details and maintain faithfulness to the text prompts.","To this end, we introduce a garment extractor to capture the detailed garment features, and employ self-attention fusion to incorporate them into the pretrained LDMs, ensuring that the garment details remain unchanged on the target character.","Then, we leverage the joint classifier-free guidance to balance the control of garment features and text prompts over the generated results.","Meanwhile, the proposed garment extractor is a plug-in module applicable to various finetuned LDMs, and it can be combined with other extensions like ControlNet and IP-Adapter to enhance the diversity and controllability of the generated characters.","Furthermore, we design Matched-Points-LPIPS (MP-LPIPS), a robust metric for evaluating the consistency of the target image to the source garment.","Extensive experiments demonstrate that our Magic Clothing achieves state-of-the-art results under various conditional controls for garment-driven image synthesis.","Our source code is available at https://github.com/ShineChen1024/MagicClothing."],"url":"http://arxiv.org/abs/2404.09512v1","category":"cs.CV"}
{"created":"2024-04-15 07:06:47","title":"Listen to the Waves: Using a Neuronal Model of the Human Auditory System to Predict Ocean Waves","abstract":"Artificial neural networks (ANNs) have evolved from the 1940s primitive models of brain function to become tools for artificial intelligence. They comprise many units, artificial neurons, interlinked through weighted connections. ANNs are trained to perform tasks through learning rules that modify the connection weights. With these rules being in the focus of research, ANNs have become a branch of machine learning developing independently from neuroscience. Although likely required for the development of truly intelligent machines, the integration of neuroscience into ANNs has remained a neglected proposition.   Here, we demonstrate that designing an ANN along biological principles results in drastically improved task performance. As a challenging real-world problem, we choose real-time ocean-wave prediction which is essential for various maritime operations. Motivated by the similarity of ocean waves measured at a single location to sound waves arriving at the eardrum, we redesign an echo state network to resemble the brain's auditory system. This yields a powerful predictive tool which is computationally lean, robust with respect to network parameters, and works efficiently across a wide range of sea states. Our results demonstrate the advantages of integrating neuroscience with machine learning and offer a tool for use in the production of green energy from ocean waves.","sentences":["Artificial neural networks (ANNs) have evolved from the 1940s primitive models of brain function to become tools for artificial intelligence.","They comprise many units, artificial neurons, interlinked through weighted connections.","ANNs are trained to perform tasks through learning rules that modify the connection weights.","With these rules being in the focus of research, ANNs have become a branch of machine learning developing independently from neuroscience.","Although likely required for the development of truly intelligent machines, the integration of neuroscience into ANNs has remained a neglected proposition.   ","Here, we demonstrate that designing an ANN along biological principles results in drastically improved task performance.","As a challenging real-world problem, we choose real-time ocean-wave prediction which is essential for various maritime operations.","Motivated by the similarity of ocean waves measured at a single location to sound waves arriving at the eardrum, we redesign an echo state network to resemble the brain's auditory system.","This yields a powerful predictive tool which is computationally lean, robust with respect to network parameters, and works efficiently across a wide range of sea states.","Our results demonstrate the advantages of integrating neuroscience with machine learning and offer a tool for use in the production of green energy from ocean waves."],"url":"http://arxiv.org/abs/2404.09510v1","category":"eess.SP"}
{"created":"2024-04-15 06:50:58","title":"Learning Tracking Representations from Single Point Annotations","abstract":"Existing deep trackers are typically trained with largescale video frames with annotated bounding boxes. However, these bounding boxes are expensive and time-consuming to annotate, in particular for large scale datasets. In this paper, we propose to learn tracking representations from single point annotations (i.e., 4.5x faster to annotate than the traditional bounding box) in a weakly supervised manner. Specifically, we propose a soft contrastive learning (SoCL) framework that incorporates target objectness prior into end-to-end contrastive learning. Our SoCL consists of adaptive positive and negative sample generation, which is memory-efficient and effective for learning tracking representations. We apply the learned representation of SoCL to visual tracking and show that our method can 1) achieve better performance than the fully supervised baseline trained with box annotations under the same annotation time cost; 2) achieve comparable performance of the fully supervised baseline by using the same number of training frames and meanwhile reducing annotation time cost by 78% and total fees by 85%; 3) be robust to annotation noise.","sentences":["Existing deep trackers are typically trained with largescale video frames with annotated bounding boxes.","However, these bounding boxes are expensive and time-consuming to annotate, in particular for large scale datasets.","In this paper, we propose to learn tracking representations from single point annotations (i.e., 4.5x faster to annotate than the traditional bounding box) in a weakly supervised manner.","Specifically, we propose a soft contrastive learning (SoCL) framework that incorporates target objectness prior into end-to-end contrastive learning.","Our SoCL consists of adaptive positive and negative sample generation, which is memory-efficient and effective for learning tracking representations.","We apply the learned representation of SoCL to visual tracking and show that our method can 1) achieve better performance than the fully supervised baseline trained with box annotations under the same annotation time cost; 2) achieve comparable performance of the fully supervised baseline by using the same number of training frames and meanwhile reducing annotation time cost by 78% and total fees by 85%; 3) be robust to annotation noise."],"url":"http://arxiv.org/abs/2404.09504v1","category":"cs.CV"}
{"created":"2024-04-15 06:38:09","title":"Learning Human Motion from Monocular Videos via Cross-Modal Manifold Alignment","abstract":"Learning 3D human motion from 2D inputs is a fundamental task in the realms of computer vision and computer graphics. Many previous methods grapple with this inherently ambiguous task by introducing motion priors into the learning process. However, these approaches face difficulties in defining the complete configurations of such priors or training a robust model. In this paper, we present the Video-to-Motion Generator (VTM), which leverages motion priors through cross-modal latent feature space alignment between 3D human motion and 2D inputs, namely videos and 2D keypoints. To reduce the complexity of modeling motion priors, we model the motion data separately for the upper and lower body parts. Additionally, we align the motion data with a scale-invariant virtual skeleton to mitigate the interference of human skeleton variations to the motion priors. Evaluated on AIST++, the VTM showcases state-of-the-art performance in reconstructing 3D human motion from monocular videos. Notably, our VTM exhibits the capabilities for generalization to unseen view angles and in-the-wild videos.","sentences":["Learning 3D human motion from 2D inputs is a fundamental task in the realms of computer vision and computer graphics.","Many previous methods grapple with this inherently ambiguous task by introducing motion priors into the learning process.","However, these approaches face difficulties in defining the complete configurations of such priors or training a robust model.","In this paper, we present the Video-to-Motion Generator (VTM), which leverages motion priors through cross-modal latent feature space alignment between 3D human motion and 2D inputs, namely videos and 2D keypoints.","To reduce the complexity of modeling motion priors, we model the motion data separately for the upper and lower body parts.","Additionally, we align the motion data with a scale-invariant virtual skeleton to mitigate the interference of human skeleton variations to the motion priors.","Evaluated on AIST++, the VTM showcases state-of-the-art performance in reconstructing 3D human motion from monocular videos.","Notably, our VTM exhibits the capabilities for generalization to unseen view angles and in-the-wild videos."],"url":"http://arxiv.org/abs/2404.09499v1","category":"cs.CV"}
{"created":"2024-04-15 06:37:21","title":"FusionMamba: Dynamic Feature Enhancement for Multimodal Image Fusion with Mamba","abstract":"Multi-modal image fusion aims to combine information from different modes to create a single image with comprehensive information and detailed textures. However, fusion models based on convolutional neural networks encounter limitations in capturing global image features due to their focus on local convolution operations. Transformer-based models, while excelling in global feature modeling, confront computational challenges stemming from their quadratic complexity. Recently, the Selective Structured State Space Model has exhibited significant potential for long-range dependency modeling with linear complexity, offering a promising avenue to address the aforementioned dilemma. In this paper, we propose FusionMamba, a novel dynamic feature enhancement method for multimodal image fusion with Mamba. Specifically, we devise an improved efficient Mamba model for image fusion, integrating efficient visual state space model with dynamic convolution and channel attention. This refined model not only upholds the performance of Mamba and global modeling capability but also diminishes channel redundancy while enhancing local enhancement capability. Additionally, we devise a dynamic feature fusion module (DFFM) comprising two dynamic feature enhancement modules (DFEM) and a cross modality fusion mamba module (CMFM). The former serves for dynamic texture enhancement and dynamic difference perception, whereas the latter enhances correlation features between modes and suppresses redundant intermodal information. FusionMamba has yielded state-of-the-art (SOTA) performance across various multimodal medical image fusion tasks (CT-MRI, PET-MRI, SPECT-MRI), infrared and visible image fusion task (IR-VIS) and multimodal biomedical image fusion dataset (GFP-PC), which is proved that our model has generalization ability. The code for FusionMamba is available at https://github.com/millieXie/FusionMamba.","sentences":["Multi-modal image fusion aims to combine information from different modes to create a single image with comprehensive information and detailed textures.","However, fusion models based on convolutional neural networks encounter limitations in capturing global image features due to their focus on local convolution operations.","Transformer-based models, while excelling in global feature modeling, confront computational challenges stemming from their quadratic complexity.","Recently, the Selective Structured State Space Model has exhibited significant potential for long-range dependency modeling with linear complexity, offering a promising avenue to address the aforementioned dilemma.","In this paper, we propose FusionMamba, a novel dynamic feature enhancement method for multimodal image fusion with Mamba.","Specifically, we devise an improved efficient Mamba model for image fusion, integrating efficient visual state space model with dynamic convolution and channel attention.","This refined model not only upholds the performance of Mamba and global modeling capability but also diminishes channel redundancy while enhancing local enhancement capability.","Additionally, we devise a dynamic feature fusion module (DFFM) comprising two dynamic feature enhancement modules (DFEM) and a cross modality fusion mamba module (CMFM).","The former serves for dynamic texture enhancement and dynamic difference perception, whereas the latter enhances correlation features between modes and suppresses redundant intermodal information.","FusionMamba has yielded state-of-the-art (SOTA) performance across various multimodal medical image fusion tasks (CT-MRI, PET-MRI, SPECT-MRI), infrared and visible image fusion task (IR-VIS) and multimodal biomedical image fusion dataset (GFP-PC), which is proved that our model has generalization ability.","The code for FusionMamba is available at https://github.com/millieXie/FusionMamba."],"url":"http://arxiv.org/abs/2404.09498v1","category":"cs.CV"}
{"created":"2024-04-15 06:33:32","title":"Towards Collaborative Autonomous Driving: Simulation Platform and End-to-End System","abstract":"Vehicle-to-everything-aided autonomous driving (V2X-AD) has a huge potential to provide a safer driving solution. Despite extensive researches in transportation and communication to support V2X-AD, the actual utilization of these infrastructures and communication resources in enhancing driving performances remains largely unexplored. This highlights the necessity of collaborative autonomous driving: a machine learning approach that optimizes the information sharing strategy to improve the driving performance of each vehicle. This effort necessitates two key foundations: a platform capable of generating data to facilitate the training and testing of V2X-AD, and a comprehensive system that integrates full driving-related functionalities with mechanisms for information sharing. From the platform perspective, we present V2Xverse, a comprehensive simulation platform for collaborative autonomous driving. This platform provides a complete pipeline for collaborative driving. From the system perspective, we introduce CoDriving, a novel end-to-end collaborative driving system that properly integrates V2X communication over the entire autonomous pipeline, promoting driving with shared perceptual information. The core idea is a novel driving-oriented communication strategy. Leveraging this strategy, CoDriving improves driving performance while optimizing communication efficiency. We make comprehensive benchmarks with V2Xverse, analyzing both modular performance and closed-loop driving performance. Experimental results show that CoDriving: i) significantly improves the driving score by 62.49% and drastically reduces the pedestrian collision rate by 53.50% compared to the SOTA end-to-end driving method, and ii) achieves sustaining driving performance superiority over dynamic constraint communication conditions.","sentences":["Vehicle-to-everything-aided autonomous driving (V2X-AD) has a huge potential to provide a safer driving solution.","Despite extensive researches in transportation and communication to support V2X-AD, the actual utilization of these infrastructures and communication resources in enhancing driving performances remains largely unexplored.","This highlights the necessity of collaborative autonomous driving: a machine learning approach that optimizes the information sharing strategy to improve the driving performance of each vehicle.","This effort necessitates two key foundations: a platform capable of generating data to facilitate the training and testing of V2X-AD, and a comprehensive system that integrates full driving-related functionalities with mechanisms for information sharing.","From the platform perspective, we present V2Xverse, a comprehensive simulation platform for collaborative autonomous driving.","This platform provides a complete pipeline for collaborative driving.","From the system perspective, we introduce CoDriving, a novel end-to-end collaborative driving system that properly integrates V2X communication over the entire autonomous pipeline, promoting driving with shared perceptual information.","The core idea is a novel driving-oriented communication strategy.","Leveraging this strategy, CoDriving improves driving performance while optimizing communication efficiency.","We make comprehensive benchmarks with V2Xverse, analyzing both modular performance and closed-loop driving performance.","Experimental results show that CoDriving: i) significantly improves the driving score by 62.49% and drastically reduces the pedestrian collision rate by 53.50% compared to the SOTA end-to-end driving method, and ii) achieves sustaining driving performance superiority over dynamic constraint communication conditions."],"url":"http://arxiv.org/abs/2404.09496v1","category":"cs.CV"}
{"created":"2024-04-15 06:28:20","title":"Bridging the Gap between Different Vocabularies for LLM Ensemble","abstract":"Ensembling different large language models (LLMs) to unleash their complementary potential and harness their individual strengths is highly valuable. Nevertheless, vocabulary discrepancies among various LLMs have constrained previous studies to either selecting or blending completely generated outputs. This limitation hinders the dynamic correction and enhancement of outputs during the generation process, resulting in a limited capacity for effective ensemble. To address this issue, we propose a novel method to Ensemble LLMs via Vocabulary Alignment (EVA). EVA bridges the lexical gap among various LLMs, enabling meticulous ensemble at each generation step. Specifically, we first learn mappings between the vocabularies of different LLMs with the assistance of overlapping tokens. Subsequently, these mappings are employed to project output distributions of LLMs into a unified space, facilitating a fine-grained ensemble. Finally, we design a filtering strategy to exclude models that generate unfaithful tokens. Experimental results on commonsense reasoning, arithmetic reasoning, machine translation, and data-to-text generation tasks demonstrate the superiority of our approach compared with individual LLMs and previous ensemble methods conducted on complete outputs. Further analyses confirm that our approach can leverage knowledge from different language models and yield consistent improvement.","sentences":["Ensembling different large language models (LLMs) to unleash their complementary potential and harness their individual strengths is highly valuable.","Nevertheless, vocabulary discrepancies among various LLMs have constrained previous studies to either selecting or blending completely generated outputs.","This limitation hinders the dynamic correction and enhancement of outputs during the generation process, resulting in a limited capacity for effective ensemble.","To address this issue, we propose a novel method to Ensemble LLMs via Vocabulary Alignment (EVA).","EVA bridges the lexical gap among various LLMs, enabling meticulous ensemble at each generation step.","Specifically, we first learn mappings between the vocabularies of different LLMs with the assistance of overlapping tokens.","Subsequently, these mappings are employed to project output distributions of LLMs into a unified space, facilitating a fine-grained ensemble.","Finally, we design a filtering strategy to exclude models that generate unfaithful tokens.","Experimental results on commonsense reasoning, arithmetic reasoning, machine translation, and data-to-text generation tasks demonstrate the superiority of our approach compared with individual LLMs and previous ensemble methods conducted on complete outputs.","Further analyses confirm that our approach can leverage knowledge from different language models and yield consistent improvement."],"url":"http://arxiv.org/abs/2404.09492v1","category":"cs.CL"}
{"created":"2024-04-15 06:26:08","title":"Large Language Models Can Automatically Engineer Features for Few-Shot Tabular Learning","abstract":"Large Language Models (LLMs), with their remarkable ability to tackle challenging and unseen reasoning problems, hold immense potential for tabular learning, that is vital for many real-world applications. In this paper, we propose a novel in-context learning framework, FeatLLM, which employs LLMs as feature engineers to produce an input data set that is optimally suited for tabular predictions. The generated features are used to infer class likelihood with a simple downstream machine learning model, such as linear regression and yields high performance few-shot learning. The proposed FeatLLM framework only uses this simple predictive model with the discovered features at inference time. Compared to existing LLM-based approaches, FeatLLM eliminates the need to send queries to the LLM for each sample at inference time. Moreover, it merely requires API-level access to LLMs, and overcomes prompt size limitations. As demonstrated across numerous tabular datasets from a wide range of domains, FeatLLM generates high-quality rules, significantly (10% on average) outperforming alternatives such as TabLLM and STUNT.","sentences":["Large Language Models (LLMs), with their remarkable ability to tackle challenging and unseen reasoning problems, hold immense potential for tabular learning, that is vital for many real-world applications.","In this paper, we propose a novel in-context learning framework, FeatLLM, which employs LLMs as feature engineers to produce an input data set that is optimally suited for tabular predictions.","The generated features are used to infer class likelihood with a simple downstream machine learning model, such as linear regression and yields high performance few-shot learning.","The proposed FeatLLM framework only uses this simple predictive model with the discovered features at inference time.","Compared to existing LLM-based approaches, FeatLLM eliminates the need to send queries to the LLM for each sample at inference time.","Moreover, it merely requires API-level access to LLMs, and overcomes prompt size limitations.","As demonstrated across numerous tabular datasets from a wide range of domains, FeatLLM generates high-quality rules, significantly (10% on average) outperforming alternatives such as TabLLM and STUNT."],"url":"http://arxiv.org/abs/2404.09491v1","category":"cs.LG"}
{"created":"2024-04-15 06:24:56","title":"Leveraging Temporal Contextualization for Video Action Recognition","abstract":"Pretrained vision-language models have shown effectiveness in video understanding. However, recent studies have not sufficiently leveraged essential temporal information from videos, simply averaging frame-wise representations or referencing consecutive frames. We introduce Temporally Contextualized CLIP (TC-CLIP), a pioneering framework for video understanding that effectively and efficiently leverages comprehensive video information. We propose Temporal Contextualization (TC), a novel layer-wise temporal information infusion mechanism for video that extracts core information from each frame, interconnects relevant information across the video to summarize into context tokens, and ultimately leverages the context tokens during the feature encoding process. Furthermore, our Video-conditional Prompting (VP) module manufactures context tokens to generate informative prompts in text modality. We conduct extensive experiments in zero-shot, few-shot, base-to-novel, and fully-supervised action recognition to validate the superiority of our TC-CLIP. Ablation studies for TC and VP guarantee our design choices. Code is available at https://github.com/naver-ai/tc-clip","sentences":["Pretrained vision-language models have shown effectiveness in video understanding.","However, recent studies have not sufficiently leveraged essential temporal information from videos, simply averaging frame-wise representations or referencing consecutive frames.","We introduce Temporally Contextualized CLIP (TC-CLIP), a pioneering framework for video understanding that effectively and efficiently leverages comprehensive video information.","We propose Temporal Contextualization (TC), a novel layer-wise temporal information infusion mechanism for video that extracts core information from each frame, interconnects relevant information across the video to summarize into context tokens, and ultimately leverages the context tokens during the feature encoding process.","Furthermore, our Video-conditional Prompting (VP) module manufactures context tokens to generate informative prompts in text modality.","We conduct extensive experiments in zero-shot, few-shot, base-to-novel, and fully-supervised action recognition to validate the superiority of our TC-CLIP.","Ablation studies for TC and VP guarantee our design choices.","Code is available at https://github.com/naver-ai/tc-clip"],"url":"http://arxiv.org/abs/2404.09490v1","category":"cs.CV"}
{"created":"2024-04-15 06:24:05","title":"Existence and uniqueness of limits at infinity for bounded variation functions","abstract":"In this paper, we study the existence of limits at infinity along almost every infinite curve for the upper and lower approximate limits of bounded variation functions on complete unbounded metric measure spaces. We prove that if the measure is doubling and supports a $1$-Poincar\\'e inequality, then for every bounded variation function $f$ and for $1$-a.e. infinite curve $\\gamma$, for both the upper approximate limit $f^\\vee$ and the lower approximate limit $f^\\wedge$ we have that \\[ \\lim_{t\\to+\\infty}f^\\vee(\\gamma(t)) {\\rm \\ \\ and\\ \\ }\\lim_{t\\to+\\infty}f^\\wedge(\\gamma(t)) \\] exist and are equal to the same finite value. We give examples showing that the conditions of doubling and a $1$-Poincar\\'e inequality are also necessary for the existence of limits. Furthermore, we establish a characterization for strictly positive $1$-modulus of the family of all infinite curves in terms of bounded variation functions. These generalize results for Sobolev functions given in \\cite{KN23}.","sentences":["In this paper, we study the existence of limits at infinity along almost every infinite curve for the upper and lower approximate limits of bounded variation functions on complete unbounded metric measure spaces.","We prove that if the measure is doubling and supports a $1$-Poincar\\'e inequality, then for every bounded variation function $f$ and for $1$-a.e.","infinite curve $\\gamma$, for both the upper approximate limit $f^\\vee$ and the lower approximate limit $f^\\wedge$ we have that \\[ \\lim_{t\\to+\\infty}f^\\vee(\\gamma(t))","{\\rm \\ \\ and\\ \\ }\\lim_{t\\to+\\infty}f^\\wedge(\\gamma(t))","\\] exist and are equal to the same finite value.","We give examples showing that the conditions of doubling and a $1$-Poincar\\'e inequality are also necessary for the existence of limits.","Furthermore, we establish a characterization for strictly positive $1$-modulus of the family of all infinite curves in terms of bounded variation functions.","These generalize results for Sobolev functions given in \\cite{KN23}."],"url":"http://arxiv.org/abs/2404.09489v1","category":"math.FA"}
{"created":"2024-04-15 06:21:28","title":"Quantum master equation and Hodge correlators","abstract":"We give a generalization of Goncharov's Hodge correlator twistor connection. Our generalized version is a connection 1-form with values in a DG Lie algebra of uni-trivalent graphs which may have loops and satisfies some Maurer--Cartan equation. This connection and the Maurer--Cartan equation can be viewed as an arithmetic analogue of effective action and quantum master equation respectively in non-acyclic Chern--Simons perturbation theory associated with the trivial local system.","sentences":["We give a generalization of Goncharov's Hodge correlator twistor connection.","Our generalized version is a connection 1-form with values in a DG Lie algebra of uni-trivalent graphs which may have loops and satisfies some Maurer--Cartan equation.","This connection and the Maurer--Cartan equation can be viewed as an arithmetic analogue of effective action and quantum master equation respectively in non-acyclic Chern--Simons perturbation theory associated with the trivial local system."],"url":"http://arxiv.org/abs/2404.09488v1","category":"math.GT"}
{"created":"2024-04-15 06:20:35","title":"The Shaping of Flying Qubits based on Quantum Optimal Control Theory","abstract":"The control of flying qubits carried by itinerant photons is ubiquitous in quantum communication networks. In addition to their logical states, the shape of flying qubits must also be tailored to match the remote receiver. In this paper, we introduce the quantum optimal control theory to the design of flying-qubit shaping protocols. A gradient-based algorithm is proposed for the generation of arbitrary-shape flying qubits with general non-ideal emitters. Simulations show that, as a joint control with the traditionally used tunable coupler, coherent driving fields can be applied to the shaping when the coupling strength is fixed or limited. The optimized control protocols can effectively suppress unwanted level leakage and multi-photon radiation. The method provides a systematic approach to high-fidelity control of flying qubits using realistic quantum devices.","sentences":["The control of flying qubits carried by itinerant photons is ubiquitous in quantum communication networks.","In addition to their logical states, the shape of flying qubits must also be tailored to match the remote receiver.","In this paper, we introduce the quantum optimal control theory to the design of flying-qubit shaping protocols.","A gradient-based algorithm is proposed for the generation of arbitrary-shape flying qubits with general non-ideal emitters.","Simulations show that, as a joint control with the traditionally used tunable coupler, coherent driving fields can be applied to the shaping when the coupling strength is fixed or limited.","The optimized control protocols can effectively suppress unwanted level leakage and multi-photon radiation.","The method provides a systematic approach to high-fidelity control of flying qubits using realistic quantum devices."],"url":"http://arxiv.org/abs/2404.09487v1","category":"quant-ph"}
{"created":"2024-04-15 06:15:46","title":"MMCode: Evaluating Multi-Modal Code Large Language Models with Visually Rich Programming Problems","abstract":"Programming often involves converting detailed and complex specifications into code, a process during which developers typically utilize visual aids to more effectively convey concepts. While recent developments in Large Multimodal Models have demonstrated remarkable abilities in visual reasoning and mathematical tasks, there is little work on investigating whether these models can effectively interpret visual elements for code generation. To this end, we present MMCode, the first multi-modal coding dataset for evaluating algorithmic problem-solving skills in visually rich contexts. MMCode contains 3,548 questions and 6,620 images collected from real-world programming challenges harvested from 10 code competition websites, presenting significant challenges due to the extreme demand for reasoning abilities. Our experiment results show that current state-of-the-art models struggle to solve these problems. The results highlight the lack of powerful vision-code models, and we hope MMCode can serve as an inspiration for future works in this domain. The data and code are publicly available at https://github.com/happylkx/MMCode.","sentences":["Programming often involves converting detailed and complex specifications into code, a process during which developers typically utilize visual aids to more effectively convey concepts.","While recent developments in Large Multimodal Models have demonstrated remarkable abilities in visual reasoning and mathematical tasks, there is little work on investigating whether these models can effectively interpret visual elements for code generation.","To this end, we present MMCode, the first multi-modal coding dataset for evaluating algorithmic problem-solving skills in visually rich contexts.","MMCode contains 3,548 questions and 6,620 images collected from real-world programming challenges harvested from 10 code competition websites, presenting significant challenges due to the extreme demand for reasoning abilities.","Our experiment results show that current state-of-the-art models struggle to solve these problems.","The results highlight the lack of powerful vision-code models, and we hope MMCode can serve as an inspiration for future works in this domain.","The data and code are publicly available at https://github.com/happylkx/MMCode."],"url":"http://arxiv.org/abs/2404.09486v1","category":"cs.CL"}
{"created":"2024-04-15 06:15:22","title":"Fukaya's immersed Lagrangian Floer theory and microlocalization of Fukaya category","abstract":"Let $\\mathfrak{Fuk}(T^*M)$ be the Fukaya category in the Fukaya's immersed Lagrangian Floer theory \\cite{fukaya:immersed} which is generated by immersed Lagrangian submanifolds with clean self-intersections. This category is monoidal in that the product of two such immersed Lagrangian submanifolds remains to be a Lagrangian immersion with clean self-intersection. Utilizing this monoidality of Fukaya's immersed Lagrangian Floer theory, we prove the following generation result in this Fukaya category of the cotangent bundle, which is the counterpart of Nadler's generation result \\cite{Nadler} for the Fukaya category generated by the exact embedded Lagrangian branes. More specifically we prove that for a given triangulation $\\mathcal T = \\{\\tau_{\\mathfrak a}\\}$ fine enough the Yoneda module $$ \\mathcal{Y}_{\\mathbb L}: = hom_{\\mathfrak{Fuk}(T^*M)}(\\cdot, \\mathbb L) $$ can be expressed as a twisted complex with terms $hom_{\\mathfrak{Fuk}(T^*M)}(\\alpha_M(\\cdot), L_{\\tau_{\\mathfrak a}*})$ for any curvature-free (aka tatologically unostructed) object $\\mathbb L$. Using this, we also extend Nadler's equivalence theorem between the dg category $Sh_c(M)$ of constructible sheaves on $M$ and the triangulated envelope of $\\mathfrak{Fuk}^0(T^*M)$ to the one over the Novikov field $\\mathbb K$.","sentences":["Let $\\mathfrak{Fuk}(T^*M)$ be the Fukaya category in the Fukaya's immersed Lagrangian Floer theory \\cite{fukaya:immersed} which is generated by immersed Lagrangian submanifolds with clean self-intersections.","This category is monoidal in that the product of two such immersed Lagrangian submanifolds remains to be a Lagrangian immersion with clean self-intersection.","Utilizing this monoidality of Fukaya's immersed Lagrangian Floer theory, we prove the following generation result in this Fukaya category of the cotangent bundle, which is the counterpart of Nadler's generation result \\cite{Nadler} for the Fukaya category generated by the exact embedded Lagrangian branes.","More specifically we prove that for a given triangulation $\\mathcal T = \\{\\tau_{\\mathfrak a}\\}$ fine enough the Yoneda module $$ \\mathcal{Y}_{\\mathbb L}: = hom_{\\mathfrak{Fuk}(T^*M)}(\\cdot, \\mathbb L) $$ can be expressed as a twisted complex with terms $hom_{\\mathfrak{Fuk}(T^*M)}(\\alpha_M(\\cdot), L_{\\tau_{\\mathfrak a}*})$ for any curvature-free (aka tatologically unostructed) object $\\mathbb L$. Using this, we also extend Nadler's equivalence theorem between the dg category $Sh_c(M)$ of constructible sheaves on $M$ and the triangulated envelope of $\\mathfrak{Fuk}^0(T^*M)$ to the one over the Novikov field $\\mathbb K$."],"url":"http://arxiv.org/abs/2404.09485v1","category":"math.SG"}
{"created":"2024-04-15 06:06:43","title":"Mitigating Hallucination in Abstractive Summarization with Domain-Conditional Mutual Information","abstract":"A primary challenge in abstractive summarization is hallucination -- the phenomenon where a model generates plausible text that is absent in the source text. We hypothesize that the domain (or topic) of the source text triggers the model to generate text that is highly probable in the domain, neglecting the details of the source text. To alleviate this model bias, we introduce a decoding strategy based on domain-conditional pointwise mutual information. This strategy adjusts the generation probability of each token by comparing it with the token's marginal probability within the domain of the source text. According to evaluation on the XSUM dataset, our method demonstrates improvement in terms of faithfulness and source relevance. The code is publicly available at \\url{https://github.com/qqplot/dcpmi}.","sentences":["A primary challenge in abstractive summarization is hallucination -- the phenomenon where a model generates plausible text that is absent in the source text.","We hypothesize that the domain (or topic) of the source text triggers the model to generate text that is highly probable in the domain, neglecting the details of the source text.","To alleviate this model bias, we introduce a decoding strategy based on domain-conditional pointwise mutual information.","This strategy adjusts the generation probability of each token by comparing it with the token's marginal probability within the domain of the source text.","According to evaluation on the XSUM dataset, our method demonstrates improvement in terms of faithfulness and source relevance.","The code is publicly available at \\url{https://github.com/qqplot/dcpmi}."],"url":"http://arxiv.org/abs/2404.09480v1","category":"cs.CL"}
{"created":"2024-04-15 06:05:39","title":"A Legal Risk Taxonomy for Generative Artificial Intelligence","abstract":"For the first time, this paper presents a taxonomy of legal risks associated with generative AI (GenAI) by breaking down complex legal concepts to provide a common understanding of potential legal challenges for developing and deploying GenAI models. The methodology is based on (1) examining the legal claims that have been filed in existing lawsuits and (2) evaluating the reasonably foreseeable legal claims that may be filed in future lawsuits. First, we identified 22 lawsuits against prominent GenAI entities and tallied the claims of each lawsuit. From there, we identified seven claims that are cited at least four times across these lawsuits as the most likely claims for future GenAI lawsuits. For each of these seven claims, we describe the elements of the claim (what the plaintiff must prove to prevail) and provide an example of how it may apply to GenAI. Next, we identified 30 other potential claims that we consider to be more speculative, because they have been included in fewer than four lawsuits or have yet to be filed. We further separated those 30 claims into 19 that are most likely to be made in relation to pre-deployment of GenAI models and 11 that are more likely to be made in connection with post-deployment of GenAI models since the legal risks will vary between entities that create versus deploy them. For each of these claims, we describe the elements of the claim and the potential remedies that plaintiffs may seek to help entities determine their legal risks in developing or deploying GenAI. Lastly, we close the paper by noting the novelty of GenAI technology and propose some applications for the paper's taxonomy in driving further research.","sentences":["For the first time, this paper presents a taxonomy of legal risks associated with generative AI (GenAI) by breaking down complex legal concepts to provide a common understanding of potential legal challenges for developing and deploying GenAI models.","The methodology is based on (1) examining the legal claims that have been filed in existing lawsuits and (2) evaluating the reasonably foreseeable legal claims that may be filed in future lawsuits.","First, we identified 22 lawsuits against prominent GenAI entities and tallied the claims of each lawsuit.","From there, we identified seven claims that are cited at least four times across these lawsuits as the most likely claims for future GenAI lawsuits.","For each of these seven claims, we describe the elements of the claim (what the plaintiff must prove to prevail) and provide an example of how it may apply to GenAI.","Next, we identified 30 other potential claims that we consider to be more speculative, because they have been included in fewer than four lawsuits or have yet to be filed.","We further separated those 30 claims into 19 that are most likely to be made in relation to pre-deployment of GenAI models and 11 that are more likely to be made in connection with post-deployment of GenAI models since the legal risks will vary between entities that create versus deploy them.","For each of these claims, we describe the elements of the claim and the potential remedies that plaintiffs may seek to help entities determine their legal risks in developing or deploying GenAI.","Lastly, we close the paper by noting the novelty of GenAI technology and propose some applications for the paper's taxonomy in driving further research."],"url":"http://arxiv.org/abs/2404.09479v1","category":"cs.CY"}
{"created":"2024-04-15 06:02:09","title":"Improving Weakly-Supervised Object Localization Using Adversarial Erasing and Pseudo Label","abstract":"Weakly-supervised learning approaches have gained significant attention due to their ability to reduce the effort required for human annotations in training neural networks. This paper investigates a framework for weakly-supervised object localization, which aims to train a neural network capable of predicting both the object class and its location using only images and their image-level class labels. The proposed framework consists of a shared feature extractor, a classifier, and a localizer. The localizer predicts pixel-level class probabilities, while the classifier predicts the object class at the image level. Since image-level class labels are insufficient for training the localizer, weakly-supervised object localization methods often encounter challenges in accurately localizing the entire object region. To address this issue, the proposed method incorporates adversarial erasing and pseudo labels to improve localization accuracy. Specifically, novel losses are designed to utilize adversarially erased foreground features and adversarially erased feature maps, reducing dependence on the most discriminative region. Additionally, the proposed method employs pseudo labels to suppress activation values in the background while increasing them in the foreground. The proposed method is applied to two backbone networks (MobileNetV1 and InceptionV3) and is evaluated on three publicly available datasets (ILSVRC-2012, CUB-200-2011, and PASCAL VOC 2012). The experimental results demonstrate that the proposed method outperforms previous state-of-the-art methods across all evaluated metrics.","sentences":["Weakly-supervised learning approaches have gained significant attention due to their ability to reduce the effort required for human annotations in training neural networks.","This paper investigates a framework for weakly-supervised object localization, which aims to train a neural network capable of predicting both the object class and its location using only images and their image-level class labels.","The proposed framework consists of a shared feature extractor, a classifier, and a localizer.","The localizer predicts pixel-level class probabilities, while the classifier predicts the object class at the image level.","Since image-level class labels are insufficient for training the localizer, weakly-supervised object localization methods often encounter challenges in accurately localizing the entire object region.","To address this issue, the proposed method incorporates adversarial erasing and pseudo labels to improve localization accuracy.","Specifically, novel losses are designed to utilize adversarially erased foreground features and adversarially erased feature maps, reducing dependence on the most discriminative region.","Additionally, the proposed method employs pseudo labels to suppress activation values in the background while increasing them in the foreground.","The proposed method is applied to two backbone networks (MobileNetV1 and InceptionV3) and is evaluated on three publicly available datasets (ILSVRC-2012, CUB-200-2011, and PASCAL VOC 2012).","The experimental results demonstrate that the proposed method outperforms previous state-of-the-art methods across all evaluated metrics."],"url":"http://arxiv.org/abs/2404.09475v1","category":"cs.CV"}
{"created":"2024-04-15 05:56:13","title":"Exploring the Nexus Between Retrievability and Query Generation Strategies","abstract":"Quantifying bias in retrieval functions through document retrievability scores is vital for assessing recall-oriented retrieval systems. However, many studies investigating retrieval model bias lack validation of their query generation methods as accurate representations of retrievability for real users and their queries. This limitation results from the absence of established criteria for query generation in retrievability assessments. Typically, researchers resort to using frequent collocations from document corpora when no query log is available. In this study, we address the issue of reproducibility and seek to validate query generation methods by comparing retrievability scores generated from artificially generated queries to those derived from query logs. Our findings demonstrate a minimal or negligible correlation between retrievability scores from artificial queries and those from query logs. This suggests that artificially generated queries may not accurately reflect retrievability scores as derived from query logs. We further explore alternative query generation techniques, uncovering a variation that exhibits the highest correlation. This alternative approach holds promise for improving reproducibility when query logs are unavailable.","sentences":["Quantifying bias in retrieval functions through document retrievability scores is vital for assessing recall-oriented retrieval systems.","However, many studies investigating retrieval model bias lack validation of their query generation methods as accurate representations of retrievability for real users and their queries.","This limitation results from the absence of established criteria for query generation in retrievability assessments.","Typically, researchers resort to using frequent collocations from document corpora when no query log is available.","In this study, we address the issue of reproducibility and seek to validate query generation methods by comparing retrievability scores generated from artificially generated queries to those derived from query logs.","Our findings demonstrate a minimal or negligible correlation between retrievability scores from artificial queries and those from query logs.","This suggests that artificially generated queries may not accurately reflect retrievability scores as derived from query logs.","We further explore alternative query generation techniques, uncovering a variation that exhibits the highest correlation.","This alternative approach holds promise for improving reproducibility when query logs are unavailable."],"url":"http://arxiv.org/abs/2404.09473v1","category":"cs.IR"}
{"created":"2024-04-15 05:53:26","title":"Q2A: Querying Implicit Fully Continuous Feature Pyramid to Align Features for Medical Image Segmentation","abstract":"Recent medical image segmentation methods apply implicit neural representation (INR) to the decoder for achieving a continuous coordinate decoding to tackle the drawback of conventional discrete grid-based data representations. However, the INR-based decoder cannot well handle the feature misalignment problem brought about by the naive latent code acquisition strategy in INR. Although there exist many feature alignment works, they all adopt a progressive multi-step aligning paradigm on a discrete feature pyramid, which is incompatible with the continuous one-step characteristics of INR-based decoder, and thus fails to be the solution. Therefore, we propose Q2A, a novel one-step query-based aligning paradigm, to solve the feature misalignment problem in the INR-based decoder. Specifically, for each target coordinate, Q2A first generates several queries depicting the spatial offsets and the cell resolutions of the contextual features aligned to the coordinate, then calculates the corresponding aligned features by feeding the queries into a novel implicit fully continuous feature pyramid (FCFP), finally fuses the aligned features to predict the class distribution. In FCFP, we further propose a novel universal partition-and-aggregate strategy (P&A) to replace the naive interpolation strategy for latent code acquisition in INR, which mitigates the information loss problem that occurs when the query cell resolution is relatively large and achieves an effective feature decoding at arbitrary continuous resolution. We conduct extensive experiments on two medical datasets, i.e. Glas and Synapse, and a universal dataset, i.e. Cityscapes, and they show the superiority of the proposed Q2A.","sentences":["Recent medical image segmentation methods apply implicit neural representation (INR) to the decoder for achieving a continuous coordinate decoding to tackle the drawback of conventional discrete grid-based data representations.","However, the INR-based decoder cannot well handle the feature misalignment problem brought about by the naive latent code acquisition strategy in INR.","Although there exist many feature alignment works, they all adopt a progressive multi-step aligning paradigm on a discrete feature pyramid, which is incompatible with the continuous one-step characteristics of INR-based decoder, and thus fails to be the solution.","Therefore, we propose Q2A, a novel one-step query-based aligning paradigm, to solve the feature misalignment problem in the INR-based decoder.","Specifically, for each target coordinate, Q2A first generates several queries depicting the spatial offsets and the cell resolutions of the contextual features aligned to the coordinate, then calculates the corresponding aligned features by feeding the queries into a novel implicit fully continuous feature pyramid (FCFP), finally fuses the aligned features to predict the class distribution.","In FCFP, we further propose a novel universal partition-and-aggregate strategy (P&A) to replace the naive interpolation strategy for latent code acquisition in INR, which mitigates the information loss problem that occurs when the query cell resolution is relatively large and achieves an effective feature decoding at arbitrary continuous resolution.","We conduct extensive experiments on two medical datasets, i.e. Glas and Synapse, and a universal dataset, i.e. Cityscapes, and they show the superiority of the proposed Q2A."],"url":"http://arxiv.org/abs/2404.09472v1","category":"cs.CV"}
{"created":"2024-04-15 05:50:46","title":"LatticeML: A data-driven application for predicting the effective Young Modulus of high temperature graph based architected materials","abstract":"Architected materials with their unique topology and geometry offer the potential to modify physical and mechanical properties. Machine learning can accelerate the design and optimization of these materials by identifying optimal designs and forecasting performance. This work presents LatticeML, a data-driven application for predicting the effective Young's Modulus of high-temperature graph-based architected materials. The study considers eleven graph-based lattice structures with two high-temperature alloys, Ti-6Al-4V and Inconel 625. Finite element simulations were used to compute the effective Young's Modulus of the 2x2x2 unit cell configurations. A machine learning framework was developed to predict Young's Modulus, involving data collection, preprocessing, implementation of regression models, and deployment of the best-performing model. Five supervised learning algorithms were evaluated, with the XGBoost Regressor achieving the highest accuracy (MSE = 2.7993, MAE = 1.1521, R-squared = 0.9875). The application uses the Streamlit framework to create an interactive web interface, allowing users to input material and geometric parameters and obtain predicted Young's Modulus values.","sentences":["Architected materials with their unique topology and geometry offer the potential to modify physical and mechanical properties.","Machine learning can accelerate the design and optimization of these materials by identifying optimal designs and forecasting performance.","This work presents LatticeML, a data-driven application for predicting the effective Young's Modulus of high-temperature graph-based architected materials.","The study considers eleven graph-based lattice structures with two high-temperature alloys, Ti-6Al-4V and Inconel 625.","Finite element simulations were used to compute the effective Young's Modulus of the 2x2x2 unit cell configurations.","A machine learning framework was developed to predict Young's Modulus, involving data collection, preprocessing, implementation of regression models, and deployment of the best-performing model.","Five supervised learning algorithms were evaluated, with the XGBoost Regressor achieving the highest accuracy (MSE = 2.7993, MAE = 1.1521, R-squared = 0.9875).","The application uses the Streamlit framework to create an interactive web interface, allowing users to input material and geometric parameters and obtain predicted Young's Modulus values."],"url":"http://arxiv.org/abs/2404.09470v1","category":"cs.LG"}
{"created":"2024-04-15 05:44:03","title":"Virtually Enriched NYU Depth V2 Dataset for Monocular Depth Estimation: Do We Need Artificial Augmentation?","abstract":"We present ANYU, a new virtually augmented version of the NYU depth v2 dataset, designed for monocular depth estimation. In contrast to the well-known approach where full 3D scenes of a virtual world are utilized to generate artificial datasets, ANYU was created by incorporating RGB-D representations of virtual reality objects into the original NYU depth v2 images. We specifically did not match each generated virtual object with an appropriate texture and a suitable location within the real-world image. Instead, an assignment of texture, location, lighting, and other rendering parameters was randomized to maximize a diversity of the training data, and to show that it is randomness that can improve the generalizing ability of a dataset. By conducting extensive experiments with our virtually modified dataset and validating on the original NYU depth v2 and iBims-1 benchmarks, we show that ANYU improves the monocular depth estimation performance and generalization of deep neural networks with considerably different architectures, especially for the current state-of-the-art VPD model. To the best of our knowledge, this is the first work that augments a real-world dataset with randomly generated virtual 3D objects for monocular depth estimation. We make our ANYU dataset publicly available in two training configurations with 10% and 100% additional synthetically enriched RGB-D pairs of training images, respectively, for efficient training and empirical exploration of virtual augmentation at https://github.com/ABrain-One/ANYU","sentences":["We present ANYU, a new virtually augmented version of the NYU depth v2 dataset, designed for monocular depth estimation.","In contrast to the well-known approach where full 3D scenes of a virtual world are utilized to generate artificial datasets, ANYU was created by incorporating RGB-D representations of virtual reality objects into the original NYU depth v2 images.","We specifically did not match each generated virtual object with an appropriate texture and a suitable location within the real-world image.","Instead, an assignment of texture, location, lighting, and other rendering parameters was randomized to maximize a diversity of the training data, and to show that it is randomness that can improve the generalizing ability of a dataset.","By conducting extensive experiments with our virtually modified dataset and validating on the original NYU depth v2 and iBims-1 benchmarks, we show that ANYU improves the monocular depth estimation performance and generalization of deep neural networks with considerably different architectures, especially for the current state-of-the-art VPD model.","To the best of our knowledge, this is the first work that augments a real-world dataset with randomly generated virtual 3D objects for monocular depth estimation.","We make our ANYU dataset publicly available in two training configurations with 10% and 100% additional synthetically enriched RGB-D pairs of training images, respectively, for efficient training and empirical exploration of virtual augmentation at https://github.com/ABrain-One/ANYU"],"url":"http://arxiv.org/abs/2404.09469v1","category":"cs.CV"}
{"created":"2024-04-15 05:40:41","title":"MyGO: Discrete Modality Information as Fine-Grained Tokens for Multi-modal Knowledge Graph Completion","abstract":"Multi-modal knowledge graphs (MMKG) store structured world knowledge containing rich multi-modal descriptive information. To overcome their inherent incompleteness, multi-modal knowledge graph completion (MMKGC) aims to discover unobserved knowledge from given MMKGs, leveraging both structural information from the triples and multi-modal information of the entities. Existing MMKGC methods usually extract multi-modal features with pre-trained models and employ a fusion module to integrate multi-modal features with triple prediction. However, this often results in a coarse handling of multi-modal data, overlooking the nuanced, fine-grained semantic details and their interactions. To tackle this shortfall, we introduce a novel framework MyGO to process, fuse, and augment the fine-grained modality information from MMKGs. MyGO tokenizes multi-modal raw data as fine-grained discrete tokens and learns entity representations with a cross-modal entity encoder. To further augment the multi-modal representations, MyGO incorporates fine-grained contrastive learning to highlight the specificity of the entity representations. Experiments on standard MMKGC benchmarks reveal that our method surpasses 20 of the latest models, underlining its superior performance. Code and data are available at https://github.com/zjukg/MyGO","sentences":["Multi-modal knowledge graphs (MMKG) store structured world knowledge containing rich multi-modal descriptive information.","To overcome their inherent incompleteness, multi-modal knowledge graph completion (MMKGC) aims to discover unobserved knowledge from given MMKGs, leveraging both structural information from the triples and multi-modal information of the entities.","Existing MMKGC methods usually extract multi-modal features with pre-trained models and employ a fusion module to integrate multi-modal features with triple prediction.","However, this often results in a coarse handling of multi-modal data, overlooking the nuanced, fine-grained semantic details and their interactions.","To tackle this shortfall, we introduce a novel framework MyGO to process, fuse, and augment the fine-grained modality information from MMKGs.","MyGO tokenizes multi-modal raw data as fine-grained discrete tokens and learns entity representations with a cross-modal entity encoder.","To further augment the multi-modal representations, MyGO incorporates fine-grained contrastive learning to highlight the specificity of the entity representations.","Experiments on standard MMKGC benchmarks reveal that our method surpasses 20 of the latest models, underlining its superior performance.","Code and data are available at https://github.com/zjukg/MyGO"],"url":"http://arxiv.org/abs/2404.09468v1","category":"cs.AI"}
{"created":"2024-04-15 05:29:23","title":"PhyScene: Physically Interactable 3D Scene Synthesis for Embodied AI","abstract":"With recent developments in Embodied Artificial Intelligence (EAI) research, there has been a growing demand for high-quality, large-scale interactive scene generation. While prior methods in scene synthesis have prioritized the naturalness and realism of the generated scenes, the physical plausibility and interactivity of scenes have been largely left unexplored. To address this disparity, we introduce PhyScene, a novel method dedicated to generating interactive 3D scenes characterized by realistic layouts, articulated objects, and rich physical interactivity tailored for embodied agents. Based on a conditional diffusion model for capturing scene layouts, we devise novel physics- and interactivity-based guidance mechanisms that integrate constraints from object collision, room layout, and object reachability. Through extensive experiments, we demonstrate that PhyScene effectively leverages these guidance functions for physically interactable scene synthesis, outperforming existing state-of-the-art scene synthesis methods by a large margin. Our findings suggest that the scenes generated by PhyScene hold considerable potential for facilitating diverse skill acquisition among agents within interactive environments, thereby catalyzing further advancements in embodied AI research. Project website: http://physcene.github.io.","sentences":["With recent developments in Embodied Artificial Intelligence (EAI) research, there has been a growing demand for high-quality, large-scale interactive scene generation.","While prior methods in scene synthesis have prioritized the naturalness and realism of the generated scenes, the physical plausibility and interactivity of scenes have been largely left unexplored.","To address this disparity, we introduce PhyScene, a novel method dedicated to generating interactive 3D scenes characterized by realistic layouts, articulated objects, and rich physical interactivity tailored for embodied agents.","Based on a conditional diffusion model for capturing scene layouts, we devise novel physics- and interactivity-based guidance mechanisms that integrate constraints from object collision, room layout, and object reachability.","Through extensive experiments, we demonstrate that PhyScene effectively leverages these guidance functions for physically interactable scene synthesis, outperforming existing state-of-the-art scene synthesis methods by a large margin.","Our findings suggest that the scenes generated by PhyScene hold considerable potential for facilitating diverse skill acquisition among agents within interactive environments, thereby catalyzing further advancements in embodied AI research.","Project website: http://physcene.github.io."],"url":"http://arxiv.org/abs/2404.09465v1","category":"cs.CV"}
{"created":"2024-04-15 05:15:29","title":"Information Gain, Operator Spreading, and Sensitivity to Perturbations as Quantifiers of Chaos in Quantum Systems","abstract":"We adopt a continuous weak measurement tomography protocol to explore the signatures of chaos in the quantum system(s). We generate the measurement record as a series of expectation values of an observable evolving under the desired dynamics, which can show a transition from integrability to chaos. We find that the rate of information gain depends on the degree of chaos in the dynamics, the choice of initial observable, and how well the operator is aligned along the density matrix. The amount of operator spreading in the Krylov subspace, as quantified by the fidelity in quantum tomography and various other metrics of information gain, increases with the degree of chaos in the system. We study operator spreading in many-body quantum systems by its potential to generate an informationally complete measurement record. Our quantifiers for operator spreading are more consistent indicators of quantum chaos than Krylov complexity. Our study gives an operational interpretation for operator spreading in terms of fidelity gain in quantum tomography.   Continuing in our journey of finding the footprints of chaos in the quantum domain, we explore the growth of errors in noisy tomography. For random states, when the measurement record is obtained from a random operator, the subsequent drop in the fidelity obtained is inversely correlated to the degree of chaos in the dynamics. This gives us an operational interpretation of Loschmidt echo for operators by connecting it to the performance of quantum tomography. We find a quantity to capture the scrambling of errors, an out-of-time-ordered correlator (OTOC) between two operators under perturbed and unperturbed dynamics that serves as a signature of chaos. Our results demonstrate a fundamental link between Loschmidt echo and scrambling of errors, as captured by OTOCs, with operational consequences in quantum information processing.","sentences":["We adopt a continuous weak measurement tomography protocol to explore the signatures of chaos in the quantum system(s).","We generate the measurement record as a series of expectation values of an observable evolving under the desired dynamics, which can show a transition from integrability to chaos.","We find that the rate of information gain depends on the degree of chaos in the dynamics, the choice of initial observable, and how well the operator is aligned along the density matrix.","The amount of operator spreading in the Krylov subspace, as quantified by the fidelity in quantum tomography and various other metrics of information gain, increases with the degree of chaos in the system.","We study operator spreading in many-body quantum systems by its potential to generate an informationally complete measurement record.","Our quantifiers for operator spreading are more consistent indicators of quantum chaos than Krylov complexity.","Our study gives an operational interpretation for operator spreading in terms of fidelity gain in quantum tomography.   ","Continuing in our journey of finding the footprints of chaos in the quantum domain, we explore the growth of errors in noisy tomography.","For random states, when the measurement record is obtained from a random operator, the subsequent drop in the fidelity obtained is inversely correlated to the degree of chaos in the dynamics.","This gives us an operational interpretation of Loschmidt echo for operators by connecting it to the performance of quantum tomography.","We find a quantity to capture the scrambling of errors, an out-of-time-ordered correlator (OTOC) between two operators under perturbed and unperturbed dynamics that serves as a signature of chaos.","Our results demonstrate a fundamental link between Loschmidt echo and scrambling of errors, as captured by OTOCs, with operational consequences in quantum information processing."],"url":"http://arxiv.org/abs/2404.09464v1","category":"quant-ph"}
{"created":"2024-04-15 05:14:52","title":"PRIME: A CyberGIS Platform for Resilience Inference Measurement and Enhancement","abstract":"In an era of increased climatic disasters, there is an urgent need to develop reliable frameworks and tools for evaluating and improving community resilience to climatic hazards at multiple geographical and temporal scales. Defining and quantifying resilience in the social domain is relatively subjective due to the intricate interplay of socioeconomic factors with disaster resilience. Meanwhile, there is a lack of computationally rigorous, user-friendly tools that can support customized resilience assessment considering local conditions. This study aims to address these gaps through the power of CyberGIS with three objectives: 1) To develop an empirically validated disaster resilience model - Customized Resilience Inference Measurement designed for multi-scale community resilience assessment and influential socioeconomic factors identification, 2) To implement a Platform for Resilience Inference Measurement and Enhancement module in the CyberGISX platform backed by high-performance computing, 3) To demonstrate the utility of PRIME through a representative study. CRIM generates vulnerability, adaptability, and overall resilience scores derived from empirical hazard parameters. Computationally intensive Machine Learning methods are employed to explain the intricate relationships between these scores and socioeconomic driving factors. PRIME provides a web-based notebook interface guiding users to select study areas, configure parameters, calculate and geo-visualize resilience scores, and interpret socioeconomic factors shaping resilience capacities. A representative study showcases the efficiency of the platform while explaining how the visual results obtained may be interpreted. The essence of this work lies in its comprehensive architecture that encapsulates the requisite data, analytical and geo-visualization functions, and ML models for resilience assessment.","sentences":["In an era of increased climatic disasters, there is an urgent need to develop reliable frameworks and tools for evaluating and improving community resilience to climatic hazards at multiple geographical and temporal scales.","Defining and quantifying resilience in the social domain is relatively subjective due to the intricate interplay of socioeconomic factors with disaster resilience.","Meanwhile, there is a lack of computationally rigorous, user-friendly tools that can support customized resilience assessment considering local conditions.","This study aims to address these gaps through the power of CyberGIS with three objectives: 1) To develop an empirically validated disaster resilience model - Customized Resilience Inference Measurement designed for multi-scale community resilience assessment and influential socioeconomic factors identification, 2) To implement a Platform for Resilience Inference Measurement and Enhancement module in the CyberGISX platform backed by high-performance computing, 3) To demonstrate the utility of PRIME through a representative study.","CRIM generates vulnerability, adaptability, and overall resilience scores derived from empirical hazard parameters.","Computationally intensive Machine Learning methods are employed to explain the intricate relationships between these scores and socioeconomic driving factors.","PRIME provides a web-based notebook interface guiding users to select study areas, configure parameters, calculate and geo-visualize resilience scores, and interpret socioeconomic factors shaping resilience capacities.","A representative study showcases the efficiency of the platform while explaining how the visual results obtained may be interpreted.","The essence of this work lies in its comprehensive architecture that encapsulates the requisite data, analytical and geo-visualization functions, and ML models for resilience assessment."],"url":"http://arxiv.org/abs/2404.09463v1","category":"cs.LG"}
{"created":"2024-04-15 05:11:07","title":"Experimental Analysis of Deep Hedging Using Artificial Market Simulations for Underlying Asset Simulators","abstract":"Derivative hedging and pricing are important and continuously studied topics in financial markets. Recently, deep hedging has been proposed as a promising approach that uses deep learning to approximate the optimal hedging strategy and can handle incomplete markets. However, deep hedging usually requires underlying asset simulations, and it is challenging to select the best model for such simulations. This study proposes a new approach using artificial market simulations for underlying asset simulations in deep hedging. Artificial market simulations can replicate the stylized facts of financial markets, and they seem to be a promising approach for deep hedging. We investigate the effectiveness of the proposed approach by comparing its results with those of the traditional approach, which uses mathematical finance models such as Brownian motion and Heston models for underlying asset simulations. The results show that the proposed approach can achieve almost the same level of performance as the traditional approach without mathematical finance models. Finally, we also reveal that the proposed approach has some limitations in terms of performance under certain conditions.","sentences":["Derivative hedging and pricing are important and continuously studied topics in financial markets.","Recently, deep hedging has been proposed as a promising approach that uses deep learning to approximate the optimal hedging strategy and can handle incomplete markets.","However, deep hedging usually requires underlying asset simulations, and it is challenging to select the best model for such simulations.","This study proposes a new approach using artificial market simulations for underlying asset simulations in deep hedging.","Artificial market simulations can replicate the stylized facts of financial markets, and they seem to be a promising approach for deep hedging.","We investigate the effectiveness of the proposed approach by comparing its results with those of the traditional approach, which uses mathematical finance models such as Brownian motion and Heston models for underlying asset simulations.","The results show that the proposed approach can achieve almost the same level of performance as the traditional approach without mathematical finance models.","Finally, we also reveal that the proposed approach has some limitations in terms of performance under certain conditions."],"url":"http://arxiv.org/abs/2404.09462v1","category":"q-fin.CP"}
{"created":"2024-04-15 05:00:40","title":"Improved Object-Based Style Transfer with Single Deep Network","abstract":"This research paper proposes a novel methodology for image-to-image style transfer on objects utilizing a single deep convolutional neural network. The proposed approach leverages the You Only Look Once version 8 (YOLOv8) segmentation model and the backbone neural network of YOLOv8 for style transfer. The primary objective is to enhance the visual appeal of objects in images by seamlessly transferring artistic styles while preserving the original object characteristics. The proposed approach's novelty lies in combining segmentation and style transfer in a single deep convolutional neural network. This approach omits the need for multiple stages or models, thus resulting in simpler training and deployment of the model for practical applications. The results of this approach are shown on two content images by applying different style images. The paper also demonstrates the ability to apply style transfer on multiple objects in the same image.","sentences":["This research paper proposes a novel methodology for image-to-image style transfer on objects utilizing a single deep convolutional neural network.","The proposed approach leverages the You Only Look Once version 8 (YOLOv8) segmentation model and the backbone neural network of YOLOv8 for style transfer.","The primary objective is to enhance the visual appeal of objects in images by seamlessly transferring artistic styles while preserving the original object characteristics.","The proposed approach's novelty lies in combining segmentation and style transfer in a single deep convolutional neural network.","This approach omits the need for multiple stages or models, thus resulting in simpler training and deployment of the model for practical applications.","The results of this approach are shown on two content images by applying different style images.","The paper also demonstrates the ability to apply style transfer on multiple objects in the same image."],"url":"http://arxiv.org/abs/2404.09461v1","category":"cs.CV"}
{"created":"2024-04-15 04:53:21","title":"Fast randomized algorithms for low-rank matrix approximations with applications in global comparative analysis of a class of data sets","abstract":"Generalized singular values (GSVs) play an essential role in the comparative analysis. In the real world data for comparative analysis, both data matrices are usually numerically low-rank. This paper proposes a randomized algorithm to first approximately extract bases and then calculate GSVs efficiently. The accuracy of both basis extration and comparative analysis quantities, angular distances, generalized fractions of the eigenexpression, and generalized normalized Shannon entropy, are rigursly analyzed. The proposed algorithm is applied to both synthetic data sets and the genome-scale expression data sets. Comparing to other GSVs algorithms, the proposed algorithm achieves the fastest runtime while preserving sufficient accuracy in comparative analysis.","sentences":["Generalized singular values (GSVs) play an essential role in the comparative analysis.","In the real world data for comparative analysis, both data matrices are usually numerically low-rank.","This paper proposes a randomized algorithm to first approximately extract bases and then calculate GSVs efficiently.","The accuracy of both basis extration and comparative analysis quantities, angular distances, generalized fractions of the eigenexpression, and generalized normalized Shannon entropy, are rigursly analyzed.","The proposed algorithm is applied to both synthetic data sets and the genome-scale expression data sets.","Comparing to other GSVs algorithms, the proposed algorithm achieves the fastest runtime while preserving sufficient accuracy in comparative analysis."],"url":"http://arxiv.org/abs/2404.09459v1","category":"math.NA"}
{"created":"2024-04-15 04:35:09","title":"Python-Based Quantum Chemistry Calculations with GPU Acceleration","abstract":"To meet the increasing demand of quantum chemistry calculations in data-driven chemical research, the collaboration between industrial stakeholders and the quantum chemistry community has led to the development of GPU4PySCF, a GPU-accelerated Python package. This open-source project is accessible via its public GitHub repository at \\url{https://github.com/pyscf/gpu4pyscf}. This paper outlines the primary features, innovations, and advantages of this package. When performing Density Functional Theory (DFT) calculations on modern GPU platforms, GPU4PySCF delivers 30 times speedup over a 32-core CPU node, resulting in approximately 90% cost savings for most DFT tasks. The performance advantages and productivity improvements have been found in multiple industrial applications, such as generating potential energy surfaces, analyzing molecular properties, calculating solvation free energy, identifying chemical reactions in lithium-ion batteries, and accelerating neural-network methods. To make the package easy to extend and integrate with other Python packages, it is designed with PySCF-compatible interfaces and Pythonic implementations. This design choice enhances its coordination with the Python ecosystem.","sentences":["To meet the increasing demand of quantum chemistry calculations in data-driven chemical research, the collaboration between industrial stakeholders and the quantum chemistry community has led to the development of GPU4PySCF, a GPU-accelerated Python package.","This open-source project is accessible via its public GitHub repository at \\url{https://github.com/pyscf/gpu4pyscf}.","This paper outlines the primary features, innovations, and advantages of this package.","When performing Density Functional Theory (DFT) calculations on modern GPU platforms, GPU4PySCF delivers 30 times speedup over a 32-core CPU node, resulting in approximately 90% cost savings for most DFT tasks.","The performance advantages and productivity improvements have been found in multiple industrial applications, such as generating potential energy surfaces, analyzing molecular properties, calculating solvation free energy, identifying chemical reactions in lithium-ion batteries, and accelerating neural-network methods.","To make the package easy to extend and integrate with other Python packages, it is designed with PySCF-compatible interfaces and Pythonic implementations.","This design choice enhances its coordination with the Python ecosystem."],"url":"http://arxiv.org/abs/2404.09452v1","category":"physics.comp-ph"}
{"created":"2024-04-15 04:31:24","title":"Contrastive Mean-Shift Learning for Generalized Category Discovery","abstract":"We address the problem of generalized category discovery (GCD) that aims to partition a partially labeled collection of images; only a small part of the collection is labeled and the total number of target classes is unknown. To address this generalized image clustering problem, we revisit the mean-shift algorithm, i.e., a classic, powerful technique for mode seeking, and incorporate it into a contrastive learning framework. The proposed method, dubbed Contrastive Mean-Shift (CMS) learning, trains an image encoder to produce representations with better clustering properties by an iterative process of mean shift and contrastive update. Experiments demonstrate that our method, both in settings with and without the total number of clusters being known, achieves state-of-the-art performance on six public GCD benchmarks without bells and whistles.","sentences":["We address the problem of generalized category discovery (GCD) that aims to partition a partially labeled collection of images; only a small part of the collection is labeled and the total number of target classes is unknown.","To address this generalized image clustering problem, we revisit the mean-shift algorithm, i.e., a classic, powerful technique for mode seeking, and incorporate it into a contrastive learning framework.","The proposed method, dubbed Contrastive Mean-Shift (CMS) learning, trains an image encoder to produce representations with better clustering properties by an iterative process of mean shift and contrastive update.","Experiments demonstrate that our method, both in settings with and without the total number of clusters being known, achieves state-of-the-art performance on six public GCD benchmarks without bells and whistles."],"url":"http://arxiv.org/abs/2404.09451v1","category":"cs.CV"}
{"created":"2024-04-15 04:17:57","title":"The final burst of the moving mirror is unrelated to the partner mode of analog Hawking radiation","abstract":"Flying mirrors with appropriate trajectories have been recognized as an analog system that mimics black hole Hawking evaporation and have been widely investigated. It has recently been suggested that the partner mode of the analog Hawking radiation emitted from a moving mirror would manifest itself through a final burst when the mirror executes a sudden stop. Here we argue the opposite via the partner formula for the moving mirror model. By expanding the theoretical foundation of the partner formula and augmenting it with numerical analysis, we demonstrate that the supposed final burst is induced by a shock that requires the input of external energy, whereas the Hawking radiation partner mode, which is associated with the zero-point vacuum fluctuations, is not responsible for the burst.","sentences":["Flying mirrors with appropriate trajectories have been recognized as an analog system that mimics black hole Hawking evaporation and have been widely investigated.","It has recently been suggested that the partner mode of the analog Hawking radiation emitted from a moving mirror would manifest itself through a final burst when the mirror executes a sudden stop.","Here we argue the opposite via the partner formula for the moving mirror model.","By expanding the theoretical foundation of the partner formula and augmenting it with numerical analysis, we demonstrate that the supposed final burst is induced by a shock that requires the input of external energy, whereas the Hawking radiation partner mode, which is associated with the zero-point vacuum fluctuations, is not responsible for the burst."],"url":"http://arxiv.org/abs/2404.09446v1","category":"gr-qc"}
{"created":"2024-04-15 04:14:42","title":"Exploring Text-to-Motion Generation with Human Preference","abstract":"This paper presents an exploration of preference learning in text-to-motion generation. We find that current improvements in text-to-motion generation still rely on datasets requiring expert labelers with motion capture systems. Instead, learning from human preference data does not require motion capture systems; a labeler with no expertise simply compares two generated motions. This is particularly efficient because evaluating the model's output is easier than gathering the motion that performs a desired task (e.g. backflip). To pioneer the exploration of this paradigm, we annotate 3,528 preference pairs generated by MotionGPT, marking the first effort to investigate various algorithms for learning from preference data. In particular, our exploration highlights important design choices when using preference data. Additionally, our experimental results show that preference learning has the potential to greatly improve current text-to-motion generative models. Our code and dataset are publicly available at https://github.com/THU-LYJ-Lab/InstructMotion}{https://github.com/THU-LYJ-Lab/InstructMotion to further facilitate research in this area.","sentences":["This paper presents an exploration of preference learning in text-to-motion generation.","We find that current improvements in text-to-motion generation still rely on datasets requiring expert labelers with motion capture systems.","Instead, learning from human preference data does not require motion capture systems; a labeler with no expertise simply compares two generated motions.","This is particularly efficient because evaluating the model's output is easier than gathering the motion that performs a desired task (e.g. backflip).","To pioneer the exploration of this paradigm, we annotate 3,528 preference pairs generated by MotionGPT, marking the first effort to investigate various algorithms for learning from preference data.","In particular, our exploration highlights important design choices when using preference data.","Additionally, our experimental results show that preference learning has the potential to greatly improve current text-to-motion generative models.","Our code and dataset are publicly available at https://github.com/THU-LYJ-Lab/InstructMotion}{https://github.com/THU-LYJ-Lab/InstructMotion to further facilitate research in this area."],"url":"http://arxiv.org/abs/2404.09445v1","category":"cs.LG"}
{"created":"2024-04-15 04:05:17","title":"Generalized Spectral Decomposition for Quantum Impurity Problems","abstract":"Solving quantum impurity problems may advance our understanding of strongly correlated electron physics, but its development in multi-impurity systems has been greatly hindered due to the presence of shared bath. Here, we propose a general operation strategy to disentangle the shared bath into multiple auxiliary baths and relate the problem to a spectral decomposition problem of function matrix for applying the numerical renormalization group (NRG). We prove exactly that such decomposition is possible for models satisfying (block) circulant symmetry, and show how to construct the auxiliary baths for arbitrary impurity configuration by mapping its graph structure to the subgraph of a regular impurity configuration. We further propose an approximate decomposition algorithm to reduce the number of auxiliary baths and save the computational workload. Our work reveals a deep connection between quantum impurity problems and the graph theory, and provides a general scheme to extend the NRG applications for realistic multi-impurity systems.","sentences":["Solving quantum impurity problems may advance our understanding of strongly correlated electron physics, but its development in multi-impurity systems has been greatly hindered due to the presence of shared bath.","Here, we propose a general operation strategy to disentangle the shared bath into multiple auxiliary baths and relate the problem to a spectral decomposition problem of function matrix for applying the numerical renormalization group (NRG).","We prove exactly that such decomposition is possible for models satisfying (block) circulant symmetry, and show how to construct the auxiliary baths for arbitrary impurity configuration by mapping its graph structure to the subgraph of a regular impurity configuration.","We further propose an approximate decomposition algorithm to reduce the number of auxiliary baths and save the computational workload.","Our work reveals a deep connection between quantum impurity problems and the graph theory, and provides a general scheme to extend the NRG applications for realistic multi-impurity systems."],"url":"http://arxiv.org/abs/2404.09444v1","category":"cond-mat.str-el"}
{"created":"2024-04-15 04:02:39","title":"Hybrid FedGraph: An efficient hybrid federated learning algorithm using graph convolutional neural network","abstract":"Federated learning is an emerging paradigm for decentralized training of machine learning models on distributed clients, without revealing the data to the central server. Most existing works have focused on horizontal or vertical data distributions, where each client possesses different samples with shared features, or each client fully shares only sample indices, respectively. However, the hybrid scheme is much less studied, even though it is much more common in the real world. Therefore, in this paper, we propose a generalized algorithm, FedGraph, that introduces a graph convolutional neural network to capture feature-sharing information while learning features from a subset of clients. We also develop a simple but effective clustering algorithm that aggregates features produced by the deep neural networks of each client while preserving data privacy.","sentences":["Federated learning is an emerging paradigm for decentralized training of machine learning models on distributed clients, without revealing the data to the central server.","Most existing works have focused on horizontal or vertical data distributions, where each client possesses different samples with shared features, or each client fully shares only sample indices, respectively.","However, the hybrid scheme is much less studied, even though it is much more common in the real world.","Therefore, in this paper, we propose a generalized algorithm, FedGraph, that introduces a graph convolutional neural network to capture feature-sharing information while learning features from a subset of clients.","We also develop a simple but effective clustering algorithm that aggregates features produced by the deep neural networks of each client while preserving data privacy."],"url":"http://arxiv.org/abs/2404.09443v1","category":"cs.LG"}
{"created":"2024-04-15 03:56:51","title":"Electron Beam Restructuring of Quantum Emitters in Hexagonal Boron Nitride","abstract":"Hexagonal boron nitride (hBN) holds promise as a solid state, van der Waals host of single photon emitters for on-chip quantum photonics. The B-centre defect emitting at 436 nm is particularly compelling as it can be generated by electron beam irradiation. However, the emitter generation mechanism is unknown, the robustness of the method is variable, and it has only been applied successfully to thick flakes of hBN (>> 10 nm). Here, we use in-situ time-resolved cathodoluminescence (CL) spectroscopy to investigate the kinetics of B-centre generation. We show that the generation of B-centres is accompanied by quenching of a carbon-related emission at ~305 nm and that both processes are rate-limited by electromigration of defects in the hBN lattice. We identify problems that limit the efficacy and reproducibility of the emitter generation method, and solve them using a combination of optimized electron beam parameters and hBN pre- and post-processing treatments. We achieve B-centre quantum emitters in hBN flakes as thin as 8 nm, elucidate the mechanisms responsible for electron beam restructuring of quantum emitters in hBN, and gain insights towards identification of the atomic structure of the B-centre quantum emitter.","sentences":["Hexagonal boron nitride (hBN) holds promise as a solid state, van der Waals host of single photon emitters for on-chip quantum photonics.","The B-centre defect emitting at 436 nm is particularly compelling as it can be generated by electron beam irradiation.","However, the emitter generation mechanism is unknown, the robustness of the method is variable, and it has only been applied successfully to thick flakes of hBN (>> 10 nm).","Here, we use in-situ time-resolved cathodoluminescence (CL) spectroscopy to investigate the kinetics of B-centre generation.","We show that the generation of B-centres is accompanied by quenching of a carbon-related emission at ~305 nm and that both processes are rate-limited by electromigration of defects in the hBN lattice.","We identify problems that limit the efficacy and reproducibility of the emitter generation method, and solve them using a combination of optimized electron beam parameters and hBN pre- and post-processing treatments.","We achieve B-centre quantum emitters in hBN flakes as thin as 8 nm, elucidate the mechanisms responsible for electron beam restructuring of quantum emitters in hBN, and gain insights towards identification of the atomic structure of the B-centre quantum emitter."],"url":"http://arxiv.org/abs/2404.09440v1","category":"physics.app-ph"}
{"created":"2024-04-15 03:55:12","title":"Characteristic Currents on Cohesive Modules","abstract":"Let $\\mathcal{F}$ be a coherent sheaf on a complex variety $X$ that has a locally free resolution $E^{\\bullet}$. In [19], the authors constructed a pseudomeromorphic current whose support is contained in $supp(E^{\\bullet})$ that represents products of Chern classes of $\\mathcal{F}.$ In this paper, we show that their construction works for general de-Rham characteristic classes and then generalize it to represent products (in de-Rham cohomology) of characteristic forms of cohesive modules defined by Block. Finally, we state a corollary to a transgression result in [16] that show that it is sufficient to only use the degree-$0$ and degree-$1$ parts of the superconnection to construct currents that represent characteristic forms of cohesive modules in the Bott-Chern cohomology.","sentences":["Let $\\mathcal{F}$ be a coherent sheaf on a complex variety $X$ that has a locally free resolution $E^{\\bullet}$.","In [19], the authors constructed a pseudomeromorphic current whose support is contained in $supp(E^{\\bullet})$ that represents products of Chern classes of $\\mathcal{F}.$ In this paper, we show that their construction works for general de-Rham characteristic classes and then generalize it to represent products (in de-Rham cohomology) of characteristic forms of cohesive modules defined by Block.","Finally, we state a corollary to a transgression result in [16] that show that it is sufficient to only use the degree-$0$ and degree-$1$ parts of the superconnection to construct currents that represent characteristic forms of cohesive modules in the Bott-Chern cohomology."],"url":"http://arxiv.org/abs/2404.09439v1","category":"math.AG"}
{"created":"2024-04-15 03:21:27","title":"Image Reconstruction with B0 Inhomogeneity using an Interpretable Deep Unrolled Network on an Open-bore MRI-Linac","abstract":"MRI-Linac systems require fast image reconstruction with high geometric fidelity to localize and track tumours for radiotherapy treatments. However, B0 field inhomogeneity distortions and slow MR acquisition potentially limit the quality of the image guidance and tumour treatments. In this study, we develop an interpretable unrolled network, referred to as RebinNet, to reconstruct distortion-free images from B0 inhomogeneity-corrupted k-space for fast MRI-guided radiotherapy applications. RebinNet includes convolutional neural network (CNN) blocks to perform image regularizations and nonuniform fast Fourier Transform (NUFFT) modules to incorporate B0 inhomogeneity information. The RebinNet was trained on a publicly available MR dataset from eleven healthy volunteers for both fully sampled and subsampled acquisitions. Grid phantom and human brain images acquired from an open-bore 1T MRI-Linac scanner were used to evaluate the performance of the proposed network. The RebinNet was compared with the conventional regularization algorithm and our recently developed UnUNet method in terms of root mean squared error (RMSE), structural similarity (SSIM), residual distortions, and computation time. Imaging results demonstrated that the RebinNet reconstructed images with lowest RMSE (<0.05) and highest SSIM (>0.92) at four-time acceleration for simulated brain images. The RebinNet could better preserve structural details and substantially improve the computational efficiency (ten-fold faster) compared to the conventional regularization methods, and had better generalization ability than the UnUNet method. The proposed RebinNet can achieve rapid image reconstruction and overcome the B0 inhomogeneity distortions simultaneously, which would facilitate accurate and fast image guidance in radiotherapy treatments.","sentences":["MRI-Linac systems require fast image reconstruction with high geometric fidelity to localize and track tumours for radiotherapy treatments.","However, B0 field inhomogeneity distortions and slow MR acquisition potentially limit the quality of the image guidance and tumour treatments.","In this study, we develop an interpretable unrolled network, referred to as RebinNet, to reconstruct distortion-free images from B0 inhomogeneity-corrupted k-space for fast MRI-guided radiotherapy applications.","RebinNet includes convolutional neural network (CNN) blocks to perform image regularizations and nonuniform fast Fourier Transform (NUFFT) modules to incorporate B0 inhomogeneity information.","The RebinNet was trained on a publicly available MR dataset from eleven healthy volunteers for both fully sampled and subsampled acquisitions.","Grid phantom and human brain images acquired from an open-bore 1T MRI-Linac scanner were used to evaluate the performance of the proposed network.","The RebinNet was compared with the conventional regularization algorithm and our recently developed UnUNet method in terms of root mean squared error (RMSE), structural similarity (SSIM), residual distortions, and computation time.","Imaging results demonstrated that the RebinNet reconstructed images with lowest RMSE (<0.05) and highest SSIM (>0.92) at four-time acceleration for simulated brain images.","The RebinNet could better preserve structural details and substantially improve the computational efficiency (ten-fold faster) compared to the conventional regularization methods, and had better generalization ability than the UnUNet method.","The proposed RebinNet can achieve rapid image reconstruction and overcome the B0 inhomogeneity distortions simultaneously, which would facilitate accurate and fast image guidance in radiotherapy treatments."],"url":"http://arxiv.org/abs/2404.09436v1","category":"physics.med-ph"}
{"created":"2024-04-15 03:20:12","title":"Device-independent Verification of Quantum Coherence without Quantum Control","abstract":"Quantum coherence plays a crucial role in manipulating and controlling quantum systems, leading to breakthroughs in various fields such as quantum information, quantum sensing, and the detection of gravitational waves. Most coherence witnesses rely on the assumption of being able to control quantum states. Here we report a device-independent coherence model by extending the standard Bell theory to multiple source scenarios. We propose a Greenberger-Horne-Zeilinger-type paradox to verify the particle and wave behaviors of a coherent carrier. We experimentally generate generalized two-photon entangled states that violate the present paradox, witnessing spatial quantum superposition through local measurements.","sentences":["Quantum coherence plays a crucial role in manipulating and controlling quantum systems, leading to breakthroughs in various fields such as quantum information, quantum sensing, and the detection of gravitational waves.","Most coherence witnesses rely on the assumption of being able to control quantum states.","Here we report a device-independent coherence model by extending the standard Bell theory to multiple source scenarios.","We propose a Greenberger-Horne-Zeilinger-type paradox to verify the particle and wave behaviors of a coherent carrier.","We experimentally generate generalized two-photon entangled states that violate the present paradox, witnessing spatial quantum superposition through local measurements."],"url":"http://arxiv.org/abs/2404.09435v1","category":"quant-ph"}
{"created":"2024-04-15 03:12:17","title":"The 8th AI City Challenge","abstract":"The eighth AI City Challenge highlighted the convergence of computer vision and artificial intelligence in areas like retail, warehouse settings, and Intelligent Traffic Systems (ITS), presenting significant research opportunities. The 2024 edition featured five tracks, attracting unprecedented interest from 726 teams in 47 countries and regions. Track 1 dealt with multi-target multi-camera (MTMC) people tracking, highlighting significant enhancements in camera count, character number, 3D annotation, and camera matrices, alongside new rules for 3D tracking and online tracking algorithm encouragement. Track 2 introduced dense video captioning for traffic safety, focusing on pedestrian accidents using multi-camera feeds to improve insights for insurance and prevention. Track 3 required teams to classify driver actions in a naturalistic driving analysis. Track 4 explored fish-eye camera analytics using the FishEye8K dataset. Track 5 focused on motorcycle helmet rule violation detection. The challenge utilized two leaderboards to showcase methods, with participants setting new benchmarks, some surpassing existing state-of-the-art achievements.","sentences":["The eighth AI City Challenge highlighted the convergence of computer vision and artificial intelligence in areas like retail, warehouse settings, and Intelligent Traffic Systems (ITS), presenting significant research opportunities.","The 2024 edition featured five tracks, attracting unprecedented interest from 726 teams in 47 countries and regions.","Track 1 dealt with multi-target multi-camera (MTMC) people tracking, highlighting significant enhancements in camera count, character number, 3D annotation, and camera matrices, alongside new rules for 3D tracking and online tracking algorithm encouragement.","Track 2 introduced dense video captioning for traffic safety, focusing on pedestrian accidents using multi-camera feeds to improve insights for insurance and prevention.","Track 3 required teams to classify driver actions in a naturalistic driving analysis.","Track 4 explored fish-eye camera analytics using the FishEye8K dataset.","Track 5 focused on motorcycle helmet rule violation detection.","The challenge utilized two leaderboards to showcase methods, with participants setting new benchmarks, some surpassing existing state-of-the-art achievements."],"url":"http://arxiv.org/abs/2404.09432v1","category":"cs.CV"}
{"created":"2024-04-15 03:12:12","title":"VFMM3D: Releasing the Potential of Image by Vision Foundation Model for Monocular 3D Object Detection","abstract":"Due to its cost-effectiveness and widespread availability, monocular 3D object detection, which relies solely on a single camera during inference, holds significant importance across various applications, including autonomous driving and robotics. Nevertheless, directly predicting the coordinates of objects in 3D space from monocular images poses challenges. Therefore, an effective solution involves transforming monocular images into LiDAR-like representations and employing a LiDAR-based 3D object detector to predict the 3D coordinates of objects. The key step in this method is accurately converting the monocular image into a reliable point cloud form. In this paper, we present VFMM3D, an innovative approach that leverages the capabilities of Vision Foundation Models (VFMs) to accurately transform single-view images into LiDAR point cloud representations. VFMM3D utilizes the Segment Anything Model (SAM) and Depth Anything Model (DAM) to generate high-quality pseudo-LiDAR data enriched with rich foreground information. Specifically, the Depth Anything Model (DAM) is employed to generate dense depth maps. Subsequently, the Segment Anything Model (SAM) is utilized to differentiate foreground and background regions by predicting instance masks. These predicted instance masks and depth maps are then combined and projected into 3D space to generate pseudo-LiDAR points. Finally, any object detectors based on point clouds can be utilized to predict the 3D coordinates of objects. Comprehensive experiments are conducted on the challenging 3D object detection dataset KITTI. Our VFMM3D establishes a new state-of-the-art performance. Additionally, experimental results demonstrate the generality of VFMM3D, showcasing its seamless integration into various LiDAR-based 3D object detectors.","sentences":["Due to its cost-effectiveness and widespread availability, monocular 3D object detection, which relies solely on a single camera during inference, holds significant importance across various applications, including autonomous driving and robotics.","Nevertheless, directly predicting the coordinates of objects in 3D space from monocular images poses challenges.","Therefore, an effective solution involves transforming monocular images into LiDAR-like representations and employing a LiDAR-based 3D object detector to predict the 3D coordinates of objects.","The key step in this method is accurately converting the monocular image into a reliable point cloud form.","In this paper, we present VFMM3D, an innovative approach that leverages the capabilities of Vision Foundation Models (VFMs) to accurately transform single-view images into LiDAR point cloud representations.","VFMM3D utilizes the Segment Anything Model (SAM) and Depth Anything Model (DAM) to generate high-quality pseudo-LiDAR data enriched with rich foreground information.","Specifically, the Depth Anything Model (DAM) is employed to generate dense depth maps.","Subsequently, the Segment Anything Model (SAM) is utilized to differentiate foreground and background regions by predicting instance masks.","These predicted instance masks and depth maps are then combined and projected into 3D space to generate pseudo-LiDAR points.","Finally, any object detectors based on point clouds can be utilized to predict the 3D coordinates of objects.","Comprehensive experiments are conducted on the challenging 3D object detection dataset KITTI.","Our VFMM3D establishes a new state-of-the-art performance.","Additionally, experimental results demonstrate the generality of VFMM3D, showcasing its seamless integration into various LiDAR-based 3D object detectors."],"url":"http://arxiv.org/abs/2404.09431v1","category":"cs.CV"}
{"created":"2024-04-15 02:46:53","title":"Boundary effect and correlations in fermionic Gaussian states","abstract":"The effect of boundaries on the bulk properties of quantum many-body systems is an intriguing subject of study. One can define a boundary effect function, which quantifies the change in the ground state as a function of the distance from the boundary. This function serves as an upper bound for the correlation functions and the entanglement entropies in the thermodynamic limit. Here, we perform numerical analyses of the boundary effect function for one-dimensional free-fermion models. We find that the upper bound established by the boundary effect fuction is tight for the examined systems, providing a deep insight into how correlations and entanglement are developed in the ground state as the system size grows. As a by-product, we derive a general fidelity formula for fermionic Gaussian states in a self-contained manner, rendering the formula easier to apprehend.","sentences":["The effect of boundaries on the bulk properties of quantum many-body systems is an intriguing subject of study.","One can define a boundary effect function, which quantifies the change in the ground state as a function of the distance from the boundary.","This function serves as an upper bound for the correlation functions and the entanglement entropies in the thermodynamic limit.","Here, we perform numerical analyses of the boundary effect function for one-dimensional free-fermion models.","We find that the upper bound established by the boundary effect fuction is tight for the examined systems, providing a deep insight into how correlations and entanglement are developed in the ground state as the system size grows.","As a by-product, we derive a general fidelity formula for fermionic Gaussian states in a self-contained manner, rendering the formula easier to apprehend."],"url":"http://arxiv.org/abs/2404.09428v1","category":"quant-ph"}
{"created":"2024-04-15 02:44:23","title":"ViFu: Multiple 360$^\\circ$ Objects Reconstruction with Clean Background via Visible Part Fusion","abstract":"In this paper, we propose a method to segment and recover a static, clean background and multiple 360$^\\circ$ objects from observations of scenes at different timestamps. Recent works have used neural radiance fields to model 3D scenes and improved the quality of novel view synthesis, while few studies have focused on modeling the invisible or occluded parts of the training images. These under-reconstruction parts constrain both scene editing and rendering view selection, thereby limiting their utility for synthetic data generation for downstream tasks. Our basic idea is that, by observing the same set of objects in various arrangement, so that parts that are invisible in one scene may become visible in others. By fusing the visible parts from each scene, occlusion-free rendering of both background and foreground objects can be achieved.   We decompose the multi-scene fusion task into two main components: (1) objects/background segmentation and alignment, where we leverage point cloud-based methods tailored to our novel problem formulation; (2) radiance fields fusion, where we introduce visibility field to quantify the visible information of radiance fields, and propose visibility-aware rendering for the fusion of series of scenes, ultimately obtaining clean background and 360$^\\circ$ object rendering. Comprehensive experiments were conducted on synthetic and real datasets, and the results demonstrate the effectiveness of our method.","sentences":["In this paper, we propose a method to segment and recover a static, clean background and multiple 360$^\\circ$ objects from observations of scenes at different timestamps.","Recent works have used neural radiance fields to model 3D scenes and improved the quality of novel view synthesis, while few studies have focused on modeling the invisible or occluded parts of the training images.","These under-reconstruction parts constrain both scene editing and rendering view selection, thereby limiting their utility for synthetic data generation for downstream tasks.","Our basic idea is that, by observing the same set of objects in various arrangement, so that parts that are invisible in one scene may become visible in others.","By fusing the visible parts from each scene, occlusion-free rendering of both background and foreground objects can be achieved.   ","We decompose the multi-scene fusion task into two main components: (1) objects/background segmentation and alignment, where we leverage point cloud-based methods tailored to our novel problem formulation; (2) radiance fields fusion, where we introduce visibility field to quantify the visible information of radiance fields, and propose visibility-aware rendering for the fusion of series of scenes, ultimately obtaining clean background and 360$^\\circ$ object rendering.","Comprehensive experiments were conducted on synthetic and real datasets, and the results demonstrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2404.09426v1","category":"cs.CV"}
{"created":"2024-04-15 02:41:55","title":"Super-resolution of biomedical volumes with 2D supervision","abstract":"Volumetric biomedical microscopy has the potential to increase the diagnostic information extracted from clinical tissue specimens and improve the diagnostic accuracy of both human pathologists and computational pathology models. Unfortunately, barriers to integrating 3-dimensional (3D) volumetric microscopy into clinical medicine include long imaging times, poor depth / z-axis resolution, and an insufficient amount of high-quality volumetric data. Leveraging the abundance of high-resolution 2D microscopy data, we introduce masked slice diffusion for super-resolution (MSDSR), which exploits the inherent equivalence in the data-generating distribution across all spatial dimensions of biological specimens. This intrinsic characteristic allows for super-resolution models trained on high-resolution images from one plane (e.g., XY) to effectively generalize to others (XZ, YZ), overcoming the traditional dependency on orientation. We focus on the application of MSDSR to stimulated Raman histology (SRH), an optical imaging modality for biological specimen analysis and intraoperative diagnosis, characterized by its rapid acquisition of high-resolution 2D images but slow and costly optical z-sectioning. To evaluate MSDSR's efficacy, we introduce a new performance metric, SliceFID, and demonstrate MSDSR's superior performance over baseline models through extensive evaluations. Our findings reveal that MSDSR not only significantly enhances the quality and resolution of 3D volumetric data, but also addresses major obstacles hindering the broader application of 3D volumetric microscopy in clinical diagnostics and biomedical research.","sentences":["Volumetric biomedical microscopy has the potential to increase the diagnostic information extracted from clinical tissue specimens and improve the diagnostic accuracy of both human pathologists and computational pathology models.","Unfortunately, barriers to integrating 3-dimensional (3D) volumetric microscopy into clinical medicine include long imaging times, poor depth / z-axis resolution, and an insufficient amount of high-quality volumetric data.","Leveraging the abundance of high-resolution 2D microscopy data, we introduce masked slice diffusion for super-resolution (MSDSR), which exploits the inherent equivalence in the data-generating distribution across all spatial dimensions of biological specimens.","This intrinsic characteristic allows for super-resolution models trained on high-resolution images from one plane (e.g., XY) to effectively generalize to others (XZ, YZ), overcoming the traditional dependency on orientation.","We focus on the application of MSDSR to stimulated Raman histology (SRH), an optical imaging modality for biological specimen analysis and intraoperative diagnosis, characterized by its rapid acquisition of high-resolution 2D images but slow and costly optical z-sectioning.","To evaluate MSDSR's efficacy, we introduce a new performance metric, SliceFID, and demonstrate MSDSR's superior performance over baseline models through extensive evaluations.","Our findings reveal that MSDSR not only significantly enhances the quality and resolution of 3D volumetric data, but also addresses major obstacles hindering the broader application of 3D volumetric microscopy in clinical diagnostics and biomedical research."],"url":"http://arxiv.org/abs/2404.09425v1","category":"eess.IV"}
{"created":"2024-04-15 02:30:22","title":"Computation and analysis of subdiffusion with variable exponent","abstract":"We consider the subdiffusion of variable exponent modeling subdiffusion phenomena with varying memory properties. The main difficulty is that this model could not be analytically solved and the variable-exponent Abel kernel may not be positive definite or monotonic. This work develops a tool called the generalized identity function to convert this model to more feasible formulations for mathematical and numerical analysis, based on which we prove its well-posedness and regularity. In particular, we characterize the singularity of the solutions in terms of the initial value of the exponent. Then the semi-discrete and fully-discrete numerical methods are developed and their error estimates are proved, without any regularity assumption on solutions or requiring specific properties of the variable-exponent Abel kernel. The convergence order is also characterized by the initial value of the exponent. Finally, we investigate an inverse problem of determining the initial value of the exponent.","sentences":["We consider the subdiffusion of variable exponent modeling subdiffusion phenomena with varying memory properties.","The main difficulty is that this model could not be analytically solved and the variable-exponent Abel kernel may not be positive definite or monotonic.","This work develops a tool called the generalized identity function to convert this model to more feasible formulations for mathematical and numerical analysis, based on which we prove its well-posedness and regularity.","In particular, we characterize the singularity of the solutions in terms of the initial value of the exponent.","Then the semi-discrete and fully-discrete numerical methods are developed and their error estimates are proved, without any regularity assumption on solutions or requiring specific properties of the variable-exponent Abel kernel.","The convergence order is also characterized by the initial value of the exponent.","Finally, we investigate an inverse problem of determining the initial value of the exponent."],"url":"http://arxiv.org/abs/2404.09421v1","category":"math.NA"}
{"created":"2024-04-15 02:15:54","title":"Predicting Accurate Hot Spots in a More Than Ten-Thousand-Core GPU with a Million-Time Speedup over FEM Enabled by a Physics-based Learning Algorithm","abstract":"The classical proper orthogonal decomposition (POD) with the Galerkin projection (GP) has been revised for chip-level thermal simulation of microprocessors with a large number of cores. An ensemble POD-GP methodology (EnPOD-GP) is introduced to significantly improve the training effectiveness and prediction accuracy by dividing a large number of heat sources into heat source blocks (HSBs) each of which may contains one or a very small number of heat sources. Although very accurate, efficient and robust to any power map, EnPOD-GP suffers from intensive training for microprocessors with an enormous number of cores. A local-domain EnPOD-GP model (LEnPOD-GP) is thus proposed to further minimize the training burden. LEnPOD-GP utilizes the concepts of local domain truncation and generic building blocks to reduce the massive training data. LEnPOD-GP has been demonstrated on thermal simulation of NVIDIA Tesla Volta GV100, a GPU with more than 13,000 cores including FP32, FP64, INT32, and Tensor Cores. Due to the domain truncation for LEnPOD-GP, the least square error (LSE) is degraded but is still as small as 1.6% over the entire space and below 1.4% in the device layer when using 4 modes per HSB. When only the maximum temperature of the entire GPU is of interest, LEnPOD-GP offers a computing speed 1.1 million times faster than the FEM with a maximum error near 1.2 degrees over the entire simulation time.","sentences":["The classical proper orthogonal decomposition (POD) with the Galerkin projection (GP) has been revised for chip-level thermal simulation of microprocessors with a large number of cores.","An ensemble POD-GP methodology (EnPOD-GP) is introduced to significantly improve the training effectiveness and prediction accuracy by dividing a large number of heat sources into heat source blocks (HSBs) each of which may contains one or a very small number of heat sources.","Although very accurate, efficient and robust to any power map, EnPOD-GP suffers from intensive training for microprocessors with an enormous number of cores.","A local-domain EnPOD-GP model (LEnPOD-GP) is thus proposed to further minimize the training burden.","LEnPOD-GP utilizes the concepts of local domain truncation and generic building blocks to reduce the massive training data.","LEnPOD-GP has been demonstrated on thermal simulation of NVIDIA Tesla Volta GV100, a GPU with more than 13,000 cores including FP32, FP64, INT32, and Tensor Cores.","Due to the domain truncation for LEnPOD-GP, the least square error (LSE) is degraded but is still as small as 1.6% over the entire space and below 1.4% in the device layer when using 4 modes per HSB.","When only the maximum temperature of the entire GPU is of interest, LEnPOD-GP offers a computing speed 1.1 million times faster than the FEM with a maximum error near 1.2 degrees over the entire simulation time."],"url":"http://arxiv.org/abs/2404.09419v1","category":"cs.CE"}
{"created":"2024-04-15 02:15:44","title":"Polarized radio emission of RRAT J1854+0306","abstract":"Polarized radio emission of RRAT J1854+0306 is investigated with single pulses using FAST. Its emission is characterized by nulls, narrow and weak pulses and occasional wide and intense bursts with a nulling fraction of 53.2%. Its burst emission is typically of one rotation, and occasionally of two or three or even 5 rotations at the most, but without significant periodicity. The integrated pulse profile has a `S' shaped position angle curve that is superposed with orthogonal modes, from which geometry parameters are obtained. Individual pulses exhibit diverse profile morphology with single, double or multiple peaks. The intensity and width of these pulses are highly correlated, and bright pulses generally have wide profiles with multiple peaks. These nulling behaviours, profile morphology and polarization demonstrate that RRAT has the same physical origins as the normal pulsars.","sentences":["Polarized radio emission of RRAT J1854+0306 is investigated with single pulses using FAST.","Its emission is characterized by nulls, narrow and weak pulses and occasional wide and intense bursts with a nulling fraction of 53.2%.","Its burst emission is typically of one rotation, and occasionally of two or three or even 5 rotations at the most, but without significant periodicity.","The integrated pulse profile has a `S' shaped position angle curve that is superposed with orthogonal modes, from which geometry parameters are obtained.","Individual pulses exhibit diverse profile morphology with single, double or multiple peaks.","The intensity and width of these pulses are highly correlated, and bright pulses generally have wide profiles with multiple peaks.","These nulling behaviours, profile morphology and polarization demonstrate that RRAT has the same physical origins as the normal pulsars."],"url":"http://arxiv.org/abs/2404.09418v1","category":"astro-ph.HE"}
{"created":"2024-04-15 02:08:28","title":"Automatic Knowledge Graph Construction for Judicial Cases","abstract":"In this paper, we explore the application of cognitive intelligence in legal knowledge, focusing on the development of judicial artificial intelligence. Utilizing natural language processing (NLP) as the core technology, we propose a method for the automatic construction of case knowledge graphs for judicial cases. Our approach centers on two fundamental NLP tasks: entity recognition and relationship extraction. We compare two pre-trained models for entity recognition to establish their efficacy. Additionally, we introduce a multi-task semantic relationship extraction model that incorporates translational embedding, leading to a nuanced contextualized case knowledge representation. Specifically, in a case study involving a \"Motor Vehicle Traffic Accident Liability Dispute,\" our approach significantly outperforms the baseline model. The entity recognition F1 score improved by 0.36, while the relationship extraction F1 score increased by 2.37. Building on these results, we detail the automatic construction process of case knowledge graphs for judicial cases, enabling the assembly of knowledge graphs for hundreds of thousands of judgments. This framework provides robust semantic support for applications of judicial AI, including the precise categorization and recommendation of related cases.","sentences":["In this paper, we explore the application of cognitive intelligence in legal knowledge, focusing on the development of judicial artificial intelligence.","Utilizing natural language processing (NLP) as the core technology, we propose a method for the automatic construction of case knowledge graphs for judicial cases.","Our approach centers on two fundamental NLP tasks: entity recognition and relationship extraction.","We compare two pre-trained models for entity recognition to establish their efficacy.","Additionally, we introduce a multi-task semantic relationship extraction model that incorporates translational embedding, leading to a nuanced contextualized case knowledge representation.","Specifically, in a case study involving a \"Motor Vehicle Traffic Accident Liability Dispute,\" our approach significantly outperforms the baseline model.","The entity recognition F1 score improved by 0.36, while the relationship extraction F1 score increased by 2.37.","Building on these results, we detail the automatic construction process of case knowledge graphs for judicial cases, enabling the assembly of knowledge graphs for hundreds of thousands of judgments.","This framework provides robust semantic support for applications of judicial AI, including the precise categorization and recommendation of related cases."],"url":"http://arxiv.org/abs/2404.09416v1","category":"cs.CL"}
{"created":"2024-04-15 02:02:15","title":"A Review on Machine Learning Algorithms for Dust Aerosol Detection using Satellite Data","abstract":"Dust storms are associated with certain respiratory illnesses across different areas in the world. Researchers have devoted time and resources to study the elements surrounding dust storm phenomena. This paper reviews the efforts of those who have investigated dust aerosols using sensors onboard of satellites using machine learning-based approaches. We have reviewed the most common issues revolving dust aerosol modeling using different datasets and different sensors from a historical perspective. Our findings suggest that multi-spectral approaches based on linear and non-linear combinations of spectral bands are some of the most successful for visualization and quantitative analysis; however, when researchers have leveraged machine learning, performance has been improved and new opportunities to solve unique problems arise.","sentences":["Dust storms are associated with certain respiratory illnesses across different areas in the world.","Researchers have devoted time and resources to study the elements surrounding dust storm phenomena.","This paper reviews the efforts of those who have investigated dust aerosols using sensors onboard of satellites using machine learning-based approaches.","We have reviewed the most common issues revolving dust aerosol modeling using different datasets and different sensors from a historical perspective.","Our findings suggest that multi-spectral approaches based on linear and non-linear combinations of spectral bands are some of the most successful for visualization and quantitative analysis; however, when researchers have leveraged machine learning, performance has been improved and new opportunities to solve unique problems arise."],"url":"http://arxiv.org/abs/2404.09415v1","category":"cs.CV"}
{"created":"2024-04-15 02:01:42","title":"General Bayesian inference for causal effects using covariate balancing procedure","abstract":"In observational studies, the propensity score plays a central role in estimating causal effects of interest. The inverse probability weighting (IPW) estimator is especially commonly used. However, if the propensity score model is misspecified, the IPW estimator may produce biased estimates of causal effects. Previous studies have proposed some robust propensity score estimation procedures; these methods, however, require consideration of parameters that dominate the uncertainty of sampling and treatment allocation. In this manuscript, we propose a novel Bayesian estimating procedure that necessitates deciding the parameter probability, rather than deterministically. Since both the IPW estimator and the propensity score estimator can be derived as solutions to certain loss functions, the general Bayesian paradigm, which does not require the consideration of the full likelihood, can be applied. In this sense, our proposed method only requires the same level of assumptions as ordinary causal inference contexts.","sentences":["In observational studies, the propensity score plays a central role in estimating causal effects of interest.","The inverse probability weighting (IPW) estimator is especially commonly used.","However, if the propensity score model is misspecified, the IPW estimator may produce biased estimates of causal effects.","Previous studies have proposed some robust propensity score estimation procedures; these methods, however, require consideration of parameters that dominate the uncertainty of sampling and treatment allocation.","In this manuscript, we propose a novel Bayesian estimating procedure that necessitates deciding the parameter probability, rather than deterministically.","Since both the IPW estimator and the propensity score estimator can be derived as solutions to certain loss functions, the general Bayesian paradigm, which does not require the consideration of the full likelihood, can be applied.","In this sense, our proposed method only requires the same level of assumptions as ordinary causal inference contexts."],"url":"http://arxiv.org/abs/2404.09414v1","category":"stat.ME"}
{"created":"2024-04-15 02:00:24","title":"On the Optimal Regret of Locally Private Linear Contextual Bandit","abstract":"Contextual bandit with linear reward functions is among one of the most extensively studied models in bandit and online learning research. Recently, there has been increasing interest in designing \\emph{locally private} linear contextual bandit algorithms, where sensitive information contained in contexts and rewards is protected against leakage to the general public. While the classical linear contextual bandit algorithm admits cumulative regret upper bounds of $\\tilde O(\\sqrt{T})$ via multiple alternative methods, it has remained open whether such regret bounds are attainable in the presence of local privacy constraints, with the state-of-the-art result being $\\tilde O(T^{3/4})$. In this paper, we show that it is indeed possible to achieve an $\\tilde O(\\sqrt{T})$ regret upper bound for locally private linear contextual bandit. Our solution relies on several new algorithmic and analytical ideas, such as the analysis of mean absolute deviation errors and layered principal component regression in order to achieve small mean absolute deviation errors.","sentences":["Contextual bandit with linear reward functions is among one of the most extensively studied models in bandit and online learning research.","Recently, there has been increasing interest in designing \\emph{locally private} linear contextual bandit algorithms, where sensitive information contained in contexts and rewards is protected against leakage to the general public.","While the classical linear contextual bandit algorithm admits cumulative regret upper bounds of $\\tilde O(\\sqrt{T})$ via multiple alternative methods, it has remained open whether such regret bounds are attainable in the presence of local privacy constraints, with the state-of-the-art result being $\\tilde O(T^{3/4})$.","In this paper, we show that it is indeed possible to achieve an $\\tilde O(\\sqrt{T})$ regret upper bound for locally private linear contextual bandit.","Our solution relies on several new algorithmic and analytical ideas, such as the analysis of mean absolute deviation errors and layered principal component regression in order to achieve small mean absolute deviation errors."],"url":"http://arxiv.org/abs/2404.09413v1","category":"stat.ML"}
{"created":"2024-04-15 01:58:18","title":"Wasserstein Wormhole: Scalable Optimal Transport Distance with Transformers","abstract":"Optimal transport (OT) and the related Wasserstein metric (W) are powerful and ubiquitous tools for comparing distributions. However, computing pairwise Wasserstein distances rapidly becomes intractable as cohort size grows. An attractive alternative would be to find an embedding space in which pairwise Euclidean distances map to OT distances, akin to standard multidimensional scaling (MDS). We present Wasserstein Wormhole, a transformer-based autoencoder that embeds empirical distributions into a latent space wherein Euclidean distances approximate OT distances. Extending MDS theory, we show that our objective function implies a bound on the error incurred when embedding non-Euclidean distances. Empirically, distances between Wormhole embeddings closely match Wasserstein distances, enabling linear time computation of OT distances. Along with an encoder that maps distributions to embeddings, Wasserstein Wormhole includes a decoder that maps embeddings back to distributions, allowing for operations in the embedding space to generalize to OT spaces, such as Wasserstein barycenter estimation and OT interpolation. By lending scalability and interpretability to OT approaches, Wasserstein Wormhole unlocks new avenues for data analysis in the fields of computational geometry and single-cell biology.","sentences":["Optimal transport (OT) and the related Wasserstein metric (W) are powerful and ubiquitous tools for comparing distributions.","However, computing pairwise Wasserstein distances rapidly becomes intractable as cohort size grows.","An attractive alternative would be to find an embedding space in which pairwise Euclidean distances map to OT distances, akin to standard multidimensional scaling (MDS).","We present Wasserstein Wormhole, a transformer-based autoencoder that embeds empirical distributions into a latent space wherein Euclidean distances approximate OT distances.","Extending MDS theory, we show that our objective function implies a bound on the error incurred when embedding non-Euclidean distances.","Empirically, distances between Wormhole embeddings closely match Wasserstein distances, enabling linear time computation of OT distances.","Along with an encoder that maps distributions to embeddings, Wasserstein Wormhole includes a decoder that maps embeddings back to distributions, allowing for operations in the embedding space to generalize to OT spaces, such as Wasserstein barycenter estimation and OT interpolation.","By lending scalability and interpretability to OT approaches, Wasserstein Wormhole unlocks new avenues for data analysis in the fields of computational geometry and single-cell biology."],"url":"http://arxiv.org/abs/2404.09411v1","category":"cs.LG"}
{"created":"2024-04-15 01:55:25","title":"$L^2$-based stability of blowup with log correction for semilinear heat equation","abstract":"We propose an alternative proof of the classical result of type-I blowup with log correction for the semilinear equation. Compared with previous proofs, we use a novel idea of enforcing stable normalizations for perturbation around the approximate profile and establish a weighted $H^k$ stability, thereby avoiding the use of a topological argument and the analysis of a linearized spectrum. Therefore, this approach can be adopted even if we only have a numerical profile and do not have explicit information on the spectrum of its linearized operator. This result generalizes the $L^2$-based stability argument to blowups that are not exactly self-similar and can be adapted to higher dimensions. Numerical results corroborate the effectiveness of our normalization, even in the large perturbation regime beyond our theoretical setting.","sentences":["We propose an alternative proof of the classical result of type-I blowup with log correction for the semilinear equation.","Compared with previous proofs, we use a novel idea of enforcing stable normalizations for perturbation around the approximate profile and establish a weighted $H^k$ stability, thereby avoiding the use of a topological argument and the analysis of a linearized spectrum.","Therefore, this approach can be adopted even if we only have a numerical profile and do not have explicit information on the spectrum of its linearized operator.","This result generalizes the $L^2$-based stability argument to blowups that are not exactly self-similar and can be adapted to higher dimensions.","Numerical results corroborate the effectiveness of our normalization, even in the large perturbation regime beyond our theoretical setting."],"url":"http://arxiv.org/abs/2404.09410v1","category":"math.AP"}
{"created":"2024-04-15 01:55:13","title":"Disorder Chaos in Short-Range, Diluted, and L\u00e9vy Spin Glasses","abstract":"In a recent breakthrough \\cite{Chatterjee}, Chatterjee proved site disorder chaos in the Edwards-Anderson (EA) short-range spin glass model \\cite{Edwards1975} utilizing the Hermite spectral method. In this paper, we demonstrate the further usefulness of this Hermite spectral approach by extending the validity of site disorder chaos in three related spin glass models.   The first, called the mixed even $p$-spin short-range model, is a generalization of the EA model where the underlying graph is a deterministic bounded degree hypergraph consisting of hyperedges with even number of vertices. The second model is the diluted mixed $p$-spin model, which is allowed to have hyperedges with both odd and even number of vertices. For both models, our results hold under general symmetric disorder distributions. The main novelty of our argument is played by an elementary algebraic equation for the Fourier-Hermite series coefficients for the two-spin correlation functions. It allows us to deduce necessary geometric conditions to determine the contributing coefficients in the overlap function, which in spirit is the same as the crucial Lemma 1 in \\cite{Chatterjee}. Finally, we also establish disorder chaos in the L\\'evy model with stable index $\\alpha \\in (1, 2)$.","sentences":["In a recent breakthrough \\cite{Chatterjee}, Chatterjee proved site disorder chaos in the Edwards-Anderson (EA) short-range spin glass model \\cite{Edwards1975} utilizing the Hermite spectral method.","In this paper, we demonstrate the further usefulness of this Hermite spectral approach by extending the validity of site disorder chaos in three related spin glass models.   ","The first, called the mixed even $p$-spin short-range model, is a generalization of the EA model where the underlying graph is a deterministic bounded degree hypergraph consisting of hyperedges with even number of vertices.","The second model is the diluted mixed $p$-spin model, which is allowed to have hyperedges with both odd and even number of vertices.","For both models, our results hold under general symmetric disorder distributions.","The main novelty of our argument is played by an elementary algebraic equation for the Fourier-Hermite series coefficients for the two-spin correlation functions.","It allows us to deduce necessary geometric conditions to determine the contributing coefficients in the overlap function, which in spirit is the same as the crucial Lemma 1 in \\cite{Chatterjee}.","Finally, we also establish disorder chaos in the L\\'evy model with stable index $\\alpha \\in (1, 2)$."],"url":"http://arxiv.org/abs/2404.09409v1","category":"math.PR"}
{"created":"2024-04-15 01:47:44","title":"Human-in-the-Loop Segmentation of Multi-species Coral Imagery","abstract":"Broad-scale marine surveys performed by underwater vehicles significantly increase the availability of coral reef imagery, however it is costly and time-consuming for domain experts to label images. Point label propagation is an approach used to leverage existing image data labeled with sparse point labels. The resulting augmented ground truth generated is then used to train a semantic segmentation model. Here, we first demonstrate that recent advances in foundation models enable generation of multi-species coral augmented ground truth masks using denoised DINOv2 features and K-Nearest Neighbors (KNN), without the need for any pre-training or custom-designed algorithms. For extremely sparsely labeled images, we propose a labeling regime based on human-in-the-loop principles, resulting in significant improvement in annotation efficiency: If only 5 point labels per image are available, our proposed human-in-the-loop approach improves on the state-of-the-art by 17.3% for pixel accuracy and 22.6% for mIoU; and by 10.6% and 19.1% when 10 point labels per image are available. Even if the human-in-the-loop labeling regime is not used, the denoised DINOv2 features with a KNN outperforms the prior state-of-the-art by 3.5% for pixel accuracy and 5.7% for mIoU (5 grid points). We also provide a detailed analysis of how point labeling style and the quantity of points per image affects the point label propagation quality and provide general recommendations on maximizing point label efficiency.","sentences":["Broad-scale marine surveys performed by underwater vehicles significantly increase the availability of coral reef imagery, however it is costly and time-consuming for domain experts to label images.","Point label propagation is an approach used to leverage existing image data labeled with sparse point labels.","The resulting augmented ground truth generated is then used to train a semantic segmentation model.","Here, we first demonstrate that recent advances in foundation models enable generation of multi-species coral augmented ground truth masks using denoised DINOv2 features and K-Nearest Neighbors (KNN), without the need for any pre-training or custom-designed algorithms.","For extremely sparsely labeled images, we propose a labeling regime based on human-in-the-loop principles, resulting in significant improvement in annotation efficiency:","If only 5 point labels per image are available, our proposed human-in-the-loop approach improves on the state-of-the-art by 17.3% for pixel accuracy and 22.6% for mIoU; and by 10.6% and 19.1% when 10 point labels per image are available.","Even if the human-in-the-loop labeling regime is not used, the denoised DINOv2 features with a KNN outperforms the prior state-of-the-art by 3.5% for pixel accuracy and 5.7% for mIoU (5 grid points).","We also provide a detailed analysis of how point labeling style and the quantity of points per image affects the point label propagation quality and provide general recommendations on maximizing point label efficiency."],"url":"http://arxiv.org/abs/2404.09406v1","category":"cs.CV"}
{"created":"2024-04-15 01:43:14","title":"Few-shot Name Entity Recognition on StackOverflow","abstract":"StackOverflow, with its vast question repository and limited labeled examples, raise an annotation challenge for us. We address this gap by proposing RoBERTa+MAML, a few-shot named entity recognition (NER) method leveraging meta-learning. Our approach, evaluated on the StackOverflow NER corpus (27 entity types), achieves a 5% F1 score improvement over the baseline. We improved the results further domain-specific phrase processing enhance results.","sentences":["StackOverflow, with its vast question repository and limited labeled examples, raise an annotation challenge for us.","We address this gap by proposing RoBERTa+MAML, a few-shot named entity recognition (NER) method leveraging meta-learning.","Our approach, evaluated on the StackOverflow NER corpus (27 entity types), achieves a 5% F1 score improvement over the baseline.","We improved the results further domain-specific phrase processing enhance results."],"url":"http://arxiv.org/abs/2404.09405v1","category":"cs.CL"}
{"created":"2024-04-15 01:28:16","title":"Neural McKean-Vlasov Processes: Distributional Dependence in Diffusion Processes","abstract":"McKean-Vlasov stochastic differential equations (MV-SDEs) provide a mathematical description of the behavior of an infinite number of interacting particles by imposing a dependence on the particle density. As such, we study the influence of explicitly including distributional information in the parameterization of the SDE. We propose a series of semi-parametric methods for representing MV-SDEs, and corresponding estimators for inferring parameters from data based on the properties of the MV-SDE. We analyze the characteristics of the different architectures and estimators, and consider their applicability in relevant machine learning problems. We empirically compare the performance of the different architectures and estimators on real and synthetic datasets for time series and probabilistic modeling. The results suggest that explicitly including distributional dependence in the parameterization of the SDE is effective in modeling temporal data with interaction under an exchangeability assumption while maintaining strong performance for standard It\\^o-SDEs due to the richer class of probability flows associated with MV-SDEs.","sentences":["McKean-Vlasov stochastic differential equations (MV-SDEs) provide a mathematical description of the behavior of an infinite number of interacting particles by imposing a dependence on the particle density.","As such, we study the influence of explicitly including distributional information in the parameterization of the SDE.","We propose a series of semi-parametric methods for representing MV-SDEs, and corresponding estimators for inferring parameters from data based on the properties of the MV-SDE.","We analyze the characteristics of the different architectures and estimators, and consider their applicability in relevant machine learning problems.","We empirically compare the performance of the different architectures and estimators on real and synthetic datasets for time series and probabilistic modeling.","The results suggest that explicitly including distributional dependence in the parameterization of the SDE is effective in modeling temporal data with interaction under an exchangeability assumption while maintaining strong performance for standard It\\^o-SDEs due to the richer class of probability flows associated with MV-SDEs."],"url":"http://arxiv.org/abs/2404.09402v1","category":"cs.LG"}
{"created":"2024-04-15 01:27:07","title":"Watermark-embedded Adversarial Examples for Copyright Protection against Diffusion Models","abstract":"Diffusion Models (DMs) have shown remarkable capabilities in various image-generation tasks. However, there are growing concerns that DMs could be used to imitate unauthorized creations and thus raise copyright issues. To address this issue, we propose a novel framework that embeds personal watermarks in the generation of adversarial examples. Such examples can force DMs to generate images with visible watermarks and prevent DMs from imitating unauthorized images. We construct a generator based on conditional adversarial networks and design three losses (adversarial loss, GAN loss, and perturbation loss) to generate adversarial examples that have subtle perturbation but can effectively attack DMs to prevent copyright violations. Training a generator for a personal watermark by our method only requires 5-10 samples within 2-3 minutes, and once the generator is trained, it can generate adversarial examples with that watermark significantly fast (0.2s per image). We conduct extensive experiments in various conditional image-generation scenarios. Compared to existing methods that generate images with chaotic textures, our method adds visible watermarks on the generated images, which is a more straightforward way to indicate copyright violations. We also observe that our adversarial examples exhibit good transferability across unknown generative models. Therefore, this work provides a simple yet powerful way to protect copyright from DM-based imitation.","sentences":["Diffusion Models (DMs) have shown remarkable capabilities in various image-generation tasks.","However, there are growing concerns that DMs could be used to imitate unauthorized creations and thus raise copyright issues.","To address this issue, we propose a novel framework that embeds personal watermarks in the generation of adversarial examples.","Such examples can force DMs to generate images with visible watermarks and prevent DMs from imitating unauthorized images.","We construct a generator based on conditional adversarial networks and design three losses (adversarial loss, GAN loss, and perturbation loss) to generate adversarial examples that have subtle perturbation but can effectively attack DMs to prevent copyright violations.","Training a generator for a personal watermark by our method only requires 5-10 samples within 2-3 minutes, and once the generator is trained, it can generate adversarial examples with that watermark significantly fast (0.2s per image).","We conduct extensive experiments in various conditional image-generation scenarios.","Compared to existing methods that generate images with chaotic textures, our method adds visible watermarks on the generated images, which is a more straightforward way to indicate copyright violations.","We also observe that our adversarial examples exhibit good transferability across unknown generative models.","Therefore, this work provides a simple yet powerful way to protect copyright from DM-based imitation."],"url":"http://arxiv.org/abs/2404.09401v1","category":"cs.CV"}
{"created":"2024-04-15 01:09:21","title":"Characterizing Kirkwood-Dirac positive states based on discrete Fourier transform","abstract":"Kirkwood-Dirac (KD) distribution is helpful to describe nonclassical phenomena and quantum advantages, which have been linked with nonpositive entries of KD distribution. Suppose that $\\mathcal{A}$ and $\\mathcal{B}$ are the eigenprojectors of the two eigenbases of two observables and the discrete Fourier transform (DFT) matrix is the transition matrix between the two eigenbases. In a system with prime dimension, the set $\\mathcal{E}_{KD+}$ of KD positive states based on the DFT matrix is convex combinations of $\\mathcal{A}$ and $\\mathcal{B}$. That is, $\\mathcal{E}_{KD+}={\\rm conv}(\\mathcal{A}\\cup\\mathcal{B})$ [arXiv:2306.00086]. In this paper, we generalize the result. That is, in a $d$-dimensional system, $\\mathcal{E}_{KD+}={\\rm conv}(\\Omega)$ for $d=p^{2}$ and $d=pq$, where $p, q$ are prime and $\\Omega$ is the set of projectors of all the pure KD positive states.","sentences":["Kirkwood-Dirac (KD) distribution is helpful to describe nonclassical phenomena and quantum advantages, which have been linked with nonpositive entries of KD distribution.","Suppose that $\\mathcal{A}$ and $\\mathcal{B}$ are the eigenprojectors of the two eigenbases of two observables and the discrete Fourier transform (DFT) matrix is the transition matrix between the two eigenbases.","In a system with prime dimension, the set $\\mathcal{E}_{KD+}$ of KD positive states based on the DFT matrix is convex combinations of $\\mathcal{A}$ and $\\mathcal{B}$. That is, $\\mathcal{E}_{KD+}={\\rm conv}(\\mathcal{A}\\cup\\mathcal{B})$","[arXiv:2306.00086].","In this paper, we generalize the result.","That is, in a $d$-dimensional system, $\\mathcal{E}_{KD+}={\\rm conv}(\\Omega)$ for $d=p^{2}$ and $d=pq$, where $p, q$ are prime and $\\Omega$ is the set of projectors of all the pure KD positive states."],"url":"http://arxiv.org/abs/2404.09399v1","category":"quant-ph"}
{"created":"2024-04-15 01:07:57","title":"A Generic Approach to Fix Test Flakiness in Real-World Projects","abstract":"Test flakiness, a non-deterministic behavior of builds irrelevant to code changes, is a major and continuing impediment to delivering reliable software. The very few techniques for the automated repair of test flakiness are specifically crafted to repair either Order-Dependent (OD) or Implementation-Dependent (ID) flakiness. They are also all symbolic approaches, i.e., leverage program analysis to detect and repair known test flakiness patterns and root causes, failing to generalize. To bridge the gap, we propose FlakyDoctor, a neuro-symbolic technique that combines the power of LLMs-generalizability-and program analysis-soundness-to fix different types of test flakiness. Our extensive evaluation using 873 confirmed flaky tests (332 OD and 541 ID) from 243 real-world projects demonstrates the ability of FlakyDoctor in repairing flakiness, achieving 57% (OD) and 59% (ID) success rate. Comparing to three alternative flakiness repair approaches, FlakyDoctor can repair 8% more ID tests than DexFix, 12% more OD flaky tests than ODRepair, and 17% more OD flaky tests than iFixFlakies. Regardless of underlying LLM, the non-LLM components of FlakyDoctor contribute to 12-31% of the overall performance, i.e., while part of the FlakyDoctor power is from using LLMs, they are not good enough to repair flaky tests in real-world projects alone. What makes the proposed technique superior to related research on test flakiness mitigation specifically and program repair, in general, is repairing 79 previously unfixed flaky tests in real-world projects. We opened pull requests for all cases with corresponding patches; 19 of them were accepted and merged at the time of submission.","sentences":["Test flakiness, a non-deterministic behavior of builds irrelevant to code changes, is a major and continuing impediment to delivering reliable software.","The very few techniques for the automated repair of test flakiness are specifically crafted to repair either Order-Dependent (OD) or Implementation-Dependent (ID) flakiness.","They are also all symbolic approaches, i.e., leverage program analysis to detect and repair known test flakiness patterns and root causes, failing to generalize.","To bridge the gap, we propose FlakyDoctor, a neuro-symbolic technique that combines the power of LLMs-generalizability-and program analysis-soundness-to fix different types of test flakiness.","Our extensive evaluation using 873 confirmed flaky tests (332 OD and 541 ID) from 243 real-world projects demonstrates the ability of FlakyDoctor in repairing flakiness, achieving 57% (OD) and 59% (ID) success rate.","Comparing to three alternative flakiness repair approaches, FlakyDoctor can repair 8% more ID tests than DexFix, 12% more OD flaky tests than ODRepair, and 17% more OD flaky tests than iFixFlakies.","Regardless of underlying LLM, the non-LLM components of FlakyDoctor contribute to 12-31% of the overall performance, i.e., while part of the FlakyDoctor power is from using LLMs, they are not good enough to repair flaky tests in real-world projects alone.","What makes the proposed technique superior to related research on test flakiness mitigation specifically and program repair, in general, is repairing 79 previously unfixed flaky tests in real-world projects.","We opened pull requests for all cases with corresponding patches; 19 of them were accepted and merged at the time of submission."],"url":"http://arxiv.org/abs/2404.09398v1","category":"cs.SE"}
{"created":"2024-04-15 00:50:27","title":"Main $\\mathbf{Q}$-eigenvalues of quasi-threshold graphs","abstract":"In this note, we present a structural description of certain connected cographs having $k \\geq 2$ main signless Laplacian eigenvalues. This result allows us to characterize the cographs which are quasi-threshold graphs with two main $\\mathbf{Q}$-eigenvalues. In addition, we describe all the quasi-threshold graphs belonging to the subclass of generalized core-satellite graphs with $k \\geq 2$ main $\\mathbf{Q}$-eigenvalues.","sentences":["In this note, we present a structural description of certain connected cographs having $k \\geq 2$ main signless Laplacian eigenvalues.","This result allows us to characterize the cographs which are quasi-threshold graphs with two main $\\mathbf{Q}$-eigenvalues.","In addition, we describe all the quasi-threshold graphs belonging to the subclass of generalized core-satellite graphs with $k \\geq 2$ main $\\mathbf{Q}$-eigenvalues."],"url":"http://arxiv.org/abs/2404.09396v1","category":"math.CO"}
{"created":"2024-04-15 00:47:17","title":"Data Analysis Methods Preliminaries for a Photon-based Hardware Random Number Generator","abstract":"High quality random numbers are necessary in the modern world. Ranging from encryption keys in cyber security to models and simulations for scientific use: it's important that these random numbers are of high quality and quickly attainable. One common solution to the generation of random numbers is that of pseudo-random number generators, or PRNGs. PRNGs generate random numbers by first quantifying some unpredictable phenomena into a number or string and feeding it into an algorithm which yields numbers randomly based on that seed. Easy places to find seeds include the user's mouse movements or the machine's uptime. These are only pseudorandom, however, as if given the same seed twice, the PRNG would generate the same 'random' output. This is great for games like Minecraft, but not so great for cybersecurity encryption key generation. By using a hardware random number generator (HRNG), random numbers that are not susceptible to the flaws found in PRNGs can be attained at a high rate.","sentences":["High quality random numbers are necessary in the modern world.","Ranging from encryption keys in cyber security to models and simulations for scientific use: it's important that these random numbers are of high quality and quickly attainable.","One common solution to the generation of random numbers is that of pseudo-random number generators, or PRNGs.","PRNGs generate random numbers by first quantifying some unpredictable phenomena into a number or string and feeding it into an algorithm which yields numbers randomly based on that seed.","Easy places to find seeds include the user's mouse movements or the machine's uptime.","These are only pseudorandom, however, as if given the same seed twice, the PRNG would generate the same 'random' output.","This is great for games like Minecraft, but not so great for cybersecurity encryption key generation.","By using a hardware random number generator (HRNG), random numbers that are not susceptible to the flaws found in PRNGs can be attained at a high rate."],"url":"http://arxiv.org/abs/2404.09395v1","category":"cs.CR"}
{"created":"2024-04-15 00:23:41","title":"Privacy at a Price: Exploring its Dual Impact on AI Fairness","abstract":"The worldwide adoption of machine learning (ML) and deep learning models, particularly in critical sectors, such as healthcare and finance, presents substantial challenges in maintaining individual privacy and fairness. These two elements are vital to a trustworthy environment for learning systems. While numerous studies have concentrated on protecting individual privacy through differential privacy (DP) mechanisms, emerging research indicates that differential privacy in machine learning models can unequally impact separate demographic subgroups regarding prediction accuracy. This leads to a fairness concern, and manifests as biased performance. Although the prevailing view is that enhancing privacy intensifies fairness disparities, a smaller, yet significant, subset of research suggests the opposite view. In this article, with extensive evaluation results, we demonstrate that the impact of differential privacy on fairness is not monotonous. Instead, we observe that the accuracy disparity initially grows as more DP noise (enhanced privacy) is added to the ML process, but subsequently diminishes at higher privacy levels with even more noise. Moreover, implementing gradient clipping in the differentially private stochastic gradient descent ML method can mitigate the negative impact of DP noise on fairness. This mitigation is achieved by moderating the disparity growth through a lower clipping threshold.","sentences":["The worldwide adoption of machine learning (ML) and deep learning models, particularly in critical sectors, such as healthcare and finance, presents substantial challenges in maintaining individual privacy and fairness.","These two elements are vital to a trustworthy environment for learning systems.","While numerous studies have concentrated on protecting individual privacy through differential privacy (DP) mechanisms, emerging research indicates that differential privacy in machine learning models can unequally impact separate demographic subgroups regarding prediction accuracy.","This leads to a fairness concern, and manifests as biased performance.","Although the prevailing view is that enhancing privacy intensifies fairness disparities, a smaller, yet significant, subset of research suggests the opposite view.","In this article, with extensive evaluation results, we demonstrate that the impact of differential privacy on fairness is not monotonous.","Instead, we observe that the accuracy disparity initially grows as more DP noise (enhanced privacy) is added to the ML process, but subsequently diminishes at higher privacy levels with even more noise.","Moreover, implementing gradient clipping in the differentially private stochastic gradient descent ML method can mitigate the negative impact of DP noise on fairness.","This mitigation is achieved by moderating the disparity growth through a lower clipping threshold."],"url":"http://arxiv.org/abs/2404.09391v1","category":"cs.LG"}
{"created":"2024-04-15 00:12:27","title":"RankCLIP: Ranking-Consistent Language-Image Pretraining","abstract":"Among the ever-evolving development of vision-language models, contrastive language-image pretraining (CLIP) has set new benchmarks in many downstream tasks such as zero-shot classifications by leveraging self-supervised contrastive learning on large amounts of text-image pairs. However, its dependency on rigid one-to-one mappings overlooks the complex and often multifaceted relationships between and within texts and images. To this end, we introduce RankCLIP, a novel pretraining method that extends beyond the rigid one-to-one matching framework of CLIP and its variants. By leveraging both in-modal and cross-modal ranking consistency, RankCLIP improves the alignment process, enabling it to capture the nuanced many-to-many relationships between and within each modality. Through comprehensive experiments, we demonstrate the enhanced capability of RankCLIP to effectively improve performance across various downstream tasks, notably achieving significant gains in zero-shot classifications over state-of-the-art methods, underscoring the potential of RankCLIP in further advancing vision-language pretraining.","sentences":["Among the ever-evolving development of vision-language models, contrastive language-image pretraining (CLIP) has set new benchmarks in many downstream tasks such as zero-shot classifications by leveraging self-supervised contrastive learning on large amounts of text-image pairs.","However, its dependency on rigid one-to-one mappings overlooks the complex and often multifaceted relationships between and within texts and images.","To this end, we introduce RankCLIP, a novel pretraining method that extends beyond the rigid one-to-one matching framework of CLIP and its variants.","By leveraging both in-modal and cross-modal ranking consistency, RankCLIP improves the alignment process, enabling it to capture the nuanced many-to-many relationships between and within each modality.","Through comprehensive experiments, we demonstrate the enhanced capability of RankCLIP to effectively improve performance across various downstream tasks, notably achieving significant gains in zero-shot classifications over state-of-the-art methods, underscoring the potential of RankCLIP in further advancing vision-language pretraining."],"url":"http://arxiv.org/abs/2404.09387v1","category":"cs.CV"}
{"created":"2024-04-14 23:45:23","title":"Tasks People Prompt: A Taxonomy of LLM Downstream Tasks in Software Verification and Falsification Approaches","abstract":"Prompting has become one of the main approaches to leverage emergent capabilities of Large Language Models [Brown et al. NeurIPS 2020, Wei et al. TMLR 2022, Wei et al. NeurIPS 2022]. During the last year, researchers and practitioners have been playing with prompts to see how to make the most of LLMs. By homogeneously dissecting 80 papers, we investigate in deep how software testing and verification research communities have been abstractly architecting their LLM-enabled solutions. More precisely, first, we want to validate whether downstream tasks are an adequate concept to convey the blueprint of prompt-based solutions. We also aim at identifying number and nature of such tasks in solutions. For such goal, we develop a novel downstream task taxonomy that enables pinpointing some engineering patterns in a rather varied spectrum of Software Engineering problems that encompasses testing, fuzzing, debugging, vulnerability detection, static analysis and program verification approaches.","sentences":["Prompting has become one of the main approaches to leverage emergent capabilities of Large Language Models","[Brown et al. NeurIPS 2020, Wei et al. TMLR 2022, Wei et al. NeurIPS 2022].","During the last year, researchers and practitioners have been playing with prompts to see how to make the most of LLMs.","By homogeneously dissecting 80 papers, we investigate in deep how software testing and verification research communities have been abstractly architecting their LLM-enabled solutions.","More precisely, first, we want to validate whether downstream tasks are an adequate concept to convey the blueprint of prompt-based solutions.","We also aim at identifying number and nature of such tasks in solutions.","For such goal, we develop a novel downstream task taxonomy that enables pinpointing some engineering patterns in a rather varied spectrum of Software Engineering problems that encompasses testing, fuzzing, debugging, vulnerability detection, static analysis and program verification approaches."],"url":"http://arxiv.org/abs/2404.09384v1","category":"cs.SE"}
{"created":"2024-04-14 23:42:01","title":"Implicit EXP-RBF techniques for modeling unsaturated flow through soils with water uptake by plant roots","abstract":"Modeling unsaturated flow through soils with water uptake by plan root has many applications in agriculture and water resources management. In this study, our aim is to develop efficient numerical techniques for solving the Richards equation with a sink term due to plant root water uptake. The Feddes model is used for water absorption by plant roots, and the van-Genuchten model is employed for capillary pressure. We introduce a numerical approach that combines the localized exponential radial basis function (EXP-RBF) method for space and the second-order backward differentiation formula (BDF2) for temporal discretization. The localized RBF methods eliminate the need for mesh generation and avoid ill-conditioning problems. This approach yields a sparse matrix for the global system, optimizing memory usage and computational time. The proposed implicit EXP-RBF techniques have advantages in terms of accuracy and computational efficiency thanks to the use of BDF2 and the localized RBF method. Modified Picards iteration method for the mixed form of the Richards equation is employed to linearize the system. Various numerical experiments are conducted to validate the proposed numerical model of infiltration with plant root water absorption. The obtained results conclusively demonstrate the effectiveness of the proposed numerical model in accurately predicting soil moisture dynamics under water uptake by plant roots. The proposed numerical techniques can be incorporated in the numerical models where unsaturated flows and water uptake by plant roots are involved such as in hydrology, agriculture, and water management.","sentences":["Modeling unsaturated flow through soils with water uptake by plan root has many applications in agriculture and water resources management.","In this study, our aim is to develop efficient numerical techniques for solving the Richards equation with a sink term due to plant root water uptake.","The Feddes model is used for water absorption by plant roots, and the van-Genuchten model is employed for capillary pressure.","We introduce a numerical approach that combines the localized exponential radial basis function (EXP-RBF) method for space and the second-order backward differentiation formula (BDF2) for temporal discretization.","The localized RBF methods eliminate the need for mesh generation and avoid ill-conditioning problems.","This approach yields a sparse matrix for the global system, optimizing memory usage and computational time.","The proposed implicit EXP-RBF techniques have advantages in terms of accuracy and computational efficiency thanks to the use of BDF2 and the localized RBF method.","Modified Picards iteration method for the mixed form of the Richards equation is employed to linearize the system.","Various numerical experiments are conducted to validate the proposed numerical model of infiltration with plant root water absorption.","The obtained results conclusively demonstrate the effectiveness of the proposed numerical model in accurately predicting soil moisture dynamics under water uptake by plant roots.","The proposed numerical techniques can be incorporated in the numerical models where unsaturated flows and water uptake by plant roots are involved such as in hydrology, agriculture, and water management."],"url":"http://arxiv.org/abs/2404.09382v1","category":"math.NA"}
{"created":"2024-04-14 23:33:51","title":"Disentangling modified gravity from a dark force with gravitational redshift","abstract":"The standard approach to test for deviations from General Relativity on cosmological scales is to combine measurements of the growth rate of structure with gravitational lensing. In this study, we show that this method suffers from an important limitation with regard to these two probes: models of dark matter with additional interactions can lead to the very same observational signatures found in modified gravity (and viceversa). Using synthetic data of redshift-space distortions, weak lensing, and cosmic microwave background, we demonstrate that this degeneracy is inevitable between modifications of gravity and a dark fifth force. We then show that the coming generation of surveys, in particular the Square Kilometer Array, will allow us to break the degeneracy between such models through measurements of gravitational redshift. Performing a Markov Chain Monte Carlo analysis of the synthetic data set, we quantify the extent to which gravitational redshift can distinguish between two representative classes of models, Generalized Brans-Dicke (modified gravity) and Coupled Quintessence (fifth force).","sentences":["The standard approach to test for deviations from General Relativity on cosmological scales is to combine measurements of the growth rate of structure with gravitational lensing.","In this study, we show that this method suffers from an important limitation with regard to these two probes: models of dark matter with additional interactions can lead to the very same observational signatures found in modified gravity (and viceversa).","Using synthetic data of redshift-space distortions, weak lensing, and cosmic microwave background, we demonstrate that this degeneracy is inevitable between modifications of gravity and a dark fifth force.","We then show that the coming generation of surveys, in particular the Square Kilometer Array, will allow us to break the degeneracy between such models through measurements of gravitational redshift.","Performing a Markov Chain Monte Carlo analysis of the synthetic data set, we quantify the extent to which gravitational redshift can distinguish between two representative classes of models, Generalized Brans-Dicke (modified gravity) and Coupled Quintessence (fifth force)."],"url":"http://arxiv.org/abs/2404.09379v1","category":"astro-ph.CO"}
{"created":"2024-04-14 23:30:35","title":"Orientation-conditioned Facial Texture Mapping for Video-based Facial Remote Photoplethysmography Estimation","abstract":"Camera-based remote photoplethysmography (rPPG) enables contactless measurement of important physiological signals such as pulse rate (PR). However, dynamic and unconstrained subject motion introduces significant variability into the facial appearance in video, confounding the ability of video-based methods to accurately extract the rPPG signal. In this study, we leverage the 3D facial surface to construct a novel orientation-conditioned facial texture video representation which improves the motion robustness of existing video-based facial rPPG estimation methods. Our proposed method achieves a significant 18.2% performance improvement in cross-dataset testing on MMPD over our baseline using the PhysNet model trained on PURE, highlighting the efficacy and generalization benefits of our designed video representation. We demonstrate significant performance improvements of up to 29.6% in all tested motion scenarios in cross-dataset testing on MMPD, even in the presence of dynamic and unconstrained subject motion. Emphasizing the benefits the benefits of disentangling motion through modeling the 3D facial surface for motion robust facial rPPG estimation. We validate the efficacy of our design decisions and the impact of different video processing steps through an ablation study. Our findings illustrate the potential strengths of exploiting the 3D facial surface as a general strategy for addressing dynamic and unconstrained subject motion in videos. The code is available at https://samcantrill.github.io/orientation-uv-rppg/.","sentences":["Camera-based remote photoplethysmography (rPPG) enables contactless measurement of important physiological signals such as pulse rate (PR).","However, dynamic and unconstrained subject motion introduces significant variability into the facial appearance in video, confounding the ability of video-based methods to accurately extract the rPPG signal.","In this study, we leverage the 3D facial surface to construct a novel orientation-conditioned facial texture video representation which improves the motion robustness of existing video-based facial rPPG estimation methods.","Our proposed method achieves a significant 18.2% performance improvement in cross-dataset testing on MMPD over our baseline using the PhysNet model trained on PURE, highlighting the efficacy and generalization benefits of our designed video representation.","We demonstrate significant performance improvements of up to 29.6% in all tested motion scenarios in cross-dataset testing on MMPD, even in the presence of dynamic and unconstrained subject motion.","Emphasizing the benefits the benefits of disentangling motion through modeling the 3D facial surface for motion robust facial rPPG estimation.","We validate the efficacy of our design decisions and the impact of different video processing steps through an ablation study.","Our findings illustrate the potential strengths of exploiting the 3D facial surface as a general strategy for addressing dynamic and unconstrained subject motion in videos.","The code is available at https://samcantrill.github.io/orientation-uv-rppg/."],"url":"http://arxiv.org/abs/2404.09378v1","category":"cs.CV"}
{"created":"2024-04-14 23:28:47","title":"Linear stability of vector Horndeski black holes","abstract":"Horndeski's vector-tensor (HVT) gravity is described by a Lagrangian in which the field strength $F_{\\mu \\nu}=\\partial_{\\mu} A_{\\nu}-\\partial_{\\nu} A_{\\mu}$ of a vector field $A_{\\mu}$ interacts with a double dual Riemann tensor $L^{\\mu \\nu \\alpha \\beta}$ in the form $\\beta L^{\\mu \\nu \\alpha \\beta} F_{\\mu \\nu} F_{\\alpha \\beta}$, where $\\beta$ is a constant. In Einstein-Maxwell-HVT theory, there are static and spherically symmetric black hole (BH) solutions with electric or magnetic charges, whose metric components are modified from those in the Reissner-Nordstr\\\"om geometry. The electric-magnetic duality of solutions is broken even at the background level by the nonvanishing coupling constant $\\beta$. We compute a second-order action of BH perturbations containing both the odd- and even-parity modes and show that there are four dynamical perturbations arising from the gravitational and vector-field sectors. We derive all the linear stability conditions associated with the absence of ghosts and radial/angular Laplacian instabilities for both the electric and magnetic BHs. These conditions exhibit the difference between the electrically and magnetically charged cases by reflecting the breaking of electric-magnetic duality at the level of perturbations. In particular, the four angular propagation speeds in the large-multipole limit are different from each other for both the electric and magnetic BHs. This suggests the breaking of eikonal correspondence between the peak position of at least one of the potentials of dynamical perturbations and the radius of photon sphere. For the electrically and magnetically charged cases, we elucidate parameter spaces of the HVT coupling and the BH charge in which the BHs without naked singularities are linearly stable.","sentences":["Horndeski's vector-tensor (HVT) gravity is described by a Lagrangian in which the field strength $F_{\\mu \\nu}=\\partial_{\\mu} A_{\\nu}-\\partial_{\\nu} A_{\\mu}$ of a vector field $A_{\\mu}$ interacts with a double dual Riemann tensor $L^{\\mu \\nu \\alpha \\beta}$ in the form $\\beta L^{\\mu \\nu \\alpha \\beta} F_{\\mu \\nu} F_{\\alpha \\beta}$, where $\\beta$ is a constant.","In Einstein-Maxwell-HVT theory, there are static and spherically symmetric black hole (BH) solutions with electric or magnetic charges, whose metric components are modified from those in the Reissner-Nordstr\\\"om geometry.","The electric-magnetic duality of solutions is broken even at the background level by the nonvanishing coupling constant $\\beta$. We compute a second-order action of BH perturbations containing both the odd- and even-parity modes and show that there are four dynamical perturbations arising from the gravitational and vector-field sectors.","We derive all the linear stability conditions associated with the absence of ghosts and radial/angular Laplacian instabilities for both the electric and magnetic BHs.","These conditions exhibit the difference between the electrically and magnetically charged cases by reflecting the breaking of electric-magnetic duality at the level of perturbations.","In particular, the four angular propagation speeds in the large-multipole limit are different from each other for both the electric and magnetic BHs.","This suggests the breaking of eikonal correspondence between the peak position of at least one of the potentials of dynamical perturbations and the radius of photon sphere.","For the electrically and magnetically charged cases, we elucidate parameter spaces of the HVT coupling and the BH charge in which the BHs without naked singularities are linearly stable."],"url":"http://arxiv.org/abs/2404.09377v1","category":"gr-qc"}
{"created":"2024-04-14 23:05:10","title":"Deceptive Patterns of Intelligent and Interactive Writing Assistants","abstract":"Large Language Models have become an integral part of new intelligent and interactive writing assistants. Many are offered commercially with a chatbot-like UI, such as ChatGPT, and provide little information about their inner workings. This makes this new type of widespread system a potential target for deceptive design patterns. For example, such assistants might exploit hidden costs by providing guidance up until a certain point before asking for a fee to see the rest. As another example, they might sneak unwanted content/edits into longer generated or revised text pieces (e.g. to influence the expressed opinion). With these and other examples, we conceptually transfer several deceptive patterns from the literature to the new context of AI writing assistants. Our goal is to raise awareness and encourage future research into how the UI and interaction design of such systems can impact people and their writing.","sentences":["Large Language Models have become an integral part of new intelligent and interactive writing assistants.","Many are offered commercially with a chatbot-like UI, such as ChatGPT, and provide little information about their inner workings.","This makes this new type of widespread system a potential target for deceptive design patterns.","For example, such assistants might exploit hidden costs by providing guidance up until a certain point before asking for a fee to see the rest.","As another example, they might sneak unwanted content/edits into longer generated or revised text pieces (e.g. to influence the expressed opinion).","With these and other examples, we conceptually transfer several deceptive patterns from the literature to the new context of AI writing assistants.","Our goal is to raise awareness and encourage future research into how the UI and interaction design of such systems can impact people and their writing."],"url":"http://arxiv.org/abs/2404.09375v1","category":"cs.HC"}
{"created":"2024-04-14 22:55:49","title":"Classical double copy in the black hole mini-superspace","abstract":"We give a novel formulation of classical double copy in the mini-superspace of static, spherically symmetric black holes where the map between the solutions of general relativity and Maxwell's theory can be realized in Boyer-Lindsquit coordinates. By employing the reduced action principle, we show that the double copy structure can be generalized to Lovelock gravities and quasi-topological gravities. However, the gravitational solutions are mapped to the purely electric solutions of Maxwell's theory: instead of a direct match between the Kerr-Schild scalar in the gravity side and the scalar potential in the gauge theory side, the scalar potential becomes a polynomial in the Kerr-Schild scalar, giving rise to a generalization of the Kerr-Schild double copy. We calculate the Regge-Teitelboim surface charges and prove that the mass of the black hole solution is identified with the electric charge corresponding to the Coulomb part of the gauge theory solution in the same way it does in the case of general relativity.","sentences":["We give a novel formulation of classical double copy in the mini-superspace of static, spherically symmetric black holes where the map between the solutions of general relativity and Maxwell's theory can be realized in Boyer-Lindsquit coordinates.","By employing the reduced action principle, we show that the double copy structure can be generalized to Lovelock gravities and quasi-topological gravities.","However, the gravitational solutions are mapped to the purely electric solutions of Maxwell's theory: instead of a direct match between the Kerr-Schild scalar in the gravity side and the scalar potential in the gauge theory side, the scalar potential becomes a polynomial in the Kerr-Schild scalar, giving rise to a generalization of the Kerr-Schild double copy.","We calculate the Regge-Teitelboim surface charges and prove that the mass of the black hole solution is identified with the electric charge corresponding to the Coulomb part of the gauge theory solution in the same way it does in the case of general relativity."],"url":"http://arxiv.org/abs/2404.09374v1","category":"gr-qc"}
{"created":"2024-04-14 22:22:58","title":"The Effect of Data Partitioning Strategy on Model Generalizability: A Case Study of Morphological Segmentation","abstract":"Recent work to enhance data partitioning strategies for more realistic model evaluation face challenges in providing a clear optimal choice. This study addresses these challenges, focusing on morphological segmentation and synthesizing limitations related to language diversity, adoption of multiple datasets and splits, and detailed model comparisons. Our study leverages data from 19 languages, including ten indigenous or endangered languages across 10 language families with diverse morphological systems (polysynthetic, fusional, and agglutinative) and different degrees of data availability. We conduct large-scale experimentation with varying sized combinations of training and evaluation sets as well as new test data. Our results show that, when faced with new test data: (1) models trained from random splits are able to achieve higher numerical scores; (2) model rankings derived from random splits tend to generalize more consistently.","sentences":["Recent work to enhance data partitioning strategies for more realistic model evaluation face challenges in providing a clear optimal choice.","This study addresses these challenges, focusing on morphological segmentation and synthesizing limitations related to language diversity, adoption of multiple datasets and splits, and detailed model comparisons.","Our study leverages data from 19 languages, including ten indigenous or endangered languages across 10 language families with diverse morphological systems (polysynthetic, fusional, and agglutinative) and different degrees of data availability.","We conduct large-scale experimentation with varying sized combinations of training and evaluation sets as well as new test data.","Our results show that, when faced with new test data: (1) models trained from random splits are able to achieve higher numerical scores; (2) model rankings derived from random splits tend to generalize more consistently."],"url":"http://arxiv.org/abs/2404.09371v1","category":"cs.CL"}
{"created":"2024-04-14 22:13:36","title":"Perelman singular manifolds","abstract":"On a Riemannian manifold with a smooth function $f: M\\to \\mathbb{R}$, we consider the linearization of the Perelman scalar curvature $\\mathcal{R}$ and its $L^2$-formal adjoint operator $\\delta\\mathcal{R}^*$. A manifold endowed with a metric $g$ whose operator $\\delta\\mathcal{R}^*$ has a nontrivial kernel is called a Perelman singular manifold. In this paper, we present examples and apply general maximum principles to obtain rigidity or nonexistence results in the underlying setting.","sentences":["On a Riemannian manifold with a smooth function $f: M\\to \\mathbb{R}$, we consider the linearization of the Perelman scalar curvature $\\mathcal{R}$ and its $L^2$-formal adjoint operator $\\delta\\mathcal{R}^*$. A manifold endowed with a metric $g$ whose operator $\\delta\\mathcal{R}^*$ has a nontrivial kernel is called a Perelman singular manifold.","In this paper, we present examples and apply general maximum principles to obtain rigidity or nonexistence results in the underlying setting."],"url":"http://arxiv.org/abs/2404.09369v1","category":"math.DG"}
{"created":"2024-04-14 21:38:50","title":"Understanding the Role of Temperature in Diverse Question Generation by GPT-4","abstract":"We conduct a preliminary study of the effect of GPT's temperature parameter on the diversity of GPT4-generated questions. We find that using higher temperature values leads to significantly higher diversity, with different temperatures exposing different types of similarity between generated sets of questions. We also demonstrate that diverse question generation is especially difficult for questions targeting lower levels of Bloom's Taxonomy.","sentences":["We conduct a preliminary study of the effect of GPT's temperature parameter on the diversity of GPT4-generated questions.","We find that using higher temperature values leads to significantly higher diversity, with different temperatures exposing different types of similarity between generated sets of questions.","We also demonstrate that diverse question generation is especially difficult for questions targeting lower levels of Bloom's Taxonomy."],"url":"http://arxiv.org/abs/2404.09366v1","category":"cs.CL"}
{"created":"2024-04-14 21:32:05","title":"Late time decay of scalar and Dirac fields around an asymptotically de Sitter black hole in the Euler-Heisenberg electrodynamics","abstract":"We compute the quasinormal modes of massive scalar and Dirac fields within the framework of asymptotically de Sitter black holes in Euler-Heisenberg non-linear electrodynamics. We pay particular attention to the regime $\\mu M/m_{P}^2 \\gg 1$, where $\\mu$ and $M$ denote the masses of the field and the black hole, respectively, and $m_{P}$ represents the Planck mass, covering a range from primordial to large astrophysical black holes. Through time-domain integration, we demonstrate that, contrary to the asymptotically flat case, the quasinormal modes also dictate the asymptotic decay of fields. Employing the 6th order WKB formula, we derive a precise analytic approximation for quasinormal modes in the regime $\\mu M/m_{P}^2 \\gg 1$ without resorting to expansion in terms of the inverse multipole number. This analytic expression takes on a concise form in the limit of linear electrodynamics, represented by the Reissner-Nordstr\\\"om black holes.","sentences":["We compute the quasinormal modes of massive scalar and Dirac fields within the framework of asymptotically de Sitter black holes in Euler-Heisenberg non-linear electrodynamics.","We pay particular attention to the regime $\\mu M/m_{P}^2 \\gg 1$, where $\\mu$ and $M$ denote the masses of the field and the black hole, respectively, and $m_{P}$ represents the Planck mass, covering a range from primordial to large astrophysical black holes.","Through time-domain integration, we demonstrate that, contrary to the asymptotically flat case, the quasinormal modes also dictate the asymptotic decay of fields.","Employing the 6th order WKB formula, we derive a precise analytic approximation for quasinormal modes in the regime $\\mu M/m_{P}^2 \\gg 1$ without resorting to expansion in terms of the inverse multipole number.","This analytic expression takes on a concise form in the limit of linear electrodynamics, represented by the Reissner-Nordstr\\\"om black holes."],"url":"http://arxiv.org/abs/2404.09364v1","category":"gr-qc"}
{"created":"2024-04-14 21:30:00","title":"Momentum-based gradient descent methods for Lie groups","abstract":"Polyak's Heavy Ball (PHB; Polyak, 1964), a.k.a. Classical Momentum, and Nesterov's Accelerated Gradient (NAG; Nesterov, 1983) are well know examples of momentum-descent methods for optimization. While the latter outperforms the former, solely generalizations of PHB-like methods to nonlinear spaces have been described in the literature. We propose here a generalization of NAG-like methods for Lie group optimization based on the variational one-to-one correspondence between classical and accelerated momentum methods (Campos et al., 2023). Numerical experiments are shown.","sentences":["Polyak's Heavy Ball (PHB; Polyak, 1964), a.k.a. Classical Momentum, and Nesterov's Accelerated Gradient (NAG; Nesterov, 1983) are well know examples of momentum-descent methods for optimization.","While the latter outperforms the former, solely generalizations of PHB-like methods to nonlinear spaces have been described in the literature.","We propose here a generalization of NAG-like methods for Lie group optimization based on the variational one-to-one correspondence between classical and accelerated momentum methods (Campos et al., 2023).","Numerical experiments are shown."],"url":"http://arxiv.org/abs/2404.09363v1","category":"math.OC"}
{"created":"2024-04-14 21:14:47","title":"Exploring Feedback Generation in Automated Skeletal Movement Assessment: A Comprehensive Overview","abstract":"The application of machine-learning solutions to movement assessment from skeleton videos has attracted significant research attention in recent years. This advancement has made rehabilitation at home more accessible, utilizing movement assessment algorithms that can operate on affordable equipment for human pose detection from 2D or 3D videos. While the primary objective of automatic assessment tasks is to score movements, the automatic generation of feedback highlighting key movement issues has the potential to significantly enhance and accelerate the rehabilitation process. In this study, we explain the types of feedback that can be generated, review existing solutions for automatic feedback generation, and discuss future research directions. To our knowledge, this is the first comprehensive review of feedback generation in skeletal movement assessment.","sentences":["The application of machine-learning solutions to movement assessment from skeleton videos has attracted significant research attention in recent years.","This advancement has made rehabilitation at home more accessible, utilizing movement assessment algorithms that can operate on affordable equipment for human pose detection from 2D or 3D videos.","While the primary objective of automatic assessment tasks is to score movements, the automatic generation of feedback highlighting key movement issues has the potential to significantly enhance and accelerate the rehabilitation process.","In this study, we explain the types of feedback that can be generated, review existing solutions for automatic feedback generation, and discuss future research directions.","To our knowledge, this is the first comprehensive review of feedback generation in skeletal movement assessment."],"url":"http://arxiv.org/abs/2404.09359v1","category":"cs.CV"}
{"created":"2024-04-14 20:49:53","title":"LLeMpower: Understanding Disparities in the Control and Access of Large Language Models","abstract":"Large Language Models (LLMs) are a powerful technology that augment human skill to create new opportunities, akin to the development of steam engines and the internet. However, LLMs come with a high cost. They require significant computing resources and energy to train and serve. Inequity in their control and access has led to concentration of ownership and power to a small collection of corporations. In our study, we collect training and inference requirements for various LLMs. We then analyze the economic strengths of nations and organizations in the context of developing and serving these models. Additionally, we also look at whether individuals around the world can access and use this emerging technology. We compare and contrast these groups to show that these technologies are monopolized by a surprisingly few entities. We conclude with a qualitative study on the ethical implications of our findings and discuss future directions towards equity in LLM access.","sentences":["Large Language Models (LLMs) are a powerful technology that augment human skill to create new opportunities, akin to the development of steam engines and the internet.","However, LLMs come with a high cost.","They require significant computing resources and energy to train and serve.","Inequity in their control and access has led to concentration of ownership and power to a small collection of corporations.","In our study, we collect training and inference requirements for various LLMs.","We then analyze the economic strengths of nations and organizations in the context of developing and serving these models.","Additionally, we also look at whether individuals around the world can access and use this emerging technology.","We compare and contrast these groups to show that these technologies are monopolized by a surprisingly few entities.","We conclude with a qualitative study on the ethical implications of our findings and discuss future directions towards equity in LLM access."],"url":"http://arxiv.org/abs/2404.09356v1","category":"cs.CY"}
{"created":"2024-04-14 20:33:39","title":"A Unified Combination Framework for Dependent Tests with Applications to Microbiome Association Studies","abstract":"We introduce a novel meta-analysis framework to combine dependent tests under a general setting, and utilize it to synthesize various microbiome association tests that are calculated from the same dataset. Our development builds upon the classical meta-analysis methods of aggregating $p$-values and also a more recent general method of combining confidence distributions, but makes generalizations to handle dependent tests. The proposed framework ensures rigorous statistical guarantees, and we provide a comprehensive study and compare it with various existing dependent combination methods. Notably, we demonstrate that the widely used Cauchy combination method for dependent tests, referred to as the vanilla Cauchy combination in this article, can be viewed as a special case within our framework. Moreover, the proposed framework provides a way to address the problem when the distributional assumptions underlying the vanilla Cauchy combination are violated. Our numerical results demonstrate that ignoring the dependence among the to-be-combined components may lead to a severe size distortion phenomenon. Compared to the existing $p$-value combination methods, including the vanilla Cauchy combination method, the proposed combination framework can handle the dependence accurately and utilizes the information efficiently to construct tests with accurate size and enhanced power. The development is applied to Microbiome Association Studies, where we aggregate information from multiple existing tests using the same dataset. The combined tests harness the strengths of each individual test across a wide range of alternative spaces, %resulting in a significant enhancement of testing power across a wide range of alternative spaces, enabling more efficient and meaningful discoveries of vital microbiome associations.","sentences":["We introduce a novel meta-analysis framework to combine dependent tests under a general setting, and utilize it to synthesize various microbiome association tests that are calculated from the same dataset.","Our development builds upon the classical meta-analysis methods of aggregating $p$-values and also a more recent general method of combining confidence distributions, but makes generalizations to handle dependent tests.","The proposed framework ensures rigorous statistical guarantees, and we provide a comprehensive study and compare it with various existing dependent combination methods.","Notably, we demonstrate that the widely used Cauchy combination method for dependent tests, referred to as the vanilla Cauchy combination in this article, can be viewed as a special case within our framework.","Moreover, the proposed framework provides a way to address the problem when the distributional assumptions underlying the vanilla Cauchy combination are violated.","Our numerical results demonstrate that ignoring the dependence among the to-be-combined components may lead to a severe size distortion phenomenon.","Compared to the existing $p$-value combination methods, including the vanilla Cauchy combination method, the proposed combination framework can handle the dependence accurately and utilizes the information efficiently to construct tests with accurate size and enhanced power.","The development is applied to Microbiome Association Studies, where we aggregate information from multiple existing tests using the same dataset.","The combined tests harness the strengths of each individual test across a wide range of alternative spaces, %resulting in a significant enhancement of testing power across a wide range of alternative spaces, enabling more efficient and meaningful discoveries of vital microbiome associations."],"url":"http://arxiv.org/abs/2404.09353v1","category":"stat.ME"}
{"created":"2024-04-14 20:28:07","title":"Counteracting Concept Drift by Learning with Future Malware Predictions","abstract":"The accuracy of deployed malware-detection classifiers degrades over time due to changes in data distributions and increasing discrepancies between training and testing data. This phenomenon is known as the concept drift. While the concept drift can be caused by various reasons in general, new malicious files are created by malware authors with a clear intention of avoiding detection. The existence of the intention opens a possibility for predicting such future samples. Including predicted samples in training data should consequently increase the accuracy of the classifiers on new testing data.   We compare two methods for predicting future samples: (1) adversarial training and (2) generative adversarial networks (GANs). The first method explicitly seeks for adversarial examples against the classifier that are then used as a part of training data. Similarly, GANs also generate synthetic training data. We use GANs to learn changes in data distributions within different time periods of training data and then apply these changes to generate samples that could be in testing data. We compare these prediction methods on two different datasets: (1) Ember public dataset and (2) the internal dataset of files incoming to Avast. We show that while adversarial training yields more robust classifiers, this method is not a good predictor of future malware in general. This is in contrast with previously reported positive results in different domains (including natural language processing and spam detection). On the other hand, we show that GANs can be successfully used as predictors of future malware. We specifically examine malware families that exhibit significant changes in their data distributions over time and the experimental results confirm that GAN-based predictions can significantly improve the accuracy of the classifier on new, previously unseen data.","sentences":["The accuracy of deployed malware-detection classifiers degrades over time due to changes in data distributions and increasing discrepancies between training and testing data.","This phenomenon is known as the concept drift.","While the concept drift can be caused by various reasons in general, new malicious files are created by malware authors with a clear intention of avoiding detection.","The existence of the intention opens a possibility for predicting such future samples.","Including predicted samples in training data should consequently increase the accuracy of the classifiers on new testing data.   ","We compare two methods for predicting future samples: (1) adversarial training and (2) generative adversarial networks (GANs).","The first method explicitly seeks for adversarial examples against the classifier that are then used as a part of training data.","Similarly, GANs also generate synthetic training data.","We use GANs to learn changes in data distributions within different time periods of training data and then apply these changes to generate samples that could be in testing data.","We compare these prediction methods on two different datasets: (1) Ember public dataset and (2) the internal dataset of files incoming to Avast.","We show that while adversarial training yields more robust classifiers, this method is not a good predictor of future malware in general.","This is in contrast with previously reported positive results in different domains (including natural language processing and spam detection).","On the other hand, we show that GANs can be successfully used as predictors of future malware.","We specifically examine malware families that exhibit significant changes in their data distributions over time and the experimental results confirm that GAN-based predictions can significantly improve the accuracy of the classifier on new, previously unseen data."],"url":"http://arxiv.org/abs/2404.09352v1","category":"cs.CR"}
{"created":"2024-04-14 20:14:38","title":"Adversarial Robustness Limits via Scaling-Law and Human-Alignment Studies","abstract":"This paper revisits the simple, long-studied, yet still unsolved problem of making image classifiers robust to imperceptible perturbations. Taking CIFAR10 as an example, SOTA clean accuracy is about $100$%, but SOTA robustness to $\\ell_{\\infty}$-norm bounded perturbations barely exceeds $70$%. To understand this gap, we analyze how model size, dataset size, and synthetic data quality affect robustness by developing the first scaling laws for adversarial training. Our scaling laws reveal inefficiencies in prior art and provide actionable feedback to advance the field. For instance, we discovered that SOTA methods diverge notably from compute-optimal setups, using excess compute for their level of robustness. Leveraging a compute-efficient setup, we surpass the prior SOTA with $20$% ($70$%) fewer training (inference) FLOPs. We trained various compute-efficient models, with our best achieving $74$% AutoAttack accuracy ($+3$% gain). However, our scaling laws also predict robustness slowly grows then plateaus at $90$%: dwarfing our new SOTA by scaling is impractical, and perfect robustness is impossible. To better understand this predicted limit, we carry out a small-scale human evaluation on the AutoAttack data that fools our top-performing model. Concerningly, we estimate that human performance also plateaus near $90$%, which we show to be attributable to $\\ell_{\\infty}$-constrained attacks' generation of invalid images not consistent with their original labels. Having characterized limiting roadblocks, we outline promising paths for future research.","sentences":["This paper revisits the simple, long-studied, yet still unsolved problem of making image classifiers robust to imperceptible perturbations.","Taking CIFAR10 as an example, SOTA clean accuracy is about $100$%, but SOTA robustness to $\\ell_{\\infty}$-norm bounded perturbations barely exceeds $70$%.","To understand this gap, we analyze how model size, dataset size, and synthetic data quality affect robustness by developing the first scaling laws for adversarial training.","Our scaling laws reveal inefficiencies in prior art and provide actionable feedback to advance the field.","For instance, we discovered that SOTA methods diverge notably from compute-optimal setups, using excess compute for their level of robustness.","Leveraging a compute-efficient setup, we surpass the prior SOTA with $20$% ($70$%) fewer training (inference) FLOPs.","We trained various compute-efficient models, with our best achieving $74$% AutoAttack accuracy ($+3$% gain).","However, our scaling laws also predict robustness slowly grows then plateaus at $90$%: dwarfing our new SOTA by scaling is impractical, and perfect robustness is impossible.","To better understand this predicted limit, we carry out a small-scale human evaluation on the AutoAttack data that fools our top-performing model.","Concerningly, we estimate that human performance also plateaus near $90$%, which we show to be attributable to $\\ell_{\\infty}$-constrained attacks' generation of invalid images not consistent with their original labels.","Having characterized limiting roadblocks, we outline promising paths for future research."],"url":"http://arxiv.org/abs/2404.09349v1","category":"cs.LG"}
{"created":"2024-04-14 19:52:05","title":"Remarks on quasilocal mass and fill-ins","abstract":"In this paper we would have a brief overview of several proposals of quasilocal mass which are based on Hamiltonian formulation. We also show the positivity of the Wang-Yau energy under a more general condition. We then further study the quasilocal mass and DEC fill-ins defined by the author in terms of completeness and shields.","sentences":["In this paper we would have a brief overview of several proposals of quasilocal mass which are based on Hamiltonian formulation.","We also show the positivity of the Wang-Yau energy under a more general condition.","We then further study the quasilocal mass and DEC fill-ins defined by the author in terms of completeness and shields."],"url":"http://arxiv.org/abs/2404.09343v1","category":"math.DG"}
{"created":"2024-04-14 19:45:47","title":"Towards Practical Tool Usage for Continually Learning LLMs","abstract":"Large language models (LLMs) show an innate skill for solving language based tasks. But insights have suggested an inability to adjust for information or task-solving skills becoming outdated, as their knowledge, stored directly within their parameters, remains static in time. Tool use helps by offloading work to systems that the LLM can access through an interface, but LLMs that use them still must adapt to nonstationary environments for prolonged use, as new tools can emerge and existing tools can change. Nevertheless, tools require less specialized knowledge, therefore we hypothesize they are better suited for continual learning (CL) as they rely less on parametric memory for solving tasks and instead focus on learning when to apply pre-defined tools. To verify this, we develop a synthetic benchmark and follow this by aggregating existing NLP tasks to form a more realistic testing scenario. While we demonstrate scaling model size is not a solution, regardless of tool usage, continual learning techniques can enable tool LLMs to both adapt faster while forgetting less, highlighting their potential as continual learners.","sentences":["Large language models (LLMs) show an innate skill for solving language based tasks.","But insights have suggested an inability to adjust for information or task-solving skills becoming outdated, as their knowledge, stored directly within their parameters, remains static in time.","Tool use helps by offloading work to systems that the LLM can access through an interface, but LLMs that use them still must adapt to nonstationary environments for prolonged use, as new tools can emerge and existing tools can change.","Nevertheless, tools require less specialized knowledge, therefore we hypothesize they are better suited for continual learning (CL) as they rely less on parametric memory for solving tasks and instead focus on learning when to apply pre-defined tools.","To verify this, we develop a synthetic benchmark and follow this by aggregating existing NLP tasks to form a more realistic testing scenario.","While we demonstrate scaling model size is not a solution, regardless of tool usage, continual learning techniques can enable tool LLMs to both adapt faster while forgetting less, highlighting their potential as continual learners."],"url":"http://arxiv.org/abs/2404.09339v1","category":"cs.CL"}
{"created":"2024-04-14 19:45:35","title":"Entropy Guided Extrapolative Decoding to Improve Factuality in Large Language Models","abstract":"Large language models (LLMs) exhibit impressive natural language capabilities but suffer from hallucination -- generating content ungrounded in the realities of training data. Recent work has focused on decoding techniques to improve factuality during inference by leveraging LLMs' hierarchical representation of factual knowledge, manipulating the predicted distributions at inference time. Current state-of-the-art approaches refine decoding by contrasting early-exit distributions from a lower layer with the final layer to exploit information related to factuality within the model forward procedure. However, such methods often assume the final layer is the most reliable and the lower layer selection process depends on it. In this work, we first propose extrapolation of critical token probabilities beyond the last layer for more accurate contrasting. We additionally employ layer-wise entropy-guided lower layer selection, decoupling the selection process from the final layer. Experiments demonstrate strong performance - surpassing state-of-the-art on multiple different datasets by large margins. Analyses show different kinds of prompts respond to different selection strategies.","sentences":["Large language models (LLMs) exhibit impressive natural language capabilities but suffer from hallucination -- generating content ungrounded in the realities of training data.","Recent work has focused on decoding techniques to improve factuality during inference by leveraging LLMs' hierarchical representation of factual knowledge, manipulating the predicted distributions at inference time.","Current state-of-the-art approaches refine decoding by contrasting early-exit distributions from a lower layer with the final layer to exploit information related to factuality within the model forward procedure.","However, such methods often assume the final layer is the most reliable and the lower layer selection process depends on it.","In this work, we first propose extrapolation of critical token probabilities beyond the last layer for more accurate contrasting.","We additionally employ layer-wise entropy-guided lower layer selection, decoupling the selection process from the final layer.","Experiments demonstrate strong performance - surpassing state-of-the-art on multiple different datasets by large margins.","Analyses show different kinds of prompts respond to different selection strategies."],"url":"http://arxiv.org/abs/2404.09338v1","category":"cs.CL"}
{"created":"2024-04-14 19:36:04","title":"Self-Selected Attention Span for Accelerating Large Language Model Inference","abstract":"Large language models (LLMs) can solve challenging tasks. However, their inference computation on modern GPUs is highly inefficient due to the increasing number of tokens they must attend to as they generate new ones. To address this inefficiency, we capitalize on LLMs' problem-solving capabilities to optimize their own inference-time efficiency. We demonstrate with two specific tasks: (a) evaluating complex arithmetic expressions and (b) summarizing news articles. For both tasks, we create custom datasets to fine-tune an LLM. The goal of fine-tuning is twofold: first, to make the LLM learn to solve the evaluation or summarization task, and second, to train it to identify the minimal attention spans required for each step of the task. As a result, the fine-tuned model is able to convert these self-identified minimal attention spans into sparse attention masks on-the-fly during inference. We develop a custom CUDA kernel to take advantage of the reduced context to attend to. We demonstrate that using this custom CUDA kernel improves the throughput of LLM inference by 28%. Our work presents an end-to-end demonstration showing that training LLMs to self-select their attention spans speeds up autoregressive inference in solving real-world tasks.","sentences":["Large language models (LLMs) can solve challenging tasks.","However, their inference computation on modern GPUs is highly inefficient due to the increasing number of tokens they must attend to as they generate new ones.","To address this inefficiency, we capitalize on LLMs' problem-solving capabilities to optimize their own inference-time efficiency.","We demonstrate with two specific tasks: (a) evaluating complex arithmetic expressions and (b) summarizing news articles.","For both tasks, we create custom datasets to fine-tune an LLM.","The goal of fine-tuning is twofold: first, to make the LLM learn to solve the evaluation or summarization task, and second, to train it to identify the minimal attention spans required for each step of the task.","As a result, the fine-tuned model is able to convert these self-identified minimal attention spans into sparse attention masks on-the-fly during inference.","We develop a custom CUDA kernel to take advantage of the reduced context to attend to.","We demonstrate that using this custom CUDA kernel improves the throughput of LLM inference by 28%.","Our work presents an end-to-end demonstration showing that training LLMs to self-select their attention spans speeds up autoregressive inference in solving real-world tasks."],"url":"http://arxiv.org/abs/2404.09336v1","category":"cs.CL"}
{"created":"2024-04-14 19:09:03","title":"A generalized Liouville equation and magnetic stability","abstract":"This work considers two related families of nonlinear and nonlocal problems in the plane $\\R^2$. The first main result derives the general integrable solution to a generalized Liouville equation using the Wronskian of two coprime complex polynomials. The second main result concerns an application to a generalized Ladyzhenskaya--Gagliardo--Nirenberg interpolation inequality, with a single real parameter $\\beta$ interpreted as the strength of a magnetic self-interaction. The optimal constant of the inequality and the corresponding minimizers of the quotient are studied and it is proved that for $\\beta \\ge 2$, for which the constant equals $2\\pi\\beta$, such minimizers only exist at quantized $\\beta \\in 2\\N$ corresponding to nonlinear generalizations of Landau levels with densities solving the generalized Liouville equation. This latter problem originates from the study of self-dual vortex solitons in the abelian Chern--Simons--Higgs theory and from the average-field-Pauli effective theory of anyons, i.e.\\ quantum particles with statistics intermediate to bosons and fermions. An immediate application is given to Keller--Lieb--Thirring stability bounds for a gas of such anyons which self-interact magnetically (vector nonlocal repulsion) as well as electrostatically (scalar local/point attraction), thus generalizing the stability theory of the 2D cubic nonlinear Schr\\\"odinger equation.","sentences":["This work considers two related families of nonlinear and nonlocal problems in the plane $\\R^2$. The first main result derives the general integrable solution to a generalized Liouville equation using the Wronskian of two coprime complex polynomials.","The second main result concerns an application to a generalized Ladyzhenskaya--Gagliardo--Nirenberg interpolation inequality, with a single real parameter $\\beta$ interpreted as the strength of a magnetic self-interaction.","The optimal constant of the inequality and the corresponding minimizers of the quotient are studied and it is proved that for $\\beta \\ge 2$, for which the constant equals $2\\pi\\beta$, such minimizers only exist at quantized $\\beta \\in 2\\N$ corresponding to nonlinear generalizations of Landau levels with densities solving the generalized Liouville equation.","This latter problem originates from the study of self-dual vortex solitons in the abelian Chern--Simons--Higgs theory and from the average-field-Pauli effective theory of anyons, i.e.\\ quantum particles with statistics intermediate to bosons and fermions.","An immediate application is given to Keller--Lieb--Thirring stability bounds for a gas of such anyons which self-interact magnetically (vector nonlocal repulsion) as well as electrostatically (scalar local/point attraction), thus generalizing the stability theory of the 2D cubic nonlinear Schr\\\"odinger equation."],"url":"http://arxiv.org/abs/2404.09332v1","category":"math.AP"}
{"created":"2024-04-14 19:06:00","title":"SNN4Agents: A Framework for Developing Energy-Efficient Embodied Spiking Neural Networks for Autonomous Agents","abstract":"Recent trends have shown that autonomous agents, such as Autonomous Ground Vehicles (AGVs), Unmanned Aerial Vehicles (UAVs), and mobile robots, effectively improve human productivity in solving diverse tasks. However, since these agents are typically powered by portable batteries, they require extremely low power/energy consumption to operate in a long lifespan. To solve this challenge, neuromorphic computing has emerged as a promising solution, where bio-inspired Spiking Neural Networks (SNNs) use spikes from event-based cameras or data conversion pre-processing to perform sparse computations efficiently. However, the studies of SNN deployments for autonomous agents are still at an early stage. Hence, the optimization stages for enabling efficient embodied SNN deployments for autonomous agents have not been defined systematically. Toward this, we propose a novel framework called SNN4Agents that consists of a set of optimization techniques for designing energy-efficient embodied SNNs targeting autonomous agent applications. Our SNN4Agents employs weight quantization, timestep reduction, and attention window reduction to jointly improve the energy efficiency, reduce the memory footprint, optimize the processing latency, while maintaining high accuracy. In the evaluation, we investigate use cases of event-based car recognition, and explore the trade-offs among accuracy, latency, memory, and energy consumption. The experimental results show that our proposed framework can maintain high accuracy (i.e., 84.12% accuracy) with 68.75% memory saving, 3.58x speed-up, and 4.03x energy efficiency improvement as compared to the state-of-the-art work for NCARS dataset, thereby enabling energy-efficient embodied SNN deployments for autonomous agents.","sentences":["Recent trends have shown that autonomous agents, such as Autonomous Ground Vehicles (AGVs), Unmanned Aerial Vehicles (UAVs), and mobile robots, effectively improve human productivity in solving diverse tasks.","However, since these agents are typically powered by portable batteries, they require extremely low power/energy consumption to operate in a long lifespan.","To solve this challenge, neuromorphic computing has emerged as a promising solution, where bio-inspired Spiking Neural Networks (SNNs) use spikes from event-based cameras or data conversion pre-processing to perform sparse computations efficiently.","However, the studies of SNN deployments for autonomous agents are still at an early stage.","Hence, the optimization stages for enabling efficient embodied SNN deployments for autonomous agents have not been defined systematically.","Toward this, we propose a novel framework called SNN4Agents that consists of a set of optimization techniques for designing energy-efficient embodied SNNs targeting autonomous agent applications.","Our SNN4Agents employs weight quantization, timestep reduction, and attention window reduction to jointly improve the energy efficiency, reduce the memory footprint, optimize the processing latency, while maintaining high accuracy.","In the evaluation, we investigate use cases of event-based car recognition, and explore the trade-offs among accuracy, latency, memory, and energy consumption.","The experimental results show that our proposed framework can maintain high accuracy (i.e., 84.12% accuracy) with 68.75% memory saving, 3.58x speed-up, and 4.03x energy efficiency improvement as compared to the state-of-the-art work for NCARS dataset, thereby enabling energy-efficient embodied SNN deployments for autonomous agents."],"url":"http://arxiv.org/abs/2404.09331v1","category":"cs.RO"}
{"created":"2024-04-14 19:01:20","title":"Large Language Models are as persuasive as humans, but why? About the cognitive effort and moral-emotional language of LLM arguments","abstract":"Large Language Models (LLMs) are already as persuasive as humans. However, we know very little about why. This paper investigates the persuasion strategies of LLMs, comparing them with human-generated arguments. Using a dataset of 1,251 participants in an experiment, we analyze the persuaion strategies of LLM-generated and human-generated arguments using measures of cognitive effort (lexical and grammatical complexity) and moral-emotional language (sentiment and moral analysis). The study reveals that LLMs produce arguments that require higher cognitive effort, exhibiting more complex grammatical and lexical structures than human counterparts. Additionally, LLMs demonstrate a significant propensity to engage more deeply with moral language, utilizing both positive and negative moral foundations more frequently than humans. In contrast with previous research, no significant difference was found in the emotional content produced by LLMs and humans. These findings contribute to the discourse on AI and persuasion, highlighting the dual potential of LLMs to both enhance and undermine informational integrity through communication strategies for digital persuasion.","sentences":["Large Language Models (LLMs) are already as persuasive as humans.","However, we know very little about why.","This paper investigates the persuasion strategies of LLMs, comparing them with human-generated arguments.","Using a dataset of 1,251 participants in an experiment, we analyze the persuaion strategies of LLM-generated and human-generated arguments using measures of cognitive effort (lexical and grammatical complexity) and moral-emotional language (sentiment and moral analysis).","The study reveals that LLMs produce arguments that require higher cognitive effort, exhibiting more complex grammatical and lexical structures than human counterparts.","Additionally, LLMs demonstrate a significant propensity to engage more deeply with moral language, utilizing both positive and negative moral foundations more frequently than humans.","In contrast with previous research, no significant difference was found in the emotional content produced by LLMs and humans.","These findings contribute to the discourse on AI and persuasion, highlighting the dual potential of LLMs to both enhance and undermine informational integrity through communication strategies for digital persuasion."],"url":"http://arxiv.org/abs/2404.09329v1","category":"cs.CL"}
{"created":"2024-04-14 18:57:38","title":"Weight Copy and Low-Rank Adaptation for Few-Shot Distillation of Vision Transformers","abstract":"Few-shot knowledge distillation recently emerged as a viable approach to harness the knowledge of large-scale pre-trained models, using limited data and computational resources. In this paper, we propose a novel few-shot feature distillation approach for vision transformers. Our approach is based on two key steps. Leveraging the fact that vision transformers have a consistent depth-wise structure, we first copy the weights from intermittent layers of existing pre-trained vision transformers (teachers) into shallower architectures (students), where the intermittence factor controls the complexity of the student transformer with respect to its teacher. Next, we employ an enhanced version of Low-Rank Adaptation (LoRA) to distill knowledge into the student in a few-shot scenario, aiming to recover the information processing carried out by the skipped teacher layers. We present comprehensive experiments with supervised and self-supervised transformers as teachers, on five data sets from various domains, including natural, medical and satellite images. The empirical results confirm the superiority of our approach over competitive baselines. Moreover, the ablation results demonstrate the usefulness of each component of the proposed pipeline.","sentences":["Few-shot knowledge distillation recently emerged as a viable approach to harness the knowledge of large-scale pre-trained models, using limited data and computational resources.","In this paper, we propose a novel few-shot feature distillation approach for vision transformers.","Our approach is based on two key steps.","Leveraging the fact that vision transformers have a consistent depth-wise structure, we first copy the weights from intermittent layers of existing pre-trained vision transformers (teachers) into shallower architectures (students), where the intermittence factor controls the complexity of the student transformer with respect to its teacher.","Next, we employ an enhanced version of Low-Rank Adaptation (LoRA) to distill knowledge into the student in a few-shot scenario, aiming to recover the information processing carried out by the skipped teacher layers.","We present comprehensive experiments with supervised and self-supervised transformers as teachers, on five data sets from various domains, including natural, medical and satellite images.","The empirical results confirm the superiority of our approach over competitive baselines.","Moreover, the ablation results demonstrate the usefulness of each component of the proposed pipeline."],"url":"http://arxiv.org/abs/2404.09326v1","category":"cs.CV"}
{"created":"2024-04-14 18:51:50","title":"Correlated Mean Field Imitation Learning","abstract":"We investigate multi-agent imitation learning (IL) within the framework of mean field games (MFGs), considering the presence of time-varying correlated signals. Existing MFG IL algorithms assume demonstrations are sampled from Mean Field Nash Equilibria (MFNE), limiting their adaptability to real-world scenarios. For example, in the traffic network equilibrium influenced by public routing recommendations, recommendations introduce time-varying correlated signals into the game, not captured by MFNE and other existing correlated equilibrium concepts. To address this gap, we propose Adaptive Mean Field Correlated Equilibrium (AMFCE), a general equilibrium incorporating time-varying correlated signals. We establish the existence of AMFCE under mild conditions and prove that MFNE is a subclass of AMFCE. We further propose Correlated Mean Field Imitation Learning (CMFIL), a novel IL framework designed to recover the AMFCE, accompanied by a theoretical guarantee on the quality of the recovered policy. Experimental results, including a real-world traffic flow prediction problem, demonstrate the superiority of CMFIL over state-of-the-art IL baselines, highlighting the potential of CMFIL in understanding large population behavior under correlated signals.","sentences":["We investigate multi-agent imitation learning (IL) within the framework of mean field games (MFGs), considering the presence of time-varying correlated signals.","Existing MFG IL algorithms assume demonstrations are sampled from Mean Field Nash Equilibria (MFNE), limiting their adaptability to real-world scenarios.","For example, in the traffic network equilibrium influenced by public routing recommendations, recommendations introduce time-varying correlated signals into the game, not captured by MFNE and other existing correlated equilibrium concepts.","To address this gap, we propose Adaptive Mean Field Correlated Equilibrium (AMFCE), a general equilibrium incorporating time-varying correlated signals.","We establish the existence of AMFCE under mild conditions and prove that MFNE is a subclass of AMFCE.","We further propose Correlated Mean Field Imitation Learning (CMFIL), a novel IL framework designed to recover the AMFCE, accompanied by a theoretical guarantee on the quality of the recovered policy.","Experimental results, including a real-world traffic flow prediction problem, demonstrate the superiority of CMFIL over state-of-the-art IL baselines, highlighting the potential of CMFIL in understanding large population behavior under correlated signals."],"url":"http://arxiv.org/abs/2404.09324v1","category":"cs.MA"}
{"created":"2024-04-14 18:42:20","title":"The intelligent prediction and assessment of financial information risk in the cloud computing model","abstract":"Cloud computing (cloud computing) is a kind of distributed computing, referring to the network \"cloud\" will be a huge data calculation and processing program into countless small programs, and then, through the system composed of multiple servers to process and analyze these small programs to get the results and return to the user. This report explores the intersection of cloud computing and financial information processing, identifying risks and challenges faced by financial institutions in adopting cloud technology. It discusses the need for intelligent solutions to enhance data processing efficiency and accuracy while addressing security and privacy concerns. Drawing on regulatory frameworks, the report proposes policy recommendations to mitigate concentration risks associated with cloud computing in the financial industry. By combining intelligent forecasting and evaluation technologies with cloud computing models, the study aims to provide effective solutions for financial data processing and management, facilitating the industry's transition towards digital transformation.","sentences":["Cloud computing (cloud computing) is a kind of distributed computing, referring to the network \"cloud\" will be a huge data calculation and processing program into countless small programs, and then, through the system composed of multiple servers to process and analyze these small programs to get the results and return to the user.","This report explores the intersection of cloud computing and financial information processing, identifying risks and challenges faced by financial institutions in adopting cloud technology.","It discusses the need for intelligent solutions to enhance data processing efficiency and accuracy while addressing security and privacy concerns.","Drawing on regulatory frameworks, the report proposes policy recommendations to mitigate concentration risks associated with cloud computing in the financial industry.","By combining intelligent forecasting and evaluation technologies with cloud computing models, the study aims to provide effective solutions for financial data processing and management, facilitating the industry's transition towards digital transformation."],"url":"http://arxiv.org/abs/2404.09322v1","category":"cs.DC"}
{"created":"2024-04-14 18:38:27","title":"Clarification of the transverse orbital angular momentum of spatiotemporal optical vortices","abstract":"Advances in the generation and the application of spatiotemporal optical vortices (STOV) are proceeding fast, but fundamental aspects of their nature remain obscure. Phys. Rev. A 107, L031501 (2023) (PRA) and Prog. Electromagn. Res. 177, 95 (2023) (PIER) provide contradictory results on the transverse orbital angular momentum (OAM) carried by STOVs. We show that the results by Porras in PIER and by Bliokh in PRA refer to different STOVs and are all correct. In PIER, STOVs are elliptical at given cross section and time, or in space-time, but not in three-dimensional space. In PRA, STOVs are elliptical in space but not in space-time. This is evidenced from two dual, equivalent theories on the transverse OAM where a wave packet is seen in space-time evolving with propagation distance or in space evolving in time, that accounts for all values of the total, intrinsic and extrinsic OAM in PIERS and PRA. However, the intrinsic OAM with respect to the photon wave function center in PRA is not generally conserved, which advocates for the energy center in PIER as the STOV center. We argue that STOVs are generated in experiments to purportedly have elliptical symmetry in space-time. The values provided in PIER should then be taken as the reference for elliptical STOVs, and the theory therein to evaluate the transverse OAM of other wave packets. Hancock et al. in Phys. Rev. Lett. 127, 193901 (2021) and Phys. Rev. X. 14, 011031 (2024) erroneously attribute the transverse OAM of elliptical STOVs in space to the elliptical STOVs in space-time they consider theoretically and can generate in their experiments.","sentences":["Advances in the generation and the application of spatiotemporal optical vortices (STOV) are proceeding fast, but fundamental aspects of their nature remain obscure.","Phys.","Rev. A 107, L031501 (2023) (PRA) and Prog.","Electromagn.","Res. 177, 95 (2023) (PIER) provide contradictory results on the transverse orbital angular momentum (OAM) carried by STOVs.","We show that the results by Porras in PIER and by Bliokh in PRA refer to different STOVs and are all correct.","In PIER, STOVs are elliptical at given cross section and time, or in space-time, but not in three-dimensional space.","In PRA, STOVs are elliptical in space but not in space-time.","This is evidenced from two dual, equivalent theories on the transverse OAM where a wave packet is seen in space-time evolving with propagation distance or in space evolving in time, that accounts for all values of the total, intrinsic and extrinsic OAM in PIERS and PRA.","However, the intrinsic OAM with respect to the photon wave function center in PRA is not generally conserved, which advocates for the energy center in PIER as the STOV center.","We argue that STOVs are generated in experiments to purportedly have elliptical symmetry in space-time.","The values provided in PIER should then be taken as the reference for elliptical STOVs, and the theory therein to evaluate the transverse OAM of other wave packets.","Hancock et al.","in Phys.","Rev. Lett.","127, 193901 (2021) and Phys.","Rev. X. 14, 011031 (2024) erroneously attribute the transverse OAM of elliptical STOVs in space to the elliptical STOVs in space-time they consider theoretically and can generate in their experiments."],"url":"http://arxiv.org/abs/2404.09321v1","category":"physics.optics"}
{"created":"2024-04-14 18:16:16","title":"Characterizing Soft-Error Resiliency in Arm's Ethos-U55 Embedded Machine Learning Accelerator","abstract":"As Neural Processing Units (NPU) or accelerators are increasingly deployed in a variety of applications including safety critical applications such as autonomous vehicle, and medical imaging, it is critical to understand the fault-tolerance nature of the NPUs. We present a reliability study of Arm's Ethos-U55, an important industrial-scale NPU being utilised in embedded and IoT applications. We perform large scale RTL-level fault injections to characterize Ethos-U55 against the Automotive Safety Integrity Level D (ASIL-D) resiliency standard commonly used for safety-critical applications such as autonomous vehicles. We show that, under soft errors, all four configurations of the NPU fall short of the required level of resiliency for a variety of neural networks running on the NPU. We show that it is possible to meet the ASIL-D level resiliency without resorting to conventional strategies like Dual Core Lock Step (DCLS) that has an area overhead of 100%. We achieve so through selective protection, where hardware structures are selectively protected (e.g., duplicated, hardened) based on their sensitivity to soft errors and their silicon areas. To identify the optimal configuration that minimizes the area overhead while meeting the ASIL-D standard, the main challenge is the large search space associated with the time-consuming RTL simulation. To address this challenge, we present a statistical analysis tool that is validated against Arm silicon and that allows us to quickly navigate hundreds of billions of fault sites without exhaustive RTL fault injections. We show that by carefully duplicating a small fraction of the functional blocks and hardening the Flops in other blocks meets the ASIL-D safety standard while introducing an area overhead of only 38%.","sentences":["As Neural Processing Units (NPU) or accelerators are increasingly deployed in a variety of applications including safety critical applications such as autonomous vehicle, and medical imaging, it is critical to understand the fault-tolerance nature of the NPUs.","We present a reliability study of Arm's Ethos-U55, an important industrial-scale NPU being utilised in embedded and IoT applications.","We perform large scale RTL-level fault injections to characterize Ethos-U55 against the Automotive Safety Integrity Level D (ASIL-D) resiliency standard commonly used for safety-critical applications such as autonomous vehicles.","We show that, under soft errors, all four configurations of the NPU fall short of the required level of resiliency for a variety of neural networks running on the NPU.","We show that it is possible to meet the ASIL-D level resiliency without resorting to conventional strategies like Dual Core Lock Step (DCLS) that has an area overhead of 100%.","We achieve so through selective protection, where hardware structures are selectively protected (e.g., duplicated, hardened) based on their sensitivity to soft errors and their silicon areas.","To identify the optimal configuration that minimizes the area overhead while meeting the ASIL-D standard, the main challenge is the large search space associated with the time-consuming RTL simulation.","To address this challenge, we present a statistical analysis tool that is validated against Arm silicon and that allows us to quickly navigate hundreds of billions of fault sites without exhaustive RTL fault injections.","We show that by carefully duplicating a small fraction of the functional blocks and hardening the Flops in other blocks meets the ASIL-D safety standard while introducing an area overhead of only 38%."],"url":"http://arxiv.org/abs/2404.09317v1","category":"cs.AR"}
{"created":"2024-04-14 18:00:05","title":"Text-to-Song: Towards Controllable Music Generation Incorporating Vocals and Accompaniment","abstract":"A song is a combination of singing voice and accompaniment. However, existing works focus on singing voice synthesis and music generation independently. Little attention was paid to explore song synthesis. In this work, we propose a novel task called text-to-song synthesis which incorporating both vocals and accompaniments generation. We develop Melodist, a two-stage text-to-song method that consists of singing voice synthesis (SVS) and vocal-to-accompaniment (V2A) synthesis. Melodist leverages tri-tower contrastive pretraining to learn more effective text representation for controllable V2A synthesis. A Chinese song dataset mined from a music website is built up to alleviate data scarcity for our research. The evaluation results on our dataset demonstrate that Melodist can synthesize songs with comparable quality and style consistency. Audio samples can be found in https://text2songMelodist.github.io/Sample/.","sentences":["A song is a combination of singing voice and accompaniment.","However, existing works focus on singing voice synthesis and music generation independently.","Little attention was paid to explore song synthesis.","In this work, we propose a novel task called text-to-song synthesis which incorporating both vocals and accompaniments generation.","We develop Melodist, a two-stage text-to-song method that consists of singing voice synthesis (SVS) and vocal-to-accompaniment (V2A) synthesis.","Melodist leverages tri-tower contrastive pretraining to learn more effective text representation for controllable V2A synthesis.","A Chinese song dataset mined from a music website is built up to alleviate data scarcity for our research.","The evaluation results on our dataset demonstrate that Melodist can synthesize songs with comparable quality and style consistency.","Audio samples can be found in https://text2songMelodist.github.io/Sample/."],"url":"http://arxiv.org/abs/2404.09313v1","category":"eess.AS"}
{"created":"2024-04-14 17:45:16","title":"A nodal based high order nonlinear stabilization for finite element approximation of Magnetohydrodynamics","abstract":"We present a novel high-order nodal artificial viscosity approach designed for solving Magnetohydrodynamics (MHD) equations. Unlike conventional methods, our approach eliminates the need for ad hoc parameters. The viscosity is mesh-dependent, yet explicit definition of the mesh size is unnecessary. Our method employs a multimesh strategy: the viscosity coefficient is constructed from a linear polynomial space constructed on the fine mesh, corresponding to the nodal values of the finite element approximation space. The residual of MHD is utilized to introduce high-order viscosity in a localized fashion near shocks and discontinuities. This approach is designed to precisely capture and resolve shocks. Then, high-order Runge-Kutta methods are employed to discretize the temporal domain. Through a comprehensive set of challenging test problems, we validate the robustness and high-order accuracy of our proposed approach for solving MHD equations.","sentences":["We present a novel high-order nodal artificial viscosity approach designed for solving Magnetohydrodynamics (MHD) equations.","Unlike conventional methods, our approach eliminates the need for ad hoc parameters.","The viscosity is mesh-dependent, yet explicit definition of the mesh size is unnecessary.","Our method employs a multimesh strategy: the viscosity coefficient is constructed from a linear polynomial space constructed on the fine mesh, corresponding to the nodal values of the finite element approximation space.","The residual of MHD is utilized to introduce high-order viscosity in a localized fashion near shocks and discontinuities.","This approach is designed to precisely capture and resolve shocks.","Then, high-order Runge-Kutta methods are employed to discretize the temporal domain.","Through a comprehensive set of challenging test problems, we validate the robustness and high-order accuracy of our proposed approach for solving MHD equations."],"url":"http://arxiv.org/abs/2404.09311v1","category":"math.NA"}
{"created":"2024-04-14 17:37:38","title":"pp-waves in conformal Killing gravity","abstract":"Recently Harada has proposed a gravitational theory which is of third order in the derivatives of the metric tensor. This has attracted some attention particularly as it predicts a late-time transition from cosmological decelaration to accelerated expansion without assuming the presence of dark energy or a non-zero cosmological constant. This theory has been dubbed conformal Killing gravity by Mantica & Molinari.   The most general exact solutions of the Harada field equations are known for a number of important physical situations: homogeneous and isotropic cosmological models, static spherically symmetric vacuum and electrovac spacetimes. These are analogues of the well-known FRWL, Schwarzschild and Reissner-Nordstr\\\"om metrics of General Relativity(GR). In this study the pp-waves in Harada's theory are studied and the most general exact solution is obtained together with its specialisation for plane waves. The generalisation from GR to Harada's theory turns out to be straightforward and the solutions only involve an extra non-propagating term. The solutions have Petrov type N (or 0) and the Ricci tensor is either zero or the Segr\\'e type is [(211)] with zero eigenvalue.   For any metric in conformal Killing gravity it is shown that more than one possible matter source can generate the solution. If the metric admits one or more Killing vectors, the ambiguity in the possible matter sources increases.","sentences":["Recently Harada has proposed a gravitational theory which is of third order in the derivatives of the metric tensor.","This has attracted some attention particularly as it predicts a late-time transition from cosmological decelaration to accelerated expansion without assuming the presence of dark energy or a non-zero cosmological constant.","This theory has been dubbed conformal Killing gravity by Mantica & Molinari.   ","The most general exact solutions of the Harada field equations are known for a number of important physical situations: homogeneous and isotropic cosmological models, static spherically symmetric vacuum and electrovac spacetimes.","These are analogues of the well-known FRWL, Schwarzschild and Reissner-Nordstr\\\"om metrics of General Relativity(GR).","In this study the pp-waves in Harada's theory are studied and the most general exact solution is obtained together with its specialisation for plane waves.","The generalisation from GR to Harada's theory turns out to be straightforward and the solutions only involve an extra non-propagating term.","The solutions have Petrov type N (or 0) and the Ricci tensor is either zero or the Segr\\'e type is [(211)] with zero eigenvalue.   ","For any metric in conformal Killing gravity it is shown that more than one possible matter source can generate the solution.","If the metric admits one or more Killing vectors, the ambiguity in the possible matter sources increases."],"url":"http://arxiv.org/abs/2404.09310v1","category":"gr-qc"}
{"created":"2024-04-14 17:08:43","title":"Minimax Optimal rates of convergence in the shuffled regression, unlinked regression, and deconvolution under vanishing noise","abstract":"Shuffled regression and unlinked regression represent intriguing challenges that have garnered considerable attention in many fields, including but not limited to ecological regression, multi-target tracking problems, image denoising, etc. However, a notable gap exists in the existing literature, particularly in vanishing noise, i.e., how the rate of estimation of the underlying signal scales with the error variance. This paper aims to bridge this gap by delving into the monotone function estimation problem under vanishing noise variance, i.e., we allow the error variance to go to $0$ as the number of observations increases. Our investigation reveals that, asymptotically, the shuffled regression problem exhibits a comparatively simpler nature than the unlinked regression; if the error variance is smaller than a threshold, then the minimax risk of the shuffled regression is smaller than that of the unlinked regression. On the other hand, the minimax estimation error is of the same order in the two problems if the noise level is larger than that threshold. Our analysis is quite general in that we do not assume any smoothness of the underlying monotone link function. Because these problems are related to deconvolution, we also provide bounds for deconvolution in a similar context. Through this exploration, we contribute to understanding the intricate relationships between these statistical problems and shed light on their behaviors when subjected to the nuanced constraint of vanishing noise.","sentences":["Shuffled regression and unlinked regression represent intriguing challenges that have garnered considerable attention in many fields, including but not limited to ecological regression, multi-target tracking problems, image denoising, etc.","However, a notable gap exists in the existing literature, particularly in vanishing noise, i.e., how the rate of estimation of the underlying signal scales with the error variance.","This paper aims to bridge this gap by delving into the monotone function estimation problem under vanishing noise variance, i.e., we allow the error variance to go to $0$ as the number of observations increases.","Our investigation reveals that, asymptotically, the shuffled regression problem exhibits a comparatively simpler nature than the unlinked regression; if the error variance is smaller than a threshold, then the minimax risk of the shuffled regression is smaller than that of the unlinked regression.","On the other hand, the minimax estimation error is of the same order in the two problems if the noise level is larger than that threshold.","Our analysis is quite general in that we do not assume any smoothness of the underlying monotone link function.","Because these problems are related to deconvolution, we also provide bounds for deconvolution in a similar context.","Through this exploration, we contribute to understanding the intricate relationships between these statistical problems and shed light on their behaviors when subjected to the nuanced constraint of vanishing noise."],"url":"http://arxiv.org/abs/2404.09306v1","category":"math.ST"}
{"created":"2024-04-14 17:07:59","title":"OWLOOP: Interfaces for Mapping OWL Axioms into OOP Hierarchies","abstract":"The paper tackles the issue of mapping logic axioms formalised in the Ontology Web Language (OWL) within the Object-Oriented Programming (OOP) paradigm. The issues of mapping OWL axioms hierarchies and OOP objects hierarchies are due to OWL-based reasoning algorithms, which might change an OWL hierarchy at runtime; instead, OOP hierarchies are usually defined as static structures. Although programming paradigms based on reflection allow changing the OOP hierarchies at runtime and mapping OWL axioms dynamically, there are no currently available mechanisms that do not limit the reasoning algorithms. Thus, the factory-based paradigm is typically used since it decouples the OWL and OOP hierarchies. However, the factory inhibits OOP polymorphism and introduces a paradigm shift with respect to widely accepted OOP paradigms. We present the OWLOOP API, which exploits the factory to not limit reasoning algorithms, and it provides novel OOP interfaces concerning the axioms in an ontology. OWLOOP is designed to limit the paradigm shift required for using ontologies while improving, through OOP-like polymorphism, the modularity of software architectures that exploit logic reasoning. The paper details our OWL to OOP mapping mechanism, and it shows the benefits and limitations of OWLOOP through examples concerning a robot in a smart environment.","sentences":["The paper tackles the issue of mapping logic axioms formalised in the Ontology Web Language (OWL) within the Object-Oriented Programming (OOP) paradigm.","The issues of mapping OWL axioms hierarchies and OOP objects hierarchies are due to OWL-based reasoning algorithms, which might change an OWL hierarchy at runtime; instead, OOP hierarchies are usually defined as static structures.","Although programming paradigms based on reflection allow changing the OOP hierarchies at runtime and mapping OWL axioms dynamically, there are no currently available mechanisms that do not limit the reasoning algorithms.","Thus, the factory-based paradigm is typically used since it decouples the OWL and OOP hierarchies.","However, the factory inhibits OOP polymorphism and introduces a paradigm shift with respect to widely accepted OOP paradigms.","We present the OWLOOP API, which exploits the factory to not limit reasoning algorithms, and it provides novel OOP interfaces concerning the axioms in an ontology.","OWLOOP is designed to limit the paradigm shift required for using ontologies while improving, through OOP-like polymorphism, the modularity of software architectures that exploit logic reasoning.","The paper details our OWL to OOP mapping mechanism, and it shows the benefits and limitations of OWLOOP through examples concerning a robot in a smart environment."],"url":"http://arxiv.org/abs/2404.09305v1","category":"cs.AI"}
{"created":"2024-04-14 17:06:20","title":"Monte Carlo Search Algorithms Discovering Monte Carlo Tree Search Exploration Terms","abstract":"Monte Carlo Tree Search and Monte Carlo Search have good results for many combinatorial problems. In this paper we propose to use Monte Carlo Search to design mathematical expressions that are used as exploration terms for Monte Carlo Tree Search algorithms. The optimized Monte Carlo Tree Search algorithms are PUCT and SHUSS. We automatically design the PUCT and the SHUSS root exploration terms. For small search budgets of 32 evaluations the discovered root exploration terms make both algorithms competitive with usual PUCT.","sentences":["Monte Carlo Tree Search and Monte Carlo Search have good results for many combinatorial problems.","In this paper we propose to use Monte Carlo Search to design mathematical expressions that are used as exploration terms for Monte Carlo Tree Search algorithms.","The optimized Monte Carlo Tree Search algorithms are PUCT and SHUSS.","We automatically design the PUCT and the SHUSS root exploration terms.","For small search budgets of 32 evaluations the discovered root exploration terms make both algorithms competitive with usual PUCT."],"url":"http://arxiv.org/abs/2404.09304v1","category":"cs.AI"}
{"created":"2024-04-14 16:57:41","title":"High Significant Fault Detection in Azure Core Workload Insights","abstract":"Azure Core workload insights have time-series data with different metric units. Faults or Anomalies are observed in these time-series data owing to faults observed with respect to metric name, resources region, dimensions, and its dimension value associated with the data. For Azure Core, an important task is to highlight faults or anomalies to the user on a dashboard that they can perceive easily. The number of anomalies reported should be highly significant and in a limited number, e.g., 5-20 anomalies reported per hour. The reported anomalies will have significant user perception and high reconstruction error in any time-series forecasting model. Hence, our task is to automatically identify 'high significant anomalies' and their associated information for user perception.","sentences":["Azure Core workload insights have time-series data with different metric units.","Faults or Anomalies are observed in these time-series data owing to faults observed with respect to metric name, resources region, dimensions, and its dimension value associated with the data.","For Azure Core, an important task is to highlight faults or anomalies to the user on a dashboard that they can perceive easily.","The number of anomalies reported should be highly significant and in a limited number, e.g., 5-20 anomalies reported per hour.","The reported anomalies will have significant user perception and high reconstruction error in any time-series forecasting model.","Hence, our task is to automatically identify 'high significant anomalies' and their associated information for user perception."],"url":"http://arxiv.org/abs/2404.09302v1","category":"cs.LG"}
{"created":"2024-04-14 16:38:02","title":"Belief Bias Identification","abstract":"This paper proposes a unified theoretical model to identify and test a comprehensive set of probabilistic updating biases within a single framework. The model achieves separate identification by focusing on the updating of belief distributions, rather than classic point-belief measurements. Testing the model in a laboratory experiment reveals significant heterogeneity at the individual level: All tested biases are present, and each participant exhibits at least one identifiable bias. Notably, motivated-belief biases (optimism and pessimism) and sequence-related biases (gambler's fallacy and hot hand fallacy) are identified as key drivers of biased inference. Moreover, at the population level, base rate neglect emerges as a persistent influence. This study contributes to the belief-updating literature by providing a methodological toolkit for researchers examining links between different conflicting biases, or exploring connections between updating biases and other behavioural phenomena.","sentences":["This paper proposes a unified theoretical model to identify and test a comprehensive set of probabilistic updating biases within a single framework.","The model achieves separate identification by focusing on the updating of belief distributions, rather than classic point-belief measurements.","Testing the model in a laboratory experiment reveals significant heterogeneity at the individual level: All tested biases are present, and each participant exhibits at least one identifiable bias.","Notably, motivated-belief biases (optimism and pessimism) and sequence-related biases (gambler's fallacy and hot hand fallacy) are identified as key drivers of biased inference.","Moreover, at the population level, base rate neglect emerges as a persistent influence.","This study contributes to the belief-updating literature by providing a methodological toolkit for researchers examining links between different conflicting biases, or exploring connections between updating biases and other behavioural phenomena."],"url":"http://arxiv.org/abs/2404.09297v1","category":"econ.GN"}
{"created":"2024-04-15 12:50:44","title":"LetsGo: Large-Scale Garage Modeling and Rendering via LiDAR-Assisted Gaussian Primitives","abstract":"Large garages are ubiquitous yet intricate scenes in our daily lives, posing challenges characterized by monotonous colors, repetitive patterns, reflective surfaces, and transparent vehicle glass. Conventional Structure from Motion (SfM) methods for camera pose estimation and 3D reconstruction fail in these environments due to poor correspondence construction. To address these challenges, this paper introduces LetsGo, a LiDAR-assisted Gaussian splatting approach for large-scale garage modeling and rendering. We develop a handheld scanner, Polar, equipped with IMU, LiDAR, and a fisheye camera, to facilitate accurate LiDAR and image data scanning. With this Polar device, we present a GarageWorld dataset consisting of five expansive garage scenes with diverse geometric structures and will release the dataset to the community for further research. We demonstrate that the collected LiDAR point cloud by the Polar device enhances a suite of 3D Gaussian splatting algorithms for garage scene modeling and rendering. We also propose a novel depth regularizer for 3D Gaussian splatting algorithm training, effectively eliminating floating artifacts in rendered images, and a lightweight Level of Detail (LOD) Gaussian renderer for real-time viewing on web-based devices. Additionally, we explore a hybrid representation that combines the advantages of traditional mesh in depicting simple geometry and colors (e.g., walls and the ground) with modern 3D Gaussian representations capturing complex details and high-frequency textures. This strategy achieves an optimal balance between memory performance and rendering quality. Experimental results on our dataset, along with ScanNet++ and KITTI-360, demonstrate the superiority of our method in rendering quality and resource efficiency.","sentences":["Large garages are ubiquitous yet intricate scenes in our daily lives, posing challenges characterized by monotonous colors, repetitive patterns, reflective surfaces, and transparent vehicle glass.","Conventional Structure from Motion (SfM) methods for camera pose estimation and 3D reconstruction fail in these environments due to poor correspondence construction.","To address these challenges, this paper introduces LetsGo, a LiDAR-assisted Gaussian splatting approach for large-scale garage modeling and rendering.","We develop a handheld scanner, Polar, equipped with IMU, LiDAR, and a fisheye camera, to facilitate accurate LiDAR and image data scanning.","With this Polar device, we present a GarageWorld dataset consisting of five expansive garage scenes with diverse geometric structures and will release the dataset to the community for further research.","We demonstrate that the collected LiDAR point cloud by the Polar device enhances a suite of 3D Gaussian splatting algorithms for garage scene modeling and rendering.","We also propose a novel depth regularizer for 3D Gaussian splatting algorithm training, effectively eliminating floating artifacts in rendered images, and a lightweight Level of Detail (LOD) Gaussian renderer for real-time viewing on web-based devices.","Additionally, we explore a hybrid representation that combines the advantages of traditional mesh in depicting simple geometry and colors (e.g., walls and the ground) with modern 3D Gaussian representations capturing complex details and high-frequency textures.","This strategy achieves an optimal balance between memory performance and rendering quality.","Experimental results on our dataset, along with ScanNet++ and KITTI-360, demonstrate the superiority of our method in rendering quality and resource efficiency."],"url":"http://arxiv.org/abs/2404.09748v1","category":"cs.CV"}
{"created":"2024-04-15 11:46:24","title":"XoFTR: Cross-modal Feature Matching Transformer","abstract":"We introduce, XoFTR, a cross-modal cross-view method for local feature matching between thermal infrared (TIR) and visible images. Unlike visible images, TIR images are less susceptible to adverse lighting and weather conditions but present difficulties in matching due to significant texture and intensity differences. Current hand-crafted and learning-based methods for visible-TIR matching fall short in handling viewpoint, scale, and texture diversities. To address this, XoFTR incorporates masked image modeling pre-training and fine-tuning with pseudo-thermal image augmentation to handle the modality differences. Additionally, we introduce a refined matching pipeline that adjusts for scale discrepancies and enhances match reliability through sub-pixel level refinement. To validate our approach, we collect a comprehensive visible-thermal dataset, and show that our method outperforms existing methods on many benchmarks.","sentences":["We introduce, XoFTR, a cross-modal cross-view method for local feature matching between thermal infrared (TIR) and visible images.","Unlike visible images, TIR images are less susceptible to adverse lighting and weather conditions but present difficulties in matching due to significant texture and intensity differences.","Current hand-crafted and learning-based methods for visible-TIR matching fall short in handling viewpoint, scale, and texture diversities.","To address this, XoFTR incorporates masked image modeling pre-training and fine-tuning with pseudo-thermal image augmentation to handle the modality differences.","Additionally, we introduce a refined matching pipeline that adjusts for scale discrepancies and enhances match reliability through sub-pixel level refinement.","To validate our approach, we collect a comprehensive visible-thermal dataset, and show that our method outperforms existing methods on many benchmarks."],"url":"http://arxiv.org/abs/2404.09692v1","category":"cs.CV"}
{"created":"2024-04-15 10:59:13","title":"Cluster analysis of the Roma-BZCAT blazars","abstract":"Based on the collected multiwavelength data, namely in the radio (NVSS, FIRST, RATAN-600), IR (WISE), optical (Pan-STARRS), UV (GALEX), and X-ray (ROSAT, Swift-XRT) ranges, we have performed a cluster analysis for the blazars of the Roma-BZCAT catalog. Using two machine learning methods, namely a combination of PCA with k-means clustering and Kohonen's self-organizing maps, we have constructed an independent classification of the blazars (five classes) and compared the classes with the known Roma-BZCAT classification (FSRQs, BL Lacs, galaxy-dominated BL Lacs, and blazars of an uncertain type) as well as with the high synchrotron peaked blazars (HSP) from the 3HSP catalog and blazars from the TeVCat catalog. The obtained groups demonstrate concordance with the BL Lac/FSRQ classification along with a continuous character of the change in the properties. The group of HSP blazars stands out against the overall distribution. We examine the characteristics of the five groups and demonstrate distinctions in their spectral energy distribution shapes. The effectiveness of the clustering technique for objective analysis of multiparametric arrays of experimental data is demonstrated.","sentences":["Based on the collected multiwavelength data, namely in the radio (NVSS, FIRST, RATAN-600), IR (WISE), optical (Pan-STARRS), UV (GALEX), and X-ray (ROSAT, Swift-XRT) ranges, we have performed a cluster analysis for the blazars of the Roma-BZCAT catalog.","Using two machine learning methods, namely a combination of PCA with k-means clustering and Kohonen's self-organizing maps, we have constructed an independent classification of the blazars (five classes) and compared the classes with the known Roma-BZCAT classification (FSRQs, BL Lacs, galaxy-dominated BL Lacs, and blazars of an uncertain type) as well as with the high synchrotron peaked blazars (HSP) from the 3HSP catalog and blazars from the TeVCat catalog.","The obtained groups demonstrate concordance with the BL Lac/FSRQ classification along with a continuous character of the change in the properties.","The group of HSP blazars stands out against the overall distribution.","We examine the characteristics of the five groups and demonstrate distinctions in their spectral energy distribution shapes.","The effectiveness of the clustering technique for objective analysis of multiparametric arrays of experimental data is demonstrated."],"url":"http://arxiv.org/abs/2404.09667v1","category":"astro-ph.GA"}
{"created":"2024-04-15 10:31:41","title":"Writhe invariants of 3-regular spatial graphs","abstract":"We give a necessary condition for two diagrams of $3$-regular spatial graphs with the same underlying abstract graph $G$ to represent isotopic spatial graphs. The test works by reading off the writhes of the knot diagrams coming from a collection of cycles in $G$ in each diagram, and checking whether the writhe tuples differ by an element in the image of a certain map of $\\mathbb{Z}$-modules determined by $G$. We exemplify by using our result to distinguish, for each $n \\ge 3$, all elements in a certain infinite family of embeddings of the M\\\"obius ladder $\\mathrm{M}_n$ into $\\mathbb{R}^3$ . We also connect these writhe tuples to a classical invariant of spatial graphs due to Wu and Taniyama.","sentences":["We give a necessary condition for two diagrams of $3$-regular spatial graphs with the same underlying abstract graph $G$ to represent isotopic spatial graphs.","The test works by reading off the writhes of the knot diagrams coming from a collection of cycles in $G$ in each diagram, and checking whether the writhe tuples differ by an element in the image of a certain map of $\\mathbb{Z}$-modules determined by $G$. We exemplify by using our result to distinguish, for each $n \\ge 3$, all elements in a certain infinite family of embeddings of the M\\\"obius ladder $\\mathrm{M}_n$ into $\\mathbb{R}^3$ .","We also connect these writhe tuples to a classical invariant of spatial graphs due to Wu and Taniyama."],"url":"http://arxiv.org/abs/2404.09649v1","category":"math.GT"}
{"created":"2024-04-15 10:24:32","title":"Real-world Instance-specific Image Goal Navigation for Service Robots: Bridging the Domain Gap with Contrastive Learning","abstract":"Improving instance-specific image goal navigation (InstanceImageNav), which locates the identical object in a real-world environment from a query image, is essential for robotic systems to assist users in finding desired objects. The challenge lies in the domain gap between low-quality images observed by the moving robot, characterized by motion blur and low-resolution, and high-quality query images provided by the user. Such domain gaps could significantly reduce the task success rate but have not been the focus of previous work. To address this, we propose a novel method called Few-shot Cross-quality Instance-aware Adaptation (CrossIA), which employs contrastive learning with an instance classifier to align features between massive low- and few high-quality images. This approach effectively reduces the domain gap by bringing the latent representations of cross-quality images closer on an instance basis. Additionally, the system integrates an object image collection with a pre-trained deblurring model to enhance the observed image quality. Our method fine-tunes the SimSiam model, pre-trained on ImageNet, using CrossIA. We evaluated our method's effectiveness through an InstanceImageNav task with 20 different types of instances, where the robot identifies the same instance in a real-world environment as a high-quality query image. Our experiments showed that our method improves the task success rate by up to three times compared to the baseline, a conventional approach based on SuperGlue. These findings highlight the potential of leveraging contrastive learning and image enhancement techniques to bridge the domain gap and improve object localization in robotic applications. The project website is https://emergentsystemlabstudent.github.io/DomainBridgingNav/.","sentences":["Improving instance-specific image goal navigation (InstanceImageNav), which locates the identical object in a real-world environment from a query image, is essential for robotic systems to assist users in finding desired objects.","The challenge lies in the domain gap between low-quality images observed by the moving robot, characterized by motion blur and low-resolution, and high-quality query images provided by the user.","Such domain gaps could significantly reduce the task success rate but have not been the focus of previous work.","To address this, we propose a novel method called Few-shot Cross-quality Instance-aware Adaptation (CrossIA), which employs contrastive learning with an instance classifier to align features between massive low- and few high-quality images.","This approach effectively reduces the domain gap by bringing the latent representations of cross-quality images closer on an instance basis.","Additionally, the system integrates an object image collection with a pre-trained deblurring model to enhance the observed image quality.","Our method fine-tunes the SimSiam model, pre-trained on ImageNet, using CrossIA.","We evaluated our method's effectiveness through an InstanceImageNav task with 20 different types of instances, where the robot identifies the same instance in a real-world environment as a high-quality query image.","Our experiments showed that our method improves the task success rate by up to three times compared to the baseline, a conventional approach based on SuperGlue.","These findings highlight the potential of leveraging contrastive learning and image enhancement techniques to bridge the domain gap and improve object localization in robotic applications.","The project website is https://emergentsystemlabstudent.github.io/DomainBridgingNav/."],"url":"http://arxiv.org/abs/2404.09645v1","category":"cs.RO"}
{"created":"2024-04-15 08:51:09","title":"Dispersionless Flat Mode and Vibrational Anomaly in Active Brownian Vibrators Induced by String-like Dynamical Defects","abstract":"In recent years, active Brownian particles have emerged as a prominent model system for comprehending the behaviors of active matter, wherein particles demonstrate self-propelled motion by harnessing energy from the surrounding environment. A fundamental objective of studying active matter is to elucidate the physical mechanisms underlying its collective behaviors. Drawing inspiration from advancements in molecular glasses, our study unveils a low-energy ``flat mode\" within the transverse spectrum of active Brownian vibrators -- a nearly two-dimensional, bi-disperse granular assembly. We demonstrate that this collective excitation induces an anomalous excess in the vibrational density of states (VDOS) beyond the phononic Debye contribution. Additionally, we establish through empirical evidence that string-like dynamical defects, discerned via the spatial distribution of each particle's contribution to the reduced transverse VDOS, serve as the microscopic origin of the flat mode and its associated anomalies. These findings underscore the pivotal role of string-like dynamical defects in elucidating the vibrational and mechanical properties of active Brownian particles.","sentences":["In recent years, active Brownian particles have emerged as a prominent model system for comprehending the behaviors of active matter, wherein particles demonstrate self-propelled motion by harnessing energy from the surrounding environment.","A fundamental objective of studying active matter is to elucidate the physical mechanisms underlying its collective behaviors.","Drawing inspiration from advancements in molecular glasses, our study unveils a low-energy ``flat mode\" within the transverse spectrum of active Brownian vibrators -- a nearly two-dimensional, bi-disperse granular assembly.","We demonstrate that this collective excitation induces an anomalous excess in the vibrational density of states (VDOS) beyond the phononic Debye contribution.","Additionally, we establish through empirical evidence that string-like dynamical defects, discerned via the spatial distribution of each particle's contribution to the reduced transverse VDOS, serve as the microscopic origin of the flat mode and its associated anomalies.","These findings underscore the pivotal role of string-like dynamical defects in elucidating the vibrational and mechanical properties of active Brownian particles."],"url":"http://arxiv.org/abs/2404.09583v1","category":"cond-mat.soft"}
{"created":"2024-04-15 06:07:10","title":"SpamDam: Towards Privacy-Preserving and Adversary-Resistant SMS Spam Detection","abstract":"In this study, we introduce SpamDam, a SMS spam detection framework designed to overcome key challenges in detecting and understanding SMS spam, such as the lack of public SMS spam datasets, increasing privacy concerns of collecting SMS data, and the need for adversary-resistant detection models. SpamDam comprises four innovative modules: an SMS spam radar that identifies spam messages from online social networks(OSNs); an SMS spam inspector for statistical analysis; SMS spam detectors(SSDs) that enable both central training and federated learning; and an SSD analyzer that evaluates model resistance against adversaries in realistic scenarios. Leveraging SpamDam, we have compiled over 76K SMS spam messages from Twitter and Weibo between 2018 and 2023, forming the largest dataset of its kind. This dataset has enabled new insights into recent spam campaigns and the training of high-performing binary and multi-label classifiers for spam detection. Furthermore, effectiveness of federated learning has been well demonstrated to enable privacy-preserving SMS spam detection. Additionally, we have rigorously tested the adversarial robustness of SMS spam detection models, introducing the novel reverse backdoor attack, which has shown effectiveness and stealthiness in practical tests.","sentences":["In this study, we introduce SpamDam, a SMS spam detection framework designed to overcome key challenges in detecting and understanding SMS spam, such as the lack of public SMS spam datasets, increasing privacy concerns of collecting SMS data, and the need for adversary-resistant detection models.","SpamDam comprises four innovative modules: an SMS spam radar that identifies spam messages from online social networks(OSNs); an SMS spam inspector for statistical analysis; SMS spam detectors(SSDs)","that enable both central training and federated learning; and an SSD analyzer that evaluates model resistance against adversaries in realistic scenarios.","Leveraging SpamDam, we have compiled over 76K SMS spam messages from Twitter and Weibo between 2018 and 2023, forming the largest dataset of its kind.","This dataset has enabled new insights into recent spam campaigns and the training of high-performing binary and multi-label classifiers for spam detection.","Furthermore, effectiveness of federated learning has been well demonstrated to enable privacy-preserving SMS spam detection.","Additionally, we have rigorously tested the adversarial robustness of SMS spam detection models, introducing the novel reverse backdoor attack, which has shown effectiveness and stealthiness in practical tests."],"url":"http://arxiv.org/abs/2404.09481v1","category":"cs.CR"}
{"created":"2024-04-14 23:17:01","title":"\\textit{sweet} -- An Open Source Modular Platform for Contactless Hand Vascular Biometric Experiments","abstract":"Current finger-vein or palm-vein recognition systems usually require direct contact of the subject with the apparatus. This can be problematic in environments where hygiene is of primary importance. In this work we present a contactless vascular biometrics sensor platform named \\sweet which can be used for hand vascular biometrics studies (wrist-, palm- and finger-vein) and surface features such as palmprint. It supports several acquisition modalities such as multi-spectral Near-Infrared (NIR), RGB-color, Stereo Vision (SV) and Photometric Stereo (PS). Using this platform we collect a dataset consisting of the fingers, palm and wrist vascular data of 120 subjects and develop a powerful 3D pipeline for the pre-processing of this data. We then present biometric experimental results, focusing on Finger-Vein Recognition (FVR). Finally, we discuss fusion of multiple modalities, such palm-vein combined with palm-print biometrics. The acquisition software, parts of the hardware design, the new FV dataset, as well as source-code for our experiments are publicly available for research purposes.","sentences":["Current finger-vein or palm-vein recognition systems usually require direct contact of the subject with the apparatus.","This can be problematic in environments where hygiene is of primary importance.","In this work we present a contactless vascular biometrics sensor platform named \\sweet which can be used for hand vascular biometrics studies (wrist-, palm- and finger-vein) and surface features such as palmprint.","It supports several acquisition modalities such as multi-spectral Near-Infrared (NIR), RGB-color, Stereo Vision (SV) and Photometric Stereo (PS).","Using this platform we collect a dataset consisting of the fingers, palm and wrist vascular data of 120 subjects and develop a powerful 3D pipeline for the pre-processing of this data.","We then present biometric experimental results, focusing on Finger-Vein Recognition (FVR).","Finally, we discuss fusion of multiple modalities, such palm-vein combined with palm-print biometrics.","The acquisition software, parts of the hardware design, the new FV dataset, as well as source-code for our experiments are publicly available for research purposes."],"url":"http://arxiv.org/abs/2404.09376v1","category":"cs.CV"}
{"created":"2024-04-14 17:27:27","title":"Cost-effective company response policy for product co-creation in company-sponsored online community","abstract":"Product co-creation based on company-sponsored online community has come to be a paradigm of developing new products collaboratively with customers. In such a product co-creation campaign, the sponsoring company needs to interact intensively with active community members about the design scheme of the product. We call the collection of the rates of the company's response to active community members at all time in the co-creation campaign as a company response policy (CRP). This paper addresses the problem of finding a cost-effective CRP (the CRP problem). First, we introduce a novel community state evolutionary model and, thereby, establish an optimal control model for the CRP problem (the CRP model). Second, based on the optimality system for the CRP model, we present an iterative algorithm for solving the CRP model (the CRP algorithm). Thirdly, through extensive numerical experiments, we conclude that the CRP algorithm converges and the resulting CRP exhibits excellent cost benefit. Consequently, we recommend the resulting CRP to companies that embrace product co-creation. Next, we discuss how to implement the resulting CRP. Finally, we investigate the effect of some factors on the cost benefit of the resulting CRP. To our knowledge, this work is the first attempt to study value co-creation through optimal control theoretic approach.","sentences":["Product co-creation based on company-sponsored online community has come to be a paradigm of developing new products collaboratively with customers.","In such a product co-creation campaign, the sponsoring company needs to interact intensively with active community members about the design scheme of the product.","We call the collection of the rates of the company's response to active community members at all time in the co-creation campaign as a company response policy (CRP).","This paper addresses the problem of finding a cost-effective CRP (the CRP problem).","First, we introduce a novel community state evolutionary model and, thereby, establish an optimal control model for the CRP problem (the CRP model).","Second, based on the optimality system for the CRP model, we present an iterative algorithm for solving the CRP model (the CRP algorithm).","Thirdly, through extensive numerical experiments, we conclude that the CRP algorithm converges and the resulting CRP exhibits excellent cost benefit.","Consequently, we recommend the resulting CRP to companies that embrace product co-creation.","Next, we discuss how to implement the resulting CRP.","Finally, we investigate the effect of some factors on the cost benefit of the resulting CRP.","To our knowledge, this work is the first attempt to study value co-creation through optimal control theoretic approach."],"url":"http://arxiv.org/abs/2404.09307v1","category":"math.OC"}
{"created":"2024-04-14 16:34:31","title":"Cross-Data Knowledge Graph Construction for LLM-enabled Educational Question-Answering System: A~Case~Study~at~HCMUT","abstract":"In today's rapidly evolving landscape of Artificial Intelligence, large language models (LLMs) have emerged as a vibrant research topic. LLMs find applications in various fields and contribute significantly. Despite their powerful language capabilities, similar to pre-trained language models (PLMs), LLMs still face challenges in remembering events, incorporating new information, and addressing domain-specific issues or hallucinations. To overcome these limitations, researchers have proposed Retrieval-Augmented Generation (RAG) techniques, some others have proposed the integration of LLMs with Knowledge Graphs (KGs) to provide factual context, thereby improving performance and delivering more accurate feedback to user queries.   Education plays a crucial role in human development and progress. With the technology transformation, traditional education is being replaced by digital or blended education. Therefore, educational data in the digital environment is increasing day by day. Data in higher education institutions are diverse, comprising various sources such as unstructured/structured text, relational databases, web/app-based API access, etc. Constructing a Knowledge Graph from these cross-data sources is not a simple task. This article proposes a method for automatically constructing a Knowledge Graph from multiple data sources and discusses some initial applications (experimental trials) of KG in conjunction with LLMs for question-answering tasks.","sentences":["In today's rapidly evolving landscape of Artificial Intelligence, large language models (LLMs) have emerged as a vibrant research topic.","LLMs find applications in various fields and contribute significantly.","Despite their powerful language capabilities, similar to pre-trained language models (PLMs), LLMs still face challenges in remembering events, incorporating new information, and addressing domain-specific issues or hallucinations.","To overcome these limitations, researchers have proposed Retrieval-Augmented Generation (RAG) techniques, some others have proposed the integration of LLMs with Knowledge Graphs (KGs) to provide factual context, thereby improving performance and delivering more accurate feedback to user queries.   ","Education plays a crucial role in human development and progress.","With the technology transformation, traditional education is being replaced by digital or blended education.","Therefore, educational data in the digital environment is increasing day by day.","Data in higher education institutions are diverse, comprising various sources such as unstructured/structured text, relational databases, web/app-based API access, etc.","Constructing a Knowledge Graph from these cross-data sources is not a simple task.","This article proposes a method for automatically constructing a Knowledge Graph from multiple data sources and discusses some initial applications (experimental trials) of KG in conjunction with LLMs for question-answering tasks."],"url":"http://arxiv.org/abs/2404.09296v1","category":"cs.CL"}
{"created":"2024-04-14 15:58:35","title":"Bridging Data Islands: Geographic Heterogeneity-Aware Federated Learning for Collaborative Remote Sensing Semantic Segmentation","abstract":"Remote sensing semantic segmentation (RSS) is an essential task in Earth Observation missions. Due to data privacy concerns, high-quality remote sensing images with annotations cannot be well shared among institutions, making it difficult to fully utilize RSS data to train a generalized model. Federated Learning (FL), a privacy-preserving collaborative learning technology, is a potential solution. However, the current research on how to effectively apply FL in RSS is still scarce and requires further investigation. Remote sensing images in various institutions often exhibit strong geographical heterogeneity. More specifically, it is reflected in terms of class-distribution heterogeneity and object-appearance heterogeneity. Unfortunately, most existing FL studies show inadequate focus on geographical heterogeneity, thus leading to performance degradation in the global model. Considering the aforementioned issues, we propose a novel Geographic Heterogeneity-Aware Federated Learning (GeoFed) framework to address privacy-preserving RSS. Through Global Feature Extension and Tail Regeneration modules, class-distribution heterogeneity is alleviated. Additionally, we design an Essential Feature Mining strategy to alleviate object-appearance heterogeneity by constructing essential features. Extensive experiments on three datasets (i.e., FBP, CASID, Inria) show that our GeoFed consistently outperforms the current state-of-the-art methods. The code will be available publicly.","sentences":["Remote sensing semantic segmentation (RSS) is an essential task in Earth Observation missions.","Due to data privacy concerns, high-quality remote sensing images with annotations cannot be well shared among institutions, making it difficult to fully utilize RSS data to train a generalized model.","Federated Learning (FL), a privacy-preserving collaborative learning technology, is a potential solution.","However, the current research on how to effectively apply FL in RSS is still scarce and requires further investigation.","Remote sensing images in various institutions often exhibit strong geographical heterogeneity.","More specifically, it is reflected in terms of class-distribution heterogeneity and object-appearance heterogeneity.","Unfortunately, most existing FL studies show inadequate focus on geographical heterogeneity, thus leading to performance degradation in the global model.","Considering the aforementioned issues, we propose a novel Geographic Heterogeneity-Aware Federated Learning (GeoFed) framework to address privacy-preserving RSS.","Through Global Feature Extension and Tail Regeneration modules, class-distribution heterogeneity is alleviated.","Additionally, we design an Essential Feature Mining strategy to alleviate object-appearance heterogeneity by constructing essential features.","Extensive experiments on three datasets (i.e., FBP, CASID, Inria) show that our GeoFed consistently outperforms the current state-of-the-art methods.","The code will be available publicly."],"url":"http://arxiv.org/abs/2404.09292v1","category":"cs.CV"}
{"created":"2024-04-14 15:38:34","title":"Artificial Intelligence enhanced Security Problems in Real-Time Scenario using Blowfish Algorithm","abstract":"In a nutshell, \"the cloud\" refers to a collection of interconnected computing resources made possible by an extensive, real-time communication network like the internet. Because of its potential to reduce processing costs, the emerging paradigm of cloud computing has recently attracted a large number of academics. The exponential expansion of cloud computing has made the rapid expansion of cloud services very remarkable. Ensuring the security of personal information in today's interconnected world is no easy task. These days, security is really crucial. Models of security that are relevant to cloud computing include confidentiality, authenticity, accessibility, data integrity, and recovery. Using the Hybrid Encryption this study, we cover all the security issues and leaks in cloud infrastructure.","sentences":["In a nutshell, \"the cloud\" refers to a collection of interconnected computing resources made possible by an extensive, real-time communication network like the internet.","Because of its potential to reduce processing costs, the emerging paradigm of cloud computing has recently attracted a large number of academics.","The exponential expansion of cloud computing has made the rapid expansion of cloud services very remarkable.","Ensuring the security of personal information in today's interconnected world is no easy task.","These days, security is really crucial.","Models of security that are relevant to cloud computing include confidentiality, authenticity, accessibility, data integrity, and recovery.","Using the Hybrid Encryption this study, we cover all the security issues and leaks in cloud infrastructure."],"url":"http://arxiv.org/abs/2404.09286v1","category":"cs.CR"}
{"created":"2024-04-14 14:51:44","title":"TrafficVLM: A Controllable Visual Language Model for Traffic Video Captioning","abstract":"Traffic video description and analysis have received much attention recently due to the growing demand for efficient and reliable urban surveillance systems. Most existing methods only focus on locating traffic event segments, which severely lack descriptive details related to the behaviour and context of all the subjects of interest in the events. In this paper, we present TrafficVLM, a novel multi-modal dense video captioning model for vehicle ego camera view. TrafficVLM models traffic video events at different levels of analysis, both spatially and temporally, and generates long fine-grained descriptions for the vehicle and pedestrian at different phases of the event. We also propose a conditional component for TrafficVLM to control the generation outputs and a multi-task fine-tuning paradigm to enhance TrafficVLM's learning capability. Experiments show that TrafficVLM performs well on both vehicle and overhead camera views. Our solution achieved outstanding results in Track 2 of the AI City Challenge 2024, ranking us third in the challenge standings. Our code is publicly available at https://github.com/quangminhdinh/TrafficVLM.","sentences":["Traffic video description and analysis have received much attention recently due to the growing demand for efficient and reliable urban surveillance systems.","Most existing methods only focus on locating traffic event segments, which severely lack descriptive details related to the behaviour and context of all the subjects of interest in the events.","In this paper, we present TrafficVLM, a novel multi-modal dense video captioning model for vehicle ego camera view.","TrafficVLM models traffic video events at different levels of analysis, both spatially and temporally, and generates long fine-grained descriptions for the vehicle and pedestrian at different phases of the event.","We also propose a conditional component for TrafficVLM to control the generation outputs and a multi-task fine-tuning paradigm to enhance TrafficVLM's learning capability.","Experiments show that TrafficVLM performs well on both vehicle and overhead camera views.","Our solution achieved outstanding results in Track 2 of the AI City Challenge 2024, ranking us third in the challenge standings.","Our code is publicly available at https://github.com/quangminhdinh/TrafficVLM."],"url":"http://arxiv.org/abs/2404.09275v1","category":"cs.CV"}
{"created":"2024-04-14 14:24:13","title":"PANet: A Physics-guided Parametric Augmentation Net for Image Dehazing by Hazing","abstract":"Image dehazing faces challenges when dealing with hazy images in real-world scenarios. A huge domain gap between synthetic and real-world haze images degrades dehazing performance in practical settings. However, collecting real-world image datasets for training dehazing models is challenging since both hazy and clean pairs must be captured under the same conditions. In this paper, we propose a Physics-guided Parametric Augmentation Network (PANet) that generates photo-realistic hazy and clean training pairs to effectively enhance real-world dehazing performance. PANet comprises a Haze-to-Parameter Mapper (HPM) to project hazy images into a parameter space and a Parameter-to-Haze Mapper (PHM) to map the resampled haze parameters back to hazy images. In the parameter space, we can pixel-wisely resample individual haze parameter maps to generate diverse hazy images with physically-explainable haze conditions unseen in the training set. Our experimental results demonstrate that PANet can augment diverse realistic hazy images to enrich existing hazy image benchmarks so as to effectively boost the performances of state-of-the-art image dehazing models.","sentences":["Image dehazing faces challenges when dealing with hazy images in real-world scenarios.","A huge domain gap between synthetic and real-world haze images degrades dehazing performance in practical settings.","However, collecting real-world image datasets for training dehazing models is challenging since both hazy and clean pairs must be captured under the same conditions.","In this paper, we propose a Physics-guided Parametric Augmentation Network (PANet) that generates photo-realistic hazy and clean training pairs to effectively enhance real-world dehazing performance.","PANet comprises a Haze-to-Parameter Mapper (HPM) to project hazy images into a parameter space and a Parameter-to-Haze Mapper (PHM) to map the resampled haze parameters back to hazy images.","In the parameter space, we can pixel-wisely resample individual haze parameter maps to generate diverse hazy images with physically-explainable haze conditions unseen in the training set.","Our experimental results demonstrate that PANet can augment diverse realistic hazy images to enrich existing hazy image benchmarks so as to effectively boost the performances of state-of-the-art image dehazing models."],"url":"http://arxiv.org/abs/2404.09269v1","category":"cs.CV"}
{"created":"2024-04-14 14:14:31","title":"Make Split, not Hijack: Preventing Feature-Space Hijacking Attacks in Split Learning","abstract":"The popularity of Machine Learning (ML) makes the privacy of sensitive data more imperative than ever. Collaborative learning techniques like Split Learning (SL) aim to protect client data while enhancing ML processes. Though promising, SL has been proved to be vulnerable to a plethora of attacks, thus raising concerns about its effectiveness on data privacy. In this work, we introduce a hybrid approach combining SL and Function Secret Sharing (FSS) to ensure client data privacy. The client adds a random mask to the activation map before sending it to the servers. The servers cannot access the original function but instead work with shares generated using FSS. Consequently, during both forward and backward propagation, the servers cannot reconstruct the client's raw data from the activation map. Furthermore, through visual invertibility, we demonstrate that the server is incapable of reconstructing the raw image data from the activation map when using FSS. It enhances privacy by reducing privacy leakage compared to other SL-based approaches where the server can access client input information. Our approach also ensures security against feature space hijacking attack, protecting sensitive information from potential manipulation. Our protocols yield promising results, reducing communication overhead by over 2x and training time by over 7x compared to the same model with FSS, without any SL. Also, we show that our approach achieves >96% accuracy and remains equivalent to the plaintext models.","sentences":["The popularity of Machine Learning (ML) makes the privacy of sensitive data more imperative than ever.","Collaborative learning techniques like Split Learning (SL) aim to protect client data while enhancing ML processes.","Though promising, SL has been proved to be vulnerable to a plethora of attacks, thus raising concerns about its effectiveness on data privacy.","In this work, we introduce a hybrid approach combining SL and Function Secret Sharing (FSS) to ensure client data privacy.","The client adds a random mask to the activation map before sending it to the servers.","The servers cannot access the original function but instead work with shares generated using FSS.","Consequently, during both forward and backward propagation, the servers cannot reconstruct the client's raw data from the activation map.","Furthermore, through visual invertibility, we demonstrate that the server is incapable of reconstructing the raw image data from the activation map when using FSS.","It enhances privacy by reducing privacy leakage compared to other SL-based approaches where the server can access client input information.","Our approach also ensures security against feature space hijacking attack, protecting sensitive information from potential manipulation.","Our protocols yield promising results, reducing communication overhead by over 2x and training time by over 7x compared to the same model with FSS, without any SL.","Also, we show that our approach achieves >96% accuracy and remains equivalent to the plaintext models."],"url":"http://arxiv.org/abs/2404.09265v1","category":"cs.CR"}
{"created":"2024-04-14 14:06:42","title":"Task-Driven Exploration: Decoupling and Inter-Task Feedback for Joint Moment Retrieval and Highlight Detection","abstract":"Video moment retrieval and highlight detection are two highly valuable tasks in video understanding, but until recently they have been jointly studied. Although existing studies have made impressive advancement recently, they predominantly follow the data-driven bottom-up paradigm. Such paradigm overlooks task-specific and inter-task effects, resulting in poor model performance. In this paper, we propose a novel task-driven top-down framework TaskWeave for joint moment retrieval and highlight detection. The framework introduces a task-decoupled unit to capture task-specific and common representations. To investigate the interplay between the two tasks, we propose an inter-task feedback mechanism, which transforms the results of one task as guiding masks to assist the other task. Different from existing methods, we present a task-dependent joint loss function to optimize the model. Comprehensive experiments and in-depth ablation studies on QVHighlights, TVSum, and Charades-STA datasets corroborate the effectiveness and flexibility of the proposed framework. Codes are available at https://github.com/EdenGabriel/TaskWeave.","sentences":["Video moment retrieval and highlight detection are two highly valuable tasks in video understanding, but until recently they have been jointly studied.","Although existing studies have made impressive advancement recently, they predominantly follow the data-driven bottom-up paradigm.","Such paradigm overlooks task-specific and inter-task effects, resulting in poor model performance.","In this paper, we propose a novel task-driven top-down framework TaskWeave for joint moment retrieval and highlight detection.","The framework introduces a task-decoupled unit to capture task-specific and common representations.","To investigate the interplay between the two tasks, we propose an inter-task feedback mechanism, which transforms the results of one task as guiding masks to assist the other task.","Different from existing methods, we present a task-dependent joint loss function to optimize the model.","Comprehensive experiments and in-depth ablation studies on QVHighlights, TVSum, and Charades-STA datasets corroborate the effectiveness and flexibility of the proposed framework.","Codes are available at https://github.com/EdenGabriel/TaskWeave."],"url":"http://arxiv.org/abs/2404.09263v1","category":"cs.CV"}
{"created":"2024-04-14 13:56:30","title":"FedCCL: Federated Dual-Clustered Feature Contrast Under Domain Heterogeneity","abstract":"Federated learning (FL) facilitates a privacy-preserving neural network training paradigm through collaboration between edge clients and a central server. One significant challenge is that the distributed data is not independently and identically distributed (non-IID), typically including both intra-domain and inter-domain heterogeneity. However, recent research is limited to simply using averaged signals as a form of regularization and only focusing on one aspect of these non-IID challenges. Given these limitations, this paper clarifies these two non-IID challenges and attempts to introduce cluster representation to address them from both local and global perspectives. Specifically, we propose a dual-clustered feature contrast-based FL framework with dual focuses. First, we employ clustering on the local representations of each client, aiming to capture intra-class information based on these local clusters at a high level of granularity. Then, we facilitate cross-client knowledge sharing by pulling the local representation closer to clusters shared by clients with similar semantics while pushing them away from clusters with dissimilar semantics. Second, since the sizes of local clusters belonging to the same class may differ for each client, we further utilize clustering on the global side and conduct averaging to create a consistent global signal for guiding each local training in a contrastive manner. Experimental results on multiple datasets demonstrate that our proposal achieves comparable or superior performance gain under intra-domain and inter-domain heterogeneity.","sentences":["Federated learning (FL) facilitates a privacy-preserving neural network training paradigm through collaboration between edge clients and a central server.","One significant challenge is that the distributed data is not independently and identically distributed (non-IID), typically including both intra-domain and inter-domain heterogeneity.","However, recent research is limited to simply using averaged signals as a form of regularization and only focusing on one aspect of these non-IID challenges.","Given these limitations, this paper clarifies these two non-IID challenges and attempts to introduce cluster representation to address them from both local and global perspectives.","Specifically, we propose a dual-clustered feature contrast-based FL framework with dual focuses.","First, we employ clustering on the local representations of each client, aiming to capture intra-class information based on these local clusters at a high level of granularity.","Then, we facilitate cross-client knowledge sharing by pulling the local representation closer to clusters shared by clients with similar semantics while pushing them away from clusters with dissimilar semantics.","Second, since the sizes of local clusters belonging to the same class may differ for each client, we further utilize clustering on the global side and conduct averaging to create a consistent global signal for guiding each local training in a contrastive manner.","Experimental results on multiple datasets demonstrate that our proposal achieves comparable or superior performance gain under intra-domain and inter-domain heterogeneity."],"url":"http://arxiv.org/abs/2404.09259v1","category":"cs.CV"}
{"created":"2024-04-14 13:39:02","title":"TEXT2TASTE: A Versatile Egocentric Vision System for Intelligent Reading Assistance Using Large Language Model","abstract":"The ability to read, understand and find important information from written text is a critical skill in our daily lives for our independence, comfort and safety. However, a significant part of our society is affected by partial vision impairment, which leads to discomfort and dependency in daily activities. To address the limitations of this part of society, we propose an intelligent reading assistant based on smart glasses with embedded RGB cameras and a Large Language Model (LLM), whose functionality goes beyond corrective lenses. The video recorded from the egocentric perspective of a person wearing the glasses is processed to localise text information using object detection and optical character recognition methods. The LLM processes the data and allows the user to interact with the text and responds to a given query, thus extending the functionality of corrective lenses with the ability to find and summarize knowledge from the text. To evaluate our method, we create a chat-based application that allows the user to interact with the system. The evaluation is conducted in a real-world setting, such as reading menus in a restaurant, and involves four participants. The results show robust accuracy in text retrieval. The system not only provides accurate meal suggestions but also achieves high user satisfaction, highlighting the potential of smart glasses and LLMs in assisting people with special needs.","sentences":["The ability to read, understand and find important information from written text is a critical skill in our daily lives for our independence, comfort and safety.","However, a significant part of our society is affected by partial vision impairment, which leads to discomfort and dependency in daily activities.","To address the limitations of this part of society, we propose an intelligent reading assistant based on smart glasses with embedded RGB cameras and a Large Language Model (LLM), whose functionality goes beyond corrective lenses.","The video recorded from the egocentric perspective of a person wearing the glasses is processed to localise text information using object detection and optical character recognition methods.","The LLM processes the data and allows the user to interact with the text and responds to a given query, thus extending the functionality of corrective lenses with the ability to find and summarize knowledge from the text.","To evaluate our method, we create a chat-based application that allows the user to interact with the system.","The evaluation is conducted in a real-world setting, such as reading menus in a restaurant, and involves four participants.","The results show robust accuracy in text retrieval.","The system not only provides accurate meal suggestions but also achieves high user satisfaction, highlighting the potential of smart glasses and LLMs in assisting people with special needs."],"url":"http://arxiv.org/abs/2404.09254v1","category":"cs.CV"}
{"created":"2024-04-14 13:25:15","title":"Test Code Generation for Telecom Software Systems using Two-Stage Generative Model","abstract":"In recent years, the evolution of Telecom towards achieving intelligent, autonomous, and open networks has led to an increasingly complex Telecom Software system, supporting various heterogeneous deployment scenarios, with multi-standard and multi-vendor support. As a result, it becomes a challenge for large-scale Telecom software companies to develop and test software for all deployment scenarios. To address these challenges, we propose a framework for Automated Test Generation for large-scale Telecom Software systems. We begin by generating Test Case Input data for test scenarios observed using a time-series Generative model trained on historical Telecom Network data during field trials. Additionally, the time-series Generative model helps in preserving the privacy of Telecom data. The generated time-series software performance data are then utilized with test descriptions written in natural language to generate Test Script using the Generative Large Language Model. Our comprehensive experiments on public datasets and Telecom datasets obtained from operational Telecom Networks demonstrate that the framework can effectively generate comprehensive test case data input and useful test code.","sentences":["In recent years, the evolution of Telecom towards achieving intelligent, autonomous, and open networks has led to an increasingly complex Telecom Software system, supporting various heterogeneous deployment scenarios, with multi-standard and multi-vendor support.","As a result, it becomes a challenge for large-scale Telecom software companies to develop and test software for all deployment scenarios.","To address these challenges, we propose a framework for Automated Test Generation for large-scale Telecom Software systems.","We begin by generating Test Case Input data for test scenarios observed using a time-series Generative model trained on historical Telecom Network data during field trials.","Additionally, the time-series Generative model helps in preserving the privacy of Telecom data.","The generated time-series software performance data are then utilized with test descriptions written in natural language to generate Test Script using the Generative Large Language Model.","Our comprehensive experiments on public datasets and Telecom datasets obtained from operational Telecom Networks demonstrate that the framework can effectively generate comprehensive test case data input and useful test code."],"url":"http://arxiv.org/abs/2404.09249v1","category":"cs.SE"}
{"created":"2024-04-14 13:19:40","title":"Knowledgeable Agents by Offline Reinforcement Learning from Large Language Model Rollouts","abstract":"Reinforcement learning (RL) trains agents to accomplish complex tasks through environmental interaction data, but its capacity is also limited by the scope of the available data. To obtain a knowledgeable agent, a promising approach is to leverage the knowledge from large language models (LLMs). Despite previous studies combining LLMs with RL, seamless integration of the two components remains challenging due to their semantic gap. This paper introduces a novel method, Knowledgeable Agents from Language Model Rollouts (KALM), which extracts knowledge from LLMs in the form of imaginary rollouts that can be easily learned by the agent through offline reinforcement learning methods. The primary challenge of KALM lies in LLM grounding, as LLMs are inherently limited to textual data, whereas environmental data often comprise numerical vectors unseen to LLMs. To address this, KALM fine-tunes the LLM to perform various tasks based on environmental data, including bidirectional translation between natural language descriptions of skills and their corresponding rollout data. This grounding process enhances the LLM's comprehension of environmental dynamics, enabling it to generate diverse and meaningful imaginary rollouts that reflect novel skills. Initial empirical evaluations on the CLEVR-Robot environment demonstrate that KALM enables agents to complete complex rephrasings of task goals and extend their capabilities to novel tasks requiring unprecedented optimal behaviors. KALM achieves a success rate of 46% in executing tasks with unseen goals, substantially surpassing the 26% success rate achieved by baseline methods. Furthermore, KALM effectively enables the LLM to comprehend environmental dynamics, resulting in the generation of meaningful imaginary rollouts that reflect novel skills and demonstrate the seamless integration of large language models and reinforcement learning.","sentences":["Reinforcement learning (RL) trains agents to accomplish complex tasks through environmental interaction data, but its capacity is also limited by the scope of the available data.","To obtain a knowledgeable agent, a promising approach is to leverage the knowledge from large language models (LLMs).","Despite previous studies combining LLMs with RL, seamless integration of the two components remains challenging due to their semantic gap.","This paper introduces a novel method, Knowledgeable Agents from Language Model Rollouts (KALM), which extracts knowledge from LLMs in the form of imaginary rollouts that can be easily learned by the agent through offline reinforcement learning methods.","The primary challenge of KALM lies in LLM grounding, as LLMs are inherently limited to textual data, whereas environmental data often comprise numerical vectors unseen to LLMs.","To address this, KALM fine-tunes the LLM to perform various tasks based on environmental data, including bidirectional translation between natural language descriptions of skills and their corresponding rollout data.","This grounding process enhances the LLM's comprehension of environmental dynamics, enabling it to generate diverse and meaningful imaginary rollouts that reflect novel skills.","Initial empirical evaluations on the CLEVR-Robot environment demonstrate that KALM enables agents to complete complex rephrasings of task goals and extend their capabilities to novel tasks requiring unprecedented optimal behaviors.","KALM achieves a success rate of 46% in executing tasks with unseen goals, substantially surpassing the 26% success rate achieved by baseline methods.","Furthermore, KALM effectively enables the LLM to comprehend environmental dynamics, resulting in the generation of meaningful imaginary rollouts that reflect novel skills and demonstrate the seamless integration of large language models and reinforcement learning."],"url":"http://arxiv.org/abs/2404.09248v1","category":"cs.LG"}
{"created":"2024-04-14 13:17:32","title":"Generalization Error Bounds for Learning under Censored Feedback","abstract":"Generalization error bounds from learning theory provide statistical guarantees on how well an algorithm will perform on previously unseen data. In this paper, we characterize the impacts of data non-IIDness due to censored feedback (a.k.a. selective labeling bias) on such bounds. We first derive an extension of the well-known Dvoretzky-Kiefer-Wolfowitz (DKW) inequality, which characterizes the gap between empirical and theoretical CDFs given IID data, to problems with non-IID data due to censored feedback. We then use this CDF error bound to provide a bound on the generalization error guarantees of a classifier trained on such non-IID data. We show that existing generalization error bounds (which do not account for censored feedback) fail to correctly capture the model's generalization guarantees, verifying the need for our bounds. We further analyze the effectiveness of (pure and bounded) exploration techniques, proposed by recent literature as a way to alleviate censored feedback, on improving our error bounds. Together, our findings illustrate how a decision maker should account for the trade-off between strengthening the generalization guarantees of an algorithm and the costs incurred in data collection when future data availability is limited by censored feedback.","sentences":["Generalization error bounds from learning theory provide statistical guarantees on how well an algorithm will perform on previously unseen data.","In this paper, we characterize the impacts of data non-IIDness due to censored feedback (a.k.a. selective labeling bias) on such bounds.","We first derive an extension of the well-known Dvoretzky-Kiefer-Wolfowitz (DKW) inequality, which characterizes the gap between empirical and theoretical CDFs given IID data, to problems with non-IID data due to censored feedback.","We then use this CDF error bound to provide a bound on the generalization error guarantees of a classifier trained on such non-IID data.","We show that existing generalization error bounds (which do not account for censored feedback) fail to correctly capture the model's generalization guarantees, verifying the need for our bounds.","We further analyze the effectiveness of (pure and bounded) exploration techniques, proposed by recent literature as a way to alleviate censored feedback, on improving our error bounds.","Together, our findings illustrate how a decision maker should account for the trade-off between strengthening the generalization guarantees of an algorithm and the costs incurred in data collection when future data availability is limited by censored feedback."],"url":"http://arxiv.org/abs/2404.09247v1","category":"cs.LG"}
{"created":"2024-04-14 13:14:13","title":"Arena: A Patch-of-Interest ViT Inference Acceleration System for Edge-Assisted Video Analytics","abstract":"The advent of edge computing has made real-time intelligent video analytics feasible. Previous works, based on traditional model architecture (e.g., CNN, RNN, etc.), employ various strategies to filter out non-region-of-interest content to minimize bandwidth and computation consumption but show inferior performance in adverse environments. Recently, visual foundation models based on transformers have shown great performance in adverse environments due to their amazing generalization capability. However, they require a large amount of computation power, which limits their applications in real-time intelligent video analytics. In this paper, we find visual foundation models like Vision Transformer (ViT) also have a dedicated acceleration mechanism for video analytics. To this end, we introduce Arena, an end-to-end edge-assisted video inference acceleration system based on ViT. We leverage the capability of ViT that can be accelerated through token pruning by only offloading and feeding Patches-of-Interest (PoIs) to the downstream models. Additionally, we employ probability-based patch sampling, which provides a simple but efficient mechanism for determining PoIs where the probable locations of objects are in subsequent frames. Through extensive evaluations on public datasets, our findings reveal that Arena can boost inference speeds by up to $1.58\\times$ and $1.82\\times$ on average while consuming only 54% and 34% of the bandwidth, respectively, all with high inference accuracy.","sentences":["The advent of edge computing has made real-time intelligent video analytics feasible.","Previous works, based on traditional model architecture (e.g., CNN, RNN, etc.), employ various strategies to filter out non-region-of-interest content to minimize bandwidth and computation consumption but show inferior performance in adverse environments.","Recently, visual foundation models based on transformers have shown great performance in adverse environments due to their amazing generalization capability.","However, they require a large amount of computation power, which limits their applications in real-time intelligent video analytics.","In this paper, we find visual foundation models like Vision Transformer (ViT) also have a dedicated acceleration mechanism for video analytics.","To this end, we introduce Arena, an end-to-end edge-assisted video inference acceleration system based on ViT.","We leverage the capability of ViT that can be accelerated through token pruning by only offloading and feeding Patches-of-Interest (PoIs) to the downstream models.","Additionally, we employ probability-based patch sampling, which provides a simple but efficient mechanism for determining PoIs where the probable locations of objects are in subsequent frames.","Through extensive evaluations on public datasets, our findings reveal that Arena can boost inference speeds by up to $1.58\\times$ and $1.82\\times$ on average while consuming only 54% and 34% of the bandwidth, respectively, all with high inference accuracy."],"url":"http://arxiv.org/abs/2404.09245v1","category":"cs.MM"}
{"created":"2024-04-14 12:59:35","title":"Fault Detection in Mobile Networks Using Diffusion Models","abstract":"In today's hyper-connected world, ensuring the reliability of telecom networks becomes increasingly crucial. Telecom networks encompass numerous underlying and intertwined software and hardware components, each providing different functionalities. To ensure the stability of telecom networks, telecom software, and hardware vendors developed several methods to detect any aberrant behavior in telecom networks and enable instant feedback and alerts. These approaches, although powerful, struggle to generalize due to the unsteady nature of the software-intensive embedded system and the complexity and diversity of multi-standard mobile networks. In this paper, we present a system to detect anomalies in telecom networks using a generative AI model. We evaluate several strategies using diffusion models to train the model for anomaly detection using multivariate time-series data. The contributions of this paper are threefold: (i) A proposal of a framework for utilizing diffusion models for time-series anomaly detection in telecom networks, (ii) A proposal of a particular Diffusion model architecture that outperforms other state-of-the-art techniques, (iii) Experiments on a real-world dataset to demonstrate that our model effectively provides explainable results, exposing some of its limitations and suggesting future research avenues to enhance its capabilities further.","sentences":["In today's hyper-connected world, ensuring the reliability of telecom networks becomes increasingly crucial.","Telecom networks encompass numerous underlying and intertwined software and hardware components, each providing different functionalities.","To ensure the stability of telecom networks, telecom software, and hardware vendors developed several methods to detect any aberrant behavior in telecom networks and enable instant feedback and alerts.","These approaches, although powerful, struggle to generalize due to the unsteady nature of the software-intensive embedded system and the complexity and diversity of multi-standard mobile networks.","In this paper, we present a system to detect anomalies in telecom networks using a generative AI model.","We evaluate several strategies using diffusion models to train the model for anomaly detection using multivariate time-series data.","The contributions of this paper are threefold: (i) A proposal of a framework for utilizing diffusion models for time-series anomaly detection in telecom networks, (ii) A proposal of a particular Diffusion model architecture that outperforms other state-of-the-art techniques, (iii) Experiments on a real-world dataset to demonstrate that our model effectively provides explainable results, exposing some of its limitations and suggesting future research avenues to enhance its capabilities further."],"url":"http://arxiv.org/abs/2404.09240v1","category":"cs.LG"}
{"created":"2024-04-14 12:15:21","title":"A Survey on Integration of Large Language Models with Intelligent Robots","abstract":"In recent years, the integration of large language models (LLMs) has revolutionized the field of robotics, enabling robots to communicate, understand, and reason with human-like proficiency. This paper explores the multifaceted impact of LLMs on robotics, addressing key challenges and opportunities for leveraging these models across various domains. By categorizing and analyzing LLM applications within core robotics elements -- communication, perception, planning, and control -- we aim to provide actionable insights for researchers seeking to integrate LLMs into their robotic systems. Our investigation focuses on LLMs developed post-GPT-3.5, primarily in text-based modalities while also considering multimodal approaches for perception and control. We offer comprehensive guidelines and examples for prompt engineering, facilitating beginners' access to LLM-based robotics solutions. Through tutorial-level examples and structured prompt construction, we illustrate how LLM-guided enhancements can be seamlessly integrated into robotics applications. This survey serves as a roadmap for researchers navigating the evolving landscape of LLM-driven robotics, offering a comprehensive overview and practical guidance for harnessing the power of language models in robotics development.","sentences":["In recent years, the integration of large language models (LLMs) has revolutionized the field of robotics, enabling robots to communicate, understand, and reason with human-like proficiency.","This paper explores the multifaceted impact of LLMs on robotics, addressing key challenges and opportunities for leveraging these models across various domains.","By categorizing and analyzing LLM applications within core robotics elements -- communication, perception, planning, and control -- we aim to provide actionable insights for researchers seeking to integrate LLMs into their robotic systems.","Our investigation focuses on LLMs developed post-GPT-3.5, primarily in text-based modalities while also considering multimodal approaches for perception and control.","We offer comprehensive guidelines and examples for prompt engineering, facilitating beginners' access to LLM-based robotics solutions.","Through tutorial-level examples and structured prompt construction, we illustrate how LLM-guided enhancements can be seamlessly integrated into robotics applications.","This survey serves as a roadmap for researchers navigating the evolving landscape of LLM-driven robotics, offering a comprehensive overview and practical guidance for harnessing the power of language models in robotics development."],"url":"http://arxiv.org/abs/2404.09228v1","category":"cs.RO"}
{"created":"2024-04-14 11:49:38","title":"Towards Fast Inference: Exploring and Improving Blockwise Parallel Drafts","abstract":"Despite the remarkable strides made by autoregressive language models, their potential is often hampered by the slow inference speeds inherent in sequential token generation. Blockwise parallel decoding (BPD) was proposed by Stern et al. (2018) as a way to improve inference speed of language models. In this paper, we make two contributions to understanding and improving BPD drafts. We first offer an analysis of the token distributions produced by the BPD prediction heads. Secondly, we use this analysis to inform algorithms to improve BPD inference speed by refining the BPD drafts using small n-gram or neural language models. We empirically show that these refined BPD drafts yield a higher average verified prefix length across tasks.","sentences":["Despite the remarkable strides made by autoregressive language models, their potential is often hampered by the slow inference speeds inherent in sequential token generation.","Blockwise parallel decoding (BPD) was proposed by Stern et al.","(2018) as a way to improve inference speed of language models.","In this paper, we make two contributions to understanding and improving BPD drafts.","We first offer an analysis of the token distributions produced by the BPD prediction heads.","Secondly, we use this analysis to inform algorithms to improve BPD inference speed by refining the BPD drafts using small n-gram or neural language models.","We empirically show that these refined BPD drafts yield a higher average verified prefix length across tasks."],"url":"http://arxiv.org/abs/2404.09221v1","category":"cs.CL"}
{"created":"2024-04-14 11:01:04","title":"Optimum Beamforming and Grating Lobe Mitigation for Intelligent Reflecting Surfaces","abstract":"Ensuring adequate wireless coverage in upcoming communication technologies such as 6G is expected to be challenging. This is because user demands of higher datarate require an increase in carrier frequencies, which in turn reduce the diffraction effects (and hence coverage) in complex multipath environments. Intelligent reflecting surfaces have been proposed as a way of restoring coverage by adaptively reflecting incoming electromagnetic waves in desired directions. This is accomplished by judiciously adding extra phases at different points on the surface. In practice, these extra phases are only available in discrete quantities due to hardware constraints. Computing these extra phases is computationally challenging when they can only be picked from a discrete distribution, and existing approaches for solving this problem were either heuristic or based on evolutionary algorithms. We solve this problem by proposing fast algorithms with provably optimal solutions. Our algorithms have linear complexity, and are presented with rigorous proofs for their optimality. We show that the proposed algorithms exhibit better performance. We analyze situations when unwanted grating lobes arise in the radiation pattern, and discuss mitigation strategies, such as the use of triangular lattices and prephasing techniques, to eliminate them. We also demonstrate how our algorithms can leverage these techniques to deliver optimum beamforming solutions.","sentences":["Ensuring adequate wireless coverage in upcoming communication technologies such as 6G is expected to be challenging.","This is because user demands of higher datarate require an increase in carrier frequencies, which in turn reduce the diffraction effects (and hence coverage) in complex multipath environments.","Intelligent reflecting surfaces have been proposed as a way of restoring coverage by adaptively reflecting incoming electromagnetic waves in desired directions.","This is accomplished by judiciously adding extra phases at different points on the surface.","In practice, these extra phases are only available in discrete quantities due to hardware constraints.","Computing these extra phases is computationally challenging when they can only be picked from a discrete distribution, and existing approaches for solving this problem were either heuristic or based on evolutionary algorithms.","We solve this problem by proposing fast algorithms with provably optimal solutions.","Our algorithms have linear complexity, and are presented with rigorous proofs for their optimality.","We show that the proposed algorithms exhibit better performance.","We analyze situations when unwanted grating lobes arise in the radiation pattern, and discuss mitigation strategies, such as the use of triangular lattices and prephasing techniques, to eliminate them.","We also demonstrate how our algorithms can leverage these techniques to deliver optimum beamforming solutions."],"url":"http://arxiv.org/abs/2404.09215v1","category":"eess.SP"}
{"created":"2024-04-14 10:23:30","title":"FedDistill: Global Model Distillation for Local Model De-Biasing in Non-IID Federated Learning","abstract":"Federated Learning (FL) is a novel approach that allows for collaborative machine learning while preserving data privacy by leveraging models trained on decentralized devices. However, FL faces challenges due to non-uniformly distributed (non-iid) data across clients, which impacts model performance and its generalization capabilities. To tackle the non-iid issue, recent efforts have utilized the global model as a teaching mechanism for local models. However, our pilot study shows that their effectiveness is constrained by imbalanced data distribution, which induces biases in local models and leads to a 'local forgetting' phenomenon, where the ability of models to generalize degrades over time, particularly for underrepresented classes. This paper introduces FedDistill, a framework enhancing the knowledge transfer from the global model to local models, focusing on the issue of imbalanced class distribution. Specifically, FedDistill employs group distillation, segmenting classes based on their frequency in local datasets to facilitate a focused distillation process to classes with fewer samples. Additionally, FedDistill dissects the global model into a feature extractor and a classifier. This separation empowers local models with more generalized data representation capabilities and ensures more accurate classification across all classes. FedDistill mitigates the adverse effects of data imbalance, ensuring that local models do not forget underrepresented classes but instead become more adept at recognizing and classifying them accurately. Our comprehensive experiments demonstrate FedDistill's effectiveness, surpassing existing baselines in accuracy and convergence speed across several benchmark datasets.","sentences":["Federated Learning (FL) is a novel approach that allows for collaborative machine learning while preserving data privacy by leveraging models trained on decentralized devices.","However, FL faces challenges due to non-uniformly distributed (non-iid) data across clients, which impacts model performance and its generalization capabilities.","To tackle the non-iid issue, recent efforts have utilized the global model as a teaching mechanism for local models.","However, our pilot study shows that their effectiveness is constrained by imbalanced data distribution, which induces biases in local models and leads to a 'local forgetting' phenomenon, where the ability of models to generalize degrades over time, particularly for underrepresented classes.","This paper introduces FedDistill, a framework enhancing the knowledge transfer from the global model to local models, focusing on the issue of imbalanced class distribution.","Specifically, FedDistill employs group distillation, segmenting classes based on their frequency in local datasets to facilitate a focused distillation process to classes with fewer samples.","Additionally, FedDistill dissects the global model into a feature extractor and a classifier.","This separation empowers local models with more generalized data representation capabilities and ensures more accurate classification across all classes.","FedDistill mitigates the adverse effects of data imbalance, ensuring that local models do not forget underrepresented classes but instead become more adept at recognizing and classifying them accurately.","Our comprehensive experiments demonstrate FedDistill's effectiveness, surpassing existing baselines in accuracy and convergence speed across several benchmark datasets."],"url":"http://arxiv.org/abs/2404.09210v1","category":"cs.LG"}
{"created":"2024-04-14 09:48:37","title":"TextHawk: Exploring Efficient Fine-Grained Perception of Multimodal Large Language Models","abstract":"Multimodal Large Language Models (MLLMs) have shown impressive results on various multimodal tasks. However, most existing MLLMs are not well suited for document-oriented tasks, which require fine-grained image perception and information compression. In this paper, we present TextHawk, a MLLM that is specifically designed for document-oriented tasks, while preserving the general capabilities of MLLMs. TextHawk is aimed to explore efficient fine-grained perception by designing four dedicated components. Firstly, a ReSampling and ReArrangement (ReSA) module is proposed to reduce the redundancy in the document texts and lower the computational cost of the MLLM. We explore encoding the positions of each local feature by presenting Scalable Positional Embeddings (SPEs), which can preserve the scalability of various image sizes. A Query Proposal Network (QPN) is then adopted to initialize the queries dynamically among different sub-images. To further enhance the fine-grained visual perceptual ability of the MLLM, we design a Multi-Level Cross-Attention (MLCA) mechanism that captures the hierarchical structure and semantic relations of document images. Furthermore, we create a new instruction-tuning dataset for document-oriented tasks by enriching the multimodal document data with Gemini Pro. We conduct extensive experiments on both general and document-oriented MLLM benchmarks, and show that TextHawk outperforms the state-of-the-art methods, demonstrating its effectiveness and superiority in fine-grained document perception and general abilities.","sentences":["Multimodal Large Language Models (MLLMs) have shown impressive results on various multimodal tasks.","However, most existing MLLMs are not well suited for document-oriented tasks, which require fine-grained image perception and information compression.","In this paper, we present TextHawk, a MLLM that is specifically designed for document-oriented tasks, while preserving the general capabilities of MLLMs.","TextHawk is aimed to explore efficient fine-grained perception by designing four dedicated components.","Firstly, a ReSampling and ReArrangement (ReSA) module is proposed to reduce the redundancy in the document texts and lower the computational cost of the MLLM.","We explore encoding the positions of each local feature by presenting Scalable Positional Embeddings (SPEs), which can preserve the scalability of various image sizes.","A Query Proposal Network (QPN) is then adopted to initialize the queries dynamically among different sub-images.","To further enhance the fine-grained visual perceptual ability of the MLLM, we design a Multi-Level Cross-Attention (MLCA) mechanism that captures the hierarchical structure and semantic relations of document images.","Furthermore, we create a new instruction-tuning dataset for document-oriented tasks by enriching the multimodal document data with Gemini Pro.","We conduct extensive experiments on both general and document-oriented MLLM benchmarks, and show that TextHawk outperforms the state-of-the-art methods, demonstrating its effectiveness and superiority in fine-grained document perception and general abilities."],"url":"http://arxiv.org/abs/2404.09204v1","category":"cs.CV"}
{"created":"2024-04-14 09:44:08","title":"Advanced Intelligent Optimization Algorithms for Multi-Objective Optimal Power Flow in Future Power Systems: A Review","abstract":"This review explores the application of intelligent optimization algorithms to Multi-Objective Optimal Power Flow (MOPF) in enhancing modern power systems. It delves into the challenges posed by the integration of renewables, smart grids, and increasing energy demands, focusing on evolutionary algorithms, swarm intelligence, and deep reinforcement learning. The effectiveness, scalability, and application of these algorithms are analyzed, with findings suggesting that algorithm selection is contingent on the specific MOPF problem at hand, and hybrid approaches offer significant promise. The importance of standard test systems for verifying solutions and the role of software tools in facilitating analysis are emphasized. Future research is directed towards exploiting machine learning for dynamic optimization, embracing decentralized energy systems, and adapting to evolving policy frameworks to improve power system efficiency and sustainability. This review aims to advance MOPF research by highlighting state-of-the-art methodologies and encouraging the development of innovative solutions for future energy challenges.","sentences":["This review explores the application of intelligent optimization algorithms to Multi-Objective Optimal Power Flow (MOPF) in enhancing modern power systems.","It delves into the challenges posed by the integration of renewables, smart grids, and increasing energy demands, focusing on evolutionary algorithms, swarm intelligence, and deep reinforcement learning.","The effectiveness, scalability, and application of these algorithms are analyzed, with findings suggesting that algorithm selection is contingent on the specific MOPF problem at hand, and hybrid approaches offer significant promise.","The importance of standard test systems for verifying solutions and the role of software tools in facilitating analysis are emphasized.","Future research is directed towards exploiting machine learning for dynamic optimization, embracing decentralized energy systems, and adapting to evolving policy frameworks to improve power system efficiency and sustainability.","This review aims to advance MOPF research by highlighting state-of-the-art methodologies and encouraging the development of innovative solutions for future energy challenges."],"url":"http://arxiv.org/abs/2404.09203v1","category":"cs.NE"}
{"created":"2024-04-14 08:56:19","title":"Prior-agnostic Multi-scale Contrastive Text-Audio Pre-training for Parallelized TTS Frontend Modeling","abstract":"Over the past decade, a series of unflagging efforts have been dedicated to developing highly expressive and controllable text-to-speech (TTS) systems. In general, the holistic TTS comprises two interconnected components: the frontend module and the backend module. The frontend excels in capturing linguistic representations from the raw text input, while the backend module converts linguistic cues to speech. The research community has shown growing interest in the study of the frontend component, recognizing its pivotal role in text-to-speech systems, including Text Normalization (TN), Prosody Boundary Prediction (PBP), and Polyphone Disambiguation (PD). Nonetheless, the limitations posed by insufficient annotated textual data and the reliance on homogeneous text signals significantly undermine the effectiveness of its supervised learning. To evade this obstacle, a novel two-stage TTS frontend prediction pipeline, named TAP-FM, is proposed in this paper. Specifically, during the first learning phase, we present a Multi-scale Contrastive Text-audio Pre-training protocol (MC-TAP), which hammers at acquiring richer insights via multi-granularity contrastive pre-training in an unsupervised manner. Instead of mining homogeneous features in prior pre-training approaches, our framework demonstrates the ability to delve deep into both global and local text-audio semantic and acoustic representations. Furthermore, a parallelized TTS frontend model is delicately devised to execute TN, PD, and PBP prediction tasks, respectively in the second stage. Finally, extensive experiments illustrate the superiority of our proposed method, achieving state-of-the-art performance.","sentences":["Over the past decade, a series of unflagging efforts have been dedicated to developing highly expressive and controllable text-to-speech (TTS) systems.","In general, the holistic TTS comprises two interconnected components: the frontend module and the backend module.","The frontend excels in capturing linguistic representations from the raw text input, while the backend module converts linguistic cues to speech.","The research community has shown growing interest in the study of the frontend component, recognizing its pivotal role in text-to-speech systems, including Text Normalization (TN), Prosody Boundary Prediction (PBP), and Polyphone Disambiguation (PD).","Nonetheless, the limitations posed by insufficient annotated textual data and the reliance on homogeneous text signals significantly undermine the effectiveness of its supervised learning.","To evade this obstacle, a novel two-stage TTS frontend prediction pipeline, named TAP-FM, is proposed in this paper.","Specifically, during the first learning phase, we present a Multi-scale Contrastive Text-audio Pre-training protocol (MC-TAP), which hammers at acquiring richer insights via multi-granularity contrastive pre-training in an unsupervised manner.","Instead of mining homogeneous features in prior pre-training approaches, our framework demonstrates the ability to delve deep into both global and local text-audio semantic and acoustic representations.","Furthermore, a parallelized TTS frontend model is delicately devised to execute TN, PD, and PBP prediction tasks, respectively in the second stage.","Finally, extensive experiments illustrate the superiority of our proposed method, achieving state-of-the-art performance."],"url":"http://arxiv.org/abs/2404.09192v1","category":"cs.SD"}
{"created":"2024-04-14 08:09:33","title":"Change Guiding Network: Incorporating Change Prior to Guide Change Detection in Remote Sensing Imagery","abstract":"The rapid advancement of automated artificial intelligence algorithms and remote sensing instruments has benefited change detection (CD) tasks. However, there is still a lot of space to study for precise detection, especially the edge integrity and internal holes phenomenon of change features. In order to solve these problems, we design the Change Guiding Network (CGNet), to tackle the insufficient expression problem of change features in the conventional U-Net structure adopted in previous methods, which causes inaccurate edge detection and internal holes. Change maps from deep features with rich semantic information are generated and used as prior information to guide multi-scale feature fusion, which can improve the expression ability of change features. Meanwhile, we propose a self-attention module named Change Guide Module (CGM), which can effectively capture the long-distance dependency among pixels and effectively overcome the problem of the insufficient receptive field of traditional convolutional neural networks. On four major CD datasets, we verify the usefulness and efficiency of the CGNet, and a large number of experiments and ablation studies demonstrate the effectiveness of CGNet. We're going to open-source our code at https://github.com/ChengxiHAN/CGNet-CD.","sentences":["The rapid advancement of automated artificial intelligence algorithms and remote sensing instruments has benefited change detection (CD) tasks.","However, there is still a lot of space to study for precise detection, especially the edge integrity and internal holes phenomenon of change features.","In order to solve these problems, we design the Change Guiding Network (CGNet), to tackle the insufficient expression problem of change features in the conventional U-Net structure adopted in previous methods, which causes inaccurate edge detection and internal holes.","Change maps from deep features with rich semantic information are generated and used as prior information to guide multi-scale feature fusion, which can improve the expression ability of change features.","Meanwhile, we propose a self-attention module named Change Guide Module (CGM), which can effectively capture the long-distance dependency among pixels and effectively overcome the problem of the insufficient receptive field of traditional convolutional neural networks.","On four major CD datasets, we verify the usefulness and efficiency of the CGNet, and a large number of experiments and ablation studies demonstrate the effectiveness of CGNet.","We're going to open-source our code at https://github.com/ChengxiHAN/CGNet-CD."],"url":"http://arxiv.org/abs/2404.09179v1","category":"cs.CV"}
{"created":"2024-04-14 07:43:45","title":"TransformerFAM: Feedback attention is working memory","abstract":"While Transformers have revolutionized deep learning, their quadratic attention complexity hinders their ability to process infinitely long inputs. We propose Feedback Attention Memory (FAM), a novel Transformer architecture that leverages a feedback loop to enable the network to attend to its own latent representations. This design fosters the emergence of working memory within the Transformer, allowing it to process indefinitely long sequences. TransformerFAM requires no additional weights, enabling seamless integration with pre-trained models. Our experiments show that TransformerFAM significantly improves Transformer performance on long-context tasks across various model sizes (1B, 8B, and 24B). These results showcase the potential to empower Large Language Models (LLMs) to process sequences of unlimited length.","sentences":["While Transformers have revolutionized deep learning, their quadratic attention complexity hinders their ability to process infinitely long inputs.","We propose Feedback Attention Memory (FAM), a novel Transformer architecture that leverages a feedback loop to enable the network to attend to its own latent representations.","This design fosters the emergence of working memory within the Transformer, allowing it to process indefinitely long sequences.","TransformerFAM requires no additional weights, enabling seamless integration with pre-trained models.","Our experiments show that TransformerFAM significantly improves Transformer performance on long-context tasks across various model sizes (1B, 8B, and 24B).","These results showcase the potential to empower Large Language Models (LLMs) to process sequences of unlimited length."],"url":"http://arxiv.org/abs/2404.09173v1","category":"cs.LG"}
{"created":"2024-04-14 07:36:18","title":"LoopAnimate: Loopable Salient Object Animation","abstract":"Research on diffusion model-based video generation has advanced rapidly. However, limitations in object fidelity and generation length hinder its practical applications. Additionally, specific domains like animated wallpapers require seamless looping, where the first and last frames of the video match seamlessly. To address these challenges, this paper proposes LoopAnimate, a novel method for generating videos with consistent start and end frames. To enhance object fidelity, we introduce a framework that decouples multi-level image appearance and textual semantic information. Building upon an image-to-image diffusion model, our approach incorporates both pixel-level and feature-level information from the input image, injecting image appearance and textual semantic embeddings at different positions of the diffusion model. Existing UNet-based video generation models require to input the entire videos during training to encode temporal and positional information at once. However, due to limitations in GPU memory, the number of frames is typically restricted to 16. To address this, this paper proposes a three-stage training strategy with progressively increasing frame numbers and reducing fine-tuning modules. Additionally, we introduce the Temporal E nhanced Motion Module(TEMM) to extend the capacity for encoding temporal and positional information up to 36 frames. The proposed LoopAnimate, which for the first time extends the single-pass generation length of UNet-based video generation models to 35 frames while maintaining high-quality video generation. Experiments demonstrate that LoopAnimate achieves state-of-the-art performance in both objective metrics, such as fidelity and temporal consistency, and subjective evaluation results.","sentences":["Research on diffusion model-based video generation has advanced rapidly.","However, limitations in object fidelity and generation length hinder its practical applications.","Additionally, specific domains like animated wallpapers require seamless looping, where the first and last frames of the video match seamlessly.","To address these challenges, this paper proposes LoopAnimate, a novel method for generating videos with consistent start and end frames.","To enhance object fidelity, we introduce a framework that decouples multi-level image appearance and textual semantic information.","Building upon an image-to-image diffusion model, our approach incorporates both pixel-level and feature-level information from the input image, injecting image appearance and textual semantic embeddings at different positions of the diffusion model.","Existing UNet-based video generation models require to input the entire videos during training to encode temporal and positional information at once.","However, due to limitations in GPU memory, the number of frames is typically restricted to 16.","To address this, this paper proposes a three-stage training strategy with progressively increasing frame numbers and reducing fine-tuning modules.","Additionally, we introduce the Temporal E nhanced Motion Module(TEMM) to extend the capacity for encoding temporal and positional information up to 36 frames.","The proposed LoopAnimate, which for the first time extends the single-pass generation length of UNet-based video generation models to 35 frames while maintaining high-quality video generation.","Experiments demonstrate that LoopAnimate achieves state-of-the-art performance in both objective metrics, such as fidelity and temporal consistency, and subjective evaluation results."],"url":"http://arxiv.org/abs/2404.09172v1","category":"cs.CV"}
{"created":"2024-04-14 07:15:59","title":"Survey on Embedding Models for Knowledge Graph and its Applications","abstract":"Knowledge Graph (KG) is a graph based data structure to represent facts of the world where nodes represent real world entities or abstract concept and edges represent relation between the entities. Graph as representation for knowledge has several drawbacks like data sparsity, computational complexity and manual feature engineering. Knowledge Graph embedding tackles the drawback by representing entities and relation in low dimensional vector space by capturing the semantic relation between them. There are different KG embedding models. Here, we discuss translation based and neural network based embedding models which differ based on semantic property, scoring function and architecture they use. Further, we discuss application of KG in some domains that use deep learning models and leverage social media data.","sentences":["Knowledge Graph (KG) is a graph based data structure to represent facts of the world where nodes represent real world entities or abstract concept and edges represent relation between the entities.","Graph as representation for knowledge has several drawbacks like data sparsity, computational complexity and manual feature engineering.","Knowledge Graph embedding tackles the drawback by representing entities and relation in low dimensional vector space by capturing the semantic relation between them.","There are different KG embedding models.","Here, we discuss translation based and neural network based embedding models which differ based on semantic property, scoring function and architecture they use.","Further, we discuss application of KG in some domains that use deep learning models and leverage social media data."],"url":"http://arxiv.org/abs/2404.09167v1","category":"cs.SI"}
{"created":"2024-04-14 06:55:42","title":"GeMQuAD : Generating Multilingual Question Answering Datasets from Large Language Models using Few Shot Learning","abstract":"The emergence of Large Language Models (LLMs) with capabilities like In-Context Learning (ICL) has ushered in new possibilities for data generation across various domains while minimizing the need for extensive data collection and modeling techniques. Researchers have explored ways to use this generated synthetic data to optimize smaller student models for reduced deployment costs and lower latency in downstream tasks. However, ICL-generated data often suffers from low quality as the task specificity is limited with few examples used in ICL. In this paper, we propose GeMQuAD - a semi-supervised learning approach, extending the WeakDAP framework, applied to a dataset generated through ICL with just one example in the target language using AlexaTM 20B Seq2Seq LLM. Through our approach, we iteratively identify high-quality data to enhance model performance, especially for low-resource multilingual setting in the context of Extractive Question Answering task. Our framework outperforms the machine translation-augmented model by 0.22/1.68 F1/EM (Exact Match) points for Hindi and 0.82/1.37 F1/EM points for Spanish on the MLQA dataset, and it surpasses the performance of model trained on an English-only dataset by 5.05/6.50 F1/EM points for Hindi and 3.81/3.69 points F1/EM for Spanish on the same dataset. Notably, our approach uses a pre-trained LLM for generation with no fine-tuning (FT), utilizing just a single annotated example in ICL to generate data, providing a cost-effective development process.","sentences":["The emergence of Large Language Models (LLMs) with capabilities like In-Context Learning (ICL) has ushered in new possibilities for data generation across various domains while minimizing the need for extensive data collection and modeling techniques.","Researchers have explored ways to use this generated synthetic data to optimize smaller student models for reduced deployment costs and lower latency in downstream tasks.","However, ICL-generated data often suffers from low quality as the task specificity is limited with few examples used in ICL.","In this paper, we propose GeMQuAD - a semi-supervised learning approach, extending the WeakDAP framework, applied to a dataset generated through ICL with just one example in the target language using AlexaTM 20B Seq2Seq LLM.","Through our approach, we iteratively identify high-quality data to enhance model performance, especially for low-resource multilingual setting in the context of Extractive Question Answering task.","Our framework outperforms the machine translation-augmented model by 0.22/1.68 F1/EM (Exact Match) points for Hindi and 0.82/1.37 F1/EM points for Spanish on the MLQA dataset, and it surpasses the performance of model trained on an English-only dataset by 5.05/6.50 F1/EM points for Hindi and 3.81/3.69 points F1/EM for Spanish on the same dataset.","Notably, our approach uses a pre-trained LLM for generation with no fine-tuning (FT), utilizing just a single annotated example in ICL to generate data, providing a cost-effective development process."],"url":"http://arxiv.org/abs/2404.09163v1","category":"cs.CL"}
{"created":"2024-04-14 06:19:46","title":"StreakNet-Arch: An Anti-scattering Network-based Architecture for Underwater Carrier LiDAR-Radar Imaging","abstract":"In this paper, we introduce StreakNet-Arch, a novel signal processing architecture designed for Underwater Carrier LiDAR-Radar (UCLR) imaging systems, to address the limitations in scatter suppression and real-time imaging. StreakNet-Arch formulates the signal processing as a real-time, end-to-end binary classification task, enabling real-time image acquisition. To achieve this, we leverage Self-Attention networks and propose a novel Double Branch Cross Attention (DBC-Attention) mechanism that surpasses the performance of traditional methods. Furthermore, we present a method for embedding streak-tube camera images into attention networks, effectively acting as a learned bandpass filter. To facilitate further research, we contribute a publicly available streak-tube camera image dataset. The dataset contains 2,695,168 real-world underwater 3D point cloud data. These advancements significantly improve UCLR capabilities, enhancing its performance and applicability in underwater imaging tasks. The source code and dataset can be found at https://github.com/BestAnHongjun/StreakNet .","sentences":["In this paper, we introduce StreakNet-Arch, a novel signal processing architecture designed for Underwater Carrier LiDAR-Radar (UCLR) imaging systems, to address the limitations in scatter suppression and real-time imaging.","StreakNet-Arch formulates the signal processing as a real-time, end-to-end binary classification task, enabling real-time image acquisition.","To achieve this, we leverage Self-Attention networks and propose a novel Double Branch Cross Attention (DBC-Attention) mechanism that surpasses the performance of traditional methods.","Furthermore, we present a method for embedding streak-tube camera images into attention networks, effectively acting as a learned bandpass filter.","To facilitate further research, we contribute a publicly available streak-tube camera image dataset.","The dataset contains 2,695,168 real-world underwater 3D point cloud data.","These advancements significantly improve UCLR capabilities, enhancing its performance and applicability in underwater imaging tasks.","The source code and dataset can be found at https://github.com/BestAnHongjun/StreakNet ."],"url":"http://arxiv.org/abs/2404.09158v1","category":"cs.CV"}
{"created":"2024-04-14 06:10:46","title":"Mitigating Heterogeneity among Factor Tensors via Lie Group Manifolds for Tensor Decomposition Based Temporal Knowledge Graph Embedding","abstract":"Recent studies have highlighted the effectiveness of tensor decomposition methods in the Temporal Knowledge Graphs Embedding (TKGE) task. However, we found that inherent heterogeneity among factor tensors in tensor decomposition significantly hinders the tensor fusion process and further limits the performance of link prediction. To overcome this limitation, we introduce a novel method that maps factor tensors onto a unified smooth Lie group manifold to make the distribution of factor tensors approximating homogeneous in tensor decomposition. We provide the theoretical proof of our motivation that homogeneous tensors are more effective than heterogeneous tensors in tensor fusion and approximating the target for tensor decomposition based TKGE methods. The proposed method can be directly integrated into existing tensor decomposition based TKGE methods without introducing extra parameters. Extensive experiments demonstrate the effectiveness of our method in mitigating the heterogeneity and in enhancing the tensor decomposition based TKGE models.","sentences":["Recent studies have highlighted the effectiveness of tensor decomposition methods in the Temporal Knowledge Graphs Embedding (TKGE) task.","However, we found that inherent heterogeneity among factor tensors in tensor decomposition significantly hinders the tensor fusion process and further limits the performance of link prediction.","To overcome this limitation, we introduce a novel method that maps factor tensors onto a unified smooth Lie group manifold to make the distribution of factor tensors approximating homogeneous in tensor decomposition.","We provide the theoretical proof of our motivation that homogeneous tensors are more effective than heterogeneous tensors in tensor fusion and approximating the target for tensor decomposition based TKGE methods.","The proposed method can be directly integrated into existing tensor decomposition based TKGE methods without introducing extra parameters.","Extensive experiments demonstrate the effectiveness of our method in mitigating the heterogeneity and in enhancing the tensor decomposition based TKGE models."],"url":"http://arxiv.org/abs/2404.09155v1","category":"cs.LG"}
{"created":"2024-04-14 05:45:41","title":"Heuristic Solution to Joint Deployment and Beamforming Design for STAR-RIS Aided Networks","abstract":"This paper tackles the deployment challenges of Simultaneous Transmitting and Reflecting Reconfigurable Intelligent Surface (STAR-RIS) in communication systems. Unlike existing works that use fixed deployment setups or solely optimize the location, this paper emphasizes the joint optimization of the location and orientation of STAR-RIS. This enables searching across all user grouping possibilities and fully boosting the system's performance. We consider a sum rate maximization problem with joint optimization and hybrid beamforming design. An offline heuristic solution is proposed for the problem, developed based on differential evolution and semi-definite programming methods. In particular, a point-point representation is proposed for characterizing and exploiting the user-grouping. A balanced grouping method is designed to achieve a desired user grouping with low complexity. Numerical results demonstrate the substantial performance gains achievable through optimal deployment design.","sentences":["This paper tackles the deployment challenges of Simultaneous Transmitting and Reflecting Reconfigurable Intelligent Surface (STAR-RIS) in communication systems.","Unlike existing works that use fixed deployment setups or solely optimize the location, this paper emphasizes the joint optimization of the location and orientation of STAR-RIS.","This enables searching across all user grouping possibilities and fully boosting the system's performance.","We consider a sum rate maximization problem with joint optimization and hybrid beamforming design.","An offline heuristic solution is proposed for the problem, developed based on differential evolution and semi-definite programming methods.","In particular, a point-point representation is proposed for characterizing and exploiting the user-grouping.","A balanced grouping method is designed to achieve a desired user grouping with low complexity.","Numerical results demonstrate the substantial performance gains achievable through optimal deployment design."],"url":"http://arxiv.org/abs/2404.09149v1","category":"eess.SY"}
{"created":"2024-04-14 05:33:26","title":"Evaluating the efficacy of haptic feedback, 360\u00b0 treadmill-integrated Virtual Reality framework and longitudinal training on decision-making performance in a complex search-and-shoot simulation","abstract":"Virtual Reality (VR) has made significant strides, offering users a multitude of ways to interact with virtual environments. Each sensory modality in VR provides distinct inputs and interactions, enhancing the user's immersion and presence. However, the potential of additional sensory modalities, such as haptic feedback and 360{\\deg} locomotion, to improve decision-making performance has not been thoroughly investigated. This study addresses this gap by evaluating the impact of a haptic feedback, 360{\\deg} locomotion-integrated VR framework and longitudinal, heterogeneous training on decision-making performance in a complex search-and-shoot simulation. The study involved 32 participants from a defence simulation base in India, who were randomly divided into two groups: experimental (haptic feedback, 360{\\deg} locomotion-integrated VR framework with longitudinal, heterogeneous training) and placebo control (longitudinal, heterogeneous VR training without extrasensory modalities). The experiment lasted 10 days. On Day 1, all subjects executed a search-and-shoot simulation closely replicating the elements/situations in the real world. From Day 2 to Day 9, the subjects underwent heterogeneous training, imparted by the design of various complexity levels in the simulation using changes in behavioral attributes/artificial intelligence of the enemies. On Day 10, they repeated the search-and-shoot simulation executed on Day 1. The results showed that the experimental group experienced a gradual increase in presence, immersion, and engagement compared to the placebo control group. However, there was no significant difference in decision-making performance between the two groups on day 10. We intend to use these findings to design multisensory VR training frameworks that enhance engagement levels and decision-making performance.","sentences":["Virtual Reality (VR) has made significant strides, offering users a multitude of ways to interact with virtual environments.","Each sensory modality in VR provides distinct inputs and interactions, enhancing the user's immersion and presence.","However, the potential of additional sensory modalities, such as haptic feedback and 360{\\deg} locomotion, to improve decision-making performance has not been thoroughly investigated.","This study addresses this gap by evaluating the impact of a haptic feedback, 360{\\deg} locomotion-integrated VR framework and longitudinal, heterogeneous training on decision-making performance in a complex search-and-shoot simulation.","The study involved 32 participants from a defence simulation base in India, who were randomly divided into two groups: experimental (haptic feedback, 360{\\deg} locomotion-integrated VR framework with longitudinal, heterogeneous training) and placebo control (longitudinal, heterogeneous VR training without extrasensory modalities).","The experiment lasted 10 days.","On Day 1, all subjects executed a search-and-shoot simulation closely replicating the elements/situations in the real world.","From Day 2 to Day 9, the subjects underwent heterogeneous training, imparted by the design of various complexity levels in the simulation using changes in behavioral attributes/artificial intelligence of the enemies.","On Day 10, they repeated the search-and-shoot simulation executed on Day 1.","The results showed that the experimental group experienced a gradual increase in presence, immersion, and engagement compared to the placebo control group.","However, there was no significant difference in decision-making performance between the two groups on day 10.","We intend to use these findings to design multisensory VR training frameworks that enhance engagement levels and decision-making performance."],"url":"http://arxiv.org/abs/2404.09147v1","category":"cs.HC"}
{"created":"2024-04-14 05:28:46","title":"Fusion-Mamba for Cross-modality Object Detection","abstract":"Cross-modality fusing complementary information from different modalities effectively improves object detection performance, making it more useful and robust for a wider range of applications. Existing fusion strategies combine different types of images or merge different backbone features through elaborated neural network modules. However, these methods neglect that modality disparities affect cross-modality fusion performance, as different modalities with different camera focal lengths, placements, and angles are hardly fused. In this paper, we investigate cross-modality fusion by associating cross-modal features in a hidden state space based on an improved Mamba with a gating mechanism. We design a Fusion-Mamba block (FMB) to map cross-modal features into a hidden state space for interaction, thereby reducing disparities between cross-modal features and enhancing the representation consistency of fused features. FMB contains two modules: the State Space Channel Swapping (SSCS) module facilitates shallow feature fusion, and the Dual State Space Fusion (DSSF) enables deep fusion in a hidden state space. Through extensive experiments on public datasets, our proposed approach outperforms the state-of-the-art methods on $m$AP with 5.9% on $M^3FD$ and 4.9% on FLIR-Aligned datasets, demonstrating superior object detection performance. To the best of our knowledge, this is the first work to explore the potential of Mamba for cross-modal fusion and establish a new baseline for cross-modality object detection.","sentences":["Cross-modality fusing complementary information from different modalities effectively improves object detection performance, making it more useful and robust for a wider range of applications.","Existing fusion strategies combine different types of images or merge different backbone features through elaborated neural network modules.","However, these methods neglect that modality disparities affect cross-modality fusion performance, as different modalities with different camera focal lengths, placements, and angles are hardly fused.","In this paper, we investigate cross-modality fusion by associating cross-modal features in a hidden state space based on an improved Mamba with a gating mechanism.","We design a Fusion-Mamba block (FMB) to map cross-modal features into a hidden state space for interaction, thereby reducing disparities between cross-modal features and enhancing the representation consistency of fused features.","FMB contains two modules: the State Space Channel Swapping (SSCS) module facilitates shallow feature fusion, and the Dual State Space Fusion (DSSF) enables deep fusion in a hidden state space.","Through extensive experiments on public datasets, our proposed approach outperforms the state-of-the-art methods on $m$AP with 5.9% on $M^3FD$ and 4.9% on FLIR-Aligned datasets, demonstrating superior object detection performance.","To the best of our knowledge, this is the first work to explore the potential of Mamba for cross-modal fusion and establish a new baseline for cross-modality object detection."],"url":"http://arxiv.org/abs/2404.09146v1","category":"cs.CV"}
{"created":"2024-04-14 05:13:37","title":"ToNER: Type-oriented Named Entity Recognition with Generative Language Model","abstract":"In recent years, the fine-tuned generative models have been proven more powerful than the previous tagging-based or span-based models on named entity recognition (NER) task. It has also been found that the information related to entities, such as entity types, can prompt a model to achieve NER better. However, it is not easy to determine the entity types indeed existing in the given sentence in advance, and inputting too many potential entity types would distract the model inevitably. To exploit entity types' merit on promoting NER task, in this paper we propose a novel NER framework, namely ToNER based on a generative model. In ToNER, a type matching model is proposed at first to identify the entity types most likely to appear in the sentence. Then, we append a multiple binary classification task to fine-tune the generative model's encoder, so as to generate the refined representation of the input sentence. Moreover, we add an auxiliary task for the model to discover the entity types which further fine-tunes the model to output more accurate results. Our extensive experiments on some NER benchmarks verify the effectiveness of our proposed strategies in ToNER that are oriented towards entity types' exploitation.","sentences":["In recent years, the fine-tuned generative models have been proven more powerful than the previous tagging-based or span-based models on named entity recognition (NER) task.","It has also been found that the information related to entities, such as entity types, can prompt a model to achieve NER better.","However, it is not easy to determine the entity types indeed existing in the given sentence in advance, and inputting too many potential entity types would distract the model inevitably.","To exploit entity types' merit on promoting NER task, in this paper we propose a novel NER framework, namely ToNER based on a generative model.","In ToNER, a type matching model is proposed at first to identify the entity types most likely to appear in the sentence.","Then, we append a multiple binary classification task to fine-tune the generative model's encoder, so as to generate the refined representation of the input sentence.","Moreover, we add an auxiliary task for the model to discover the entity types which further fine-tunes the model to output more accurate results.","Our extensive experiments on some NER benchmarks verify the effectiveness of our proposed strategies in ToNER that are oriented towards entity types' exploitation."],"url":"http://arxiv.org/abs/2404.09145v1","category":"cs.CL"}
{"created":"2024-04-14 04:25:41","title":"From Bytes to Borsch: Fine-Tuning Gemma and Mistral for the Ukrainian Language Representation","abstract":"In the rapidly advancing field of AI and NLP, generative large language models (LLMs) stand at the forefront of innovation, showcasing unparalleled abilities in text understanding and generation. However, the limited representation of low-resource languages like Ukrainian poses a notable challenge, restricting the reach and relevance of this technology. Our paper addresses this by fine-tuning the open-source Gemma and Mistral LLMs with Ukrainian datasets, aiming to improve their linguistic proficiency and benchmarking them against other existing models capable of processing Ukrainian language. This endeavor not only aims to mitigate language bias in technology but also promotes inclusivity in the digital realm. Our transparent and reproducible approach encourages further NLP research and development. Additionally, we present the Ukrainian Knowledge and Instruction Dataset (UKID) to aid future efforts in language model fine-tuning. Our research not only advances the field of NLP but also highlights the importance of linguistic diversity in AI, which is crucial for cultural preservation, education, and expanding AI's global utility. Ultimately, we advocate for a future where technology is inclusive, enabling AI to communicate effectively across all languages, especially those currently underrepresented.","sentences":["In the rapidly advancing field of AI and NLP, generative large language models (LLMs) stand at the forefront of innovation, showcasing unparalleled abilities in text understanding and generation.","However, the limited representation of low-resource languages like Ukrainian poses a notable challenge, restricting the reach and relevance of this technology.","Our paper addresses this by fine-tuning the open-source Gemma and Mistral LLMs with Ukrainian datasets, aiming to improve their linguistic proficiency and benchmarking them against other existing models capable of processing Ukrainian language.","This endeavor not only aims to mitigate language bias in technology but also promotes inclusivity in the digital realm.","Our transparent and reproducible approach encourages further NLP research and development.","Additionally, we present the Ukrainian Knowledge and Instruction Dataset (UKID) to aid future efforts in language model fine-tuning.","Our research not only advances the field of NLP but also highlights the importance of linguistic diversity in AI, which is crucial for cultural preservation, education, and expanding AI's global utility.","Ultimately, we advocate for a future where technology is inclusive, enabling AI to communicate effectively across all languages, especially those currently underrepresented."],"url":"http://arxiv.org/abs/2404.09138v1","category":"cs.CL"}
{"created":"2024-04-14 04:14:30","title":"TLDR at SemEval-2024 Task 2: T5-generated clinical-Language summaries for DeBERTa Report Analysis","abstract":"This paper introduces novel methodologies for the Natural Language Inference for Clinical Trials (NLI4CT) task. We present TLDR (T5-generated clinical-Language summaries for DeBERTa Report Analysis) which incorporates T5-model generated premise summaries for improved entailment and contradiction analysis in clinical NLI tasks. This approach overcomes the challenges posed by small context windows and lengthy premises, leading to a substantial improvement in Macro F1 scores: a 0.184 increase over truncated premises. Our comprehensive experimental evaluation, including detailed error analysis and ablations, confirms the superiority of TLDR in achieving consistency and faithfulness in predictions against semantically altered inputs.","sentences":["This paper introduces novel methodologies for the Natural Language Inference for Clinical Trials (NLI4CT) task.","We present TLDR (T5-generated clinical-Language summaries for DeBERTa Report Analysis) which incorporates T5-model generated premise summaries for improved entailment and contradiction analysis in clinical NLI tasks.","This approach overcomes the challenges posed by small context windows and lengthy premises, leading to a substantial improvement in Macro F1 scores: a 0.184 increase over truncated premises.","Our comprehensive experimental evaluation, including detailed error analysis and ablations, confirms the superiority of TLDR in achieving consistency and faithfulness in predictions against semantically altered inputs."],"url":"http://arxiv.org/abs/2404.09136v1","category":"cs.CL"}
{"created":"2024-04-14 03:44:54","title":"Interactive Generative AI Agents for Satellite Networks through a Mixture of Experts Transmission","abstract":"In response to the needs of 6G global communications, satellite communication networks have emerged as a key solution. However, the large-scale development of satellite communication networks is constrained by the complex system models, whose modeling is challenging for massive users. Moreover, transmission interference between satellites and users seriously affects communication performance. To solve these problems, this paper develops generative artificial intelligence (AI) agents for model formulation and then applies a mixture of experts (MoE) approach to design transmission strategies. Specifically, we leverage large language models (LLMs) to build an interactive modeling paradigm and utilize retrieval-augmented generation (RAG) to extract satellite expert knowledge that supports mathematical modeling. Afterward, by integrating the expertise of multiple specialized components, we propose an MoE-proximal policy optimization (PPO) approach to solve the formulated problem. Each expert can optimize the optimization variables at which it excels through specialized training through its own network and then aggregates them through the gating network to perform joint optimization. The simulation results validate the accuracy and effectiveness of employing a generative agent for problem formulation. Furthermore, the superiority of the proposed MoE-ppo approach over other benchmarks is confirmed in solving the formulated problem. The adaptability of MoE-PPO to various customized modeling problems has also been demonstrated.","sentences":["In response to the needs of 6G global communications, satellite communication networks have emerged as a key solution.","However, the large-scale development of satellite communication networks is constrained by the complex system models, whose modeling is challenging for massive users.","Moreover, transmission interference between satellites and users seriously affects communication performance.","To solve these problems, this paper develops generative artificial intelligence (AI) agents for model formulation and then applies a mixture of experts (MoE) approach to design transmission strategies.","Specifically, we leverage large language models (LLMs) to build an interactive modeling paradigm and utilize retrieval-augmented generation (RAG) to extract satellite expert knowledge that supports mathematical modeling.","Afterward, by integrating the expertise of multiple specialized components, we propose an MoE-proximal policy optimization (PPO) approach to solve the formulated problem.","Each expert can optimize the optimization variables at which it excels through specialized training through its own network and then aggregates them through the gating network to perform joint optimization.","The simulation results validate the accuracy and effectiveness of employing a generative agent for problem formulation.","Furthermore, the superiority of the proposed MoE-ppo approach over other benchmarks is confirmed in solving the formulated problem.","The adaptability of MoE-PPO to various customized modeling problems has also been demonstrated."],"url":"http://arxiv.org/abs/2404.09134v1","category":"cs.NI"}
{"created":"2024-04-14 02:40:43","title":"Confidence Calibration and Rationalization for LLMs via Multi-Agent Deliberation","abstract":"Uncertainty estimation is a significant issue for current large language models (LLMs) that are generally poorly calibrated and over-confident, especially with reinforcement learning from human feedback (RLHF). Unlike humans, whose decisions and confidences not only stem from intrinsic beliefs but can also be adjusted through daily observations, existing calibration methods for LLMs focus on estimating or eliciting individual confidence without taking full advantage of the \"Collective Wisdom\": the interaction among multiple LLMs that can collectively improve both accuracy and calibration. In this work, we propose Collaborative Calibration, a post-hoc training-free calibration strategy that leverages the collaborative and expressive capabilities of multiple tool-augmented LLM agents in a simulated group deliberation process. We demonstrate the effectiveness of Collaborative Calibration on generative QA tasks across various domains, showing its potential in harnessing the rationalization of collectively calibrated confidence assessments and improving the reliability of model predictions.","sentences":["Uncertainty estimation is a significant issue for current large language models (LLMs) that are generally poorly calibrated and over-confident, especially with reinforcement learning from human feedback (RLHF).","Unlike humans, whose decisions and confidences not only stem from intrinsic beliefs but can also be adjusted through daily observations, existing calibration methods for LLMs focus on estimating or eliciting individual confidence without taking full advantage of the \"Collective Wisdom\": the interaction among multiple LLMs that can collectively improve both accuracy and calibration.","In this work, we propose Collaborative Calibration, a post-hoc training-free calibration strategy that leverages the collaborative and expressive capabilities of multiple tool-augmented LLM agents in a simulated group deliberation process.","We demonstrate the effectiveness of Collaborative Calibration on generative QA tasks across various domains, showing its potential in harnessing the rationalization of collectively calibrated confidence assessments and improving the reliability of model predictions."],"url":"http://arxiv.org/abs/2404.09127v1","category":"cs.CL"}
{"created":"2024-04-14 02:18:07","title":"Provable Interactive Learning with Hindsight Instruction Feedback","abstract":"We study interactive learning in a setting where the agent has to generate a response (e.g., an action or trajectory) given a context and an instruction. In contrast, to typical approaches that train the system using reward or expert supervision on response, we study learning with hindsight instruction where a teacher provides an instruction that is most suitable for the agent's generated response. This hindsight labeling of instruction is often easier to provide than providing expert supervision of the optimal response which may require expert knowledge or can be impractical to elicit. We initiate the theoretical analysis of interactive learning with hindsight labeling. We first provide a lower bound showing that in general, the regret of any algorithm must scale with the size of the agent's response space. We then study a specialized setting where the underlying instruction-response distribution can be decomposed as a low-rank matrix. We introduce an algorithm called LORIL for this setting and show that its regret scales as $\\sqrt{T}$ where $T$ is the number of rounds and depends on the intrinsic rank but does not depend on the size of the agent's response space. We provide experiments in two domains showing that LORIL outperforms baselines even when the low-rank assumption is violated.","sentences":["We study interactive learning in a setting where the agent has to generate a response (e.g., an action or trajectory) given a context and an instruction.","In contrast, to typical approaches that train the system using reward or expert supervision on response, we study learning with hindsight instruction where a teacher provides an instruction that is most suitable for the agent's generated response.","This hindsight labeling of instruction is often easier to provide than providing expert supervision of the optimal response which may require expert knowledge or can be impractical to elicit.","We initiate the theoretical analysis of interactive learning with hindsight labeling.","We first provide a lower bound showing that in general, the regret of any algorithm must scale with the size of the agent's response space.","We then study a specialized setting where the underlying instruction-response distribution can be decomposed as a low-rank matrix.","We introduce an algorithm called LORIL for this setting and show that its regret scales as $\\sqrt{T}$ where $T$ is the number of rounds and depends on the intrinsic rank but does not depend on the size of the agent's response space.","We provide experiments in two domains showing that LORIL outperforms baselines even when the low-rank assumption is violated."],"url":"http://arxiv.org/abs/2404.09123v1","category":"cs.LG"}
{"created":"2024-04-14 01:44:58","title":"Intelligent Chemical Purification Technique Based on Machine Learning","abstract":"We present an innovative of artificial intelligence with column chromatography, aiming to resolve inefficiencies and standardize data collection in chemical separation and purification domain. By developing an automated platform for precise data acquisition and employing advanced machine learning algorithms, we constructed predictive models to forecast key separation parameters, thereby enhancing the efficiency and quality of chromatographic processes. The application of transfer learning allows the model to adapt across various column specifications, broadening its utility. A novel metric, separation probability ($S_p$), quantifies the likelihood of effective compound separation, validated through experimental verification. This study signifies a significant step forward int the application of AI in chemical research, offering a scalable solution to traditional chromatography challenges and providing a foundation for future technological advancements in chemical analysis and purification.","sentences":["We present an innovative of artificial intelligence with column chromatography, aiming to resolve inefficiencies and standardize data collection in chemical separation and purification domain.","By developing an automated platform for precise data acquisition and employing advanced machine learning algorithms, we constructed predictive models to forecast key separation parameters, thereby enhancing the efficiency and quality of chromatographic processes.","The application of transfer learning allows the model to adapt across various column specifications, broadening its utility.","A novel metric, separation probability ($S_p$), quantifies the likelihood of effective compound separation, validated through experimental verification.","This study signifies a significant step forward int the application of AI in chemical research, offering a scalable solution to traditional chromatography challenges and providing a foundation for future technological advancements in chemical analysis and purification."],"url":"http://arxiv.org/abs/2404.09114v1","category":"cs.LG"}
{"created":"2024-04-14 01:23:19","title":"Exploring Generative AI for Sim2Real in Driving Data Synthesis","abstract":"Datasets are essential for training and testing vehicle perception algorithms. However, the collection and annotation of real-world images is time-consuming and expensive. Driving simulators offer a solution by automatically generating various driving scenarios with corresponding annotations, but the simulation-to-reality (Sim2Real) domain gap remains a challenge. While most of the Generative Artificial Intelligence (AI) follows the de facto Generative Adversarial Nets (GANs)-based methods, the recent emerging diffusion probabilistic models have not been fully explored in mitigating Sim2Real challenges for driving data synthesis. To explore the performance, this paper applied three different generative AI methods to leverage semantic label maps from a driving simulator as a bridge for the creation of realistic datasets. A comparative analysis of these methods is presented from the perspective of image quality and perception. New synthetic datasets, which include driving images and auto-generated high-quality annotations, are produced with low costs and high scene variability. The experimental results show that although GAN-based methods are adept at generating high-quality images when provided with manually annotated labels, ControlNet produces synthetic datasets with fewer artefacts and more structural fidelity when using simulator-generated labels. This suggests that the diffusion-based approach may provide improved stability and an alternative method for addressing Sim2Real challenges.","sentences":["Datasets are essential for training and testing vehicle perception algorithms.","However, the collection and annotation of real-world images is time-consuming and expensive.","Driving simulators offer a solution by automatically generating various driving scenarios with corresponding annotations, but the simulation-to-reality (Sim2Real) domain gap remains a challenge.","While most of the Generative Artificial Intelligence (AI) follows the de facto Generative Adversarial Nets (GANs)-based methods, the recent emerging diffusion probabilistic models have not been fully explored in mitigating Sim2Real challenges for driving data synthesis.","To explore the performance, this paper applied three different generative AI methods to leverage semantic label maps from a driving simulator as a bridge for the creation of realistic datasets.","A comparative analysis of these methods is presented from the perspective of image quality and perception.","New synthetic datasets, which include driving images and auto-generated high-quality annotations, are produced with low costs and high scene variability.","The experimental results show that although GAN-based methods are adept at generating high-quality images when provided with manually annotated labels, ControlNet produces synthetic datasets with fewer artefacts and more structural fidelity when using simulator-generated labels.","This suggests that the diffusion-based approach may provide improved stability and an alternative method for addressing Sim2Real challenges."],"url":"http://arxiv.org/abs/2404.09111v1","category":"cs.CV"}
{"created":"2024-04-14 01:02:19","title":"ProSAS: An O-RAN Approach to Spectrum Sharing between NR and LTE","abstract":"The Open Radio Access Network (O-RAN), an industry-driven initiative, utilizes intelligent Radio Access Network (RAN) controllers and open interfaces to facilitate efficient spectrum sharing between LTE and NR RANs. In this paper, we introduce the Proactive Spectrum Adaptation Scheme (ProSAS), a data-driven, O-RAN-compatible spectrum sharing solution. ProSAS is an intelligent radio resource demand prediction and management scheme for intent-driven spectrum management that minimizes surplus or deficit experienced by both RANs. We illustrate the effectiveness of this solution using real-world LTE resource usage data and synthetically generated NR data. Lastly, we discuss a high-level O-RAN-compatible architecture of the proposed solution.","sentences":["The Open Radio Access Network (O-RAN), an industry-driven initiative, utilizes intelligent Radio Access Network (RAN) controllers and open interfaces to facilitate efficient spectrum sharing between LTE and NR RANs.","In this paper, we introduce the Proactive Spectrum Adaptation Scheme (ProSAS), a data-driven, O-RAN-compatible spectrum sharing solution.","ProSAS is an intelligent radio resource demand prediction and management scheme for intent-driven spectrum management that minimizes surplus or deficit experienced by both RANs.","We illustrate the effectiveness of this solution using real-world LTE resource usage data and synthetically generated NR data.","Lastly, we discuss a high-level O-RAN-compatible architecture of the proposed solution."],"url":"http://arxiv.org/abs/2404.09110v1","category":"cs.NI"}
{"created":"2024-04-14 00:08:56","title":"EGGS: Edge Guided Gaussian Splatting for Radiance Fields","abstract":"The Gaussian splatting methods are getting popular. However, their loss function only contains the $\\ell_1$ norm and the structural similarity between the rendered and input images, without considering the edges in these images. It is well-known that the edges in an image provide important information. Therefore, in this paper, we propose an Edge Guided Gaussian Splatting (EGGS) method that leverages the edges in the input images. More specifically, we give the edge region a higher weight than the flat region. With such edge guidance, the resulting Gaussian particles focus more on the edges instead of the flat regions. Moreover, such edge guidance does not crease the computation cost during the training and rendering stage. The experiments confirm that such simple edge-weighted loss function indeed improves about $1\\sim2$ dB on several difference data sets. With simply plugging in the edge guidance, the proposed method can improve all Gaussian splatting methods in different scenarios, such as human head modeling, building 3D reconstruction, etc.","sentences":["The Gaussian splatting methods are getting popular.","However, their loss function only contains the $\\ell_1$ norm and the structural similarity between the rendered and input images, without considering the edges in these images.","It is well-known that the edges in an image provide important information.","Therefore, in this paper, we propose an Edge Guided Gaussian Splatting (EGGS) method that leverages the edges in the input images.","More specifically, we give the edge region a higher weight than the flat region.","With such edge guidance, the resulting Gaussian particles focus more on the edges instead of the flat regions.","Moreover, such edge guidance does not crease the computation cost during the training and rendering stage.","The experiments confirm that such simple edge-weighted loss function indeed improves about $1\\sim2$ dB on several difference data sets.","With simply plugging in the edge guidance, the proposed method can improve all Gaussian splatting methods in different scenarios, such as human head modeling, building 3D reconstruction, etc."],"url":"http://arxiv.org/abs/2404.09105v1","category":"cs.CV"}
{"created":"2024-04-13 23:20:16","title":"Mixture of Experts Soften the Curse of Dimensionality in Operator Learning","abstract":"In this paper, we construct a mixture of neural operators (MoNOs) between function spaces whose complexity is distributed over a network of expert neural operators (NOs), with each NO satisfying parameter scaling restrictions. Our main result is a \\textit{distributed} universal approximation theorem guaranteeing that any Lipschitz non-linear operator between $L^2([0,1]^d)$ spaces can be approximated uniformly over the Sobolev unit ball therein, to any given $\\varepsilon>0$ accuracy, by an MoNO while satisfying the constraint that: each expert NO has a depth, width, and rank of $\\mathcal{O}(\\varepsilon^{-1})$. Naturally, our result implies that the required number of experts must be large, however, each NO is guaranteed to be small enough to be loadable into the active memory of most computers for reasonable accuracies $\\varepsilon$. During our analysis, we also obtain new quantitative expression rates for classical NOs approximating uniformly continuous non-linear operators uniformly on compact subsets of $L^2([0,1]^d)$.","sentences":["In this paper, we construct a mixture of neural operators (MoNOs) between function spaces whose complexity is distributed over a network of expert neural operators (NOs), with each NO satisfying parameter scaling restrictions.","Our main result is a \\textit{distributed} universal approximation theorem guaranteeing that any Lipschitz non-linear operator between $L^2([0,1]^d)$ spaces can be approximated uniformly over the Sobolev unit ball therein, to any given $\\varepsilon>0$ accuracy, by an MoNO while satisfying the constraint that: each expert NO has a depth, width, and rank of $\\mathcal{O}(\\varepsilon^{-1})$. Naturally, our result implies that the required number of experts must be large, however, each NO is guaranteed to be small enough to be loadable into the active memory of most computers for reasonable accuracies $\\varepsilon$. During our analysis, we also obtain new quantitative expression rates for classical NOs approximating uniformly continuous non-linear operators uniformly on compact subsets of $L^2([0,1]^d)$."],"url":"http://arxiv.org/abs/2404.09101v1","category":"cs.LG"}
{"created":"2024-04-13 22:37:16","title":"Learning Surface Terrain Classifications from Ground Penetrating Radar","abstract":"Terrain classification is an important problem for mobile robots operating in extreme environments as it can aid downstream tasks such as autonomous navigation and planning. While RGB cameras are widely used for terrain identification, vision-based methods can suffer due to poor lighting conditions and occlusions. In this paper, we propose the novel use of Ground Penetrating Radar (GPR) for terrain characterization for mobile robot platforms. Our approach leverages machine learning for surface terrain classification from GPR data. We collect a new dataset consisting of four different terrain types, and present qualitative and quantitative results. Our results demonstrate that classification networks can learn terrain categories from GPR signals. Additionally, we integrate our GPR-based classification approach into a multimodal semantic mapping framework to demonstrate a practical use case of GPR for surface terrain classification on mobile robots. Overall, this work extends the usability of GPR sensors deployed on robots to enable terrain classification in addition to GPR's existing scientific use cases.","sentences":["Terrain classification is an important problem for mobile robots operating in extreme environments as it can aid downstream tasks such as autonomous navigation and planning.","While RGB cameras are widely used for terrain identification, vision-based methods can suffer due to poor lighting conditions and occlusions.","In this paper, we propose the novel use of Ground Penetrating Radar (GPR) for terrain characterization for mobile robot platforms.","Our approach leverages machine learning for surface terrain classification from GPR data.","We collect a new dataset consisting of four different terrain types, and present qualitative and quantitative results.","Our results demonstrate that classification networks can learn terrain categories from GPR signals.","Additionally, we integrate our GPR-based classification approach into a multimodal semantic mapping framework to demonstrate a practical use case of GPR for surface terrain classification on mobile robots.","Overall, this work extends the usability of GPR sensors deployed on robots to enable terrain classification in addition to GPR's existing scientific use cases."],"url":"http://arxiv.org/abs/2404.09094v1","category":"cs.RO"}
{"created":"2024-04-13 22:18:14","title":"Semantic In-Domain Product Identification for Search Queries","abstract":"Accurate explicit and implicit product identification in search queries is critical for enhancing user experiences, especially at a company like Adobe which has over 50 products and covers queries across hundreds of tools. In this work, we present a novel approach to training a product classifier from user behavioral data. Our semantic model led to >25% relative improvement in CTR (click through rate) across the deployed surfaces; a >50% decrease in null rate; a 2x increase in the app cards surfaced, which helps drive product visibility.","sentences":["Accurate explicit and implicit product identification in search queries is critical for enhancing user experiences, especially at a company like Adobe which has over 50 products and covers queries across hundreds of tools.","In this work, we present a novel approach to training a product classifier from user behavioral data.","Our semantic model led to >25% relative improvement in CTR (click through rate) across the deployed surfaces; a >50% decrease in null rate; a 2x increase in the app cards surfaced, which helps drive product visibility."],"url":"http://arxiv.org/abs/2404.09091v1","category":"cs.IR"}
{"created":"2024-04-13 20:43:46","title":"CuriousLLM: Elevating Multi-Document QA with Reasoning-Infused Knowledge Graph Prompting","abstract":"In the field of Question Answering (QA), unifying large language models (LLMs) with external databases has shown great success. However, these methods often fall short in providing the advanced reasoning needed for complex QA tasks. To address these issues, we improve over a novel approach called Knowledge Graph Prompting (KGP), which combines knowledge graphs with a LLM-based agent to improve reasoning and search accuracy. Nevertheless, the original KGP framework necessitates costly fine-tuning with large datasets yet still suffers from LLM hallucination. Therefore, we propose a reasoning-infused LLM agent to enhance this framework. This agent mimics human curiosity to ask follow-up questions to more efficiently navigate the search. This simple modification significantly boosts the LLM performance in QA tasks without the high costs and latency associated with the initial KGP framework. Our ultimate goal is to further develop this approach, leading to more accurate, faster, and cost-effective solutions in the QA domain.","sentences":["In the field of Question Answering (QA), unifying large language models (LLMs) with external databases has shown great success.","However, these methods often fall short in providing the advanced reasoning needed for complex QA tasks.","To address these issues, we improve over a novel approach called Knowledge Graph Prompting (KGP), which combines knowledge graphs with a LLM-based agent to improve reasoning and search accuracy.","Nevertheless, the original KGP framework necessitates costly fine-tuning with large datasets yet still suffers from LLM hallucination.","Therefore, we propose a reasoning-infused LLM agent to enhance this framework.","This agent mimics human curiosity to ask follow-up questions to more efficiently navigate the search.","This simple modification significantly boosts the LLM performance in QA tasks without the high costs and latency associated with the initial KGP framework.","Our ultimate goal is to further develop this approach, leading to more accurate, faster, and cost-effective solutions in the QA domain."],"url":"http://arxiv.org/abs/2404.09077v1","category":"cs.CL"}
{"created":"2024-04-13 19:34:14","title":"Exploring Explainability in Video Action Recognition","abstract":"Image Classification and Video Action Recognition are perhaps the two most foundational tasks in computer vision. Consequently, explaining the inner workings of trained deep neural networks is of prime importance. While numerous efforts focus on explaining the decisions of trained deep neural networks in image classification, exploration in the domain of its temporal version, video action recognition, has been scant. In this work, we take a deeper look at this problem. We begin by revisiting Grad-CAM, one of the popular feature attribution methods for Image Classification, and its extension to Video Action Recognition tasks and examine the method's limitations. To address these, we introduce Video-TCAV, by building on TCAV for Image Classification tasks, which aims to quantify the importance of specific concepts in the decision-making process of Video Action Recognition models. As the scalable generation of concepts is still an open problem, we propose a machine-assisted approach to generate spatial and spatiotemporal concepts relevant to Video Action Recognition for testing Video-TCAV. We then establish the importance of temporally-varying concepts by demonstrating the superiority of dynamic spatiotemporal concepts over trivial spatial concepts. In conclusion, we introduce a framework for investigating hypotheses in action recognition and quantitatively testing them, thus advancing research in the explainability of deep neural networks used in video action recognition.","sentences":["Image Classification and Video Action Recognition are perhaps the two most foundational tasks in computer vision.","Consequently, explaining the inner workings of trained deep neural networks is of prime importance.","While numerous efforts focus on explaining the decisions of trained deep neural networks in image classification, exploration in the domain of its temporal version, video action recognition, has been scant.","In this work, we take a deeper look at this problem.","We begin by revisiting Grad-CAM, one of the popular feature attribution methods for Image Classification, and its extension to Video Action Recognition tasks and examine the method's limitations.","To address these, we introduce Video-TCAV, by building on TCAV for Image Classification tasks, which aims to quantify the importance of specific concepts in the decision-making process of Video Action Recognition models.","As the scalable generation of concepts is still an open problem, we propose a machine-assisted approach to generate spatial and spatiotemporal concepts relevant to Video Action Recognition for testing Video-TCAV.","We then establish the importance of temporally-varying concepts by demonstrating the superiority of dynamic spatiotemporal concepts over trivial spatial concepts.","In conclusion, we introduce a framework for investigating hypotheses in action recognition and quantitatively testing them, thus advancing research in the explainability of deep neural networks used in video action recognition."],"url":"http://arxiv.org/abs/2404.09067v1","category":"cs.CV"}
{"created":"2024-04-13 17:31:11","title":"Rethinking Iterative Stereo Matching from Diffusion Bridge Model Perspective","abstract":"Recently, iteration-based stereo matching has shown great potential. However, these models optimize the disparity map using RNN variants. The discrete optimization process poses a challenge of information loss, which restricts the level of detail that can be expressed in the generated disparity map. In order to address these issues, we propose a novel training approach that incorporates diffusion models into the iterative optimization process. We designed a Time-based Gated Recurrent Unit (T-GRU) to correlate temporal and disparity outputs. Unlike standard recurrent units, we employ Agent Attention to generate more expressive features. We also designed an attention-based context network to capture a large amount of contextual information. Experiments on several public benchmarks show that we have achieved competitive stereo matching performance. Our model ranks first in the Scene Flow dataset, achieving over a 7% improvement compared to competing methods, and requires only 8 iterations to achieve state-of-the-art results.","sentences":["Recently, iteration-based stereo matching has shown great potential.","However, these models optimize the disparity map using RNN variants.","The discrete optimization process poses a challenge of information loss, which restricts the level of detail that can be expressed in the generated disparity map.","In order to address these issues, we propose a novel training approach that incorporates diffusion models into the iterative optimization process.","We designed a Time-based Gated Recurrent Unit (T-GRU) to correlate temporal and disparity outputs.","Unlike standard recurrent units, we employ Agent Attention to generate more expressive features.","We also designed an attention-based context network to capture a large amount of contextual information.","Experiments on several public benchmarks show that we have achieved competitive stereo matching performance.","Our model ranks first in the Scene Flow dataset, achieving over a 7% improvement compared to competing methods, and requires only 8 iterations to achieve state-of-the-art results."],"url":"http://arxiv.org/abs/2404.09051v1","category":"cs.CV"}
{"created":"2024-04-13 17:11:35","title":"Adapting Mental Health Prediction Tasks for Cross-lingual Learning via Meta-Training and In-context Learning with Large Language Model","abstract":"Timely identification is essential for the efficient handling of mental health illnesses such as depression. However, the current research fails to adequately address the prediction of mental health conditions from social media data in low-resource African languages like Swahili. This study introduces two distinct approaches utilising model-agnostic meta-learning and leveraging large language models (LLMs) to address this gap. Experiments are conducted on three datasets translated to low-resource language and applied to four mental health tasks, which include stress, depression, depression severity and suicidal ideation prediction. we first apply a meta-learning model with self-supervision, which results in improved model initialisation for rapid adaptation and cross-lingual transfer. The results show that our meta-trained model performs significantly better than standard fine-tuning methods, outperforming the baseline fine-tuning in macro F1 score with 18\\% and 0.8\\% over XLM-R and mBERT. In parallel, we use LLMs' in-context learning capabilities to assess their performance accuracy across the Swahili mental health prediction tasks by analysing different cross-lingual prompting approaches. Our analysis showed that Swahili prompts performed better than cross-lingual prompts but less than English prompts. Our findings show that in-context learning can be achieved through cross-lingual transfer through carefully crafted prompt templates with examples and instructions.","sentences":["Timely identification is essential for the efficient handling of mental health illnesses such as depression.","However, the current research fails to adequately address the prediction of mental health conditions from social media data in low-resource African languages like Swahili.","This study introduces two distinct approaches utilising model-agnostic meta-learning and leveraging large language models (LLMs) to address this gap.","Experiments are conducted on three datasets translated to low-resource language and applied to four mental health tasks, which include stress, depression, depression severity and suicidal ideation prediction.","we first apply a meta-learning model with self-supervision, which results in improved model initialisation for rapid adaptation and cross-lingual transfer.","The results show that our meta-trained model performs significantly better than standard fine-tuning methods, outperforming the baseline fine-tuning in macro F1 score with 18\\% and 0.8\\% over XLM-R and mBERT.","In parallel, we use LLMs' in-context learning capabilities to assess their performance accuracy across the Swahili mental health prediction tasks by analysing different cross-lingual prompting approaches.","Our analysis showed that Swahili prompts performed better than cross-lingual prompts but less than English prompts.","Our findings show that in-context learning can be achieved through cross-lingual transfer through carefully crafted prompt templates with examples and instructions."],"url":"http://arxiv.org/abs/2404.09045v1","category":"cs.CL"}
{"created":"2024-04-13 16:57:37","title":"Improving Personalisation in Valence and Arousal Prediction using Data Augmentation","abstract":"In the field of emotion recognition and Human-Machine Interaction (HMI), personalised approaches have exhibited their efficacy in capturing individual-specific characteristics and enhancing affective prediction accuracy. However, personalisation techniques often face the challenge of limited data for target individuals. This paper presents our work on an enhanced personalisation strategy, that leverages data augmentation to develop tailored models for continuous valence and arousal prediction. Our proposed approach, Distance Weighting Augmentation (DWA), employs a weighting-based augmentation method that expands a target individual's dataset, leveraging distance metrics to identify similar samples at the segment-level. Experimental results on the MuSe-Personalisation 2023 Challenge dataset demonstrate that our method significantly improves the performance of features sets which have low baseline performance, on the test set. This improvement in poor-performing features comes without sacrificing performance on high-performing features. In particular, our method achieves a maximum combined testing CCC of 0.78, compared to the reported baseline score of 0.76 (reproduced at 0.72). It also achieved a peak arousal and valence scores of 0.81 and 0.76, compared to reproduced baseline scores of 0.76 and 0.67 respectively. Through this work, we make significant contributions to the advancement of personalised affective computing models, enhancing the practicality and adaptability of data-level personalisation in real world contexts.","sentences":["In the field of emotion recognition and Human-Machine Interaction (HMI), personalised approaches have exhibited their efficacy in capturing individual-specific characteristics and enhancing affective prediction accuracy.","However, personalisation techniques often face the challenge of limited data for target individuals.","This paper presents our work on an enhanced personalisation strategy, that leverages data augmentation to develop tailored models for continuous valence and arousal prediction.","Our proposed approach, Distance Weighting Augmentation (DWA), employs a weighting-based augmentation method that expands a target individual's dataset, leveraging distance metrics to identify similar samples at the segment-level.","Experimental results on the MuSe-Personalisation 2023 Challenge dataset demonstrate that our method significantly improves the performance of features sets which have low baseline performance, on the test set.","This improvement in poor-performing features comes without sacrificing performance on high-performing features.","In particular, our method achieves a maximum combined testing CCC of 0.78, compared to the reported baseline score of 0.76 (reproduced at 0.72).","It also achieved a peak arousal and valence scores of 0.81 and 0.76, compared to reproduced baseline scores of 0.76 and 0.67 respectively.","Through this work, we make significant contributions to the advancement of personalised affective computing models, enhancing the practicality and adaptability of data-level personalisation in real world contexts."],"url":"http://arxiv.org/abs/2404.09042v1","category":"cs.LG"}
{"created":"2024-04-13 16:55:50","title":"Three Disclaimers for Safe Disclosure: A Cardwriter for Reporting the Use of Generative AI in Writing Process","abstract":"Generative artificial intelligence (AI) and large language models (LLMs) are increasingly being used in the academic writing process. This is despite the current lack of unified framework for reporting the use of machine assistance. In this work, we propose \"Cardwriter\", an intuitive interface that produces a short report for authors to declare their use of generative AI in their writing process. The demo is available online, at https://cardwriter.vercel.app","sentences":["Generative artificial intelligence (AI) and large language models (LLMs) are increasingly being used in the academic writing process.","This is despite the current lack of unified framework for reporting the use of machine assistance.","In this work, we propose \"Cardwriter\", an intuitive interface that produces a short report for authors to declare their use of generative AI in their writing process.","The demo is available online, at https://cardwriter.vercel.app"],"url":"http://arxiv.org/abs/2404.09041v1","category":"cs.CY"}
{"created":"2024-04-13 16:48:37","title":"Intention-Aware Control Based on Belief-Space Specifications and Stochastic Expansion","abstract":"This paper develops a correct-by-design controller for an autonomous vehicle interacting with opponent vehicles with unknown intentions. We use discrete-valued random variables to model unknown intentions. Based on this, we define an intention-aware control problem for an autonomous vehicle and a collection of opponent agents with epistemic uncertainty. To this end, we focus on a control objective specified in the belief space with temporal logic specifications. From this stochastic control problem, we derive a sound deterministic control problem using stochastic expansion and solve it using shrinking-horizon model predictive control. The solved intention-aware controller allows a vehicle to adjust its behaviors according to its opponents' intentions. It ensures provable safety by restricting the probabilistic risk under a desired level. We show with experimental studies that the proposed method ensures strict limitation of risk probabilities, validating its efficacy in autonomous driving cases. This work provides a novel solution for the risk-aware control of interactive vehicles with formal safety guarantees.","sentences":["This paper develops a correct-by-design controller for an autonomous vehicle interacting with opponent vehicles with unknown intentions.","We use discrete-valued random variables to model unknown intentions.","Based on this, we define an intention-aware control problem for an autonomous vehicle and a collection of opponent agents with epistemic uncertainty.","To this end, we focus on a control objective specified in the belief space with temporal logic specifications.","From this stochastic control problem, we derive a sound deterministic control problem using stochastic expansion and solve it using shrinking-horizon model predictive control.","The solved intention-aware controller allows a vehicle to adjust its behaviors according to its opponents' intentions.","It ensures provable safety by restricting the probabilistic risk under a desired level.","We show with experimental studies that the proposed method ensures strict limitation of risk probabilities, validating its efficacy in autonomous driving cases.","This work provides a novel solution for the risk-aware control of interactive vehicles with formal safety guarantees."],"url":"http://arxiv.org/abs/2404.09037v1","category":"eess.SY"}
{"created":"2024-04-13 15:40:39","title":"Active Learning for Control-Oriented Identification of Nonlinear Systems","abstract":"Model-based reinforcement learning is an effective approach for controlling an unknown system. It is based on a longstanding pipeline familiar to the control community in which one performs experiments on the environment to collect a dataset, uses the resulting dataset to identify a model of the system, and finally performs control synthesis using the identified model. As interacting with the system may be costly and time consuming, targeted exploration is crucial for developing an effective control-oriented model with minimal experimentation. Motivated by this challenge, recent work has begun to study finite sample data requirements and sample efficient algorithms for the problem of optimal exploration in model-based reinforcement learning. However, existing theory and algorithms are limited to model classes which are linear in the parameters. Our work instead focuses on models with nonlinear parameter dependencies, and presents the first finite sample analysis of an active learning algorithm suitable for a general class of nonlinear dynamics. In certain settings, the excess control cost of our algorithm achieves the optimal rate, up to logarithmic factors. We validate our approach in simulation, showcasing the advantage of active, control-oriented exploration for controlling nonlinear systems.","sentences":["Model-based reinforcement learning is an effective approach for controlling an unknown system.","It is based on a longstanding pipeline familiar to the control community in which one performs experiments on the environment to collect a dataset, uses the resulting dataset to identify a model of the system, and finally performs control synthesis using the identified model.","As interacting with the system may be costly and time consuming, targeted exploration is crucial for developing an effective control-oriented model with minimal experimentation.","Motivated by this challenge, recent work has begun to study finite sample data requirements and sample efficient algorithms for the problem of optimal exploration in model-based reinforcement learning.","However, existing theory and algorithms are limited to model classes which are linear in the parameters.","Our work instead focuses on models with nonlinear parameter dependencies, and presents the first finite sample analysis of an active learning algorithm suitable for a general class of nonlinear dynamics.","In certain settings, the excess control cost of our algorithm achieves the optimal rate, up to logarithmic factors.","We validate our approach in simulation, showcasing the advantage of active, control-oriented exploration for controlling nonlinear systems."],"url":"http://arxiv.org/abs/2404.09030v1","category":"eess.SY"}
{"created":"2024-04-13 15:14:53","title":"An Agent-Based Model of Elephant Crop Raid Dynamics in the Periyar-Agasthyamalai Complex, India","abstract":"Human-wildlife conflict poses significant challenges to conservation efforts around the world and requires innovative solutions for effective management. We developed an agent-based model to simulate complex interactions between humans and Asian elephants (particularly solitary bull elephants) in the Periyar-Agasthyamalai complex of the Western Ghats in Kerala, India. Incorporating factors such as crop habituation, thermoregulation needs, and aggression models, this framework enables the evaluation of various experimental scenarios to quantify elephant behaviors and the resulting conflict situations. The ODD protocol, the various cognition models and environmental factors are provided in detail. We simulate different scenarios of food availability to analyze the behavior of elephant agents and assess the influence of environmental factors on space use and emergent conflict patterns. Validation is performed using field data from the region, and elephant movement parameters are tuned using relocation data. Through extensive experimentation, we show that wet months consistently exhibit increased conflict. Furthermore, the experiments reveal that thermoregulation requirements act as a crucial driver of elephant space use, which subsequently influences crop raid patterns. Our findings show how starvation drives wildlife toward crop damage, while crop habituation further exacerbates raid patterns, particularly in regions with limited forest food resources. This agent-based model offers valuable information to develop an intelligent decision support system for wildlife management and decision-making. This is the first step towards development of such a tool, specifically, a primary model that can, over time, be enhanced with layers of complexity and subtlety across various dimensions.","sentences":["Human-wildlife conflict poses significant challenges to conservation efforts around the world and requires innovative solutions for effective management.","We developed an agent-based model to simulate complex interactions between humans and Asian elephants (particularly solitary bull elephants) in the Periyar-Agasthyamalai complex of the Western Ghats in Kerala, India.","Incorporating factors such as crop habituation, thermoregulation needs, and aggression models, this framework enables the evaluation of various experimental scenarios to quantify elephant behaviors and the resulting conflict situations.","The ODD protocol, the various cognition models and environmental factors are provided in detail.","We simulate different scenarios of food availability to analyze the behavior of elephant agents and assess the influence of environmental factors on space use and emergent conflict patterns.","Validation is performed using field data from the region, and elephant movement parameters are tuned using relocation data.","Through extensive experimentation, we show that wet months consistently exhibit increased conflict.","Furthermore, the experiments reveal that thermoregulation requirements act as a crucial driver of elephant space use, which subsequently influences crop raid patterns.","Our findings show how starvation drives wildlife toward crop damage, while crop habituation further exacerbates raid patterns, particularly in regions with limited forest food resources.","This agent-based model offers valuable information to develop an intelligent decision support system for wildlife management and decision-making.","This is the first step towards development of such a tool, specifically, a primary model that can, over time, be enhanced with layers of complexity and subtlety across various dimensions."],"url":"http://arxiv.org/abs/2404.09024v1","category":"cs.MA"}
{"created":"2024-04-13 15:03:03","title":"Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies","abstract":"With the surge of ChatGPT,the use of large models has significantly increased,rapidly rising to prominence across the industry and sweeping across the internet. This article is a comprehensive review of fine-tuning methods for large models. This paper investigates the latest technological advancements and the application of advanced methods in aspects such as task-adaptive fine-tuning,domain-adaptive fine-tuning,few-shot learning,knowledge distillation,multi-task learning,parameter-efficient fine-tuning,and dynamic fine-tuning.","sentences":["With the surge of ChatGPT,the use of large models has significantly increased,rapidly rising to prominence across the industry and sweeping across the internet.","This article is a comprehensive review of fine-tuning methods for large models.","This paper investigates the latest technological advancements and the application of advanced methods in aspects such as task-adaptive fine-tuning,domain-adaptive fine-tuning,few-shot learning,knowledge distillation,multi-task learning,parameter-efficient fine-tuning,and dynamic fine-tuning."],"url":"http://arxiv.org/abs/2404.09022v1","category":"cs.LG"}
{"created":"2024-04-13 14:08:56","title":"Theoretical research on generative diffusion models: an overview","abstract":"Generative diffusion models showed high success in many fields with a powerful theoretical background. They convert the data distribution to noise and remove the noise back to obtain a similar distribution. Many existing reviews focused on the specific application areas without concentrating on the research about the algorithm. Unlike them we investigated the theoretical developments of the generative diffusion models. These approaches mainly divide into two: training-based and sampling-based. Awakening to this allowed us a clear and understandable categorization for the researchers who will make new developments in the future.","sentences":["Generative diffusion models showed high success in many fields with a powerful theoretical background.","They convert the data distribution to noise and remove the noise back to obtain a similar distribution.","Many existing reviews focused on the specific application areas without concentrating on the research about the algorithm.","Unlike them we investigated the theoretical developments of the generative diffusion models.","These approaches mainly divide into two: training-based and sampling-based.","Awakening to this allowed us a clear and understandable categorization for the researchers who will make new developments in the future."],"url":"http://arxiv.org/abs/2404.09016v1","category":"cs.LG"}
{"created":"2024-04-13 13:18:40","title":"Proof-of-Learning with Incentive Security","abstract":"Most concurrent blockchain systems rely heavily on the Proof-of-Work (PoW) or Proof-of-Stake (PoS) mechanisms for decentralized consensus and security assurance. However, the substantial energy expenditure stemming from computationally intensive yet meaningless tasks has raised considerable concerns surrounding traditional PoW approaches, The PoS mechanism, while free of energy consumption, is subject to security and economic issues. Addressing these issues, the paradigm of Proof-of-Useful-Work (PoUW) seeks to employ challenges of practical significance as PoW, thereby imbuing energy consumption with tangible value. While previous efforts in Proof of Learning (PoL) explored the utilization of deep learning model training SGD tasks as PoUW challenges, recent research has revealed its vulnerabilities to adversarial attacks and the theoretical hardness in crafting a byzantine-secure PoL mechanism. In this paper, we introduce the concept of incentive-security that incentivizes rational provers to behave honestly for their best interest, bypassing the existing hardness to design a PoL mechanism with computational efficiency, a provable incentive-security guarantee and controllable difficulty. Particularly, our work is secure against two attacks to the recent work of Jia et al. [2021], and also improves the computational overhead from $\\Theta(1)$ to $O(\\frac{\\log E}{E})$. Furthermore, while most recent research assumes trusted problem providers and verifiers, our design also guarantees frontend incentive-security even when problem providers are untrusted, and verifier incentive-security that bypasses the Verifier's Dilemma. By incorporating ML training into blockchain consensus mechanisms with provable guarantees, our research not only proposes an eco-friendly solution to blockchain systems, but also provides a proposal for a completely decentralized computing power market in the new AI age.","sentences":["Most concurrent blockchain systems rely heavily on the Proof-of-Work (PoW) or Proof-of-Stake (PoS) mechanisms for decentralized consensus and security assurance.","However, the substantial energy expenditure stemming from computationally intensive yet meaningless tasks has raised considerable concerns surrounding traditional PoW approaches, The PoS mechanism, while free of energy consumption, is subject to security and economic issues.","Addressing these issues, the paradigm of Proof-of-Useful-Work (PoUW) seeks to employ challenges of practical significance as PoW, thereby imbuing energy consumption with tangible value.","While previous efforts in Proof of Learning (PoL) explored the utilization of deep learning model training SGD tasks as PoUW challenges, recent research has revealed its vulnerabilities to adversarial attacks and the theoretical hardness in crafting a byzantine-secure PoL mechanism.","In this paper, we introduce the concept of incentive-security that incentivizes rational provers to behave honestly for their best interest, bypassing the existing hardness to design a PoL mechanism with computational efficiency, a provable incentive-security guarantee and controllable difficulty.","Particularly, our work is secure against two attacks to the recent work of Jia et al.","[2021], and also improves the computational overhead from $\\Theta(1)$ to $O(\\frac{\\log E}{E})$.","Furthermore, while most recent research assumes trusted problem providers and verifiers, our design also guarantees frontend incentive-security even when problem providers are untrusted, and verifier incentive-security that bypasses the Verifier's Dilemma.","By incorporating ML training into blockchain consensus mechanisms with provable guarantees, our research not only proposes an eco-friendly solution to blockchain systems, but also provides a proposal for a completely decentralized computing power market in the new AI age."],"url":"http://arxiv.org/abs/2404.09005v1","category":"cs.CR"}
{"created":"2024-04-13 13:03:59","title":"Smart Help: Strategic Opponent Modeling for Proactive and Adaptive Robot Assistance in Households","abstract":"Despite the significant demand for assistive technology among vulnerable groups (e.g., the elderly, children, and the disabled) in daily tasks, research into advanced AI-driven assistive solutions that genuinely accommodate their diverse needs remains sparse. Traditional human-machine interaction tasks often require machines to simply help without nuanced consideration of human abilities and feelings, such as their opportunity for practice and learning, sense of self-improvement, and self-esteem. Addressing this gap, we define a pivotal and novel challenge Smart Help, which aims to provide proactive yet adaptive support to human agents with diverse disabilities and dynamic goals in various tasks and environments. To establish this challenge, we leverage AI2-THOR to build a new interactive 3D realistic household environment for the Smart Help task. We introduce an innovative opponent modeling module that provides a nuanced understanding of the main agent's capabilities and goals, in order to optimize the assisting agent's helping policy. Rigorous experiments validate the efficacy of our model components and show the superiority of our holistic approach against established baselines. Our findings illustrate the potential of AI-imbued assistive robots in improving the well-being of vulnerable groups.","sentences":["Despite the significant demand for assistive technology among vulnerable groups (e.g., the elderly, children, and the disabled) in daily tasks, research into advanced AI-driven assistive solutions that genuinely accommodate their diverse needs remains sparse.","Traditional human-machine interaction tasks often require machines to simply help without nuanced consideration of human abilities and feelings, such as their opportunity for practice and learning, sense of self-improvement, and self-esteem.","Addressing this gap, we define a pivotal and novel challenge Smart Help, which aims to provide proactive yet adaptive support to human agents with diverse disabilities and dynamic goals in various tasks and environments.","To establish this challenge, we leverage AI2-THOR to build a new interactive 3D realistic household environment for the Smart Help task.","We introduce an innovative opponent modeling module that provides a nuanced understanding of the main agent's capabilities and goals, in order to optimize the assisting agent's helping policy.","Rigorous experiments validate the efficacy of our model components and show the superiority of our holistic approach against established baselines.","Our findings illustrate the potential of AI-imbued assistive robots in improving the well-being of vulnerable groups."],"url":"http://arxiv.org/abs/2404.09001v1","category":"cs.RO"}
{"created":"2024-04-13 12:41:40","title":"Beyond Known Clusters: Probe New Prototypes for Efficient Generalized Class Discovery","abstract":"Generalized Class Discovery (GCD) aims to dynamically assign labels to unlabelled data partially based on knowledge learned from labelled data, where the unlabelled data may come from known or novel classes. The prevailing approach generally involves clustering across all data and learning conceptions by prototypical contrastive learning. However, existing methods largely hinge on the performance of clustering algorithms and are thus subject to their inherent limitations. Firstly, the estimated cluster number is often smaller than the ground truth, making the existing methods suffer from the lack of prototypes for comprehensive conception learning. To address this issue, we propose an adaptive probing mechanism that introduces learnable potential prototypes to expand cluster prototypes (centers). As there is no ground truth for the potential prototype, we develop a self-supervised prototype learning framework to optimize the potential prototype in an end-to-end fashion. Secondly, clustering is computationally intensive, and the conventional strategy of clustering both labelled and unlabelled instances exacerbates this issue. To counteract this inefficiency, we opt to cluster only the unlabelled instances and subsequently expand the cluster prototypes with our introduced potential prototypes to fast explore novel classes. Despite the simplicity of our proposed method, extensive empirical analysis on a wide range of datasets confirms that our method consistently delivers state-of-the-art results. Specifically, our method surpasses the nearest competitor by a significant margin of \\textbf{9.7}$\\%$ within the Stanford Cars dataset and \\textbf{12$\\times$} clustering efficiency within the Herbarium 19 dataset. We will make the code and checkpoints publicly available at \\url{https://github.com/xjtuYW/PNP.git}.","sentences":["Generalized Class Discovery (GCD) aims to dynamically assign labels to unlabelled data partially based on knowledge learned from labelled data, where the unlabelled data may come from known or novel classes.","The prevailing approach generally involves clustering across all data and learning conceptions by prototypical contrastive learning.","However, existing methods largely hinge on the performance of clustering algorithms and are thus subject to their inherent limitations.","Firstly, the estimated cluster number is often smaller than the ground truth, making the existing methods suffer from the lack of prototypes for comprehensive conception learning.","To address this issue, we propose an adaptive probing mechanism that introduces learnable potential prototypes to expand cluster prototypes (centers).","As there is no ground truth for the potential prototype, we develop a self-supervised prototype learning framework to optimize the potential prototype in an end-to-end fashion.","Secondly, clustering is computationally intensive, and the conventional strategy of clustering both labelled and unlabelled instances exacerbates this issue.","To counteract this inefficiency, we opt to cluster only the unlabelled instances and subsequently expand the cluster prototypes with our introduced potential prototypes to fast explore novel classes.","Despite the simplicity of our proposed method, extensive empirical analysis on a wide range of datasets confirms that our method consistently delivers state-of-the-art results.","Specifically, our method surpasses the nearest competitor by a significant margin of \\textbf{9.7}$\\%$ within the Stanford Cars dataset and \\textbf{12$\\times$} clustering efficiency within the Herbarium 19 dataset.","We will make the code and checkpoints publicly available at \\url{https://github.com/xjtuYW/PNP.git}."],"url":"http://arxiv.org/abs/2404.08995v1","category":"cs.LG"}
{"created":"2024-04-13 12:36:20","title":"Business models for the simulation hypothesis","abstract":"The simulation hypothesis suggests that we live in a computer simulation. That notion has attracted significant scholarly and popular interest. This article explores the simulation hypothesis from a business perspective. Due to the lack of a name for a universe consistent with the simulation hypothesis, we propose the term simuverse. We argue that if we live in a simulation, there must be a business justification. Therefore, we ask: If we live in a simuverse, what is its business model? We identify and explore business model scenarios, such as simuverse as a project, service, or platform. We also explore business model pathways and risk management issues. The article contributes to the simulation hypothesis literature and is the first to provide a business model perspective on the simulation hypothesis. The article discusses theoretical and practical implications and identifies opportunities for future research related to sustainability, digital transformation, and Artificial Intelligence (AI).","sentences":["The simulation hypothesis suggests that we live in a computer simulation.","That notion has attracted significant scholarly and popular interest.","This article explores the simulation hypothesis from a business perspective.","Due to the lack of a name for a universe consistent with the simulation hypothesis, we propose the term simuverse.","We argue that if we live in a simulation, there must be a business justification.","Therefore, we ask: If we live in a simuverse, what is its business model?","We identify and explore business model scenarios, such as simuverse as a project, service, or platform.","We also explore business model pathways and risk management issues.","The article contributes to the simulation hypothesis literature and is the first to provide a business model perspective on the simulation hypothesis.","The article discusses theoretical and practical implications and identifies opportunities for future research related to sustainability, digital transformation, and Artificial Intelligence (AI)."],"url":"http://arxiv.org/abs/2404.08991v1","category":"cs.CY"}
{"created":"2024-04-13 12:28:40","title":"A Fourier-enhanced multi-modal 3D small object optical mark recognition and positioning method for percutaneous abdominal puncture surgical navigation","abstract":"Navigation for thoracoabdominal puncture surgery is used to locate the needle entry point on the patient's body surface. The traditional reflective ball navigation method is difficult to position the needle entry point on the soft, irregular, smooth chest and abdomen. Due to the lack of clear characteristic points on the body surface using structured light technology, it is difficult to identify and locate arbitrary needle insertion points. Based on the high stability and high accuracy requirements of surgical navigation, this paper proposed a novel method, a muti-modal 3D small object medical marker detection method, which identifies the center of a small single ring as the needle insertion point. Moreover, this novel method leverages Fourier transform enhancement technology to augment the dataset, enrich image details, and enhance the network's capability. The method extracts the Region of Interest (ROI) of the feature image from both enhanced and original images, followed by generating a mask map. Subsequently, the point cloud of the ROI from the depth map is obtained through the registration of ROI point cloud contour fitting. In addition, this method employs Tukey loss for optimal precision. The experimental results show this novel method proposed in this paper not only achieves high-precision and high-stability positioning, but also enables the positioning of any needle insertion point.","sentences":["Navigation for thoracoabdominal puncture surgery is used to locate the needle entry point on the patient's body surface.","The traditional reflective ball navigation method is difficult to position the needle entry point on the soft, irregular, smooth chest and abdomen.","Due to the lack of clear characteristic points on the body surface using structured light technology, it is difficult to identify and locate arbitrary needle insertion points.","Based on the high stability and high accuracy requirements of surgical navigation, this paper proposed a novel method, a muti-modal 3D small object medical marker detection method, which identifies the center of a small single ring as the needle insertion point.","Moreover, this novel method leverages Fourier transform enhancement technology to augment the dataset, enrich image details, and enhance the network's capability.","The method extracts the Region of Interest (ROI) of the feature image from both enhanced and original images, followed by generating a mask map.","Subsequently, the point cloud of the ROI from the depth map is obtained through the registration of ROI point cloud contour fitting.","In addition, this method employs Tukey loss for optimal precision.","The experimental results show this novel method proposed in this paper not only achieves high-precision and high-stability positioning, but also enables the positioning of any needle insertion point."],"url":"http://arxiv.org/abs/2404.08990v1","category":"cs.CV"}
{"created":"2024-04-13 12:18:36","title":"On the critical path to implant backdoors and the effectiveness of potential mitigation techniques: Early learnings from XZ","abstract":"An emerging supply-chain attack due to a backdoor in XZ Utils has been identified. The backdoor allows an attacker to run commands remotely on vulnerable servers utilizing SSH without prior authentication. We have started to collect available information with regards to this attack to discuss current mitigation strategies for such kinds of supply-chain attacks. This paper introduces the critical attack path of the XZ backdoor and provides an overview about potential mitigation techniques related to relevant stages of the attack path.","sentences":["An emerging supply-chain attack due to a backdoor in XZ Utils has been identified.","The backdoor allows an attacker to run commands remotely on vulnerable servers utilizing SSH without prior authentication.","We have started to collect available information with regards to this attack to discuss current mitigation strategies for such kinds of supply-chain attacks.","This paper introduces the critical attack path of the XZ backdoor and provides an overview about potential mitigation techniques related to relevant stages of the attack path."],"url":"http://arxiv.org/abs/2404.08987v1","category":"cs.CR"}
{"created":"2024-04-13 12:18:19","title":"Airship Formations for Animal Motion Capture and Behavior Analysis","abstract":"Using UAVs for wildlife observation and motion capture offers manifold advantages for studying animals in the wild, especially grazing herds in open terrain. The aerial perspective allows observation at a scale and depth that is not possible on the ground, offering new insights into group behavior. However, the very nature of wildlife field-studies puts traditional fixed wing and multi-copter systems to their limits: limited flight time, noise and safety aspects affect their efficacy, where lighter than air systems can remain on station for many hours. Nevertheless, airships are challenging from a ground handling perspective as well as from a control point of view, being voluminous and highly affected by wind. In this work, we showcase a system designed to use airship formations to track, follow, and visually record wild horses from multiple angles, including airship design, simulation, control, on board computer vision, autonomous operation and practical aspects of field experiments.","sentences":["Using UAVs for wildlife observation and motion capture offers manifold advantages for studying animals in the wild, especially grazing herds in open terrain.","The aerial perspective allows observation at a scale and depth that is not possible on the ground, offering new insights into group behavior.","However, the very nature of wildlife field-studies puts traditional fixed wing and multi-copter systems to their limits: limited flight time, noise and safety aspects affect their efficacy, where lighter than air systems can remain on station for many hours.","Nevertheless, airships are challenging from a ground handling perspective as well as from a control point of view, being voluminous and highly affected by wind.","In this work, we showcase a system designed to use airship formations to track, follow, and visually record wild horses from multiple angles, including airship design, simulation, control, on board computer vision, autonomous operation and practical aspects of field experiments."],"url":"http://arxiv.org/abs/2404.08986v1","category":"cs.RO"}
{"created":"2024-04-13 12:14:58","title":"Intuition-aware Mixture-of-Rank-1-Experts for Parameter Efficient Finetuning","abstract":"Large Language Models (LLMs) have demonstrated significant potential in performing multiple tasks in multimedia applications, ranging from content generation to interactive entertainment, and artistic creation. However, the diversity of downstream tasks in multitask scenarios presents substantial adaptation challenges for LLMs. While traditional methods often succumb to knowledge confusion on their monolithic dense models, Mixture-of-Experts (MoE) has been emerged as a promising solution with its sparse architecture for effective task decoupling. Inspired by the principles of human cognitive neuroscience, we design a novel framework \\texttt{Intuition-MoR1E} that leverages the inherent semantic clustering of instances to mimic the human brain to deal with multitask, offering implicit guidance to router for optimized feature allocation. Moreover, we introduce cutting-edge Rank-1 Experts formulation designed to manage a spectrum of intuitions, demonstrating enhanced parameter efficiency and effectiveness in multitask LLM finetuning. Extensive experiments demonstrate that Intuition-MoR1E achieves superior efficiency and 2.15\\% overall accuracy improvement across 14 public datasets against other state-of-the-art baselines.","sentences":["Large Language Models (LLMs) have demonstrated significant potential in performing multiple tasks in multimedia applications, ranging from content generation to interactive entertainment, and artistic creation.","However, the diversity of downstream tasks in multitask scenarios presents substantial adaptation challenges for LLMs.","While traditional methods often succumb to knowledge confusion on their monolithic dense models, Mixture-of-Experts (MoE) has been emerged as a promising solution with its sparse architecture for effective task decoupling.","Inspired by the principles of human cognitive neuroscience, we design a novel framework \\texttt{Intuition-MoR1E} that leverages the inherent semantic clustering of instances to mimic the human brain to deal with multitask, offering implicit guidance to router for optimized feature allocation.","Moreover, we introduce cutting-edge Rank-1 Experts formulation designed to manage a spectrum of intuitions, demonstrating enhanced parameter efficiency and effectiveness in multitask LLM finetuning.","Extensive experiments demonstrate that Intuition-MoR1E achieves superior efficiency and 2.15\\% overall accuracy improvement across 14 public datasets against other state-of-the-art baselines."],"url":"http://arxiv.org/abs/2404.08985v1","category":"cs.LG"}
{"created":"2024-04-13 12:02:19","title":"Incremental Residual Concept Bottleneck Models","abstract":"Concept Bottleneck Models (CBMs) map the black-box visual representations extracted by deep neural networks onto a set of interpretable concepts and use the concepts to make predictions, enhancing the transparency of the decision-making process. Multimodal pre-trained models can match visual representations with textual concept embeddings, allowing for obtaining the interpretable concept bottleneck without the expertise concept annotations. Recent research has focused on the concept bank establishment and the high-quality concept selection. However, it is challenging to construct a comprehensive concept bank through humans or large language models, which severely limits the performance of CBMs. In this work, we propose the Incremental Residual Concept Bottleneck Model (Res-CBM) to address the challenge of concept completeness. Specifically, the residual concept bottleneck model employs a set of optimizable vectors to complete missing concepts, then the incremental concept discovery module converts the complemented vectors with unclear meanings into potential concepts in the candidate concept bank. Our approach can be applied to any user-defined concept bank, as a post-hoc processing method to enhance the performance of any CBMs. Furthermore, to measure the descriptive efficiency of CBMs, the Concept Utilization Efficiency (CUE) metric is proposed. Experiments show that the Res-CBM outperforms the current state-of-the-art methods in terms of both accuracy and efficiency and achieves comparable performance to black-box models across multiple datasets.","sentences":["Concept Bottleneck Models (CBMs) map the black-box visual representations extracted by deep neural networks onto a set of interpretable concepts and use the concepts to make predictions, enhancing the transparency of the decision-making process.","Multimodal pre-trained models can match visual representations with textual concept embeddings, allowing for obtaining the interpretable concept bottleneck without the expertise concept annotations.","Recent research has focused on the concept bank establishment and the high-quality concept selection.","However, it is challenging to construct a comprehensive concept bank through humans or large language models, which severely limits the performance of CBMs.","In this work, we propose the Incremental Residual Concept Bottleneck Model (Res-CBM) to address the challenge of concept completeness.","Specifically, the residual concept bottleneck model employs a set of optimizable vectors to complete missing concepts, then the incremental concept discovery module converts the complemented vectors with unclear meanings into potential concepts in the candidate concept bank.","Our approach can be applied to any user-defined concept bank, as a post-hoc processing method to enhance the performance of any CBMs.","Furthermore, to measure the descriptive efficiency of CBMs, the Concept Utilization Efficiency (CUE) metric is proposed.","Experiments show that the Res-CBM outperforms the current state-of-the-art methods in terms of both accuracy and efficiency and achieves comparable performance to black-box models across multiple datasets."],"url":"http://arxiv.org/abs/2404.08978v1","category":"cs.LG"}
{"created":"2024-04-13 11:06:49","title":"Understanding Multimodal Deep Neural Networks: A Concept Selection View","abstract":"The multimodal deep neural networks, represented by CLIP, have generated rich downstream applications owing to their excellent performance, thus making understanding the decision-making process of CLIP an essential research topic. Due to the complex structure and the massive pre-training data, it is often regarded as a black-box model that is too difficult to understand and interpret. Concept-based models map the black-box visual representations extracted by deep neural networks onto a set of human-understandable concepts and use the concepts to make predictions, enhancing the transparency of the decision-making process. However, these methods involve the datasets labeled with fine-grained attributes by expert knowledge, which incur high costs and introduce excessive human prior knowledge and bias. In this paper, we observe the long-tail distribution of concepts, based on which we propose a two-stage Concept Selection Model (CSM) to mine core concepts without introducing any human priors. The concept greedy rough selection algorithm is applied to extract head concepts, and then the concept mask fine selection method performs the extraction of core concepts. Experiments show that our approach achieves comparable performance to end-to-end black-box models, and human evaluation demonstrates that the concepts discovered by our method are interpretable and comprehensible for humans.","sentences":["The multimodal deep neural networks, represented by CLIP, have generated rich downstream applications owing to their excellent performance, thus making understanding the decision-making process of CLIP an essential research topic.","Due to the complex structure and the massive pre-training data, it is often regarded as a black-box model that is too difficult to understand and interpret.","Concept-based models map the black-box visual representations extracted by deep neural networks onto a set of human-understandable concepts and use the concepts to make predictions, enhancing the transparency of the decision-making process.","However, these methods involve the datasets labeled with fine-grained attributes by expert knowledge, which incur high costs and introduce excessive human prior knowledge and bias.","In this paper, we observe the long-tail distribution of concepts, based on which we propose a two-stage Concept Selection Model (CSM) to mine core concepts without introducing any human priors.","The concept greedy rough selection algorithm is applied to extract head concepts, and then the concept mask fine selection method performs the extraction of core concepts.","Experiments show that our approach achieves comparable performance to end-to-end black-box models, and human evaluation demonstrates that the concepts discovered by our method are interpretable and comprehensible for humans."],"url":"http://arxiv.org/abs/2404.08964v1","category":"cs.CV"}
{"created":"2024-04-13 09:56:50","title":"Large Language Models for Mobile GUI Text Input Generation: An Empirical Study","abstract":"Mobile applications (apps) have become an essential part of our daily lives, making ensuring their quality an important activity. GUI testing, a quality assurance method, has frequently been used for mobile apps. When conducting GUI testing, it is important to generate effective text inputs for the text-input components. Some GUIs require these text inputs to move from one page to the next, which remains a challenge to achieving complete UI exploration. Recently, Large Language Models (LLMs) have shown excellent text-generation capabilities. Among the LLMs, OpenAI's GPT series has been widely discussed and used. However, it may not be possible to use these LLMs for GUI testing actual mobile apps, due to the security and privacy issues related to the production data. Therefore, it is necessary to explore the potential of different LLMs to guide text-input generation in mobile GUI testing. This paper reports on a large-scale empirical study that extensively investigates the effectiveness of nine state-of-the-art LLMs in Android text-input generation for UI pages. We collected 114 UI pages from 62 open-source Android apps and extracted contextual information from the UI pages to construct prompts for LLMs to generate text inputs. The experimental results show that some LLMs can generate relatively more effective and higher-quality text inputs, achieving a 50.58% to 66.67% page-pass-through rate, and even detecting some real bugs in open-source apps. Compared with the GPT-3.5 and GPT-4 LLMs, other LLMs reduce the page-pass-through rates by 17.97% to 84.79% and 21.93% to 85.53%, respectively. We also found that using more complete UI contextual information can increase the page-pass-through rates of LLMs for generating text inputs. In addition, we also describe six insights gained regarding the use of LLMs for Android testing: These insights will benefit the Android testing community.","sentences":["Mobile applications (apps) have become an essential part of our daily lives, making ensuring their quality an important activity.","GUI testing, a quality assurance method, has frequently been used for mobile apps.","When conducting GUI testing, it is important to generate effective text inputs for the text-input components.","Some GUIs require these text inputs to move from one page to the next, which remains a challenge to achieving complete UI exploration.","Recently, Large Language Models (LLMs) have shown excellent text-generation capabilities.","Among the LLMs, OpenAI's GPT series has been widely discussed and used.","However, it may not be possible to use these LLMs for GUI testing actual mobile apps, due to the security and privacy issues related to the production data.","Therefore, it is necessary to explore the potential of different LLMs to guide text-input generation in mobile GUI testing.","This paper reports on a large-scale empirical study that extensively investigates the effectiveness of nine state-of-the-art LLMs in Android text-input generation for UI pages.","We collected 114 UI pages from 62 open-source Android apps and extracted contextual information from the UI pages to construct prompts for LLMs to generate text inputs.","The experimental results show that some LLMs can generate relatively more effective and higher-quality text inputs, achieving a 50.58% to 66.67% page-pass-through rate, and even detecting some real bugs in open-source apps.","Compared with the GPT-3.5 and GPT-4 LLMs, other LLMs reduce the page-pass-through rates by 17.97% to 84.79% and 21.93% to 85.53%, respectively.","We also found that using more complete UI contextual information can increase the page-pass-through rates of LLMs for generating text inputs.","In addition, we also describe six insights gained regarding the use of LLMs for Android testing: These insights will benefit the Android testing community."],"url":"http://arxiv.org/abs/2404.08948v1","category":"cs.SE"}
{"created":"2024-04-13 09:47:07","title":"Zero-Shot Code Representation Learning via Prompt Tuning","abstract":"Learning code representations has been the core prerequisite of many software engineering tasks such as code clone detection and code generation. State-of-the-art program representation techniques mainly utilize pre-trained language models (PLMs) such as CodeBERT. A Transformer encoder is firstly pre-trained on a large-scale code corpus to acquire general knowledge about source code. The pre-trained model is then fine-tuned on specific tasks using an amount of labeled data. However, gathering training samples for the downstream tasks can be prohibitively expensive and impractical for domain-specific languages or project-specific tasks. Besides, pre-training and downstream tasks are usually heterogeneous, which makes it difficult to fully explore the knowledge learned during pre-training. In this paper, we propose Zecoler, a zero-shot approach for learning code representations. Zecoler is built upon a pre-trained programming language model. In order to elicit knowledge from the PLMs efficiently, Zecoler casts the downstream tasks to the same form of pre-training objectives by inserting train-able prompts into the original input. These prompts can guide PLMs on how to generate better results. Subsequently, we employ the prompt tuning technique to search for the optimal prompts for PLMs automatically. This enables the representation model to efficiently fit the downstream tasks through fine-tuning on the dataset in source language domain and then reuse the pre-trained knowledge for the target domain in a zero-shot style. We evaluate Zecoler in five code intelligence tasks including code clone detection, code search, method name prediction, code summarization, and code generation. The results show that our approach significantly outperforms baseline models under the zero-shot setting.","sentences":["Learning code representations has been the core prerequisite of many software engineering tasks such as code clone detection and code generation.","State-of-the-art program representation techniques mainly utilize pre-trained language models (PLMs) such as CodeBERT.","A Transformer encoder is firstly pre-trained on a large-scale code corpus to acquire general knowledge about source code.","The pre-trained model is then fine-tuned on specific tasks using an amount of labeled data.","However, gathering training samples for the downstream tasks can be prohibitively expensive and impractical for domain-specific languages or project-specific tasks.","Besides, pre-training and downstream tasks are usually heterogeneous, which makes it difficult to fully explore the knowledge learned during pre-training.","In this paper, we propose Zecoler, a zero-shot approach for learning code representations.","Zecoler is built upon a pre-trained programming language model.","In order to elicit knowledge from the PLMs efficiently, Zecoler casts the downstream tasks to the same form of pre-training objectives by inserting train-able prompts into the original input.","These prompts can guide PLMs on how to generate better results.","Subsequently, we employ the prompt tuning technique to search for the optimal prompts for PLMs automatically.","This enables the representation model to efficiently fit the downstream tasks through fine-tuning on the dataset in source language domain and then reuse the pre-trained knowledge for the target domain in a zero-shot style.","We evaluate Zecoler in five code intelligence tasks including code clone detection, code search, method name prediction, code summarization, and code generation.","The results show that our approach significantly outperforms baseline models under the zero-shot setting."],"url":"http://arxiv.org/abs/2404.08947v1","category":"cs.SE"}
{"created":"2024-04-13 09:45:46","title":"Image Scanning Microscopy Reconstruction by Autocorrelation Inversion","abstract":"Confocal laser scanning microscopy (CLSM) stands out as one of the most widely used microscopy techniques, thanks to its three-dimensional imaging capability and its sub-diffraction spatial resolution, achieved through the closure of a pinhole in front of a single-element detector. However, the pinhole also rejects useful photons and beating the diffraction limit comes at the price of irremediably compromising the signal-to-noise ratio (SNR) of the data. Image scanning microscopy (ISM) emerged as the natural evolution of CLSM, exploiting a small array detector in place of the pinhole and the single-element detector. Each sensitive element is small enough to achieve sub-diffraction resolution through the confocal effect, but the size of the whole detector is large enough to guarantee excellent collection efficiency and SNR. However, the raw data produced by an ISM setup consists of a 4D dataset which can be seen as a set of confocal-like images. Thus, fusing the dataset into a single super-resolved image requires a dedicated reconstruction algorithm. Conventional methods are multi-image deconvolution, which requires prior knowledge of the system point spread functions (PSF), or adaptive pixel reassignment (APR), which is effective only on a limited range of experimental conditions. In this work, we describe and validate a novel concept for ISM image reconstruction based on autocorrelation inversion. We leverage unique properties of the autocorrelation to discard low-frequency components and maximize the resolution of the reconstructed image, without any assumption on the image or any knowledge of the PSF. Our results push the quality of the ISM reconstruction beyond the level provided by APR and open new perspectives for multi-dimensional image processing.","sentences":["Confocal laser scanning microscopy (CLSM) stands out as one of the most widely used microscopy techniques, thanks to its three-dimensional imaging capability and its sub-diffraction spatial resolution, achieved through the closure of a pinhole in front of a single-element detector.","However, the pinhole also rejects useful photons and beating the diffraction limit comes at the price of irremediably compromising the signal-to-noise ratio (SNR) of the data.","Image scanning microscopy (ISM) emerged as the natural evolution of CLSM, exploiting a small array detector in place of the pinhole and the single-element detector.","Each sensitive element is small enough to achieve sub-diffraction resolution through the confocal effect, but the size of the whole detector is large enough to guarantee excellent collection efficiency and SNR.","However, the raw data produced by an ISM setup consists of a 4D dataset which can be seen as a set of confocal-like images.","Thus, fusing the dataset into a single super-resolved image requires a dedicated reconstruction algorithm.","Conventional methods are multi-image deconvolution, which requires prior knowledge of the system point spread functions (PSF), or adaptive pixel reassignment (APR), which is effective only on a limited range of experimental conditions.","In this work, we describe and validate a novel concept for ISM image reconstruction based on autocorrelation inversion.","We leverage unique properties of the autocorrelation to discard low-frequency components and maximize the resolution of the reconstructed image, without any assumption on the image or any knowledge of the PSF.","Our results push the quality of the ISM reconstruction beyond the level provided by APR and open new perspectives for multi-dimensional image processing."],"url":"http://arxiv.org/abs/2404.08946v1","category":"physics.optics"}
{"created":"2024-04-13 09:24:50","title":"NeurIT: Pushing the Limit of Neural Inertial Tracking for Indoor Robotic IoT","abstract":"Inertial tracking is vital for robotic IoT and has gained popularity thanks to the ubiquity of low-cost Inertial Measurement Units (IMUs) and deep learning-powered tracking algorithms. Existing works, however, have not fully utilized IMU measurements, particularly magnetometers, nor maximized the potential of deep learning to achieve the desired accuracy. To enhance the tracking accuracy for indoor robotic applications, we introduce NeurIT, a sequence-to-sequence framework that elevates tracking accuracy to a new level. NeurIT employs a Time-Frequency Block-recurrent Transformer (TF-BRT) at its core, combining the power of recurrent neural network (RNN) and Transformer to learn representative features in both time and frequency domains. To fully utilize IMU information, we strategically employ body-frame differentiation of the magnetometer, which considerably reduces the tracking error. NeurIT is implemented on a customized robotic platform and evaluated in various indoor environments. Experimental results demonstrate that NeurIT achieves a mere 1-meter tracking error over a 300-meter distance. Notably, it significantly outperforms state-of-the-art baselines by 48.21% on unseen data. NeurIT also performs comparably to the visual-inertial approach (Tango Phone) in vision-favored conditions and surpasses it in plain environments. We believe NeurIT takes an important step forward toward practical neural inertial tracking for ubiquitous and scalable tracking of robotic things. NeurIT, including the source code and the dataset, is open-sourced here: https://github.com/NeurIT-Project/NeurIT.","sentences":["Inertial tracking is vital for robotic IoT and has gained popularity thanks to the ubiquity of low-cost Inertial Measurement Units (IMUs) and deep learning-powered tracking algorithms.","Existing works, however, have not fully utilized IMU measurements, particularly magnetometers, nor maximized the potential of deep learning to achieve the desired accuracy.","To enhance the tracking accuracy for indoor robotic applications, we introduce NeurIT, a sequence-to-sequence framework that elevates tracking accuracy to a new level.","NeurIT employs a Time-Frequency Block-recurrent Transformer (TF-BRT) at its core, combining the power of recurrent neural network (RNN) and Transformer to learn representative features in both time and frequency domains.","To fully utilize IMU information, we strategically employ body-frame differentiation of the magnetometer, which considerably reduces the tracking error.","NeurIT is implemented on a customized robotic platform and evaluated in various indoor environments.","Experimental results demonstrate that NeurIT achieves a mere 1-meter tracking error over a 300-meter distance.","Notably, it significantly outperforms state-of-the-art baselines by 48.21% on unseen data.","NeurIT also performs comparably to the visual-inertial approach (Tango Phone) in vision-favored conditions and surpasses it in plain environments.","We believe NeurIT takes an important step forward toward practical neural inertial tracking for ubiquitous and scalable tracking of robotic things.","NeurIT, including the source code and the dataset, is open-sourced here: https://github.com/NeurIT-Project/NeurIT."],"url":"http://arxiv.org/abs/2404.08939v1","category":"cs.RO"}
{"created":"2024-04-13 09:17:51","title":"ChimpVLM: Ethogram-Enhanced Chimpanzee Behaviour Recognition","abstract":"We show that chimpanzee behaviour understanding from camera traps can be enhanced by providing visual architectures with access to an embedding of text descriptions that detail species behaviours. In particular, we present a vision-language model which employs multi-modal decoding of visual features extracted directly from camera trap videos to process query tokens representing behaviours and output class predictions. Query tokens are initialised using a standardised ethogram of chimpanzee behaviour, rather than using random or name-based initialisations. In addition, the effect of initialising query tokens using a masked language model fine-tuned on a text corpus of known behavioural patterns is explored. We evaluate our system on the PanAf500 and PanAf20K datasets and demonstrate the performance benefits of our multi-modal decoding approach and query initialisation strategy on multi-class and multi-label recognition tasks, respectively. Results and ablations corroborate performance improvements. We achieve state-of-the-art performance over vision and vision-language models in top-1 accuracy (+6.34%) on PanAf500 and overall (+1.1%) and tail-class (+2.26%) mean average precision on PanAf20K. We share complete source code and network weights for full reproducibility of results and easy utilisation.","sentences":["We show that chimpanzee behaviour understanding from camera traps can be enhanced by providing visual architectures with access to an embedding of text descriptions that detail species behaviours.","In particular, we present a vision-language model which employs multi-modal decoding of visual features extracted directly from camera trap videos to process query tokens representing behaviours and output class predictions.","Query tokens are initialised using a standardised ethogram of chimpanzee behaviour, rather than using random or name-based initialisations.","In addition, the effect of initialising query tokens using a masked language model fine-tuned on a text corpus of known behavioural patterns is explored.","We evaluate our system on the PanAf500 and PanAf20K datasets and demonstrate the performance benefits of our multi-modal decoding approach and query initialisation strategy on multi-class and multi-label recognition tasks, respectively.","Results and ablations corroborate performance improvements.","We achieve state-of-the-art performance over vision and vision-language models in top-1 accuracy (+6.34%) on PanAf500 and overall (+1.1%) and tail-class (+2.26%) mean average precision on PanAf20K. We share complete source code and network weights for full reproducibility of results and easy utilisation."],"url":"http://arxiv.org/abs/2404.08937v1","category":"cs.CV"}
{"created":"2024-04-13 08:49:17","title":"Label-free Anomaly Detection in Aerial Agricultural Images with Masked Image Modeling","abstract":"Detecting various types of stresses (nutritional, water, nitrogen, etc.) in agricultural fields is critical for farmers to ensure maximum productivity. However, stresses show up in different shapes and sizes across different crop types and varieties. Hence, this is posed as an anomaly detection task in agricultural images. Accurate anomaly detection in agricultural UAV images is vital for early identification of field irregularities. Traditional supervised learning faces challenges in adapting to diverse anomalies, necessitating extensive annotated data. In this work, we overcome this limitation with self-supervised learning using a masked image modeling approach. Masked Autoencoders (MAE) extract meaningful normal features from unlabeled image samples which produces high reconstruction error for the abnormal pixels during reconstruction. To remove the need of using only ``normal\" data while training, we use an anomaly suppression loss mechanism that effectively minimizes the reconstruction of anomalous pixels and allows the model to learn anomalous areas without explicitly separating ``normal\" images for training. Evaluation on the Agriculture-Vision data challenge shows a mIOU score improvement in comparison to prior state of the art in unsupervised and self-supervised methods. A single model generalizes across all the anomaly categories in the Agri-Vision Challenge Dataset","sentences":["Detecting various types of stresses (nutritional, water, nitrogen, etc.) in agricultural fields is critical for farmers to ensure maximum productivity.","However, stresses show up in different shapes and sizes across different crop types and varieties.","Hence, this is posed as an anomaly detection task in agricultural images.","Accurate anomaly detection in agricultural UAV images is vital for early identification of field irregularities.","Traditional supervised learning faces challenges in adapting to diverse anomalies, necessitating extensive annotated data.","In this work, we overcome this limitation with self-supervised learning using a masked image modeling approach.","Masked Autoencoders (MAE) extract meaningful normal features from unlabeled image samples which produces high reconstruction error for the abnormal pixels during reconstruction.","To remove the need of using only ``normal\" data while training, we use an anomaly suppression loss mechanism that effectively minimizes the reconstruction of anomalous pixels and allows the model to learn anomalous areas without explicitly separating ``normal\" images for training.","Evaluation on the Agriculture-Vision data challenge shows a mIOU score improvement in comparison to prior state of the art in unsupervised and self-supervised methods.","A single model generalizes across all the anomaly categories in the Agri-Vision Challenge Dataset"],"url":"http://arxiv.org/abs/2404.08931v1","category":"cs.CV"}
{"created":"2024-04-15 12:53:26","title":"Quenched Mixing Rates for Doubly Intermittent Maps","abstract":"We study quenched mixing rates for two classes of random interval maps characterized by the presence of two indifferent fixed points and singular points. Using a random tower construction we prove the existence of an equivariant absolutely continuous probability measures with polynomial decay of correlations.","sentences":["We study quenched mixing rates for two classes of random interval maps characterized by the presence of two indifferent fixed points and singular points.","Using a random tower construction we prove the existence of an equivariant absolutely continuous probability measures with polynomial decay of correlations."],"url":"http://arxiv.org/abs/2404.09751v1","category":"math.DS"}
{"created":"2024-04-15 12:51:03","title":"Connectivity in Symmetric Semi-Algebraic Sets","abstract":"Semi-algebraic set is a subset of the real space defined by polynomial equations and inequalities. In this paper, we consider the problem of deciding whether two given points in a semi-algebraic set are connected. We restrict to the case when all equations and inequalities are invariant under the action of the symmetric group and their degrees at most $d<n$, where $n$ is the number of variables. Additionally, we assume that the two points are in the same fundamental domain of the action of the symmetric group, by assuming that the coordinates of two given points are sorted in non-decreasing order. We construct and analyze an algorithm that solves this problem, by taking advantage of the group action, and has a complexity being polynomial in $n$.","sentences":["Semi-algebraic set is a subset of the real space defined by polynomial equations and inequalities.","In this paper, we consider the problem of deciding whether two given points in a semi-algebraic set are connected.","We restrict to the case when all equations and inequalities are invariant under the action of the symmetric group and their degrees at most $d<n$, where $n$ is the number of variables.","Additionally, we assume that the two points are in the same fundamental domain of the action of the symmetric group, by assuming that the coordinates of two given points are sorted in non-decreasing order.","We construct and analyze an algorithm that solves this problem, by taking advantage of the group action, and has a complexity being polynomial in $n$."],"url":"http://arxiv.org/abs/2404.09749v1","category":"cs.SC"}
{"created":"2024-04-15 12:50:15","title":"Optimizing Off-Axis Fields for Vector Magnetometry with Point Defects","abstract":"Vector magnetometry is an essential tool in characterizing the distribution of currents and magnetization in a broad range of systems. Point defect sensors, like the nitrogen vacancy (NV) center in diamond, have demonstrated impressive sensitivity and spatial resolution for detecting these fields. Measuring the vector field at a single point in space using single defects, however, remains an outstanding challenge. We demonstrate that careful optimization of the static bias field can enable simultaneous measurement of multiple magnetic field components with enhanced sensitivity by leveraging the nonlinear Zeeman shift from transverse magnetic fields. This work quantifies the trade-off between the increased frequency shift from second-order Zeeman effects with decreasing contrast as off-axis field components increase, demonstrating the measurement of multiple components of the magnetic field from an exemplar antiferromagnet with a complex magnetic texture.","sentences":["Vector magnetometry is an essential tool in characterizing the distribution of currents and magnetization in a broad range of systems.","Point defect sensors, like the nitrogen vacancy (NV) center in diamond, have demonstrated impressive sensitivity and spatial resolution for detecting these fields.","Measuring the vector field at a single point in space using single defects, however, remains an outstanding challenge.","We demonstrate that careful optimization of the static bias field can enable simultaneous measurement of multiple magnetic field components with enhanced sensitivity by leveraging the nonlinear Zeeman shift from transverse magnetic fields.","This work quantifies the trade-off between the increased frequency shift from second-order Zeeman effects with decreasing contrast as off-axis field components increase, demonstrating the measurement of multiple components of the magnetic field from an exemplar antiferromagnet with a complex magnetic texture."],"url":"http://arxiv.org/abs/2404.09747v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-15 12:46:14","title":"Relatively Anosov groups: finiteness, measure of maximal entropy, and reparameterization","abstract":"For a geometrically finite Kleinian group $\\Gamma$, the Bowen-Margulis-Sullivan measure is finite and is the unique measure of maximal entropy for the geodesic flow. Moreover, it is strongly mixing. We obtain a higher rank analogue of this theorem. Let $\\Gamma$ be a relatively Anosov subgroup of a semisimple real algebraic group $G$. For every linear form $\\psi$ tangent to the growth indicator of $\\Gamma$, there is a canonical one-dimensional flow space $(\\Omega_\\psi, m_\\psi, \\phi_t)$ such that the non-wandering set $\\Omega_\\Gamma$ for the diagonal flow is a $\\ker\\psi$-vector bundle over $\\Omega_\\psi$ and the Bowen-Margulis-Sullivan measure on $\\Omega_\\Gamma$ is the product $m_\\psi \\otimes \\mathrm{Leb}_{\\ker\\psi}$. We show that $m_{\\psi}$ is of finite measure and is the unique measure of maximal entropy for the flow $\\{\\phi_t\\}$. Moreover, $(\\Omega_{\\psi}, m_{\\psi}, \\phi_t)$ is strongly mixing. The main ingredient is our construction of a reparameterization of the flow $\\{\\phi_t\\}$ on $\\Omega_{\\psi}$ by the geodesic flow on the Groves-Manning cusp space of $\\Gamma$, which has an exponentially expanding property along unstable foliations.","sentences":["For a geometrically finite Kleinian group $\\Gamma$, the Bowen-Margulis-Sullivan measure is finite and is the unique measure of maximal entropy for the geodesic flow.","Moreover, it is strongly mixing.","We obtain a higher rank analogue of this theorem.","Let $\\Gamma$ be a relatively Anosov subgroup of a semisimple real algebraic group $G$.","For every linear form $\\psi$ tangent to the growth indicator of $\\Gamma$, there is a canonical one-dimensional flow space $(\\Omega_\\psi, m_\\psi, \\phi_t)$ such that the non-wandering set $\\Omega_\\Gamma$ for the diagonal flow is a $\\ker\\psi$-vector bundle over $\\Omega_\\psi$ and the Bowen-Margulis-Sullivan measure on $\\Omega_\\Gamma$ is the product $m_\\psi \\otimes \\mathrm{Leb}_{\\ker\\psi}$.","We show that $m_{\\psi}$ is of finite measure and is the unique measure of maximal entropy for the flow $\\{\\phi_t\\}$. Moreover, $(\\Omega_{\\psi}, m_{\\psi}, \\phi_t)$ is strongly mixing.","The main ingredient is our construction of a reparameterization of the flow $\\{\\phi_t\\}$ on $\\Omega_{\\psi}$ by the geodesic flow on the Groves-Manning cusp space of $\\Gamma$, which has an exponentially expanding property along unstable foliations."],"url":"http://arxiv.org/abs/2404.09745v1","category":"math.DS"}
{"created":"2024-04-15 12:40:17","title":"Simulation of perovskite thin layer crystallization with varying evaporation rates","abstract":"Perovskite solar cells (PSC) are promising potential competitors to established photovoltaic technologies due to their superior efficiency and low-cost solution processability. However, the limited understanding of the crystallization behaviour hinders the technological transition from lab-scale cells to modules. In this work, we perform Phase Field (PF) simulations of the doctor-bladed film formation to obtain mechanistic and morphological information that is experimentally challenging to access. PF simulations are validated extensively using in- and ex-situ experiments for different solvent evaporation rates. The well-known transition from a film with many pinholes, for a low evaporation rate, to a smooth film, for high evaporation rates, is recovered in simulation and experiment. From the simulation, the transition can be assigned to the change in the ratio of evaporation to crystallization rate because of two distinct mechanisms. Firstly, for larger evaporation rates, nuclei appear at higher concentrations, which favors nucleation as compared to growth. Secondly, the growth of the crystals is confined in a thinner film, which limits their vertical size. Both effects are expected to be valid independent of the specific chemistry of the chosen experimental system, as long as the evaporation time of the solvent is comparable to the crystallization time.","sentences":["Perovskite solar cells (PSC) are promising potential competitors to established photovoltaic technologies due to their superior efficiency and low-cost solution processability.","However, the limited understanding of the crystallization behaviour hinders the technological transition from lab-scale cells to modules.","In this work, we perform Phase Field (PF) simulations of the doctor-bladed film formation to obtain mechanistic and morphological information that is experimentally challenging to access.","PF simulations are validated extensively using in- and ex-situ experiments for different solvent evaporation rates.","The well-known transition from a film with many pinholes, for a low evaporation rate, to a smooth film, for high evaporation rates, is recovered in simulation and experiment.","From the simulation, the transition can be assigned to the change in the ratio of evaporation to crystallization rate because of two distinct mechanisms.","Firstly, for larger evaporation rates, nuclei appear at higher concentrations, which favors nucleation as compared to growth.","Secondly, the growth of the crystals is confined in a thinner film, which limits their vertical size.","Both effects are expected to be valid independent of the specific chemistry of the chosen experimental system, as long as the evaporation time of the solvent is comparable to the crystallization time."],"url":"http://arxiv.org/abs/2404.09739v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-15 12:34:51","title":"Weighted Sum-Rate Maximization for Movable Antenna-Enhanced Wireless Networks","abstract":"This letter investigates the weighted sum rate maximization problem in movable antenna (MA)-enhanced systems. To reduce the computational complexity, we transform it into a more tractable weighted minimum mean square error (WMMSE) problem well-suited for MA. We then adopt the WMMSE algorithm and majorization-minimization algorithm to optimize the beamforming and antenna positions, respectively. Moreover, we propose a planar movement mode, which constrains each MA to a specified area, we obtain a low-complexity closed-form solution. Numerical results demonstrate that the MA-enhanced system outperforms the conventional system. Besides, the computation time for the planar movement mode is reduced by approximately 30\\% at a little performance expense.","sentences":["This letter investigates the weighted sum rate maximization problem in movable antenna (MA)-enhanced systems.","To reduce the computational complexity, we transform it into a more tractable weighted minimum mean square error (WMMSE) problem well-suited for MA.","We then adopt the WMMSE algorithm and majorization-minimization algorithm to optimize the beamforming and antenna positions, respectively.","Moreover, we propose a planar movement mode, which constrains each MA to a specified area, we obtain a low-complexity closed-form solution.","Numerical results demonstrate that the MA-enhanced system outperforms the conventional system.","Besides, the computation time for the planar movement mode is reduced by approximately 30\\% at a little performance expense."],"url":"http://arxiv.org/abs/2404.09734v1","category":"cs.IT"}
{"created":"2024-04-15 12:29:16","title":"Amplitude-Phase Fusion for Enhanced Electrocardiogram Morphological Analysis","abstract":"Considering the variability of amplitude and phase patterns in electrocardiogram (ECG) signals due to cardiac activity and individual differences, existing entropy-based studies have not fully utilized these two patterns and lack integration. To address this gap, this paper proposes a novel fusion entropy metric, morphological ECG entropy (MEE) for the first time, specifically designed for ECG morphology, to comprehensively describe the fusion of amplitude and phase patterns. MEE is computed based on beat-level samples, enabling detailed analysis of each cardiac cycle. Experimental results demonstrate that MEE achieves rapid, accurate, and label-free localization of abnormal ECG arrhythmia regions. Furthermore, MEE provides a method for assessing sample diversity, facilitating compression of imbalanced training sets (via representative sample selection), and outperforms random pruning. Additionally, MEE exhibits the ability to describe areas of poor quality. By discussing, it proves the robustness of MEE value calculation to noise interference and its low computational complexity. Finally, we integrate this method into a clinical interactive interface to provide a more convenient and intuitive user experience. These findings indicate that MEE serves as a valuable clinical descriptor for ECG characterization. The implementation code can be referenced at the following link: https://github.com/fdu-harry/ECG-MEE-metric.","sentences":["Considering the variability of amplitude and phase patterns in electrocardiogram (ECG) signals due to cardiac activity and individual differences, existing entropy-based studies have not fully utilized these two patterns and lack integration.","To address this gap, this paper proposes a novel fusion entropy metric, morphological ECG entropy (MEE) for the first time, specifically designed for ECG morphology, to comprehensively describe the fusion of amplitude and phase patterns.","MEE is computed based on beat-level samples, enabling detailed analysis of each cardiac cycle.","Experimental results demonstrate that MEE achieves rapid, accurate, and label-free localization of abnormal ECG arrhythmia regions.","Furthermore, MEE provides a method for assessing sample diversity, facilitating compression of imbalanced training sets (via representative sample selection), and outperforms random pruning.","Additionally, MEE exhibits the ability to describe areas of poor quality.","By discussing, it proves the robustness of MEE value calculation to noise interference and its low computational complexity.","Finally, we integrate this method into a clinical interactive interface to provide a more convenient and intuitive user experience.","These findings indicate that MEE serves as a valuable clinical descriptor for ECG characterization.","The implementation code can be referenced at the following link: https://github.com/fdu-harry/ECG-MEE-metric."],"url":"http://arxiv.org/abs/2404.09729v1","category":"eess.SP"}
{"created":"2024-04-15 12:27:24","title":"Homogenization of Phase Transition Problems with Evolving Microstructure","abstract":"We consider the mathematical analysis and homogenization of a moving boundary problem posed for a highly heterogeneous, periodically perforated domain. More specifically, we are looking at a one-phase thermo-elasticity system with phase transformations where small inclusions, initially periodically distributed, are growing or shrinking based on a kinetic under-cooling-type law and where surface stresses are created based on the curvature of the phase interface. This growth is assumed to be uniform in each individual cell of the the perforated domain. After transforming to the initial reference configuration (utilizing the Hanzawa transformation), we use the contraction mapping principle to show the existence of a unique solution for a possibly small but $\\varespilon$-independent time interval ($\\varespilon$ is here the scale of heterogeneity). In the homogenization limit, we discover a macroscopic thermo-elasticity problem which is strongly non-linearly coupled (via an internal parameter called height function) to local changes in geometry. As a direct byproduct of the mathematical analysis work, we present an alternative equivalent formulation which lends itself to an effective precomputing strategy that is very much needed as the limit problem is computationally expensive.","sentences":["We consider the mathematical analysis and homogenization of a moving boundary problem posed for a highly heterogeneous, periodically perforated domain.","More specifically, we are looking at a one-phase thermo-elasticity system with phase transformations where small inclusions, initially periodically distributed, are growing or shrinking based on a kinetic under-cooling-type law and where surface stresses are created based on the curvature of the phase interface.","This growth is assumed to be uniform in each individual cell of the the perforated domain.","After transforming to the initial reference configuration (utilizing the Hanzawa transformation), we use the contraction mapping principle to show the existence of a unique solution for a possibly small but $\\varespilon$-independent time interval ($\\varespilon$ is here the scale of heterogeneity).","In the homogenization limit, we discover a macroscopic thermo-elasticity problem which is strongly non-linearly coupled (via an internal parameter called height function) to local changes in geometry.","As a direct byproduct of the mathematical analysis work, we present an alternative equivalent formulation which lends itself to an effective precomputing strategy that is very much needed as the limit problem is computationally expensive."],"url":"http://arxiv.org/abs/2404.09726v1","category":"math.AP"}
{"created":"2024-04-15 12:27:07","title":"Privacy-Preserving Federated Unlearning with Certified Client Removal","abstract":"In recent years, Federated Unlearning (FU) has gained attention for addressing the removal of a client's influence from the global model in Federated Learning (FL) systems, thereby ensuring the ``right to be forgotten\" (RTBF). State-of-the-art methods for unlearning use historical data from FL clients, such as gradients or locally trained models. However, studies have revealed significant information leakage in this setting, with the possibility of reconstructing a user's local data from their uploaded information. Addressing this, we propose Starfish, a privacy-preserving federated unlearning scheme using Two-Party Computation (2PC) techniques and shared historical client data between two non-colluding servers. Starfish builds upon existing FU methods to ensure privacy in unlearning processes. To enhance the efficiency of privacy-preserving FU evaluations, we suggest 2PC-friendly alternatives for certain FU algorithm operations. We also implement strategies to reduce costs associated with 2PC operations and lessen cumulative approximation errors. Moreover, we establish a theoretical bound for the difference between the unlearned global model via Starfish and a global model retrained from scratch for certified client removal. Our theoretical and experimental analyses demonstrate that Starfish achieves effective unlearning with reasonable efficiency, maintaining privacy and security in FL systems.","sentences":["In recent years, Federated Unlearning (FU) has gained attention for addressing the removal of a client's influence from the global model in Federated Learning (FL) systems, thereby ensuring the ``right to be forgotten\" (RTBF).","State-of-the-art methods for unlearning use historical data from FL clients, such as gradients or locally trained models.","However, studies have revealed significant information leakage in this setting, with the possibility of reconstructing a user's local data from their uploaded information.","Addressing this, we propose Starfish, a privacy-preserving federated unlearning scheme using Two-Party Computation (2PC) techniques and shared historical client data between two non-colluding servers.","Starfish builds upon existing FU methods to ensure privacy in unlearning processes.","To enhance the efficiency of privacy-preserving FU evaluations, we suggest 2PC-friendly alternatives for certain FU algorithm operations.","We also implement strategies to reduce costs associated with 2PC operations and lessen cumulative approximation errors.","Moreover, we establish a theoretical bound for the difference between the unlearned global model via Starfish and a global model retrained from scratch for certified client removal.","Our theoretical and experimental analyses demonstrate that Starfish achieves effective unlearning with reasonable efficiency, maintaining privacy and security in FL systems."],"url":"http://arxiv.org/abs/2404.09724v1","category":"cs.CR"}
{"created":"2024-04-15 12:22:03","title":"Symmetry of uniformly rotating solutions for the vortex-wave system","abstract":"In this paper, we study the radial symmetry properties of stationary and uniformly rotating solutions of the vortex-wave system introduced by Marchioro and Pulvirenti \\cite{Mar1}. We show that every uniformly rotating patch $\\left(D,x_1,x_2,..,x_k\\right)$ with angular velocity $\\Omega\\leq 0$ must be radial with respect to the only point vortex $x_1$, implying that $k=1$. In other words, the background vorticity consists of finite nested annulus and the point vortex is located at the center of these annulus. In contrast to the case where the angular velocity is non-positive, we prove that there exists a family of uniformly rotating patch $(D^n, x_1^n)_n$ solutions, which are associated with a sequence of positive angular velocities $\\{\\Omega_n\\}$ and are not annular. Furthermore, we find that the set of bifurcating angular velocities $\\{\\Omega_n\\}$ is dense in the interval $(0,+\\infty)$, a novel feature that distinguishes this behavior from that observed in the classical Euler equation and gSQG equation.","sentences":["In this paper, we study the radial symmetry properties of stationary and uniformly rotating solutions of the vortex-wave system introduced by Marchioro and Pulvirenti \\cite{Mar1}.","We show that every uniformly rotating patch $\\left(D,x_1,x_2,..",",x_k\\right)$ with angular velocity $\\Omega\\leq 0$ must be radial with respect to the only point vortex $x_1$, implying that $k=1$. In other words, the background vorticity consists of finite nested annulus and the point vortex is located at the center of these annulus.","In contrast to the case where the angular velocity is non-positive, we prove that there exists a family of uniformly rotating patch $(D^n, x_1^n)_n$ solutions, which are associated with a sequence of positive angular velocities $\\{\\Omega_n\\}$ and are not annular.","Furthermore, we find that the set of bifurcating angular velocities $\\{\\Omega_n\\}$ is dense in the interval $(0,+\\infty)$, a novel feature that distinguishes this behavior from that observed in the classical Euler equation and gSQG equation."],"url":"http://arxiv.org/abs/2404.09719v1","category":"math.AP"}
{"created":"2024-04-15 12:21:07","title":"Counting, mixing and equidistribution for GPS systems with applications to relatively Anosov groups","abstract":"We establish counting, mixing and equidistribution results for finite BMS measures on flow spaces associated to geometrically finite convergence group actions. We show that, in particular, these results apply to flow spaces associated to relatively Anosov groups.","sentences":["We establish counting, mixing and equidistribution results for finite BMS measures on flow spaces associated to geometrically finite convergence group actions.","We show that, in particular, these results apply to flow spaces associated to relatively Anosov groups."],"url":"http://arxiv.org/abs/2404.09718v1","category":"math.DS"}
{"created":"2024-04-15 12:06:22","title":"Kernel-based learning with guarantees for multi-agent applications","abstract":"This paper addresses a kernel-based learning problem for a network of agents locally observing a latent multidimensional, nonlinear phenomenon in a noisy environment. We propose a learning algorithm that requires only mild a priori knowledge about the phenomenon under investigation and delivers a model with corresponding non-asymptotic high probability error bounds. Both non-asymptotic analysis of the method and numerical simulation results are presented and discussed in the paper.","sentences":["This paper addresses a kernel-based learning problem for a network of agents locally observing a latent multidimensional, nonlinear phenomenon in a noisy environment.","We propose a learning algorithm that requires only mild a priori knowledge about the phenomenon under investigation and delivers a model with corresponding non-asymptotic high probability error bounds.","Both non-asymptotic analysis of the method and numerical simulation results are presented and discussed in the paper."],"url":"http://arxiv.org/abs/2404.09708v1","category":"cs.MA"}
{"created":"2024-04-15 12:02:41","title":"Floquet expansion by counting pump photons","abstract":"Periodically-driven systems engender a rich competition between the time scales of the drives and those of the system, leading to a limited ability to describe the system in full. We present a framework for the description of interacting bosonic driven systems via a Floquet expansion on top of a quantization that \"counts\" the drive photons, and provide compelling arguments for the superior performance of our method relative to standard Floquet approaches. Crucially, our approach extends beyond the rotating wave approximation and addresses the long-standing issue of mismatch between the quantum Floquet formalism and its classical counterpart. We, furthermore, pinpoint key corrections to the positions of multiphoton resonances, which are commonly used in the calibration and operation of qubit architectures.","sentences":["Periodically-driven systems engender a rich competition between the time scales of the drives and those of the system, leading to a limited ability to describe the system in full.","We present a framework for the description of interacting bosonic driven systems via a Floquet expansion on top of a quantization that \"counts\" the drive photons, and provide compelling arguments for the superior performance of our method relative to standard Floquet approaches.","Crucially, our approach extends beyond the rotating wave approximation and addresses the long-standing issue of mismatch between the quantum Floquet formalism and its classical counterpart.","We, furthermore, pinpoint key corrections to the positions of multiphoton resonances, which are commonly used in the calibration and operation of qubit architectures."],"url":"http://arxiv.org/abs/2404.09704v1","category":"quant-ph"}
{"created":"2024-04-15 11:59:19","title":"HSIDMamba: Exploring Bidirectional State-Space Models for Hyperspectral Denoising","abstract":"Effectively discerning spatial-spectral dependencies in HSI denoising is crucial, but prevailing methods using convolution or transformers still face computational efficiency limitations. Recently, the emerging Selective State Space Model(Mamba) has risen with its nearly linear computational complexity in processing natural language sequences, which inspired us to explore its potential in handling long spectral sequences. In this paper, we propose HSIDMamba(HSDM), tailored to exploit the linear complexity for effectively capturing spatial-spectral dependencies in HSI denoising. In particular, HSDM comprises multiple Hyperspectral Continuous Scan Blocks, incorporating BCSM(Bidirectional Continuous Scanning Mechanism), scale residual, and spectral attention mechanisms to enhance the capture of long-range and local spatial-spectral information. BCSM strengthens spatial-spectral interactions by linking forward and backward scans and enhancing information from eight directions through SSM, significantly enhancing the perceptual capability of HSDM and improving denoising performance more effectively. Extensive evaluations against HSI denoising benchmarks validate the superior performance of HSDM, achieving state-of-the-art results in performance and surpassing the efficiency of the latest transformer architectures by $30\\%$.","sentences":["Effectively discerning spatial-spectral dependencies in HSI denoising is crucial, but prevailing methods using convolution or transformers still face computational efficiency limitations.","Recently, the emerging Selective State Space Model(Mamba) has risen with its nearly linear computational complexity in processing natural language sequences, which inspired us to explore its potential in handling long spectral sequences.","In this paper, we propose HSIDMamba(HSDM), tailored to exploit the linear complexity for effectively capturing spatial-spectral dependencies in HSI denoising.","In particular, HSDM comprises multiple Hyperspectral Continuous Scan Blocks, incorporating BCSM(Bidirectional Continuous Scanning Mechanism), scale residual, and spectral attention mechanisms to enhance the capture of long-range and local spatial-spectral information.","BCSM strengthens spatial-spectral interactions by linking forward and backward scans and enhancing information from eight directions through SSM, significantly enhancing the perceptual capability of HSDM and improving denoising performance more effectively.","Extensive evaluations against HSI denoising benchmarks validate the superior performance of HSDM, achieving state-of-the-art results in performance and surpassing the efficiency of the latest transformer architectures by $30\\%$."],"url":"http://arxiv.org/abs/2404.09697v1","category":"cs.CV"}
{"created":"2024-04-15 11:47:17","title":"Structure and dynamics of active string fluids and gels formed by dipolar active Brownian particles","abstract":"Self-propelled particles possessing permanent magnetic dipole moments occur naturally in magnetotactic bacteria and in man-made systems like active colloids or micro-robots. Yet, understanding the interplay between self-propulsion and anisotropic dipole-dipole interactions on dynamic self-assembly in three dimensions (3D) remains poorly understood. We conduct Brownian dynamics simulations on active dipolar particles in 3D, focusing on the low-density regime, where dipolar hard spheres tend to form chain-like aggregates and percolated networks with increasing dipolar coupling strength. Strong active forces override dipolar attractions, effectively inhibiting chain-like aggregation and network formation. Conversely, activating particles with low to moderate forces yields an active fluid consisting of chains and rings and an active gel with increasing dipolar coupling strength. Although the overall structure of an active gel remains interconnected, the network experiences more frequent configurational rearrangements due to the reduced bond lifetime of active dipolar particles. Consequently, particles exhibit enhanced translational and rotational diffusion within active fluid of strings and active gels compared to their passive counterparts. We quantify the influence of activity on aggregates topology, observing a transition from branched structures to unconnected chains and rings. Our findings are summarized in a state diagram, delineating the impact of dipolar coupling strength and active force magnitude on the system.","sentences":["Self-propelled particles possessing permanent magnetic dipole moments occur naturally in magnetotactic bacteria and in man-made systems like active colloids or micro-robots.","Yet, understanding the interplay between self-propulsion and anisotropic dipole-dipole interactions on dynamic self-assembly in three dimensions (3D) remains poorly understood.","We conduct Brownian dynamics simulations on active dipolar particles in 3D, focusing on the low-density regime, where dipolar hard spheres tend to form chain-like aggregates and percolated networks with increasing dipolar coupling strength.","Strong active forces override dipolar attractions, effectively inhibiting chain-like aggregation and network formation.","Conversely, activating particles with low to moderate forces yields an active fluid consisting of chains and rings and an active gel with increasing dipolar coupling strength.","Although the overall structure of an active gel remains interconnected, the network experiences more frequent configurational rearrangements due to the reduced bond lifetime of active dipolar particles.","Consequently, particles exhibit enhanced translational and rotational diffusion within active fluid of strings and active gels compared to their passive counterparts.","We quantify the influence of activity on aggregates topology, observing a transition from branched structures to unconnected chains and rings.","Our findings are summarized in a state diagram, delineating the impact of dipolar coupling strength and active force magnitude on the system."],"url":"http://arxiv.org/abs/2404.09693v1","category":"cond-mat.soft"}
{"created":"2024-04-15 11:38:28","title":"UAV Navigation in Tunnels with 2D tilted LiDARs","abstract":"Navigation of UAVs in challenging environments like tunnels or mines, where it is not possible to use GNSS methods to self-localize, illumination may be uneven or nonexistent, and wall features are likely to be scarce, is a complex task, especially if the navigation has to be done at high speed. In this paper we propose a novel proof-of-concept navigation technique for UAVs based on the use of LiDAR information through the joint use of geometric and machine-learning algorithms. The perceived information is processed by a deep neural network to establish the yaw of the UAV with respect to the tunnel's longitudinal axis, in order to adjust the direction of navigation. Additionally, a geometric method is used to compute the safest location inside the tunnel (i.e. the one that maximizes the distance to the closest obstacle). This information proves to be sufficient for simple yet effective navigation in straight and curved tunnels.","sentences":["Navigation of UAVs in challenging environments like tunnels or mines, where it is not possible to use GNSS methods to self-localize, illumination may be uneven or nonexistent, and wall features are likely to be scarce, is a complex task, especially if the navigation has to be done at high speed.","In this paper we propose a novel proof-of-concept navigation technique for UAVs based on the use of LiDAR information through the joint use of geometric and machine-learning algorithms.","The perceived information is processed by a deep neural network to establish the yaw of the UAV with respect to the tunnel's longitudinal axis, in order to adjust the direction of navigation.","Additionally, a geometric method is used to compute the safest location inside the tunnel (i.e. the one that maximizes the distance to the closest obstacle).","This information proves to be sufficient for simple yet effective navigation in straight and curved tunnels."],"url":"http://arxiv.org/abs/2404.09688v1","category":"cs.RO"}
{"created":"2024-04-15 11:17:29","title":"On the chemical potential and grand potential density of solids under non-hydrostatic stress","abstract":"Non-hydrostatic stress has a peculiar effect on the phase equilibrium between solids and liquids. This was already pointed out by Gibbs. Gibbs derived his formulation of the condition for liquid-solid coexistence applying a surface accretion process without imposing chemical equilibrium between liquid and solid. Adding particles to the bulk of a solid was not possible in his view at the time. Chemical potentials for solids were later introduced by material scientists. This required extending chemical and mechanical equilibrium with a third condition involving a relation between grand potential densities controlling the migration of the interface. These issues are investigated using a non-linear elastic continuum model (technically an open compressible neo-Hookean material) developed in a previous publication (M. Sprik, J. Chem. Phys. 155, (2021) 244701). In common with a liquid, the grand potential density of the model is equal to minus the mean pressure even if the stress is non-hydrostatic. Applying isothermal compression normal to a liquid-solid interface initially in hydrostatic equilibrium drives the system away from coexistence. We derive the Gibbs-Thomson correction to the pressure of the liquid required to restore phase equilibrium. We find that the coupling between chemical potential of the solid and shear stress is a purely non-linear effect.","sentences":["Non-hydrostatic stress has a peculiar effect on the phase equilibrium between solids and liquids.","This was already pointed out by Gibbs.","Gibbs derived his formulation of the condition for liquid-solid coexistence applying a surface accretion process without imposing chemical equilibrium between liquid and solid.","Adding particles to the bulk of a solid was not possible in his view at the time.","Chemical potentials for solids were later introduced by material scientists.","This required extending chemical and mechanical equilibrium with a third condition involving a relation between grand potential densities controlling the migration of the interface.","These issues are investigated using a non-linear elastic continuum model (technically an open compressible neo-Hookean material) developed in a previous publication (M. Sprik, J. Chem.","Phys. 155, (2021) 244701).","In common with a liquid, the grand potential density of the model is equal to minus the mean pressure even if the stress is non-hydrostatic.","Applying isothermal compression normal to a liquid-solid interface initially in hydrostatic equilibrium drives the system away from coexistence.","We derive the Gibbs-Thomson correction to the pressure of the liquid required to restore phase equilibrium.","We find that the coupling between chemical potential of the solid and shear stress is a purely non-linear effect."],"url":"http://arxiv.org/abs/2404.09678v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-15 11:09:11","title":"Astrochemistry of the molecular gas in Dusty Star-Forming Galaxies at the Cosmic Noon","abstract":"FIR and submm observations have established the fundamental role of dust-obscured star formation in the assembly of stellar mass over the past 12 billion years. At z between 2 and 4, the bulk of star formation is enshrouded in dust, and dusty star forming galaxies (DSFGs) contain about half of the total stellar mass density. Star formation develops in dense molecular clouds, and is regulated by a complex interplay between all the ISM components that contribute to the energy budget of a galaxy: gas, dust, cosmic rays, interstellar electromagnetic fields, gravitational field, dark matter. Molecular gas is the actual link between star forming gas and its complex environment, providing by far the richest amount of information about the star formation process. However, molecular lines interpretation requires complex modeling of astrochemical networks, which regulate the molecular formation and establishes molecular abundances in a cloud, and a modeling of the physical conditions of the gas in which molecular energy levels become populated. This paper critically reviews the main astrochemical parameters needed to get predictions about molecular signals in DSFGs. We review the current knowledge and the open questions about the interstellar medium of DSFGs, outlying the key role of molecular gas as a tracer and shaper of the star formation process.","sentences":["FIR and submm observations have established the fundamental role of dust-obscured star formation in the assembly of stellar mass over the past 12 billion years.","At z between 2 and 4, the bulk of star formation is enshrouded in dust, and dusty star forming galaxies (DSFGs) contain about half of the total stellar mass density.","Star formation develops in dense molecular clouds, and is regulated by a complex interplay between all the ISM components that contribute to the energy budget of a galaxy: gas, dust, cosmic rays, interstellar electromagnetic fields, gravitational field, dark matter.","Molecular gas is the actual link between star forming gas and its complex environment, providing by far the richest amount of information about the star formation process.","However, molecular lines interpretation requires complex modeling of astrochemical networks, which regulate the molecular formation and establishes molecular abundances in a cloud, and a modeling of the physical conditions of the gas in which molecular energy levels become populated.","This paper critically reviews the main astrochemical parameters needed to get predictions about molecular signals in DSFGs.","We review the current knowledge and the open questions about the interstellar medium of DSFGs, outlying the key role of molecular gas as a tracer and shaper of the star formation process."],"url":"http://arxiv.org/abs/2404.09673v1","category":"astro-ph.GA"}
{"created":"2024-04-15 11:02:36","title":"Exploring field-evolution and dynamical-capture coalescing binary black holes in GWTC-3","abstract":"The continuously expanding sample of gravitational-wave observations is revealing the formation and evolutionary mechanism of merging compact binaries. Two primary channels, namely, isolated field binary evolution and dynamical capture, are widely accepted as potential producers of merging binary black holes (BBHs), which are distinguishable with the spin-orientation distributions of the BBHs. We investigate the two formation channels in GWTC-3, with a dedicated semi-parametric population model, i.e., a mixture of two sub-populations with different spin-orientation distributions (one is nearly-aligned and the other is nearly-isotropic). It turns out that the two sub-populations have different mass and mass-ratio distributions. The nearly-aligned sub-population, which is consistent with the isolated field formation channels, has a less preference for symmetric systems, and likely dominate the 10-solar-mass peak in the primary-mass function. While the isotropic sub-population shows a stronger preference for symmetric systems, and mainly contribute to the 35-solar-mass peak in the primary-mass function, consistent with the dynamical channels. Moreover, our results show that the purely isotropic-spin and the single well-aligned (i.e., the width of $\\cos\\theta$ distribution $\\sigma_{\\rm t}<0.5$) scenario are ruled out (by a Bayes factor of $\\ln\\mathcal{B}=5.2$ and $\\ln\\mathcal{B}=9.8$).","sentences":["The continuously expanding sample of gravitational-wave observations is revealing the formation and evolutionary mechanism of merging compact binaries.","Two primary channels, namely, isolated field binary evolution and dynamical capture, are widely accepted as potential producers of merging binary black holes (BBHs), which are distinguishable with the spin-orientation distributions of the BBHs.","We investigate the two formation channels in GWTC-3, with a dedicated semi-parametric population model, i.e., a mixture of two sub-populations with different spin-orientation distributions (one is nearly-aligned and the other is nearly-isotropic).","It turns out that the two sub-populations have different mass and mass-ratio distributions.","The nearly-aligned sub-population, which is consistent with the isolated field formation channels, has a less preference for symmetric systems, and likely dominate the 10-solar-mass peak in the primary-mass function.","While the isotropic sub-population shows a stronger preference for symmetric systems, and mainly contribute to the 35-solar-mass peak in the primary-mass function, consistent with the dynamical channels.","Moreover, our results show that the purely isotropic-spin and the single well-aligned (i.e., the width of $\\cos\\theta$ distribution $\\sigma_{\\rm t}<0.5$) scenario are ruled out (by a Bayes factor of $\\ln\\mathcal{B}=5.2$ and $\\ln\\mathcal{B}=9.8$)."],"url":"http://arxiv.org/abs/2404.09668v1","category":"astro-ph.HE"}
{"created":"2024-04-15 10:56:24","title":"Demonstration of a Networked Music Performance Experience with MEVO","abstract":"In this paper we present a Networked Music Performance system currently under development at Politecnico di Torino. We demonstrate its use in a distributed concert held in June 2023, which featured three musicians in Turin (Italy) and three musicians in Wroc{\\l}aw (Poland). Although in its early stages, the system proved to be already stable enough to appear transparent to the remote audience.","sentences":["In this paper we present a Networked Music Performance system currently under development at Politecnico di Torino.","We demonstrate its use in a distributed concert held in June 2023, which featured three musicians in Turin (Italy) and three musicians in Wroc{\\l}aw (Poland).","Although in its early stages, the system proved to be already stable enough to appear transparent to the remote audience."],"url":"http://arxiv.org/abs/2404.09665v1","category":"cs.NI"}
{"created":"2024-04-15 10:53:13","title":"Quantum Computers, Quantum Computing and Quantum Thermodynamics","abstract":"Quantum thermodynamics aims at extending standard thermodynamics and non-equilibrium statistical physics to systems with sizes well below the thermodynamic limit. A rapidly evolving research field, which promises to change our understanding of the foundations of physics, while enabling the discovery of novel thermodynamic techniques and applications at the nanoscale. Thermal management turned into a major obstacle in pushing the limits of conventional digital computers, and it will represent a crucial issue also for quantum computers. The practical realization of quantum computers with superconducting loops requires working at cryogenic temperatures to eliminate thermal noise; ion-trap qubits need as well low temperatures to minimize collisional noise; in both cases, the sub-nanometric sizes also bring about thermal broadening of the quantum states. A number of thermal and thermodynamic questions therefore take center stage, such as quantum re-definitions of work and heat, thermalization and randomization of quantum states, the overlap of quantum and thermal fluctuations, and many other, even including a proper definition of temperature for the small open systems constantly out of equilibrium that are the qubits. This overview provides an introductory perspective on a selection of current trends in quantum thermodynamics and their impact on quantum computers and quantum computing, with a language accessible also to postgraduate students and researchers from different fields.","sentences":["Quantum thermodynamics aims at extending standard thermodynamics and non-equilibrium statistical physics to systems with sizes well below the thermodynamic limit.","A rapidly evolving research field, which promises to change our understanding of the foundations of physics, while enabling the discovery of novel thermodynamic techniques and applications at the nanoscale.","Thermal management turned into a major obstacle in pushing the limits of conventional digital computers, and it will represent a crucial issue also for quantum computers.","The practical realization of quantum computers with superconducting loops requires working at cryogenic temperatures to eliminate thermal noise; ion-trap qubits need as well low temperatures to minimize collisional noise; in both cases, the sub-nanometric sizes also bring about thermal broadening of the quantum states.","A number of thermal and thermodynamic questions therefore take center stage, such as quantum re-definitions of work and heat, thermalization and randomization of quantum states, the overlap of quantum and thermal fluctuations, and many other, even including a proper definition of temperature for the small open systems constantly out of equilibrium that are the qubits.","This overview provides an introductory perspective on a selection of current trends in quantum thermodynamics and their impact on quantum computers and quantum computing, with a language accessible also to postgraduate students and researchers from different fields."],"url":"http://arxiv.org/abs/2404.09663v1","category":"quant-ph"}
{"created":"2024-04-15 10:33:35","title":"Coherent control of levitated nanoparticles via dipole-dipole interaction","abstract":"We propose a scheme to create and transfer thermal squeezed states and random-phase coherent states in a system of two interacting levitated nanoparticles. In this coupled levitated system, we create a thermal squeezed state of motion in one of the nanoparticles by parametrically driving it and then transferring the state to the other nanoparticle with high fidelity. The transfer mechanism is based on inducing a non-reciprocal type of coupling in the system by suitably modulating the phases of the trapping lasers and the inter-particle distance between the levitated nanoparticles. This non-reciprocal coupling creates a unidirectional channel where information flows from one nanoparticle to the other nanoparticle but not vice versa, thereby allowing for transfer of mechanical states between the nanoparticles with high fidelity. We also affirm this transfer mechanism by creating and efficiently transferring a random-phase coherent state in the coupled levitated system. Further, we make use of the feedback nonlinearity and parametric driving to create simultaneous bistability in the coupled levitated system. Our results may have potential applications in quantum information processing, quantum metrology, and in exploring many-body physics under a controlled environment.","sentences":["We propose a scheme to create and transfer thermal squeezed states and random-phase coherent states in a system of two interacting levitated nanoparticles.","In this coupled levitated system, we create a thermal squeezed state of motion in one of the nanoparticles by parametrically driving it and then transferring the state to the other nanoparticle with high fidelity.","The transfer mechanism is based on inducing a non-reciprocal type of coupling in the system by suitably modulating the phases of the trapping lasers and the inter-particle distance between the levitated nanoparticles.","This non-reciprocal coupling creates a unidirectional channel where information flows from one nanoparticle to the other nanoparticle but not vice versa, thereby allowing for transfer of mechanical states between the nanoparticles with high fidelity.","We also affirm this transfer mechanism by creating and efficiently transferring a random-phase coherent state in the coupled levitated system.","Further, we make use of the feedback nonlinearity and parametric driving to create simultaneous bistability in the coupled levitated system.","Our results may have potential applications in quantum information processing, quantum metrology, and in exploring many-body physics under a controlled environment."],"url":"http://arxiv.org/abs/2404.09651v1","category":"quant-ph"}
{"created":"2024-04-15 10:32:31","title":"Impact of chirality on active Brownian particle: Exact moments in two and three dimensions","abstract":"In this work, we investigate the effects of chirality, accounting for translational diffusion, on active Brownian particles in two and three dimensions. Despite the inherent complexity in solving the Fokker-Planck equation, we demonstrate a Laplace transform method for precisely calculating the temporal evolution of various dynamic moments. Our analysis yields explicit expressions for multiple moments, such as the second and fourth moments of displacement, revealing the impact of persistence and chirality. These moments exhibit oscillatory behaviour, and excess kurtosis indicates deviations from the Gaussian distribution during intermediate time intervals.","sentences":["In this work, we investigate the effects of chirality, accounting for translational diffusion, on active Brownian particles in two and three dimensions.","Despite the inherent complexity in solving the Fokker-Planck equation, we demonstrate a Laplace transform method for precisely calculating the temporal evolution of various dynamic moments.","Our analysis yields explicit expressions for multiple moments, such as the second and fourth moments of displacement, revealing the impact of persistence and chirality.","These moments exhibit oscillatory behaviour, and excess kurtosis indicates deviations from the Gaussian distribution during intermediate time intervals."],"url":"http://arxiv.org/abs/2404.09650v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-15 10:31:31","title":"Closing Optical Bloch Equations in waveguide QED: Dynamics, Energetics","abstract":"Optical Bloch Equations (OBE) model the dynamics of a classically driven two-level atom coupled to a thermal electromagnetic field. From a global viewpoint, they derive from the unitary evolution of a closed, isolated atom-field system. We study the emergence of the OBE in the case where the driving and the thermal fields are confined in one spatial dimension -- a situation usually found in waveguide-QED. The joint atom-field system forms a \"one-dimensional atom\" (1D atom) whose closed dynamics can be solved, providing access to light-matter correlations. Such closure of the OBE unveils a new term capturing the driving of the atom by itself, or self-drive, which is proportional to the atom coherences in the energy basis. A 1D atom also constitutes an autonomous, energy-conserving system. Hence, energy exchanges between the atom and the field can be conveniently analyzed as closed first laws, where work-like (heat-like) flows stem from effective unitaries (correlations) exerted by one system on the other. We show that the closed and the open approaches only differ by the atom self-work, which yields a tighter expression of the second law. We quantitatively relate this tightening to the extra-knowledge acquired by closing the OBE. The concepts and effects we introduce deepen our understanding of thermodynamics in the quantum regime and its potential for energy management at quantum scales. They can be probed in state-of-the-art quantum hardware, e.g. superconducting and photonic circuits.","sentences":["Optical Bloch Equations (OBE) model the dynamics of a classically driven two-level atom coupled to a thermal electromagnetic field.","From a global viewpoint, they derive from the unitary evolution of a closed, isolated atom-field system.","We study the emergence of the OBE in the case where the driving and the thermal fields are confined in one spatial dimension -- a situation usually found in waveguide-QED.","The joint atom-field system forms a \"one-dimensional atom\" (1D atom) whose closed dynamics can be solved, providing access to light-matter correlations.","Such closure of the OBE unveils a new term capturing the driving of the atom by itself, or self-drive, which is proportional to the atom coherences in the energy basis.","A 1D atom also constitutes an autonomous, energy-conserving system.","Hence, energy exchanges between the atom and the field can be conveniently analyzed as closed first laws, where work-like (heat-like) flows stem from effective unitaries (correlations) exerted by one system on the other.","We show that the closed and the open approaches only differ by the atom self-work, which yields a tighter expression of the second law.","We quantitatively relate this tightening to the extra-knowledge acquired by closing the OBE.","The concepts and effects we introduce deepen our understanding of thermodynamics in the quantum regime and its potential for energy management at quantum scales.","They can be probed in state-of-the-art quantum hardware, e.g. superconducting and photonic circuits."],"url":"http://arxiv.org/abs/2404.09648v1","category":"quant-ph"}
{"created":"2024-04-15 10:00:41","title":"Optimal design of ride-pooling as on-demand feeder services","abstract":"The technology-enabled ride-pooling (RP) is designed as an on-demand feeder service to connect remote areas to transit terminals (or activity centers). We propose the so-called ``hold-dispatch'' operation strategy, which imposes a target number of shared rides (termed the ride-pooling size) for each vehicle to enhance RP's transportation efficiency. Analytical models are formulated at the planning level to estimate the costs of the RP operator and the patrons. Accordingly, the design problem is constructed to minimize the total system cost concerning the system layout (i.e., in terms of service zone partitioning), resource deployment (i.e., fleet size), and operational decision (i.e., ride-pooling size). The proposed models admit spatial heterogeneity arising from the non-uniformity of demand distributions and service locations, and can furnish heterogeneous designs. Closed-form formulas for the optimal zoning and fleet size are developed, which unveil fundamental insights regarding the impacts of key operating factors (e.g., demand density and distance to the terminal). Extensive numerical experiments demonstrate (i) the effectiveness of heterogeneous service designs and (ii) the advantage of the proposed RP service with hold-dispatch strategy over alternative designs studied in the literature, i.e., RP with a ``quick-dispatch'' strategy and flexible-route transit, in a wide range of operating scenarios. These findings can assist transportation network companies and transit agencies in successfully integrating RP and transit services.","sentences":["The technology-enabled ride-pooling (RP) is designed as an on-demand feeder service to connect remote areas to transit terminals (or activity centers).","We propose the so-called ``hold-dispatch'' operation strategy, which imposes a target number of shared rides (termed the ride-pooling size) for each vehicle to enhance RP's transportation efficiency.","Analytical models are formulated at the planning level to estimate the costs of the RP operator and the patrons.","Accordingly, the design problem is constructed to minimize the total system cost concerning the system layout (i.e., in terms of service zone partitioning), resource deployment (i.e., fleet size), and operational decision (i.e., ride-pooling size).","The proposed models admit spatial heterogeneity arising from the non-uniformity of demand distributions and service locations, and can furnish heterogeneous designs.","Closed-form formulas for the optimal zoning and fleet size are developed, which unveil fundamental insights regarding the impacts of key operating factors (e.g., demand density and distance to the terminal).","Extensive numerical experiments demonstrate (i) the effectiveness of heterogeneous service designs and (ii) the advantage of the proposed RP service with hold-dispatch strategy over alternative designs studied in the literature, i.e., RP with a ``quick-dispatch'' strategy and flexible-route transit, in a wide range of operating scenarios.","These findings can assist transportation network companies and transit agencies in successfully integrating RP and transit services."],"url":"http://arxiv.org/abs/2404.09630v1","category":"physics.soc-ph"}
{"created":"2024-04-15 09:57:51","title":"Centralization in Proof-of-Stake Blockchains: A Game-Theoretic Analysis of Bootstrapping Protocols","abstract":"Proof-of-stake (PoS) has emerged as a natural alternative to the resource-intensive Proof-of-Work (PoW) blockchain, as was recently seen with the Ethereum Merge. PoS-based blockchains require an initial stake distribution among the participants. Typically, this initial stake distribution is called bootstrapping. This paper argues that existing bootstrapping protocols are prone to centralization. To address centralization due to bootstrapping, we propose a novel game $\\Gamma_\\textsf{bootstrap}$. Next, we define three conditions: (i) Individual Rationality (IR), (ii) Incentive Compatibility (IC), and (iii) $(\\tau,\\delta,\\epsilon)-$ Decentralization that an \\emph{ideal} bootstrapping protocol must satisfy. $(\\tau,\\delta,\\epsilon)$ are certain parameters to quantify decentralization. Towards this, we propose a novel centralization metric, C-NORM, to measure centralization in a PoS System. We define a centralization game -- $\\Gamma_\\textsf{cent}$, to analyze the efficacy of centralization metrics. We show that C-NORM effectively captures centralization in the presence of strategic players capable of launching Sybil attacks. With C-NORM, we analyze popular bootstrapping protocols such as Airdrop and Proof-of-Burn (PoB) and prove that they do not satisfy IC and IR, respectively. Motivated by the Ethereum Merge, we study W2SB (a PoW-based bootstrapping protocol) and prove it is ideal. In addition, we conduct synthetic simulations to empirically validate that W2SB bootstrapped PoS is decentralized.","sentences":["Proof-of-stake (PoS) has emerged as a natural alternative to the resource-intensive Proof-of-Work (PoW) blockchain, as was recently seen with the Ethereum Merge.","PoS-based blockchains require an initial stake distribution among the participants.","Typically, this initial stake distribution is called bootstrapping.","This paper argues that existing bootstrapping protocols are prone to centralization.","To address centralization due to bootstrapping, we propose a novel game $\\Gamma_\\textsf{bootstrap}$.","Next, we define three conditions: (i) Individual Rationality (IR), (ii) Incentive Compatibility (IC), and (iii) $(\\tau,\\delta,\\epsilon)-$ Decentralization that an \\emph{ideal} bootstrapping protocol must satisfy.","$(\\tau,\\delta,\\epsilon)$ are certain parameters to quantify decentralization.","Towards this, we propose a novel centralization metric, C-NORM, to measure centralization in a PoS System.","We define a centralization game -- $\\Gamma_\\textsf{cent}$, to analyze the efficacy of centralization metrics.","We show that C-NORM effectively captures centralization in the presence of strategic players capable of launching Sybil attacks.","With C-NORM, we analyze popular bootstrapping protocols such as Airdrop and Proof-of-Burn (PoB) and prove that they do not satisfy IC and IR, respectively.","Motivated by the Ethereum Merge, we study W2SB (a PoW-based bootstrapping protocol) and prove it is ideal.","In addition, we conduct synthetic simulations to empirically validate that W2SB bootstrapped PoS is decentralized."],"url":"http://arxiv.org/abs/2404.09627v1","category":"cs.GT"}
{"created":"2024-04-15 09:49:17","title":"AAM-VDT: Vehicle Digital Twin for Tele-Operations in Advanced Air Mobility","abstract":"This study advanced tele-operations in Advanced Air Mobility (AAM) through the creation of a Vehicle Digital Twin (VDT) system for eVTOL aircraft, tailored to enhance remote control safety and efficiency, especially for Beyond Visual Line of Sight (BVLOS) operations. By synergizing digital twin technology with immersive Virtual Reality (VR) interfaces, we notably elevate situational awareness and control precision for remote operators. Our VDT framework integrates immersive tele-operation with a high-fidelity aerodynamic database, essential for authentically simulating flight dynamics and control tactics. At the heart of our methodology lies an eVTOL's high-fidelity digital replica, placed within a simulated reality that accurately reflects physical laws, enabling operators to manage the aircraft via a master-slave dynamic, substantially outperforming traditional 2D interfaces. The architecture of the designed system ensures seamless interaction between the operator, the digital twin, and the actual aircraft, facilitating exact, instantaneous feedback. Experimental assessments, involving propulsion data gathering, simulation database fidelity verification, and tele-operation testing, verify the system's capability in precise control command transmission and maintaining the digital-physical eVTOL synchronization. Our findings underscore the VDT system's potential in augmenting AAM efficiency and safety, paving the way for broader digital twin application in autonomous aerial vehicles.","sentences":["This study advanced tele-operations in Advanced Air Mobility (AAM) through the creation of a Vehicle Digital Twin (VDT) system for eVTOL aircraft, tailored to enhance remote control safety and efficiency, especially for Beyond Visual Line of Sight (BVLOS) operations.","By synergizing digital twin technology with immersive Virtual Reality (VR) interfaces, we notably elevate situational awareness and control precision for remote operators.","Our VDT framework integrates immersive tele-operation with a high-fidelity aerodynamic database, essential for authentically simulating flight dynamics and control tactics.","At the heart of our methodology lies an eVTOL's high-fidelity digital replica, placed within a simulated reality that accurately reflects physical laws, enabling operators to manage the aircraft via a master-slave dynamic, substantially outperforming traditional 2D interfaces.","The architecture of the designed system ensures seamless interaction between the operator, the digital twin, and the actual aircraft, facilitating exact, instantaneous feedback.","Experimental assessments, involving propulsion data gathering, simulation database fidelity verification, and tele-operation testing, verify the system's capability in precise control command transmission and maintaining the digital-physical eVTOL synchronization.","Our findings underscore the VDT system's potential in augmenting AAM efficiency and safety, paving the way for broader digital twin application in autonomous aerial vehicles."],"url":"http://arxiv.org/abs/2404.09621v1","category":"eess.SY"}
{"created":"2024-04-15 09:48:11","title":"Leveraging the Doppler Effect for Channel Charting","abstract":"Channel Charting is a dimensionality reduction technique that reconstructs a map of the radio environment from similarity relationships found in channel state information. Distances in the channel chart are often computed based on some dissimilarity metric, which can be derived from angular-domain information, channel impulse responses, measured phase differences or simply timestamps. Using such information implicitly makes strong assumptions about the level of phase and time synchronization between base station antennas or assumes approximately constant transmitter velocity. Many practical systems, however, may not provide phase and time synchronization and single-antenna base stations may not even have angular-domain information. We propose a Doppler effect-based loss function for Channel Charting that only requires frequency synchronization between spatially distributed base station antennas, which is a much weaker assumption. We use a dataset measured in an indoor environment to demonstrate that the proposed method is practically feasible with just four base station antennas, that it produces a channel chart that is suitable for localization in the global coordinate frame and that it outperforms other state-of-the-art methods under the given limitations.","sentences":["Channel Charting is a dimensionality reduction technique that reconstructs a map of the radio environment from similarity relationships found in channel state information.","Distances in the channel chart are often computed based on some dissimilarity metric, which can be derived from angular-domain information, channel impulse responses, measured phase differences or simply timestamps.","Using such information implicitly makes strong assumptions about the level of phase and time synchronization between base station antennas or assumes approximately constant transmitter velocity.","Many practical systems, however, may not provide phase and time synchronization and single-antenna base stations may not even have angular-domain information.","We propose a Doppler effect-based loss function for Channel Charting that only requires frequency synchronization between spatially distributed base station antennas, which is a much weaker assumption.","We use a dataset measured in an indoor environment to demonstrate that the proposed method is practically feasible with just four base station antennas, that it produces a channel chart that is suitable for localization in the global coordinate frame and that it outperforms other state-of-the-art methods under the given limitations."],"url":"http://arxiv.org/abs/2404.09620v1","category":"cs.IT"}
{"created":"2024-04-15 09:14:10","title":"Curvature of Gaussian quantum states","abstract":"The space of quantum states can be endowed with a metric structure using the second order derivatives of the relative entropy, giving rise to the so-called Kubo-Mori-Bogoliubov inner product. We explore its geometric properties on the submanifold of faithful, zero-displacement Gaussian states parameterised by their covariance matrices, deriving expressions for the geodesic equations, curvature tensors and scalar curvature. Our analysis suggests that the curvature of the manifold is strictly monotonic with respect to the von Neumann entropy, and thus can be interpreted as a measure of state uncertainty. This provides supporting evidence for the Petz conjecture in continuous variable systems.","sentences":["The space of quantum states can be endowed with a metric structure using the second order derivatives of the relative entropy, giving rise to the so-called Kubo-Mori-Bogoliubov inner product.","We explore its geometric properties on the submanifold of faithful, zero-displacement Gaussian states parameterised by their covariance matrices, deriving expressions for the geodesic equations, curvature tensors and scalar curvature.","Our analysis suggests that the curvature of the manifold is strictly monotonic with respect to the von Neumann entropy, and thus can be interpreted as a measure of state uncertainty.","This provides supporting evidence for the Petz conjecture in continuous variable systems."],"url":"http://arxiv.org/abs/2404.09600v1","category":"quant-ph"}
{"created":"2024-04-15 09:10:10","title":"Non-invasive Diver Respiration Rate Monitoring in Hyperbaric Lifeboat Environments using Short-Range Radar","abstract":"The monitoring of diver health during emergency events is crucial to ensuring the safety of personnel. A non-invasive system continuously providing a measure of the respiration rate of individual divers is exceedingly beneficial in this context. The paper reports on the application of short-range radar to record the respiration rate of divers within hyperbaric lifeboat environments. Results demonstrate that the respiratory motion can be extracted from the radar return signal applying routine signal processing. Further, evidence is provided that the radar-based approach yields a more accurate measure of respiration rate than an audio signal from a headset microphone. The system promotes an improvement in evacuation protocols under critical operational scenarios.","sentences":["The monitoring of diver health during emergency events is crucial to ensuring the safety of personnel.","A non-invasive system continuously providing a measure of the respiration rate of individual divers is exceedingly beneficial in this context.","The paper reports on the application of short-range radar to record the respiration rate of divers within hyperbaric lifeboat environments.","Results demonstrate that the respiratory motion can be extracted from the radar return signal applying routine signal processing.","Further, evidence is provided that the radar-based approach yields a more accurate measure of respiration rate than an audio signal from a headset microphone.","The system promotes an improvement in evacuation protocols under critical operational scenarios."],"url":"http://arxiv.org/abs/2404.09598v1","category":"eess.SP"}
{"created":"2024-04-15 09:07:25","title":"About some properties of the canonical density matrix versus the canonical Bloch equation","abstract":"We examine some properties of the non-normalized (or canonical) density matrix in the coherent states representation, by two equivalent ways. On the one hand by its definition, and on the other hand as a solution to Bloch's canonical equation. It is concluded that, since in many cases Bloch's differential equation is difficult to solve, in applications it is preferable to use the expression obtained directly from the definition of the canonical density matrix in the coherent states representation. This conclusion is verified by examining several cases of quantum systems with linear or quadratic energy spectrum.","sentences":["We examine some properties of the non-normalized (or canonical) density matrix in the coherent states representation, by two equivalent ways.","On the one hand by its definition, and on the other hand as a solution to Bloch's canonical equation.","It is concluded that, since in many cases Bloch's differential equation is difficult to solve, in applications it is preferable to use the expression obtained directly from the definition of the canonical density matrix in the coherent states representation.","This conclusion is verified by examining several cases of quantum systems with linear or quadratic energy spectrum."],"url":"http://arxiv.org/abs/2404.09596v1","category":"quant-ph"}
{"created":"2024-04-15 09:03:05","title":"Improving Recall of Large Language Models: A Model Collaboration Approach for Relational Triple Extraction","abstract":"Relation triple extraction, which outputs a set of triples from long sentences, plays a vital role in knowledge acquisition. Large language models can accurately extract triples from simple sentences through few-shot learning or fine-tuning when given appropriate instructions. However, they often miss out when extracting from complex sentences. In this paper, we design an evaluation-filtering framework that integrates large language models with small models for relational triple extraction tasks. The framework includes an evaluation model that can extract related entity pairs with high precision. We propose a simple labeling principle and a deep neural network to build the model, embedding the outputs as prompts into the extraction process of the large model. We conduct extensive experiments to demonstrate that the proposed method can assist large language models in obtaining more accurate extraction results, especially from complex sentences containing multiple relational triples. Our evaluation model can also be embedded into traditional extraction models to enhance their extraction precision from complex sentences.","sentences":["Relation triple extraction, which outputs a set of triples from long sentences, plays a vital role in knowledge acquisition.","Large language models can accurately extract triples from simple sentences through few-shot learning or fine-tuning when given appropriate instructions.","However, they often miss out when extracting from complex sentences.","In this paper, we design an evaluation-filtering framework that integrates large language models with small models for relational triple extraction tasks.","The framework includes an evaluation model that can extract related entity pairs with high precision.","We propose a simple labeling principle and a deep neural network to build the model, embedding the outputs as prompts into the extraction process of the large model.","We conduct extensive experiments to demonstrate that the proposed method can assist large language models in obtaining more accurate extraction results, especially from complex sentences containing multiple relational triples.","Our evaluation model can also be embedded into traditional extraction models to enhance their extraction precision from complex sentences."],"url":"http://arxiv.org/abs/2404.09593v1","category":"cs.CL"}
{"created":"2024-04-15 09:02:33","title":"The \"C\": The large Chameleon-Musca-Coalsack cloud","abstract":"Recent advancements in 3D dust mapping have transformed our understanding of the Milky Way's local interstellar medium (ISM), enabling us to explore its structure in three spatial dimensions for the first time. In this letter, we use the Edenhofer et al. (2023) 3D dust map to study the well-known Chameleon, Musca, and Coalsack cloud complexes, located about 200 pc from the Sun. We find that these three complexes are not isolated but rather connect to form a surprisingly well-defined half-ring, constituting a single \"C\"-shaped cloud with a radius of about 50 pc, a thickness of about 45 pc, and a total mass of about $9 \\times 10^{4}~\\mathrm{M}_{\\odot}$. Despite the absence of an evident feedback source at its center, the dynamics of young stellar clusters associated with the \"C\" structure suggest that a single supernova explosion about 4 Myr to 10 Myr ago likely shaped this structure. Our findings support a single origin story for these cloud complexes, suggesting that they were formed by feedback-driven gas compression. This interpretation challenges scenarios in which magnetic fields are the primary drivers behind the formation of star-forming molecular clouds in this region, offering new insights into the processes that govern the birth of star-forming clouds in feedback-dominated regions, such as Sco-Cen.","sentences":["Recent advancements in 3D dust mapping have transformed our understanding of the Milky Way's local interstellar medium (ISM), enabling us to explore its structure in three spatial dimensions for the first time.","In this letter, we use the Edenhofer et al.","(2023) 3D dust map to study the well-known Chameleon, Musca, and Coalsack cloud complexes, located about 200 pc from the Sun.","We find that these three complexes are not isolated but rather connect to form a surprisingly well-defined half-ring, constituting a single \"C\"-shaped cloud with a radius of about 50 pc, a thickness of about 45 pc, and a total mass of about $9 \\times 10^{4}~\\mathrm{M}_{\\odot}$.","Despite the absence of an evident feedback source at its center, the dynamics of young stellar clusters associated with the \"C\" structure suggest that a single supernova explosion about 4 Myr to 10 Myr ago likely shaped this structure.","Our findings support a single origin story for these cloud complexes, suggesting that they were formed by feedback-driven gas compression.","This interpretation challenges scenarios in which magnetic fields are the primary drivers behind the formation of star-forming molecular clouds in this region, offering new insights into the processes that govern the birth of star-forming clouds in feedback-dominated regions, such as Sco-Cen."],"url":"http://arxiv.org/abs/2404.09592v1","category":"astro-ph.GA"}
{"created":"2024-04-15 09:01:12","title":"On Models and Approaches for Human Vital Signs Extraction from Short Range Radar Signals","abstract":"The paper centres on an assessment of the modelling approaches for the processing of signals in CW and FMCW radar-based systems for the detection of vital signs. It is shown that the use of the widely adopted phase extraction method, which relies on the approximation of the target as a single point scatterer, has limitations in respect of the simultaneous estimation of both respiratory and heart rates. A method based on a velocity spectrum is proposed as an alternative with the ability to treat a wider range of application scenarios.","sentences":["The paper centres on an assessment of the modelling approaches for the processing of signals in CW and FMCW radar-based systems for the detection of vital signs.","It is shown that the use of the widely adopted phase extraction method, which relies on the approximation of the target as a single point scatterer, has limitations in respect of the simultaneous estimation of both respiratory and heart rates.","A method based on a velocity spectrum is proposed as an alternative with the ability to treat a wider range of application scenarios."],"url":"http://arxiv.org/abs/2404.09590v1","category":"eess.SP"}
{"created":"2024-04-15 08:51:49","title":"GeoSACS: Geometric Shared Autonomy via Canal Surfaces","abstract":"We introduce GeoSACS, a geometric framework for shared autonomy (SA). In variable environments, SA methods can be used to combine robotic capabilities with real-time human input in a way that offloads the physical task from the human. To remain intuitive, it can be helpful to simplify requirements for human input (i.e., reduce the dimensionality), which create challenges for to map low-dimensional human inputs to the higher dimensional control space of robots without requiring large amounts of data. We built GeoSACS on canal surfaces, a geometric framework that represents potential robot trajectories as a canal from as few as two demonstrations. GeoSACS maps user corrections on the cross-sections of this canal to provide an efficient SA framework. We extend canal surfaces to consider orientation and update the control frames to support intuitive mapping from user input to robot motions. Finally, we demonstrate GeoSACS in two preliminary studies, including a complex manipulation task where a robot loads laundry into a washer.","sentences":["We introduce GeoSACS, a geometric framework for shared autonomy (SA).","In variable environments, SA methods can be used to combine robotic capabilities with real-time human input in a way that offloads the physical task from the human.","To remain intuitive, it can be helpful to simplify requirements for human input (i.e., reduce the dimensionality), which create challenges for to map low-dimensional human inputs to the higher dimensional control space of robots without requiring large amounts of data.","We built GeoSACS on canal surfaces, a geometric framework that represents potential robot trajectories as a canal from as few as two demonstrations.","GeoSACS maps user corrections on the cross-sections of this canal to provide an efficient SA framework.","We extend canal surfaces to consider orientation and update the control frames to support intuitive mapping from user input to robot motions.","Finally, we demonstrate GeoSACS in two preliminary studies, including a complex manipulation task where a robot loads laundry into a washer."],"url":"http://arxiv.org/abs/2404.09584v1","category":"cs.RO"}
{"created":"2024-04-15 08:50:42","title":"Stokes phenomenon of Kloosterman and Airy connections","abstract":"We define categories of Stokes filtered and Stokes graded $G$-local systems for reductive groups $G$ and use the formalism of Tannakian categories to show that they are equivalent to the category of $G$-connections. We then use the interpretation of moduli spaces of Stokes filtered $G$-local systems as braid varieties to prove physical rigidity of two well-known families of cohomologically rigid connections, the Kloosterman and Airy connections. In the Kloosterman case, our proof relies on Steinberg's cross-section.","sentences":["We define categories of Stokes filtered and Stokes graded $G$-local systems for reductive groups $G$ and use the formalism of Tannakian categories to show that they are equivalent to the category of $G$-connections.","We then use the interpretation of moduli spaces of Stokes filtered $G$-local systems as braid varieties to prove physical rigidity of two well-known families of cohomologically rigid connections, the Kloosterman and Airy connections.","In the Kloosterman case, our proof relies on Steinberg's cross-section."],"url":"http://arxiv.org/abs/2404.09582v1","category":"math.AG"}
{"created":"2024-04-15 08:32:41","title":"MTKD: Multi-Teacher Knowledge Distillation for Image Super-Resolution","abstract":"Knowledge distillation (KD) has emerged as a promising technique in deep learning, typically employed to enhance a compact student network through learning from their high-performance but more complex teacher variant. When applied in the context of image super-resolution, most KD approaches are modified versions of methods developed for other computer vision tasks, which are based on training strategies with a single teacher and simple loss functions. In this paper, we propose a novel Multi-Teacher Knowledge Distillation (MTKD) framework specifically for image super-resolution. It exploits the advantages of multiple teachers by combining and enhancing the outputs of these teacher models, which then guides the learning process of the compact student network. To achieve more effective learning performance, we have also developed a new wavelet-based loss function for MTKD, which can better optimize the training process by observing differences in both the spatial and frequency domains. We fully evaluate the effectiveness of the proposed method by comparing it to five commonly used KD methods for image super-resolution based on three popular network architectures. The results show that the proposed MTKD method achieves evident improvements in super-resolution performance, up to 0.46dB (based on PSNR), over state-of-the-art KD approaches across different network structures. The source code of MTKD will be made available here for public evaluation.","sentences":["Knowledge distillation (KD) has emerged as a promising technique in deep learning, typically employed to enhance a compact student network through learning from their high-performance but more complex teacher variant.","When applied in the context of image super-resolution, most KD approaches are modified versions of methods developed for other computer vision tasks, which are based on training strategies with a single teacher and simple loss functions.","In this paper, we propose a novel Multi-Teacher Knowledge Distillation (MTKD) framework specifically for image super-resolution.","It exploits the advantages of multiple teachers by combining and enhancing the outputs of these teacher models, which then guides the learning process of the compact student network.","To achieve more effective learning performance, we have also developed a new wavelet-based loss function for MTKD, which can better optimize the training process by observing differences in both the spatial and frequency domains.","We fully evaluate the effectiveness of the proposed method by comparing it to five commonly used KD methods for image super-resolution based on three popular network architectures.","The results show that the proposed MTKD method achieves evident improvements in super-resolution performance, up to 0.46dB (based on PSNR), over state-of-the-art KD approaches across different network structures.","The source code of MTKD will be made available here for public evaluation."],"url":"http://arxiv.org/abs/2404.09571v1","category":"eess.IV"}
{"created":"2024-04-15 08:29:24","title":"A competitive game optimization algorithm for Unmanned Aerial Vehicle path planning","abstract":"To solve the Unmanned Aerial Vehicle (UAV) path planning problem, a meta-heuristic optimization algorithm called competitive game optimizer (CGO) is proposed. In the CGO model, three phases of exploration and exploitation, and candidate replacement, are established, corresponding to the player's search for supplies and combat, and the movement toward a safe zone. In the algorithm exploration phase, Levy flight is introduced to improve the global convergence of the algorithm. The encounter probability which adaptively changes with the number of iterations is also introduced in the CGO. The balance between exploration and exploitation of solution space of optimization problem is realized, and each step is described and modeled mathematically. The performance of the CGO was evaluated on a set of 41 test functions taken from CEC2017 and CEC2022. It was then compared with eight widely recognized meta-heuristic optimization algorithms. The simulation results demonstrate that the proposed algorithm successfully achieves a balanced trade-off between exploration and exploitation, showcasing remarkable advantages when compared to seven classical algorithms. In addition, in order to further verify the effectiveness of the CGO, the CGO is applied to 8 practical engineering design problems and UAV path planning, and the results show that the CGO has strong performance in dealing with these practical optimization problems, and has a good application prospect.","sentences":["To solve the Unmanned Aerial Vehicle (UAV) path planning problem, a meta-heuristic optimization algorithm called competitive game optimizer (CGO) is proposed.","In the CGO model, three phases of exploration and exploitation, and candidate replacement, are established, corresponding to the player's search for supplies and combat, and the movement toward a safe zone.","In the algorithm exploration phase, Levy flight is introduced to improve the global convergence of the algorithm.","The encounter probability which adaptively changes with the number of iterations is also introduced in the CGO.","The balance between exploration and exploitation of solution space of optimization problem is realized, and each step is described and modeled mathematically.","The performance of the CGO was evaluated on a set of 41 test functions taken from CEC2017 and CEC2022.","It was then compared with eight widely recognized meta-heuristic optimization algorithms.","The simulation results demonstrate that the proposed algorithm successfully achieves a balanced trade-off between exploration and exploitation, showcasing remarkable advantages when compared to seven classical algorithms.","In addition, in order to further verify the effectiveness of the CGO, the CGO is applied to 8 practical engineering design problems and UAV path planning, and the results show that the CGO has strong performance in dealing with these practical optimization problems, and has a good application prospect."],"url":"http://arxiv.org/abs/2404.09567v1","category":"eess.SY"}
{"created":"2024-04-15 08:28:46","title":"Moving horizon estimation for nonlinear systems with time-varying parameters","abstract":"We propose a moving horizon estimation scheme for estimating the states and time-varying parameters of nonlinear systems. We consider the case where observability of the parameters depends on the excitation of the system and may be absent during operation, with the parameter dynamics fulfilling a weak incremental bounded-energy bounded-state property to ensure boundedness of the estimation error (with respect to the disturbance energy). The proposed estimation scheme involves a standard quadratic cost function with an adaptive regularization term depending on the current parameter observability. We develop robustness guarantees for the overall estimation error that are valid for all times, and that improve the more often the parameters are detected to be observable during operation. The theoretical results are illustrated by a simulation example.","sentences":["We propose a moving horizon estimation scheme for estimating the states and time-varying parameters of nonlinear systems.","We consider the case where observability of the parameters depends on the excitation of the system and may be absent during operation, with the parameter dynamics fulfilling a weak incremental bounded-energy bounded-state property to ensure boundedness of the estimation error (with respect to the disturbance energy).","The proposed estimation scheme involves a standard quadratic cost function with an adaptive regularization term depending on the current parameter observability.","We develop robustness guarantees for the overall estimation error that are valid for all times, and that improve the more often the parameters are detected to be observable during operation.","The theoretical results are illustrated by a simulation example."],"url":"http://arxiv.org/abs/2404.09566v1","category":"eess.SY"}
{"created":"2024-04-15 08:24:18","title":"The turbulent variability of accretion discs observed at high energies","abstract":"We use numerical stochastic-viscous hydrodynamic simulations and new analytical results from thin disc theory to probe the turbulent variability of accretion flows, as observed at high energies. We show that the act of observing accretion discs in the Wien tail exponentially enhances small-scale temperature variability in the flow, which in a real disc will be driven by magnetohydrodynamic turbulence, to large amplitude luminosity fluctuations (as predicted analytically). In particular, we demonstrate that discs with more spatially coherent turbulence (as might be expected of thicker discs), and relativistic discs observed at larger inclinations, show significantly enhancement in their Wien-tail variability. We believe this is the first analysis of relativistic viewing-angle effects on turbulent variability in the literature. Using these results we argue that tidal disruption events represent particularly interesting systems with which to study accretion flow variability, and may in fact be the best astrophysical probes of small scale disc turbulence. This is a result of a typical tidal disruption event disc being naturally observed in the Wien-tail and likely having a somewhat thicker disc and cleaner X-ray spectrum than other sources. We argue for dedicated X-ray observational campaigns of tidal disruption events, with the aim of studying accretion flow variability.","sentences":["We use numerical stochastic-viscous hydrodynamic simulations and new analytical results from thin disc theory to probe the turbulent variability of accretion flows, as observed at high energies.","We show that the act of observing accretion discs in the Wien tail exponentially enhances small-scale temperature variability in the flow, which in a real disc will be driven by magnetohydrodynamic turbulence, to large amplitude luminosity fluctuations (as predicted analytically).","In particular, we demonstrate that discs with more spatially coherent turbulence (as might be expected of thicker discs), and relativistic discs observed at larger inclinations, show significantly enhancement in their Wien-tail variability.","We believe this is the first analysis of relativistic viewing-angle effects on turbulent variability in the literature.","Using these results we argue that tidal disruption events represent particularly interesting systems with which to study accretion flow variability, and may in fact be the best astrophysical probes of small scale disc turbulence.","This is a result of a typical tidal disruption event disc being naturally observed in the Wien-tail and likely having a somewhat thicker disc and cleaner X-ray spectrum than other sources.","We argue for dedicated X-ray observational campaigns of tidal disruption events, with the aim of studying accretion flow variability."],"url":"http://arxiv.org/abs/2404.09564v1","category":"astro-ph.HE"}
{"created":"2024-04-15 08:16:43","title":"Interpolating supersymmetric pair of Fokker-Planck equations","abstract":"We consider Fokker-Planck equations that interpolate a pair of supersymmetrically related Fokker-Planck equations with constant coefficients. Based on the interesting property of shape-invariance, various one-parameter interpolations of the solutions of the supersymmetric pair of Fokker-Planck systems can be directly constructed.","sentences":["We consider Fokker-Planck equations that interpolate a pair of supersymmetrically related Fokker-Planck equations with constant coefficients.","Based on the interesting property of shape-invariance, various one-parameter interpolations of the solutions of the supersymmetric pair of Fokker-Planck systems can be directly constructed."],"url":"http://arxiv.org/abs/2404.09551v1","category":"math-ph"}
{"created":"2024-04-15 08:16:43","title":"Entropy on the Path Space and Application to Singular Diffusions and Mean-field Models","abstract":"In this paper we introduce a (partly) new approach for the study of McKean-Vlasov equations, including singular interactions. This approach is based on the relativeentropy on the path space in the spirit of our previous works together with C. L{\\'e}onard. Itis also used in a recent work of D. Lacker.We show how it can be used to derive existence and uniqueness for some singular diffusions, in particular linear mean field stochastic particle systems and non linear SDE of McKean-Vlasov type, including Lp- Lq models, the 2D vortex model associated to the 2D Navier-Stokes equation, sub-Coulombic interactions models or the Patlak-Keller-Segel model.We also show the convergence and propagation of chaos as the number of particles grows to infinity. This is obtained at the process level, not only at the Liouville equation level.The paper thus contains new proofs and extensions of known results, as well as new results.The main results are given at the end of the Introduction.","sentences":["In this paper we introduce a (partly) new approach for the study of McKean-Vlasov equations, including singular interactions.","This approach is based on the relativeentropy on the path space in the spirit of our previous works together with C. L{\\'e}onard.","Itis also used in a recent work of D. Lacker.","We show how it can be used to derive existence and uniqueness for some singular diffusions, in particular linear mean field stochastic particle systems and non linear SDE of McKean-Vlasov type, including Lp- Lq models, the 2D vortex model associated to the 2D Navier-Stokes equation, sub-Coulombic interactions models or the Patlak-Keller-Segel model.","We also show the convergence and propagation of chaos as the number of particles grows to infinity.","This is obtained at the process level, not only at the Liouville equation level.","The paper thus contains new proofs and extensions of known results, as well as new results.","The main results are given at the end of the Introduction."],"url":"http://arxiv.org/abs/2404.09552v1","category":"math.AP"}
{"created":"2024-04-15 07:55:48","title":"Dual-comb-enhanced microwave clock synchronization over commercial fiber","abstract":"The large-scale clock network is the key ingredient to obtain high precision in many scenarios, from fundamental research to cutting-edge applications. However, time synchronization among microwave clocks has become an upper limit of the time-frequency network because of the inherent constrained precision in optical pulse measurements. Here, we demonstrate a femtosecond-level time synchronization of microwave clocks through a commercial link of 205.86 km via dual-comb-enhanced optical two-way time transfer, which achieves a 6.23-fs residual time deviation between synchronized timescales at 1 s and an instability below 6E-18 at 10,000 s. Further, the high-precision time synchronization of microwave clocks significantly enhances the probe ability of subtle reciprocity changes of fiber to the sub-picosecond level. This work provides a path toward secure fiber time-frequency networks to support future microwave-clock-based precise timing and sensing systems.","sentences":["The large-scale clock network is the key ingredient to obtain high precision in many scenarios, from fundamental research to cutting-edge applications.","However, time synchronization among microwave clocks has become an upper limit of the time-frequency network because of the inherent constrained precision in optical pulse measurements.","Here, we demonstrate a femtosecond-level time synchronization of microwave clocks through a commercial link of 205.86 km via dual-comb-enhanced optical two-way time transfer, which achieves a 6.23-fs residual time deviation between synchronized timescales at 1 s and an instability below 6E-18 at 10,000 s. Further, the high-precision time synchronization of microwave clocks significantly enhances the probe ability of subtle reciprocity changes of fiber to the sub-picosecond level.","This work provides a path toward secure fiber time-frequency networks to support future microwave-clock-based precise timing and sensing systems."],"url":"http://arxiv.org/abs/2404.09535v1","category":"physics.optics"}
{"created":"2024-04-15 07:45:50","title":"Dynamical Mean Field Theory for Real Materials on a Quantum Computer","abstract":"Quantum computers (QC) could harbor the potential to significantly advance materials simulations, particularly at the atomistic scale involving strongly correlated fermionic systems where an accurate description of quantum many-body effects scales unfavorably with size. While a full-scale treatment of condensed matter systems with currently available noisy quantum computers remains elusive, quantum embedding schemes like dynamical mean-field theory (DMFT) allow the mapping of an effective, reduced subspace Hamiltonian to available devices to improve the accuracy of ab initio calculations such as density functional theory (DFT). Here, we report on the development of a hybrid quantum-classical DFT+DMFT simulation framework which relies on a quantum impurity solver based on the Lehmann representation of the impurity Green's function. Hardware experiments with up to 14 qubits on the IBM Quantum system are conducted, using advanced error mitigation methods and a novel calibration scheme for an improved zero-noise extrapolation to effectively reduce adverse effects from inherent noise on current quantum devices. We showcase the utility of our quantum DFT+DMFT workflow by assessing the correlation effects on the electronic structure of a real material, Ca2CuO2Cl2, and by carefully benchmarking our quantum results with respect to exact reference solutions and experimental spectroscopy measurements.","sentences":["Quantum computers (QC) could harbor the potential to significantly advance materials simulations, particularly at the atomistic scale involving strongly correlated fermionic systems where an accurate description of quantum many-body effects scales unfavorably with size.","While a full-scale treatment of condensed matter systems with currently available noisy quantum computers remains elusive, quantum embedding schemes like dynamical mean-field theory (DMFT) allow the mapping of an effective, reduced subspace Hamiltonian to available devices to improve the accuracy of ab initio calculations such as density functional theory (DFT).","Here, we report on the development of a hybrid quantum-classical DFT+DMFT simulation framework which relies on a quantum impurity solver based on the Lehmann representation of the impurity Green's function.","Hardware experiments with up to 14 qubits on the IBM Quantum system are conducted, using advanced error mitigation methods and a novel calibration scheme for an improved zero-noise extrapolation to effectively reduce adverse effects from inherent noise on current quantum devices.","We showcase the utility of our quantum DFT+DMFT workflow by assessing the correlation effects on the electronic structure of a real material, Ca2CuO2Cl2, and by carefully benchmarking our quantum results with respect to exact reference solutions and experimental spectroscopy measurements."],"url":"http://arxiv.org/abs/2404.09527v1","category":"cond-mat.str-el"}
{"created":"2024-04-15 07:45:04","title":"LoongServe: Efficiently Serving Long-context Large Language Models with Elastic Sequence Parallelism","abstract":"The context window of large language models (LLMs) is rapidly increasing, leading to a huge variance in resource usage between different requests as well as between different phases of the same request. Restricted by static parallelism strategies, existing LLM serving systems cannot efficiently utilize the underlying resources to serve variable-length requests in different phases. To address this problem, we propose a new parallelism paradigm, elastic sequence parallelism (ESP), to elastically adapt to the variance between different requests and phases. Based on ESP, we design and build LoongServe, an LLM serving system that (1) improves computation efficiency by elastically adjusting the degree of parallelism in real-time, (2) improves communication efficiency by reducing key-value cache migration overhead and overlapping partial decoding communication with computation, and (3) improves GPU memory efficiency by reducing key-value cache fragmentation across instances. Our evaluation under diverse real-world datasets shows that LoongServe improves the maximum throughput by up to 3.85$\\times$ compared to the chunked prefill and 5.81$\\times$ compared to the prefill-decoding disaggregation.","sentences":["The context window of large language models (LLMs) is rapidly increasing, leading to a huge variance in resource usage between different requests as well as between different phases of the same request.","Restricted by static parallelism strategies, existing LLM serving systems cannot efficiently utilize the underlying resources to serve variable-length requests in different phases.","To address this problem, we propose a new parallelism paradigm, elastic sequence parallelism (ESP), to elastically adapt to the variance between different requests and phases.","Based on ESP, we design and build LoongServe, an LLM serving system that (1) improves computation efficiency by elastically adjusting the degree of parallelism in real-time, (2) improves communication efficiency by reducing key-value cache migration overhead and overlapping partial decoding communication with computation, and (3) improves GPU memory efficiency by reducing key-value cache fragmentation across instances.","Our evaluation under diverse real-world datasets shows that LoongServe improves the maximum throughput by up to 3.85$\\times$ compared to the chunked prefill and 5.81$\\times$ compared to the prefill-decoding disaggregation."],"url":"http://arxiv.org/abs/2404.09526v1","category":"cs.DC"}
{"created":"2024-04-15 07:41:35","title":"Dynamic fault detection and diagnosis of industrial alkaline water electrolyzer process with variational Bayesian dictionary learning","abstract":"Alkaline Water Electrolysis (AWE) is one of the simplest green hydrogen production method using renewable energy.   AWE system typically yields process variables that are serially correlated and contaminated by measurement uncertainty.   A novel robust dynamic variational Bayesian dictionary learning (RDVDL) monitoring approach is proposed to improve the reliability and safety of AWE operation.   RDVDL employs a sparse Bayesian dictionary learning to preserve the dynamic mechanism information of AWE process which allows the easy interpretation of fault detection results.   To improve the robustness to measurement uncertainty, a low-rank vector autoregressive (VAR) method is derived to reliably extract the serial correlation from process variables.   The effectiveness of the proposed approach is demonstrated with an industrial hydrogen production process, and RDVDL can efficiently detect and diagnose critical AWE faults.","sentences":["Alkaline Water Electrolysis (AWE) is one of the simplest green hydrogen production method using renewable energy.   ","AWE system typically yields process variables that are serially correlated and contaminated by measurement uncertainty.   ","A novel robust dynamic variational Bayesian dictionary learning (RDVDL) monitoring approach is proposed to improve the reliability and safety of AWE operation.   ","RDVDL employs a sparse Bayesian dictionary learning to preserve the dynamic mechanism information of AWE process which allows the easy interpretation of fault detection results.   ","To improve the robustness to measurement uncertainty, a low-rank vector autoregressive (VAR) method is derived to reliably extract the serial correlation from process variables.   ","The effectiveness of the proposed approach is demonstrated with an industrial hydrogen production process, and RDVDL can efficiently detect and diagnose critical AWE faults."],"url":"http://arxiv.org/abs/2404.09524v1","category":"cs.LG"}
{"created":"2024-04-15 07:34:32","title":"The Physalis system: Discovery of ORC-like radio shells around a massive pair of interacting early-type galaxies with offset X-ray emission","abstract":"We present the discovery of large radio shells around a massive pair of interacting galaxies and extended diffuse X-ray emission within the shells. The radio data were obtained with the Australian Square Kilometer Array Pathfinder (ASKAP) in two frequency bands centred at 944 MHz and 1.4 GHz, respectively, while the X-ray data are from the XMM-Newton observatory. The host galaxy pair, which consists of the early-type galaxies ESO 184-G042 and LEDA 418116, is part of a loose group at a distance of only 75 Mpc (redshift z = 0.017). The observed outer radio shells (diameter ~ 145 kpc) and ridge-like central emission of the system, ASKAP J1914-5433 (Physalis), are likely associated with merger shocks during the formation of the central galaxy (ESO 184-G042) and resemble the new class of odd radio circles (ORCs). This is supported by the brightest X-ray emission found offset from the centre of the Physalis system, instead centered at the less massive galaxy, LEDA 418116. The host galaxy pair is embedded in an irregular envelope of diffuse light, highlighting on-going interactions. We complement our combined radio and X-ray study with high-resolution simulations of the circumgalactic medium (CGM) around galaxy mergers from the Magneticum project to analyse the evolutionary state of the Physalis system. We argue that ORCs / radio shells could be produced by a combination of energy release from the central AGN and subsequent lightening up in radio emission by merger shocks traveling through the CGM of these systems.","sentences":["We present the discovery of large radio shells around a massive pair of interacting galaxies and extended diffuse X-ray emission within the shells.","The radio data were obtained with the Australian Square Kilometer Array Pathfinder (ASKAP) in two frequency bands centred at 944 MHz and 1.4 GHz, respectively, while the X-ray data are from the XMM-Newton observatory.","The host galaxy pair, which consists of the early-type galaxies ESO 184-G042 and LEDA 418116, is part of a loose group at a distance of only 75 Mpc (redshift z = 0.017).","The observed outer radio shells (diameter ~ 145 kpc) and ridge-like central emission of the system, ASKAP J1914-5433 (Physalis), are likely associated with merger shocks during the formation of the central galaxy (ESO 184-G042) and resemble the new class of odd radio circles (ORCs).","This is supported by the brightest X-ray emission found offset from the centre of the Physalis system, instead centered at the less massive galaxy, LEDA 418116.","The host galaxy pair is embedded in an irregular envelope of diffuse light, highlighting on-going interactions.","We complement our combined radio and X-ray study with high-resolution simulations of the circumgalactic medium (CGM) around galaxy mergers from the Magneticum project to analyse the evolutionary state of the Physalis system.","We argue that ORCs / radio shells could be produced by a combination of energy release from the central AGN and subsequent lightening up in radio emission by merger shocks traveling through the CGM of these systems."],"url":"http://arxiv.org/abs/2404.09522v1","category":"astro-ph.GA"}
{"created":"2024-04-15 07:30:26","title":"Nonlinear sparse variational Bayesian learning based model predictive control with application to PEMFC temperature control","abstract":"The accuracy of the underlying model predictions is crucial for the success of model predictive control (MPC) applications. If the model is unable to accurately analyze the dynamics of the controlled system, the performance and stability guarantees provided by MPC may not be achieved. Learning-based MPC can learn models from data, improving the applicability and reliability of MPC. This study develops a nonlinear sparse variational Bayesian learning based MPC (NSVB-MPC) for nonlinear systems, where the model is learned by the developed NSVB method. Variational inference is used by NSVB-MPC to assess the predictive accuracy and make the necessary corrections to quantify system uncertainty. The suggested approach ensures input-to-state (ISS) and the feasibility of recursive constraints in accordance with the concept of an invariant terminal region. Finally, a PEMFC temperature control model experiment confirms the effectiveness of the NSVB-MPC method.","sentences":["The accuracy of the underlying model predictions is crucial for the success of model predictive control (MPC) applications.","If the model is unable to accurately analyze the dynamics of the controlled system, the performance and stability guarantees provided by MPC may not be achieved.","Learning-based MPC can learn models from data, improving the applicability and reliability of MPC.","This study develops a nonlinear sparse variational Bayesian learning based MPC (NSVB-MPC) for nonlinear systems, where the model is learned by the developed NSVB method.","Variational inference is used by NSVB-MPC to assess the predictive accuracy and make the necessary corrections to quantify system uncertainty.","The suggested approach ensures input-to-state (ISS) and the feasibility of recursive constraints in accordance with the concept of an invariant terminal region.","Finally, a PEMFC temperature control model experiment confirms the effectiveness of the NSVB-MPC method."],"url":"http://arxiv.org/abs/2404.09519v1","category":"cs.LG"}
{"created":"2024-04-15 07:26:36","title":"Bridging the Gap: Automated Analysis of Sancus","abstract":"Techniques for verifying or invalidating the security of computer systems have come a long way in recent years. Extremely sophisticated tools are available to specify and formally verify the behavior of a system and, at the same time, attack techniques have evolved to the point of questioning the possibility of obtaining adequate levels of security, especially in critical applications. In a recent paper, Bognar et al. have clearly highlighted this inconsistency between the two worlds: on one side, formal verification allows writing irrefutable proofs of the security of a system, on the other side concrete attacks make these proofs waver, exhibiting a gap between models and implementations which is very complex to bridge. In this paper, we propose a new method to reduce this gap in the Sancus embedded security architecture, by exploiting some peculiarities of both approaches. Our technique first extracts a behavioral model by directly interacting with the real Sancus system and then analyzes it to identify attacks and anomalies. Given a threat model, our method either finds attacks in the given threat model or gives probabilistic guarantees on the security of the system. We implement our method and use it to systematically rediscover known attacks and uncover new ones.","sentences":["Techniques for verifying or invalidating the security of computer systems have come a long way in recent years.","Extremely sophisticated tools are available to specify and formally verify the behavior of a system and, at the same time, attack techniques have evolved to the point of questioning the possibility of obtaining adequate levels of security, especially in critical applications.","In a recent paper, Bognar et al. have clearly highlighted this inconsistency between the two worlds: on one side, formal verification allows writing irrefutable proofs of the security of a system, on the other side concrete attacks make these proofs waver, exhibiting a gap between models and implementations which is very complex to bridge.","In this paper, we propose a new method to reduce this gap in the Sancus embedded security architecture, by exploiting some peculiarities of both approaches.","Our technique first extracts a behavioral model by directly interacting with the real Sancus system and then analyzes it to identify attacks and anomalies.","Given a threat model, our method either finds attacks in the given threat model or gives probabilistic guarantees on the security of the system.","We implement our method and use it to systematically rediscover known attacks and uncover new ones."],"url":"http://arxiv.org/abs/2404.09518v1","category":"cs.CR"}
{"created":"2024-04-15 07:25:23","title":"Odd-frequency superconducting pairing and multiple Majorana edge modes in driven topological superconductors","abstract":"Majorana zero modes have been shown to be the simplest quasiparticles exhibiting pure odd-frequency pairing, an effect that has so far been theoretically established in the static regime. In this work we investigate the formation of Majorana modes and odd-frequency pairing in $p$-wave spin-polarized superconductors under a time-dependent drive. We first show that the driven system hosts multiple Majorana modes emerging at zero and $\\pi$, whose formation can be controlled by an appropriate tuning of the drive frequency and chemical potential. Then we explore the induced pair correlations and find that odd-frequency spin-polarized $s$-wave pairing is broadly induced, acquiring large values in the presence of Majorana modes. We discover that, while odd-frequency pairing is proportional to $\\sim1/\\omega$ in the presence of Majorana zero modes, it is proportional to $\\sim 1/(\\omega-\\pi\\hbar/T)$ in the presence of Majorana $\\pi$ modes, where $T$ is the periodicity of the drive. Furthermore, we find that the amount of odd-frequency pairing becomes larger when multiple Majorana modes appear but the overall divergent profile as a function of frequency remains. Our work thus paves the way for understanding the emergent pair correlations in driven topological superconductors","sentences":["Majorana zero modes have been shown to be the simplest quasiparticles exhibiting pure odd-frequency pairing, an effect that has so far been theoretically established in the static regime.","In this work we investigate the formation of Majorana modes and odd-frequency pairing in $p$-wave spin-polarized superconductors under a time-dependent drive.","We first show that the driven system hosts multiple Majorana modes emerging at zero and $\\pi$, whose formation can be controlled by an appropriate tuning of the drive frequency and chemical potential.","Then we explore the induced pair correlations and find that odd-frequency spin-polarized $s$-wave pairing is broadly induced, acquiring large values in the presence of Majorana modes.","We discover that, while odd-frequency pairing is proportional to $\\sim1/\\omega$ in the presence of Majorana zero modes, it is proportional to $\\sim 1/(\\omega-\\pi\\hbar/T)$ in the presence of Majorana $\\pi$ modes, where $T$ is the periodicity of the drive.","Furthermore, we find that the amount of odd-frequency pairing becomes larger when multiple Majorana modes appear but the overall divergent profile as a function of frequency remains.","Our work thus paves the way for understanding the emergent pair correlations in driven topological superconductors"],"url":"http://arxiv.org/abs/2404.09517v1","category":"cond-mat.supr-con"}
{"created":"2024-04-15 07:01:54","title":"From laminar to chaotic flow via stochastic resonance in viscoelastic channel flow","abstract":"Recent research indicates that low-inertia viscoelastic channel flow experiences supercritical non-normal mode elastic instability from laminar to sustained chaotic flow due to finite-size perturbations. The challenge of this study is to elucidate a realization of such a pathway when the intensity of the elastic wave is too low to amplify velocity fluctuations above the instability onset. The study identifies two subregions in the transition flow regime at Weissenberg number $Wi>Wi_c$, the instability onset. In the lower subregion at $Wi_c\\leq Wi\\leq 300$, we discover periodic spikes in the streamwise velocity time series $u(t)$ that appear in the chaotic power spectrum as low-frequency, high-intensity peaks resembling stochastic resonance (SR). In contrast, the spanwise velocity power spectrum, $E_w$, remains flat with low-intensity, noisy, and broad elastic wave peaks. The spikes significantly distort the probability density function of $u$, initiating and amplifying random streaks and wall-normal vorticity fluctuations. The SR appearance is similar to dynamical systems where chaotic attractor and limit cycle interact with external white noise. This similarity is confirmed by presenting a phase portrait in two subregions of the transition regime. In the upper subregion at $Wi>400$ the periodic spikes disappear and $E_w$ becomes chaotic with a large intensity elastic wave sufficient to self-organize and synchronize the streaks into cycles and to amplify the wall normal vorticity according to a recently proposed mechanism.","sentences":["Recent research indicates that low-inertia viscoelastic channel flow experiences supercritical non-normal mode elastic instability from laminar to sustained chaotic flow due to finite-size perturbations.","The challenge of this study is to elucidate a realization of such a pathway when the intensity of the elastic wave is too low to amplify velocity fluctuations above the instability onset.","The study identifies two subregions in the transition flow regime at Weissenberg number $Wi>Wi_c$, the instability onset.","In the lower subregion at $Wi_c\\leq Wi\\leq 300$, we discover periodic spikes in the streamwise velocity time series $u(t)$ that appear in the chaotic power spectrum as low-frequency, high-intensity peaks resembling stochastic resonance (SR).","In contrast, the spanwise velocity power spectrum, $E_w$, remains flat with low-intensity, noisy, and broad elastic wave peaks.","The spikes significantly distort the probability density function of $u$, initiating and amplifying random streaks and wall-normal vorticity fluctuations.","The SR appearance is similar to dynamical systems where chaotic attractor and limit cycle interact with external white noise.","This similarity is confirmed by presenting a phase portrait in two subregions of the transition regime.","In the upper subregion at $Wi>400$ the periodic spikes disappear and $E_w$ becomes chaotic with a large intensity elastic wave sufficient to self-organize and synchronize the streaks into cycles and to amplify the wall normal vorticity according to a recently proposed mechanism."],"url":"http://arxiv.org/abs/2404.09508v1","category":"physics.flu-dyn"}
{"created":"2024-04-15 06:54:38","title":"Performance analysis of satellite-terrestrial integrated radio access networks based on stochastic geometry","abstract":"To enhance coverage and improve service continuity, satellite-terrestrial integrated radio access network (STIRAN) has been seen as an essential trend in the development of 6G. However, there is still a lack of theoretical analysis on its coverage performance. To fill this gap, we first establish a system model to characterize a typical scenario where low-earth-orbit (LEO) satellites and terrestrial base stations are both deployed. Then, stochastic geometry is utilized to analyze the downlink coverage probability under the setting of shared frequency and distinct frequencies. Specifically, we derive mathematical expressions for the distances distribution from the serving station to the typical user and the associated probability based on the maximum bias power selection strategy (Max-BPR). Taking into account real-world satellite antenna beamforming patterns in two system scenarios, we derive the downlink coverage probabilities in terms of parameters such as base station density and orbital inclination. Finally, the correctness of the theoretical derivations is verified through experimental simulations, and the influence of network design parameters on the downlink coverage probability is analyzed.","sentences":["To enhance coverage and improve service continuity, satellite-terrestrial integrated radio access network (STIRAN) has been seen as an essential trend in the development of 6G.","However, there is still a lack of theoretical analysis on its coverage performance.","To fill this gap, we first establish a system model to characterize a typical scenario where low-earth-orbit (LEO) satellites and terrestrial base stations are both deployed.","Then, stochastic geometry is utilized to analyze the downlink coverage probability under the setting of shared frequency and distinct frequencies.","Specifically, we derive mathematical expressions for the distances distribution from the serving station to the typical user and the associated probability based on the maximum bias power selection strategy (Max-BPR).","Taking into account real-world satellite antenna beamforming patterns in two system scenarios, we derive the downlink coverage probabilities in terms of parameters such as base station density and orbital inclination.","Finally, the correctness of the theoretical derivations is verified through experimental simulations, and the influence of network design parameters on the downlink coverage probability is analyzed."],"url":"http://arxiv.org/abs/2404.09506v1","category":"cs.IT"}
{"created":"2024-04-15 06:45:06","title":"SparseOcc: Rethinking Sparse Latent Representation for Vision-Based Semantic Occupancy Prediction","abstract":"Vision-based perception for autonomous driving requires an explicit modeling of a 3D space, where 2D latent representations are mapped and subsequent 3D operators are applied. However, operating on dense latent spaces introduces a cubic time and space complexity, which limits scalability in terms of perception range or spatial resolution. Existing approaches compress the dense representation using projections like Bird's Eye View (BEV) or Tri-Perspective View (TPV). Although efficient, these projections result in information loss, especially for tasks like semantic occupancy prediction. To address this, we propose SparseOcc, an efficient occupancy network inspired by sparse point cloud processing. It utilizes a lossless sparse latent representation with three key innovations. Firstly, a 3D sparse diffuser performs latent completion using spatially decomposed 3D sparse convolutional kernels. Secondly, a feature pyramid and sparse interpolation enhance scales with information from others. Finally, the transformer head is redesigned as a sparse variant. SparseOcc achieves a remarkable 74.9% reduction on FLOPs over the dense baseline. Interestingly, it also improves accuracy, from 12.8% to 14.1% mIOU, which in part can be attributed to the sparse representation's ability to avoid hallucinations on empty voxels.","sentences":["Vision-based perception for autonomous driving requires an explicit modeling of a 3D space, where 2D latent representations are mapped and subsequent 3D operators are applied.","However, operating on dense latent spaces introduces a cubic time and space complexity, which limits scalability in terms of perception range or spatial resolution.","Existing approaches compress the dense representation using projections like Bird's Eye View (BEV) or Tri-Perspective View (TPV).","Although efficient, these projections result in information loss, especially for tasks like semantic occupancy prediction.","To address this, we propose SparseOcc, an efficient occupancy network inspired by sparse point cloud processing.","It utilizes a lossless sparse latent representation with three key innovations.","Firstly, a 3D sparse diffuser performs latent completion using spatially decomposed 3D sparse convolutional kernels.","Secondly, a feature pyramid and sparse interpolation enhance scales with information from others.","Finally, the transformer head is redesigned as a sparse variant.","SparseOcc achieves a remarkable 74.9% reduction on FLOPs over the dense baseline.","Interestingly, it also improves accuracy, from 12.8% to 14.1% mIOU, which in part can be attributed to the sparse representation's ability to avoid hallucinations on empty voxels."],"url":"http://arxiv.org/abs/2404.09502v1","category":"cs.CV"}
{"created":"2024-04-15 06:01:48","title":"TCCT-Net: Two-Stream Network Architecture for Fast and Efficient Engagement Estimation via Behavioral Feature Signals","abstract":"Engagement analysis finds various applications in healthcare, education, advertisement, services. Deep Neural Networks, used for analysis, possess complex architecture and need large amounts of input data, computational power, inference time. These constraints challenge embedding systems into devices for real-time use. To address these limitations, we present a novel two-stream feature fusion \"Tensor-Convolution and Convolution-Transformer Network\" (TCCT-Net) architecture. To better learn the meaningful patterns in the temporal-spatial domain, we design a \"CT\" stream that integrates a hybrid convolutional-transformer. In parallel, to efficiently extract rich patterns from the temporal-frequency domain and boost processing speed, we introduce a \"TC\" stream that uses Continuous Wavelet Transform (CWT) to represent information in a 2D tensor form. Evaluated on the EngageNet dataset, the proposed method outperforms existing baselines, utilizing only two behavioral features (head pose rotations) compared to the 98 used in baseline models. Furthermore, comparative analysis shows TCCT-Net's architecture offers an order-of-magnitude improvement in inference speed compared to state-of-the-art image-based Recurrent Neural Network (RNN) methods. The code will be released at https://github.com/vedernikovphoto/TCCT_Net.","sentences":["Engagement analysis finds various applications in healthcare, education, advertisement, services.","Deep Neural Networks, used for analysis, possess complex architecture and need large amounts of input data, computational power, inference time.","These constraints challenge embedding systems into devices for real-time use.","To address these limitations, we present a novel two-stream feature fusion \"Tensor-Convolution and Convolution-Transformer Network\" (TCCT-Net) architecture.","To better learn the meaningful patterns in the temporal-spatial domain, we design a \"CT\" stream that integrates a hybrid convolutional-transformer.","In parallel, to efficiently extract rich patterns from the temporal-frequency domain and boost processing speed, we introduce a \"TC\" stream that uses Continuous Wavelet Transform (CWT) to represent information in a 2D tensor form.","Evaluated on the EngageNet dataset, the proposed method outperforms existing baselines, utilizing only two behavioral features (head pose rotations) compared to the 98 used in baseline models.","Furthermore, comparative analysis shows TCCT-Net's architecture offers an order-of-magnitude improvement in inference speed compared to state-of-the-art image-based Recurrent Neural Network (RNN) methods.","The code will be released at https://github.com/vedernikovphoto/TCCT_Net."],"url":"http://arxiv.org/abs/2404.09474v1","category":"cs.CV"}
{"created":"2024-04-15 05:51:33","title":"LightningSimV2: Faster and Scalable Simulation for High-Level Synthesis via Graph Compilation and Optimization","abstract":"High-Level Synthesis (HLS) enables rapid prototyping of complex hardware designs by translating C or C++ code to low-level RTL code. However, the testing and evaluation of HLS designs still typically rely on slow RTL-level simulators that can take hours to provide feedback, especially for complex designs. A recent work, LightningSim, helps to solve this problem by providing a simulation workflow one to two orders of magnitude faster than RTL simulation. However, it still exhibits inefficiencies due to several types of redundant computation, making it slow for large design simulation and design space exploration. Addressing these inefficiencies, we introduce LightningSimV2, a much faster and scalable simulation tool. LightningSimV2 features three main innovations. First, we perform compile-time static analysis, exploiting the repetitive structures in HLS designs, e.g., loops, to reduce the simulation workload. Second, we propose a novel graph-based simulation approach, with decoupled simulation graph construction step and graph traversal step, significantly reducing repeated computation. Third, benefiting from the decoupled approach, LightningSimV2 can perform incremental stall analysis extremely fast, enabling highly efficient design space exploration of large numbers of complex hardware parameters, e.g., optimal FIFO depths. Moreover, the DSE is well-suited for parallel computing, further improving the DSE efficiency. Compared with LightningSim, LightningSimV2 achieves up to 3.5x speedup in full simulation and up to 577x speed up for incremental DSE. Our code is open-source on GitHub.","sentences":["High-Level Synthesis (HLS) enables rapid prototyping of complex hardware designs by translating C or C++ code to low-level RTL code.","However, the testing and evaluation of HLS designs still typically rely on slow RTL-level simulators that can take hours to provide feedback, especially for complex designs.","A recent work, LightningSim, helps to solve this problem by providing a simulation workflow one to two orders of magnitude faster than RTL simulation.","However, it still exhibits inefficiencies due to several types of redundant computation, making it slow for large design simulation and design space exploration.","Addressing these inefficiencies, we introduce LightningSimV2, a much faster and scalable simulation tool.","LightningSimV2 features three main innovations.","First, we perform compile-time static analysis, exploiting the repetitive structures in HLS designs, e.g., loops, to reduce the simulation workload.","Second, we propose a novel graph-based simulation approach, with decoupled simulation graph construction step and graph traversal step, significantly reducing repeated computation.","Third, benefiting from the decoupled approach, LightningSimV2 can perform incremental stall analysis extremely fast, enabling highly efficient design space exploration of large numbers of complex hardware parameters, e.g., optimal FIFO depths.","Moreover, the DSE is well-suited for parallel computing, further improving the DSE efficiency.","Compared with LightningSim, LightningSimV2 achieves up to 3.5x speedup in full simulation and up to 577x speed up for incremental DSE.","Our code is open-source on GitHub."],"url":"http://arxiv.org/abs/2404.09471v1","category":"cs.PF"}
{"created":"2024-04-15 05:37:43","title":"The Role of Carbon Pricing in Food Inflation: Evidence from Canadian Provinces","abstract":"Carbon pricing, including carbon tax and cap-and-trade, is usually seen as an effective policy tool for mitigating emissions. Although such policies are often perceived as worsening affordability issues, earlier studies find insignificant or deflationary effects of carbon pricing. We verify this result for the food sector by using provincial-level data on food CPI from Canada. By using a staggered difference-in-difference (DiD) approach, we show that the deflationary effects of carbon pricing on food do exist. Additionally, such effects are weak at first and grow stronger after two years of implementation. However, the overall magnitudes are too small to make carbon pricing blamable for the current high inflation. Our subsequent analysis suggests a reduction in consumption is likely to be the cause of deflation. Contrarily, carbon pricing has little impact on farm production costs owing to the special treatment farmers receive within carbon pricing systems. This paper decomposes the long-term influence Canadian carbon pricing has on food affordability and its possible mechanisms.","sentences":["Carbon pricing, including carbon tax and cap-and-trade, is usually seen as an effective policy tool for mitigating emissions.","Although such policies are often perceived as worsening affordability issues, earlier studies find insignificant or deflationary effects of carbon pricing.","We verify this result for the food sector by using provincial-level data on food CPI from Canada.","By using a staggered difference-in-difference (DiD) approach, we show that the deflationary effects of carbon pricing on food do exist.","Additionally, such effects are weak at first and grow stronger after two years of implementation.","However, the overall magnitudes are too small to make carbon pricing blamable for the current high inflation.","Our subsequent analysis suggests a reduction in consumption is likely to be the cause of deflation.","Contrarily, carbon pricing has little impact on farm production costs owing to the special treatment farmers receive within carbon pricing systems.","This paper decomposes the long-term influence Canadian carbon pricing has on food affordability and its possible mechanisms."],"url":"http://arxiv.org/abs/2404.09467v1","category":"econ.EM"}
{"created":"2024-04-15 04:56:15","title":"Optimal Real-time Bidding Strategy For EV Aggregators in Wholesale Electricity Markets","abstract":"With the rapid growth of electric vehicles (EVs), EV aggregators have been playing a increasingly vital role in power systems by not merely providing charging management but also participating in wholesale electricity markets. This work studies the optimal real-time bidding strategy for an EV aggregator. Since the charging process of EVs is time-coupled, it is necessary for EV aggregators to consider future operational conditions (e.g., future EV arrivals) when deciding the current bidding strategy. However, accurately forecasting future operational conditions is challenging under the inherent uncertainties. Hence, there demands a real-time bidding strategy based solely on the up-to-date information, which is the main goal of this work. We start by developing an online optimal EV charging management algorithm for the EV aggregator via Lyapunov optimization. Based on this, an optimal real-time bidding strategy (bidding cost curve and bounds) for the aggregator is derived. Then, an efficient yet practical algorithm is proposed to obtain the bidding strategy. It shows that with the proposed bidding strategy, the aggregator's profit is nearly offline optimal. Moreover, the wholesale electricity market clearing result aligns with the individual aggregator's optimal charging strategy given the prices. Case studies against several benchmarks are conducted to evaluate the performance of the proposed method.","sentences":["With the rapid growth of electric vehicles (EVs), EV aggregators have been playing a increasingly vital role in power systems by not merely providing charging management but also participating in wholesale electricity markets.","This work studies the optimal real-time bidding strategy for an EV aggregator.","Since the charging process of EVs is time-coupled, it is necessary for EV aggregators to consider future operational conditions (e.g., future EV arrivals) when deciding the current bidding strategy.","However, accurately forecasting future operational conditions is challenging under the inherent uncertainties.","Hence, there demands a real-time bidding strategy based solely on the up-to-date information, which is the main goal of this work.","We start by developing an online optimal EV charging management algorithm for the EV aggregator via Lyapunov optimization.","Based on this, an optimal real-time bidding strategy (bidding cost curve and bounds) for the aggregator is derived.","Then, an efficient yet practical algorithm is proposed to obtain the bidding strategy.","It shows that with the proposed bidding strategy, the aggregator's profit is nearly offline optimal.","Moreover, the wholesale electricity market clearing result aligns with the individual aggregator's optimal charging strategy given the prices.","Case studies against several benchmarks are conducted to evaluate the performance of the proposed method."],"url":"http://arxiv.org/abs/2404.09460v1","category":"math.OC"}
{"created":"2024-04-15 04:45:49","title":"Hyperbolic Heterogeneous Graph Attention Networks","abstract":"Most previous heterogeneous graph embedding models represent elements in a heterogeneous graph as vector representations in a low-dimensional Euclidean space. However, because heterogeneous graphs inherently possess complex structures, such as hierarchical or power-law structures, distortions can occur when representing them in Euclidean space. To overcome this limitation, we propose Hyperbolic Heterogeneous Graph Attention Networks (HHGAT) that learn vector representations in hyperbolic spaces with meta-path instances. We conducted experiments on three real-world heterogeneous graph datasets, demonstrating that HHGAT outperforms state-of-the-art heterogeneous graph embedding models in node classification and clustering tasks.","sentences":["Most previous heterogeneous graph embedding models represent elements in a heterogeneous graph as vector representations in a low-dimensional Euclidean space.","However, because heterogeneous graphs inherently possess complex structures, such as hierarchical or power-law structures, distortions can occur when representing them in Euclidean space.","To overcome this limitation, we propose Hyperbolic Heterogeneous Graph Attention Networks (HHGAT) that learn vector representations in hyperbolic spaces with meta-path instances.","We conducted experiments on three real-world heterogeneous graph datasets, demonstrating that HHGAT outperforms state-of-the-art heterogeneous graph embedding models in node classification and clustering tasks."],"url":"http://arxiv.org/abs/2404.09456v1","category":"cs.LG"}
{"created":"2024-04-15 04:43:53","title":"Utility-Fairness Trade-Offs and How to Find Them","abstract":"When building classification systems with demographic fairness considerations, there are two objectives to satisfy: 1) maximizing utility for the specific task and 2) ensuring fairness w.r.t. a known demographic attribute. These objectives often compete, so optimizing both can lead to a trade-off between utility and fairness. While existing works acknowledge the trade-offs and study their limits, two questions remain unanswered: 1) What are the optimal trade-offs between utility and fairness? and 2) How can we numerically quantify these trade-offs from data for a desired prediction task and demographic attribute of interest? This paper addresses these questions. We introduce two utility-fairness trade-offs: the Data-Space and Label-Space Trade-off. The trade-offs reveal three regions within the utility-fairness plane, delineating what is fully and partially possible and impossible. We propose U-FaTE, a method to numerically quantify the trade-offs for a given prediction task and group fairness definition from data samples. Based on the trade-offs, we introduce a new scheme for evaluating representations. An extensive evaluation of fair representation learning methods and representations from over 1000 pre-trained models revealed that most current approaches are far from the estimated and achievable fairness-utility trade-offs across multiple datasets and prediction tasks.","sentences":["When building classification systems with demographic fairness considerations, there are two objectives to satisfy: 1) maximizing utility for the specific task and 2) ensuring fairness w.r.t.","a known demographic attribute.","These objectives often compete, so optimizing both can lead to a trade-off between utility and fairness.","While existing works acknowledge the trade-offs and study their limits, two questions remain unanswered: 1) What are the optimal trade-offs between utility and fairness?","and 2) How can we numerically quantify these trade-offs from data for a desired prediction task and demographic attribute of interest?","This paper addresses these questions.","We introduce two utility-fairness trade-offs: the Data-Space and Label-Space Trade-off.","The trade-offs reveal three regions within the utility-fairness plane, delineating what is fully and partially possible and impossible.","We propose U-FaTE, a method to numerically quantify the trade-offs for a given prediction task and group fairness definition from data samples.","Based on the trade-offs, we introduce a new scheme for evaluating representations.","An extensive evaluation of fair representation learning methods and representations from over 1000 pre-trained models revealed that most current approaches are far from the estimated and achievable fairness-utility trade-offs across multiple datasets and prediction tasks."],"url":"http://arxiv.org/abs/2404.09454v1","category":"cs.CV"}
{"created":"2024-04-15 04:20:17","title":"On maximum residual block Kaczmarz method for solving large consistent linear systems","abstract":"For solving large consistent linear systems by iteration methods, inspired by the maximum residual Kaczmarz method and the randomized block Kaczmarz method, we propose the maximum residual block Kaczmarz method, which is designed to preferentially eliminate the largest block in the residual vector $r_{k}$ at each iteration. At the same time, in order to further improve the convergence rate, we construct the maximum residual average block Kaczmarz method to avoid the calculation of pseudo-inverse in block iteration, which completes the iteration by projecting the iteration vector $x_{k}$ to each row of the constrained subset of $A$ and applying different extrapolation step sizes to average them. We prove the convergence of these two methods and give the upper bounds on their convergence rates, respectively. Numerical experiments validate our theory and show that our proposed methods are superior to some other block Kaczmarz methods.","sentences":["For solving large consistent linear systems by iteration methods, inspired by the maximum residual Kaczmarz method and the randomized block Kaczmarz method, we propose the maximum residual block Kaczmarz method, which is designed to preferentially eliminate the largest block in the residual vector $r_{k}$ at each iteration.","At the same time, in order to further improve the convergence rate, we construct the maximum residual average block Kaczmarz method to avoid the calculation of pseudo-inverse in block iteration, which completes the iteration by projecting the iteration vector $x_{k}$ to each row of the constrained subset of $A$ and applying different extrapolation step sizes to average them.","We prove the convergence of these two methods and give the upper bounds on their convergence rates, respectively.","Numerical experiments validate our theory and show that our proposed methods are superior to some other block Kaczmarz methods."],"url":"http://arxiv.org/abs/2404.09448v1","category":"math.NA"}
{"created":"2024-04-15 04:00:21","title":"Entanglement-assisted quantum transduction","abstract":"A quantum transducer converts an input signal to an output at a different frequency, while maintaining the quantum information with high fidelity. When operating between the microwave and optical frequencies, it is crucial for quantum networking between quantum computers via low-loss optical links, and thereby enabling distributed quantum computing. However, the state-of-the-art quantum transducers suffer from low transduction efficiency due to weak nonlinear coupling, wherein increasing pump power to enhance efficiency leads to inevitable thermal noise from heating. Moreover, we reveal that the efficiency-bandwidth product in such systems is fundamentally limited by pump power and nonlinear coupling coefficient, irrespective of cavity engineering efforts. To resolve the conundrum, we propose to boost the transduction efficiency by consuming entanglement within the same frequency band (e.g., microwave-microwave or optical-optical entanglement). Via a squeezer-coupler-antisqueezer sandwich structure, the protocol enhances the transduction efficiency to unity in the ideal lossless case, given an arbitrarily weak nonlinear coupling, which establishes a high-fidelity quantum communication link without any signal encoding. In practical cavity systems, our entanglement-assisted protocol surpasses the non-assisted fundamental limit of the efficiency-bandwidth product and reduces the threshold cooperativity for positive quantum capacity by a factor proportional to two-mode squeezing gain. Given a fixed cooperativity, our approach increases the broadband quantum capacity by orders of magnitude.","sentences":["A quantum transducer converts an input signal to an output at a different frequency, while maintaining the quantum information with high fidelity.","When operating between the microwave and optical frequencies, it is crucial for quantum networking between quantum computers via low-loss optical links, and thereby enabling distributed quantum computing.","However, the state-of-the-art quantum transducers suffer from low transduction efficiency due to weak nonlinear coupling, wherein increasing pump power to enhance efficiency leads to inevitable thermal noise from heating.","Moreover, we reveal that the efficiency-bandwidth product in such systems is fundamentally limited by pump power and nonlinear coupling coefficient, irrespective of cavity engineering efforts.","To resolve the conundrum, we propose to boost the transduction efficiency by consuming entanglement within the same frequency band (e.g., microwave-microwave or optical-optical entanglement).","Via a squeezer-coupler-antisqueezer sandwich structure, the protocol enhances the transduction efficiency to unity in the ideal lossless case, given an arbitrarily weak nonlinear coupling, which establishes a high-fidelity quantum communication link without any signal encoding.","In practical cavity systems, our entanglement-assisted protocol surpasses the non-assisted fundamental limit of the efficiency-bandwidth product and reduces the threshold cooperativity for positive quantum capacity by a factor proportional to two-mode squeezing gain.","Given a fixed cooperativity, our approach increases the broadband quantum capacity by orders of magnitude."],"url":"http://arxiv.org/abs/2404.09441v1","category":"quant-ph"}
{"created":"2024-04-15 03:32:50","title":"Revisiting some classical linearizations of the quadratic binary optimization problem","abstract":"In this paper, we present several new linearizations of a quadratic binary optimization problem (QBOP), primarily using the method of aggregations. Although aggregations were studied in the past in the context of solving system of Diophantine equations in non-negative variables, none of the approaches developed produced practical models, particularly due to the large size of associate multipliers. Exploiting the special structure of QBOP we show that selective aggregation of constraints provide valid linearizations with interesting properties. For our aggregations, multipliers can be any non-zero real numbers. Moreover, choosing the multipliers appropriately, we demonstrate that the resulting LP relaxations have value identical to the corresponding non-aggregated models. We also provide a review of existing explicit linearizations of QBOP and presents the first systematic study of such models. Theoretical and experimental comparisons of new and existing models are also provided.","sentences":["In this paper, we present several new linearizations of a quadratic binary optimization problem (QBOP), primarily using the method of aggregations.","Although aggregations were studied in the past in the context of solving system of Diophantine equations in non-negative variables, none of the approaches developed produced practical models, particularly due to the large size of associate multipliers.","Exploiting the special structure of QBOP we show that selective aggregation of constraints provide valid linearizations with interesting properties.","For our aggregations, multipliers can be any non-zero real numbers.","Moreover, choosing the multipliers appropriately, we demonstrate that the resulting LP relaxations have value identical to the corresponding non-aggregated models.","We also provide a review of existing explicit linearizations of QBOP and presents the first systematic study of such models.","Theoretical and experimental comparisons of new and existing models are also provided."],"url":"http://arxiv.org/abs/2404.09437v1","category":"cs.DM"}
{"created":"2024-04-15 02:44:48","title":"Pump-locked microcavity Brillouin laser","abstract":"Microcavity-based microlasers are the kernel light sources for integrating photonics and optoelectronics. The traditional pump light frequency locking mainly utilizes a complex system with optoelectronic feedback, which requires a high-cost narrow-linewidth pump laser and limits the application of microlasers in integrated optoelectronic systems. We propose to utilize Rayleigh scattering of microcavities to lock the frequency of the pump laser to the resonant frequency of the laser microcavity with an all-optical method. While compressing the linewidth of the pump laser, it can greatly improve the long-term stability of the optically pumped microcavity laser. In the experiment, the linewidth of the semiconductor pump laser is compressed from the MHz level to the kHz level. The microcavity Brillouin laser achieves an ultra-narrow intrinsic linewidth of 100 Hz, with an ultra-low frequency noise of 35 Hz2/Hz. The constructed microlaser obtains a locking time up to 1 hour, which does not require any temperature control or vibration isolation of the laser system. This work is the first demonstration to achieve an optically pump-locked microcavity Brillouin laser, which provides a stable and reliable low-cost experimental platform for ultra-narrow linewidth lasers, precision laser sensors, microwave-photonic signal synthesizer, and optomechanical systems.","sentences":["Microcavity-based microlasers are the kernel light sources for integrating photonics and optoelectronics.","The traditional pump light frequency locking mainly utilizes a complex system with optoelectronic feedback, which requires a high-cost narrow-linewidth pump laser and limits the application of microlasers in integrated optoelectronic systems.","We propose to utilize Rayleigh scattering of microcavities to lock the frequency of the pump laser to the resonant frequency of the laser microcavity with an all-optical method.","While compressing the linewidth of the pump laser, it can greatly improve the long-term stability of the optically pumped microcavity laser.","In the experiment, the linewidth of the semiconductor pump laser is compressed from the MHz level to the kHz level.","The microcavity Brillouin laser achieves an ultra-narrow intrinsic linewidth of 100 Hz, with an ultra-low frequency noise of 35 Hz2/Hz.","The constructed microlaser obtains a locking time up to 1 hour, which does not require any temperature control or vibration isolation of the laser system.","This work is the first demonstration to achieve an optically pump-locked microcavity Brillouin laser, which provides a stable and reliable low-cost experimental platform for ultra-narrow linewidth lasers, precision laser sensors, microwave-photonic signal synthesizer, and optomechanical systems."],"url":"http://arxiv.org/abs/2404.09427v1","category":"physics.optics"}
{"created":"2024-04-15 02:41:36","title":"Polynomial Fourier Decay For Patterson-Sullivan Measures","abstract":"We show that the Fourier transform of Patterson-Sullivan measures associated to convex cocompact groups of isometries of real hyperbolic space decays polynomially quickly at infinity. The proof is based on the $L^2$-flattening theorem obtained in prior work of the author, combined with a method based on dynamical self-similarity for ruling out the sparse set of potential frequencies where the Fourier transform can be large.","sentences":["We show that the Fourier transform of Patterson-Sullivan measures associated to convex cocompact groups of isometries of real hyperbolic space decays polynomially quickly at infinity.","The proof is based on the $L^2$-flattening theorem obtained in prior work of the author, combined with a method based on dynamical self-similarity for ruling out the sparse set of potential frequencies where the Fourier transform can be large."],"url":"http://arxiv.org/abs/2404.09424v1","category":"math.DS"}
{"created":"2024-04-15 02:36:08","title":"Manipulation of magnetic systems by quantized surface acoustic wave via piezomagnetic effect","abstract":"The quantized surface acoustic wave (SAW) in the piezoelectric medium has recently been studied, and is used to control electric dipoles of quantum systems via the electric field produced through piezoelectric effect. However, it is not easy and convenient to manipulate magnetic moments directly by the electric field. We here study a quantum theory of SAW in the piezomagnetic medium. We show that the intrinsic properties of the piezomagnetic medium enable the SAW in the piezomagnetic medium to directly interact with magnetic moments of quantum systems via magnetic field induced by piezomagnetic effect. By taking the strip SAW waveguide made of piezomagnetic medium as an example, we further study the coupling strengths between different magnetic quantum systems with magnetic moments and the quantized single-mode SAW in the waveguide. Based on this, we discuss the interaction between magnetic quantum systems mediated by the quantized multi-mode SAW in piezomagnetic waveguide. Our study provides a convenient way to directly control magnetic quantum systems by quantized SAW, and offers potential applications to on-chip information processing based on solid-state quantum systems via quantized acoustic wave.","sentences":["The quantized surface acoustic wave (SAW) in the piezoelectric medium has recently been studied, and is used to control electric dipoles of quantum systems via the electric field produced through piezoelectric effect.","However, it is not easy and convenient to manipulate magnetic moments directly by the electric field.","We here study a quantum theory of SAW in the piezomagnetic medium.","We show that the intrinsic properties of the piezomagnetic medium enable the SAW in the piezomagnetic medium to directly interact with magnetic moments of quantum systems via magnetic field induced by piezomagnetic effect.","By taking the strip SAW waveguide made of piezomagnetic medium as an example, we further study the coupling strengths between different magnetic quantum systems with magnetic moments and the quantized single-mode SAW in the waveguide.","Based on this, we discuss the interaction between magnetic quantum systems mediated by the quantized multi-mode SAW in piezomagnetic waveguide.","Our study provides a convenient way to directly control magnetic quantum systems by quantized SAW, and offers potential applications to on-chip information processing based on solid-state quantum systems via quantized acoustic wave."],"url":"http://arxiv.org/abs/2404.09423v1","category":"quant-ph"}
{"created":"2024-04-15 01:34:44","title":"Neuro-Inspired Information-Theoretic Hierarchical Perception for Multimodal Learning","abstract":"Integrating and processing information from various sources or modalities are critical for obtaining a comprehensive and accurate perception of the real world in autonomous systems and cyber-physical systems. Drawing inspiration from neuroscience, we develop the Information-Theoretic Hierarchical Perception (ITHP) model, which utilizes the concept of information bottleneck. Different from most traditional fusion models that incorporate all modalities identically in neural networks, our model designates a prime modality and regards the remaining modalities as detectors in the information pathway, serving to distill the flow of information. Our proposed perception model focuses on constructing an effective and compact information flow by achieving a balance between the minimization of mutual information between the latent state and the input modal state, and the maximization of mutual information between the latent states and the remaining modal states. This approach leads to compact latent state representations that retain relevant information while minimizing redundancy, thereby substantially enhancing the performance of multimodal representation learning. Experimental evaluations on the MUStARD, CMU-MOSI, and CMU-MOSEI datasets demonstrate that our model consistently distills crucial information in multimodal learning scenarios, outperforming state-of-the-art benchmarks. Remarkably, on the CMU-MOSI dataset, ITHP surpasses human-level performance in the multimodal sentiment binary classification task across all evaluation metrics (i.e., Binary Accuracy, F1 Score, Mean Absolute Error, and Pearson Correlation).","sentences":["Integrating and processing information from various sources or modalities are critical for obtaining a comprehensive and accurate perception of the real world in autonomous systems and cyber-physical systems.","Drawing inspiration from neuroscience, we develop the Information-Theoretic Hierarchical Perception (ITHP) model, which utilizes the concept of information bottleneck.","Different from most traditional fusion models that incorporate all modalities identically in neural networks, our model designates a prime modality and regards the remaining modalities as detectors in the information pathway, serving to distill the flow of information.","Our proposed perception model focuses on constructing an effective and compact information flow by achieving a balance between the minimization of mutual information between the latent state and the input modal state, and the maximization of mutual information between the latent states and the remaining modal states.","This approach leads to compact latent state representations that retain relevant information while minimizing redundancy, thereby substantially enhancing the performance of multimodal representation learning.","Experimental evaluations on the MUStARD, CMU-MOSI, and CMU-MOSEI datasets demonstrate that our model consistently distills crucial information in multimodal learning scenarios, outperforming state-of-the-art benchmarks.","Remarkably, on the CMU-MOSI dataset, ITHP surpasses human-level performance in the multimodal sentiment binary classification task across all evaluation metrics (i.e., Binary Accuracy, F1 Score, Mean Absolute Error, and Pearson Correlation)."],"url":"http://arxiv.org/abs/2404.09403v1","category":"cs.LG"}
{"created":"2024-04-15 00:37:59","title":"Fluorite-type materials in the monolayer limit","abstract":"The 2H, 1T, and their distorted structures are known as prototype structures of $AB_2$ monolayers. Here, we study a puckered structure that is truncated from the (110) surface of fluorite-type materials. 53 fluorite-type materials are investigated based on first-principles approach. The formation energy calculations indicate that seven systems form the puckered structure in the monolayer limit, while other systems form either 1T, 2H, or distorted 1T structures. The puckered structures of PbF$_2$, PRh$_2$, and Ga$_2$Au exhibit negative Poisson's ratio (NPR) in the out-of-plane direction. An analytical model for the NPR is derived. The surface energy calculations predict the appearance of NPR.","sentences":["The 2H, 1T, and their distorted structures are known as prototype structures of $AB_2$ monolayers.","Here, we study a puckered structure that is truncated from the (110) surface of fluorite-type materials.","53 fluorite-type materials are investigated based on first-principles approach.","The formation energy calculations indicate that seven systems form the puckered structure in the monolayer limit, while other systems form either 1T, 2H, or distorted 1T structures.","The puckered structures of PbF$_2$, PRh$_2$, and Ga$_2$Au exhibit negative Poisson's ratio (NPR) in the out-of-plane direction.","An analytical model for the NPR is derived.","The surface energy calculations predict the appearance of NPR."],"url":"http://arxiv.org/abs/2404.09394v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-15 00:25:12","title":"An Autoencoder-Based Constellation Design for AirComp in Wireless Federated Learning","abstract":"Wireless federated learning (FL) relies on efficient uplink communications to aggregate model updates across distributed edge devices. Over-the-air computation (a.k.a. AirComp) has emerged as a promising approach for addressing the scalability challenge of FL over wireless links with limited communication resources. Unlike conventional methods, AirComp allows multiple edge devices to transmit uplink signals simultaneously, enabling the parameter server to directly decode the average global model. However, existing AirComp solutions are intrinsically analog, while modern wireless systems predominantly adopt digital modulations. Consequently, careful constellation designs are necessary to accurately decode the sum model updates without ambiguity. In this paper, we propose an end-to-end communication system supporting AirComp with digital modulation, aiming to overcome the challenges associated with accurate decoding of the sum signal with constellation designs. We leverage autoencoder network structures and explore the joint optimization of transmitter and receiver components. Our approach fills an important gap in the context of accurately decoding the sum signal in digital modulation-based AirComp, which can advance the deployment of FL in contemporary wireless systems.","sentences":["Wireless federated learning (FL) relies on efficient uplink communications to aggregate model updates across distributed edge devices.","Over-the-air computation (a.k.a. AirComp) has emerged as a promising approach for addressing the scalability challenge of FL over wireless links with limited communication resources.","Unlike conventional methods, AirComp allows multiple edge devices to transmit uplink signals simultaneously, enabling the parameter server to directly decode the average global model.","However, existing AirComp solutions are intrinsically analog, while modern wireless systems predominantly adopt digital modulations.","Consequently, careful constellation designs are necessary to accurately decode the sum model updates without ambiguity.","In this paper, we propose an end-to-end communication system supporting AirComp with digital modulation, aiming to overcome the challenges associated with accurate decoding of the sum signal with constellation designs.","We leverage autoencoder network structures and explore the joint optimization of transmitter and receiver components.","Our approach fills an important gap in the context of accurately decoding the sum signal in digital modulation-based AirComp, which can advance the deployment of FL in contemporary wireless systems."],"url":"http://arxiv.org/abs/2404.09392v1","category":"cs.IT"}
{"created":"2024-04-15 00:22:09","title":"Skyrmion-mechanical hybrid quantum systems: Manipulation of skyrmion qubits via phonons","abstract":"Skyrmion qubits are a new highly promising logic element for quantum information processing. However, their scalability to multiple interacting qubits remains challenging. We propose a hybrid quantum setup with skyrmion qubits strongly coupled to nanomechanical cantilevers via magnetic coupling, which harnesses phonons as quantum interfaces for the manipulation of distant skyrmion qubits. A linear drive is utilized to achieve the modulation of the stiffness coefficient of the cantilever, resulting in an exponential enhancement of the coupling strength between the skyrmion qubit and the mechanical mode. We also consider the case of a topological resonator array, which allows us to study interactions between skyrmion qubits and topological phonon band structure, as well as chiral skyrmion-skyrmion interactions. The scheme suggested here offers a fascinating platform for investigating quantum information processing and quantum simulation with magnetic microstructures.","sentences":["Skyrmion qubits are a new highly promising logic element for quantum information processing.","However, their scalability to multiple interacting qubits remains challenging.","We propose a hybrid quantum setup with skyrmion qubits strongly coupled to nanomechanical cantilevers via magnetic coupling, which harnesses phonons as quantum interfaces for the manipulation of distant skyrmion qubits.","A linear drive is utilized to achieve the modulation of the stiffness coefficient of the cantilever, resulting in an exponential enhancement of the coupling strength between the skyrmion qubit and the mechanical mode.","We also consider the case of a topological resonator array, which allows us to study interactions between skyrmion qubits and topological phonon band structure, as well as chiral skyrmion-skyrmion interactions.","The scheme suggested here offers a fascinating platform for investigating quantum information processing and quantum simulation with magnetic microstructures."],"url":"http://arxiv.org/abs/2404.09390v1","category":"quant-ph"}
{"created":"2024-04-15 00:19:23","title":"Magnon-Skyrmion Hybrid Quantum Systems: Tailoring Interactions via Magnons","abstract":"Coherent and dissipative interactions between different quantum systems are essential for the construction of hybrid quantum systems and the investigation of novel quantum phenomena. Here, we propose and analyze a magnon-skyrmion hybrid quantum system, consisting of a micromagnet and nearby magnetic skyrmions. We predict a strong coupling mechanism between the magnonic mode of the micromagnet and the quantized helicity degree of freedom of the skyrmion. We show that with this hybrid setup it is possible to induce magnon-mediated nonreciprocal interactions and responses between distant skyrmion qubits or between skyrmion qubits and other quantum systems like superconducting qubits. This work provides a quantum platform for the investigation of diverse quantum effects and quantum information processing with magnetic microstructures.","sentences":["Coherent and dissipative interactions between different quantum systems are essential for the construction of hybrid quantum systems and the investigation of novel quantum phenomena.","Here, we propose and analyze a magnon-skyrmion hybrid quantum system, consisting of a micromagnet and nearby magnetic skyrmions.","We predict a strong coupling mechanism between the magnonic mode of the micromagnet and the quantized helicity degree of freedom of the skyrmion.","We show that with this hybrid setup it is possible to induce magnon-mediated nonreciprocal interactions and responses between distant skyrmion qubits or between skyrmion qubits and other quantum systems like superconducting qubits.","This work provides a quantum platform for the investigation of diverse quantum effects and quantum information processing with magnetic microstructures."],"url":"http://arxiv.org/abs/2404.09388v1","category":"quant-ph"}
{"created":"2024-04-15 00:11:01","title":"Integrating Marketing Channels into Quantile Transformation and Bayesian Optimization of Ensemble Kernels for Sales Prediction with Gaussian Process Models","abstract":"This study introduces an innovative Gaussian Process (GP) model utilizing an ensemble kernel that integrates Radial Basis Function (RBF), Rational Quadratic, and Mat\\'ern kernels for product sales forecasting. By applying Bayesian optimization, we efficiently find the optimal weights for each kernel, enhancing the model's ability to handle complex sales data patterns. Our approach significantly outperforms traditional GP models, achieving a notable 98\\% accuracy and superior performance across key metrics including Mean Squared Error (MSE), Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Coefficient of Determination ($R^2$). This advancement underscores the effectiveness of ensemble kernels and Bayesian optimization in improving predictive accuracy, offering profound implications for machine learning applications in sales forecasting.","sentences":["This study introduces an innovative Gaussian Process (GP) model utilizing an ensemble kernel that integrates Radial Basis Function (RBF), Rational Quadratic, and Mat\\'ern kernels for product sales forecasting.","By applying Bayesian optimization, we efficiently find the optimal weights for each kernel, enhancing the model's ability to handle complex sales data patterns.","Our approach significantly outperforms traditional GP models, achieving a notable 98\\% accuracy and superior performance across key metrics including Mean Squared Error (MSE), Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Coefficient of Determination ($R^2$).","This advancement underscores the effectiveness of ensemble kernels and Bayesian optimization in improving predictive accuracy, offering profound implications for machine learning applications in sales forecasting."],"url":"http://arxiv.org/abs/2404.09386v1","category":"cs.LG"}
{"created":"2024-04-14 23:44:49","title":"Low-Resource Named Entity Recognition with Cross-Lingual, Character-Level Neural Conditional Random Fields","abstract":"Low-resource named entity recognition is still an open problem in NLP. Most state-of-the-art systems require tens of thousands of annotated sentences in order to obtain high performance. However, for most of the world's languages, it is unfeasible to obtain such annotation. In this paper, we present a transfer learning scheme, whereby we train character-level neural CRFs to predict named entities for both high-resource languages and low resource languages jointly. Learning character representations for multiple related languages allows transfer among the languages, improving F1 by up to 9.8 points over a loglinear CRF baseline.","sentences":["Low-resource named entity recognition is still an open problem in NLP.","Most state-of-the-art systems require tens of thousands of annotated sentences in order to obtain high performance.","However, for most of the world's languages, it is unfeasible to obtain such annotation.","In this paper, we present a transfer learning scheme, whereby we train character-level neural CRFs to predict named entities for both high-resource languages and low resource languages jointly.","Learning character representations for multiple related languages allows transfer among the languages, improving F1 by up to 9.8 points over a loglinear CRF baseline."],"url":"http://arxiv.org/abs/2404.09383v1","category":"cs.CL"}
{"created":"2024-04-14 23:38:55","title":"The effects of finite electron inertia on helicity-barrier-mediated turbulence","abstract":"Understanding the partitioning of turbulent energy between ions and electrons in weakly collisional plasmas is crucial for the accurate interpretation of observations and modelling of various astrophysical phenomena. Many such plasmas are \"imbalanced\", wherein the large-scale energy input is dominated by Alfv\\'enic fluctuations propagating in a single direction. In this paper, we demonstrate that when strongly-magnetised plasma turbulence is imbalanced, nonlinear conservation laws imply the existence of a critical value of the electron plasma beta that separates two dramatically different types of turbulence in parameter space. For betas below the critical value, the free energy injected on the largest scales is able to undergo a familiar Kolmogorov-type cascade to small scales where it is dissipated, heating electrons. For betas above the critical value, the system forms a \"helicity barrier\" that prevents the cascade from proceeding past the ion Larmor radius, causing the majority of the injected free energy to be deposited into ion heating. Physically, the helicity barrier results from the inability of the system to adjust to the disparity between the perpendicular-wavenumber scalings of the free energy and generalised helicity below the ion Larmor radius; restoring finite electron inertia can annul, or even reverse, this disparity, giving rise to the aforementioned critical beta. We relate this physics to the \"dynamic phase alignment\" mechanism, and characterise various other important features of the helicity barrier, including the nature of the nonlinear wavenumber-space fluxes, dissipation rates, and energy spectra. The existence of such a critical beta has important implications for heating, as it suggests that the dominant recipient of the turbulent energy -- ions or electrons -- can depend sensitively on the characteristics of the plasma at large scales.","sentences":["Understanding the partitioning of turbulent energy between ions and electrons in weakly collisional plasmas is crucial for the accurate interpretation of observations and modelling of various astrophysical phenomena.","Many such plasmas are \"imbalanced\", wherein the large-scale energy input is dominated by Alfv\\'enic fluctuations propagating in a single direction.","In this paper, we demonstrate that when strongly-magnetised plasma turbulence is imbalanced, nonlinear conservation laws imply the existence of a critical value of the electron plasma beta that separates two dramatically different types of turbulence in parameter space.","For betas below the critical value, the free energy injected on the largest scales is able to undergo a familiar Kolmogorov-type cascade to small scales where it is dissipated, heating electrons.","For betas above the critical value, the system forms a \"helicity barrier\" that prevents the cascade from proceeding past the ion Larmor radius, causing the majority of the injected free energy to be deposited into ion heating.","Physically, the helicity barrier results from the inability of the system to adjust to the disparity between the perpendicular-wavenumber scalings of the free energy and generalised helicity below the ion Larmor radius; restoring finite electron inertia can annul, or even reverse, this disparity, giving rise to the aforementioned critical beta.","We relate this physics to the \"dynamic phase alignment\" mechanism, and characterise various other important features of the helicity barrier, including the nature of the nonlinear wavenumber-space fluxes, dissipation rates, and energy spectra.","The existence of such a critical beta has important implications for heating, as it suggests that the dominant recipient of the turbulent energy -- ions or electrons -- can depend sensitively on the characteristics of the plasma at large scales."],"url":"http://arxiv.org/abs/2404.09380v1","category":"physics.plasm-ph"}
{"created":"2024-04-14 22:16:19","title":"Locuaz: an in-silico platform for antibody fragments optimization","abstract":"Motivation: Engineering high-affinity binders targeting specific antigenic determinants remains a challenging and often daunting task, requiring extensive experimental screening. Computational methods have the potential to accelerate this process, reducing costs and time, but only if they demonstrate broad applicability and efficiency in exploring mutations, evaluating affinity, and pruning unproductive mutation paths. Results: In response to these challenges, we introduce a new computational platform for optimizing protein binders towards their targets. The platform is organized as a series of modules, performing mutation selection and application, molecular dynamics (MD) simulations to sample conformations around interaction poses, and mutation prioritization using suitable scoring functions. Notably, the platform supports parallel exploration of different mutation streams, enabling in silico high-throughput screening on HPC systems. Furthermore, the platform is highly customizable, allowing users to implement their own protocols. Availability and implementation: the source code is available at https://github. com/pgbarletta/locuaz and documentation is at https://locuaz.readthedocs.io/","sentences":["Motivation: Engineering high-affinity binders targeting specific antigenic determinants remains a challenging and often daunting task, requiring extensive experimental screening.","Computational methods have the potential to accelerate this process, reducing costs and time, but only if they demonstrate broad applicability and efficiency in exploring mutations, evaluating affinity, and pruning unproductive mutation paths.","Results:","In response to these challenges, we introduce a new computational platform for optimizing protein binders towards their targets.","The platform is organized as a series of modules, performing mutation selection and application, molecular dynamics (MD) simulations to sample conformations around interaction poses, and mutation prioritization using suitable scoring functions.","Notably, the platform supports parallel exploration of different mutation streams, enabling in silico high-throughput screening on HPC systems.","Furthermore, the platform is highly customizable, allowing users to implement their own protocols.","Availability and implementation: the source code is available at https://github. com/pgbarletta/locuaz and documentation is at https://locuaz.readthedocs.io/"],"url":"http://arxiv.org/abs/2404.09370v1","category":"physics.bio-ph"}
{"created":"2024-04-14 22:04:55","title":"Entangled nematic disclinations using multi-particle collision dynamics","abstract":"Colloids dispersed in nematic liquid crystals form topological composites in which colloid-associated defects mediate interactions while adhering to fundamental topological constraints. Better realising the promise of such materials requires numerical methods that model nematic inclusions in dynamic and complex scenarios. We employ a mesoscale approach for simulating colloids as mobile surfaces embedded in a fluctuating nematohydrodynamic medium to study the kinetics of colloidal entanglement. In addition to reproducing far-field interactions, topological properties of disclination loops are resolved to reveal their metastable states and topological transitions during relaxation towards ground state. The intrinsic hydrodynamic fluctuations distinguish formerly unexplored far-from-equilibrium disclination states, including configurations with localised positive winding profiles. The adaptability and precision of this numerical approach offers promising avenues for studying the dynamics of colloids and topological defects in designed and out-of-equilibrium situations.","sentences":["Colloids dispersed in nematic liquid crystals form topological composites in which colloid-associated defects mediate interactions while adhering to fundamental topological constraints.","Better realising the promise of such materials requires numerical methods that model nematic inclusions in dynamic and complex scenarios.","We employ a mesoscale approach for simulating colloids as mobile surfaces embedded in a fluctuating nematohydrodynamic medium to study the kinetics of colloidal entanglement.","In addition to reproducing far-field interactions, topological properties of disclination loops are resolved to reveal their metastable states and topological transitions during relaxation towards ground state.","The intrinsic hydrodynamic fluctuations distinguish formerly unexplored far-from-equilibrium disclination states, including configurations with localised positive winding profiles.","The adaptability and precision of this numerical approach offers promising avenues for studying the dynamics of colloids and topological defects in designed and out-of-equilibrium situations."],"url":"http://arxiv.org/abs/2404.09368v1","category":"cond-mat.soft"}
{"created":"2024-04-14 20:57:32","title":"Service Weaver: A Promising Direction for Cloud-native Systems?","abstract":"Cloud-native and microservice architectures have taken over the development world by storm. While being incredibly scalable and resilient, microservice architectures also come at the cost of increased overhead to build and maintain. Google's Service Weaver aims to simplify the complexities associated with implementing cloud-native systems by introducing the concept of a single modular binary composed of agent-like components, thereby abstracting away the microservice architecture notion of individual services. While Service Weaver presents a promising approach to streamline the development of cloud-native applications and addresses nearly all significant aspects of conventional cloud-native systems, there are existing tradeoffs affecting the overall functionality of the system. Notably, Service Weaver's straightforward implementation and deployment of components alleviate the overhead of constructing a complex microservice architecture. However, it is important to acknowledge that certain features, including separate code bases, routing mechanisms, resiliency, and security, are presently lacking in the framework.","sentences":["Cloud-native and microservice architectures have taken over the development world by storm.","While being incredibly scalable and resilient, microservice architectures also come at the cost of increased overhead to build and maintain.","Google's Service Weaver aims to simplify the complexities associated with implementing cloud-native systems by introducing the concept of a single modular binary composed of agent-like components, thereby abstracting away the microservice architecture notion of individual services.","While Service Weaver presents a promising approach to streamline the development of cloud-native applications and addresses nearly all significant aspects of conventional cloud-native systems, there are existing tradeoffs affecting the overall functionality of the system.","Notably, Service Weaver's straightforward implementation and deployment of components alleviate the overhead of constructing a complex microservice architecture.","However, it is important to acknowledge that certain features, including separate code bases, routing mechanisms, resiliency, and security, are presently lacking in the framework."],"url":"http://arxiv.org/abs/2404.09357v1","category":"cs.SE"}
{"created":"2024-04-14 20:48:43","title":"Shape equilibria of vesicles with rigid planar inclusions","abstract":"Motivated by recent studies of two-phase lipid vesicles possessing 2D solid domains integrated within a fluid bilayer phase, we study the shape equilibria of closed vesicles possessing a single planar, circular inclusion. While 2D solid elasticity tends to expel Gaussian curvature, topology requires closed vesicles to maintain an average, non-zero Gaussian curvature leading to an elementary mechanism of shape frustration that increases with inclusion size. We study elastic ground states of the Helfrich model of the planar-fluid composite vesicles, analytically and computationally, as a function of planar fraction and reduced volume. Notably, we show that incorporation of a planar inclusion of only a few percent dramatically shifts the ground state shapes of vesicles from predominantly {\\it prolate} to {\\it oblate}, and moreover, shifts the optimal surface to volume ratio far from spherical shapes. We show that for sufficiently small planar inclusions, the elastic ground states break symmetry via a complex variety of asymmetric oblate, prolate, and triaxial shapes, while inclusion sizes above about $8\\%$ drive composite vesicles to adopt axisymmetric oblate shapes. These predictions cast useful light on the emergent shape and mechanical responses of fluid-solid composite vesicles.","sentences":["Motivated by recent studies of two-phase lipid vesicles possessing 2D solid domains integrated within a fluid bilayer phase, we study the shape equilibria of closed vesicles possessing a single planar, circular inclusion.","While 2D solid elasticity tends to expel Gaussian curvature, topology requires closed vesicles to maintain an average, non-zero Gaussian curvature leading to an elementary mechanism of shape frustration that increases with inclusion size.","We study elastic ground states of the Helfrich model of the planar-fluid composite vesicles, analytically and computationally, as a function of planar fraction and reduced volume.","Notably, we show that incorporation of a planar inclusion of only a few percent dramatically shifts the ground state shapes of vesicles from predominantly {\\it prolate} to {\\it oblate}, and moreover, shifts the optimal surface to volume ratio far from spherical shapes.","We show that for sufficiently small planar inclusions, the elastic ground states break symmetry via a complex variety of asymmetric oblate, prolate, and triaxial shapes, while inclusion sizes above about $8\\%$ drive composite vesicles to adopt axisymmetric oblate shapes.","These predictions cast useful light on the emergent shape and mechanical responses of fluid-solid composite vesicles."],"url":"http://arxiv.org/abs/2404.09355v1","category":"cond-mat.soft"}
{"created":"2024-04-14 20:17:14","title":"Machine learning-based identification of Gaia astrometric exoplanet orbits","abstract":"The third Gaia data release (DR3) contains $\\sim$170 000 astrometric orbit solutions of two-body systems located within $\\sim$500 pc of the Sun. Determining component masses in these systems, in particular of stars hosting exoplanets, usually hinges on incorporating complementary observations in addition to the astrometry, e.g. spectroscopy and radial velocities. Several DR3 two-body systems with exoplanet, brown-dwarf, stellar, and black-hole components have been confirmed in this way. We developed an alternative machine learning approach that uses only the DR3 orbital solutions with the aim of identifying the best candidates for exoplanets and brown-dwarf companions. Based on confirmed substellar companions in the literature, we use semi-supervised anomaly detection methods in combination with extreme gradient boosting and random forest classifiers to determine likely low-mass outliers in the population of non-single sources. We employ and study feature importance to investigate the method's plausibility and produced a list of 22 best candidates of which four are exoplanet candidates and another five are either very-massive brown dwarfs or very-low mass stars. Three candidates, including one initial exoplanet candidate, correspond to false-positive solutions where longer-period binary star motion was fitted with a biased shorter-period orbit. We highlight nine candidates with brown-dwarf companions for preferential follow-up. One candidate companion around the Sun-like star G 15-6 could be confirmed as a genuine brown dwarf using external radial-velocity data. This new approach is a powerful complement to the traditional identification methods for substellar companions among Gaia astrometric orbits. It is particularly relevant in the context of Gaia DR4 and its expected exoplanet discovery yield.","sentences":["The third Gaia data release (DR3) contains $\\sim$170 000 astrometric orbit solutions of two-body systems located within $\\sim$500 pc of the Sun.","Determining component masses in these systems, in particular of stars hosting exoplanets, usually hinges on incorporating complementary observations in addition to the astrometry, e.g. spectroscopy and radial velocities.","Several DR3 two-body systems with exoplanet, brown-dwarf, stellar, and black-hole components have been confirmed in this way.","We developed an alternative machine learning approach that uses only the DR3 orbital solutions with the aim of identifying the best candidates for exoplanets and brown-dwarf companions.","Based on confirmed substellar companions in the literature, we use semi-supervised anomaly detection methods in combination with extreme gradient boosting and random forest classifiers to determine likely low-mass outliers in the population of non-single sources.","We employ and study feature importance to investigate the method's plausibility and produced a list of 22 best candidates of which four are exoplanet candidates and another five are either very-massive brown dwarfs or very-low mass stars.","Three candidates, including one initial exoplanet candidate, correspond to false-positive solutions where longer-period binary star motion was fitted with a biased shorter-period orbit.","We highlight nine candidates with brown-dwarf companions for preferential follow-up.","One candidate companion around the Sun-like star G 15-6 could be confirmed as a genuine brown dwarf using external radial-velocity data.","This new approach is a powerful complement to the traditional identification methods for substellar companions among Gaia astrometric orbits.","It is particularly relevant in the context of Gaia DR4 and its expected exoplanet discovery yield."],"url":"http://arxiv.org/abs/2404.09350v1","category":"astro-ph.EP"}
{"created":"2024-04-14 20:12:29","title":"Multifractal Analysis of F-exponents for Finitely Irreducible Conformal Graph Directed Markov Systems","abstract":"Let $\\Phi = \\{\\phi_e\\}_{e\\in E}$ be a finitely irreducible conformal graph directed Markov system (CGDMS) with symbolic representation $E_A^{\\infty}$ and limit set $J$. Under a mild condition on the system, we give a multifractal analysis of level sets of Birkhoff averages with respect to Hausdorff dimension for a large family of functions. We then apply these results to a few examples in the case of both $E$ finite and $E$ countably infinite.","sentences":["Let $\\Phi = \\{\\phi_e\\}_{e\\in E}$ be a finitely irreducible conformal graph directed Markov system (CGDMS) with symbolic representation $E_A^{\\infty}$ and limit set $J$. Under a mild condition on the system, we give a multifractal analysis of level sets of Birkhoff averages with respect to Hausdorff dimension for a large family of functions.","We then apply these results to a few examples in the case of both $E$ finite and $E$ countably infinite."],"url":"http://arxiv.org/abs/2404.09348v1","category":"math.DS"}
{"created":"2024-04-14 19:55:21","title":"Threshold Current Density for Diffusion-controlled Stability of Electrolytic Surface Nanobubbles","abstract":"Understanding the stability mechanism of surface micro/nanobubbles adhered to gas-evolving electrodes is essential for improving the efficiency of water electrolysis, which is known to be hindered by the bubble coverage on electrodes. Using molecular simulations, the diffusion-controlled evolution of single electrolytic nanobubbles on wettability-patterned nanoelectrodes is investigated. These nanoelectrodes feature hydrophobic islands as preferential nucleation sites and allow the growth of nanobubbles in the pinning mode. In these simulations, a threshold current density distinguishing stable nanobubbles from unstable nanobubbles is found. When the current density remains below the threshold value, nucleated nanobubbles grow to their equilibrium states, maintaining their nanoscopic size. However, for the current density above the threshold value, nanobubbles undergo unlimited growth and can eventually detach due to buoyancy. Increasing the pinning length of nanobubbles increases the degree of nanobubble instability. By connecting the current density with the local gas oversaturation, an extension of the stability theory for surface nanobubbles [Lohse and Zhang, Phys. Rev. E, 2015, 91, 031003(R)] accurately predicts the nanobubble behavior found in molecular simulations, including equilibrium contact angles and the threshold current density. For larger systems that are not accessible to molecular simulations, continuum numerical simulations with the finite difference method combined with the immersed boundary method are performed, again demonstrating good agreement between numerics and theories.","sentences":["Understanding the stability mechanism of surface micro/nanobubbles adhered to gas-evolving electrodes is essential for improving the efficiency of water electrolysis, which is known to be hindered by the bubble coverage on electrodes.","Using molecular simulations, the diffusion-controlled evolution of single electrolytic nanobubbles on wettability-patterned nanoelectrodes is investigated.","These nanoelectrodes feature hydrophobic islands as preferential nucleation sites and allow the growth of nanobubbles in the pinning mode.","In these simulations, a threshold current density distinguishing stable nanobubbles from unstable nanobubbles is found.","When the current density remains below the threshold value, nucleated nanobubbles grow to their equilibrium states, maintaining their nanoscopic size.","However, for the current density above the threshold value, nanobubbles undergo unlimited growth and can eventually detach due to buoyancy.","Increasing the pinning length of nanobubbles increases the degree of nanobubble instability.","By connecting the current density with the local gas oversaturation, an extension of the stability theory for surface nanobubbles","[Lohse and Zhang, Phys.","Rev. E, 2015, 91, 031003(R)] accurately predicts the nanobubble behavior found in molecular simulations, including equilibrium contact angles and the threshold current density.","For larger systems that are not accessible to molecular simulations, continuum numerical simulations with the finite difference method combined with the immersed boundary method are performed, again demonstrating good agreement between numerics and theories."],"url":"http://arxiv.org/abs/2404.09344v1","category":"physics.flu-dyn"}
{"created":"2024-04-14 19:51:32","title":"Face-voice Association in Multilingual Environments (FAME) Challenge 2024 Evaluation Plan","abstract":"The advancements of technology have led to the use of multimodal systems in various real-world applications. Among them, the audio-visual systems are one of the widely used multimodal systems. In the recent years, associating face and voice of a person has gained attention due to presence of unique correlation between them. The Face-voice Association in Multilingual Environments (FAME) Challenge 2024 focuses on exploring face-voice association under a unique condition of multilingual scenario. This condition is inspired from the fact that half of the world's population is bilingual and most often people communicate under multilingual scenario. The challenge uses a dataset namely, Multilingual Audio-Visual (MAV-Celeb) for exploring face-voice association in multilingual environments. This report provides the details of the challenge, dataset, baselines and task details for the FAME Challenge.","sentences":["The advancements of technology have led to the use of multimodal systems in various real-world applications.","Among them, the audio-visual systems are one of the widely used multimodal systems.","In the recent years, associating face and voice of a person has gained attention due to presence of unique correlation between them.","The Face-voice Association in Multilingual Environments (FAME) Challenge 2024 focuses on exploring face-voice association under a unique condition of multilingual scenario.","This condition is inspired from the fact that half of the world's population is bilingual and most often people communicate under multilingual scenario.","The challenge uses a dataset namely, Multilingual Audio-Visual (MAV-Celeb) for exploring face-voice association in multilingual environments.","This report provides the details of the challenge, dataset, baselines and task details for the FAME Challenge."],"url":"http://arxiv.org/abs/2404.09342v1","category":"cs.CV"}
{"created":"2024-04-14 19:33:42","title":"Asymptotics of Bergman polynomials for domains with reflection-invariant corners","abstract":"We study the asymptotic behavior of the Bergman orthogonal polynomials $(p_n)_{n=0}^{\\infty}$ for a class of bounded simply connected domains $D$. The class is defined by the requirement that conformal maps $\\varphi$ of $D$ onto the unit disk extend analytically across the boundary $L$ of $D$, and that $\\varphi'$ has a finite number of zeros $z_1,\\ldots, z_q$ on $L$. The boundary $L$ is then piecewise analytic with corners at the zeros of $\\varphi'$. A result of Stylianopoulos implies that a Carleman-type strong asymptotic formula for $p_n$ holds on the exterior domain $\\mathbb{C}\\setminus\\overline{D}$. We prove that the same formula remains valid across $L\\setminus\\{z_1,\\ldots,z_q\\}$ and on a maximal open subset of $D$. As a consequence, the only boundary points that attract zeros of $p_n$ are the corners. This is in stark contrast to the case when $\\varphi$ fails to admit an analytic extension past $L$, since when this happens the zero counting measure of $p_n$ is known to approach the equilibrium measure for $L$ along suitable subsequences.","sentences":["We study the asymptotic behavior of the Bergman orthogonal polynomials $(p_n)_{n=0}^{\\infty}$ for a class of bounded simply connected domains $D$. The class is defined by the requirement that conformal maps $\\varphi$ of $D$ onto the unit disk extend analytically across the boundary $L$ of $D$, and that $\\varphi'$ has a finite number of zeros $z_1,\\ldots, z_q$ on $L$. The boundary $L$ is then piecewise analytic with corners at the zeros of $\\varphi'$. A result of Stylianopoulos implies that a Carleman-type strong asymptotic formula for $p_n$ holds on the exterior domain $\\mathbb{C}\\setminus\\overline{D}$. We prove that the same formula remains valid across $L\\setminus\\{z_1,\\ldots,z_q\\}$ and on a maximal open subset of $D$. As a consequence, the only boundary points that attract zeros of $p_n$ are the corners.","This is in stark contrast to the case when $\\varphi$ fails to admit an analytic extension past $L$, since when this happens the zero counting measure of $p_n$ is known to approach the equilibrium measure for $L$ along suitable subsequences."],"url":"http://arxiv.org/abs/2404.09335v1","category":"math.CV"}
{"created":"2024-04-14 18:53:49","title":"Exceptionally High Two-Photon Absorption in Diazaacene-Bithiophene Derivatives: A Combined Experimental and Theoretical Approach","abstract":"This study delves into the enhancement of two-photon absorption (2PA) properties in diazaacene-bithiophene derivatives through a synergistic approach combining theoretical analysis and experimental validation. By investigating the structural modifications and their impact on 2PA cross sections, we identify key factors that significantly influence the 2PA efficiency. For all molecular systems studied, our state-of-the-art quantum chemical calculations show a very high involvement of the first excited singlet state (S1) in the 2PA processes into higher excited states, even if this state itself has only a small 2PA cross section for symmetry reasons. Consequently, both the oscillator strength of S1 and the transition dipole moments between S1 and other excited states are of importance, underscoring the role of electronic polarizability in facilitating effective two-photon interactions. The investigated compounds exhibit large 2PA cross sections over a wide near-infrared spectral range reaching giant values of 42000 GM. The introduction of diazine and diazaacene moieties into bithiophene derivatives not only induces charge transfer but also opens up pathways for the creation of materials with tailored nonlinear optical responses, suggesting potential applications in nonlinear optics.","sentences":["This study delves into the enhancement of two-photon absorption (2PA) properties in diazaacene-bithiophene derivatives through a synergistic approach combining theoretical analysis and experimental validation.","By investigating the structural modifications and their impact on 2PA cross sections, we identify key factors that significantly influence the 2PA efficiency.","For all molecular systems studied, our state-of-the-art quantum chemical calculations show a very high involvement of the first excited singlet state (S1) in the 2PA processes into higher excited states, even if this state itself has only a small 2PA cross section for symmetry reasons.","Consequently, both the oscillator strength of S1 and the transition dipole moments between S1 and other excited states are of importance, underscoring the role of electronic polarizability in facilitating effective two-photon interactions.","The investigated compounds exhibit large 2PA cross sections over a wide near-infrared spectral range reaching giant values of 42000 GM.","The introduction of diazine and diazaacene moieties into bithiophene derivatives not only induces charge transfer but also opens up pathways for the creation of materials with tailored nonlinear optical responses, suggesting potential applications in nonlinear optics."],"url":"http://arxiv.org/abs/2404.09325v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-14 18:35:55","title":"MPC Based Linear Equivalence with Control Barrier Functions for VTOL-UAVs","abstract":"In this work, we propose a cascaded scheme of linear Model prediction Control (MPC) based on Control Barrier Functions (CBF) with Dynamic Feedback Linearization (DFL) for Vertical Take-off and Landing (VTOL) Unmanned Aerial Vehicles (UAVs). CBF is a tool that allows enforcement of forward invariance of a set using Lyapunov-like functions to ensure safety. The First control synthesis that employed CBF was based on Quadratic Program (QP) that modifies the existing controller to satisfy the safety requirements. However, the CBF-QP-based controllers leading to longer detours and undesirable transient performance. Recent contributions utilize the framework of MPC benefiting from the prediction capabilities and constraints imposed on the state and control inputs. Due to the intrinsic nonlinearities of the dynamics of robotics systems, all the existing MPC-CBF solutions rely on nonlinear MPC formulations or operate on less accurate linear models. In contrast, our novel solution unlocks the benefits of linear MPC-CBF while considering the full underactuated dynamics without any linear approximations. The cascaded scheme converts the problem of safe VTOL-UAV navigation to a Quadratic Constraint Quadratic Programming (QCQP) problem solved efficiently by off-the-shelf solvers. The closed-loop stability and recursive feasibility is proved along with numerical simulations showing the effective and robust solutions. Keywords: Unmanned Aerial Vehicles, Vertical Take-off and Landing, Model Predictive Control, MPC, Nonlinearity, Dynamic Feedback Linearization, Optimal Control.","sentences":["In this work, we propose a cascaded scheme of linear Model prediction Control (MPC) based on Control Barrier Functions (CBF) with Dynamic Feedback Linearization (DFL) for Vertical Take-off and Landing (VTOL) Unmanned Aerial Vehicles (UAVs).","CBF is a tool that allows enforcement of forward invariance of a set using Lyapunov-like functions to ensure safety.","The First control synthesis that employed CBF was based on Quadratic Program (QP) that modifies the existing controller to satisfy the safety requirements.","However, the CBF-QP-based controllers leading to longer detours and undesirable transient performance.","Recent contributions utilize the framework of MPC benefiting from the prediction capabilities and constraints imposed on the state and control inputs.","Due to the intrinsic nonlinearities of the dynamics of robotics systems, all the existing MPC-CBF solutions rely on nonlinear MPC formulations or operate on less accurate linear models.","In contrast, our novel solution unlocks the benefits of linear MPC-CBF while considering the full underactuated dynamics without any linear approximations.","The cascaded scheme converts the problem of safe VTOL-UAV navigation to a Quadratic Constraint Quadratic Programming (QCQP) problem solved efficiently by off-the-shelf solvers.","The closed-loop stability and recursive feasibility is proved along with numerical simulations showing the effective and robust solutions.","Keywords: Unmanned Aerial Vehicles, Vertical Take-off and Landing, Model Predictive Control, MPC, Nonlinearity, Dynamic Feedback Linearization, Optimal Control."],"url":"http://arxiv.org/abs/2404.09320v1","category":"eess.SY"}
{"created":"2024-04-14 18:31:37","title":"Impact of curved elements for flows over orography with a Discontinuous Galerkin scheme","abstract":"We present a quantitative assessment of the impact of high-order mappings on the simulation of flows over complex orography. Curved boundaries were not used in earlier numerical methods, whereas they are employed nowadays to an increasing extent in combination with high-order methods, such as the Finite Element Method (FEM) and the Spectral Element Method (SEM). We consider here a specific Discontinuous Galerkin (DG) method implemented in the framework of the deal.II library, which natively supports high-order mappings. A number of numerical experiments based on classical benchmarks over idealized orographic profiles demonstrate the positive impact of curved boundaries on the accuracy of the results. These findings are also supported by results of the application of this approach to non-smooth and realistic orographic profiles.","sentences":["We present a quantitative assessment of the impact of high-order mappings on the simulation of flows over complex orography.","Curved boundaries were not used in earlier numerical methods, whereas they are employed nowadays to an increasing extent in combination with high-order methods, such as the Finite Element Method (FEM) and the Spectral Element Method (SEM).","We consider here a specific Discontinuous Galerkin (DG) method implemented in the framework of the deal.","II library, which natively supports high-order mappings.","A number of numerical experiments based on classical benchmarks over idealized orographic profiles demonstrate the positive impact of curved boundaries on the accuracy of the results.","These findings are also supported by results of the application of this approach to non-smooth and realistic orographic profiles."],"url":"http://arxiv.org/abs/2404.09319v1","category":"physics.ao-ph"}
{"created":"2024-04-14 18:27:44","title":"Unraveling stochastic fundamental diagrams considering empirical knowledge: modeling, limitation and further discussion","abstract":"Traffic flow modeling relies heavily on fundamental diagrams. However, deterministic fundamental diagrams, such as single or multi-regime models, cannot capture the uncertainty pattern that underlies traffic flow. To address this limitation, a sparse non-parametric regression model is proposed in this paper to formulate the stochastic fundamental diagram. Unlike parametric stochastic fundamental diagram models, a non-parametric model is insensitive to parameters, flexible, and applicable. The computation complexity and the huge memory required for training in the Gaussian process regression have been reduced by introducing the sparse Gaussian process regression. The paper also discusses how empirical knowledge influences the modeling process. The paper analyzes the influence of modeling empirical knowledge in the prior of the stochastic fundamental diagram model and whether empirical knowledge can improve the robustness and accuracy of the proposed model. By introducing several well-known single-regime fundamental diagram models as the prior and testing the model's robustness and accuracy with different sampling methods given real-world data, the authors find that empirical knowledge can only benefit the model under small inducing samples given a relatively clean and large dataset. A pure data-driven approach is sufficient to estimate and describe the pattern of the density-speed relationship.","sentences":["Traffic flow modeling relies heavily on fundamental diagrams.","However, deterministic fundamental diagrams, such as single or multi-regime models, cannot capture the uncertainty pattern that underlies traffic flow.","To address this limitation, a sparse non-parametric regression model is proposed in this paper to formulate the stochastic fundamental diagram.","Unlike parametric stochastic fundamental diagram models, a non-parametric model is insensitive to parameters, flexible, and applicable.","The computation complexity and the huge memory required for training in the Gaussian process regression have been reduced by introducing the sparse Gaussian process regression.","The paper also discusses how empirical knowledge influences the modeling process.","The paper analyzes the influence of modeling empirical knowledge in the prior of the stochastic fundamental diagram model and whether empirical knowledge can improve the robustness and accuracy of the proposed model.","By introducing several well-known single-regime fundamental diagram models as the prior and testing the model's robustness and accuracy with different sampling methods given real-world data, the authors find that empirical knowledge can only benefit the model under small inducing samples given a relatively clean and large dataset.","A pure data-driven approach is sufficient to estimate and describe the pattern of the density-speed relationship."],"url":"http://arxiv.org/abs/2404.09318v1","category":"stat.AP"}
{"created":"2024-04-14 18:08:23","title":"Numerical Discretization Methods for the Extended Linear Quadratic Control Problem","abstract":"In this study, we introduce numerical methods for discretizing continuous-time linear-quadratic optimal control problems (LQ-OCPs). The discretization of continuous-time LQ-OCPs is formulated into differential equation systems, and we can obtain the discrete equivalent by solving these systems. We present the ordinary differential equation (ODE), matrix exponential, and a novel step-doubling method for the discretization of LQ-OCPs. Utilizing Euler-Maruyama discretization with a fine step, we reformulate the costs of continuous-time stochastic LQ-OCPs into a quadratic form, and show that the stochastic cost follows the $\\chi^2$ distribution. In the numerical experiment, we test and compare the proposed numerical methods. The results ensure that the discrete-time LQ-OCP derived using the proposed numerical methods is equivalent to the original problem.","sentences":["In this study, we introduce numerical methods for discretizing continuous-time linear-quadratic optimal control problems (LQ-OCPs).","The discretization of continuous-time LQ-OCPs is formulated into differential equation systems, and we can obtain the discrete equivalent by solving these systems.","We present the ordinary differential equation (ODE), matrix exponential, and a novel step-doubling method for the discretization of LQ-OCPs.","Utilizing Euler-Maruyama discretization with a fine step, we reformulate the costs of continuous-time stochastic LQ-OCPs into a quadratic form, and show that the stochastic cost follows the $\\chi^2$ distribution.","In the numerical experiment, we test and compare the proposed numerical methods.","The results ensure that the discrete-time LQ-OCP derived using the proposed numerical methods is equivalent to the original problem."],"url":"http://arxiv.org/abs/2404.09316v1","category":"eess.SY"}
{"created":"2024-04-14 17:59:08","title":"Exclusive-or encoded algebraic structure for efficient quantum dynamics","abstract":"We propose a formalism that captures the algebraic structure of many-body two-level quantum systems, and directly motivates an efficient numerical method. This formalism is based on the binary representation of the enumeration-indices of the elements of the corresponding Lie algebra. The action of arbitrarily large elements of that algebra reduces to a few bit-wise exclusive-or operations. This formalism naturally produces sparse representations of many-body density operators, the size of which we control through a dynamic truncation method. We demonstrate how this formalism applies to real-time evolution, dissipative Lindblad action, imaginary-time evolution, and projective measurement processes. We find that this approach to calculating quantum dynamics scales close to linearly with the number of non-zero components in the density operator. We refer to this exclusive-or represented quantum algebra as ORQA. As a proof of concept, we provide a numerical demonstration of this formalism by simulating quantum annealing processes for the maximum independent set problem for up to 22 two-level systems.","sentences":["We propose a formalism that captures the algebraic structure of many-body two-level quantum systems, and directly motivates an efficient numerical method.","This formalism is based on the binary representation of the enumeration-indices of the elements of the corresponding Lie algebra.","The action of arbitrarily large elements of that algebra reduces to a few bit-wise exclusive-or operations.","This formalism naturally produces sparse representations of many-body density operators, the size of which we control through a dynamic truncation method.","We demonstrate how this formalism applies to real-time evolution, dissipative Lindblad action, imaginary-time evolution, and projective measurement processes.","We find that this approach to calculating quantum dynamics scales close to linearly with the number of non-zero components in the density operator.","We refer to this exclusive-or represented quantum algebra as ORQA.","As a proof of concept, we provide a numerical demonstration of this formalism by simulating quantum annealing processes for the maximum independent set problem for up to 22 two-level systems."],"url":"http://arxiv.org/abs/2404.09312v1","category":"cond-mat.other"}
{"created":"2024-04-14 17:37:30","title":"Julia as a universal platform for statistical software development","abstract":"Like Python and Java, which are integrated into Stata, Julia is a free programming language that runs on all major operating systems. The julia package links Stata to Julia as well. Users can transfer data between Stata and Julia at high speed, issue Julia commands from Stata to analyze and plot, and pass results back to Stata. Julia's econometric software ecosystem is not as mature as Stata's or R's, or even Python's. But Julia is an excellent environment for developing high-performance numerical applications, which can then be called from many platforms. The boottest program for wild bootstrap-based inference (Roodman et al. 2019) can call a Julia back end for a 33-50% speed-up, even as the R package fwildclusterboot (Fischer and Roodman 2021) uses the same back end for inference after instrumental variables estimation. reghdfejl mimics reghdfe (Correia 2016) in fitting linear models with high-dimensional fixed effects but calls an independently developed Julia package for tenfold acceleration on hard problems. reghdfejl also supports nonlinear models--preliminarily, as the Julia package for that purpose matures.","sentences":["Like Python and Java, which are integrated into Stata, Julia is a free programming language that runs on all major operating systems.","The julia package links Stata to Julia as well.","Users can transfer data between Stata and Julia at high speed, issue Julia commands from Stata to analyze and plot, and pass results back to Stata.","Julia's econometric software ecosystem is not as mature as Stata's or R's, or even Python's.","But Julia is an excellent environment for developing high-performance numerical applications, which can then be called from many platforms.","The boottest program for wild bootstrap-based inference (Roodman et al. 2019) can call a Julia back end for a 33-50% speed-up, even as the R package fwildclusterboot (Fischer and Roodman 2021) uses the same back end for inference after instrumental variables estimation.","reghdfejl mimics reghdfe (Correia 2016) in fitting linear models with high-dimensional fixed effects but calls an independently developed Julia package for tenfold acceleration on hard problems.","reghdfejl also supports nonlinear models--preliminarily, as the Julia package for that purpose matures."],"url":"http://arxiv.org/abs/2404.09309v1","category":"econ.EM"}
{"created":"2024-04-14 17:04:15","title":"Sinestesia as a model for HCI: a Systematic Review","abstract":"Synesthesia, conceived as a neuropsychological condition, may prove valuable in studying the interaction between humans and machines by analyzing the co-occurrence of sensory or cognitive responses triggered by a stimulus. In our approach, synesthesia is elevated beyond a mere perceptual-cognitive anomaly, offering insights into the reciprocal interaction between humans and the digital system, steering novel experimental design and enriching results interpretations.This review broadens the traditional scope, conventionally rooted in neuroscience and psychology, by considering how computer science can approach this condition. The interdisciplinary examination revolves around two primary viewpoints: one associating this condition with specific cognitive, perceptual, and behavioral anomalies, and the other acknowledging it as a prevalent human experience. Synesthesia, in this review, emerges as a significant model for Human Computer Interaction (HCI). The exploration of this specific condition aims to decipher how atypical pathways of perception and cognition can be encoded, empowering machines to actively engage in processing information from both the body and the environment. The authors attempt to amalgamate findings and insights from various disciplines, fostering collaboration between computer science, neuroscience, psychology, and philosophy.The overarching objective is to construct a comprehensive framework that elucidates how synesthesia and anomalies in information processing can be harnessed within HCI, with a particular emphasis on contributing to digital technologies for medical research and enhancing patients care and comfort. In this sense, the review endeavors also to fill the gap between theoretical understanding and practical application.","sentences":["Synesthesia, conceived as a neuropsychological condition, may prove valuable in studying the interaction between humans and machines by analyzing the co-occurrence of sensory or cognitive responses triggered by a stimulus.","In our approach, synesthesia is elevated beyond a mere perceptual-cognitive anomaly, offering insights into the reciprocal interaction between humans and the digital system, steering novel experimental design and enriching results interpretations.","This review broadens the traditional scope, conventionally rooted in neuroscience and psychology, by considering how computer science can approach this condition.","The interdisciplinary examination revolves around two primary viewpoints: one associating this condition with specific cognitive, perceptual, and behavioral anomalies, and the other acknowledging it as a prevalent human experience.","Synesthesia, in this review, emerges as a significant model for Human Computer Interaction (HCI).","The exploration of this specific condition aims to decipher how atypical pathways of perception and cognition can be encoded, empowering machines to actively engage in processing information from both the body and the environment.","The authors attempt to amalgamate findings and insights from various disciplines, fostering collaboration between computer science, neuroscience, psychology, and philosophy.","The overarching objective is to construct a comprehensive framework that elucidates how synesthesia and anomalies in information processing can be harnessed within HCI, with a particular emphasis on contributing to digital technologies for medical research and enhancing patients care and comfort.","In this sense, the review endeavors also to fill the gap between theoretical understanding and practical application."],"url":"http://arxiv.org/abs/2404.09303v1","category":"cs.HC"}
{"created":"2024-04-14 15:50:10","title":"RoofDiffusion: Constructing Roofs from Severely Corrupted Point Data via Diffusion","abstract":"Accurate completion and denoising of roof height maps are crucial to reconstructing high-quality 3D buildings. Repairing sparse points can enhance low-cost sensor use and reduce UAV flight overlap. RoofDiffusion is a new end-to-end self-supervised diffusion technique for robustly completing, in particular difficult, roof height maps. RoofDiffusion leverages widely-available curated footprints and can so handle up to 99\\% point sparsity and 80\\% roof area occlusion (regional incompleteness). A variant, No-FP RoofDiffusion, simultaneously predicts building footprints and heights. Both quantitatively outperform state-of-the-art unguided depth completion and representative inpainting methods for Digital Elevation Models (DEM), on both a roof-specific benchmark and the BuildingNet dataset. Qualitative assessments show the effectiveness of RoofDiffusion for datasets with real-world scans including AHN3, Dales3D, and USGS 3DEP LiDAR. Tested with the leading City3D algorithm, preprocessing height maps with RoofDiffusion noticeably improves 3D building reconstruction. RoofDiffusion is complemented by a new dataset of 13k complex roof geometries, focusing on long-tail issues in remote sensing; a novel simulation of tree occlusion; and a wide variety of large-area roof cut-outs for data augmentation and benchmarking.","sentences":["Accurate completion and denoising of roof height maps are crucial to reconstructing high-quality 3D buildings.","Repairing sparse points can enhance low-cost sensor use and reduce UAV flight overlap.","RoofDiffusion is a new end-to-end self-supervised diffusion technique for robustly completing, in particular difficult, roof height maps.","RoofDiffusion leverages widely-available curated footprints and can so handle up to 99\\% point sparsity and 80\\% roof area occlusion (regional incompleteness).","A variant, No-FP RoofDiffusion, simultaneously predicts building footprints and heights.","Both quantitatively outperform state-of-the-art unguided depth completion and representative inpainting methods for Digital Elevation Models (DEM), on both a roof-specific benchmark and the BuildingNet dataset.","Qualitative assessments show the effectiveness of RoofDiffusion for datasets with real-world scans including AHN3, Dales3D, and USGS 3DEP LiDAR.","Tested with the leading City3D algorithm, preprocessing height maps with RoofDiffusion noticeably improves 3D building reconstruction.","RoofDiffusion is complemented by a new dataset of 13k complex roof geometries, focusing on long-tail issues in remote sensing; a novel simulation of tree occlusion; and a wide variety of large-area roof cut-outs for data augmentation and benchmarking."],"url":"http://arxiv.org/abs/2404.09290v1","category":"cs.CV"}
{"created":"2024-04-14 15:33:11","title":"Deriving a GENERIC system from a Hamiltonian system","abstract":"We reconsider the fundamental problem of coarse-graining infinite-dimensional Hamiltonian dynamics to obtain a macroscopic system which includes dissipative mechanisms. In particular, we study the thermodynamical implications concerning Hamiltonians, energy, and entropy and the induced geometric structures such as Poisson and Onsager brackets (symplectic and dissipative brackets).   We start from a general finite-dimensional Hamiltonian system that is coupled linearly to an infinite-dimensional heat bath with linear dynamics. The latter is assumed to admit a compression to a finite-dimensional dissipative semigroup (i.e., the heat bath is a dilation of the semigroup) describing the dissipative evolution of new macroscopic variables.   Already in the finite-energy case (zero-temperature heat bath) we obtain the so-called GENERIC structure (General Equations for Non-Equilibrium Reversible Irreversibe Coupling), with conserved energy, nondecreasing entropy, a new Poisson structure, and an Onsager operator describing the dissipation. However, their origin is not obvious at this stage. After extending the system in a natural way to the case of positive temperature, giving a heat bath with infinite energy, the compression property leads to an exact multivariate Ornstein-Uhlenbeck process that drives the rest of the system. Thus, we are able to identify a conserved energy, an entropy, and an Onsager operator (involving the Green-Kubo formalism) which indeed provide a GENERIC structure for the macroscopic system.","sentences":["We reconsider the fundamental problem of coarse-graining infinite-dimensional Hamiltonian dynamics to obtain a macroscopic system which includes dissipative mechanisms.","In particular, we study the thermodynamical implications concerning Hamiltonians, energy, and entropy and the induced geometric structures such as Poisson and Onsager brackets (symplectic and dissipative brackets).   ","We start from a general finite-dimensional Hamiltonian system that is coupled linearly to an infinite-dimensional heat bath with linear dynamics.","The latter is assumed to admit a compression to a finite-dimensional dissipative semigroup (i.e., the heat bath is a dilation of the semigroup) describing the dissipative evolution of new macroscopic variables.   ","Already in the finite-energy case (zero-temperature heat bath) we obtain the so-called GENERIC structure (General Equations for Non-Equilibrium Reversible Irreversibe Coupling), with conserved energy, nondecreasing entropy, a new Poisson structure, and an Onsager operator describing the dissipation.","However, their origin is not obvious at this stage.","After extending the system in a natural way to the case of positive temperature, giving a heat bath with infinite energy, the compression property leads to an exact multivariate Ornstein-Uhlenbeck process that drives the rest of the system.","Thus, we are able to identify a conserved energy, an entropy, and an Onsager operator (involving the Green-Kubo formalism) which indeed provide a GENERIC structure for the macroscopic system."],"url":"http://arxiv.org/abs/2404.09284v1","category":"math-ph"}
{"created":"2024-04-14 15:23:46","title":"A Model Predictive Control Scheme for Flight Scheduling and Energy Management of Electric Aviation Networks","abstract":"This paper presents a Model Predictive Control (MPC) scheme for flight scheduling and energy management of electric aviation networks, where electric aircraft transport passengers between electrified airports equipped with sustainable energy sources and battery storage, with the goal of minimizing grid dependency. Specifically, we first model the aircraft flight and charge scheduling problem jointly with the airport energy management problem, explicitly accounting for local weather forecasts. Second, we frame the minimum-grid-energy operational problem as a mixed-integer linear program and solve it in a receding horizon fashion, where the route assignment and charging decisions of each aircraft can be dynamically reassigned to mitigate disruptions. We showcase the proposed MPC scheme on real-world data taken from conventional flights and weather conditions in the Dutch Leeward Antilles. Our results show that MPC can effectively guarantee operation of the network by efficiently re-assigning flights and rescheduling aircraft charging, whilst maximizing the efficiency of the on-site energy systems.","sentences":["This paper presents a Model Predictive Control (MPC) scheme for flight scheduling and energy management of electric aviation networks, where electric aircraft transport passengers between electrified airports equipped with sustainable energy sources and battery storage, with the goal of minimizing grid dependency.","Specifically, we first model the aircraft flight and charge scheduling problem jointly with the airport energy management problem, explicitly accounting for local weather forecasts.","Second, we frame the minimum-grid-energy operational problem as a mixed-integer linear program and solve it in a receding horizon fashion, where the route assignment and charging decisions of each aircraft can be dynamically reassigned to mitigate disruptions.","We showcase the proposed MPC scheme on real-world data taken from conventional flights and weather conditions in the Dutch Leeward Antilles.","Our results show that MPC can effectively guarantee operation of the network by efficiently re-assigning flights and rescheduling aircraft charging, whilst maximizing the efficiency of the on-site energy systems."],"url":"http://arxiv.org/abs/2404.09282v1","category":"eess.SY"}
{"created":"2024-04-14 15:07:57","title":"Improved Optimization for the Neural-network Quantum States and Tests on the Chromium Dimer","abstract":"The advent of Neural-network Quantum States (NQS) has significantly advanced wave function ansatz research, sparking a resurgence in orbital space variational Monte Carlo (VMC) exploration. This work introduces three algorithmic enhancements to reduce computational demands of VMC optimization using NQS: an adaptive learning rate algorithm, constrained optimization, and block optimization. We evaluate the refined algorithm on complex multireference bond stretches of $\\rm H_2O$ and $\\rm N_2$ within the cc-pVDZ basis set and calculate the ground-state energy of the strongly correlated chromium dimer ($\\rm Cr_2$) in the Ahlrichs SV basis set. Our results achieve superior accuracy compared to coupled cluster theory at a relatively modest CPU cost. This work demonstrates how to enhance optimization efficiency and robustness using these strategies, opening a new path to optimize large-scale Restricted Boltzmann Machine (RBM)-based NQS more effectively and marking a substantial advancement in NQS's practical quantum chemistry applications.","sentences":["The advent of Neural-network Quantum States (NQS) has significantly advanced wave function ansatz research, sparking a resurgence in orbital space variational Monte Carlo (VMC) exploration.","This work introduces three algorithmic enhancements to reduce computational demands of VMC optimization using NQS: an adaptive learning rate algorithm, constrained optimization, and block optimization.","We evaluate the refined algorithm on complex multireference bond stretches of $\\rm H_2O$ and $\\rm N_2$ within the cc-pVDZ basis set and calculate the ground-state energy of the strongly correlated chromium dimer ($\\rm Cr_2$) in the Ahlrichs SV basis set.","Our results achieve superior accuracy compared to coupled cluster theory at a relatively modest CPU cost.","This work demonstrates how to enhance optimization efficiency and robustness using these strategies, opening a new path to optimize large-scale Restricted Boltzmann Machine (RBM)-based NQS more effectively and marking a substantial advancement in NQS's practical quantum chemistry applications."],"url":"http://arxiv.org/abs/2404.09280v1","category":"physics.chem-ph"}
{"created":"2024-04-14 15:02:28","title":"Optimal Control of a Markovian Qubit with Unitary Control","abstract":"We study a single Markovian qubit governed by a Lindblad master equation and subject to fast unitary control. Using reduced control systems and optimal control theory we determine (i) controls for cooling and heating such systems in a time-optimal way as well as (ii) the set of stabilizable states in the Bloch ball. No restrictions on the Lindblad equation are assumed, and several known results, for instance for the Bloch equations, are recovered. Furthermore we introduce integral systems, for which the solutions take a particularly nice form. These integral systems include all systems with real Lindblad terms as well as all coolable systems. The method allows for intuitive visualizations and is mostly analytical, making use of only basic numerical methods.","sentences":["We study a single Markovian qubit governed by a Lindblad master equation and subject to fast unitary control.","Using reduced control systems and optimal control theory we determine (i) controls for cooling and heating such systems in a time-optimal way as well as (ii) the set of stabilizable states in the Bloch ball.","No restrictions on the Lindblad equation are assumed, and several known results, for instance for the Bloch equations, are recovered.","Furthermore we introduce integral systems, for which the solutions take a particularly nice form.","These integral systems include all systems with real Lindblad terms as well as all coolable systems.","The method allows for intuitive visualizations and is mostly analytical, making use of only basic numerical methods."],"url":"http://arxiv.org/abs/2404.09279v1","category":"quant-ph"}
{"created":"2024-04-14 14:28:47","title":"STAR-FDTD : Space-time modulated acousto-optic guidestar in disordered media","abstract":"We developed a 2D Finite-Difference Time-Domain (FDTD) method for modeling a space-time modulated guidestar targeting wavefront shaping applications in disordered media. Space-time modulation in general (a particular example being the acousto-optic effect) is used here as a guidestar for the transverse confinement of light around the tagged region surrounded by disorder. Together with the guidestar, the iterative optical phase conjugation (IOPC) method is used to overcome the diffusion of light due to multiple scattering. A phase sensitive lock-in detection technique is utilized to estimate the steady-state amplitude and phase of the modulated wavefronts emerging from the guidestar region continuously operating in the Raman-Nath regime. As the IOPC scheme naturally converges to the maximally transmitting eigenchannel profile, one could use the position of the guidestar within the disorder to channelize the maximal transmission through the tagged region. The associated code developed in MATLAB is provided as an open source (MIT license) package. The code package is referred by the acronym STAR-FDTD where STAR stands for Space-Time modulated Acousto-optic guidestaR.","sentences":["We developed a 2D Finite-Difference Time-Domain (FDTD) method for modeling a space-time modulated guidestar targeting wavefront shaping applications in disordered media.","Space-time modulation in general (a particular example being the acousto-optic effect) is used here as a guidestar for the transverse confinement of light around the tagged region surrounded by disorder.","Together with the guidestar, the iterative optical phase conjugation (IOPC) method is used to overcome the diffusion of light due to multiple scattering.","A phase sensitive lock-in detection technique is utilized to estimate the steady-state amplitude and phase of the modulated wavefronts emerging from the guidestar region continuously operating in the Raman-Nath regime.","As the IOPC scheme naturally converges to the maximally transmitting eigenchannel profile, one could use the position of the guidestar within the disorder to channelize the maximal transmission through the tagged region.","The associated code developed in MATLAB is provided as an open source (MIT license) package.","The code package is referred by the acronym STAR-FDTD where STAR stands for Space-Time modulated Acousto-optic guidestaR."],"url":"http://arxiv.org/abs/2404.09273v1","category":"physics.optics"}
{"created":"2024-04-14 14:18:55","title":"Tangram: High-resolution Video Analytics on Serverless Platform with SLO-aware Batching","abstract":"Cloud-edge collaborative computing paradigm is a promising solution to high-resolution video analytics systems. The key lies in reducing redundant data and managing fluctuating inference workloads effectively. Previous work has focused on extracting regions of interest (RoIs) from videos and transmitting them to the cloud for processing. However, a naive Infrastructure as a Service (IaaS) resource configuration falls short in handling highly fluctuating workloads, leading to violations of Service Level Objectives (SLOs) and inefficient resource utilization. Besides, these methods neglect the potential benefits of RoIs batching to leverage parallel processing. In this work, we introduce Tangram, an efficient serverless cloud-edge video analytics system fully optimized for both communication and computation. Tangram adaptively aligns the RoIs into patches and transmits them to the scheduler in the cloud. The system employs a unique ``stitching'' method to batch the patches with various sizes from the edge cameras. Additionally, we develop an online SLO-aware batching algorithm that judiciously determines the optimal invoking time of the serverless function. Experiments on our prototype reveal that Tangram can reduce bandwidth consumption and computation cost up to 74.30\\% and 66.35\\%, respectively, while maintaining SLO violations within 5\\% and the accuracy loss negligible.","sentences":["Cloud-edge collaborative computing paradigm is a promising solution to high-resolution video analytics systems.","The key lies in reducing redundant data and managing fluctuating inference workloads effectively.","Previous work has focused on extracting regions of interest (RoIs) from videos and transmitting them to the cloud for processing.","However, a naive Infrastructure as a Service (IaaS) resource configuration falls short in handling highly fluctuating workloads, leading to violations of Service Level Objectives (SLOs) and inefficient resource utilization.","Besides, these methods neglect the potential benefits of RoIs batching to leverage parallel processing.","In this work, we introduce Tangram, an efficient serverless cloud-edge video analytics system fully optimized for both communication and computation.","Tangram adaptively aligns the RoIs into patches and transmits them to the scheduler in the cloud.","The system employs a unique ``stitching'' method to batch the patches with various sizes from the edge cameras.","Additionally, we develop an online SLO-aware batching algorithm that judiciously determines the optimal invoking time of the serverless function.","Experiments on our prototype reveal that Tangram can reduce bandwidth consumption and computation cost up to 74.30\\% and 66.35\\%, respectively, while maintaining SLO violations within 5\\% and the accuracy loss negligible."],"url":"http://arxiv.org/abs/2404.09267v1","category":"cs.DC"}
{"created":"2024-04-14 14:07:58","title":"A Reinforcement Learning Based Backfilling Strategy for HPC Batch Jobs","abstract":"High Performance Computing (HPC) systems are used across a wide range of disciplines for both large and complex computations. HPC systems often receive many thousands of computational tasks at a time, colloquially referred to as jobs. These jobs must then be scheduled as optimally as possible so they can be completed within a reasonable timeframe. HPC scheduling systems often employ a technique called backfilling, wherein low-priority jobs are scheduled earlier to use the available resources that are waiting for the pending high-priority jobs. To make it work, backfilling largely relies on job runtime to calculate the start time of the ready-to-schedule jobs and avoid delaying them. It is a common belief that better estimations of job runtime will lead to better backfilling and more effective scheduling. However, our experiments show a different conclusion: there is a missing trade-off between prediction accuracy and backfilling opportunities. To learn how to achieve the best trade-off, we believe reinforcement learning (RL) can be effectively leveraged. Reinforcement Learning relies on an agent which makes decisions from observing the environment, and gains rewards or punishments based on the quality of its decision-making. Based on this idea, we designed RLBackfilling, a reinforcement learning-based backfilling algorithm. We show how RLBackfilling can learn effective backfilling strategies via trial-and-error on existing job traces. Our evaluation results show up to 59% better scheduling performance (based on average bounded job slowdown) compared to EASY backfilling using user-provided job runtime and 30% better performance compared with EASY using the ideal predicted job runtime (the actual job runtime).","sentences":["High Performance Computing (HPC) systems are used across a wide range of disciplines for both large and complex computations.","HPC systems often receive many thousands of computational tasks at a time, colloquially referred to as jobs.","These jobs must then be scheduled as optimally as possible so they can be completed within a reasonable timeframe.","HPC scheduling systems often employ a technique called backfilling, wherein low-priority jobs are scheduled earlier to use the available resources that are waiting for the pending high-priority jobs.","To make it work, backfilling largely relies on job runtime to calculate the start time of the ready-to-schedule jobs and avoid delaying them.","It is a common belief that better estimations of job runtime will lead to better backfilling and more effective scheduling.","However, our experiments show a different conclusion: there is a missing trade-off between prediction accuracy and backfilling opportunities.","To learn how to achieve the best trade-off, we believe reinforcement learning (RL) can be effectively leveraged.","Reinforcement Learning relies on an agent which makes decisions from observing the environment, and gains rewards or punishments based on the quality of its decision-making.","Based on this idea, we designed RLBackfilling, a reinforcement learning-based backfilling algorithm.","We show how RLBackfilling can learn effective backfilling strategies via trial-and-error on existing job traces.","Our evaluation results show up to 59% better scheduling performance (based on average bounded job slowdown) compared to EASY backfilling using user-provided job runtime and 30% better performance compared with EASY using the ideal predicted job runtime (the actual job runtime)."],"url":"http://arxiv.org/abs/2404.09264v1","category":"cs.DC"}
{"created":"2024-04-14 14:04:09","title":"Quantized polarization in a generalized Rice-Mele model at arbitrary filling","abstract":"We discuss the charge polarization in a generalized Rice-Mele model at arbitrary particle filling per site as a model of charge ordered systems in one dimension. The model possesses neither the conventional bond-centered inversion symmetry nor the one site translation symmetry alone, but has combinations of these symmetries. We show that the charge polarization in the ground state is quantized by the combined symmetry and is characterized solely by the filling. Especially, the polarization can be $1/2$ (mod 1) in the zero filling limit. Under the open boundary condition, there exist excess charges accumulated near edges of the system irrespective of existence or absence of edge modes. Correspondingly, we decompose the polarization into a bulk contribution and an edge contribution, and numerically demonstrate that the polarization is dominated by the former (latter) when the energy gap is large (small). We also discuss a simple generalization of our model and examine absence/existence of a gapless edge mode protected by the inversion symmetry by introducing intra unit cell and inter unit cell contributions of the charge polarization.","sentences":["We discuss the charge polarization in a generalized Rice-Mele model at arbitrary particle filling per site as a model of charge ordered systems in one dimension.","The model possesses neither the conventional bond-centered inversion symmetry nor the one site translation symmetry alone, but has combinations of these symmetries.","We show that the charge polarization in the ground state is quantized by the combined symmetry and is characterized solely by the filling.","Especially, the polarization can be $1/2$ (mod 1) in the zero filling limit.","Under the open boundary condition, there exist excess charges accumulated near edges of the system irrespective of existence or absence of edge modes.","Correspondingly, we decompose the polarization into a bulk contribution and an edge contribution, and numerically demonstrate that the polarization is dominated by the former (latter) when the energy gap is large (small).","We also discuss a simple generalization of our model and examine absence/existence of a gapless edge mode protected by the inversion symmetry by introducing intra unit cell and inter unit cell contributions of the charge polarization."],"url":"http://arxiv.org/abs/2404.09262v1","category":"cond-mat.str-el"}
{"created":"2024-04-14 13:49:56","title":"On the dynamical evolution of the asteroid belt in a massive star-neutron star binary","abstract":"Some fast radio bursts (FRBs) exhibit repetitive behaviors and their origins remain enigmatic. It has been argued that repeating FRBs could be produced by the interaction between a neutron star and an asteroid belt. Here we consider the systems in which an asteroid belt dwells around a massive star, while a neutron star, as a companion of the massive star, interacts with the belt through gravitational force. Various orbital configurations are assumed for the system. Direct N-body simulations are performed to investigate the dynamical evolution of the asteroids belt. It is found that a larger orbital eccentricity of the neutron star will destroy the belt more quickly, with a large number of asteroids being scattered out of the system. A non-zero mutual inclination angle suppresses the ejection rate of asteroids, but it also leads to a reduction in the collision rate of asteroids with the neutron star since many asteroids are essentially scattered into the 3D space. Among the various configurations, a clear periodicity is observed in the collision events for the case with an orbital eccentricity of 0.7 and mutual inclination of $0^{\\circ}$. It is found that such a periodicity can be sustained for at least 8 neutron star orbital periods, supporting this mechanism as a possible explanation for periodically repeating FRBs. Our studies also suggest that the active stage of these kinds of FRB sources should be limited, since the asteroid belt would finally be destroyed by the neutron star after multiple passages.","sentences":["Some fast radio bursts (FRBs) exhibit repetitive behaviors and their origins remain enigmatic.","It has been argued that repeating FRBs could be produced by the interaction between a neutron star and an asteroid belt.","Here we consider the systems in which an asteroid belt dwells around a massive star, while a neutron star, as a companion of the massive star, interacts with the belt through gravitational force.","Various orbital configurations are assumed for the system.","Direct N-body simulations are performed to investigate the dynamical evolution of the asteroids belt.","It is found that a larger orbital eccentricity of the neutron star will destroy the belt more quickly, with a large number of asteroids being scattered out of the system.","A non-zero mutual inclination angle suppresses the ejection rate of asteroids, but it also leads to a reduction in the collision rate of asteroids with the neutron star since many asteroids are essentially scattered into the 3D space.","Among the various configurations, a clear periodicity is observed in the collision events for the case with an orbital eccentricity of 0.7 and mutual inclination of $0^{\\circ}$. It is found that such a periodicity can be sustained for at least 8 neutron star orbital periods, supporting this mechanism as a possible explanation for periodically repeating FRBs.","Our studies also suggest that the active stage of these kinds of FRB sources should be limited, since the asteroid belt would finally be destroyed by the neutron star after multiple passages."],"url":"http://arxiv.org/abs/2404.09258v1","category":"astro-ph.HE"}
{"created":"2024-04-14 13:49:42","title":"Mixing is easy: new insights for cosmochemical evolution from pre-stellar core collapse","abstract":"Signposts of early planet formation are ubiquitous in sub-structured young discs. Dense, hot and high-pressure regions formed during gravitational collapse process, integral to star formation, facilitate dynamical mixing of dust within the protostellar disc. This provides an incentive to constrain the role of gas-dust interaction and resolve zones of dust concentration during star-disc formation. We explore if thermal and dynamical conditions developed during disc formation can generate gas flows that efficiently mix and transport well-coupled gas and dust components. We simulate the collapse of dusty molecular cloud cores with the hydrodynamics code PLUTO augmented with radiation transport and self-gravity. We use a 2D axisymmetric geometry and follow the azimuthal component of velocity. Dust is treated as Lagrangian particles that are subject to drag from the gas, whose motion is computed on a Eulerian grid. We consider 1, 10 and 100 micron-sized neutral spherical dust. Importantly, the equation of state accurately includes molecular hydrogen dissociation. We focus on molecular cloud core masses of 1 and 3 Msun and explore effects of initial rotation rates and cloud core sizes. Our study underlines mechanisms for early transport of dust from inner hot disc regions via the occurrence of meridional flows and outflow. The vortical flow fosters dynamical mixing and retention of dust, while thermal pressure driven outflow replenishes dust in the outer disc. Young dynamical precursors to planet-forming discs exhibit regions with complex hydrodynamical gas features and high-temperature structures. These can play a crucial role in concentrating dust for subsequent growth into protoplanets. Dust transport, especially, from sub-au scales surrounding the protostar to outer relatively cooler parts, offers an efficient pathway for thermal reprocessing during pre-stellar core collapse. [Abridged]","sentences":["Signposts of early planet formation are ubiquitous in sub-structured young discs.","Dense, hot and high-pressure regions formed during gravitational collapse process, integral to star formation, facilitate dynamical mixing of dust within the protostellar disc.","This provides an incentive to constrain the role of gas-dust interaction and resolve zones of dust concentration during star-disc formation.","We explore if thermal and dynamical conditions developed during disc formation can generate gas flows that efficiently mix and transport well-coupled gas and dust components.","We simulate the collapse of dusty molecular cloud cores with the hydrodynamics code PLUTO augmented with radiation transport and self-gravity.","We use a 2D axisymmetric geometry and follow the azimuthal component of velocity.","Dust is treated as Lagrangian particles that are subject to drag from the gas, whose motion is computed on a Eulerian grid.","We consider 1, 10 and 100 micron-sized neutral spherical dust.","Importantly, the equation of state accurately includes molecular hydrogen dissociation.","We focus on molecular cloud core masses of 1 and 3 Msun and explore effects of initial rotation rates and cloud core sizes.","Our study underlines mechanisms for early transport of dust from inner hot disc regions via the occurrence of meridional flows and outflow.","The vortical flow fosters dynamical mixing and retention of dust, while thermal pressure driven outflow replenishes dust in the outer disc.","Young dynamical precursors to planet-forming discs exhibit regions with complex hydrodynamical gas features and high-temperature structures.","These can play a crucial role in concentrating dust for subsequent growth into protoplanets.","Dust transport, especially, from sub-au scales surrounding the protostar to outer relatively cooler parts, offers an efficient pathway for thermal reprocessing during pre-stellar core collapse.","[Abridged]"],"url":"http://arxiv.org/abs/2404.09257v1","category":"astro-ph.SR"}
{"created":"2024-04-14 13:42:57","title":"Quiver matroids -- Matroid morphisms, quiver Grassmannians, their Euler characteristics and $\\mathbb{F}_1$-points","abstract":"In this paper, we introduce morphisms for matroids with coefficients (in the sense of Baker and Bowler) and quiver matroids. We investigate their basic properties, such as functoriality, duality, minors and cryptomorphic characterizations in terms of vectors, circuits and bases (a.k.a. Grassmann-Pl\\\"ucker functions). We generalize quiver matroids to quiver matroid bundles and construct their moduli space, which is an $\\mathbb{F}_1$-analogue of a complex quiver Grassmannian. Eventually we introduce a suitable interpretation of $\\mathbb{F}_1$-points for these moduli spaces, so that in 'nice' cases their number is equal to the Euler characteristic of the associated complex quiver Grassmannian.","sentences":["In this paper, we introduce morphisms for matroids with coefficients (in the sense of Baker and Bowler) and quiver matroids.","We investigate their basic properties, such as functoriality, duality, minors and cryptomorphic characterizations in terms of vectors, circuits and bases (a.k.a. Grassmann-Pl\\\"ucker functions).","We generalize quiver matroids to quiver matroid bundles and construct their moduli space, which is an $\\mathbb{F}_1$-analogue of a complex quiver Grassmannian.","Eventually we introduce a suitable interpretation of $\\mathbb{F}_1$-points for these moduli spaces, so that in 'nice' cases their number is equal to the Euler characteristic of the associated complex quiver Grassmannian."],"url":"http://arxiv.org/abs/2404.09255v1","category":"math.CO"}
{"created":"2024-04-14 13:29:06","title":"Multipolar Electromagnetic Emission of Newborn Magnetar","abstract":"It is generally recognized that the electromagnetic multipolar emission from magnetars can be used to explain radiation from Soft Gamma Repeaters (SGRs) or Anomalous X-ray Pulsars (AXPs), but they have little impact on the spindown of magnetars. We here present a comprehensive analytical solution for the neutron star multipolar electromagnetic fields and their associated expected luminosities. We find that for newborn millisecond magnetars, the spin-down luminosity from higher multipolar components can match or even exceed that from the dipole component. Such high-intensity radiation will undoubtedly affect related astrophysical phenomena at the birth of a magnetar. We show that the spin-down luminosity from multipoles can well explain the majority of Gamma-Ray Bursts (GRBs) afterglows, from the plateau starting at several hundred seconds till the normal decay phase lasting for many years. The fitted magnetar parameters for GRB afterglows are all typical values, with spins in the millisecond range and magnetic field strengths in the order of $10^{14} - 10^{15}$ Gauss. Our results in turn, provide support for the hypothesis that GRBs originate from the birth of magnetars with a few millisecond period, thus deepening our understanding of the complex magnetic field structure and the equation of state of magnetars.","sentences":["It is generally recognized that the electromagnetic multipolar emission from magnetars can be used to explain radiation from Soft Gamma Repeaters (SGRs) or Anomalous X-ray Pulsars (AXPs), but they have little impact on the spindown of magnetars.","We here present a comprehensive analytical solution for the neutron star multipolar electromagnetic fields and their associated expected luminosities.","We find that for newborn millisecond magnetars, the spin-down luminosity from higher multipolar components can match or even exceed that from the dipole component.","Such high-intensity radiation will undoubtedly affect related astrophysical phenomena at the birth of a magnetar.","We show that the spin-down luminosity from multipoles can well explain the majority of Gamma-Ray Bursts (GRBs) afterglows, from the plateau starting at several hundred seconds till the normal decay phase lasting for many years.","The fitted magnetar parameters for GRB afterglows are all typical values, with spins in the millisecond range and magnetic field strengths in the order of $10^{14} - 10^{15}$ Gauss.","Our results in turn, provide support for the hypothesis that GRBs originate from the birth of magnetars with a few millisecond period, thus deepening our understanding of the complex magnetic field structure and the equation of state of magnetars."],"url":"http://arxiv.org/abs/2404.09251v1","category":"astro-ph.HE"}
{"created":"2024-04-14 13:10:44","title":"A Joint Data Compression and Time-Delay Estimation Method For Distributed Systems via Extremum Encoding","abstract":"Motivated by the proliferation of mobile devices, we consider a basic form of the ubiquitous problem of time-delay estimation (TDE), but with communication constraints between two non co-located sensors. In this setting, when joint processing of the received signals is not possible, a compression technique that is tailored to TDE is desirable. For our basic TDE formulation, we develop such a joint compression-estimation strategy based on the notion of what we term \"extremum encoding\", whereby we send the index of the maximum of a finite-length time-series from one sensor to another. Subsequent joint processing of the encoded message with locally observed data gives rise to our proposed time-delay \"maximum-index\"-based estimator. We derive an exponentially tight upper bound on its error probability, establishing its consistency with respect to the number of transmitted bits. We further validate our analysis via simulations, and comment on potential extensions and generalizations of the basic methodology.","sentences":["Motivated by the proliferation of mobile devices, we consider a basic form of the ubiquitous problem of time-delay estimation (TDE), but with communication constraints between two non co-located sensors.","In this setting, when joint processing of the received signals is not possible, a compression technique that is tailored to TDE is desirable.","For our basic TDE formulation, we develop such a joint compression-estimation strategy based on the notion of what we term \"extremum encoding\", whereby we send the index of the maximum of a finite-length time-series from one sensor to another.","Subsequent joint processing of the encoded message with locally observed data gives rise to our proposed time-delay \"maximum-index\"-based estimator.","We derive an exponentially tight upper bound on its error probability, establishing its consistency with respect to the number of transmitted bits.","We further validate our analysis via simulations, and comment on potential extensions and generalizations of the basic methodology."],"url":"http://arxiv.org/abs/2404.09244v1","category":"eess.SP"}
{"created":"2024-04-14 13:08:21","title":"LSROM: Learning Self-Refined Organizing Map for Fast Imbalanced Streaming Data Clustering","abstract":"Streaming data clustering is a popular research topic in the fields of data mining and machine learning. Compared to static data, streaming data, which is usually analyzed in data chunks, is more susceptible to encountering the dynamic cluster imbalanced issue. That is, the imbalanced degree of clusters varies in different streaming data chunks, leading to corruption in either the accuracy or the efficiency of streaming data analysis based on existing clustering methods. Therefore, we propose an efficient approach called Learning Self-Refined Organizing Map (LSROM) to handle the imbalanced streaming data clustering problem, where we propose an advanced SOM for representing the global data distribution. The constructed SOM is first refined for guiding the partition of the dataset to form many micro-clusters to avoid the missing small clusters in imbalanced data. Then an efficient merging of the micro-clusters is conducted through quick retrieval based on the SOM, which can automatically yield a true number of imbalanced clusters. In comparison to existing imbalanced data clustering approaches, LSROM is with a lower time complexity $O(n\\log n)$, while achieving very competitive clustering accuracy. Moreover, LSROM is interpretable and insensitive to hyper-parameters. Extensive experiments have verified its efficacy.","sentences":["Streaming data clustering is a popular research topic in the fields of data mining and machine learning.","Compared to static data, streaming data, which is usually analyzed in data chunks, is more susceptible to encountering the dynamic cluster imbalanced issue.","That is, the imbalanced degree of clusters varies in different streaming data chunks, leading to corruption in either the accuracy or the efficiency of streaming data analysis based on existing clustering methods.","Therefore, we propose an efficient approach called Learning Self-Refined Organizing Map (LSROM) to handle the imbalanced streaming data clustering problem, where we propose an advanced SOM for representing the global data distribution.","The constructed SOM is first refined for guiding the partition of the dataset to form many micro-clusters to avoid the missing small clusters in imbalanced data.","Then an efficient merging of the micro-clusters is conducted through quick retrieval based on the SOM, which can automatically yield a true number of imbalanced clusters.","In comparison to existing imbalanced data clustering approaches, LSROM is with a lower time complexity $O(n\\log n)$, while achieving very competitive clustering accuracy.","Moreover, LSROM is interpretable and insensitive to hyper-parameters.","Extensive experiments have verified its efficacy."],"url":"http://arxiv.org/abs/2404.09243v1","category":"cs.LG"}
{"created":"2024-04-14 12:53:34","title":"Early warning signals of the tipping point in strongly interacting Rydberg atoms","abstract":"The identification of tipping points is essential for prediction of collapses or other sudden changes in complex systems. Applications include studies of ecology, thermodynamics, climatology, and epidemiology. However, detecting early signs of proximity to a tipping is made challenging by complexity and non-linearity. Strongly interacting Rydberg atom gases offer a model systems that offer both complexity and non-linearity, including phase transition and critical slowing down. Here, via an external probe we observe prior warning of the proximity of a phase transition of Rydberg thermal gases. This warning signal is manifested as a cessation of the variance growth with increasing probe intensity. We also observed the dynamics of the critical slowing down behavior versus different time scales, driving intensities, and atomic densities, thus providing insights into the study of a Rydberg atom system's critical behavior. Our experiment suggests that the full critical slowing down dynamics of strongly-interacting Rydberg atoms can be probed systematically, thus providing a benchmark with which to identify critical phenomena in quantum many-body systems.","sentences":["The identification of tipping points is essential for prediction of collapses or other sudden changes in complex systems.","Applications include studies of ecology, thermodynamics, climatology, and epidemiology.","However, detecting early signs of proximity to a tipping is made challenging by complexity and non-linearity.","Strongly interacting Rydberg atom gases offer a model systems that offer both complexity and non-linearity, including phase transition and critical slowing down.","Here, via an external probe we observe prior warning of the proximity of a phase transition of Rydberg thermal gases.","This warning signal is manifested as a cessation of the variance growth with increasing probe intensity.","We also observed the dynamics of the critical slowing down behavior versus different time scales, driving intensities, and atomic densities, thus providing insights into the study of a Rydberg atom system's critical behavior.","Our experiment suggests that the full critical slowing down dynamics of strongly-interacting Rydberg atoms can be probed systematically, thus providing a benchmark with which to identify critical phenomena in quantum many-body systems."],"url":"http://arxiv.org/abs/2404.09239v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-14 12:49:54","title":"Eigenvalue Preferential Attachment Networks A Dandelion Structure","abstract":"In this paper we introduce a new type of preferential attachment network, the growth of which is based on the eigenvalue centrality. In this network, the agents attach most probably to the nodes with larger eigenvalue centrality which represents that the agent has stronger connections. A new network is presented, namely a dandelion network, which shares some properties of star-like structure and also a hierarchical network. We show that this network, having hub-and-spoke topology is not generally scale free, and shows essential differences with respect to the Barab{\\'a}si-Albert preferential attachment model. Most importantly, there is a super hub agent in the system (identified by a pronounced peak in the spectrum), and the other agents are classified in terms of the distance to this super-hub. We explore a plenty of statistical centralities like the nodes degree, the betweenness and the eigenvalue centrality, along with various measures of structure like the community and hierarchical structures, and the clustering coefficient. Global measures like the shortest path statistics and the self-similarity are also examined.","sentences":["In this paper we introduce a new type of preferential attachment network, the growth of which is based on the eigenvalue centrality.","In this network, the agents attach most probably to the nodes with larger eigenvalue centrality which represents that the agent has stronger connections.","A new network is presented, namely a dandelion network, which shares some properties of star-like structure and also a hierarchical network.","We show that this network, having hub-and-spoke topology is not generally scale free, and shows essential differences with respect to the Barab{\\'a}si-Albert preferential attachment model.","Most importantly, there is a super hub agent in the system (identified by a pronounced peak in the spectrum), and the other agents are classified in terms of the distance to this super-hub.","We explore a plenty of statistical centralities like the nodes degree, the betweenness and the eigenvalue centrality, along with various measures of structure like the community and hierarchical structures, and the clustering coefficient.","Global measures like the shortest path statistics and the self-similarity are also examined."],"url":"http://arxiv.org/abs/2404.09238v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-14 12:47:47","title":"On the complexity of some cycle convexity parameters","abstract":"The subject of graph convexity is well explored in the literature, the so-called interval convexities above all. In this work, we explore the cycle convexity, whose interval function is $I(S) = S \\cup \\{u \\mid G[S \\cup \\{u\\}]$ has a cycle containing $u\\}$. In this convexity, we prove that the decision problems associated to the parameters rank and convexity number are in \\NP-complete and \\W[1]-hard when parameterized by the solution size. We also prove that to determine whether the percolation time of a graph is at least $k$ is \\NP-complete, but polynomial for cacti or when $k\\leq2$","sentences":["The subject of graph convexity is well explored in the literature, the so-called interval convexities above all.","In this work, we explore the cycle convexity, whose interval function is $I(S) = S \\cup \\{u \\mid G[S \\cup \\{u\\}]$ has a cycle containing $u\\}$. In this convexity, we prove that the decision problems associated to the parameters rank and convexity number are in \\NP-complete and \\W[1]-hard when parameterized by the solution size.","We also prove that to determine whether the percolation time of a graph is at least $k$ is \\NP-complete, but polynomial for cacti or when $k\\leq2$"],"url":"http://arxiv.org/abs/2404.09236v1","category":"cs.CC"}
{"created":"2024-04-14 12:26:15","title":"Distribution and Recovery Phase of Geomagnetic Storms During Solar Cycles 23 and 24","abstract":"Coronal mass ejections (CMEs) and Stream Interaction Regions (SIRs) are the main drivers of intense geomagnetic storms. We study the distribution of geomagnetic storms associated with different drivers during solar cycles 23 and 24 (1996-2019). Although the annual occurrence rate of geomagnetic storms in both cycles tracks the sunspot cycle, the second peak in storm activity lags the second sunspot peak. SIRs contribute significantly to the second peak in storm numbers in both cycles, particularly for moderate to stronger-than-moderate storms. We note semiannual peaks in storm numbers much closer to equinoxes for moderate storms, and slightly shifted from equinoxes for intense and stronger-than-intense storms. We note a significant fraction of multiple-peak storms in both cycles due to isolated ICMEs/SIRs, while single-peak storms from multiple interacting drivers, suggesting a complex relationship between storm steps and their drivers. Our study focuses on investigating the recovery phases of geomagnetic storms and examining their dependencies on various storm parameters. Multiple-peak storms in both cycles have recovery phase duration strongly influenced by slow and fast decay phases with no correlation with the main phase buildup rate and Dst peak. However, the recovery phase in single-peak storms for both cycles depends to some extent on the main phase buildup rate and Dst peak, in addition to slow and fast decay phases. Future research should explore recovery phases of single and multiple-peak storms incorporating in-situ solar wind observations for a deeper understanding of storm evolution and decay processes.","sentences":["Coronal mass ejections (CMEs) and Stream Interaction Regions (SIRs) are the main drivers of intense geomagnetic storms.","We study the distribution of geomagnetic storms associated with different drivers during solar cycles 23 and 24 (1996-2019).","Although the annual occurrence rate of geomagnetic storms in both cycles tracks the sunspot cycle, the second peak in storm activity lags the second sunspot peak.","SIRs contribute significantly to the second peak in storm numbers in both cycles, particularly for moderate to stronger-than-moderate storms.","We note semiannual peaks in storm numbers much closer to equinoxes for moderate storms, and slightly shifted from equinoxes for intense and stronger-than-intense storms.","We note a significant fraction of multiple-peak storms in both cycles due to isolated ICMEs/SIRs, while single-peak storms from multiple interacting drivers, suggesting a complex relationship between storm steps and their drivers.","Our study focuses on investigating the recovery phases of geomagnetic storms and examining their dependencies on various storm parameters.","Multiple-peak storms in both cycles have recovery phase duration strongly influenced by slow and fast decay phases with no correlation with the main phase buildup rate and Dst peak.","However, the recovery phase in single-peak storms for both cycles depends to some extent on the main phase buildup rate and Dst peak, in addition to slow and fast decay phases.","Future research should explore recovery phases of single and multiple-peak storms incorporating in-situ solar wind observations for a deeper understanding of storm evolution and decay processes."],"url":"http://arxiv.org/abs/2404.09234v1","category":"astro-ph.SR"}
{"created":"2024-04-14 12:25:28","title":"Dynamical Behavior of a Stochastic Epidemiological Model: Stationary Distribution and Extinction of a SIRS Model with Stochastic Perturbations","abstract":"This paper deals with a new epidemiological model of SIRS with stochastic perturbations. The primary objective is to establish the existence of a unique non-negative nonlocal solution. Using the basic reproduction number $\\mathscr{R}_0$ derived from the associated deterministic model, we demonstrate the existence of a stationary distribution in the stochastic model. In addition, we study the fluctuation of the unique solution of the deterministic problem around the disease-free equilibrium under certain conditions. In particular, we reveal scenarios where random effects induce disease extinction, contrary to the persistence predicted by the deterministic model. The theoretical insights are complemented by numerical simulations, which provide further validation of our findings.","sentences":["This paper deals with a new epidemiological model of SIRS with stochastic perturbations.","The primary objective is to establish the existence of a unique non-negative nonlocal solution.","Using the basic reproduction number $\\mathscr{R}_0$ derived from the associated deterministic model, we demonstrate the existence of a stationary distribution in the stochastic model.","In addition, we study the fluctuation of the unique solution of the deterministic problem around the disease-free equilibrium under certain conditions.","In particular, we reveal scenarios where random effects induce disease extinction, contrary to the persistence predicted by the deterministic model.","The theoretical insights are complemented by numerical simulations, which provide further validation of our findings."],"url":"http://arxiv.org/abs/2404.09233v1","category":"math.DS"}
{"created":"2024-04-14 12:17:15","title":"Dynamics of spherical telescopic linear driven rotation robots","abstract":"Lunar caves are promising features for long-term and permanent human presence on the moon. However, given their inaccessibility to imaging from survey satellites, the concrete environment within the underground cavities is not well known. Thus, to further the efforts of human presence on the moon, these caves are to be explored by robotic systems. However, a set of environmental factors make this exploration particularly challenging. Among those are the very fine lunar dust that damages exposed sensors and actuators and the unknown composition of the surface and obstacles within the cavity. One robotic system that is particularly fit to meet these challenges is that of a spherical robot, as the exterior shell completely separates the sensors and actuators from the hazardous environment. This work introduces the mathematical description in the form of a dynamic model of a novel locomotion approach for this form factor that adds additional functionality. A set of telescopic linearly extending rods moves the robot using a combination of pushing away from the ground and leveraging the gravitational torque. The approach allows the system to locomote, overcome objects by hoisting its center of gravity on top, and transform into a terrestrial laser scanner by using the rods as a tripod.","sentences":["Lunar caves are promising features for long-term and permanent human presence on the moon.","However, given their inaccessibility to imaging from survey satellites, the concrete environment within the underground cavities is not well known.","Thus, to further the efforts of human presence on the moon, these caves are to be explored by robotic systems.","However, a set of environmental factors make this exploration particularly challenging.","Among those are the very fine lunar dust that damages exposed sensors and actuators and the unknown composition of the surface and obstacles within the cavity.","One robotic system that is particularly fit to meet these challenges is that of a spherical robot, as the exterior shell completely separates the sensors and actuators from the hazardous environment.","This work introduces the mathematical description in the form of a dynamic model of a novel locomotion approach for this form factor that adds additional functionality.","A set of telescopic linearly extending rods moves the robot using a combination of pushing away from the ground and leveraging the gravitational torque.","The approach allows the system to locomote, overcome objects by hoisting its center of gravity on top, and transform into a terrestrial laser scanner by using the rods as a tripod."],"url":"http://arxiv.org/abs/2404.09230v1","category":"cs.RO"}
{"created":"2024-04-14 12:13:07","title":"DreamScape: 3D Scene Creation via Gaussian Splatting joint Correlation Modeling","abstract":"Recent progress in text-to-3D creation has been propelled by integrating the potent prior of Diffusion Models from text-to-image generation into the 3D domain. Nevertheless, generating 3D scenes characterized by multiple instances and intricate arrangements remains challenging. In this study, we present DreamScape, a method for creating highly consistent 3D scenes solely from textual descriptions, leveraging the strong 3D representation capabilities of Gaussian Splatting and the complex arrangement abilities of large language models (LLMs). Our approach involves a 3D Gaussian Guide ($3{DG^2}$) for scene representation, consisting of semantic primitives (objects) and their spatial transformations and relationships derived directly from text prompts using LLMs. This compositional representation allows for local-to-global optimization of the entire scene. A progressive scale control is tailored during local object generation, ensuring that objects of different sizes and densities adapt to the scene, which addresses training instability issue arising from simple blending in the subsequent global optimization stage. To mitigate potential biases of LLM priors, we model collision relationships between objects at the global level, enhancing physical correctness and overall realism. Additionally, to generate pervasive objects like rain and snow distributed extensively across the scene, we introduce a sparse initialization and densification strategy. Experiments demonstrate that DreamScape offers high usability and controllability, enabling the generation of high-fidelity 3D scenes from only text prompts and achieving state-of-the-art performance compared to other methods.","sentences":["Recent progress in text-to-3D creation has been propelled by integrating the potent prior of Diffusion Models from text-to-image generation into the 3D domain.","Nevertheless, generating 3D scenes characterized by multiple instances and intricate arrangements remains challenging.","In this study, we present DreamScape, a method for creating highly consistent 3D scenes solely from textual descriptions, leveraging the strong 3D representation capabilities of Gaussian Splatting and the complex arrangement abilities of large language models (LLMs).","Our approach involves a 3D Gaussian Guide ($3{DG^2}$) for scene representation, consisting of semantic primitives (objects) and their spatial transformations and relationships derived directly from text prompts using LLMs.","This compositional representation allows for local-to-global optimization of the entire scene.","A progressive scale control is tailored during local object generation, ensuring that objects of different sizes and densities adapt to the scene, which addresses training instability issue arising from simple blending in the subsequent global optimization stage.","To mitigate potential biases of LLM priors, we model collision relationships between objects at the global level, enhancing physical correctness and overall realism.","Additionally, to generate pervasive objects like rain and snow distributed extensively across the scene, we introduce a sparse initialization and densification strategy.","Experiments demonstrate that DreamScape offers high usability and controllability, enabling the generation of high-fidelity 3D scenes from only text prompts and achieving state-of-the-art performance compared to other methods."],"url":"http://arxiv.org/abs/2404.09227v1","category":"cs.CV"}
{"created":"2024-04-14 11:26:15","title":"Opportunistic Information-Bottleneck for Goal-oriented Feature Extraction and Communication","abstract":"The Information Bottleneck (IB) method is an information theoretical framework to design a parsimonious and tunable feature-extraction mechanism, such that the extracted features are maximally relevant to a specific learning or inference task. Despite its theoretical value, the IB is based on a functional optimization problem that admits a closed form solution only on specific cases (e.g., Gaussian distributions), making it difficult to be applied in most applications, where it is necessary to resort to complex and approximated variational implementations. To overcome this limitation, we propose an approach to adapt the closed-form solution of the Gaussian IB to a general task. Whichever is the inference task to be performed by a (possibly deep) neural-network, the key idea is to opportunistically design a regression sub-task, embedded in the original problem, where we can safely assume a (joint) multivariate normality between the sub-task's inputs and outputs. In this way we can exploit a fixed and pre-trained neural network to process the input data, using a tunable number of features, to trade data-size and complexity for accuracy. This approach is particularly useful every time a device needs to transmit data (or features) to a server that has to fulfil an inference task, as it provides a principled way to extract the most relevant features for the task to be executed, while looking for the best trade-off between the size of the feature vector to be transmitted, inference accuracy, and complexity. Extensive simulation results testify the effectiveness of the proposed methodhttps://info.arxiv.org/help/prep#comments and encourage to further investigate this research line.","sentences":["The Information Bottleneck (IB) method is an information theoretical framework to design a parsimonious and tunable feature-extraction mechanism, such that the extracted features are maximally relevant to a specific learning or inference task.","Despite its theoretical value, the IB is based on a functional optimization problem that admits a closed form solution only on specific cases (e.g., Gaussian distributions), making it difficult to be applied in most applications, where it is necessary to resort to complex and approximated variational implementations.","To overcome this limitation, we propose an approach to adapt the closed-form solution of the Gaussian IB to a general task.","Whichever is the inference task to be performed by a (possibly deep) neural-network, the key idea is to opportunistically design a regression sub-task, embedded in the original problem, where we can safely assume a (joint) multivariate normality between the sub-task's inputs and outputs.","In this way we can exploit a fixed and pre-trained neural network to process the input data, using a tunable number of features, to trade data-size and complexity for accuracy.","This approach is particularly useful every time a device needs to transmit data (or features) to a server that has to fulfil an inference task, as it provides a principled way to extract the most relevant features for the task to be executed, while looking for the best trade-off between the size of the feature vector to be transmitted, inference accuracy, and complexity.","Extensive simulation results testify the effectiveness of the proposed methodhttps://info.arxiv.org/help/prep#comments and encourage to further investigate this research line."],"url":"http://arxiv.org/abs/2404.09218v1","category":"eess.SP"}
{"created":"2024-04-14 11:16:58","title":"Bosonic construction of CKP tau function","abstract":"The CKP tau function has been an important topic in mathematical physics. In this paper, the inverse of vacuum expectation value of exponential of certain bosonic fields, is showed to be the CKP tau function given by Chang and Wu, in the language of CKP Darboux transformation. In fact, computation of the above vacuum expectation value is usually quite difficult, since the square of bosonic fields is usually not zero. Here the corresponding vacuum expectation value is understood as successive application of CKP Darboux transformations, so that we can compute it by using the methods of integrable systems, where a useful formula is given. For applications, we construct solutions of KdV hierarchy by vacuum expectation value of bosonic fields, by the fact that KdV hierarchy is the 2-reduction of CKP hierarchy.","sentences":["The CKP tau function has been an important topic in mathematical physics.","In this paper, the inverse of vacuum expectation value of exponential of certain bosonic fields, is showed to be the CKP tau function given by Chang and Wu, in the language of CKP Darboux transformation.","In fact, computation of the above vacuum expectation value is usually quite difficult, since the square of bosonic fields is usually not zero.","Here the corresponding vacuum expectation value is understood as successive application of CKP Darboux transformations, so that we can compute it by using the methods of integrable systems, where a useful formula is given.","For applications, we construct solutions of KdV hierarchy by vacuum expectation value of bosonic fields, by the fact that KdV hierarchy is the 2-reduction of CKP hierarchy."],"url":"http://arxiv.org/abs/2404.09217v1","category":"nlin.SI"}
{"created":"2024-04-14 10:55:15","title":"PrintListener: Uncovering the Vulnerability of Fingerprint Authentication via the Finger Friction Sound","abstract":"Fingerprint authentication has been extensively employed in contemporary identity verification systems owing to its rapidity and cost-effectiveness. Due to its widespread use, fingerprint leakage may cause sensitive information theft, enormous economic and personnel losses, and even a potential compromise of national security. As a fingerprint that can coincidentally match a specific proportion of the overall fingerprint population, MasterPrint rings the alarm bells for the security of fingerprint authentication. In this paper, we propose a new side-channel attack on the minutiae-based Automatic Fingerprint Identification System (AFIS), called PrintListener, which leverages users' fingertip swiping actions on the screen to extract fingerprint pattern features (the first-level features) and synthesizes a stronger targeted PatternMasterPrint with potential second-level features. The attack scenario of PrintListener is extensive and covert. It only needs to record users' fingertip friction sound and can be launched by leveraging a large number of social media platforms. Extensive experimental results in realworld scenarios show that Printlistener can significantly improve the attack potency of MasterPrint.","sentences":["Fingerprint authentication has been extensively employed in contemporary identity verification systems owing to its rapidity and cost-effectiveness.","Due to its widespread use, fingerprint leakage may cause sensitive information theft, enormous economic and personnel losses, and even a potential compromise of national security.","As a fingerprint that can coincidentally match a specific proportion of the overall fingerprint population, MasterPrint rings the alarm bells for the security of fingerprint authentication.","In this paper, we propose a new side-channel attack on the minutiae-based Automatic Fingerprint Identification System (AFIS), called PrintListener, which leverages users' fingertip swiping actions on the screen to extract fingerprint pattern features (the first-level features) and synthesizes a stronger targeted PatternMasterPrint with potential second-level features.","The attack scenario of PrintListener is extensive and covert.","It only needs to record users' fingertip friction sound and can be launched by leveraging a large number of social media platforms.","Extensive experimental results in realworld scenarios show that Printlistener can significantly improve the attack potency of MasterPrint."],"url":"http://arxiv.org/abs/2404.09214v1","category":"cs.CR"}
{"created":"2024-04-14 10:52:01","title":"Qandle: Accelerating State Vector Simulation Using Gate-Matrix Caching and Circuit Splitting","abstract":"To address the computational complexity associated with state-vector simulation for quantum circuits, we propose a combination of advanced techniques to accelerate circuit execution. Quantum gate matrix caching reduces the overhead of repeated applications of the Kronecker product when applying a gate matrix to the state vector by storing decomposed partial matrices for each gate. Circuit splitting divides the circuit into sub-circuits with fewer gates by constructing a dependency graph, enabling parallel or sequential execution on disjoint subsets of the state vector. These techniques are implemented using the PyTorch machine learning framework. We demonstrate the performance of our approach by comparing it to other PyTorch-compatible quantum state-vector simulators. Our implementation, named Qandle, is designed to seamlessly integrate with existing machine learning workflows, providing a user-friendly API and compatibility with the OpenQASM format. Qandle is an open-source project hosted on GitHub https://github.com/gstenzel/qandle and PyPI https://pypi.org/project/qandle/ .","sentences":["To address the computational complexity associated with state-vector simulation for quantum circuits, we propose a combination of advanced techniques to accelerate circuit execution.","Quantum gate matrix caching reduces the overhead of repeated applications of the Kronecker product when applying a gate matrix to the state vector by storing decomposed partial matrices for each gate.","Circuit splitting divides the circuit into sub-circuits with fewer gates by constructing a dependency graph, enabling parallel or sequential execution on disjoint subsets of the state vector.","These techniques are implemented using the PyTorch machine learning framework.","We demonstrate the performance of our approach by comparing it to other PyTorch-compatible quantum state-vector simulators.","Our implementation, named Qandle, is designed to seamlessly integrate with existing machine learning workflows, providing a user-friendly API and compatibility with the OpenQASM format.","Qandle is an open-source project hosted on GitHub https://github.com/gstenzel/qandle and PyPI https://pypi.org/project/qandle/ ."],"url":"http://arxiv.org/abs/2404.09213v1","category":"quant-ph"}
{"created":"2024-04-14 10:22:57","title":"Numerical Methods for Optimal Boundary Control of Advection-Diffusion-Reaction Systems","abstract":"This paper considers the optimal boundary control of chemical systems described by advection-diffusion-reaction (ADR) equations. We use a discontinuous Galerkin finite element method (DG-FEM) for the spatial discretization of the governing partial differential equations, and the optimal control problem is directly discretized using multiple shooting. The temporal discretization and the corresponding sensitivity calculations are achieved by an explicit singly diagonally-implicit Runge Kutta (ESDIRK) method. ADR systems arise in process systems engineering and their operation can potentially be improved by nonlinear model predictive control (NMPC). We demonstrate a numerical approach for the solution to their optimal control problems (OCPs) in a chromatography case study. Preparative liquid chromatography is an important downstream process in biopharmaceutical manufacturing. We show that multi-step elution trajectories for batch processes can be optimized for economic objectives, providing superior performance compared to classical gradient elution trajectories.","sentences":["This paper considers the optimal boundary control of chemical systems described by advection-diffusion-reaction (ADR) equations.","We use a discontinuous Galerkin finite element method (DG-FEM) for the spatial discretization of the governing partial differential equations, and the optimal control problem is directly discretized using multiple shooting.","The temporal discretization and the corresponding sensitivity calculations are achieved by an explicit singly diagonally-implicit Runge Kutta (ESDIRK) method.","ADR systems arise in process systems engineering and their operation can potentially be improved by nonlinear model predictive control (NMPC).","We demonstrate a numerical approach for the solution to their optimal control problems (OCPs) in a chromatography case study.","Preparative liquid chromatography is an important downstream process in biopharmaceutical manufacturing.","We show that multi-step elution trajectories for batch processes can be optimized for economic objectives, providing superior performance compared to classical gradient elution trajectories."],"url":"http://arxiv.org/abs/2404.09209v1","category":"math.NA"}
{"created":"2024-04-14 10:08:31","title":"Logarithmic multicanonical systems of smooth affine surfaces of logarithmic Kodaira dimension one","abstract":"Let $S$ be a smooth affine surface of logarithmic Kodaira dimension one and let $(V,D)$ be a pair of a smooth projective surface $V$ and a simple normal crossing divisor $D$ on $V$ such that $V \\setminus \\operatorname{Supp} D = S$. In this paper, we consider the logarithmic multicanonical system $|m(K_V + D)|$. We prove that, for any $m \\geq 8$, $|m(K_V+D)|$ gives an $\\mathbb{P}^1$-fibration form $V$ onto a smooth projective curve.","sentences":["Let $S$ be a smooth affine surface of logarithmic Kodaira dimension one and let $(V,D)$ be a pair of a smooth projective surface $V$ and a simple normal crossing divisor $D$ on $V$ such that $V \\setminus \\operatorname{Supp} D = S$.","In this paper, we consider the logarithmic multicanonical system $|m(K_V + D)|$. We prove that, for any $m \\geq 8$, $|m(K_V+D)|$ gives an $\\mathbb{P}^1$-fibration form $V$ onto a smooth projective curve."],"url":"http://arxiv.org/abs/2404.09208v1","category":"math.AG"}
{"created":"2024-04-15 12:35:10","title":"Equipping Diffusion Models with Differentiable Spatial Entropy for Low-Light Image Enhancement","abstract":"Image restoration, which aims to recover high-quality images from their corrupted counterparts, often faces the challenge of being an ill-posed problem that allows multiple solutions for a single input. However, most deep learning based works simply employ l1 loss to train their network in a deterministic way, resulting in over-smoothed predictions with inferior perceptual quality. In this work, we propose a novel method that shifts the focus from a deterministic pixel-by-pixel comparison to a statistical perspective, emphasizing the learning of distributions rather than individual pixel values. The core idea is to introduce spatial entropy into the loss function to measure the distribution difference between predictions and targets. To make this spatial entropy differentiable, we employ kernel density estimation (KDE) to approximate the probabilities for specific intensity values of each pixel with their neighbor areas. Specifically, we equip the entropy with diffusion models and aim for superior accuracy and enhanced perceptual quality over l1 based noise matching loss. In the experiments, we evaluate the proposed method for low light enhancement on two datasets and the NTIRE challenge 2024. All these results illustrate the effectiveness of our statistic-based entropy loss. Code is available at https://github.com/shermanlian/spatial-entropy-loss.","sentences":["Image restoration, which aims to recover high-quality images from their corrupted counterparts, often faces the challenge of being an ill-posed problem that allows multiple solutions for a single input.","However, most deep learning based works simply employ l1 loss to train their network in a deterministic way, resulting in over-smoothed predictions with inferior perceptual quality.","In this work, we propose a novel method that shifts the focus from a deterministic pixel-by-pixel comparison to a statistical perspective, emphasizing the learning of distributions rather than individual pixel values.","The core idea is to introduce spatial entropy into the loss function to measure the distribution difference between predictions and targets.","To make this spatial entropy differentiable, we employ kernel density estimation (KDE) to approximate the probabilities for specific intensity values of each pixel with their neighbor areas.","Specifically, we equip the entropy with diffusion models and aim for superior accuracy and enhanced perceptual quality over l1 based noise matching loss.","In the experiments, we evaluate the proposed method for low light enhancement on two datasets and the NTIRE challenge 2024.","All these results illustrate the effectiveness of our statistic-based entropy loss.","Code is available at https://github.com/shermanlian/spatial-entropy-loss."],"url":"http://arxiv.org/abs/2404.09735v1","category":"cs.CV"}
{"created":"2024-04-15 12:10:07","title":"Nonconvergence of a sum-of-squares hierarchy for global polynomial optimization based on push-forward measures","abstract":"Let $\\mathbf{X} \\subseteq \\mathbb{R}^n$ be a closed set, and consider the problem of computing the minimum $f_{\\min}$ of a polynomial $f$ on $\\mathbf{X}$. Given a measure $\\mu$ supported on $\\mathbf{X}$, Lasserre (SIAM J. Optim. 21(3), 2011) proposes a decreasing sequence of upper bounds on $f_{\\min}$, each of which may be computed by solving a semidefinite program. When $\\mathbf{X}$ is compact, these bounds converge to $f_{\\min}$ under minor assumptions on $\\mu$. Later, Lasserre (Math. Program. 190, 2020) introduces a related, but far more economical sequence of upper bounds which rely on the push-forward measure of $\\mu$ by $f$. While these new bounds are weaker a priori, they actually achieve similar asymptotic convergence rates on compact sets. In this work, we show that no such free lunch exists in the non-compact setting. While convergence of the standard bounds to $f_{\\min}$ is guaranteed when $\\mathbf{X} = \\mathbb{R}^n$ and $\\mu$ is a Gaussian distribution, we prove that the bounds relying on the push-forward measure fail to converge to $f_{\\min}$ in that setting already for polynomials of degree $6$.","sentences":["Let $\\mathbf{X} \\subseteq \\mathbb{R}^n$ be a closed set, and consider the problem of computing the minimum $f_{\\min}$ of a polynomial $f$ on $\\mathbf{X}$. Given a measure $\\mu$ supported on $\\mathbf{X}$, Lasserre (SIAM J. Optim. 21(3), 2011) proposes a decreasing sequence of upper bounds on $f_{\\min}$, each of which may be computed by solving a semidefinite program.","When $\\mathbf{X}$ is compact, these bounds converge to $f_{\\min}$ under minor assumptions on $\\mu$. Later, Lasserre (Math. Program.","190, 2020) introduces a related, but far more economical sequence of upper bounds which rely on the push-forward measure of $\\mu$ by $f$. While these new bounds are weaker a priori, they actually achieve similar asymptotic convergence rates on compact sets.","In this work, we show that no such free lunch exists in the non-compact setting.","While convergence of the standard bounds to $f_{\\min}$ is guaranteed when $\\mathbf{X} = \\mathbb{R}^n$ and $\\mu$ is a Gaussian distribution, we prove that the bounds relying on the push-forward measure fail to converge to $f_{\\min}$ in that setting already for polynomials of degree $6$."],"url":"http://arxiv.org/abs/2404.09710v1","category":"math.OC"}
{"created":"2024-04-15 12:04:24","title":"Adaptive integration of history variables in constrained mixture models for organ-scale growth and remodeling","abstract":"In the last decades, many computational models have been developed to predict soft tissue growth and remodeling (G&R). The constrained mixture theory describes fundamental mechanobiological processes in soft tissue G&R and has been widely adopted in cardiovascular models of G&R. However, even after two decades of work, large organ-scale models are rare, mainly due to high computational costs (model evaluation and memory consumption), especially in long-range simulations. We propose two strategies to adaptively integrate history variables in constrained mixture models to enable large organ-scale simulations of G&R. Both strategies exploit that the influence of deposited tissue on the current mixture decreases over time through degradation. One strategy is independent of external loading, allowing the estimation of the computational resources ahead of the simulation. The other adapts the history snapshots based on the local mechanobiological environment so that the additional integration errors can be controlled and kept negligibly small, even in G&R scenarios with severe perturbations. We analyze the adaptively integrated constrained mixture model on a tissue patch for a parameter study and show the performance under different G&R scenarios. To confirm that adaptive strategies enable large organ-scale examples, we show simulations of different hypertension conditions with a real-world example of a biventricular heart discretized with a finite element mesh. In our example, adaptive integrations sped up simulations by a factor of three and reduced memory requirements to one-sixth. The reduction of the computational costs gets even more pronounced for simulations over longer periods. Adaptive integration of the history variables allows studying more finely resolved models and longer G&R periods while computational costs are drastically reduced and largely constant in time.","sentences":["In the last decades, many computational models have been developed to predict soft tissue growth and remodeling (G&R).","The constrained mixture theory describes fundamental mechanobiological processes in soft tissue G&R and has been widely adopted in cardiovascular models of G&R.","However, even after two decades of work, large organ-scale models are rare, mainly due to high computational costs (model evaluation and memory consumption), especially in long-range simulations.","We propose two strategies to adaptively integrate history variables in constrained mixture models to enable large organ-scale simulations of G&R.","Both strategies exploit that the influence of deposited tissue on the current mixture decreases over time through degradation.","One strategy is independent of external loading, allowing the estimation of the computational resources ahead of the simulation.","The other adapts the history snapshots based on the local mechanobiological environment so that the additional integration errors can be controlled and kept negligibly small, even in G&R scenarios with severe perturbations.","We analyze the adaptively integrated constrained mixture model on a tissue patch for a parameter study and show the performance under different G&R scenarios.","To confirm that adaptive strategies enable large organ-scale examples, we show simulations of different hypertension conditions with a real-world example of a biventricular heart discretized with a finite element mesh.","In our example, adaptive integrations sped up simulations by a factor of three and reduced memory requirements to one-sixth.","The reduction of the computational costs gets even more pronounced for simulations over longer periods.","Adaptive integration of the history variables allows studying more finely resolved models and longer G&R periods while computational costs are drastically reduced and largely constant in time."],"url":"http://arxiv.org/abs/2404.09706v1","category":"q-bio.TO"}
{"created":"2024-04-15 11:12:12","title":"A Circus of Circuits: Connections Between Decision Diagrams, Circuits, and Automata","abstract":"This document is an introduction to two related formalisms to define Boolean functions: binary decision diagrams, and Boolean circuits. It presents these formalisms and several of their variants studied in the setting of knowledge compilation. Last, it explains how these formalisms can be connected to the notions of automata over words and trees.","sentences":["This document is an introduction to two related formalisms to define Boolean functions: binary decision diagrams, and Boolean circuits.","It presents these formalisms and several of their variants studied in the setting of knowledge compilation.","Last, it explains how these formalisms can be connected to the notions of automata over words and trees."],"url":"http://arxiv.org/abs/2404.09674v1","category":"cs.DS"}
{"created":"2024-04-15 10:47:25","title":"OpenAirLink: Reproducible Wireless Channel Emulation using Software Defined Radios","abstract":"This paper presents OpenAirLink(OAL), an open-source channel emulator for reproducible testing of wireless scenarios. OAL is implemented on off-the-shelf software-defined radios (SDR) and presents a smaller-scale alternative to expensive commercially available channel emulators. Path loss and propagation delay are the fundamental aspects of emulating a wireless channel. OAL provides a simple method to change these aspects in real-time. The emulator is implemented using a finite impulse response (FIR) filter. The FIR filter is written in Verilog and flashed on the SDRs Field Programmable Gate Array (FPGA). Most processing transpires on the FPGA, so OAL does not require high-performance computing hardware and SDRs. We validate the performance of OAL and demonstrate the utility of such a channel emulation tool using two examples. We believe that open-source channel emulators such as OAL can make reproducible wireless experiments accessible to many researchers in the scientific community.","sentences":["This paper presents OpenAirLink(OAL), an open-source channel emulator for reproducible testing of wireless scenarios.","OAL is implemented on off-the-shelf software-defined radios (SDR) and presents a smaller-scale alternative to expensive commercially available channel emulators.","Path loss and propagation delay are the fundamental aspects of emulating a wireless channel.","OAL provides a simple method to change these aspects in real-time.","The emulator is implemented using a finite impulse response (FIR) filter.","The FIR filter is written in Verilog and flashed on the SDRs Field Programmable Gate Array (FPGA).","Most processing transpires on the FPGA, so OAL does not require high-performance computing hardware and SDRs.","We validate the performance of OAL and demonstrate the utility of such a channel emulation tool using two examples.","We believe that open-source channel emulators such as OAL can make reproducible wireless experiments accessible to many researchers in the scientific community."],"url":"http://arxiv.org/abs/2404.09660v1","category":"cs.NI"}
{"created":"2024-04-15 10:25:14","title":"Object Instance Retrieval in Assistive Robotics: Leveraging Fine-Tuned SimSiam with Multi-View Images Based on 3D Semantic Map","abstract":"Robots that assist in daily life are required to locate specific instances of objects that match the user's desired object in the environment. This task is known as Instance-Specific Image Goal Navigation (InstanceImageNav), which requires a model capable of distinguishing between different instances within the same class. One significant challenge in robotics is that when a robot observes the same object from various 3D viewpoints, its appearance may differ greatly, making it difficult to recognize and locate the object accurately. In this study, we introduce a method, SimView, that leverages multi-view images based on a 3D semantic map of the environment and self-supervised learning by SimSiam to train an instance identification model on-site. The effectiveness of our approach is validated using a photorealistic simulator, Habitat Matterport 3D, created by scanning real home environments. Our results demonstrate a 1.7-fold improvement in task accuracy compared to CLIP, which is pre-trained multimodal contrastive learning for object search. This improvement highlights the benefits of our proposed fine-tuning method in enhancing the performance of assistive robots in InstanceImageNav tasks. The project website is https://emergentsystemlabstudent.github.io/MultiViewRetrieve/.","sentences":["Robots that assist in daily life are required to locate specific instances of objects that match the user's desired object in the environment.","This task is known as Instance-Specific Image Goal Navigation (InstanceImageNav), which requires a model capable of distinguishing between different instances within the same class.","One significant challenge in robotics is that when a robot observes the same object from various 3D viewpoints, its appearance may differ greatly, making it difficult to recognize and locate the object accurately.","In this study, we introduce a method, SimView, that leverages multi-view images based on a 3D semantic map of the environment and self-supervised learning by SimSiam to train an instance identification model on-site.","The effectiveness of our approach is validated using a photorealistic simulator, Habitat Matterport 3D, created by scanning real home environments.","Our results demonstrate a 1.7-fold improvement in task accuracy compared to CLIP, which is pre-trained multimodal contrastive learning for object search.","This improvement highlights the benefits of our proposed fine-tuning method in enhancing the performance of assistive robots in InstanceImageNav tasks.","The project website is https://emergentsystemlabstudent.github.io/MultiViewRetrieve/."],"url":"http://arxiv.org/abs/2404.09647v1","category":"cs.RO"}
{"created":"2024-04-15 10:19:39","title":"CREST: Cross-modal Resonance through Evidential Deep Learning for Enhanced Zero-Shot Learning","abstract":"Zero-shot learning (ZSL) enables the recognition of novel classes by leveraging semantic knowledge transfer from known to unknown categories. This knowledge, typically encapsulated in attribute descriptions, aids in identifying class-specific visual features, thus facilitating visual-semantic alignment and improving ZSL performance. However, real-world challenges such as distribution imbalances and attribute co-occurrence among instances often hinder the discernment of local variances in images, a problem exacerbated by the scarcity of fine-grained, region-specific attribute annotations. Moreover, the variability in visual presentation within categories can also skew attribute-category associations. In response, we propose a bidirectional cross-modal ZSL approach CREST. It begins by extracting representations for attribute and visual localization and employs Evidential Deep Learning (EDL) to measure underlying epistemic uncertainty, thereby enhancing the model's resilience against hard negatives. CREST incorporates dual learning pathways, focusing on both visual-category and attribute-category alignments, to ensure robust correlation between latent and observable spaces. Moreover, we introduce an uncertainty-informed cross-modal fusion technique to refine visual-attribute inference. Extensive experiments demonstrate our model's effectiveness and unique explainability across multiple datasets. Our code and data are available at: Comments: Ongoing work; 10 pages, 2 Tables, 9 Figures; Repo is available at https://github.com/JethroJames/CREST.","sentences":["Zero-shot learning (ZSL) enables the recognition of novel classes by leveraging semantic knowledge transfer from known to unknown categories.","This knowledge, typically encapsulated in attribute descriptions, aids in identifying class-specific visual features, thus facilitating visual-semantic alignment and improving ZSL performance.","However, real-world challenges such as distribution imbalances and attribute co-occurrence among instances often hinder the discernment of local variances in images, a problem exacerbated by the scarcity of fine-grained, region-specific attribute annotations.","Moreover, the variability in visual presentation within categories can also skew attribute-category associations.","In response, we propose a bidirectional cross-modal ZSL approach CREST.","It begins by extracting representations for attribute and visual localization and employs Evidential Deep Learning (EDL) to measure underlying epistemic uncertainty, thereby enhancing the model's resilience against hard negatives.","CREST incorporates dual learning pathways, focusing on both visual-category and attribute-category alignments, to ensure robust correlation between latent and observable spaces.","Moreover, we introduce an uncertainty-informed cross-modal fusion technique to refine visual-attribute inference.","Extensive experiments demonstrate our model's effectiveness and unique explainability across multiple datasets.","Our code and data are available at: Comments: Ongoing work; 10 pages, 2 Tables, 9 Figures; Repo is available at https://github.com/JethroJames/CREST."],"url":"http://arxiv.org/abs/2404.09640v1","category":"cs.CV"}
{"created":"2024-04-15 09:08:02","title":"Using Tangible Interaction to Design Musicking Artifacts for Non-musicians","abstract":"This paper presents a Research through Design exploration of the potential for using tangible interactions to enable active music experiences - musicking - for non-musicians. We present the Tubularium prototype, which aims to help non-musicians play music without requiring any initial skill. We present the initial design of the prototype and the features implemented in order to enable music-making by non-musicians, and offer some reflections based on observations of informal initial user explorations of the prototype.","sentences":["This paper presents a Research through Design exploration of the potential for using tangible interactions to enable active music experiences - musicking - for non-musicians.","We present the Tubularium prototype, which aims to help non-musicians play music without requiring any initial skill.","We present the initial design of the prototype and the features implemented in order to enable music-making by non-musicians, and offer some reflections based on observations of informal initial user explorations of the prototype."],"url":"http://arxiv.org/abs/2404.09597v1","category":"cs.HC"}
{"created":"2024-04-15 08:54:33","title":"Mitigating the Curse of Dimensionality for Certified Robustness via Dual Randomized Smoothing","abstract":"Randomized Smoothing (RS) has been proven a promising method for endowing an arbitrary image classifier with certified robustness. However, the substantial uncertainty inherent in the high-dimensional isotropic Gaussian noise imposes the curse of dimensionality on RS. Specifically, the upper bound of ${\\ell_2}$ certified robustness radius provided by RS exhibits a diminishing trend with the expansion of the input dimension $d$, proportionally decreasing at a rate of $1/\\sqrt{d}$. This paper explores the feasibility of providing ${\\ell_2}$ certified robustness for high-dimensional input through the utilization of dual smoothing in the lower-dimensional space. The proposed Dual Randomized Smoothing (DRS) down-samples the input image into two sub-images and smooths the two sub-images in lower dimensions. Theoretically, we prove that DRS guarantees a tight ${\\ell_2}$ certified robustness radius for the original input and reveal that DRS attains a superior upper bound on the ${\\ell_2}$ robustness radius, which decreases proportionally at a rate of $(1/\\sqrt m + 1/\\sqrt n )$ with $m+n=d$. Extensive experiments demonstrate the generalizability and effectiveness of DRS, which exhibits a notable capability to integrate with established methodologies, yielding substantial improvements in both accuracy and ${\\ell_2}$ certified robustness baselines of RS on the CIFAR-10 and ImageNet datasets. Code is available at https://github.com/xiasong0501/DRS.","sentences":["Randomized Smoothing (RS) has been proven a promising method for endowing an arbitrary image classifier with certified robustness.","However, the substantial uncertainty inherent in the high-dimensional isotropic Gaussian noise imposes the curse of dimensionality on RS.","Specifically, the upper bound of ${\\ell_2}$ certified robustness radius provided by RS exhibits a diminishing trend with the expansion of the input dimension $d$, proportionally decreasing at a rate of $1/\\sqrt{d}$. This paper explores the feasibility of providing ${\\ell_2}$ certified robustness for high-dimensional input through the utilization of dual smoothing in the lower-dimensional space.","The proposed Dual Randomized Smoothing (DRS) down-samples the input image into two sub-images and smooths the two sub-images in lower dimensions.","Theoretically, we prove that DRS guarantees a tight ${\\ell_2}$ certified robustness radius for the original input and reveal that DRS attains a superior upper bound on the ${\\ell_2}$ robustness radius, which decreases proportionally at a rate of $(1/\\sqrt m + 1/\\sqrt n )$ with $m+n=d$. Extensive experiments demonstrate the generalizability and effectiveness of DRS, which exhibits a notable capability to integrate with established methodologies, yielding substantial improvements in both accuracy and ${\\ell_2}$ certified robustness baselines of RS on the CIFAR-10 and ImageNet datasets.","Code is available at https://github.com/xiasong0501/DRS."],"url":"http://arxiv.org/abs/2404.09586v1","category":"cs.CV"}
{"created":"2024-04-15 08:52:51","title":"Pseudo-label Learning with Calibrated Confidence Using an Energy-based Model","abstract":"In pseudo-labeling (PL), which is a type of semi-supervised learning, pseudo-labels are assigned based on the confidence scores provided by the classifier; therefore, accurate confidence is important for successful PL. In this study, we propose a PL algorithm based on an energy-based model (EBM), which is referred to as the energy-based PL (EBPL). In EBPL, a neural network-based classifier and an EBM are jointly trained by sharing their feature extraction parts. This approach enables the model to learn both the class decision boundary and input data distribution, enhancing confidence calibration during network training. The experimental results demonstrate that EBPL outperforms the existing PL method in semi-supervised image classification tasks, with superior confidence calibration error and recognition accuracy.","sentences":["In pseudo-labeling (PL), which is a type of semi-supervised learning, pseudo-labels are assigned based on the confidence scores provided by the classifier; therefore, accurate confidence is important for successful PL.","In this study, we propose a PL algorithm based on an energy-based model (EBM), which is referred to as the energy-based PL (EBPL).","In EBPL, a neural network-based classifier and an EBM are jointly trained by sharing their feature extraction parts.","This approach enables the model to learn both the class decision boundary and input data distribution, enhancing confidence calibration during network training.","The experimental results demonstrate that EBPL outperforms the existing PL method in semi-supervised image classification tasks, with superior confidence calibration error and recognition accuracy."],"url":"http://arxiv.org/abs/2404.09585v1","category":"cs.CV"}
{"created":"2024-04-15 08:44:06","title":"The central limit theorem for sum-functions of m-tuples of spacings","abstract":"Let n points be taken at random on a circle of unit circumference and clockwise ordered. Uniform spacings are defined as the clockwise arc-lengths between the successive points from this sample. We are interested in the asymptotic behavior of the sum of functions of the m-tuples of successive spacings under the assumption that m can grow together with n. Asymptotic formulas for the expectation and variance of the sum and its asymptotic normality are established.","sentences":["Let n points be taken at random on a circle of unit circumference and clockwise ordered.","Uniform spacings are defined as the clockwise arc-lengths between the successive points from this sample.","We are interested in the asymptotic behavior of the sum of functions of the m-tuples of successive spacings under the assumption that m can grow together with n. Asymptotic formulas for the expectation and variance of the sum and its asymptotic normality are established."],"url":"http://arxiv.org/abs/2404.09581v1","category":"math.PR"}
{"created":"2024-04-15 08:42:39","title":"The metric for matrix degenerate Kato square root operators","abstract":"We prove a Kato square root estimate with anisotropically degenerate matrix coefficients. We do so by doing the harmonic analysis using an auxiliary Riemannian metric adapted to the operator. We also derive $L^2$-solvability estimates for boundary value problems for divergence form elliptic equations with matrix degenerate coefficients. Main tools are chain rules and Piola transformations for fields in matrix weighted $L^2$ spaces, under $W^{1,1}$ homeomorphism.","sentences":["We prove a Kato square root estimate with anisotropically degenerate matrix coefficients.","We do so by doing the harmonic analysis using an auxiliary Riemannian metric adapted to the operator.","We also derive $L^2$-solvability estimates for boundary value problems for divergence form elliptic equations with matrix degenerate coefficients.","Main tools are chain rules and Piola transformations for fields in matrix weighted $L^2$ spaces, under $W^{1,1}$ homeomorphism."],"url":"http://arxiv.org/abs/2404.09580v1","category":"math.AP"}
{"created":"2024-04-15 08:31:59","title":"Surprising pressure-induced magnetic transformations from Helimagnetic order to Antiferromagnetic state in NiI2","abstract":"Interlayer magnetic interactions play a pivotal role in determining the magnetic arrangement within van der Waals (vdW) magnets, and the remarkable tunability of these interactions through applied pressure further enhances their significance. Here, we investigate NiI2 flakes, a representative vdW magnet, under hydrostatic pressures up to 11 GPa. We reveal a notable increase in magnetic transition temperatures for both helimagnetic and antiferromagnetic states, and find that a reversible transition from helimagnetic to antiferromagnetic (AFM) phases at approximately 7 GPa challenges established theoretical and experimental expectations. While the increase in transition temperature aligns with pressure-enhanced overall exchange interaction strengths, we identify the significant role of the second-nearest neighbor interlayer interaction, which competes with intra-layer frustration and favors the AFM state as demonstrated in the Monte Carlo simulations. Experimental and simulated results converge on the existence of an intermediate helimagnetic ordered state in NiI2 before transitioning to the AFM state. These findings underscore the pivotal role of interlayer interactions in shaping the magnetic ground state, providing fresh perspectives for innovative applications in nanoscale magnetic device design.","sentences":["Interlayer magnetic interactions play a pivotal role in determining the magnetic arrangement within van der Waals (vdW) magnets, and the remarkable tunability of these interactions through applied pressure further enhances their significance.","Here, we investigate NiI2 flakes, a representative vdW magnet, under hydrostatic pressures up to 11 GPa.","We reveal a notable increase in magnetic transition temperatures for both helimagnetic and antiferromagnetic states, and find that a reversible transition from helimagnetic to antiferromagnetic (AFM) phases at approximately 7 GPa challenges established theoretical and experimental expectations.","While the increase in transition temperature aligns with pressure-enhanced overall exchange interaction strengths, we identify the significant role of the second-nearest neighbor interlayer interaction, which competes with intra-layer frustration and favors the AFM state as demonstrated in the Monte Carlo simulations.","Experimental and simulated results converge on the existence of an intermediate helimagnetic ordered state in NiI2 before transitioning to the AFM state.","These findings underscore the pivotal role of interlayer interactions in shaping the magnetic ground state, providing fresh perspectives for innovative applications in nanoscale magnetic device design."],"url":"http://arxiv.org/abs/2404.09569v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-15 08:24:01","title":"Upper Limit of Sound Speed in Nuclear Matter: A Harmonious Interplay of Transport Calculation and Perturbative QCD Constraint","abstract":"Very recently, it has been shown that there is an upper bound on the squared sound speed of nuclear matter from the transport, which reads $c_{\\rm s}^2 \\leq 0.781$ \\citep{2024arXiv240214085H}. In this work, we demonstrate that this upper bound is corroborated by the reconstructed equation of state (EOS) for ultra-dense matter. The reconstruction integrates multi-messenger observation for neutron stars (NSs), as well as the theoretical constraints, including that from chiral effective field theory and perturbative quantum chromodynamics (pQCD). In particular, the latest radius measurements for PSR J0437-4715 ($11.36^{+0.95}_{-0.63}$ km) and PSR J0030+0451 ($11.71^{+0.88}_{-0.83}$ km, in the ST+PDT model) by NICER have been adopted. The result shows in all cases, the $c_{\\rm s}^2 \\leq 0.781$ upper limit for EOS will naturally yield the properties of matter near center of the massive neutron star consistent with the causality-driven constraint from pQCD, where in practice, the constraint is applied at ten nuclear saturation density ($n_{\\rm L}=10n_{\\rm s}$). We also note that there is a strong correlation for the maximum square of sound speed $c_s^2$ with $n_{\\rm L}$, and $c_{\\rm s}^2 \\leq 0.781$ is somehow violated when $n_{\\rm L} = n_{\\rm c,TOV}$. The result indicates that a higher density in implementing the pQCD constraint, even considering the uncertainties from statistics, is more natural. Moreover, the remarkable agreement between the outcomes derived from these two distinct and independent constraints (i.e., the transport calculation and pQCD boundary) lends strong support to their validity.","sentences":["Very recently, it has been shown that there is an upper bound on the squared sound speed of nuclear matter from the transport, which reads $c_{\\rm s}^2 \\leq 0.781$ \\citep{2024arXiv240214085H}.","In this work, we demonstrate that this upper bound is corroborated by the reconstructed equation of state (EOS) for ultra-dense matter.","The reconstruction integrates multi-messenger observation for neutron stars (NSs), as well as the theoretical constraints, including that from chiral effective field theory and perturbative quantum chromodynamics (pQCD).","In particular, the latest radius measurements for PSR J0437-4715 ($11.36^{+0.95}_{-0.63}$ km) and PSR J0030","+0451 ($11.71^{+0.88}_{-0.83}$ km, in the ST+PDT model) by NICER have been adopted.","The result shows in all cases, the $c_{\\rm s}^2 \\leq 0.781$ upper limit for EOS will naturally yield the properties of matter near center of the massive neutron star consistent with the causality-driven constraint from pQCD, where in practice, the constraint is applied at ten nuclear saturation density ($n_{\\rm L}=10n_{\\rm s}$).","We also note that there is a strong correlation for the maximum square of sound speed $c_s^2$ with $n_{\\rm L}$, and $c_{\\rm s}^2 \\leq 0.781$ is somehow violated when $n_{\\rm L} = n_{\\rm c,TOV}$.","The result indicates that a higher density in implementing the pQCD constraint, even considering the uncertainties from statistics, is more natural.","Moreover, the remarkable agreement between the outcomes derived from these two distinct and independent constraints (i.e., the transport calculation and pQCD boundary) lends strong support to their validity."],"url":"http://arxiv.org/abs/2404.09563v1","category":"astro-ph.HE"}
{"created":"2024-04-15 08:12:58","title":"Detection of simultaneous QPO triplets in 4U 1728-34 and constraining the neutron star mass and moment of inertia","abstract":"We report simultaneous detection of twin kHz and $\\sim 40$ Hz quasi-periodic oscillations (QPOs) in the time-resolved analysis of the AstroSat/LAXPC observation of the neutron star low mass X-ray binary, 4U 1728-34. The frequencies of the multiple sets of triplets are correlated with each other and are consistent with their identification as the orbital, periastron and twice the nodal precessions frequencies. The observed relations, along with the known spin of the neutron star, put constraints on the mass and the ratio of moment of inertia to the mass of the neutron star to be $M^*_\\odot = 1.92\\pm 0.01$ and $I_{45}/M^*_\\odot = 1.07\\pm 0.01$ under the simplistic assumption that the metric is a Kerr one. We crudely estimate that the mass and moment of inertia values obtained may differ by about 1 % and 5 %, respectively, if a self-consistent metric is invoked. Using the TOV equations for computing the moment of inertia of a neutron star in slow rotation approximation, having different equations of state, we find that the predicted values of neutron star parameters favor stiffer equations of state. We expect more stringent constraints would be obtained using a more detailed treatment, where the EOS-dependent metric is used to compute the expected frequencies rather than the Kerr metric used here. The results provide insight into both the nature of these QPOs and the neutron star interior.","sentences":["We report simultaneous detection of twin kHz and $\\sim 40$ Hz quasi-periodic oscillations (QPOs) in the time-resolved analysis of the AstroSat/LAXPC observation of the neutron star low mass X-ray binary, 4U 1728-34.","The frequencies of the multiple sets of triplets are correlated with each other and are consistent with their identification as the orbital, periastron and twice the nodal precessions frequencies.","The observed relations, along with the known spin of the neutron star, put constraints on the mass and the ratio of moment of inertia to the mass of the neutron star to be $M^*_\\odot = 1.92\\pm 0.01$ and $I_{45}/M^*_\\odot = 1.07\\pm 0.01$ under the simplistic assumption that the metric is a Kerr one.","We crudely estimate that the mass and moment of inertia values obtained may differ by about 1 % and 5 %, respectively, if a self-consistent metric is invoked.","Using the TOV equations for computing the moment of inertia of a neutron star in slow rotation approximation, having different equations of state, we find that the predicted values of neutron star parameters favor stiffer equations of state.","We expect more stringent constraints would be obtained using a more detailed treatment, where the EOS-dependent metric is used to compute the expected frequencies rather than the Kerr metric used here.","The results provide insight into both the nature of these QPOs and the neutron star interior."],"url":"http://arxiv.org/abs/2404.09546v1","category":"astro-ph.HE"}
{"created":"2024-04-15 08:03:56","title":"Light single-gluon hybrid states with various (exotic) quantum numbers","abstract":"We apply the QCD sum rule method to study the light single-gluon hybrid states with various (exotic) quantum numbers. We construct twenty-four single-gluon hybrid currents, and use eighteen of them to calculate the masses of forty-four single-gluon hybrid states with the quark-gluon contents $\\bar q q g$ ($q=u/d$) and $\\bar s s g$. We concentrate on the hybrid states with the exotic quantum number $J^{PC} = 1^{-+}$, whose masses and widths are calculated to be $M_{|\\bar q q g;1^-1^{-+}\\rangle} =1.67^{+0.15}_{-0.17}$ GeV, $\\Gamma_{|\\bar q q g;1^-1^{-+}\\rangle} = 530^{+540}_{-330}$ MeV, $M_{|\\bar q q g;0^+1^{-+}\\rangle} = 1.67^{+0.15}_{-0.17}$ GeV, $\\Gamma_{|\\bar q q g;0^+1^{-+}\\rangle} = 120^{+160}_{-110}$ MeV, $M_{|\\bar s s g;0^+1^{-+}\\rangle} = 1.84^{+0.14}_{-0.15}$ GeV, and $\\Gamma_{|\\bar s s g;0^+1^{-+}\\rangle} = 100^{+110}_{-~80}$ MeV. Our results support the interpretations of the $\\pi_1(1600)$ and $\\eta_1(1855)$ as the hybrid states $|\\bar q q g;1^-1^{-+}\\rangle$ and $|\\bar s s g;0^+1^{-+}\\rangle$, respectively. Considering the uncertainties, our results suggest that the $\\pi_1(1600)$ and $\\eta_1(1855)$ may also be interpreted as the hybrid states $|\\bar q q g;1^-1^{-+}\\rangle$ and $|\\bar q q g;0^+1^{-+}\\rangle$, respectively. To differentiate these two assignments and to verify whether they are hybrid states or not, we propose to examine the $a_1(1260) \\pi$ decay channel in future experiments.","sentences":["We apply the QCD sum rule method to study the light single-gluon hybrid states with various (exotic) quantum numbers.","We construct twenty-four single-gluon hybrid currents, and use eighteen of them to calculate the masses of forty-four single-gluon hybrid states with the quark-gluon contents $\\bar q q g$ ($q=u/d$) and $\\bar s s g$. We concentrate on the hybrid states with the exotic quantum number $J^{PC} = 1^{-+}$, whose masses and widths are calculated to be $M_{|\\bar q q g;1^-1^{-+}\\rangle} =1.67^{+0.15}_{-0.17}$ GeV, $\\Gamma_{|\\bar q q g;1^-1^{-+}\\rangle} = 530^{+540}_{-330}$ MeV, $M_{|\\bar q q g;0^+1^{-+}\\rangle} = 1.67^{+0.15}_{-0.17}$ GeV, $\\Gamma_{|\\bar q q g;0^+1^{-+}\\rangle} = 120^{+160}_{-110}$ MeV, $M_{|\\bar s s g;0^+1^{-+}\\rangle} = 1.84^{+0.14}_{-0.15}$ GeV, and $\\Gamma_{|\\bar s s g;0^+1^{-+}\\rangle} = 100^{+110}_{-~80}$ MeV.","Our results support the interpretations of the $\\pi_1(1600)$ and $\\eta_1(1855)$ as the hybrid states $|\\bar q q g;1^-1^{-+}\\rangle$ and $|\\bar s s g;0^+1^{-+}\\rangle$, respectively.","Considering the uncertainties, our results suggest that the $\\pi_1(1600)$ and $\\eta_1(1855)$ may also be interpreted as the hybrid states $|\\bar q q g;1^-1^{-+}\\rangle$ and $|\\bar q q g;0^+1^{-+}\\rangle$, respectively.","To differentiate these two assignments and to verify whether they are hybrid states or not, we propose to examine the $a_1(1260) \\pi$ decay channel in future experiments."],"url":"http://arxiv.org/abs/2404.09538v1","category":"hep-ph"}
{"created":"2024-04-15 07:39:56","title":"More, better or different? Trade-offs between group size and competence development in jury theorems","abstract":"In many circumstances there is a trade off between the number of voters and the time they can be given before having to make a decision since both aspects are costly. An example is the hiring of a committee with a fixed salary budget: more people but a shorter time for each to develop their competence about the issue at hand or less people with a longer time for competence development? In this paper we investigate the interaction between the number of voters, the development of their competence over time and the final probability for an optimal majority decision. Among other things we consider how different learning profiles, or rates of relevant competence increase, for the members of a committee affects the optimal committee size.   To the best of our knowledge, our model is the first that includes the potentially positive effects of having a heterogeneous group of voters on majority decisions in a satisfactory way. We also discuss how some earlier attempts fail to capture the effect of heterogeneity correctly.","sentences":["In many circumstances there is a trade off between the number of voters and the time they can be given before having to make a decision since both aspects are costly.","An example is the hiring of a committee with a fixed salary budget: more people but a shorter time for each to develop their competence about the issue at hand or less people with a longer time for competence development?","In this paper we investigate the interaction between the number of voters, the development of their competence over time and the final probability for an optimal majority decision.","Among other things we consider how different learning profiles, or rates of relevant competence increase, for the members of a committee affects the optimal committee size.   ","To the best of our knowledge, our model is the first that includes the potentially positive effects of having a heterogeneous group of voters on majority decisions in a satisfactory way.","We also discuss how some earlier attempts fail to capture the effect of heterogeneity correctly."],"url":"http://arxiv.org/abs/2404.09523v1","category":"econ.TH"}
{"created":"2024-04-15 06:09:17","title":"Deep Learning for Cosmological Parameter Inference from Dark Matter Halo Density Field","abstract":"We propose a lightweight deep convolutional neural network (lCNN) to estimate cosmological parameters from simulated three-dimensional DM halo distributions and associated statistics. The training dataset comprises 2000 realizations of a cubic box with a side length of 1000 $h^{-1}{\\rm Mpc}$, and interpolated over a cubic grid of $300^3$ voxels, with each simulation produced using $512^3$ DM particles and $512^3$ neutrinos . Under the flat $\\Lambda$CDM model, simulations vary standard six cosmological parameters including $\\Omega_m$, $\\Omega_b$, $h$, $n_s$, $\\sigma_8$, $w$, along with the neutrino mass sum, $M_\\nu$. We find that: 1) within the framework of lCNN, extracting large-scale structure information is more efficient from the halo density field compared to relying on the statistical quantities including the power spectrum, the two-point correlation function, and the coefficients from wavelet scattering transform; 2) combining the halo density field with its Fourier transformed counterpart enhances predictions, while augmenting the training dataset with measured statistics further improves performance; 3) achieving high accuracy in inferring $\\Omega_m$, $h$, $n_s$, and $\\sigma_8$ by the neural network model, while being inefficient in predicting $\\Omega_b$,$M_\\nu$ and $w$; 4) compared to the simple random forest network trained with three statistical quantities, lCNN yields unbiased estimations with reduced statistical errors: approximately 33.3\\% for $\\Omega_m$, 20.0\\% for $h$, 8.3\\% for $n_s$, and 40.0\\% for $\\sigma_8$. Our study emphasizes this lCNN-based novel approach in extracting large-scale structure information and estimating cosmological parameters.","sentences":["We propose a lightweight deep convolutional neural network (lCNN) to estimate cosmological parameters from simulated three-dimensional DM halo distributions and associated statistics.","The training dataset comprises 2000 realizations of a cubic box with a side length of 1000 $h^{-1}{\\rm Mpc}$, and interpolated over a cubic grid of $300^3$ voxels, with each simulation produced using $512^3$ DM particles and $512^3$ neutrinos .","Under the flat $\\Lambda$CDM model, simulations vary standard six cosmological parameters including $\\Omega_m$, $\\Omega_b$, $h$, $n_s$, $\\sigma_8$, $w$, along with the neutrino mass sum, $M_\\nu$. We find that: 1) within the framework of lCNN, extracting large-scale structure information is more efficient from the halo density field compared to relying on the statistical quantities including the power spectrum, the two-point correlation function, and the coefficients from wavelet scattering transform; 2) combining the halo density field with its Fourier transformed counterpart enhances predictions, while augmenting the training dataset with measured statistics further improves performance; 3) achieving high accuracy in inferring $\\Omega_m$, $h$, $n_s$, and $\\sigma_8$ by the neural network model, while being inefficient in predicting $\\Omega_b$,$M_\\nu$ and $w$; 4) compared to the simple random forest network trained with three statistical quantities, lCNN yields unbiased estimations with reduced statistical errors: approximately 33.3\\% for $\\Omega_m$, 20.0\\% for $h$, 8.3\\% for $n_s$, and 40.0\\% for $\\sigma_8$. Our study emphasizes this lCNN-based novel approach in extracting large-scale structure information and estimating cosmological parameters."],"url":"http://arxiv.org/abs/2404.09483v1","category":"astro-ph.CO"}
{"created":"2024-04-15 06:07:50","title":"Binary microlensing by high eccentric stellar-mass black hole binaries","abstract":"Microlensing is one of the most promising tools for discovering stellar-mass black holes (BHs) in the Milky Way because it allows us to probe dark or faint celestial compact objects. While the existence of stellar-mass BHs has been confirmed through observation of X-ray binaries within our galaxy and gravitational waves from extragalactic BH binaries, a conclusive observation of microlensing events caused by Galactic BH binaries has yet to be achieved. In this study, we focus on those with high eccentricity, including unbound orbits, which can dynamically form in star clusters and could potentially increase the observation rate. We demonstrate parameter estimation for simulated light curves supposing various orbital configurations of BH binary lenses. We employ a model-based fitting using the Nelder-Mead method and Bayesian inference based on the Markov chain Monte Carlo method for the demonstration. The results show that we can retrieve true values of the parameters of high eccentric BH binary lenses within the 1$\\sigma$ uncertainty of inferred values. We conclude it is feasible to find high eccentric Galactic BH binaries from the observation of binary microlensing events.","sentences":["Microlensing is one of the most promising tools for discovering stellar-mass black holes (BHs) in the Milky Way because it allows us to probe dark or faint celestial compact objects.","While the existence of stellar-mass BHs has been confirmed through observation of X-ray binaries within our galaxy and gravitational waves from extragalactic BH binaries, a conclusive observation of microlensing events caused by Galactic BH binaries has yet to be achieved.","In this study, we focus on those with high eccentricity, including unbound orbits, which can dynamically form in star clusters and could potentially increase the observation rate.","We demonstrate parameter estimation for simulated light curves supposing various orbital configurations of BH binary lenses.","We employ a model-based fitting using the Nelder-Mead method and Bayesian inference based on the Markov chain Monte Carlo method for the demonstration.","The results show that we can retrieve true values of the parameters of high eccentric BH binary lenses within the 1$\\sigma$ uncertainty of inferred values.","We conclude it is feasible to find high eccentric Galactic BH binaries from the observation of binary microlensing events."],"url":"http://arxiv.org/abs/2404.09482v1","category":"astro-ph.HE"}
{"created":"2024-04-15 05:35:09","title":"Scoring Intervals using Non-hierarchical Transformer For Automatic Piano Transcription","abstract":"The neural semi-Markov Conditional Random Field (semi-CRF) framework has demonstrated promise for event-based piano transcription. In this framework, all events (notes or pedals) are represented as closed intervals tied to specific event types. The neural semi-CRF approach requires an interval scoring matrix that assigns a score for every candidate interval. However, designing an efficient and expressive architecture for scoring intervals is not trivial. In this paper, we introduce a simple method for scoring intervals using scaled inner product operations that resemble how attention scoring is done in transformers. We show theoretically that, due to the special structure from encoding the non-overlapping intervals, under a mild condition, the inner product operations are expressive enough to represent an ideal scoring matrix that can yield the correct transcription result. We then demonstrate that an encoder-only non-hierarchical transformer backbone, operating only on a low-time-resolution feature map, is capable of transcribing piano notes and pedals with high accuracy and time precision. The experiment shows that our approach achieves the new state-of-the-art performance across all subtasks in terms of the F1 measure on the Maestro dataset.","sentences":["The neural semi-Markov Conditional Random Field (semi-CRF) framework has demonstrated promise for event-based piano transcription.","In this framework, all events (notes or pedals) are represented as closed intervals tied to specific event types.","The neural semi-CRF approach requires an interval scoring matrix that assigns a score for every candidate interval.","However, designing an efficient and expressive architecture for scoring intervals is not trivial.","In this paper, we introduce a simple method for scoring intervals using scaled inner product operations that resemble how attention scoring is done in transformers.","We show theoretically that, due to the special structure from encoding the non-overlapping intervals, under a mild condition, the inner product operations are expressive enough to represent an ideal scoring matrix that can yield the correct transcription result.","We then demonstrate that an encoder-only non-hierarchical transformer backbone, operating only on a low-time-resolution feature map, is capable of transcribing piano notes and pedals with high accuracy and time precision.","The experiment shows that our approach achieves the new state-of-the-art performance across all subtasks in terms of the F1 measure on the Maestro dataset."],"url":"http://arxiv.org/abs/2404.09466v1","category":"cs.SD"}
{"created":"2024-04-15 04:20:01","title":"kNN-CLIP: Retrieval Enables Training-Free Segmentation on Continually Expanding Large Vocabularies","abstract":"Rapid advancements in continual segmentation have yet to bridge the gap of scaling to large continually expanding vocabularies under compute-constrained scenarios. We discover that traditional continual training leads to catastrophic forgetting under compute constraints, unable to outperform zero-shot segmentation methods. We introduce a novel strategy for semantic and panoptic segmentation with zero forgetting, capable of adapting to continually growing vocabularies without the need for retraining or large memory costs. Our training-free approach, kNN-CLIP, leverages a database of instance embeddings to enable open-vocabulary segmentation approaches to continually expand their vocabulary on any given domain with a single-pass through data, while only storing embeddings minimizing both compute and memory costs. This method achieves state-of-the-art mIoU performance across large-vocabulary semantic and panoptic segmentation datasets. We hope kNN-CLIP represents a step forward in enabling more efficient and adaptable continual segmentation, paving the way for advances in real-world large-vocabulary continual segmentation methods.","sentences":["Rapid advancements in continual segmentation have yet to bridge the gap of scaling to large continually expanding vocabularies under compute-constrained scenarios.","We discover that traditional continual training leads to catastrophic forgetting under compute constraints, unable to outperform zero-shot segmentation methods.","We introduce a novel strategy for semantic and panoptic segmentation with zero forgetting, capable of adapting to continually growing vocabularies without the need for retraining or large memory costs.","Our training-free approach, kNN-CLIP, leverages a database of instance embeddings to enable open-vocabulary segmentation approaches to continually expand their vocabulary on any given domain with a single-pass through data, while only storing embeddings minimizing both compute and memory costs.","This method achieves state-of-the-art mIoU performance across large-vocabulary semantic and panoptic segmentation datasets.","We hope kNN-CLIP represents a step forward in enabling more efficient and adaptable continual segmentation, paving the way for advances in real-world large-vocabulary continual segmentation methods."],"url":"http://arxiv.org/abs/2404.09447v1","category":"cs.CV"}
{"created":"2024-04-15 03:50:47","title":"Developing Lagrangian-based Methods for Nonsmooth Nonconvex Optimization","abstract":"In this paper, we consider the minimization of a nonsmooth nonconvex objective function $f(x)$ over a closed convex subset $\\mathcal{X}$ of $\\mathbb{R}^n$, with additional nonsmooth nonconvex constraints $c(x) = 0$. We develop a unified framework for developing Lagrangian-based methods, which takes a single-step update to the primal variables by some subgradient methods in each iteration. These subgradient methods are ``embedded'' into our framework, in the sense that they are incorporated as black-box updates to the primal variables. We prove that our proposed framework inherits the global convergence guarantees from these embedded subgradient methods under mild conditions. In addition, we show that our framework can be extended to solve constrained optimization problems with expectation constraints. Based on the proposed framework, we show that a wide range of existing stochastic subgradient methods, including the proximal SGD, proximal momentum SGD, and proximal ADAM, can be embedded into Lagrangian-based methods. Preliminary numerical experiments on deep learning tasks illustrate that our proposed framework yields efficient variants of Lagrangian-based methods with convergence guarantees for nonconvex nonsmooth constrained optimization problems.","sentences":["In this paper, we consider the minimization of a nonsmooth nonconvex objective function $f(x)$ over a closed convex subset $\\mathcal{X}$ of $\\mathbb{R}^n$, with additional nonsmooth nonconvex constraints $c(x) =","0$.","We develop a unified framework for developing Lagrangian-based methods, which takes a single-step update to the primal variables by some subgradient methods in each iteration.","These subgradient methods are ``embedded'' into our framework, in the sense that they are incorporated as black-box updates to the primal variables.","We prove that our proposed framework inherits the global convergence guarantees from these embedded subgradient methods under mild conditions.","In addition, we show that our framework can be extended to solve constrained optimization problems with expectation constraints.","Based on the proposed framework, we show that a wide range of existing stochastic subgradient methods, including the proximal SGD, proximal momentum SGD, and proximal ADAM, can be embedded into Lagrangian-based methods.","Preliminary numerical experiments on deep learning tasks illustrate that our proposed framework yields efficient variants of Lagrangian-based methods with convergence guarantees for nonconvex nonsmooth constrained optimization problems."],"url":"http://arxiv.org/abs/2404.09438v1","category":"math.OC"}
{"created":"2024-04-15 03:04:37","title":"On the Efficiency of Privacy Attacks in Federated Learning","abstract":"Recent studies have revealed severe privacy risks in federated learning, represented by Gradient Leakage Attacks. However, existing studies mainly aim at increasing the privacy attack success rate and overlook the high computation costs for recovering private data, making the privacy attack impractical in real applications. In this study, we examine privacy attacks from the perspective of efficiency and propose a framework for improving the Efficiency of Privacy Attacks in Federated Learning (EPAFL). We make three novel contributions. First, we systematically evaluate the computational costs for representative privacy attacks in federated learning, which exhibits a high potential to optimize efficiency. Second, we propose three early-stopping techniques to effectively reduce the computational costs of these privacy attacks. Third, we perform experiments on benchmark datasets and show that our proposed method can significantly reduce computational costs and maintain comparable attack success rates for state-of-the-art privacy attacks in federated learning. We provide the codes on GitHub at https://github.com/mlsysx/EPAFL.","sentences":["Recent studies have revealed severe privacy risks in federated learning, represented by Gradient Leakage Attacks.","However, existing studies mainly aim at increasing the privacy attack success rate and overlook the high computation costs for recovering private data, making the privacy attack impractical in real applications.","In this study, we examine privacy attacks from the perspective of efficiency and propose a framework for improving the Efficiency of Privacy Attacks in Federated Learning (EPAFL).","We make three novel contributions.","First, we systematically evaluate the computational costs for representative privacy attacks in federated learning, which exhibits a high potential to optimize efficiency.","Second, we propose three early-stopping techniques to effectively reduce the computational costs of these privacy attacks.","Third, we perform experiments on benchmark datasets and show that our proposed method can significantly reduce computational costs and maintain comparable attack success rates for state-of-the-art privacy attacks in federated learning.","We provide the codes on GitHub at https://github.com/mlsysx/EPAFL."],"url":"http://arxiv.org/abs/2404.09430v1","category":"cs.CR"}
{"created":"2024-04-15 01:58:54","title":"DeferredGS: Decoupled and Editable Gaussian Splatting with Deferred Shading","abstract":"Reconstructing and editing 3D objects and scenes both play crucial roles in computer graphics and computer vision. Neural radiance fields (NeRFs) can achieve realistic reconstruction and editing results but suffer from inefficiency in rendering. Gaussian splatting significantly accelerates rendering by rasterizing Gaussian ellipsoids. However, Gaussian splatting utilizes a single Spherical Harmonic (SH) function to model both texture and lighting, limiting independent editing capabilities of these components. Recently, attempts have been made to decouple texture and lighting with the Gaussian splatting representation but may fail to produce plausible geometry and decomposition results on reflective scenes. Additionally, the forward shading technique they employ introduces noticeable blending artifacts during relighting, as the geometry attributes of Gaussians are optimized under the original illumination and may not be suitable for novel lighting conditions. To address these issues, we introduce DeferredGS, a method for decoupling and editing the Gaussian splatting representation using deferred shading. To achieve successful decoupling, we model the illumination with a learnable environment map and define additional attributes such as texture parameters and normal direction on Gaussians, where the normal is distilled from a jointly trained signed distance function. More importantly, we apply deferred shading, resulting in more realistic relighting effects compared to previous methods. Both qualitative and quantitative experiments demonstrate the superior performance of DeferredGS in novel view synthesis and editing tasks.","sentences":["Reconstructing and editing 3D objects and scenes both play crucial roles in computer graphics and computer vision.","Neural radiance fields (NeRFs) can achieve realistic reconstruction and editing results but suffer from inefficiency in rendering.","Gaussian splatting significantly accelerates rendering by rasterizing Gaussian ellipsoids.","However, Gaussian splatting utilizes a single Spherical Harmonic (SH) function to model both texture and lighting, limiting independent editing capabilities of these components.","Recently, attempts have been made to decouple texture and lighting with the Gaussian splatting representation but may fail to produce plausible geometry and decomposition results on reflective scenes.","Additionally, the forward shading technique they employ introduces noticeable blending artifacts during relighting, as the geometry attributes of Gaussians are optimized under the original illumination and may not be suitable for novel lighting conditions.","To address these issues, we introduce DeferredGS, a method for decoupling and editing the Gaussian splatting representation using deferred shading.","To achieve successful decoupling, we model the illumination with a learnable environment map and define additional attributes such as texture parameters and normal direction on Gaussians, where the normal is distilled from a jointly trained signed distance function.","More importantly, we apply deferred shading, resulting in more realistic relighting effects compared to previous methods.","Both qualitative and quantitative experiments demonstrate the superior performance of DeferredGS in novel view synthesis and editing tasks."],"url":"http://arxiv.org/abs/2404.09412v1","category":"cs.CV"}
{"created":"2024-04-15 00:36:20","title":"Identification of cardiovascular diseases through ECG classification using wavelet transformation","abstract":"Cardiovascular diseases are the leading cause of mortality globally, necessitating advancements in diagnostic techniques. This study explores the application of wavelet transformation for classifying electrocardiogram (ECG) signals to identify various cardiovascular conditions. Utilizing the MIT-BIH Arrhythmia Database, we employed both continuous and discrete wavelet transforms to decompose ECG signals into frequency sub-bands, from which we extracted eight statistical features per band. These features were then used to train and test various classifiers, including K-Nearest Neighbors and Support Vector Machines, among others. The classifiers demonstrated high efficacy, with some achieving an accuracy of up to 96% on test data, suggesting that wavelet-based feature extraction significantly enhances the prediction of cardiovascular abnormalities in ECG data. The findings advocate for further exploration of wavelet transforms in medical diagnostics to improve automation and accuracy in disease detection. Future work will focus on optimizing feature selection and classifier parameters to refine predictive performance further.","sentences":["Cardiovascular diseases are the leading cause of mortality globally, necessitating advancements in diagnostic techniques.","This study explores the application of wavelet transformation for classifying electrocardiogram (ECG) signals to identify various cardiovascular conditions.","Utilizing the MIT-BIH Arrhythmia Database, we employed both continuous and discrete wavelet transforms to decompose ECG signals into frequency sub-bands, from which we extracted eight statistical features per band.","These features were then used to train and test various classifiers, including K-Nearest Neighbors and Support Vector Machines, among others.","The classifiers demonstrated high efficacy, with some achieving an accuracy of up to 96% on test data, suggesting that wavelet-based feature extraction significantly enhances the prediction of cardiovascular abnormalities in ECG data.","The findings advocate for further exploration of wavelet transforms in medical diagnostics to improve automation and accuracy in disease detection.","Future work will focus on optimizing feature selection and classifier parameters to refine predictive performance further."],"url":"http://arxiv.org/abs/2404.09393v1","category":"cs.CE"}
{"created":"2024-04-14 23:39:48","title":"Eddington envelopes: The fate of stars on parabolic orbits tidally disrupted by supermassive black holes","abstract":"Stars falling too close to massive black holes in the centres of galaxies can be torn apart by the strong tidal forces. Simulating the subsequent feeding of the black hole with disrupted material has proved challenging because of the range of timescales involved. Here we report a set of simulations that capture the relativistic disruption of the star, followed by one year of evolution of the returning debris stream. These reveal the formation of an expanding asymmetric bubble of material extending to hundreds of astronomical units -- an Eddington envelope with an optically thick inner region. Such envelopes have been hypothesised as the reprocessing layer needed to explain optical/UV emission in tidal disruption events, but never produced self-consistently in a simulation. Our model broadly matches the observed light curves with low temperatures, faint luminosities, and line widths of 10,000--20,000 km/s.","sentences":["Stars falling too close to massive black holes in the centres of galaxies can be torn apart by the strong tidal forces.","Simulating the subsequent feeding of the black hole with disrupted material has proved challenging because of the range of timescales involved.","Here we report a set of simulations that capture the relativistic disruption of the star, followed by one year of evolution of the returning debris stream.","These reveal the formation of an expanding asymmetric bubble of material extending to hundreds of astronomical units -- an Eddington envelope with an optically thick inner region.","Such envelopes have been hypothesised as the reprocessing layer needed to explain optical/UV emission in tidal disruption events, but never produced self-consistently in a simulation.","Our model broadly matches the observed light curves with low temperatures, faint luminosities, and line widths of 10,000--20,000 km/s."],"url":"http://arxiv.org/abs/2404.09381v1","category":"astro-ph.HE"}
{"created":"2024-04-14 21:27:41","title":"A Bayesian Joint Modelling for Misclassified Interval-censoring and Competing Risks","abstract":"In active surveillance of prostate cancer, cancer progression is interval-censored and the examination to detect progression is subject to misclassification, usually false negatives. Meanwhile, patients may initiate early treatment before progression detection, constituting a competing risk. We developed the Misclassification-Corrected Interval-censored Cause-specific Joint Model (MCICJM) to estimate the association between longitudinal biomarkers and cancer progression in this setting. The sensitivity of the examination is considered in the likelihood of this model via a parameter that may be set to a specific value if the sensitivity is known, or for which a prior distribution can be specified if the sensitivity is unknown. Our simulation results show that misspecification of the sensitivity parameter or ignoring it entirely impacts the model parameters, especially the parameter uncertainty and the baseline hazards. Moreover, specification of a prior distribution for the sensitivity parameter may reduce the risk of misspecification in settings where the exact sensitivity is unknown, but may cause identifiability issues. Thus, imposing restrictions on the baseline hazards is recommended. A trade-off between modelling with a sensitivity constant at the risk of misspecification and a sensitivity prior at the cost of flexibility needs to be decided.","sentences":["In active surveillance of prostate cancer, cancer progression is interval-censored and the examination to detect progression is subject to misclassification, usually false negatives.","Meanwhile, patients may initiate early treatment before progression detection, constituting a competing risk.","We developed the Misclassification-Corrected Interval-censored Cause-specific Joint Model (MCICJM) to estimate the association between longitudinal biomarkers and cancer progression in this setting.","The sensitivity of the examination is considered in the likelihood of this model via a parameter that may be set to a specific value if the sensitivity is known, or for which a prior distribution can be specified if the sensitivity is unknown.","Our simulation results show that misspecification of the sensitivity parameter or ignoring it entirely impacts the model parameters, especially the parameter uncertainty and the baseline hazards.","Moreover, specification of a prior distribution for the sensitivity parameter may reduce the risk of misspecification in settings where the exact sensitivity is unknown, but may cause identifiability issues.","Thus, imposing restrictions on the baseline hazards is recommended.","A trade-off between modelling with a sensitivity constant at the risk of misspecification and a sensitivity prior at the cost of flexibility needs to be decided."],"url":"http://arxiv.org/abs/2404.09362v1","category":"stat.ME"}
{"created":"2024-04-14 18:46:54","title":"Incremental data compression for PDE-constrained optimization with a data assimilation application","abstract":"This paper proposes and analyzes an inexact gradient method based on incremental proper orthogonal decomposition (iPOD) to address the data storage difficulty in time-dependent PDE-constrained optimization, particularly for a data assimilation problem as a detailed demonstration for the key ideas. The proposed method is proved robust by rigorous analysis. We first derive a sharp data compression error estimate of the iPOD with the help of Hilbert-Schmidt operators. Then we demonstrate a numerical PDE analysis to show how to properly choose Hilbert space for the iPOD data compression so that the gradient error is under control. We further prove that for a convex problem with appropriately bounded gradient error, the inexact gradient method achieves the accuracy level of the optimal solution while not hurting the convergence rate compared with the usual gradient method. Finally, numerical experiments are provided to verify the theoretical results and validate the proposed method.","sentences":["This paper proposes and analyzes an inexact gradient method based on incremental proper orthogonal decomposition (iPOD) to address the data storage difficulty in time-dependent PDE-constrained optimization, particularly for a data assimilation problem as a detailed demonstration for the key ideas.","The proposed method is proved robust by rigorous analysis.","We first derive a sharp data compression error estimate of the iPOD with the help of Hilbert-Schmidt operators.","Then we demonstrate a numerical PDE analysis to show how to properly choose Hilbert space for the iPOD data compression so that the gradient error is under control.","We further prove that for a convex problem with appropriately bounded gradient error, the inexact gradient method achieves the accuracy level of the optimal solution while not hurting the convergence rate compared with the usual gradient method.","Finally, numerical experiments are provided to verify the theoretical results and validate the proposed method."],"url":"http://arxiv.org/abs/2404.09323v1","category":"math.OC"}
{"created":"2024-04-14 17:33:33","title":"In My Perspective, In My Hands: Accurate Egocentric 2D Hand Pose and Action Recognition","abstract":"Action recognition is essential for egocentric video understanding, allowing automatic and continuous monitoring of Activities of Daily Living (ADLs) without user effort. Existing literature focuses on 3D hand pose input, which requires computationally intensive depth estimation networks or wearing an uncomfortable depth sensor. In contrast, there has been insufficient research in understanding 2D hand pose for egocentric action recognition, despite the availability of user-friendly smart glasses in the market capable of capturing a single RGB image. Our study aims to fill this research gap by exploring the field of 2D hand pose estimation for egocentric action recognition, making two contributions. Firstly, we introduce two novel approaches for 2D hand pose estimation, namely EffHandNet for single-hand estimation and EffHandEgoNet, tailored for an egocentric perspective, capturing interactions between hands and objects. Both methods outperform state-of-the-art models on H2O and FPHA public benchmarks. Secondly, we present a robust action recognition architecture from 2D hand and object poses. This method incorporates EffHandEgoNet, and a transformer-based action recognition method. Evaluated on H2O and FPHA datasets, our architecture has a faster inference time and achieves an accuracy of 91.32% and 94.43%, respectively, surpassing state of the art, including 3D-based methods. Our work demonstrates that using 2D skeletal data is a robust approach for egocentric action understanding. Extensive evaluation and ablation studies show the impact of the hand pose estimation approach, and how each input affects the overall performance.","sentences":["Action recognition is essential for egocentric video understanding, allowing automatic and continuous monitoring of Activities of Daily Living (ADLs) without user effort.","Existing literature focuses on 3D hand pose input, which requires computationally intensive depth estimation networks or wearing an uncomfortable depth sensor.","In contrast, there has been insufficient research in understanding 2D hand pose for egocentric action recognition, despite the availability of user-friendly smart glasses in the market capable of capturing a single RGB image.","Our study aims to fill this research gap by exploring the field of 2D hand pose estimation for egocentric action recognition, making two contributions.","Firstly, we introduce two novel approaches for 2D hand pose estimation, namely EffHandNet for single-hand estimation and EffHandEgoNet, tailored for an egocentric perspective, capturing interactions between hands and objects.","Both methods outperform state-of-the-art models on H2O and FPHA public benchmarks.","Secondly, we present a robust action recognition architecture from 2D hand and object poses.","This method incorporates EffHandEgoNet, and a transformer-based action recognition method.","Evaluated on H2O and FPHA datasets, our architecture has a faster inference time and achieves an accuracy of 91.32% and 94.43%, respectively, surpassing state of the art, including 3D-based methods.","Our work demonstrates that using 2D skeletal data is a robust approach for egocentric action understanding.","Extensive evaluation and ablation studies show the impact of the hand pose estimation approach, and how each input affects the overall performance."],"url":"http://arxiv.org/abs/2404.09308v1","category":"cs.CV"}
{"created":"2024-04-14 16:47:38","title":"Reap the Wild Wind: Detecting Media Storms in Large-Scale News Corpora","abstract":"Media Storms, dramatic outbursts of attention to a story, are central components of media dynamics and the attention landscape. Despite their significance, there has been little systematic and empirical research on this concept due to issues of measurement and operationalization. We introduce an iterative human-in-the-loop method to identify media storms in a large-scale corpus of news articles. The text is first transformed into signals of dispersion based on several textual characteristics. In each iteration, we apply unsupervised anomaly detection to these signals; each anomaly is then validated by an expert to confirm the presence of a storm, and those results are then used to tune the anomaly detection in the next iteration. We demonstrate the applicability of this method in two scenarios: first, supplementing an initial list of media storms within a specific time frame; and second, detecting media storms in new time periods. We make available a media storm dataset compiled using both scenarios. Both the method and dataset offer the basis for comprehensive empirical research into the concept of media storms, including characterizing them and predicting their outbursts and durations, in mainstream media or social media platforms.","sentences":["Media Storms, dramatic outbursts of attention to a story, are central components of media dynamics and the attention landscape.","Despite their significance, there has been little systematic and empirical research on this concept due to issues of measurement and operationalization.","We introduce an iterative human-in-the-loop method to identify media storms in a large-scale corpus of news articles.","The text is first transformed into signals of dispersion based on several textual characteristics.","In each iteration, we apply unsupervised anomaly detection to these signals; each anomaly is then validated by an expert to confirm the presence of a storm, and those results are then used to tune the anomaly detection in the next iteration.","We demonstrate the applicability of this method in two scenarios: first, supplementing an initial list of media storms within a specific time frame; and second, detecting media storms in new time periods.","We make available a media storm dataset compiled using both scenarios.","Both the method and dataset offer the basis for comprehensive empirical research into the concept of media storms, including characterizing them and predicting their outbursts and durations, in mainstream media or social media platforms."],"url":"http://arxiv.org/abs/2404.09299v1","category":"cs.CL"}
{"created":"2024-04-14 14:32:55","title":"Experimental observation of Hawking radiation","abstract":"In this manuscript we examine the Unruh-thermalized CERN-NA63 radiation reaction data set from the point of view of a diphoton Rindler bath. Under the assumption that these Hawking-Unruh diphoton pairs are microscopic trans-Planckian black holes, we find the resultant heat capacity describes the measured energy spectrum and is thus a dual description of the data set. Then, employing an n-dimensional Stefan-Boltzmann analysis, we find the power radiated by a black hole in the standard 3+1 spacetime dimensions in complete agreement with the data. Finally, we utilize this power spectrum to directly measure Newtons constant of gravitation.","sentences":["In this manuscript we examine the Unruh-thermalized CERN-NA63 radiation reaction data set from the point of view of a diphoton Rindler bath.","Under the assumption that these Hawking-Unruh diphoton pairs are microscopic trans-Planckian black holes, we find the resultant heat capacity describes the measured energy spectrum and is thus a dual description of the data set.","Then, employing an n-dimensional Stefan-Boltzmann analysis, we find the power radiated by a black hole in the standard 3+1 spacetime dimensions in complete agreement with the data.","Finally, we utilize this power spectrum to directly measure Newtons constant of gravitation."],"url":"http://arxiv.org/abs/2404.09274v1","category":"gr-qc"}
{"created":"2024-04-14 13:07:39","title":"Constraining Near-Simultaneous Radio Emission from Short Gamma-ray Bursts using CHIME/FRB","abstract":"We use the Canadian Hydrogen Intensity Mapping Experiment (CHIME) Fast Radio Burst (FRB) Project to search for FRBs that are temporally and spatially coincident with gamma-ray bursts (GRBs) occurring between 2018 July 7 and 2023 August 3. We do not find any temporal (within 1 week) and spatial (within overlapping 3 sigma localization regions) coincidences between any CHIME/FRB candidates and all GRBs with 1 sigma localization uncertainties <1 deg. As such, we use CHIME/FRB to constrain the possible FRB-like radio emission for 27 short gamma-ray bursts (SGRBs) that were within 17 deg. of CHIME/FRB's meridian at a point either 6 hrs prior up to 12 hrs after the high-energy emission. Two SGRBs, GRB 210909A and GRB 230208A, were above the horizon at CHIME at the time of their high-energy emission and we place some of the first constraints on simultaneous FRB-like radio emission from SGRBs. While neither of these two SGRBs have known redshifts, we construct a redshift range for each GRB based on their high-energy fluence and a derived SGRB energy distribution. For GRB 210909A, this redshift range corresponds to z = [0.009, 1.64] with a mean of z=0.13. Thus, for GRB 210909A, we constrain the radio luminosity at the time of the high-energy emission to L <2 x 10e46 erg s-1, L < 5 x 10e44 erg s-1, and L < 3 x 10e42 erg s-1 assuming redshifts of z=0.85, z=0.16, and z=0.013, respectively. We compare these constraints with the predicted simultaneous radio luminosities from different compact object merger models.","sentences":["We use the Canadian Hydrogen Intensity Mapping Experiment (CHIME) Fast Radio Burst (FRB) Project to search for FRBs that are temporally and spatially coincident with gamma-ray bursts (GRBs) occurring between 2018 July 7 and 2023 August 3.","We do not find any temporal (within 1 week) and spatial (within overlapping 3 sigma localization regions) coincidences between any CHIME/FRB candidates and all GRBs with 1 sigma localization uncertainties <1 deg.","As such, we use CHIME/FRB to constrain the possible FRB-like radio emission for 27 short gamma-ray bursts (SGRBs) that were within 17 deg.","of CHIME/FRB's meridian","at a point either 6 hrs prior up to 12 hrs after the high-energy emission.","Two SGRBs, GRB 210909A and GRB 230208A, were above the horizon at CHIME at the time of their high-energy emission and we place some of the first constraints on simultaneous FRB-like radio emission from SGRBs.","While neither of these two SGRBs have known redshifts",", we construct a redshift range for each GRB based on their high-energy fluence and a derived SGRB energy distribution.","For GRB 210909A, this redshift range corresponds to z =","[0.009, 1.64] with a mean of z=0.13.","Thus, for GRB 210909A, we constrain the radio luminosity at the time of the high-energy emission to L <2 x 10e46 erg s-1, L < 5 x 10e44 erg s-1, and L < 3 x 10e42 erg s-1 assuming redshifts of z=0.85, z=0.16, and z=0.013, respectively.","We compare these constraints with the predicted simultaneous radio luminosities from different compact object merger models."],"url":"http://arxiv.org/abs/2404.09242v1","category":"astro-ph.HE"}
{"created":"2024-04-14 12:09:47","title":"Breast Cancer Image Classification Method Based on Deep Transfer Learning","abstract":"To address the issues of limited samples, time-consuming feature design, and low accuracy in detection and classification of breast cancer pathological images, a breast cancer image classification model algorithm combining deep learning and transfer learning is proposed. This algorithm is based on the DenseNet structure of deep neural networks, and constructs a network model by introducing attention mechanisms, and trains the enhanced dataset using multi-level transfer learning. Experimental results demonstrate that the algorithm achieves an efficiency of over 84.0\\% in the test set, with a significantly improved classification accuracy compared to previous models, making it applicable to medical breast cancer detection tasks.","sentences":["To address the issues of limited samples, time-consuming feature design, and low accuracy in detection and classification of breast cancer pathological images, a breast cancer image classification model algorithm combining deep learning and transfer learning is proposed.","This algorithm is based on the DenseNet structure of deep neural networks, and constructs a network model by introducing attention mechanisms, and trains the enhanced dataset using multi-level transfer learning.","Experimental results demonstrate that the algorithm achieves an efficiency of over 84.0\\% in the test set, with a significantly improved classification accuracy compared to previous models, making it applicable to medical breast cancer detection tasks."],"url":"http://arxiv.org/abs/2404.09226v1","category":"eess.IV"}
{"created":"2024-04-14 09:25:11","title":"Unsourced Random Access in MIMO Quasi-Static Rayleigh Fading Channels with Finite Blocklength","abstract":"This paper explores the fundamental limits of unsourced random access (URA) with a random and unknown number ${\\rm{K}}_a$ of active users in MIMO quasi-static Rayleigh fading channels. First, we derive an upper bound on the probability of incorrectly estimating the number of active users. We prove that it exponentially decays with the number of receive antennas and eventually vanishes, whereas reaches a plateau as the power and blocklength increase. Then, we derive non-asymptotic achievability and converse bounds on the minimum energy-per-bit required by each active user to reliably transmit $J$ bits with blocklength $n$. Numerical results verify the tightness of our bounds, suggesting that they provide benchmarks to evaluate existing schemes. The extra required energy-per-bit due to the uncertainty of the number of active users decreases as $\\mathbb{E}[{\\rm{K}}_a]$ increases. Compared to random access with individual codebooks, the URA paradigm achieves higher spectral and energy efficiency. Moreover, using codewords distributed on a sphere is shown to outperform the Gaussian random coding scheme in the non-asymptotic regime.","sentences":["This paper explores the fundamental limits of unsourced random access (URA) with a random and unknown number ${\\rm{K}}_a$ of active users in MIMO quasi-static Rayleigh fading channels.","First, we derive an upper bound on the probability of incorrectly estimating the number of active users.","We prove that it exponentially decays with the number of receive antennas and eventually vanishes, whereas reaches a plateau as the power and blocklength increase.","Then, we derive non-asymptotic achievability and converse bounds on the minimum energy-per-bit required by each active user to reliably transmit $J$ bits with blocklength $n$. Numerical results verify the tightness of our bounds, suggesting that they provide benchmarks to evaluate existing schemes.","The extra required energy-per-bit due to the uncertainty of the number of active users decreases as $\\mathbb{E}[{\\rm{K}}_a]$ increases.","Compared to random access with individual codebooks, the URA paradigm achieves higher spectral and energy efficiency.","Moreover, using codewords distributed on a sphere is shown to outperform the Gaussian random coding scheme in the non-asymptotic regime."],"url":"http://arxiv.org/abs/2404.09198v1","category":"cs.IT"}
{"created":"2024-04-14 09:17:40","title":"Investigating Cosmic Homogeneity Using Multi-fractal Analysis of the SDSS-IV eBOSS DR16 Quasar Catalog","abstract":"We analyze the volume-limited subsamples extracted from the sixteenth data release of the SDSS-IV eBOSS quasar survey spanning a redshift interval of $0.8 < z < 2.2$, to estimate the scale of transition to homogeneity in the Universe. The multi-fractal analysis used for this purpose considers the scaling behavior of different moments of quasar distribution in different density environments. This analysis gives the spectrum of generalized dimension $D_q$, where positive values of $q$ characterize the scaling behavior in over-dense regions and the negative ones in under-dense regions. We expect fractal correlation dimension $D_q(r) = 3$, for a homogeneous, random point distribution in 3-Dimensions. The fractal correlation dimension $D_q(r)$, corresponding to $q=2$ obtained in our study stabilizes in the range (2.8-2.9) for scales $r>80$ $h^{-1}$ Mpc. The observed quasar distribution shows consistency with the simulated mock data and the random distribution of quasars within one sigma. Further, the generalized dimension spectrum $D_q(r)$ also reveals transition to homogeneity beyond $>110$ $h^{-1}$ Mpc, and the dominance of clustering at small scales $r<80$ $h^{-1}$ Mpc. Consequently, our study provides strong evidence for the homogeneity in SDSS quasar distribution, offering insights into large-scale structure properties and, thus can play a pivotal role in scrutinizing the clustering properties of quasars and its evolution in various upcoming surveys such as Dark Energy Spectroscopic Instrument (DESI) and Extremely Large Telescope (ELT).","sentences":["We analyze the volume-limited subsamples extracted from the sixteenth data release of the SDSS-IV eBOSS quasar survey spanning a redshift interval of $0.8 < z < 2.2$, to estimate the scale of transition to homogeneity in the Universe.","The multi-fractal analysis used for this purpose considers the scaling behavior of different moments of quasar distribution in different density environments.","This analysis gives the spectrum of generalized dimension $D_q$, where positive values of $q$ characterize the scaling behavior in over-dense regions and the negative ones in under-dense regions.","We expect fractal correlation dimension $D_q(r) = 3$, for a homogeneous, random point distribution in 3-Dimensions.","The fractal correlation dimension $D_q(r)$, corresponding to $q=2$ obtained in our study stabilizes in the range (2.8-2.9) for scales $r>80$ $h^{-1}$ Mpc.","The observed quasar distribution shows consistency with the simulated mock data and the random distribution of quasars within one sigma.","Further, the generalized dimension spectrum $D_q(r)$ also reveals transition to homogeneity beyond $>110$ $h^{-1}$ Mpc, and the dominance of clustering at small scales $r<80$ $h^{-1}$ Mpc.","Consequently, our study provides strong evidence for the homogeneity in SDSS quasar distribution, offering insights into large-scale structure properties and, thus can play a pivotal role in scrutinizing the clustering properties of quasars and its evolution in various upcoming surveys such as Dark Energy Spectroscopic Instrument (DESI) and Extremely Large Telescope (ELT)."],"url":"http://arxiv.org/abs/2404.09197v1","category":"astro-ph.CO"}
{"created":"2024-04-14 09:01:26","title":"FaceCat: Enhancing Face Recognition Security with a Unified Generative Model Framework","abstract":"Face anti-spoofing (FAS) and adversarial detection (FAD) have been regarded as critical technologies to ensure the safety of face recognition systems. As a consequence of their limited practicality and generalization, some existing methods aim to devise a framework capable of concurrently detecting both threats to address the challenge. Nevertheless, these methods still encounter challenges of insufficient generalization and suboptimal robustness, potentially owing to the inherent drawback of discriminative models. Motivated by the rich structural and detailed features of face generative models, we propose FaceCat which utilizes the face generative model as a pre-trained model to improve the performance of FAS and FAD. Specifically, FaceCat elaborately designs a hierarchical fusion mechanism to capture rich face semantic features of the generative model. These features then serve as a robust foundation for a lightweight head, designed to execute FAS and FAD tasks simultaneously. As relying solely on single-modality data often leads to suboptimal performance, we further propose a novel text-guided multi-modal alignment strategy that utilizes text prompts to enrich feature representation, thereby enhancing performance. For fair evaluations, we build a comprehensive protocol with a wide range of 28 attack types to benchmark the performance. Extensive experiments validate the effectiveness of FaceCat generalizes significantly better and obtains excellent robustness against input transformations.","sentences":["Face anti-spoofing (FAS) and adversarial detection (FAD) have been regarded as critical technologies to ensure the safety of face recognition systems.","As a consequence of their limited practicality and generalization, some existing methods aim to devise a framework capable of concurrently detecting both threats to address the challenge.","Nevertheless, these methods still encounter challenges of insufficient generalization and suboptimal robustness, potentially owing to the inherent drawback of discriminative models.","Motivated by the rich structural and detailed features of face generative models, we propose FaceCat which utilizes the face generative model as a pre-trained model to improve the performance of FAS and FAD.","Specifically, FaceCat elaborately designs a hierarchical fusion mechanism to capture rich face semantic features of the generative model.","These features then serve as a robust foundation for a lightweight head, designed to execute FAS and FAD tasks simultaneously.","As relying solely on single-modality data often leads to suboptimal performance, we further propose a novel text-guided multi-modal alignment strategy that utilizes text prompts to enrich feature representation, thereby enhancing performance.","For fair evaluations, we build a comprehensive protocol with a wide range of 28 attack types to benchmark the performance.","Extensive experiments validate the effectiveness of FaceCat generalizes significantly better and obtains excellent robustness against input transformations."],"url":"http://arxiv.org/abs/2404.09193v1","category":"cs.CV"}
{"created":"2024-04-14 08:43:17","title":"On Joint Convergence of Traffic State and Weight Vector in Learning-Based Dynamic Routing with Value Function Approximation","abstract":"Learning-based approaches are increasingly popular for traffic control problems. However, these approaches are applied typically as black boxes with limited theoretical guarantees and interpretability. In this paper, we consider the theory of dynamic routing over parallel servers, a representative traffic control task, using semi-gradient on-policy control algorithm, a representative reinforcement learning method. We consider a linear value function approximation on an infinite state space; a Lyapunov function is also derived from the approximator. In particular, the structure of the approximator naturally makes possible idling policies, which is an interesting and useful advantage over existing dynamic routing schemes. We show that the convergence of the approximation weights is coupled with the convergence of the traffic state. We show that if the system is stabilizable, then (i) the weight vector converges to a bounded region, and (ii) the traffic state is bounded in the mean. We also empirically show that the proposed algorithm is computationally efficient with an insignificant optimality gap.","sentences":["Learning-based approaches are increasingly popular for traffic control problems.","However, these approaches are applied typically as black boxes with limited theoretical guarantees and interpretability.","In this paper, we consider the theory of dynamic routing over parallel servers, a representative traffic control task, using semi-gradient on-policy control algorithm, a representative reinforcement learning method.","We consider a linear value function approximation on an infinite state space; a Lyapunov function is also derived from the approximator.","In particular, the structure of the approximator naturally makes possible idling policies, which is an interesting and useful advantage over existing dynamic routing schemes.","We show that the convergence of the approximation weights is coupled with the convergence of the traffic state.","We show that if the system is stabilizable, then (i) the weight vector converges to a bounded region, and (ii) the traffic state is bounded in the mean.","We also empirically show that the proposed algorithm is computationally efficient with an insignificant optimality gap."],"url":"http://arxiv.org/abs/2404.09188v1","category":"eess.SY"}
{"created":"2024-04-14 08:29:30","title":"Feasibility Study of Function Splits in RAN Architectures with LEO Satellites","abstract":"This paper explores the evolution of Radio Access Network (RAN) architectures and their integration into Non-Terrestrial Networks (NTN) to address escalating mobile traffic demands. Focusing on Low Earth Orbit (LEO) satellites as key components of NTN, we examine the feasibility of RAN function splits (FSs) in terms of fronthaul (FH) latency, elevation angle, and bandwidth (BW) across LEO satellites and ground stations (GS), alongside evaluating performance of Conditional Handover (CHO) procedures under diverse scenarios. By assessing performance metrics such as handover duration, disconnection time, and control traffic volume, we provide insights on several aspects such as stringent constraints for Low Layer Splits (LLSs), leading to longer delays during mobility procedures and increased control traffic across the feeder link in comparison with the case when gNodeB is onboard satellite. Despite challenges, LLSs demonstrate minimal onboard satellite computational requirements, promising reduced power consumption and payload weight. These findings underscore the architectural possibilities and challenges within the telecommunications industry, paving the way for future advancements in NTN RAN design and operation.","sentences":["This paper explores the evolution of Radio Access Network (RAN) architectures and their integration into Non-Terrestrial Networks (NTN) to address escalating mobile traffic demands.","Focusing on Low Earth Orbit (LEO) satellites as key components of NTN, we examine the feasibility of RAN function splits (FSs) in terms of fronthaul (FH) latency, elevation angle, and bandwidth (BW) across LEO satellites and ground stations (GS), alongside evaluating performance of Conditional Handover (CHO) procedures under diverse scenarios.","By assessing performance metrics such as handover duration, disconnection time, and control traffic volume, we provide insights on several aspects such as stringent constraints for Low Layer Splits (LLSs), leading to longer delays during mobility procedures and increased control traffic across the feeder link in comparison with the case when gNodeB is onboard satellite.","Despite challenges, LLSs demonstrate minimal onboard satellite computational requirements, promising reduced power consumption and payload weight.","These findings underscore the architectural possibilities and challenges within the telecommunications industry, paving the way for future advancements in NTN RAN design and operation."],"url":"http://arxiv.org/abs/2404.09186v1","category":"cs.NI"}
{"created":"2024-04-14 08:21:06","title":"Some algebras with trivial rings of differential operators","abstract":"Let $k$ be an arbitrary field. We construct examples of regular local $k$-algebras $R$ (of positive dimension) for which the ring of differential operators $D_k(R)$ is trivial in the sense that it contains {\\it no} operators of positive order. The examples are excellent in characteristic zero but not in positive characteristic. These rings can be viewed as being non-singular but they are not simple as $D$-modules, laying to rest speculation that $D$-simplicity might characterize a nice class of singularities in general. In prime characteristic, the construction also provides examples of {\\it regular} local rings $R$ (with fraction field a function field) whose Frobenius push-forward $F_*^eR$ is {\\it indecomposable} as an $R$-module for all $e\\in \\mathbb N$. Along the way, we investigate hypotheses on a local ring $(R, m)$ under which $D$-simplicity for $R$ is equivalent to $D$-simplicity for its $m$-adic completion, and give examples of rings for which the differential operators do not behave well under completion. We also generalize a characterization of $D$-simplicity due to Jeffries in the $\\mathbb N$-graded case: for a Noetherian local $k$-algebra $(R, m, k)$, $D$-simplicity of $R$ is equivalent to surjectivity of the natural map $D_k(R)\\to D_k(R, k)$.","sentences":["Let $k$ be an arbitrary field.","We construct examples of regular local $k$-algebras $R$ (of positive dimension) for which the ring of differential operators $D_k(R)$ is trivial in the sense that it contains {\\it no} operators of positive order.","The examples are excellent in characteristic zero but not in positive characteristic.","These rings can be viewed as being non-singular but they are not simple as $D$-modules, laying to rest speculation that $D$-simplicity might characterize a nice class of singularities in general.","In prime characteristic, the construction also provides examples of {\\it regular} local rings $R$ (with fraction field a function field) whose Frobenius push-forward $F_*^eR$ is {\\it indecomposable} as an $R$-module for all $e\\in \\mathbb N$. Along the way, we investigate hypotheses on a local ring $(R, m)$ under which $D$-simplicity for $R$ is equivalent to $D$-simplicity for its $m$-adic completion, and give examples of rings for which the differential operators do not behave well under completion.","We also generalize a characterization of $D$-simplicity due to Jeffries in the $\\mathbb N$-graded case: for a Noetherian local $k$-algebra $(R, m, k)$, $D$-simplicity of $R$ is equivalent to surjectivity of the natural map $D_k(R)\\to D_k(R, k)$."],"url":"http://arxiv.org/abs/2404.09184v1","category":"math.AC"}
{"created":"2024-04-14 08:13:46","title":"The Lunar Gravitational-wave Antenna: Mission Studies and Science Case","abstract":"The Lunar Gravitational-wave Antenna (LGWA) is a proposed array of next-generation inertial sensors to monitor the response of the Moon to gravitational waves (GWs). Given the size of the Moon and the expected noise produced by the lunar seismic background, the LGWA would be able to observe GWs from about 1 mHz to 1 Hz. This would make the LGWA the missing link between space-borne detectors like LISA with peak sensitivities around a few millihertz and proposed future terrestrial detectors like Einstein Telescope or Cosmic Explorer. In this article, we provide a first comprehensive analysis of the LGWA science case including its multi-messenger aspects and lunar science with LGWA data. We also describe the scientific analyses of the Moon required to plan the LGWA mission.","sentences":["The Lunar Gravitational-wave Antenna (LGWA) is a proposed array of next-generation inertial sensors to monitor the response of the Moon to gravitational waves (GWs).","Given the size of the Moon and the expected noise produced by the lunar seismic background, the LGWA would be able to observe GWs from about 1 mHz to 1 Hz.","This would make the LGWA the missing link between space-borne detectors like LISA with peak sensitivities around a few millihertz and proposed future terrestrial detectors like Einstein Telescope or Cosmic Explorer.","In this article, we provide a first comprehensive analysis of the LGWA science case including its multi-messenger aspects and lunar science with LGWA data.","We also describe the scientific analyses of the Moon required to plan the LGWA mission."],"url":"http://arxiv.org/abs/2404.09181v1","category":"gr-qc"}
{"created":"2024-04-14 07:47:24","title":"Expansions in completions of global function fields","abstract":"It is well known that any power series over a finite field represents a rational function if and only if its sequence of coefficients is ultimately periodic. The famous Christol's Theorem states that a power series over a finite field is algebraic if and only if its sequence of coefficients is $p$-automatic. In this paper, we extend these two results to expansions of elements in the completion of a global function field under a nontrivial valuation. As application of our generalization of Christol's theorem, we answer some questions about $\\beta$-expansions of formal Laurent series over finite fields.","sentences":["It is well known that any power series over a finite field represents a rational function if and only if its sequence of coefficients is ultimately periodic.","The famous Christol's Theorem states that a power series over a finite field is algebraic if and only if its sequence of coefficients is $p$-automatic.","In this paper, we extend these two results to expansions of elements in the completion of a global function field under a nontrivial valuation.","As application of our generalization of Christol's theorem, we answer some questions about $\\beta$-expansions of formal Laurent series over finite fields."],"url":"http://arxiv.org/abs/2404.09175v1","category":"math.NT"}
{"created":"2024-04-14 07:31:10","title":"On the solutions of the generalized Fermat equation over totally real number fields","abstract":"Let $K$ be a totally real number field, and $ \\mathcal{O}_K$ be the ring of integers of $K$. In this article, we study the asymptotic solutions of the generalized Fermat equation, i.e., $Ax^p+By^p+Cz^p=0$ over $K$ of prime exponent $p$, where $A,B,C \\in \\mathcal{O}_K \\setminus \\{0\\}$ with $ABC$ is even (in the sense that $\\mathfrak{P}| ABC$, for some prime ideal $\\mathfrak{P}$ of $ \\mathcal{O}_K$ with $\\mathfrak{P} |2$). For certain class of fields $K$, we prove that the equation $Ax^p+By^p+Cz^p=0$ has no asymptotic solution in $K^3$ (resp., of certain type in $K^3$), under some assumptions on $A,B,C$ (resp., for all $A,B,C \\in \\mathcal{O}_K \\setminus \\{0\\}$ with $ABC$ is even). We also present several purely local criteria of $K$ such that $Ax^p+By^p+Cz^p=0$ has no asymptotic solutions in $K^3$.","sentences":["Let $K$ be a totally real number field, and $ \\mathcal{O}_K$ be the ring of integers of $K$. In this article, we study the asymptotic solutions of the generalized Fermat equation, i.e., $Ax^p+By^p+Cz^p=0$ over $K$ of prime exponent $p$, where $A,B,C \\in \\mathcal{O}_K \\setminus \\{0\\}$ with $ABC$ is even (in the sense that $\\mathfrak{P}| ABC$, for some prime ideal $\\mathfrak{P}$ of $ \\mathcal{O}_K$ with $\\mathfrak{P} |2$).","For certain class of fields $K$, we prove that the equation $Ax^p+By^p+Cz^p=0$ has no asymptotic solution in $K^3$ (resp., of certain type in $K^3$), under some assumptions on $A,B,C$ (resp., for all $A,B,C \\in \\mathcal{O}_K \\setminus \\{0\\}$ with $ABC$ is even).","We also present several purely local criteria of $K$ such that $Ax^p+By^p+Cz^p=0$ has no asymptotic solutions in $K^3$."],"url":"http://arxiv.org/abs/2404.09171v1","category":"math.NT"}
{"created":"2024-04-14 07:19:27","title":"Post-Semantic-Thinking: A Robust Strategy to Distill Reasoning Capacity from Large Language Models","abstract":"Chain of thought finetuning aims to endow small student models with reasoning capacity to improve their performance towards a specific task by allowing them to imitate the reasoning procedure of large language models (LLMs) beyond simply predicting the answer to the question. However, the existing methods 1) generate rationale before the answer, making their answer correctness sensitive to the hallucination in the rationale;2) force the student model to repeat the exact LLMs rationale expression word-after-word, which could have the model biased towards learning the expression in rationale but count against the model from understanding the core logic behind it. Therefore, we propose a robust Post-Semantic-Thinking (PST) strategy to generate answers before rationale. Thanks to this answer-first setting, 1) the answering procedure can escape from the adverse effects caused by hallucinations in the rationale; 2) the complex reasoning procedure is tightly bound with the relatively concise answer, making the reasoning for questions easier with the prior information in the answer; 3) the efficiency of the method can also benefit from the setting since users can stop the generation right after answers are outputted when inference is conducted. Furthermore, the PST strategy loose the constraint against the generated rationale to be close to the LLMs gold standard in the hidden semantic space instead of the vocabulary space, thus making the small student model better comprehend the semantic reasoning logic in rationale. Extensive experiments conducted across 12 reasoning tasks demonstrate the effectiveness of PST.","sentences":["Chain of thought finetuning aims to endow small student models with reasoning capacity to improve their performance towards a specific task by allowing them to imitate the reasoning procedure of large language models (LLMs) beyond simply predicting the answer to the question.","However, the existing methods 1) generate rationale before the answer, making their answer correctness sensitive to the hallucination in the rationale;2) force the student model to repeat the exact LLMs rationale expression word-after-word, which could have the model biased towards learning the expression in rationale but count against the model from understanding the core logic behind it.","Therefore, we propose a robust Post-Semantic-Thinking (PST) strategy to generate answers before rationale.","Thanks to this answer-first setting, 1) the answering procedure can escape from the adverse effects caused by hallucinations in the rationale; 2) the complex reasoning procedure is tightly bound with the relatively concise answer, making the reasoning for questions easier with the prior information in the answer; 3) the efficiency of the method can also benefit from the setting since users can stop the generation right after answers are outputted when inference is conducted.","Furthermore, the PST strategy loose the constraint against the generated rationale to be close to the LLMs gold standard in the hidden semantic space instead of the vocabulary space, thus making the small student model better comprehend the semantic reasoning logic in rationale.","Extensive experiments conducted across 12 reasoning tasks demonstrate the effectiveness of PST."],"url":"http://arxiv.org/abs/2404.09170v1","category":"cs.CL"}
{"created":"2024-04-14 05:03:18","title":"Blind Interference Alignment for MapReduce: Exploiting Side-information with Reconfigurable Antennas","abstract":"In order to explore how blind interference alignment (BIA) schemes may take advantage of side-information in computation tasks, we study the degrees of freedom (DoF) of a $K$ user wireless network setting that arises in full-duplex wireless MapReduce applications. In this setting the receivers are assumed to have reconfigurable antennas and channel knowledge, while the transmitters have neither, i.e., the transmitters lack channel knowledge and are only equipped with conventional antennas. The central ingredient of the problem formulation is the message structure arising out of MapReduce, whereby each transmitter has a subset of messages that need to be delivered to various receivers, and each receiver has a subset of messages available to it in advance as side-information. The challenge resides in both achievability and converse arguments. Unlike conventional BIA where alignments occur only within the symbols of the same message (intra-message) the new achievable scheme also requires inter-message alignments, as well as an outer MDS (maximum distance separable) code structure. The scheme emerges from two essential ideas: 1) understanding the DoF of a $K$ user vector broadcast channel with groupcast messages, and 2) a mapping of messages from the broadcast setting to the MapReduce setting that makes use of inter-message alignment. On the converse side, whereas prior BIA converse bounds relied only on a compound channel argument, in the new setting our converse bounds also require a statistical equivalence assumption.","sentences":["In order to explore how blind interference alignment (BIA) schemes may take advantage of side-information in computation tasks, we study the degrees of freedom (DoF) of a $K$ user wireless network setting that arises in full-duplex wireless MapReduce applications.","In this setting the receivers are assumed to have reconfigurable antennas and channel knowledge, while the transmitters have neither, i.e., the transmitters lack channel knowledge and are only equipped with conventional antennas.","The central ingredient of the problem formulation is the message structure arising out of MapReduce, whereby each transmitter has a subset of messages that need to be delivered to various receivers, and each receiver has a subset of messages available to it in advance as side-information.","The challenge resides in both achievability and converse arguments.","Unlike conventional BIA where alignments occur only within the symbols of the same message (intra-message) the new achievable scheme also requires inter-message alignments, as well as an outer MDS (maximum distance separable) code structure.","The scheme emerges from two essential ideas: 1) understanding the DoF of a $K$ user vector broadcast channel with groupcast messages, and 2) a mapping of messages from the broadcast setting to the MapReduce setting that makes use of inter-message alignment.","On the converse side, whereas prior BIA converse bounds relied only on a compound channel argument, in the new setting our converse bounds also require a statistical equivalence assumption."],"url":"http://arxiv.org/abs/2404.09141v1","category":"cs.IT"}
{"created":"2024-04-14 04:56:05","title":"RF-Diffusion: Radio Signal Generation via Time-Frequency Diffusion","abstract":"Along with AIGC shines in CV and NLP, its potential in the wireless domain has also emerged in recent years. Yet, existing RF-oriented generative solutions are ill-suited for generating high-quality, time-series RF data due to limited representation capabilities. In this work, inspired by the stellar achievements of the diffusion model in CV and NLP, we adapt it to the RF domain and propose RF-Diffusion. To accommodate the unique characteristics of RF signals, we first introduce a novel Time-Frequency Diffusion theory to enhance the original diffusion model, enabling it to tap into the information within the time, frequency, and complex-valued domains of RF signals. On this basis, we propose a Hierarchical Diffusion Transformer to translate the theory into a practical generative DNN through elaborated design spanning network architecture, functional block, and complex-valued operator, making RF-Diffusion a versatile solution to generate diverse, high-quality, and time-series RF data. Performance comparison with three prevalent generative models demonstrates the RF-Diffusion's superior performance in synthesizing Wi-Fi and FMCW signals. We also showcase the versatility of RF-Diffusion in boosting Wi-Fi sensing systems and performing channel estimation in 5G networks.","sentences":["Along with AIGC shines in CV and NLP, its potential in the wireless domain has also emerged in recent years.","Yet, existing RF-oriented generative solutions are ill-suited for generating high-quality, time-series RF data due to limited representation capabilities.","In this work, inspired by the stellar achievements of the diffusion model in CV and NLP, we adapt it to the RF domain and propose RF-Diffusion.","To accommodate the unique characteristics of RF signals, we first introduce a novel Time-Frequency Diffusion theory to enhance the original diffusion model, enabling it to tap into the information within the time, frequency, and complex-valued domains of RF signals.","On this basis, we propose a Hierarchical Diffusion Transformer to translate the theory into a practical generative DNN through elaborated design spanning network architecture, functional block, and complex-valued operator, making RF-Diffusion a versatile solution to generate diverse, high-quality, and time-series RF data.","Performance comparison with three prevalent generative models demonstrates the RF-Diffusion's superior performance in synthesizing Wi-Fi and FMCW signals.","We also showcase the versatility of RF-Diffusion in boosting Wi-Fi sensing systems and performing channel estimation in 5G networks."],"url":"http://arxiv.org/abs/2404.09140v1","category":"cs.LG"}
{"created":"2024-04-14 02:48:34","title":"Design of Artificial Interference Signals for Covert Communication Aided by Multiple Friendly Nodes","abstract":"In this paper, we consider the scenario of covert communication aided by multiple friendly interference nodes. The objective is to conceal the legitimate communication link under the surveillance of a warden. We propose a novel strategy for generating artificial noise signals. In the absence of accurate channel fading information between the friendly interference nodes and the legitimate receiver, we leverage the statistical information of channel coefficients to optimize the basis matrix of the artificial noise signals space. The optimization aims to design artificial noise signals within the space to facilitate covert communication while minimizing the impact on the performance of legitimate communication. Due to the non-convex nature of the basis matrix constraints, the optimization problem is challenging to solve. Therefore, we employ the Riemannian optimization framework to analyze the geometric structure of the basis matrix constraints and transform the original non-convex optimization problem into an unconstrained problem on the complex Stiefel manifold for solution. Specifically, we utilize the Riemannian Stochastic Variance Reduced Gradient (R-SVRG) algorithm on the complex Stiefel manifold to solve the problem, significantly reducing the computational burden per iteration compared to full gradient algorithms. Additionally, we theoretically prove the convergence of the proposed algorithm to a stationary point. Finally, the performance of the proposed artificial noise strategy can be evaluated through numerical simulations, and compared to the Gaussian artificial noise strategy without optimization, the proposed strategy significantly improves covert performance.","sentences":["In this paper, we consider the scenario of covert communication aided by multiple friendly interference nodes.","The objective is to conceal the legitimate communication link under the surveillance of a warden.","We propose a novel strategy for generating artificial noise signals.","In the absence of accurate channel fading information between the friendly interference nodes and the legitimate receiver, we leverage the statistical information of channel coefficients to optimize the basis matrix of the artificial noise signals space.","The optimization aims to design artificial noise signals within the space to facilitate covert communication while minimizing the impact on the performance of legitimate communication.","Due to the non-convex nature of the basis matrix constraints, the optimization problem is challenging to solve.","Therefore, we employ the Riemannian optimization framework to analyze the geometric structure of the basis matrix constraints and transform the original non-convex optimization problem into an unconstrained problem on the complex Stiefel manifold for solution.","Specifically, we utilize the Riemannian Stochastic Variance Reduced Gradient (R-SVRG) algorithm on the complex Stiefel manifold to solve the problem, significantly reducing the computational burden per iteration compared to full gradient algorithms.","Additionally, we theoretically prove the convergence of the proposed algorithm to a stationary point.","Finally, the performance of the proposed artificial noise strategy can be evaluated through numerical simulations, and compared to the Gaussian artificial noise strategy without optimization, the proposed strategy significantly improves covert performance."],"url":"http://arxiv.org/abs/2404.09131v1","category":"eess.SP"}
{"created":"2024-04-14 02:47:32","title":"When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models","abstract":"Recent studies suggest that self-reflective prompting can significantly enhance the reasoning capabilities of Large Language Models (LLMs). However, the use of external feedback as a stop criterion raises doubts about the true extent of LLMs' ability to emulate human-like self-reflection. In this paper, we set out to clarify these capabilities under a more stringent evaluation setting in which we disallow any kind of external feedback. Our findings under this setting show a split: while self-reflection enhances performance in TruthfulQA, it adversely affects results in HotpotQA. We conduct follow-up analyses to clarify the contributing factors in these patterns, and find that the influence of self-reflection is impacted both by reliability of accuracy in models' initial responses, and by overall question difficulty: specifically, self-reflection shows the most benefit when models are less likely to be correct initially, and when overall question difficulty is higher. We also find that self-reflection reduces tendency toward majority voting. Based on our findings, we propose guidelines for decisions on when to implement self-reflection. We release the codebase for reproducing our experiments at https://github.com/yanhong-lbh/LLM-SelfReflection-Eval.","sentences":["Recent studies suggest that self-reflective prompting can significantly enhance the reasoning capabilities of Large Language Models (LLMs).","However, the use of external feedback as a stop criterion raises doubts about the true extent of LLMs' ability to emulate human-like self-reflection.","In this paper, we set out to clarify these capabilities under a more stringent evaluation setting in which we disallow any kind of external feedback.","Our findings under this setting show a split: while self-reflection enhances performance in TruthfulQA, it adversely affects results in HotpotQA.","We conduct follow-up analyses to clarify the contributing factors in these patterns, and find that the influence of self-reflection is impacted both by reliability of accuracy in models' initial responses, and by overall question difficulty: specifically, self-reflection shows the most benefit when models are less likely to be correct initially, and when overall question difficulty is higher.","We also find that self-reflection reduces tendency toward majority voting.","Based on our findings, we propose guidelines for decisions on when to implement self-reflection.","We release the codebase for reproducing our experiments at https://github.com/yanhong-lbh/LLM-SelfReflection-Eval."],"url":"http://arxiv.org/abs/2404.09129v1","category":"cs.CL"}
{"created":"2024-04-14 02:02:13","title":"Identifying Causal Effects under Kink Setting: Theory and Evidence","abstract":"This paper develops a generalized framework for identifying causal impacts in a reduced-form manner under kinked settings when agents can manipulate their choices around the threshold. The causal estimation using a bunching framework was initially developed by Diamond and Persson (2017) under notched settings. Many empirical applications of bunching designs involve kinked settings. We propose a model-free causal estimator in kinked settings with sharp bunching and then extend to the scenarios with diffuse bunching, misreporting, optimization frictions, and heterogeneity. The estimation method is mostly non-parametric and accounts for the interior response under kinked settings. Applying the proposed approach, we estimate how medical subsidies affect outpatient behaviors in China.","sentences":["This paper develops a generalized framework for identifying causal impacts in a reduced-form manner under kinked settings when agents can manipulate their choices around the threshold.","The causal estimation using a bunching framework was initially developed by Diamond and Persson (2017) under notched settings.","Many empirical applications of bunching designs involve kinked settings.","We propose a model-free causal estimator in kinked settings with sharp bunching and then extend to the scenarios with diffuse bunching, misreporting, optimization frictions, and heterogeneity.","The estimation method is mostly non-parametric and accounts for the interior response under kinked settings.","Applying the proposed approach, we estimate how medical subsidies affect outpatient behaviors in China."],"url":"http://arxiv.org/abs/2404.09117v1","category":"econ.EM"}
{"created":"2024-04-14 01:51:11","title":"GCC: Generative Calibration Clustering","abstract":"Deep clustering as an important branch of unsupervised representation learning focuses on embedding semantically similar samples into the identical feature space. This core demand inspires the exploration of contrastive learning and subspace clustering. However, these solutions always rely on the basic assumption that there are sufficient and category-balanced samples for generating valid high-level representation. This hypothesis actually is too strict to be satisfied for real-world applications. To overcome such a challenge, the natural strategy is utilizing generative models to augment considerable instances. How to use these novel samples to effectively fulfill clustering performance improvement is still difficult and under-explored. In this paper, we propose a novel Generative Calibration Clustering (GCC) method to delicately incorporate feature learning and augmentation into clustering procedure. First, we develop a discriminative feature alignment mechanism to discover intrinsic relationship across real and generated samples. Second, we design a self-supervised metric learning to generate more reliable cluster assignment to boost the conditional diffusion generation. Extensive experimental results on three benchmarks validate the effectiveness and advantage of our proposed method over the state-of-the-art methods.","sentences":["Deep clustering as an important branch of unsupervised representation learning focuses on embedding semantically similar samples into the identical feature space.","This core demand inspires the exploration of contrastive learning and subspace clustering.","However, these solutions always rely on the basic assumption that there are sufficient and category-balanced samples for generating valid high-level representation.","This hypothesis actually is too strict to be satisfied for real-world applications.","To overcome such a challenge, the natural strategy is utilizing generative models to augment considerable instances.","How to use these novel samples to effectively fulfill clustering performance improvement is still difficult and under-explored.","In this paper, we propose a novel Generative Calibration Clustering (GCC) method to delicately incorporate feature learning and augmentation into clustering procedure.","First, we develop a discriminative feature alignment mechanism to discover intrinsic relationship across real and generated samples.","Second, we design a self-supervised metric learning to generate more reliable cluster assignment to boost the conditional diffusion generation.","Extensive experimental results on three benchmarks validate the effectiveness and advantage of our proposed method over the state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.09115v1","category":"cs.CV"}
{"created":"2024-04-14 00:20:50","title":"Power law coupling Higgs-Palatini inflation with a congruence between physical and geometrical symmetries","abstract":"In this paper we investigate a power law coupling Higgs inflationary model in which the background geometry is determined by the Palatini's variational principle. The geometrical symmetries of the background geometry determine the invariant form of the action of the model and the background geometry resulted is of the Weyl-integrable type. The invariant action results also invariant under the $U(1)$ group, which in general is not compatible with the Weyl group of invariance of the background geometry. However, we found compatibility conditions between the geometrical and physical symmetries of the action in the strong coupling limit. We found that if we start with a non-minimally coupled to gravity action, when we impose the congruence between the both groups of symmetries we end with an invariant action of the scalar-tensor type. We obtain a nearly scale invariant power spectrum for the inflaton fluctuations for certain values of some parameters of the model. Also we obtain va\\-lues for the tensor to scalar ratio in agreement with PLANCK and BICEP observational data: $r<0.032$.","sentences":["In this paper we investigate a power law coupling Higgs inflationary model in which the background geometry is determined by the Palatini's variational principle.","The geometrical symmetries of the background geometry determine the invariant form of the action of the model and the background geometry resulted is of the Weyl-integrable type.","The invariant action results also invariant under the $U(1)$ group, which in general is not compatible with the Weyl group of invariance of the background geometry.","However, we found compatibility conditions between the geometrical and physical symmetries of the action in the strong coupling limit.","We found that if we start with a non-minimally coupled to gravity action, when we impose the congruence between the both groups of symmetries we end with an invariant action of the scalar-tensor type.","We obtain a nearly scale invariant power spectrum for the inflaton fluctuations for certain values of some parameters of the model.","Also we obtain va\\-lues for the tensor to scalar ratio in agreement with PLANCK and BICEP observational data: $r<0.032$."],"url":"http://arxiv.org/abs/2404.09107v1","category":"gr-qc"}
{"created":"2024-04-13 22:34:48","title":"Gophy: Novel Proof-of-Useful-Work blockchain architecture for High Energy Physics","abstract":"In this publication, a novel architecture for Proof-of-Useful-Work blockchain consensus which aims to replace hash-based block problems with Monte Carlo simulation-based block problems to donate computational power to real-world HEP experiments is described. Design decisions are detailed and challenges are addressed. The architecture is being implemented using Golang and can be run inside the CbmRoot software environment. The goal is to build a bridge between the disciplines HEP and blockchain to build a novel blockchain network in which the network's computational power is not wasted but instead used to support a scientific experiment while at the same time securing the underlying permissioned blockchain. The blockchain features a token-based cryptocurrency that is rewarded to miners that donate computational power and acts as an additional incentive to participate which traditional volunteer computing can not provide. The implementation named gophy is being implemented in Golang and is expected to be open-sourced before the end of 2024.","sentences":["In this publication, a novel architecture for Proof-of-Useful-Work blockchain consensus which aims to replace hash-based block problems with Monte Carlo simulation-based block problems to donate computational power to real-world HEP experiments is described.","Design decisions are detailed and challenges are addressed.","The architecture is being implemented using Golang and can be run inside the CbmRoot software environment.","The goal is to build a bridge between the disciplines HEP and blockchain to build a novel blockchain network in which the network's computational power is not wasted but instead used to support a scientific experiment while at the same time securing the underlying permissioned blockchain.","The blockchain features a token-based cryptocurrency that is rewarded to miners that donate computational power and acts as an additional incentive to participate which traditional volunteer computing can not provide.","The implementation named gophy is being implemented in Golang and is expected to be open-sourced before the end of 2024."],"url":"http://arxiv.org/abs/2404.09093v1","category":"cs.CR"}
{"created":"2024-04-13 22:09:27","title":"DEX Specs: A Mean Field Approach to DeFi Currency Exchanges","abstract":"We investigate the behavior of liquidity providers (LPs) by modeling a decentralized cryptocurrency exchange (DEX) based on Uniswap v3. LPs with heterogeneous characteristics choose optimal liquidity positions subject to uncertainty regarding the size of exogenous incoming transactions and the prices of assets in the wider market. They engage in a game among themselves, and the resulting liquidity distribution determines the exchange rate dynamics and potential arbitrage opportunities of the pool. We calibrate the distribution of LP characteristics based on Uniswap data and the equilibrium strategy resulting from this mean-field game produces pool exchange rate dynamics and liquidity evolution consistent with observed pool behavior. We subsequently introduce Maximal Extractable Value (MEV) bots who perform Just-In-Time (JIT) liquidity attacks, and develop a Stackelberg game between LPs and bots. This addition results in more accurate simulated pool exchange rate dynamics and stronger predictive power regarding the evolution of the pool liquidity distribution.","sentences":["We investigate the behavior of liquidity providers (LPs) by modeling a decentralized cryptocurrency exchange (DEX) based on Uniswap v3.","LPs with heterogeneous characteristics choose optimal liquidity positions subject to uncertainty regarding the size of exogenous incoming transactions and the prices of assets in the wider market.","They engage in a game among themselves, and the resulting liquidity distribution determines the exchange rate dynamics and potential arbitrage opportunities of the pool.","We calibrate the distribution of LP characteristics based on Uniswap data and the equilibrium strategy resulting from this mean-field game produces pool exchange rate dynamics and liquidity evolution consistent with observed pool behavior.","We subsequently introduce Maximal Extractable Value (MEV) bots who perform Just-In-Time (JIT) liquidity attacks, and develop a Stackelberg game between LPs and bots.","This addition results in more accurate simulated pool exchange rate dynamics and stronger predictive power regarding the evolution of the pool liquidity distribution."],"url":"http://arxiv.org/abs/2404.09090v1","category":"q-fin.TR"}
{"created":"2024-04-13 22:02:33","title":"A biological circuit to anticipate trend","abstract":"Organisms gain by anticipating future changes in the environment. Those environmental changes often follow stochastic trends. The greater the slope of the trend, the more likely the trend's momentum carries the future trend in the same direction. This article presents a simple biological circuit that measures the momentum, providing a prediction about future trend. The circuit calculates the momentum by the difference between a short-term and a long-term exponential moving average. The time lengths of the two moving averages can be adjusted by changing the decay rates of state variables. Different time lengths for those averages trade off between errors caused by noise and errors caused by lags in predicting a change in the direction of the trend. Prior studies have emphasized circuits that make similar calculations about trends. However, those prior studies embedded their analyses in the details of particular applications, obscuring the simple generality and wide applicability of the approach. The model here contributes to the topic by clarifying the great simplicity and generality of anticipation for stochastic trends. This article also notes that, in financial analysis, the difference between moving averages is widely used to predict future trends in asset prices. The financial measure is called the moving average convergence-divergence (MACD) indicator. Connecting the biological problem to financial analysis opens the way for future studies in biology to exploit the variety of highly developed trend models in finance.","sentences":["Organisms gain by anticipating future changes in the environment.","Those environmental changes often follow stochastic trends.","The greater the slope of the trend, the more likely the trend's momentum carries the future trend in the same direction.","This article presents a simple biological circuit that measures the momentum, providing a prediction about future trend.","The circuit calculates the momentum by the difference between a short-term and a long-term exponential moving average.","The time lengths of the two moving averages can be adjusted by changing the decay rates of state variables.","Different time lengths for those averages trade off between errors caused by noise and errors caused by lags in predicting a change in the direction of the trend.","Prior studies have emphasized circuits that make similar calculations about trends.","However, those prior studies embedded their analyses in the details of particular applications, obscuring the simple generality and wide applicability of the approach.","The model here contributes to the topic by clarifying the great simplicity and generality of anticipation for stochastic trends.","This article also notes that, in financial analysis, the difference between moving averages is widely used to predict future trends in asset prices.","The financial measure is called the moving average convergence-divergence (MACD) indicator.","Connecting the biological problem to financial analysis opens the way for future studies in biology to exploit the variety of highly developed trend models in finance."],"url":"http://arxiv.org/abs/2404.09089v1","category":"q-bio.PE"}
{"created":"2024-04-13 20:55:15","title":"Safe Reinforcement Learning on the Constraint Manifold: Theory and Applications","abstract":"Integrating learning-based techniques, especially reinforcement learning, into robotics is promising for solving complex problems in unstructured environments. However, most existing approaches are trained in well-tuned simulators and subsequently deployed on real robots without online fine-tuning. In this setting, the simulation's realism seriously impacts the deployment's success rate. Instead, learning with real-world interaction data offers a promising alternative: not only eliminates the need for a fine-tuned simulator but also applies to a broader range of tasks where accurate modeling is unfeasible. One major problem for on-robot reinforcement learning is ensuring safety, as uncontrolled exploration can cause catastrophic damage to the robot or the environment. Indeed, safety specifications, often represented as constraints, can be complex and non-linear, making safety challenging to guarantee in learning systems. In this paper, we show how we can impose complex safety constraints on learning-based robotics systems in a principled manner, both from theoretical and practical points of view. Our approach is based on the concept of the Constraint Manifold, representing the set of safe robot configurations. Exploiting differential geometry techniques, i.e., the tangent space, we can construct a safe action space, allowing learning agents to sample arbitrary actions while ensuring safety. We demonstrate the method's effectiveness in a real-world Robot Air Hockey task, showing that our method can handle high-dimensional tasks with complex constraints. Videos of the real robot experiments are available on the project website (https://puzeliu.github.io/TRO-ATACOM).","sentences":["Integrating learning-based techniques, especially reinforcement learning, into robotics is promising for solving complex problems in unstructured environments.","However, most existing approaches are trained in well-tuned simulators and subsequently deployed on real robots without online fine-tuning.","In this setting, the simulation's realism seriously impacts the deployment's success rate.","Instead, learning with real-world interaction data offers a promising alternative: not only eliminates the need for a fine-tuned simulator but also applies to a broader range of tasks where accurate modeling is unfeasible.","One major problem for on-robot reinforcement learning is ensuring safety, as uncontrolled exploration can cause catastrophic damage to the robot or the environment.","Indeed, safety specifications, often represented as constraints, can be complex and non-linear, making safety challenging to guarantee in learning systems.","In this paper, we show how we can impose complex safety constraints on learning-based robotics systems in a principled manner, both from theoretical and practical points of view.","Our approach is based on the concept of the Constraint Manifold, representing the set of safe robot configurations.","Exploiting differential geometry techniques, i.e., the tangent space, we can construct a safe action space, allowing learning agents to sample arbitrary actions while ensuring safety.","We demonstrate the method's effectiveness in a real-world Robot Air Hockey task, showing that our method can handle high-dimensional tasks with complex constraints.","Videos of the real robot experiments are available on the project website (https://puzeliu.github.io/TRO-ATACOM)."],"url":"http://arxiv.org/abs/2404.09080v1","category":"cs.RO"}
{"created":"2024-04-13 20:38:14","title":"Moving impedance profiles make one--way, spectrum--reshaping mirrors","abstract":"We find a new set of exact solutions to Maxwell's equations in space--time varying materials, where the refractive index is constant, while the impedance exhibits effective motion, i.e. it is a function of $x-vt$. We find that waves co--propagating with the modulation are not reflected within the material, while counter--propagating waves are continually reflected by the changing impedance. For a finite section of such a material we find analogues of transmission resonances, where specially shaped `eigenpulses' enter without reflection. We also find that there is a strong asymmetry in reflection from the medium when the impedance modulation is small but rapid, the material reflecting strongly from one side, and negligibly from the other. Unlike stationary media, the spectrum of the reflected wave can be significantly different from the incident one.","sentences":["We find a new set of exact solutions to Maxwell's equations in space--time varying materials, where the refractive index is constant, while the impedance exhibits effective motion, i.e. it is a function of $x-vt$.","We find that waves co--propagating with the modulation are not reflected within the material, while counter--propagating waves are continually reflected by the changing impedance.","For a finite section of such a material we find analogues of transmission resonances, where specially shaped `eigenpulses' enter without reflection.","We also find that there is a strong asymmetry in reflection from the medium when the impedance modulation is small but rapid, the material reflecting strongly from one side, and negligibly from the other.","Unlike stationary media, the spectrum of the reflected wave can be significantly different from the incident one."],"url":"http://arxiv.org/abs/2404.09075v1","category":"physics.optics"}
{"created":"2024-04-13 20:29:42","title":"Data-driven turbulence modeling","abstract":"This chapter provides an introduction to data-driven techniques for the development and calibration of closure models for the Reynolds-Averaged Navier--Stokes (RANS) equations. RANS models are the workhorse for engineering applications of computational fluid dynamics (CFD) and are expected to play an important role for decades to come. However, RANS model inadequacies for complex, non-equilibrium flows and uncertainties in modeling assumptions and calibration data are still a major obstacle to the predictive capability of RANS simulations. In the following, we briefly recall the origin and limitations of RANS models, and then review their shortcomings and uncertainties. Then, we provide an introduction to data-driven approaches to RANS turbulence modeling. The latter can range from simple model parameter inference to sophisticated machine learning techniques. We conclude with some perspectives on current and future research trends.","sentences":["This chapter provides an introduction to data-driven techniques for the development and calibration of closure models for the Reynolds-Averaged Navier--Stokes (RANS) equations.","RANS models are the workhorse for engineering applications of computational fluid dynamics (CFD) and are expected to play an important role for decades to come.","However, RANS model inadequacies for complex, non-equilibrium flows and uncertainties in modeling assumptions and calibration data are still a major obstacle to the predictive capability of RANS simulations.","In the following, we briefly recall the origin and limitations of RANS models, and then review their shortcomings and uncertainties.","Then, we provide an introduction to data-driven approaches to RANS turbulence modeling.","The latter can range from simple model parameter inference to sophisticated machine learning techniques.","We conclude with some perspectives on current and future research trends."],"url":"http://arxiv.org/abs/2404.09074v1","category":"physics.flu-dyn"}
{"created":"2024-04-13 19:30:58","title":"CodeCloak: A Method for Evaluating and Mitigating Code Leakage by LLM Code Assistants","abstract":"LLM-based code assistants are becoming increasingly popular among developers. These tools help developers improve their coding efficiency and reduce errors by providing real-time suggestions based on the developer's codebase. While beneficial, these tools might inadvertently expose the developer's proprietary code to the code assistant service provider during the development process. In this work, we propose two complementary methods to mitigate the risk of code leakage when using LLM-based code assistants. The first is a technique for reconstructing a developer's original codebase from code segments sent to the code assistant service (i.e., prompts) during the development process, enabling assessment and evaluation of the extent of code leakage to third parties (or adversaries). The second is CodeCloak, a novel deep reinforcement learning agent that manipulates the prompts before sending them to the code assistant service. CodeCloak aims to achieve the following two contradictory goals: (i) minimizing code leakage, while (ii) preserving relevant and useful suggestions for the developer. Our evaluation, employing GitHub Copilot, StarCoder, and CodeLlama LLM-based code assistants models, demonstrates the effectiveness of our CodeCloak approach on a diverse set of code repositories of varying sizes, as well as its transferability across different models. In addition, we generate a realistic simulated coding environment to thoroughly analyze code leakage risks and evaluate the effectiveness of our proposed mitigation techniques under practical development scenarios.","sentences":["LLM-based code assistants are becoming increasingly popular among developers.","These tools help developers improve their coding efficiency and reduce errors by providing real-time suggestions based on the developer's codebase.","While beneficial, these tools might inadvertently expose the developer's proprietary code to the code assistant service provider during the development process.","In this work, we propose two complementary methods to mitigate the risk of code leakage when using LLM-based code assistants.","The first is a technique for reconstructing a developer's original codebase from code segments sent to the code assistant service (i.e., prompts) during the development process, enabling assessment and evaluation of the extent of code leakage to third parties (or adversaries).","The second is CodeCloak, a novel deep reinforcement learning agent that manipulates the prompts before sending them to the code assistant service.","CodeCloak aims to achieve the following two contradictory goals: (i) minimizing code leakage, while (ii) preserving relevant and useful suggestions for the developer.","Our evaluation, employing GitHub Copilot, StarCoder, and CodeLlama LLM-based code assistants models, demonstrates the effectiveness of our CodeCloak approach on a diverse set of code repositories of varying sizes, as well as its transferability across different models.","In addition, we generate a realistic simulated coding environment to thoroughly analyze code leakage risks and evaluate the effectiveness of our proposed mitigation techniques under practical development scenarios."],"url":"http://arxiv.org/abs/2404.09066v1","category":"cs.CR"}
{"created":"2024-04-13 19:28:24","title":"VRPD-DT: Vehicle Routing Problem with Drones Under Dynamically Changing Traffic Conditions","abstract":"The vehicle routing problem with drones (VRP-D) is to determine the optimal routes of trucks and drones such that the total operational cost is minimized in a scenario where the trucks work in tandem with the drones to deliver parcels to customers. While various heuristic algorithms have been developed to address the problem, existing solutions are built based on simplistic cost models, overlooking the temporal dynamics of the costs, which fluctuate depending on the dynamically changing traffic conditions. In this paper, we present a novel problem called the vehicle routing problem with drones under dynamically changing traffic conditions (VRPD-DT) to address the limitation of existing VRP-D solutions. We design a novel cost model that factors in the actual travel distance and projected travel time, computed using a machine learning-driven travel time prediction algorithm. A variable neighborhood descent (VND) algorithm is developed to find the optimal truck-drone routes under the dynamics of traffic conditions through incorporation of the travel time prediction model. A simulation study was performed to evaluate the performance compared with a state-of-the-art VRP-D heuristic solution. The results demonstrate that the proposed algorithm outperforms the state-of-the-art algorithm in various delivery scenarios.","sentences":["The vehicle routing problem with drones (VRP-D) is to determine the optimal routes of trucks and drones such that the total operational cost is minimized in a scenario where the trucks work in tandem with the drones to deliver parcels to customers.","While various heuristic algorithms have been developed to address the problem, existing solutions are built based on simplistic cost models, overlooking the temporal dynamics of the costs, which fluctuate depending on the dynamically changing traffic conditions.","In this paper, we present a novel problem called the vehicle routing problem with drones under dynamically changing traffic conditions (VRPD-DT) to address the limitation of existing VRP-D solutions.","We design a novel cost model that factors in the actual travel distance and projected travel time, computed using a machine learning-driven travel time prediction algorithm.","A variable neighborhood descent (VND) algorithm is developed to find the optimal truck-drone routes under the dynamics of traffic conditions through incorporation of the travel time prediction model.","A simulation study was performed to evaluate the performance compared with a state-of-the-art VRP-D heuristic solution.","The results demonstrate that the proposed algorithm outperforms the state-of-the-art algorithm in various delivery scenarios."],"url":"http://arxiv.org/abs/2404.09065v1","category":"cs.CY"}
{"created":"2024-04-13 18:43:59","title":"Prevalence estimation methods for time-dependent antibody kinetics of infected and vaccinated individuals: a graph-theoretic approach","abstract":"Immune events such as infection, vaccination, and a combination of the two result in distinct time-dependent antibody responses in affected individuals. These responses and event prevalences combine non-trivially to govern antibody levels sampled from a population. Time-dependence and disease prevalence pose considerable modeling challenges that need to be addressed to provide a rigorous mathematical underpinning of the underlying biology. We propose a time-inhomogeneous Markov chain model for event-to-event transitions coupled with a probabilistic framework for anti-body kinetics and demonstrate its use in a setting in which individuals can be infected or vaccinated but not both. We prove the equivalency of this approach to the framework developed in our previous work. Synthetic data are used to demonstrate the modeling process and conduct prevalence estimation via transition probability matrices. This approach is ideal to model sequences of infections and vaccinations, or personal trajectories in a population, making it an important first step towards a mathematical characterization of reinfection, vaccination boosting, and cross-events of infection after vaccination or vice versa.","sentences":["Immune events such as infection, vaccination, and a combination of the two result in distinct time-dependent antibody responses in affected individuals.","These responses and event prevalences combine non-trivially to govern antibody levels sampled from a population.","Time-dependence and disease prevalence pose considerable modeling challenges that need to be addressed to provide a rigorous mathematical underpinning of the underlying biology.","We propose a time-inhomogeneous Markov chain model for event-to-event transitions coupled with a probabilistic framework for anti-body kinetics and demonstrate its use in a setting in which individuals can be infected or vaccinated but not both.","We prove the equivalency of this approach to the framework developed in our previous work.","Synthetic data are used to demonstrate the modeling process and conduct prevalence estimation via transition probability matrices.","This approach is ideal to model sequences of infections and vaccinations, or personal trajectories in a population, making it an important first step towards a mathematical characterization of reinfection, vaccination boosting, and cross-events of infection after vaccination or vice versa."],"url":"http://arxiv.org/abs/2404.09059v1","category":"q-bio.PE"}
{"created":"2024-04-13 17:27:47","title":"Quantum deformed phantom dynamics in light of the generalized uncertainty principle","abstract":"Quantum gravity has been baffling the theoretical physicist for decades now: both for its mathematical obscurity and phenomenological testing. Nevertheless, the new era of precision cosmology presents a promising avenue to test the effects of quantum gravity. In this study, we consider a bottom-up approach. Without resorting to any candidate quantum gravity, we invoke a generalized uncertainty principle (GUP) directly into the cosmological Hamiltonian for a universe sourced by a phantom scalar field with potential to study the early epoch of the evolution. This is followed by a systematic analysis of the dynamics, both qualitatively and quantitatively. Our qualitative analysis shows that the introduction of GUP significantly alters the existence of fixed points for the potential considered in this contribution. In addition, we confirm the existence of an inflationary epoch and analyze the behavior of relevant cosmological parameters with respect to the strength of GUP distortion.","sentences":["Quantum gravity has been baffling the theoretical physicist for decades now: both for its mathematical obscurity and phenomenological testing.","Nevertheless, the new era of precision cosmology presents a promising avenue to test the effects of quantum gravity.","In this study, we consider a bottom-up approach.","Without resorting to any candidate quantum gravity, we invoke a generalized uncertainty principle (GUP) directly into the cosmological Hamiltonian for a universe sourced by a phantom scalar field with potential to study the early epoch of the evolution.","This is followed by a systematic analysis of the dynamics, both qualitatively and quantitatively.","Our qualitative analysis shows that the introduction of GUP significantly alters the existence of fixed points for the potential considered in this contribution.","In addition, we confirm the existence of an inflationary epoch and analyze the behavior of relevant cosmological parameters with respect to the strength of GUP distortion."],"url":"http://arxiv.org/abs/2404.09049v1","category":"gr-qc"}
{"created":"2024-04-13 17:20:00","title":"Adaptive User-Centric Entanglement Routing in Quantum Data Networks","abstract":"Distributed quantum computing (DQC) holds immense promise in harnessing the potential of quantum computing by interconnecting multiple small quantum computers (QCs) through a quantum data network (QDN). Establishing long-distance quantum entanglement between two QCs for quantum teleportation within the QDN is a critical aspect, and it involves entanglement routing - finding a route between QCs and efficiently allocating qubits along that route. Existing approaches have mainly focused on optimizing entanglement performance for current entanglement connection (EC) requests. However, they often overlook the user's perspective, wherein the user making EC requests operates under a budget constraint over an extended period. Furthermore, both QDN resources (quantum channels and qubits) and the EC requests, reflecting the DQC workload, vary over time. In this paper, we present a novel user-centric entanglement routing problem that spans an extended period to maximize the entanglement success rate while adhering to the user's budget constraint. To address this challenge, we leverage the Lyapunov drift-plus-penalty framework to decompose the long-term optimization problem into per-slot problems, allowing us to find solutions using only the current system information. Subsequently, we develop efficient algorithms based on continuous-relaxation and Gibbs-sampling techniques to solve the per-slot entanglement routing problem. Theoretical performance guarantees are provided for both the per-slot and long-term problems. Extensive simulations demonstrate that our algorithm significantly outperforms baseline approaches in terms of entanglement success rate and budget adherence.","sentences":["Distributed quantum computing (DQC) holds immense promise in harnessing the potential of quantum computing by interconnecting multiple small quantum computers (QCs) through a quantum data network (QDN).","Establishing long-distance quantum entanglement between two QCs for quantum teleportation within the QDN is a critical aspect, and it involves entanglement routing - finding a route between QCs and efficiently allocating qubits along that route.","Existing approaches have mainly focused on optimizing entanglement performance for current entanglement connection (EC) requests.","However, they often overlook the user's perspective, wherein the user making EC requests operates under a budget constraint over an extended period.","Furthermore, both QDN resources (quantum channels and qubits) and the EC requests, reflecting the DQC workload, vary over time.","In this paper, we present a novel user-centric entanglement routing problem that spans an extended period to maximize the entanglement success rate while adhering to the user's budget constraint.","To address this challenge, we leverage the Lyapunov drift-plus-penalty framework to decompose the long-term optimization problem into per-slot problems, allowing us to find solutions using only the current system information.","Subsequently, we develop efficient algorithms based on continuous-relaxation and Gibbs-sampling techniques to solve the per-slot entanglement routing problem.","Theoretical performance guarantees are provided for both the per-slot and long-term problems.","Extensive simulations demonstrate that our algorithm significantly outperforms baseline approaches in terms of entanglement success rate and budget adherence."],"url":"http://arxiv.org/abs/2404.09048v1","category":"quant-ph"}
{"created":"2024-04-13 16:59:28","title":"Do LLMs Play Dice? Exploring Probability Distribution Sampling in Large Language Models for Behavioral Simulation","abstract":"With the rapid advancement of large language models (LLMs) and their remarkable capabilities in handling complex language tasks, an increasing number of studies are employing LLMs as agents to emulate the sequential decision-making processes of humans often represented as Markov decision-making processes (MDPs). The actions within this decision-making framework adhere to specific probability distributions and require iterative sampling. This arouses our curiosity regarding the capacity of LLM agents to comprehend probability distributions, thereby guiding the agent's behavioral decision-making through probabilistic sampling and generating behavioral sequences. To answer the above question, we divide the problem into two main aspects: simulation where the exact probability distribution is known, and generation of sequences where the probability distribution is ambiguous. In the first case, the agent is required to give the type and parameters of the probability distribution through the problem description, and then give the sampling sequence. However, our analysis shows that LLM agents perform poorly in this case, but the sampling success rate can be improved through programming tools. Real-world scenarios often entail unknown probability distributions. Thus, in the second case, we ask the agents to change the activity level in online social networks and analyze the frequency of actions. Ultimately, our analysis shows that LLM agents cannot sample probability distributions even using programming tools. Therefore, careful consideration is still required before directly applying LLM agents as agents to simulate human behavior.","sentences":["With the rapid advancement of large language models (LLMs) and their remarkable capabilities in handling complex language tasks, an increasing number of studies are employing LLMs as agents to emulate the sequential decision-making processes of humans often represented as Markov decision-making processes (MDPs).","The actions within this decision-making framework adhere to specific probability distributions and require iterative sampling.","This arouses our curiosity regarding the capacity of LLM agents to comprehend probability distributions, thereby guiding the agent's behavioral decision-making through probabilistic sampling and generating behavioral sequences.","To answer the above question, we divide the problem into two main aspects: simulation where the exact probability distribution is known, and generation of sequences where the probability distribution is ambiguous.","In the first case, the agent is required to give the type and parameters of the probability distribution through the problem description, and then give the sampling sequence.","However, our analysis shows that LLM agents perform poorly in this case, but the sampling success rate can be improved through programming tools.","Real-world scenarios often entail unknown probability distributions.","Thus, in the second case, we ask the agents to change the activity level in online social networks and analyze the frequency of actions.","Ultimately, our analysis shows that LLM agents cannot sample probability distributions even using programming tools.","Therefore, careful consideration is still required before directly applying LLM agents as agents to simulate human behavior."],"url":"http://arxiv.org/abs/2404.09043v1","category":"cs.CL"}
{"created":"2024-04-13 14:07:47","title":"On the universal validity of Case B recombination theory","abstract":"In an ongoing search for low-mass extreme emission line galaxies, we identified a galaxy with a Ha/Hb Balmer line ratio of 2.620 +- 0.078. Ha/Hb Balmer ratios lower than the dust-free Case~B value appear relatively frequently in extreme emission line galaxies. These low values suggest that the Case~B assumption may not be valid in these objects. After ruling out the possibility that the low Ha/Hb ratio is due to systematic errors introduced by observational effects, we use constraints from the total Hb luminosity, the [OIII]/[OII] line ratio and the Balmer line equivalent widths, to suggest that the gas is optically thick to both Ha and Lya photons, and the geometry and orientation of the scattering gas causes Ha photons to be preferentially removed from the line of sight with respect to higher order Balmer series photons. Finally, we use data from the SDSS survey to show that Balmer self-absorption may be more important than previously assumed in high excitation emission line galaxies, where Lya pumping of the hydrogen excited state can be effective. If not recognized, Balmer self-absorption could lead to inaccurate estimates of galaxy physical properties. As an example, the effect of dust extinction could be over-estimated, for spherically symmetric scattering medium, or under-estimated, for a not spherically-symmetric distribution.","sentences":["In an ongoing search for low-mass extreme emission line galaxies, we identified a galaxy with a Ha/Hb Balmer line ratio of 2.620 +- 0.078.","Ha/Hb Balmer ratios lower than the dust-free Case~B value appear relatively frequently in extreme emission line galaxies.","These low values suggest that the Case~B assumption may not be valid in these objects.","After ruling out the possibility that the low Ha/Hb ratio is due to systematic errors introduced by observational effects, we use constraints from the total Hb luminosity, the [OIII]/[OII] line ratio and the Balmer line equivalent widths, to suggest that the gas is optically thick to both Ha and Lya photons, and the geometry and orientation of the scattering gas causes Ha photons to be preferentially removed from the line of sight with respect to higher order Balmer series photons.","Finally, we use data from the SDSS survey to show that Balmer self-absorption may be more important than previously assumed in high excitation emission line galaxies, where Lya pumping of the hydrogen excited state can be effective.","If not recognized, Balmer self-absorption could lead to inaccurate estimates of galaxy physical properties.","As an example, the effect of dust extinction could be over-estimated, for spherically symmetric scattering medium, or under-estimated, for a not spherically-symmetric distribution."],"url":"http://arxiv.org/abs/2404.09015v1","category":"astro-ph.GA"}
{"created":"2024-04-13 13:07:32","title":"WikiSplit++: Easy Data Refinement for Split and Rephrase","abstract":"The task of Split and Rephrase, which splits a complex sentence into multiple simple sentences with the same meaning, improves readability and enhances the performance of downstream tasks in natural language processing (NLP). However, while Split and Rephrase can be improved using a text-to-text generation approach that applies encoder-decoder models fine-tuned with a large-scale dataset, it still suffers from hallucinations and under-splitting. To address these issues, this paper presents a simple and strong data refinement approach. Here, we create WikiSplit++ by removing instances in WikiSplit where complex sentences do not entail at least one of the simpler sentences and reversing the order of reference simple sentences. Experimental results show that training with WikiSplit++ leads to better performance than training with WikiSplit, even with fewer training instances. In particular, our approach yields significant gains in the number of splits and the entailment ratio, a proxy for measuring hallucinations.","sentences":["The task of Split and Rephrase, which splits a complex sentence into multiple simple sentences with the same meaning, improves readability and enhances the performance of downstream tasks in natural language processing (NLP).","However, while Split and Rephrase can be improved using a text-to-text generation approach that applies encoder-decoder models fine-tuned with a large-scale dataset, it still suffers from hallucinations and under-splitting.","To address these issues, this paper presents a simple and strong data refinement approach.","Here, we create WikiSplit++ by removing instances in WikiSplit where complex sentences do not entail at least one of the simpler sentences and reversing the order of reference simple sentences.","Experimental results show that training with WikiSplit++ leads to better performance than training with WikiSplit, even with fewer training instances.","In particular, our approach yields significant gains in the number of splits and the entailment ratio, a proxy for measuring hallucinations."],"url":"http://arxiv.org/abs/2404.09002v1","category":"cs.CL"}
{"created":"2024-04-13 13:03:19","title":"MaSkel: A Model for Human Whole-body X-rays Generation from Human Masking Images","abstract":"The human whole-body X-rays could offer a valuable reference for various applications, including medical diagnostics, digital animation modeling, and ergonomic design. The traditional method of obtaining X-ray information requires the use of CT (Computed Tomography) scan machines, which emit potentially harmful radiation. Thus it faces a significant limitation for realistic applications because it lacks adaptability and safety. In our work, We proposed a new method to directly generate the 2D human whole-body X-rays from the human masking images. The predicted images will be similar to the real ones with the same image style and anatomic structure. We employed a data-driven strategy. By leveraging advanced generative techniques, our model MaSkel(Masking image to Skeleton X-rays) could generate a high-quality X-ray image from a human masking image without the need for invasive and harmful radiation exposure, which not only provides a new path to generate highly anatomic and customized data but also reduces health risks. To our knowledge, our model MaSkel is the first work for predicting whole-body X-rays. In this paper, we did two parts of the work. The first one is to solve the data limitation problem, the diffusion-based techniques are utilized to make a data augmentation, which provides two synthetic datasets for preliminary pretraining. Then we designed a two-stage training strategy to train MaSkel. At last, we make qualitative and quantitative evaluations of the generated X-rays. In addition, we invite some professional doctors to assess our predicted data. These evaluations demonstrate the MaSkel's superior ability to generate anatomic X-rays from human masking images. The related code and links of the dataset are available at https://github.com/2022yingjie/MaSkel.","sentences":["The human whole-body X-rays could offer a valuable reference for various applications, including medical diagnostics, digital animation modeling, and ergonomic design.","The traditional method of obtaining X-ray information requires the use of CT (Computed Tomography) scan machines, which emit potentially harmful radiation.","Thus it faces a significant limitation for realistic applications because it lacks adaptability and safety.","In our work, We proposed a new method to directly generate the 2D human whole-body X-rays from the human masking images.","The predicted images will be similar to the real ones with the same image style and anatomic structure.","We employed a data-driven strategy.","By leveraging advanced generative techniques, our model MaSkel(Masking image to Skeleton X-rays) could generate a high-quality X-ray image from a human masking image without the need for invasive and harmful radiation exposure, which not only provides a new path to generate highly anatomic and customized data but also reduces health risks.","To our knowledge, our model MaSkel is the first work for predicting whole-body X-rays.","In this paper, we did two parts of the work.","The first one is to solve the data limitation problem, the diffusion-based techniques are utilized to make a data augmentation, which provides two synthetic datasets for preliminary pretraining.","Then we designed a two-stage training strategy to train MaSkel.","At last, we make qualitative and quantitative evaluations of the generated X-rays.","In addition, we invite some professional doctors to assess our predicted data.","These evaluations demonstrate the MaSkel's superior ability to generate anatomic X-rays from human masking images.","The related code and links of the dataset are available at https://github.com/2022yingjie/MaSkel."],"url":"http://arxiv.org/abs/2404.09000v1","category":"eess.IV"}
{"created":"2024-04-13 12:37:24","title":"A Machine Learning Approach for Optimizing Hybrid Quantum Noise Clusters for Gaussian Quantum Channel Capacity","abstract":"This work contributes to the advancement of quantum communication by visualizing hybrid quantum noise in higher dimensions and optimizing the capacity of the quantum channel by using machine learning (ML). Employing the expectation maximization (EM) algorithm, the quantum channel parameters are iteratively adjusted to estimate the channel capacity, facilitating the categorization of quantum noise data in higher dimensions into a finite number of clusters. In contrast to previous investigations that represented the model in lower dimensions, our work describes the quantum noise as a Gaussian Mixture Model (GMM) with mixing weights derived from a Poisson distribution. The objective was to model the quantum noise using a finite mixture of Gaussian components while preserving the mixing coefficients from the Poisson distribution. Approximating the infinite Gaussian mixture with a finite number of components makes it feasible to visualize clusters of quantum noise data without modifying the original probability density function. By implementing the EM algorithm, the research fine-tuned the channel parameters, identified optimal clusters, improved channel capacity estimation, and offered insights into the characteristics of quantum noise within an ML framework.","sentences":["This work contributes to the advancement of quantum communication by visualizing hybrid quantum noise in higher dimensions and optimizing the capacity of the quantum channel by using machine learning (ML).","Employing the expectation maximization (EM) algorithm, the quantum channel parameters are iteratively adjusted to estimate the channel capacity, facilitating the categorization of quantum noise data in higher dimensions into a finite number of clusters.","In contrast to previous investigations that represented the model in lower dimensions, our work describes the quantum noise as a Gaussian Mixture Model (GMM) with mixing weights derived from a Poisson distribution.","The objective was to model the quantum noise using a finite mixture of Gaussian components while preserving the mixing coefficients from the Poisson distribution.","Approximating the infinite Gaussian mixture with a finite number of components makes it feasible to visualize clusters of quantum noise data without modifying the original probability density function.","By implementing the EM algorithm, the research fine-tuned the channel parameters, identified optimal clusters, improved channel capacity estimation, and offered insights into the characteristics of quantum noise within an ML framework."],"url":"http://arxiv.org/abs/2404.08993v1","category":"eess.SP"}
{"created":"2024-04-13 12:36:41","title":"A water structure indicator suitable for generic contexts: two-liquid behavior at hydration and nanoconfinement conditions and a molecular approach to hydrophobicity and wetting","abstract":"In a recent work we have briefly introduced a new structural index for water that, unlike previous indicators, was devised specifically for generic contexts beyond bulk conditions, making it suitable for hydration and nanoconfinement settings. In this work we shall study this metric in detail, demonstrating its ability to reveal the existence of a fine-tuned interplay between local structure and energetics in liquid water. This molecular principle enables the establishment of an extended hydrogen bond network, while simultaneously allowing for the existence of network defects by compensating for uncoordinated sites. By studying different water models and different temperatures encompassing both the normal liquid and the supercooled regime, this molecular mechanism will be shown to underlie the two-state behavior of bulk water. Additionally, by studying functionalized self-assembled monolayers and diverse graphene-like surfaces, we shall show that this principle is also operative at hydration and nanoconfinement conditions, thus generalizing the validity of the two-liquids scenario of water to these contexts. This approach will allow us to define conditions for wettability, providing an accurate measure of hydrophobicity and a reliable predictor of filling and drying transitions. Hence, it might open the possibility of elucidating the active role of water in broad fields of biophysics and materials science. As a preliminary step, we shall study the hydration structure and hydrophilicity of graphene-like systems (parallel graphene sheets and carbon nanotubes) as a function of the confinement dimensionality. FOR A PROGRAMMING CODE TO IMPLEMENT V4s, PLEASE GO TO: https://github.com/nicolas-loubet/V4S","sentences":["In a recent work we have briefly introduced a new structural index for water that, unlike previous indicators, was devised specifically for generic contexts beyond bulk conditions, making it suitable for hydration and nanoconfinement settings.","In this work we shall study this metric in detail, demonstrating its ability to reveal the existence of a fine-tuned interplay between local structure and energetics in liquid water.","This molecular principle enables the establishment of an extended hydrogen bond network, while simultaneously allowing for the existence of network defects by compensating for uncoordinated sites.","By studying different water models and different temperatures encompassing both the normal liquid and the supercooled regime, this molecular mechanism will be shown to underlie the two-state behavior of bulk water.","Additionally, by studying functionalized self-assembled monolayers and diverse graphene-like surfaces, we shall show that this principle is also operative at hydration and nanoconfinement conditions, thus generalizing the validity of the two-liquids scenario of water to these contexts.","This approach will allow us to define conditions for wettability, providing an accurate measure of hydrophobicity and a reliable predictor of filling and drying transitions.","Hence, it might open the possibility of elucidating the active role of water in broad fields of biophysics and materials science.","As a preliminary step, we shall study the hydration structure and hydrophilicity of graphene-like systems (parallel graphene sheets and carbon nanotubes) as a function of the confinement dimensionality.","FOR A PROGRAMMING CODE TO IMPLEMENT V4s, PLEASE GO TO: https://github.com/nicolas-loubet/V4S"],"url":"http://arxiv.org/abs/2404.08992v1","category":"cond-mat.soft"}
{"created":"2024-04-13 12:28:30","title":"High order homoclinic tangencies of corank 2","abstract":"We prove that in the space of $C^r$ maps $(r=2,\\ldots,\\infty,\\omega)$ of a smooth manifold of dimension at least 4 there exist open regions where maps with infinitely many corank-2 homoclinic tangencies of all orders are dense. The result is applied to show the existence of maps with universal two-dimensional dynamics, i.e. maps whose iterations approximate the dynamics of every map of a two-dimensional disk with an arbitrarily good accuracy. We show that maps with universal two-dimensional dynamics are $C^r$-generic in the regions under consideration.","sentences":["We prove that in the space of $C^r$ maps $(r=2,\\ldots,\\infty,\\omega)$ of a smooth manifold of dimension at least 4 there exist open regions where maps with infinitely many corank-2 homoclinic tangencies of all orders are dense.","The result is applied to show the existence of maps with universal two-dimensional dynamics, i.e. maps whose iterations approximate the dynamics of every map of a two-dimensional disk with an arbitrarily good accuracy.","We show that maps with universal two-dimensional dynamics are $C^r$-generic in the regions under consideration."],"url":"http://arxiv.org/abs/2404.08989v1","category":"math.DS"}
{"created":"2024-04-13 12:21:21","title":"Ferroelectrovalley in Two-Dimensional Multiferroic Lattices","abstract":"Engineering valley index is essential and highly sought for valley physics, but currently it is exclusively based on the paradigm of the challenging ferrovalley with spin-orientation reversal under magnetic field. Here, an alternative strategy, i.e., the so-called ferroelectrovalley, is proposed to tackle the insurmountable spin-orientation reversal, which reveres valley index with the feasible ferroelectricity. Using symmetry arguments and tight-binding model, the C_2 rotation is unveiled to be able to take the place of time reversal for operating valley index in two-dimensional multiferroic kagome lattices, which enables the ferroelectricity-engineered valley index, thereby generating the concept of ferroelectrovalley. Based on first-principles calculations, this concept is further demonstrated in the breathing kagome lattice of single-layer Ti3Br8, wherein ferroelectricity couples the breathing process. These findings open a new direction for valleytronics and two-dimensional materials research.","sentences":["Engineering valley index is essential and highly sought for valley physics, but currently it is exclusively based on the paradigm of the challenging ferrovalley with spin-orientation reversal under magnetic field.","Here, an alternative strategy, i.e., the so-called ferroelectrovalley, is proposed to tackle the insurmountable spin-orientation reversal, which reveres valley index with the feasible ferroelectricity.","Using symmetry arguments and tight-binding model, the C_2 rotation is unveiled to be able to take the place of time reversal for operating valley index in two-dimensional multiferroic kagome lattices, which enables the ferroelectricity-engineered valley index, thereby generating the concept of ferroelectrovalley.","Based on first-principles calculations, this concept is further demonstrated in the breathing kagome lattice of single-layer Ti3Br8, wherein ferroelectricity couples the breathing process.","These findings open a new direction for valleytronics and two-dimensional materials research."],"url":"http://arxiv.org/abs/2404.08988v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-13 12:13:54","title":"Long run consequence of p-hacking","abstract":"We study the theoretical consequence of p-hacking on the accumulation of knowledge under the framework of mis-specified Bayesian learning. A sequence of researchers, in turn, choose projects that generate noisy information in a field. In choosing projects, researchers need to carefully balance as projects generates big information are less likely to succeed. In doing the project, a researcher p-hacks at intensity $\\varepsilon$ so that the success probability of a chosen project increases (unduly) by a constant $\\varepsilon$. In interpreting previous results, researcher behaves as if there is no p-hacking because the intensity $\\varepsilon$ is unknown and presumably small. We show that over-incentivizing information provision leads to the failure of learning as long as $\\varepsilon\\neq 0$. If the incentives of information provision is properly provided, learning is correct almost surely as long as $\\varepsilon$ is small.","sentences":["We study the theoretical consequence of p-hacking on the accumulation of knowledge under the framework of mis-specified Bayesian learning.","A sequence of researchers, in turn, choose projects that generate noisy information in a field.","In choosing projects, researchers need to carefully balance as projects generates big information are less likely to succeed.","In doing the project, a researcher p-hacks at intensity $\\varepsilon$ so that the success probability of a chosen project increases (unduly) by a constant $\\varepsilon$. In interpreting previous results, researcher behaves as if there is no p-hacking because the intensity $\\varepsilon$ is unknown and presumably small.","We show that over-incentivizing information provision leads to the failure of learning as long as $\\varepsilon\\neq 0$.","If the incentives of information provision is properly provided, learning is correct almost surely as long as $\\varepsilon$ is small."],"url":"http://arxiv.org/abs/2404.08984v1","category":"econ.TH"}
{"created":"2024-04-13 11:48:14","title":"Uncovering the first-infall history of the LMC through its dynamical impact in the Milky Way halo","abstract":"The gravitational interactions between the LMC and the Milky Way can give rise to dynamical perturbations in the MW halo, leading to a biased distribution of stellar density and other kinematic signals. These disequilibrium phenomena exhibit variations under different parameter combinations of the MW-LMC model. In this work, we run 50 high-resolution N-body simulations spanning different masses and halo shapes of the Milky Way and LMC and investigate how the LMC-induced perturbations evolve with these model parameters. We measure the magnitude of kinematic perturbations from the mean velocities of simulated halo stars and identify a discontinuity between the first-infall and second-passage scenarios of the LMC's orbital history. We demonstrate that, due to the short dynamical times of the Galactic inner halo, the reduced perturbation magnitude in the second-passage scenario is mainly a result of the LMC's second infall into the MW, which starts at a much lower velocity relative to the inner halo compared to the first-infall scenario. Using a subset of $\\sim 1200$ RR Lyrae stars located in the outer halo ($50 \\leq R_{\\mathrm{GC}} < 100$ kpc), which are selected from a larger sample of 135,873 RR Lyrae stars with precise distance estimates from Gaia, we find the mean latitudinal velocity ($v_{b}$) in the heliocentric frame to be $\\langle v_{b} \\rangle = 30.8 \\pm 4.0$ km/s. The observation contradicts the second-passage scenario and supports the first-infall scenario with a massive LMC ($\\sim 2.1 \\times 10^{11} \\mathrm{M}_{\\odot}$) at infall, an oblate MW halo with a virial mass $M_{200} < 1.4 \\times 10^{12} \\mathrm{M}_{\\odot}$ and a flattening parameter $q > 0.7$.","sentences":["The gravitational interactions between the LMC and the Milky Way can give rise to dynamical perturbations in the MW halo, leading to a biased distribution of stellar density and other kinematic signals.","These disequilibrium phenomena exhibit variations under different parameter combinations of the MW-LMC model.","In this work, we run 50 high-resolution N-body simulations spanning different masses and halo shapes of the Milky Way and LMC and investigate how the LMC-induced perturbations evolve with these model parameters.","We measure the magnitude of kinematic perturbations from the mean velocities of simulated halo stars and identify a discontinuity between the first-infall and second-passage scenarios of the LMC's orbital history.","We demonstrate that, due to the short dynamical times of the Galactic inner halo, the reduced perturbation magnitude in the second-passage scenario is mainly a result of the LMC's second infall into the MW, which starts at a much lower velocity relative to the inner halo compared to the first-infall scenario.","Using a subset of $\\sim 1200$ RR Lyrae stars located in the outer halo ($50 \\leq R_{\\mathrm{GC}} < 100$ kpc), which are selected from a larger sample of 135,873 RR Lyrae stars with precise distance estimates from Gaia, we find the mean latitudinal velocity ($v_{b}$) in the heliocentric frame to be $\\langle v_{b} \\rangle = 30.8 \\pm 4.0$ km/s. The observation contradicts the second-passage scenario and supports the first-infall scenario with a massive LMC ($\\sim 2.1 \\times 10^{11} \\mathrm{M}_{\\odot}$) at infall, an oblate MW halo with a virial mass $M_{200} < 1.4 \\times 10^{12} \\mathrm{M}_{\\odot}$ and a flattening parameter $q > 0.7$."],"url":"http://arxiv.org/abs/2404.08975v1","category":"astro-ph.GA"}
{"created":"2024-04-13 11:40:06","title":"OOVs in the Spotlight: How to Inflect them?","abstract":"We focus on morphological inflection in out-of-vocabulary (OOV) conditions, an under-researched subtask in which state-of-the-art systems usually are less effective. We developed three systems: a retrograde model and two sequence-to-sequence (seq2seq) models based on LSTM and Transformer. For testing in OOV conditions, we automatically extracted a large dataset of nouns in the morphologically rich Czech language, with lemma-disjoint data splits, and we further manually annotated a real-world OOV dataset of neologisms. In the standard OOV conditions, Transformer achieves the best results, with increasing performance in ensemble with LSTM, the retrograde model and SIGMORPHON baselines. On the real-world OOV dataset of neologisms, the retrograde model outperforms all neural models. Finally, our seq2seq models achieve state-of-the-art results in 9 out of 16 languages from SIGMORPHON 2022 shared task data in the OOV evaluation (feature overlap) in the large data condition. We release the Czech OOV Inflection Dataset for rigorous evaluation in OOV conditions. Further, we release the inflection system with the seq2seq models as a ready-to-use Python library.","sentences":["We focus on morphological inflection in out-of-vocabulary (OOV) conditions, an under-researched subtask in which state-of-the-art systems usually are less effective.","We developed three systems: a retrograde model and two sequence-to-sequence (seq2seq) models based on LSTM and Transformer.","For testing in OOV conditions, we automatically extracted a large dataset of nouns in the morphologically rich Czech language, with lemma-disjoint data splits, and we further manually annotated a real-world OOV dataset of neologisms.","In the standard OOV conditions, Transformer achieves the best results, with increasing performance in ensemble with LSTM, the retrograde model and SIGMORPHON baselines.","On the real-world OOV dataset of neologisms, the retrograde model outperforms all neural models.","Finally, our seq2seq models achieve state-of-the-art results in 9 out of 16 languages from SIGMORPHON 2022 shared task data in the OOV evaluation (feature overlap) in the large data condition.","We release the Czech OOV Inflection Dataset for rigorous evaluation in OOV conditions.","Further, we release the inflection system with the seq2seq models as a ready-to-use Python library."],"url":"http://arxiv.org/abs/2404.08974v1","category":"cs.CL"}
{"created":"2024-04-13 11:23:34","title":"Fast Gradient Computation for Gromov-Wasserstein Distance","abstract":"The Gromov-Wasserstein distance is a notable extension of optimal transport. In contrast to the classic Wasserstein distance, it solves a quadratic assignment problem that minimizes the pair-wise distance distortion under the transportation of distributions and thus could apply to distributions in different spaces. These properties make Gromov-Wasserstein widely applicable to many fields, such as computer graphics and machine learning. However, the computation of the Gromov-Wasserstein distance and transport plan is expensive. The well-known Entropic Gromov-Wasserstein approach has a cubic complexity since the matrix multiplication operations need to be repeated in computing the gradient of Gromov-Wasserstein loss. This becomes a key bottleneck of the method. Currently, existing methods accelerate the computation focus on sampling and approximation, which leads to low accuracy or incomplete transport plan. In this work, we propose a novel method to accelerate accurate gradient computation by dynamic programming techniques, reducing the complexity from cubic to quadratic. In this way, the original computational bottleneck is broken and the new entropic solution can be obtained with total quadratic time, which is almost optimal complexity. Furthermore, it can be extended to some variants easily. Extensive experiments validate the efficiency and effectiveness of our method.","sentences":["The Gromov-Wasserstein distance is a notable extension of optimal transport.","In contrast to the classic Wasserstein distance, it solves a quadratic assignment problem that minimizes the pair-wise distance distortion under the transportation of distributions and thus could apply to distributions in different spaces.","These properties make Gromov-Wasserstein widely applicable to many fields, such as computer graphics and machine learning.","However, the computation of the Gromov-Wasserstein distance and transport plan is expensive.","The well-known Entropic Gromov-Wasserstein approach has a cubic complexity since the matrix multiplication operations need to be repeated in computing the gradient of Gromov-Wasserstein loss.","This becomes a key bottleneck of the method.","Currently, existing methods accelerate the computation focus on sampling and approximation, which leads to low accuracy or incomplete transport plan.","In this work, we propose a novel method to accelerate accurate gradient computation by dynamic programming techniques, reducing the complexity from cubic to quadratic.","In this way, the original computational bottleneck is broken and the new entropic solution can be obtained with total quadratic time, which is almost optimal complexity.","Furthermore, it can be extended to some variants easily.","Extensive experiments validate the efficiency and effectiveness of our method."],"url":"http://arxiv.org/abs/2404.08970v1","category":"cs.LG"}
{"created":"2024-04-13 11:13:56","title":"MCPNet: An Interpretable Classifier via Multi-Level Concept Prototypes","abstract":"Recent advancements in post-hoc and inherently interpretable methods have markedly enhanced the explanations of black box classifier models. These methods operate either through post-analysis or by integrating concept learning during model training. Although being effective in bridging the semantic gap between a model's latent space and human interpretation, these explanation methods only partially reveal the model's decision-making process. The outcome is typically limited to high-level semantics derived from the last feature map. We argue that the explanations lacking insights into the decision processes at low and mid-level features are neither fully faithful nor useful. Addressing this gap, we introduce the Multi-Level Concept Prototypes Classifier (MCPNet), an inherently interpretable model. MCPNet autonomously learns meaningful concept prototypes across multiple feature map levels using Centered Kernel Alignment (CKA) loss and an energy-based weighted PCA mechanism, and it does so without reliance on predefined concept labels. Further, we propose a novel classifier paradigm that learns and aligns multi-level concept prototype distributions for classification purposes via Class-aware Concept Distribution (CCD) loss. Our experiments reveal that our proposed MCPNet while being adaptable to various model architectures, offers comprehensive multi-level explanations while maintaining classification accuracy. Additionally, its concept distribution-based classification approach shows improved generalization capabilities in few-shot classification scenarios.","sentences":["Recent advancements in post-hoc and inherently interpretable methods have markedly enhanced the explanations of black box classifier models.","These methods operate either through post-analysis or by integrating concept learning during model training.","Although being effective in bridging the semantic gap between a model's latent space and human interpretation, these explanation methods only partially reveal the model's decision-making process.","The outcome is typically limited to high-level semantics derived from the last feature map.","We argue that the explanations lacking insights into the decision processes at low and mid-level features are neither fully faithful nor useful.","Addressing this gap, we introduce the Multi-Level Concept Prototypes Classifier (MCPNet), an inherently interpretable model.","MCPNet autonomously learns meaningful concept prototypes across multiple feature map levels using Centered Kernel Alignment (CKA) loss and an energy-based weighted PCA mechanism, and it does so without reliance on predefined concept labels.","Further, we propose a novel classifier paradigm that learns and aligns multi-level concept prototype distributions for classification purposes via Class-aware Concept Distribution (CCD) loss.","Our experiments reveal that our proposed MCPNet while being adaptable to various model architectures, offers comprehensive multi-level explanations while maintaining classification accuracy.","Additionally, its concept distribution-based classification approach shows improved generalization capabilities in few-shot classification scenarios."],"url":"http://arxiv.org/abs/2404.08968v1","category":"cs.CV"}
{"created":"2024-04-13 11:09:42","title":"Beam Management in Low Earth Orbit Satellite Communication With Handover Frequency Control and Satellite-Terrestrial Spectrum Sharing","abstract":"To achieve ubiquitous wireless connectivity, low earth orbit (LEO) satellite networks have drawn much attention. However, effective beam management is challenging due to time-varying cell load, high dynamic network topology, and complex interference situations. In this paper, under inter-satellite handover frequency and satellite-terrestrial/inter-beam interference constraints, we formulate a practical beam management problem, aiming to maximize the long-term service satisfaction of cells. Particularly, Lyapunov framework is leveraged to equivalently transform the primal problem into multiple single epoch optimization problems, where virtual queue stability constraints replace inter-satellite handover frequency constraints. Since each single epoch problem is NP-hard, we further decompose it into three subproblems, including inter-satellite handover decision, beam hopping design and satellite-terrestrial spectrum sharing. First, a proactive inter-satellite handover mechanism is developed to balance handover frequency and satellite loads. Subsequently, a beam hopping design algorithm is presented based on conflict graphs to achieve interference mitigation among beams, and then a flexible satellite-terrestrial spectrum sharing algorithm is designed to satisfy the demands of beam cells and improve spectral efficiency. Simulation results show that our proposal significantly improves service satisfaction compared with baselines, where the average data queue length of beam cells is reduced by over 50% with affordable handover frequency.","sentences":["To achieve ubiquitous wireless connectivity, low earth orbit (LEO) satellite networks have drawn much attention.","However, effective beam management is challenging due to time-varying cell load, high dynamic network topology, and complex interference situations.","In this paper, under inter-satellite handover frequency and satellite-terrestrial/inter-beam interference constraints, we formulate a practical beam management problem, aiming to maximize the long-term service satisfaction of cells.","Particularly, Lyapunov framework is leveraged to equivalently transform the primal problem into multiple single epoch optimization problems, where virtual queue stability constraints replace inter-satellite handover frequency constraints.","Since each single epoch problem is NP-hard, we further decompose it into three subproblems, including inter-satellite handover decision, beam hopping design and satellite-terrestrial spectrum sharing.","First, a proactive inter-satellite handover mechanism is developed to balance handover frequency and satellite loads.","Subsequently, a beam hopping design algorithm is presented based on conflict graphs to achieve interference mitigation among beams, and then a flexible satellite-terrestrial spectrum sharing algorithm is designed to satisfy the demands of beam cells and improve spectral efficiency.","Simulation results show that our proposal significantly improves service satisfaction compared with baselines, where the average data queue length of beam cells is reduced by over 50% with affordable handover frequency."],"url":"http://arxiv.org/abs/2404.08967v1","category":"cs.SI"}
{"created":"2024-04-13 11:07:10","title":"Seeing Text in the Dark: Algorithm and Benchmark","abstract":"Localizing text in low-light environments is challenging due to visual degradations. Although a straightforward solution involves a two-stage pipeline with low-light image enhancement (LLE) as the initial step followed by detector, LLE is primarily designed for human vision instead of machine and can accumulate errors. In this work, we propose an efficient and effective single-stage approach for localizing text in dark that circumvents the need for LLE. We introduce a constrained learning module as an auxiliary mechanism during the training stage of the text detector. This module is designed to guide the text detector in preserving textual spatial features amidst feature map resizing, thus minimizing the loss of spatial information in texts under low-light visual degradations. Specifically, we incorporate spatial reconstruction and spatial semantic constraints within this module to ensure the text detector acquires essential positional and contextual range knowledge. Our approach enhances the original text detector's ability to identify text's local topological features using a dynamic snake feature pyramid network and adopts a bottom-up contour shaping strategy with a novel rectangular accumulation technique for accurate delineation of streamlined text features. In addition, we present a comprehensive low-light dataset for arbitrary-shaped text, encompassing diverse scenes and languages. Notably, our method achieves state-of-the-art results on this low-light dataset and exhibits comparable performance on standard normal light datasets. The code and dataset will be released.","sentences":["Localizing text in low-light environments is challenging due to visual degradations.","Although a straightforward solution involves a two-stage pipeline with low-light image enhancement (LLE) as the initial step followed by detector, LLE is primarily designed for human vision instead of machine and can accumulate errors.","In this work, we propose an efficient and effective single-stage approach for localizing text in dark that circumvents the need for LLE.","We introduce a constrained learning module as an auxiliary mechanism during the training stage of the text detector.","This module is designed to guide the text detector in preserving textual spatial features amidst feature map resizing, thus minimizing the loss of spatial information in texts under low-light visual degradations.","Specifically, we incorporate spatial reconstruction and spatial semantic constraints within this module to ensure the text detector acquires essential positional and contextual range knowledge.","Our approach enhances the original text detector's ability to identify text's local topological features using a dynamic snake feature pyramid network and adopts a bottom-up contour shaping strategy with a novel rectangular accumulation technique for accurate delineation of streamlined text features.","In addition, we present a comprehensive low-light dataset for arbitrary-shaped text, encompassing diverse scenes and languages.","Notably, our method achieves state-of-the-art results on this low-light dataset and exhibits comparable performance on standard normal light datasets.","The code and dataset will be released."],"url":"http://arxiv.org/abs/2404.08965v1","category":"cs.CV"}
{"created":"2024-04-13 10:50:29","title":"Timing Advance Estimation in Low Earth Orbit Satellite Networks","abstract":"Low earth orbit (LEO) satellite communication based on 3GPP standard is seen as a promising solution to rolling out communication services in areas without terrestrial base stations. However, due to the fast movement of satellites and large beam footprint size, the existing 5G timing advance (TA) estimation mechanism cannot be directly applied when global navigation satellite system is unavailable. In this article, an enhanced TA estimation approach is proposed for LEO satellite communication networks. Specifically, a user-side time-frequency pre-compensation method is introduced at first, which leverages frequency offset measurement on synchronization signal blocks broadcasted by satellites in initial cell search phase. For the random access phase, the upper bound of inter-preamble interference incurred by partial-period cross-correlation operations is derived for a preamble format advised by 3GPP, and it is shown that the interference level is closely related to the square of the number of such operations. Inspired by this result, a cyclic prefix free preamble format is further designed, which features extended guard time, differential power allocation and flexible preamble structure. Numerical results show that our proposal can reduce the missed detection rate of preamble within a beam. Particularly, the missed detection rates of preamble under 32, 48, and 64 users are lower than 1% when SNR = -6 dB, which is a significant improvement compared to baselines. In addition, our proposal can limit the TA estimation error of the detected users to the time length of 25 time-domain sampling points when the subcarrier spacing is 30 kHz and operation frequency is 27 GHz.","sentences":["Low earth orbit (LEO) satellite communication based on 3GPP standard is seen as a promising solution to rolling out communication services in areas without terrestrial base stations.","However, due to the fast movement of satellites and large beam footprint size, the existing 5G timing advance (TA) estimation mechanism cannot be directly applied when global navigation satellite system is unavailable.","In this article, an enhanced TA estimation approach is proposed for LEO satellite communication networks.","Specifically, a user-side time-frequency pre-compensation method is introduced at first, which leverages frequency offset measurement on synchronization signal blocks broadcasted by satellites in initial cell search phase.","For the random access phase, the upper bound of inter-preamble interference incurred by partial-period cross-correlation operations is derived for a preamble format advised by 3GPP, and it is shown that the interference level is closely related to the square of the number of such operations.","Inspired by this result, a cyclic prefix free preamble format is further designed, which features extended guard time, differential power allocation and flexible preamble structure.","Numerical results show that our proposal can reduce the missed detection rate of preamble within a beam.","Particularly, the missed detection rates of preamble under 32, 48, and 64 users are lower than 1% when SNR = -6 dB, which is a significant improvement compared to baselines.","In addition, our proposal can limit the TA estimation error of the detected users to the time length of 25 time-domain sampling points when the subcarrier spacing is 30 kHz and operation frequency is 27 GHz."],"url":"http://arxiv.org/abs/2404.08960v1","category":"cs.SI"}
{"created":"2024-04-13 10:50:21","title":"Beam Management in Low Earth Orbit Satellite Networks with Random Traffic Arrival and Time-varying Topology","abstract":"Low earth orbit (LEO) satellite communication networks have been considered as promising solutions to providing high data rate and seamless coverage, where satellite beam management plays a key role. However, due to the limitation of beam resource, dynamic network topology, beam spectrum reuse, time-varying traffic arrival and service continuity requirement, it is challenging to effectively allocate time-frequency resource of satellite beams to multiple cells. In this paper, aiming at reducing time-averaged beam revisit time and mitigate inter-satellite handover, a beam management problem is formulated for dynamic LEO satellite communication networks, under inter-cell interference and network stability constraints. Particularly, inter-cell interference constraints are further simplified into off-axis angle based constraints, which provide tractable rules for spectrum sharing between two beam cells. To deal with the long-term performance optimization, the primal problem is transformed into a series of single epoch problems by adopting Lyapunov optimization framework. Since the transformed problem is NP-hard, it is further divided into three subproblems, including serving beam allocation, beam service time allocation and serving satellite allocation. With the help of conflict graphs built with off-axis angle based constraints, serving beam allocation and beam service time allocation algorithms are developed to reduce beam revisit time and cell packet queue length. Then, we further develop a satellite-cell service relationship optimization algorithm to better adapt to dynamic network topology. Compared with baselines, numerical results show that our proposal can reduce average beam revisit time by 20.8% and keep strong network stability with similar inter-satellite handover frequency.","sentences":["Low earth orbit (LEO) satellite communication networks have been considered as promising solutions to providing high data rate and seamless coverage, where satellite beam management plays a key role.","However, due to the limitation of beam resource, dynamic network topology, beam spectrum reuse, time-varying traffic arrival and service continuity requirement, it is challenging to effectively allocate time-frequency resource of satellite beams to multiple cells.","In this paper, aiming at reducing time-averaged beam revisit time and mitigate inter-satellite handover, a beam management problem is formulated for dynamic LEO satellite communication networks, under inter-cell interference and network stability constraints.","Particularly, inter-cell interference constraints are further simplified into off-axis angle based constraints, which provide tractable rules for spectrum sharing between two beam cells.","To deal with the long-term performance optimization, the primal problem is transformed into a series of single epoch problems by adopting Lyapunov optimization framework.","Since the transformed problem is NP-hard, it is further divided into three subproblems, including serving beam allocation, beam service time allocation and serving satellite allocation.","With the help of conflict graphs built with off-axis angle based constraints, serving beam allocation and beam service time allocation algorithms are developed to reduce beam revisit time and cell packet queue length.","Then, we further develop a satellite-cell service relationship optimization algorithm to better adapt to dynamic network topology.","Compared with baselines, numerical results show that our proposal can reduce average beam revisit time by 20.8% and keep strong network stability with similar inter-satellite handover frequency."],"url":"http://arxiv.org/abs/2404.08959v1","category":"cs.SI"}
{"created":"2024-04-13 09:39:20","title":"Physics-Aware Iterative Learning and Prediction of Saliency Map for Bimanual Grasp Planning","abstract":"Learning the skill of human bimanual grasping can extend the capabilities of robotic systems when grasping large or heavy objects. However, it requires a much larger search space for grasp points than single-hand grasping and numerous bimanual grasping annotations for network learning, making both data-driven or analytical grasping methods inefficient and insufficient. We propose a framework for bimanual grasp saliency learning that aims to predict the contact points for bimanual grasping based on existing human single-handed grasping data. We learn saliency corresponding vectors through minimal bimanual contact annotations that establishes correspondences between grasp positions of both hands, capable of eliminating the need for training a large-scale bimanual grasp dataset. The existing single-handed grasp saliency value serves as the initial value for bimanual grasp saliency, and we learn a saliency adjusted score that adds the initial value to obtain the final bimanual grasp saliency value, capable of predicting preferred bimanual grasp positions from single-handed grasp saliency. We also introduce a physics-balance loss function and a physics-aware refinement module that enables physical grasp balance, capable of enhancing the generalization of unknown objects. Comprehensive experiments in simulation and comparisons on dexterous grippers have demonstrated that our method can achieve balanced bimanual grasping effectively.","sentences":["Learning the skill of human bimanual grasping can extend the capabilities of robotic systems when grasping large or heavy objects.","However, it requires a much larger search space for grasp points than single-hand grasping and numerous bimanual grasping annotations for network learning, making both data-driven or analytical grasping methods inefficient and insufficient.","We propose a framework for bimanual grasp saliency learning that aims to predict the contact points for bimanual grasping based on existing human single-handed grasping data.","We learn saliency corresponding vectors through minimal bimanual contact annotations that establishes correspondences between grasp positions of both hands, capable of eliminating the need for training a large-scale bimanual grasp dataset.","The existing single-handed grasp saliency value serves as the initial value for bimanual grasp saliency, and we learn a saliency adjusted score that adds the initial value to obtain the final bimanual grasp saliency value, capable of predicting preferred bimanual grasp positions from single-handed grasp saliency.","We also introduce a physics-balance loss function and a physics-aware refinement module that enables physical grasp balance, capable of enhancing the generalization of unknown objects.","Comprehensive experiments in simulation and comparisons on dexterous grippers have demonstrated that our method can achieve balanced bimanual grasping effectively."],"url":"http://arxiv.org/abs/2404.08944v1","category":"cs.RO"}
{"created":"2024-04-13 09:36:15","title":"A Novel State-Centric Necessary Condition for Time-Optimal Control of Controllable Linear Systems Based on Augmented Switching Laws","abstract":"Most existing necessary conditions for optimal control based on adjoining methods require both state information and costate information, yet the lack of costates for a given feasible trajectory in practice impedes the determination of optimality. This paper establishes a novel theoretical framework for time-optimal control of controllable linear systems, proposing the augmented switching law that represents the input control and the feasibility in a compact form. Given a feasible trajectory, the disturbed trajectory under the constraints of augmented switching law is guaranteed to be feasible, resulting in a novel state-centric necessary condition without dependence on costate information. A first order necessary condition is proposed that the Jacobian matrix of the augmented switching law is not full row rank, which also results in an approach to optimizing a given feasible trajectory further. The proposed necessary condition is applied to the chain-of-integrators systems with full box constraints, contributing to some conclusions challenging to reason by traditional costate-based necessary conditions.","sentences":["Most existing necessary conditions for optimal control based on adjoining methods require both state information and costate information, yet the lack of costates for a given feasible trajectory in practice impedes the determination of optimality.","This paper establishes a novel theoretical framework for time-optimal control of controllable linear systems, proposing the augmented switching law that represents the input control and the feasibility in a compact form.","Given a feasible trajectory, the disturbed trajectory under the constraints of augmented switching law is guaranteed to be feasible, resulting in a novel state-centric necessary condition without dependence on costate information.","A first order necessary condition is proposed that the Jacobian matrix of the augmented switching law is not full row rank, which also results in an approach to optimizing a given feasible trajectory further.","The proposed necessary condition is applied to the chain-of-integrators systems with full box constraints, contributing to some conclusions challenging to reason by traditional costate-based necessary conditions."],"url":"http://arxiv.org/abs/2404.08943v1","category":"math.OC"}
{"created":"2024-04-13 09:10:33","title":"Shifting Spotlight for Co-supervision: A Simple yet Efficient Single-branch Network to See Through Camouflage","abstract":"Efficient and accurate camouflaged object detection (COD) poses a challenge in the field of computer vision. Recent approaches explored the utility of edge information for network co-supervision, achieving notable advancements. However, these approaches introduce an extra branch for complex edge extraction, complicate the model architecture and increases computational demands. Addressing this issue, our work replicates the effect that animal's camouflage can be easily revealed under a shifting spotlight, and leverages it for network co-supervision to form a compact yet efficient single-branch network, the Co-Supervised Spotlight Shifting Network (CS$^3$Net). The spotlight shifting strategy allows CS$^3$Net to learn additional prior within a single-branch framework, obviating the need for resource demanding multi-branch design. To leverage the prior of spotlight shifting co-supervision, we propose Shadow Refinement Module (SRM) and Projection Aware Attention (PAA) for feature refinement and enhancement. To ensure the continuity of multi-scale features aggregation, we utilize the Extended Neighbor Connection Decoder (ENCD) for generating the final predictions. Empirical evaluations on public datasets confirm that our CS$^3$Net offers an optimal balance between efficiency and performance: it accomplishes a 32.13% reduction in Multiply-Accumulate (MACs) operations compared to leading efficient COD models, while also delivering superior performance.","sentences":["Efficient and accurate camouflaged object detection (COD) poses a challenge in the field of computer vision.","Recent approaches explored the utility of edge information for network co-supervision, achieving notable advancements.","However, these approaches introduce an extra branch for complex edge extraction, complicate the model architecture and increases computational demands.","Addressing this issue, our work replicates the effect that animal's camouflage can be easily revealed under a shifting spotlight, and leverages it for network co-supervision to form a compact yet efficient single-branch network, the Co-Supervised Spotlight Shifting Network (CS$^3$Net).","The spotlight shifting strategy allows CS$^3$Net to learn additional prior within a single-branch framework, obviating the need for resource demanding multi-branch design.","To leverage the prior of spotlight shifting co-supervision, we propose Shadow Refinement Module (SRM) and Projection Aware Attention (PAA) for feature refinement and enhancement.","To ensure the continuity of multi-scale features aggregation, we utilize the Extended Neighbor Connection Decoder (ENCD) for generating the final predictions.","Empirical evaluations on public datasets confirm that our CS$^3$Net offers an optimal balance between efficiency and performance: it accomplishes a 32.13% reduction in Multiply-Accumulate (MACs) operations compared to leading efficient COD models, while also delivering superior performance."],"url":"http://arxiv.org/abs/2404.08936v1","category":"cs.CV"}
{"created":"2024-04-13 09:10:05","title":"Developing An Attention-Based Ensemble Learning Framework for Financial Portfolio Optimisation","abstract":"In recent years, deep or reinforcement learning approaches have been applied to optimise investment portfolios through learning the spatial and temporal information under the dynamic financial market. Yet in most cases, the existing approaches may produce biased trading signals based on the conventional price data due to a lot of market noises, which possibly fails to balance the investment returns and risks. Accordingly, a multi-agent and self-adaptive portfolio optimisation framework integrated with attention mechanisms and time series, namely the MASAAT, is proposed in this work in which multiple trading agents are created to observe and analyse the price series and directional change data that recognises the significant changes of asset prices at different levels of granularity for enhancing the signal-to-noise ratio of price series. Afterwards, by reconstructing the tokens of financial data in a sequence, the attention-based cross-sectional analysis module and temporal analysis module of each agent can effectively capture the correlations between assets and the dependencies between time points. Besides, a portfolio generator is integrated into the proposed framework to fuse the spatial-temporal information and then summarise the portfolios suggested by all trading agents to produce a newly ensemble portfolio for reducing biased trading actions and balancing the overall returns and risks. The experimental results clearly demonstrate that the MASAAT framework achieves impressive enhancement when compared with many well-known portfolio optimsation approaches on three challenging data sets of DJIA, S&P 500 and CSI 300. More importantly, our proposal has potential strengths in many possible applications for future study.","sentences":["In recent years, deep or reinforcement learning approaches have been applied to optimise investment portfolios through learning the spatial and temporal information under the dynamic financial market.","Yet in most cases, the existing approaches may produce biased trading signals based on the conventional price data due to a lot of market noises, which possibly fails to balance the investment returns and risks.","Accordingly, a multi-agent and self-adaptive portfolio optimisation framework integrated with attention mechanisms and time series, namely the MASAAT, is proposed in this work in which multiple trading agents are created to observe and analyse the price series and directional change data that recognises the significant changes of asset prices at different levels of granularity for enhancing the signal-to-noise ratio of price series.","Afterwards, by reconstructing the tokens of financial data in a sequence, the attention-based cross-sectional analysis module and temporal analysis module of each agent can effectively capture the correlations between assets and the dependencies between time points.","Besides, a portfolio generator is integrated into the proposed framework to fuse the spatial-temporal information and then summarise the portfolios suggested by all trading agents to produce a newly ensemble portfolio for reducing biased trading actions and balancing the overall returns and risks.","The experimental results clearly demonstrate that the MASAAT framework achieves impressive enhancement when compared with many well-known portfolio optimsation approaches on three challenging data sets of DJIA, S&P 500 and CSI 300.","More importantly, our proposal has potential strengths in many possible applications for future study."],"url":"http://arxiv.org/abs/2404.08935v1","category":"q-fin.PM"}
{"created":"2024-04-13 08:36:13","title":"DeDoDe v2: Analyzing and Improving the DeDoDe Keypoint Detector","abstract":"In this paper, we analyze and improve into the recently proposed DeDoDe keypoint detector. We focus our analysis on some key issues. First, we find that DeDoDe keypoints tend to cluster together, which we fix by performing non-max suppression on the target distribution of the detector during training. Second, we address issues related to data augmentation. In particular, the DeDoDe detector is sensitive to large rotations. We fix this by including 90-degree rotations as well as horizontal flips. Finally, the decoupled nature of the DeDoDe detector makes evaluation of downstream usefulness problematic. We fix this by matching the keypoints with a pretrained dense matcher (RoMa) and evaluating two-view pose estimates. We find that the original long training is detrimental to performance, and therefore propose a much shorter training schedule. We integrate all these improvements into our proposed detector DeDoDe v2 and evaluate it with the original DeDoDe descriptor on the MegaDepth-1500 and IMC2022 benchmarks. Our proposed detector significantly increases pose estimation results, notably from 75.9 to 78.3 mAA on the IMC2022 challenge. Code and weights are available at https://github.com/Parskatt/DeDoDe","sentences":["In this paper, we analyze and improve into the recently proposed DeDoDe keypoint detector.","We focus our analysis on some key issues.","First, we find that DeDoDe keypoints tend to cluster together, which we fix by performing non-max suppression on the target distribution of the detector during training.","Second, we address issues related to data augmentation.","In particular, the DeDoDe detector is sensitive to large rotations.","We fix this by including 90-degree rotations as well as horizontal flips.","Finally, the decoupled nature of the DeDoDe detector makes evaluation of downstream usefulness problematic.","We fix this by matching the keypoints with a pretrained dense matcher (RoMa) and evaluating two-view pose estimates.","We find that the original long training is detrimental to performance, and therefore propose a much shorter training schedule.","We integrate all these improvements into our proposed detector DeDoDe v2 and evaluate it with the original DeDoDe descriptor on the MegaDepth-1500 and IMC2022 benchmarks.","Our proposed detector significantly increases pose estimation results, notably from 75.9 to 78.3 mAA on the IMC2022 challenge.","Code and weights are available at https://github.com/Parskatt/DeDoDe"],"url":"http://arxiv.org/abs/2404.08928v1","category":"cs.CV"}
{"created":"2024-04-15 12:54:31","title":"Personalized Collaborative Fine-Tuning for On-Device Large Language Models","abstract":"We explore on-device self-supervised collaborative fine-tuning of large language models with limited local data availability. Taking inspiration from the collaborative learning community, we introduce three distinct trust-weighted gradient aggregation schemes: weight similarity-based, prediction similarity-based and validation performance-based. To minimize communication overhead, we integrate Low-Rank Adaptation (LoRA) and only exchange LoRA weight updates. Our protocols, driven by prediction and performance metrics, surpass both FedAvg and local fine-tuning methods, which is particularly evident in realistic scenarios with more diverse local data distributions. The results underscore the effectiveness of our approach in addressing heterogeneity and scarcity within local datasets.","sentences":["We explore on-device self-supervised collaborative fine-tuning of large language models with limited local data availability.","Taking inspiration from the collaborative learning community, we introduce three distinct trust-weighted gradient aggregation schemes: weight similarity-based, prediction similarity-based and validation performance-based.","To minimize communication overhead, we integrate Low-Rank Adaptation (LoRA) and only exchange LoRA weight updates.","Our protocols, driven by prediction and performance metrics, surpass both FedAvg and local fine-tuning methods, which is particularly evident in realistic scenarios with more diverse local data distributions.","The results underscore the effectiveness of our approach in addressing heterogeneity and scarcity within local datasets."],"url":"http://arxiv.org/abs/2404.09753v1","category":"cs.CL"}
{"created":"2024-04-15 12:37:26","title":"FSRT: Facial Scene Representation Transformer for Face Reenactment from Factorized Appearance, Head-pose, and Facial Expression Features","abstract":"The task of face reenactment is to transfer the head motion and facial expressions from a driving video to the appearance of a source image, which may be of a different person (cross-reenactment). Most existing methods are CNN-based and estimate optical flow from the source image to the current driving frame, which is then inpainted and refined to produce the output animation. We propose a transformer-based encoder for computing a set-latent representation of the source image(s). We then predict the output color of a query pixel using a transformer-based decoder, which is conditioned with keypoints and a facial expression vector extracted from the driving frame. Latent representations of the source person are learned in a self-supervised manner that factorize their appearance, head pose, and facial expressions. Thus, they are perfectly suited for cross-reenactment. In contrast to most related work, our method naturally extends to multiple source images and can thus adapt to person-specific facial dynamics. We also propose data augmentation and regularization schemes that are necessary to prevent overfitting and support generalizability of the learned representations. We evaluated our approach in a randomized user study. The results indicate superior performance compared to the state-of-the-art in terms of motion transfer quality and temporal consistency.","sentences":["The task of face reenactment is to transfer the head motion and facial expressions from a driving video to the appearance of a source image, which may be of a different person (cross-reenactment).","Most existing methods are CNN-based and estimate optical flow from the source image to the current driving frame, which is then inpainted and refined to produce the output animation.","We propose a transformer-based encoder for computing a set-latent representation of the source image(s).","We then predict the output color of a query pixel using a transformer-based decoder, which is conditioned with keypoints and a facial expression vector extracted from the driving frame.","Latent representations of the source person are learned in a self-supervised manner that factorize their appearance, head pose, and facial expressions.","Thus, they are perfectly suited for cross-reenactment.","In contrast to most related work, our method naturally extends to multiple source images and can thus adapt to person-specific facial dynamics.","We also propose data augmentation and regularization schemes that are necessary to prevent overfitting and support generalizability of the learned representations.","We evaluated our approach in a randomized user study.","The results indicate superior performance compared to the state-of-the-art in terms of motion transfer quality and temporal consistency."],"url":"http://arxiv.org/abs/2404.09736v1","category":"cs.CV"}
{"created":"2024-04-15 12:27:20","title":"Nonparametric density estimation for the small jumps of L\u00e9vy processes","abstract":"We consider the problem of estimating the density of the process associated with the small jumps of a pure jump L\\'evy process, possibly of infinite variation, from discrete observations of one trajectory. The interest of such a question lies on the observation that even when the L\\'evy measure is known, the density of the increments of the small jumps of the process cannot be computed. We discuss results both from low and high frequency observations. In a low frequency setting, assuming the L\\'evy density associated with the jumps larger than $\\varepsilon\\in (0,1]$ in absolute value is known, a spectral estimator relying on the convolution structure of the problem achieves minimax parametric rates of convergence with respect to the integrated $L_2$ loss, up to a logarithmic factor. In a high frequency setting, we remove the assumption on the knowledge of the L\\'evy measure of the large jumps and show that the rate of convergence depends both on the sampling scheme and on the behaviour of the L\\'evy measure in a neighborhood of zero. We show that the rate we find is minimax up to a log-factor. An adaptive penalized procedure is studied to select the cutoff parameter. These results are extended to encompass the case where a Brownian component is present in the L\\'evy process. Furthermore, we illustrate the performances of our procedures through an extensive simulation study.","sentences":["We consider the problem of estimating the density of the process associated with the small jumps of a pure jump L\\'evy process, possibly of infinite variation, from discrete observations of one trajectory.","The interest of such a question lies on the observation that even when the L\\'evy measure is known, the density of the increments of the small jumps of the process cannot be computed.","We discuss results both from low and high frequency observations.","In a low frequency setting, assuming the L\\'evy density associated with the jumps larger than $\\varepsilon\\in (0,1]$ in absolute value is known, a spectral estimator relying on the convolution structure of the problem achieves minimax parametric rates of convergence with respect to the integrated $L_2$ loss, up to a logarithmic factor.","In a high frequency setting, we remove the assumption on the knowledge of the L\\'evy measure of the large jumps and show that the rate of convergence depends both on the sampling scheme and on the behaviour of the L\\'evy measure in a neighborhood of zero.","We show that the rate we find is minimax up to a log-factor.","An adaptive penalized procedure is studied to select the cutoff parameter.","These results are extended to encompass the case where a Brownian component is present in the L\\'evy process.","Furthermore, we illustrate the performances of our procedures through an extensive simulation study."],"url":"http://arxiv.org/abs/2404.09725v1","category":"math.ST"}
{"created":"2024-04-15 12:08:44","title":"Scenario-Adaptive Fine-Grained Personalization Network: Tailoring User Behavior Representation to the Scenario Context","abstract":"Existing methods often adjust representations adaptively only after aggregating user behavior sequences. This coarse-grained approach to re-weighting the entire user sequence hampers the model's ability to accurately model the user interest migration across different scenarios. To enhance the model's capacity to capture user interests from historical behavior sequences in each scenario, we develop a ranking framework named the Scenario-Adaptive Fine-Grained Personalization Network (SFPNet), which designs a kind of fine-grained method for multi-scenario personalized recommendations. Specifically, SFPNet comprises a series of blocks named as Scenario-Tailoring Block, stacked sequentially. Each block initially deploys a parameter personalization unit to integrate scenario information at a coarse-grained level by redefining fundamental features. Subsequently, we consolidate scenario-adaptively adjusted feature representations to serve as context information. By employing residual connection, we incorporate this context into the representation of each historical behavior, allowing for context-aware fine-grained customization of the behavior representations at the scenario-level, which in turn supports scenario-aware user interest modeling.","sentences":["Existing methods often adjust representations adaptively only after aggregating user behavior sequences.","This coarse-grained approach to re-weighting the entire user sequence hampers the model's ability to accurately model the user interest migration across different scenarios.","To enhance the model's capacity to capture user interests from historical behavior sequences in each scenario, we develop a ranking framework named the Scenario-Adaptive Fine-Grained Personalization Network (SFPNet), which designs a kind of fine-grained method for multi-scenario personalized recommendations.","Specifically, SFPNet comprises a series of blocks named as Scenario-Tailoring Block, stacked sequentially.","Each block initially deploys a parameter personalization unit to integrate scenario information at a coarse-grained level by redefining fundamental features.","Subsequently, we consolidate scenario-adaptively adjusted feature representations to serve as context information.","By employing residual connection, we incorporate this context into the representation of each historical behavior, allowing for context-aware fine-grained customization of the behavior representations at the scenario-level, which in turn supports scenario-aware user interest modeling."],"url":"http://arxiv.org/abs/2404.09709v1","category":"cs.IR"}
{"created":"2024-04-15 11:20:44","title":"AntDT: A Self-Adaptive Distributed Training Framework for Leader and Straggler Nodes","abstract":"Many distributed training techniques like Parameter Server and AllReduce have been proposed to take advantage of the increasingly large data and rich features. However, stragglers frequently occur in distributed training due to resource contention and hardware heterogeneity, which significantly hampers the training efficiency. Previous works only address part of the stragglers and could not adaptively solve various stragglers in practice. Additionally, it is challenging to use a systematic framework to address all stragglers because different stragglers require diverse data allocation and fault-tolerance mechanisms. Therefore, this paper proposes a unified distributed training framework called AntDT (Ant Distributed Training Framework) to adaptively solve the straggler problems. Firstly, the framework consists of four components, including the Stateful Dynamic Data Sharding service, Monitor, Controller, and Agent. These components work collaboratively to efficiently distribute workloads and provide a range of pre-defined straggler mitigation methods with fault tolerance, thereby hiding messy details of data allocation and fault handling. Secondly, the framework provides a high degree of flexibility, allowing for the customization of straggler mitigation solutions based on the specific circumstances of the cluster. Leveraging this flexibility, we introduce two straggler mitigation solutions, namely AntDT-ND for non-dedicated clusters and AntDT-DD for dedicated clusters, as practical examples to resolve various types of stragglers at Ant Group. Justified by our comprehensive experiments and industrial deployment statistics, AntDT outperforms other SOTA methods more than 3x in terms of training efficiency. Additionally, in Alipay's homepage recommendation scenario, using AntDT reduces the training duration of the ranking model from 27.8 hours to just 5.4 hours.","sentences":["Many distributed training techniques like Parameter Server and AllReduce have been proposed to take advantage of the increasingly large data and rich features.","However, stragglers frequently occur in distributed training due to resource contention and hardware heterogeneity, which significantly hampers the training efficiency.","Previous works only address part of the stragglers and could not adaptively solve various stragglers in practice.","Additionally, it is challenging to use a systematic framework to address all stragglers because different stragglers require diverse data allocation and fault-tolerance mechanisms.","Therefore, this paper proposes a unified distributed training framework called AntDT (Ant Distributed Training Framework) to adaptively solve the straggler problems.","Firstly, the framework consists of four components, including the Stateful Dynamic Data Sharding service, Monitor, Controller, and Agent.","These components work collaboratively to efficiently distribute workloads and provide a range of pre-defined straggler mitigation methods with fault tolerance, thereby hiding messy details of data allocation and fault handling.","Secondly, the framework provides a high degree of flexibility, allowing for the customization of straggler mitigation solutions based on the specific circumstances of the cluster.","Leveraging this flexibility, we introduce two straggler mitigation solutions, namely AntDT-ND for non-dedicated clusters and AntDT-DD for dedicated clusters, as practical examples to resolve various types of stragglers at Ant Group.","Justified by our comprehensive experiments and industrial deployment statistics, AntDT outperforms other SOTA methods more than 3x in terms of training efficiency.","Additionally, in Alipay's homepage recommendation scenario, using AntDT reduces the training duration of the ranking model from 27.8 hours to just 5.4 hours."],"url":"http://arxiv.org/abs/2404.09679v1","category":"cs.DC"}
{"created":"2024-04-15 10:47:59","title":"Reconstructing Curves from Sparse Samples on Riemannian Manifolds","abstract":"Reconstructing 2D curves from sample points has long been a critical challenge in computer graphics, finding essential applications in vector graphics. The design and editing of curves on surfaces has only recently begun to receive attention, primarily relying on human assistance, and where not, limited by very strict sampling conditions. In this work, we formally improve on the state-of-the-art requirements and introduce an innovative algorithm capable of reconstructing closed curves directly on surfaces from a given sparse set of sample points. We extend and adapt a state-of-the-art planar curve reconstruction method to the realm of surfaces while dealing with the challenges arising from working on non-Euclidean domains. We demonstrate the robustness of our method by reconstructing multiple curves on various surface meshes. We explore novel potential applications of our approach, allowing for automated reconstruction of curves on Riemannian manifolds.","sentences":["Reconstructing 2D curves from sample points has long been a critical challenge in computer graphics, finding essential applications in vector graphics.","The design and editing of curves on surfaces has only recently begun to receive attention, primarily relying on human assistance, and where not, limited by very strict sampling conditions.","In this work, we formally improve on the state-of-the-art requirements and introduce an innovative algorithm capable of reconstructing closed curves directly on surfaces from a given sparse set of sample points.","We extend and adapt a state-of-the-art planar curve reconstruction method to the realm of surfaces while dealing with the challenges arising from working on non-Euclidean domains.","We demonstrate the robustness of our method by reconstructing multiple curves on various surface meshes.","We explore novel potential applications of our approach, allowing for automated reconstruction of curves on Riemannian manifolds."],"url":"http://arxiv.org/abs/2404.09661v1","category":"cs.CG"}
{"created":"2024-04-15 10:34:22","title":"Stiffness-Tuneable Limb Segment with Flexible Spine for Malleable Robots","abstract":"Robotic arms built from stiffness-adjustable, continuously bending segments serially connected with revolute joints have the ability to change their mechanical architecture and workspace, thus allowing high flexibility and adaptation to different tasks with less than six degrees of freedom, a concept that we call malleable robots. Known stiffening mechanisms may be used to implement suitable links for these novel robotic manipulators; however, these solutions usually show a reduced performance when bending due to structural deformation. By including an inner support structure this deformation can be minimised, resulting in an increased stiffening performance. This paper presents a new multi-material spine-inspired flexible structure for providing support in stiffness-controllable layer-jamming-based robotic links of large diameter. The proposed spine mechanism is highly movable with type and range of motions that match those of a robotic link using solely layer jamming, whilst maintaining a hollow and light structure. The mechanics and design of the flexible spine are explored, and a prototype of a link utilising it is developed and compared with limb segments based on granular jamming and layer jamming without support structure. Results of experiments verify the advantages of the proposed design, demonstrating that it maintains a constant central diameter across bending angles and presents an improvement of more than 203% of resisting force at 180 degrees.","sentences":["Robotic arms built from stiffness-adjustable, continuously bending segments serially connected with revolute joints have the ability to change their mechanical architecture and workspace, thus allowing high flexibility and adaptation to different tasks with less than six degrees of freedom, a concept that we call malleable robots.","Known stiffening mechanisms may be used to implement suitable links for these novel robotic manipulators; however, these solutions usually show a reduced performance when bending due to structural deformation.","By including an inner support structure this deformation can be minimised, resulting in an increased stiffening performance.","This paper presents a new multi-material spine-inspired flexible structure for providing support in stiffness-controllable layer-jamming-based robotic links of large diameter.","The proposed spine mechanism is highly movable with type and range of motions that match those of a robotic link using solely layer jamming, whilst maintaining a hollow and light structure.","The mechanics and design of the flexible spine are explored, and a prototype of a link utilising it is developed and compared with limb segments based on granular jamming and layer jamming without support structure.","Results of experiments verify the advantages of the proposed design, demonstrating that it maintains a constant central diameter across bending angles and presents an improvement of more than 203% of resisting force at 180 degrees."],"url":"http://arxiv.org/abs/2404.09653v1","category":"cs.RO"}
{"created":"2024-04-15 09:46:12","title":"Safeguarding adaptive methods: global convergence of Barzilai-Borwein and other stepsize choices","abstract":"Leveraging on recent advancements on adaptive methods for convex minimization problems, this paper provides a linesearch-free proximal gradient framework for globalizing the convergence of popular stepsize choices such as Barzilai-Borwein and one-dimensional Anderson acceleration. This framework can cope with problems in which the gradient of the differentiable function is merely locally H\\\"older continuous. Our analysis not only encompasses but also refines existing results upon which it builds. The theory is corroborated by numerical evidence that showcases the synergetic interplay between fast stepsize selections and adaptive methods.","sentences":["Leveraging on recent advancements on adaptive methods for convex minimization problems, this paper provides a linesearch-free proximal gradient framework for globalizing the convergence of popular stepsize choices such as Barzilai-Borwein and one-dimensional Anderson acceleration.","This framework can cope with problems in which the gradient of the differentiable function is merely locally H\\\"older continuous.","Our analysis not only encompasses but also refines existing results upon which it builds.","The theory is corroborated by numerical evidence that showcases the synergetic interplay between fast stepsize selections and adaptive methods."],"url":"http://arxiv.org/abs/2404.09617v1","category":"math.OC"}
{"created":"2024-04-15 08:32:18","title":"The revenge of BiSeNet: Efficient Multi-Task Image Segmentation","abstract":"Recent advancements in image segmentation have focused on enhancing the efficiency of the models to meet the demands of real-time applications, especially on edge devices. However, existing research has primarily concentrated on single-task settings, especially on semantic segmentation, leading to redundant efforts and specialized architectures for different tasks. To address this limitation, we propose a novel architecture for efficient multi-task image segmentation, capable of handling various segmentation tasks without sacrificing efficiency or accuracy. We introduce BiSeNetFormer, that leverages the efficiency of two-stream semantic segmentation architectures and it extends them into a mask classification framework. Our approach maintains the efficient spatial and context paths to capture detailed and semantic information, respectively, while leveraging an efficient transformed-based segmentation head that computes the binary masks and class probabilities. By seamlessly supporting multiple tasks, namely semantic and panoptic segmentation, BiSeNetFormer offers a versatile solution for multi-task segmentation. We evaluate our approach on popular datasets, Cityscapes and ADE20K, demonstrating impressive inference speeds while maintaining competitive accuracy compared to state-of-the-art architectures. Our results indicate that BiSeNetFormer represents a significant advancement towards fast, efficient, and multi-task segmentation networks, bridging the gap between model efficiency and task adaptability.","sentences":["Recent advancements in image segmentation have focused on enhancing the efficiency of the models to meet the demands of real-time applications, especially on edge devices.","However, existing research has primarily concentrated on single-task settings, especially on semantic segmentation, leading to redundant efforts and specialized architectures for different tasks.","To address this limitation, we propose a novel architecture for efficient multi-task image segmentation, capable of handling various segmentation tasks without sacrificing efficiency or accuracy.","We introduce BiSeNetFormer, that leverages the efficiency of two-stream semantic segmentation architectures and it extends them into a mask classification framework.","Our approach maintains the efficient spatial and context paths to capture detailed and semantic information, respectively, while leveraging an efficient transformed-based segmentation head that computes the binary masks and class probabilities.","By seamlessly supporting multiple tasks, namely semantic and panoptic segmentation, BiSeNetFormer offers a versatile solution for multi-task segmentation.","We evaluate our approach on popular datasets, Cityscapes and ADE20K, demonstrating impressive inference speeds while maintaining competitive accuracy compared to state-of-the-art architectures.","Our results indicate that BiSeNetFormer represents a significant advancement towards fast, efficient, and multi-task segmentation networks, bridging the gap between model efficiency and task adaptability."],"url":"http://arxiv.org/abs/2404.09570v1","category":"cs.CV"}
{"created":"2024-04-15 07:51:29","title":"Oblique-MERF: Revisiting and Improving MERF for Oblique Photography","abstract":"Neural implicit fields have established a new paradigm for scene representation, with subsequent work achieving high-quality real-time rendering. However, reconstructing 3D scenes from oblique aerial photography presents unique challenges, such as varying spatial scale distributions and a constrained range of tilt angles, often resulting in high memory consumption and reduced rendering quality at extrapolated viewpoints. In this paper, we enhance MERF to accommodate these data characteristics by introducing an innovative adaptive occupancy plane optimized during the volume rendering process and a smoothness regularization term for view-dependent color to address these issues. Our approach, termed Oblique-MERF, surpasses state-of-the-art real-time methods by approximately 0.7 dB, reduces VRAM usage by about 40%, and achieves higher rendering frame rates with more realistic rendering outcomes across most viewpoints.","sentences":["Neural implicit fields have established a new paradigm for scene representation, with subsequent work achieving high-quality real-time rendering.","However, reconstructing 3D scenes from oblique aerial photography presents unique challenges, such as varying spatial scale distributions and a constrained range of tilt angles, often resulting in high memory consumption and reduced rendering quality at extrapolated viewpoints.","In this paper, we enhance MERF to accommodate these data characteristics by introducing an innovative adaptive occupancy plane optimized during the volume rendering process and a smoothness regularization term for view-dependent color to address these issues.","Our approach, termed Oblique-MERF, surpasses state-of-the-art real-time methods by approximately 0.7 dB, reduces VRAM usage by about 40%, and achieves higher rendering frame rates with more realistic rendering outcomes across most viewpoints."],"url":"http://arxiv.org/abs/2404.09531v1","category":"cs.CV"}
{"created":"2024-04-15 04:29:24","title":"Crooked indifferentiability of the Feistel Construction","abstract":"The Feistel construction is a fundamental technique for building pseudorandom permutations and block ciphers. This paper shows that a simple adaptation of the construction is resistant, even to algorithm substitution attacks -- that is, adversarial subversion -- of the component round functions. Specifically, we establish that a Feistel-based construction with more than $2000n/\\log(1/\\epsilon)$ rounds can transform a subverted random function -- which disagrees with the original one at a small fraction (denoted by $\\epsilon$) of inputs -- into an object that is \\emph{crooked-indifferentiable} from a random permutation, even if the adversary is aware of all the randomness used in the transformation. We also provide a lower bound showing that the construction cannot use fewer than $2n/\\log(1/\\epsilon)$ rounds to achieve crooked-indifferentiable security.","sentences":["The Feistel construction is a fundamental technique for building pseudorandom permutations and block ciphers.","This paper shows that a simple adaptation of the construction is resistant, even to algorithm substitution attacks -- that is, adversarial subversion -- of the component round functions.","Specifically, we establish that a Feistel-based construction with more than $2000n/\\log(1/\\epsilon)$ rounds can transform a subverted random function -- which disagrees with the original one at a small fraction (denoted by $\\epsilon$) of inputs -- into an object that is \\emph{crooked-indifferentiable} from a random permutation, even if the adversary is aware of all the randomness used in the transformation.","We also provide a lower bound showing that the construction cannot use fewer than $2n/\\log(1/\\epsilon)$ rounds to achieve crooked-indifferentiable security."],"url":"http://arxiv.org/abs/2404.09450v1","category":"cs.CR"}
{"created":"2024-04-14 20:24:13","title":"Extrapolation via Sawyer-type inequalities","abstract":"We present a multi-variable extension of Rubio de Francia's restricted weak-type extrapolation theory that does not involve Rubio de Francia's iteration algorithm; instead, we rely on the following Sawyer-type inequality for the weighted Hardy-Littlewood maximal operator $M_u$:   $$ \\left \\Vert \\frac{M_u (fv)}{v} \\right \\Vert_{L^{1,\\infty}(uv)} \\leq C_{u,v} \\Vert f \\Vert_{L^1(uv)}, \\quad u, \\, uv \\in A_{\\infty}. $$   Our approach can be adapted to recover weak-type $A_{\\vec P}$ extrapolation schemes, including an endpoint result that falls outside the classical theory.   Among the applications of our work, we highlight extending outside the Banach range the well-known equivalence between restricted weak-type and weak-type for characteristic functions, and obtaining mixed and restricted weak-type bounds with $A_{p}^{\\mathcal R}$ weights for relevant families of multi-variable operators, addressing the lack in the literature of these types of estimates. We also reveal several standalone properties of the class $A_{p}^{\\mathcal R}$.","sentences":["We present a multi-variable extension of Rubio de Francia's restricted weak-type extrapolation theory that does not involve Rubio de Francia's iteration algorithm; instead, we rely on the following Sawyer-type inequality for the weighted Hardy-Littlewood maximal operator $M_u$:   $$ \\left \\Vert \\frac{M_u","(fv)}{v} \\right \\Vert_{L^{1,\\infty}(uv)} \\leq C_{u,v} \\Vert f \\Vert_{L^1(uv)}, \\quad u, \\, uv \\in A_{\\infty}.","$$   Our approach can be adapted to recover weak-type $A_{\\vec P}$ extrapolation schemes, including an endpoint result that falls outside the classical theory.   ","Among the applications of our work, we highlight extending outside the Banach range the well-known equivalence between restricted weak-type and weak-type for characteristic functions, and obtaining mixed and restricted weak-type bounds with $A_{p}^{\\mathcal R}$ weights for relevant families of multi-variable operators, addressing the lack in the literature of these types of estimates.","We also reveal several standalone properties of the class $A_{p}^{\\mathcal R}$."],"url":"http://arxiv.org/abs/2404.09351v1","category":"math.FA"}
{"created":"2024-04-14 14:01:53","title":"JaFIn: Japanese Financial Instruction Dataset","abstract":"We construct an instruction dataset for the large language model (LLM) in the Japanese finance domain. Domain adaptation of language models, including LLMs, is receiving more attention as language models become more popular. This study demonstrates the effectiveness of domain adaptation through instruction tuning. To achieve this, we propose an instruction tuning data in Japanese called JaFIn, the Japanese Financial Instruction Dataset. JaFIn is manually constructed based on multiple data sources, including Japanese government websites, which provide extensive financial knowledge. We then utilize JaFIn to apply instruction tuning for several LLMs, demonstrating that our models specialized in finance have better domain adaptability than the original models. The financial-specialized LLMs created were evaluated using a quantitative Japanese financial benchmark and qualitative response comparisons, showing improved performance over the originals.","sentences":["We construct an instruction dataset for the large language model (LLM) in the Japanese finance domain.","Domain adaptation of language models, including LLMs, is receiving more attention as language models become more popular.","This study demonstrates the effectiveness of domain adaptation through instruction tuning.","To achieve this, we propose an instruction tuning data in Japanese called JaFIn, the Japanese Financial Instruction Dataset.","JaFIn is manually constructed based on multiple data sources, including Japanese government websites, which provide extensive financial knowledge.","We then utilize JaFIn to apply instruction tuning for several LLMs, demonstrating that our models specialized in finance have better domain adaptability than the original models.","The financial-specialized LLMs created were evaluated using a quantitative Japanese financial benchmark and qualitative response comparisons, showing improved performance over the originals."],"url":"http://arxiv.org/abs/2404.09260v1","category":"cs.CL"}
{"created":"2024-04-14 13:48:24","title":"Foundational GPT Model for MEG","abstract":"Deep learning techniques can be used to first training unsupervised models on large amounts of unlabelled data, before fine-tuning the models on specific tasks. This approach has seen massive success for various kinds of data, e.g. images, language, audio, and holds the promise of improving performance in various downstream tasks (e.g. encoding or decoding brain data). However, there has been limited progress taking this approach for modelling brain signals, such as Magneto-/electroencephalography (M/EEG). Here we propose two classes of deep learning foundational models that can be trained using forecasting of unlabelled MEG. First, we consider a modified Wavenet; and second, we consider a modified Transformer-based (GPT2) model. The modified GPT2 includes a novel application of tokenisation and embedding methods, allowing a model developed initially for the discrete domain of language to be applied to continuous multichannel time series data. We also extend the forecasting framework to include condition labels as inputs, enabling better modelling (encoding) of task data. We compare the performance of these deep learning models with standard linear autoregressive (AR) modelling on MEG data. This shows that GPT2-based models provide better modelling capabilities than Wavenet and linear AR models, by better reproducing the temporal, spatial and spectral characteristics of real data and evoked activity in task data. We show how the GPT2 model scales well to multiple subjects, while adapting its model to each subject through subject embedding. Finally, we show how such a model can be useful in downstream decoding tasks through data simulation. All code is available on GitHub (https://github.com/ricsinaruto/MEG-transfer-decoding).","sentences":["Deep learning techniques can be used to first training unsupervised models on large amounts of unlabelled data, before fine-tuning the models on specific tasks.","This approach has seen massive success for various kinds of data, e.g. images, language, audio, and holds the promise of improving performance in various downstream tasks (e.g. encoding or decoding brain data).","However, there has been limited progress taking this approach for modelling brain signals, such as Magneto-/electroencephalography (M/EEG).","Here we propose two classes of deep learning foundational models that can be trained using forecasting of unlabelled MEG.","First, we consider a modified Wavenet; and second, we consider a modified Transformer-based (GPT2) model.","The modified GPT2 includes a novel application of tokenisation and embedding methods, allowing a model developed initially for the discrete domain of language to be applied to continuous multichannel time series data.","We also extend the forecasting framework to include condition labels as inputs, enabling better modelling (encoding) of task data.","We compare the performance of these deep learning models with standard linear autoregressive (AR) modelling on MEG data.","This shows that GPT2-based models provide better modelling capabilities than Wavenet and linear AR models, by better reproducing the temporal, spatial and spectral characteristics of real data and evoked activity in task data.","We show how the GPT2 model scales well to multiple subjects, while adapting its model to each subject through subject embedding.","Finally, we show how such a model can be useful in downstream decoding tasks through data simulation.","All code is available on GitHub (https://github.com/ricsinaruto/MEG-transfer-decoding)."],"url":"http://arxiv.org/abs/2404.09256v1","category":"cs.LG"}
{"created":"2024-04-14 12:19:16","title":"Tri-modal Confluence with Temporal Dynamics for Scene Graph Generation in Operating Rooms","abstract":"A comprehensive understanding of surgical scenes allows for monitoring of the surgical process, reducing the occurrence of accidents and enhancing efficiency for medical professionals. Semantic modeling within operating rooms, as a scene graph generation (SGG) task, is challenging since it involves consecutive recognition of subtle surgical actions over prolonged periods. To address this challenge, we propose a Tri-modal (i.e., images, point clouds, and language) confluence with Temporal dynamics framework, termed TriTemp-OR. Diverging from previous approaches that integrated temporal information via memory graphs, our method embraces two advantages: 1) we directly exploit bi-modal temporal information from the video streaming for hierarchical feature interaction, and 2) the prior knowledge from Large Language Models (LLMs) is embedded to alleviate the class-imbalance problem in the operating theatre. Specifically, our model performs temporal interactions across 2D frames and 3D point clouds, including a scale-adaptive multi-view temporal interaction (ViewTemp) and a geometric-temporal point aggregation (PointTemp). Furthermore, we transfer knowledge from the biomedical LLM, LLaVA-Med, to deepen the comprehension of intraoperative relations. The proposed TriTemp-OR enables the aggregation of tri-modal features through relation-aware unification to predict relations so as to generate scene graphs. Experimental results on the 4D-OR benchmark demonstrate the superior performance of our model for long-term OR streaming.","sentences":["A comprehensive understanding of surgical scenes allows for monitoring of the surgical process, reducing the occurrence of accidents and enhancing efficiency for medical professionals.","Semantic modeling within operating rooms, as a scene graph generation (SGG) task, is challenging since it involves consecutive recognition of subtle surgical actions over prolonged periods.","To address this challenge, we propose a Tri-modal (i.e., images, point clouds, and language) confluence with Temporal dynamics framework, termed TriTemp-OR.","Diverging from previous approaches that integrated temporal information via memory graphs, our method embraces two advantages: 1) we directly exploit bi-modal temporal information from the video streaming for hierarchical feature interaction, and 2) the prior knowledge from Large Language Models (LLMs) is embedded to alleviate the class-imbalance problem in the operating theatre.","Specifically, our model performs temporal interactions across 2D frames and 3D point clouds, including a scale-adaptive multi-view temporal interaction (ViewTemp) and a geometric-temporal point aggregation (PointTemp).","Furthermore, we transfer knowledge from the biomedical LLM, LLaVA-Med, to deepen the comprehension of intraoperative relations.","The proposed TriTemp-OR enables the aggregation of tri-modal features through relation-aware unification to predict relations so as to generate scene graphs.","Experimental results on the 4D-OR benchmark demonstrate the superior performance of our model for long-term OR streaming."],"url":"http://arxiv.org/abs/2404.09231v1","category":"cs.CV"}
{"created":"2024-04-14 10:41:48","title":"Physics-informed tracking of qubit fluctuations","abstract":"Environmental fluctuations degrade the performance of solid-state qubits but can in principle be mitigated by real-time Hamiltonian estimation down to time scales set by the estimation efficiency. We implement a physics-informed and an adaptive Bayesian estimation strategy and apply them in real time to a semiconductor spin qubit. The physics-informed strategy propagates a probability distribution inside the quantum controller according to the Fokker-Planck equation, appropriate for describing the effects of nuclear spin diffusion in gallium-arsenide. Evaluating and narrowing the anticipated distribution by a predetermined qubit probe sequence enables improved dynamical tracking of the uncontrolled magnetic field gradient within the singlet-triplet qubit. The adaptive strategy replaces the probe sequence by a small number of qubit probe cycles, with each probe time conditioned on the previous measurement outcomes, thereby further increasing the estimation efficiency. The combined real-time estimation strategy efficiently tracks low-frequency nuclear spin fluctuations in solid-state qubits, and can be applied to other qubit platforms by tailoring the appropriate update equation to capture their distinct noise sources.","sentences":["Environmental fluctuations degrade the performance of solid-state qubits but can in principle be mitigated by real-time Hamiltonian estimation down to time scales set by the estimation efficiency.","We implement a physics-informed and an adaptive Bayesian estimation strategy and apply them in real time to a semiconductor spin qubit.","The physics-informed strategy propagates a probability distribution inside the quantum controller according to the Fokker-Planck equation, appropriate for describing the effects of nuclear spin diffusion in gallium-arsenide.","Evaluating and narrowing the anticipated distribution by a predetermined qubit probe sequence enables improved dynamical tracking of the uncontrolled magnetic field gradient within the singlet-triplet qubit.","The adaptive strategy replaces the probe sequence by a small number of qubit probe cycles, with each probe time conditioned on the previous measurement outcomes, thereby further increasing the estimation efficiency.","The combined real-time estimation strategy efficiently tracks low-frequency nuclear spin fluctuations in solid-state qubits, and can be applied to other qubit platforms by tailoring the appropriate update equation to capture their distinct noise sources."],"url":"http://arxiv.org/abs/2404.09212v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-14 06:10:42","title":"Extreme quantile regression with deep learning","abstract":"Estimation of extreme conditional quantiles is often required for risk assessment of natural hazards in climate and geo-environmental sciences and for quantitative risk management in statistical finance, econometrics, and actuarial sciences. Interest often lies in extrapolating to quantile levels that exceed any past observations. Therefore, it is crucial to use a statistical framework that is well-adapted and especially designed for this purpose, and here extreme-value theory plays a key role. This chapter reviews how extreme quantile regression may be performed using theoretically-justified models, and how modern deep learning approaches can be harnessed in this context to enhance the model's performance in complex high-dimensional settings. The power of deep learning combined with the rigor of theoretically-justified extreme-value methods opens the door to efficient extreme quantile regression, in cases where both the number of covariates and the quantile level of interest can be simultaneously ``extreme''.","sentences":["Estimation of extreme conditional quantiles is often required for risk assessment of natural hazards in climate and geo-environmental sciences and for quantitative risk management in statistical finance, econometrics, and actuarial sciences.","Interest often lies in extrapolating to quantile levels that exceed any past observations.","Therefore, it is crucial to use a statistical framework that is well-adapted and especially designed for this purpose, and here extreme-value theory plays a key role.","This chapter reviews how extreme quantile regression may be performed using theoretically-justified models, and how modern deep learning approaches can be harnessed in this context to enhance the model's performance in complex high-dimensional settings.","The power of deep learning combined with the rigor of theoretically-justified extreme-value methods opens the door to efficient extreme quantile regression, in cases where both the number of covariates and the quantile level of interest can be simultaneously ``extreme''."],"url":"http://arxiv.org/abs/2404.09154v1","category":"stat.ME"}
{"created":"2024-04-14 05:58:52","title":"Learning Cross-hand Policies for High-DOF Reaching and Grasping","abstract":"Reaching-and-grasping is a fundamental skill for robotic manipulation, but existing methods usually train models on a specific gripper and cannot be reused on another gripper without retraining. In this paper, we propose a novel method that can learn a unified policy model that can be easily transferred to different dexterous grippers. Our method consists of two stages: a gripper-agnostic policy model that predicts the displacements of predefined key points on the gripper, and a gripper specific adaptation model that translates these displacements into adjustments for controlling the grippers' joints. The gripper state and interactions with objects are captured at the finger level using robust geometric representations, integrated with a transformer-based network to address variations in gripper morphology and geometry. In the experimental part, we evaluate our method on several dexterous grippers and objects of diverse shapes, and the result shows that our method significantly outperforms the baseline methods. Pioneering the transfer of grasp policies across different dexterous grippers, our method effectively demonstrates its potential for learning generalizable and transferable manipulation skills for various robotic hands","sentences":["Reaching-and-grasping is a fundamental skill for robotic manipulation, but existing methods usually train models on a specific gripper and cannot be reused on another gripper without retraining.","In this paper, we propose a novel method that can learn a unified policy model that can be easily transferred to different dexterous grippers.","Our method consists of two stages: a gripper-agnostic policy model that predicts the displacements of predefined key points on the gripper, and a gripper specific adaptation model that translates these displacements into adjustments for controlling the grippers' joints.","The gripper state and interactions with objects are captured at the finger level using robust geometric representations, integrated with a transformer-based network to address variations in gripper morphology and geometry.","In the experimental part, we evaluate our method on several dexterous grippers and objects of diverse shapes, and the result shows that our method significantly outperforms the baseline methods.","Pioneering the transfer of grasp policies across different dexterous grippers, our method effectively demonstrates its potential for learning generalizable and transferable manipulation skills for various robotic hands"],"url":"http://arxiv.org/abs/2404.09150v1","category":"cs.RO"}
{"created":"2024-04-14 05:10:05","title":"Controlling the False Discovery Rate in Subspace Selection","abstract":"Controlling the false discovery rate (FDR) is a popular approach to multiple testing, variable selection, and related problems of simultaneous inference. In many contemporary applications, models are not specified by discrete variables, which necessitates a broadening of the scope of the FDR control paradigm. Motivated by the ubiquity of low-rank models for high-dimensional matrices, we present methods for subspace selection in principal components analysis that provide control on a geometric analog of FDR that is adapted to subspace selection. Our methods crucially rely on recently-developed tools from random matrix theory, in particular on a characterization of the limiting behavior of eigenvectors and the gaps between successive eigenvalues of large random matrices. Our procedure is parameter-free, and we show that it provides FDR control in subspace selection for common noise models considered in the literature. We demonstrate the utility of our algorithm with numerical experiments on synthetic data and on problems arising in single-cell RNA sequencing and hyperspectral imaging.","sentences":["Controlling the false discovery rate (FDR) is a popular approach to multiple testing, variable selection, and related problems of simultaneous inference.","In many contemporary applications, models are not specified by discrete variables, which necessitates a broadening of the scope of the FDR control paradigm.","Motivated by the ubiquity of low-rank models for high-dimensional matrices, we present methods for subspace selection in principal components analysis that provide control on a geometric analog of FDR that is adapted to subspace selection.","Our methods crucially rely on recently-developed tools from random matrix theory, in particular on a characterization of the limiting behavior of eigenvectors and the gaps between successive eigenvalues of large random matrices.","Our procedure is parameter-free, and we show that it provides FDR control in subspace selection for common noise models considered in the literature.","We demonstrate the utility of our algorithm with numerical experiments on synthetic data and on problems arising in single-cell RNA sequencing and hyperspectral imaging."],"url":"http://arxiv.org/abs/2404.09142v1","category":"math.ST"}
{"created":"2024-04-13 15:28:52","title":"MING-MOE: Enhancing Medical Multi-Task Learning in Large Language Models with Sparse Mixture of Low-Rank Adapter Experts","abstract":"Large language models like ChatGPT have shown substantial progress in natural language understanding and generation, proving valuable across various disciplines, including the medical field. Despite advancements, challenges persist due to the complexity and diversity inherent in medical tasks which often require multi-task learning capabilities. Previous approaches, although beneficial, fall short in real-world applications because they necessitate task-specific annotations at inference time, limiting broader generalization. This paper introduces MING-MOE, a novel Mixture-of-Expert~(MOE)-based medical large language model designed to manage diverse and complex medical tasks without requiring task-specific annotations, thus enhancing its usability across extensive datasets. MING-MOE employs a Mixture of Low-Rank Adaptation (MoLoRA) technique, allowing for efficient parameter usage by maintaining base model parameters static while adapting through a minimal set of trainable parameters. We demonstrate that MING-MOE achieves state-of-the-art (SOTA) performance on over 20 medical tasks, illustrating a significant improvement over existing models. This approach not only extends the capabilities of medical language models but also improves inference efficiency.","sentences":["Large language models like ChatGPT have shown substantial progress in natural language understanding and generation, proving valuable across various disciplines, including the medical field.","Despite advancements, challenges persist due to the complexity and diversity inherent in medical tasks which often require multi-task learning capabilities.","Previous approaches, although beneficial, fall short in real-world applications because they necessitate task-specific annotations at inference time, limiting broader generalization.","This paper introduces MING-MOE, a novel Mixture-of-Expert~(MOE)-based medical large language model designed to manage diverse and complex medical tasks without requiring task-specific annotations, thus enhancing its usability across extensive datasets.","MING-MOE employs a Mixture of Low-Rank Adaptation (MoLoRA) technique, allowing for efficient parameter usage by maintaining base model parameters static while adapting through a minimal set of trainable parameters.","We demonstrate that MING-MOE achieves state-of-the-art (SOTA) performance on over 20 medical tasks, illustrating a significant improvement over existing models.","This approach not only extends the capabilities of medical language models but also improves inference efficiency."],"url":"http://arxiv.org/abs/2404.09027v1","category":"cs.CL"}
{"created":"2024-04-13 15:23:10","title":"Maximal tori in infinite-dimensional Hamiltonian systems: a Renormalization Group approach","abstract":"We study the existence of infinite-dimensional invariant tori in a mechanical system of infinitely many rotators weakly interacting with each other. We consider explicitly interactions depending only on the angles, with the aim of discussing in a simple case the analyticity properties to be required on the perturbation of the integrable system in order to ensure the persistence of a large measure set of invariant tori with finite energy. The proof we provide of the persistence of the invariant tori implements the Renormalization Group scheme based on the tree formalism -- i.e. the graphical representation of the solutions of the equations of motion in terms of trees -- which has been widely used in finite-dimensional problems. The method is very effectual and flexible: it naturally extends, once the functional setting has been fixed, to the infinite-dimensional case with only minor technical-natured adaptations.","sentences":["We study the existence of infinite-dimensional invariant tori in a mechanical system of infinitely many rotators weakly interacting with each other.","We consider explicitly interactions depending only on the angles, with the aim of discussing in a simple case the analyticity properties to be required on the perturbation of the integrable system in order to ensure the persistence of a large measure set of invariant tori with finite energy.","The proof we provide of the persistence of the invariant tori implements the Renormalization Group scheme based on the tree formalism -- i.e. the graphical representation of the solutions of the equations of motion in terms of trees -- which has been widely used in finite-dimensional problems.","The method is very effectual and flexible: it naturally extends, once the functional setting has been fixed, to the infinite-dimensional case with only minor technical-natured adaptations."],"url":"http://arxiv.org/abs/2404.09025v1","category":"math.DS"}
{"created":"2024-04-13 13:39:26","title":"MMA-DFER: MultiModal Adaptation of unimodal models for Dynamic Facial Expression Recognition in-the-wild","abstract":"Dynamic Facial Expression Recognition (DFER) has received significant interest in the recent years dictated by its pivotal role in enabling empathic and human-compatible technologies. Achieving robustness towards in-the-wild data in DFER is particularly important for real-world applications. One of the directions aimed at improving such models is multimodal emotion recognition based on audio and video data. Multimodal learning in DFER increases the model capabilities by leveraging richer, complementary data representations. Within the field of multimodal DFER, recent methods have focused on exploiting advances of self-supervised learning (SSL) for pre-training of strong multimodal encoders. Another line of research has focused on adapting pre-trained static models for DFER. In this work, we propose a different perspective on the problem and investigate the advancement of multimodal DFER performance by adapting SSL-pre-trained disjoint unimodal encoders. We identify main challenges associated with this task, namely, intra-modality adaptation, cross-modal alignment, and temporal adaptation, and propose solutions to each of them. As a result, we demonstrate improvement over current state-of-the-art on two popular DFER benchmarks, namely DFEW and MFAW.","sentences":["Dynamic Facial Expression Recognition (DFER) has received significant interest in the recent years dictated by its pivotal role in enabling empathic and human-compatible technologies.","Achieving robustness towards in-the-wild data in DFER is particularly important for real-world applications.","One of the directions aimed at improving such models is multimodal emotion recognition based on audio and video data.","Multimodal learning in DFER increases the model capabilities by leveraging richer, complementary data representations.","Within the field of multimodal DFER, recent methods have focused on exploiting advances of self-supervised learning (SSL) for pre-training of strong multimodal encoders.","Another line of research has focused on adapting pre-trained static models for DFER.","In this work, we propose a different perspective on the problem and investigate the advancement of multimodal DFER performance by adapting SSL-pre-trained disjoint unimodal encoders.","We identify main challenges associated with this task, namely, intra-modality adaptation, cross-modal alignment, and temporal adaptation, and propose solutions to each of them.","As a result, we demonstrate improvement over current state-of-the-art on two popular DFER benchmarks, namely DFEW and MFAW."],"url":"http://arxiv.org/abs/2404.09010v1","category":"cs.CV"}
{"created":"2024-04-13 13:25:58","title":"A Framework for Safe Probabilistic Invariance Verification of Stochastic Dynamical Systems","abstract":"Ensuring safety through set invariance has proven to be a valuable method in various robotics and control applications. This paper introduces a comprehensive framework for the safe probabilistic invariance verification of both discrete- and continuous-time stochastic dynamical systems over an infinite time horizon. The objective is to ascertain the lower and upper bounds of the liveness probability for a given safe set and set of initial states. This probability signifies the likelihood of the system remaining within the safe set indefinitely, starting from the set of initial states. To address this problem, we propose optimizations for verifying safe probabilistic invariance in discrete-time and continuous-time stochastic dynamical systems. These optimizations adapt classical stochastic barrier certificates, which are based on Doob's non-negative supermartingale inequality, and the equations described in [29],[31], which can precisely define the probability of reaching a target set while avoiding unsafe states. Finally, we demonstrate the effectiveness of these optimizations through several examples using semi-definite programming tools.","sentences":["Ensuring safety through set invariance has proven to be a valuable method in various robotics and control applications.","This paper introduces a comprehensive framework for the safe probabilistic invariance verification of both discrete- and continuous-time stochastic dynamical systems over an infinite time horizon.","The objective is to ascertain the lower and upper bounds of the liveness probability for a given safe set and set of initial states.","This probability signifies the likelihood of the system remaining within the safe set indefinitely, starting from the set of initial states.","To address this problem, we propose optimizations for verifying safe probabilistic invariance in discrete-time and continuous-time stochastic dynamical systems.","These optimizations adapt classical stochastic barrier certificates, which are based on Doob's non-negative supermartingale inequality, and the equations described in [29],[31], which can precisely define the probability of reaching a target set while avoiding unsafe states.","Finally, we demonstrate the effectiveness of these optimizations through several examples using semi-definite programming tools."],"url":"http://arxiv.org/abs/2404.09007v1","category":"eess.SY"}
{"created":"2024-04-13 12:09:37","title":"Fast Fishing: Approximating BAIT for Efficient and Scalable Deep Active Image Classification","abstract":"Deep active learning (AL) seeks to minimize the annotation costs for training deep neural networks. BAIT, a recently proposed AL strategy based on the Fisher Information, has demonstrated impressive performance across various datasets. However, BAIT's high computational and memory requirements hinder its applicability on large-scale classification tasks, resulting in current research neglecting BAIT in their evaluation. This paper introduces two methods to enhance BAIT's computational efficiency and scalability. Notably, we significantly reduce its time complexity by approximating the Fisher Information. In particular, we adapt the original formulation by i) taking the expectation over the most probable classes, and ii) constructing a binary classification task, leading to an alternative likelihood for gradient computations. Consequently, this allows the efficient use of BAIT on large-scale datasets, including ImageNet. Our unified and comprehensive evaluation across a variety of datasets demonstrates that our approximations achieve strong performance with considerably reduced time complexity. Furthermore, we provide an extensive open-source toolbox that implements recent state-of-the-art AL strategies, available at https://github.com/dhuseljic/dal-toolbox.","sentences":["Deep active learning (AL) seeks to minimize the annotation costs for training deep neural networks.","BAIT, a recently proposed AL strategy based on the Fisher Information, has demonstrated impressive performance across various datasets.","However, BAIT's high computational and memory requirements hinder its applicability on large-scale classification tasks, resulting in current research neglecting BAIT in their evaluation.","This paper introduces two methods to enhance BAIT's computational efficiency and scalability.","Notably, we significantly reduce its time complexity by approximating the Fisher Information.","In particular, we adapt the original formulation by i) taking the expectation over the most probable classes, and ii) constructing a binary classification task, leading to an alternative likelihood for gradient computations.","Consequently, this allows the efficient use of BAIT on large-scale datasets, including ImageNet.","Our unified and comprehensive evaluation across a variety of datasets demonstrates that our approximations achieve strong performance with considerably reduced time complexity.","Furthermore, we provide an extensive open-source toolbox that implements recent state-of-the-art AL strategies, available at https://github.com/dhuseljic/dal-toolbox."],"url":"http://arxiv.org/abs/2404.08981v1","category":"cs.CV"}
{"created":"2024-04-13 11:40:05","title":"PraFFL: A Preference-Aware Scheme in Fair Federated Learning","abstract":"Fairness in federated learning has emerged as a critical concern, aiming to develop an unbiased model for any special group (e.g., male or female) of sensitive features. However, there is a trade-off between model performance and fairness, i.e., improving fairness will decrease model performance. Existing approaches have characterized such a trade-off by introducing hyperparameters to quantify client's preferences for fairness and model performance. Nevertheless, these methods are limited to scenarios where each client has only a single pre-defined preference. In practical systems, each client may simultaneously have multiple preferences for the model performance and fairness. The key challenge is to design a method that allows the model to adapt to diverse preferences of each client in real time. To this end, we propose a Preference-aware scheme in Fair Federated Learning paradigm (called PraFFL). PraFFL can adaptively adjust the model based on each client's preferences to meet their needs. We theoretically prove that PraFFL can provide the optimal model for client's arbitrary preferences. Experimental results show that our proposed PraFFL outperforms five existing fair federated learning algorithms in terms of the model's capability in adapting to clients' different preferences.","sentences":["Fairness in federated learning has emerged as a critical concern, aiming to develop an unbiased model for any special group (e.g., male or female) of sensitive features.","However, there is a trade-off between model performance and fairness, i.e., improving fairness will decrease model performance.","Existing approaches have characterized such a trade-off by introducing hyperparameters to quantify client's preferences for fairness and model performance.","Nevertheless, these methods are limited to scenarios where each client has only a single pre-defined preference.","In practical systems, each client may simultaneously have multiple preferences for the model performance and fairness.","The key challenge is to design a method that allows the model to adapt to diverse preferences of each client in real time.","To this end, we propose a Preference-aware scheme in Fair Federated Learning paradigm (called PraFFL).","PraFFL can adaptively adjust the model based on each client's preferences to meet their needs.","We theoretically prove that PraFFL can provide the optimal model for client's arbitrary preferences.","Experimental results show that our proposed PraFFL outperforms five existing fair federated learning algorithms in terms of the model's capability in adapting to clients' different preferences."],"url":"http://arxiv.org/abs/2404.08973v1","category":"cs.LG"}
{"created":"2024-04-13 11:22:53","title":"Concentration properties of fractional posterior in 1-bit matrix completion","abstract":"The problem of estimating a matrix based on a set of its observed entries is commonly referred to as the matrix completion problem. In this work, we specifically address the scenario of binary observations, often termed as 1-bit matrix completion. While numerous studies have explored Bayesian and frequentist methods for real-value matrix completion, there has been a lack of theoretical exploration regarding Bayesian approaches in 1-bit matrix completion. We tackle this gap by considering a general, non-uniform sampling scheme and providing theoretical assurances on the efficacy of the fractional posterior. Our contributions include obtaining concentration results for the fractional posterior and demonstrating its effectiveness in recovering the underlying parameter matrix. We accomplish this using two distinct types of prior distributions: low-rank factorization priors and a spectral scaled Student prior, with the latter requiring fewer assumptions. Importantly, our results exhibit an adaptive nature by not mandating prior knowledge of the rank of the parameter matrix. Our findings are comparable to those found in the frequentist literature, yet demand fewer restrictive assumptions.","sentences":["The problem of estimating a matrix based on a set of its observed entries is commonly referred to as the matrix completion problem.","In this work, we specifically address the scenario of binary observations, often termed as 1-bit matrix completion.","While numerous studies have explored Bayesian and frequentist methods for real-value matrix completion, there has been a lack of theoretical exploration regarding Bayesian approaches in 1-bit matrix completion.","We tackle this gap by considering a general, non-uniform sampling scheme and providing theoretical assurances on the efficacy of the fractional posterior.","Our contributions include obtaining concentration results for the fractional posterior and demonstrating its effectiveness in recovering the underlying parameter matrix.","We accomplish this using two distinct types of prior distributions: low-rank factorization priors and a spectral scaled Student prior, with the latter requiring fewer assumptions.","Importantly, our results exhibit an adaptive nature by not mandating prior knowledge of the rank of the parameter matrix.","Our findings are comparable to those found in the frequentist literature, yet demand fewer restrictive assumptions."],"url":"http://arxiv.org/abs/2404.08969v1","category":"stat.ML"}
{"created":"2024-04-13 11:01:26","title":"Fabry-Perot superconducting diode","abstract":"Superconducting diode effects (SDEs) occur in systems with asymmetric critical supercurrents $|I^c_+|\\neq |I^c_-|$ yielding dissipationless flow in one direction $(e.g., +)$, while dissipative transport in the opposite direction $(-)$. Here we investigate the SDE in a phase-biased $\\phi$ Josephson junction with a double-barrier resonant-tunneling InAs nanowire nested between proximitized InAs/Al leads with finite momentum $\\hbar q$ Cooper pairing. Within the Bogoliubov-de Gennes (BdG) approach, we obtain the exact BCS ground state energy $\\mathcal{E}_G(q,\\phi)$ and $I^{c}_{+} \\neq |I^{c}_{-}|$ from the current-phase relation $I_G(q,\\phi) \\sim \\partial_{\\phi}\\mathcal{E}_G(q,\\phi)$. The SDE arises from the accrued Andreev phase shifts $\\delta \\phi_{L,R}(q,\\phi)$ leading to asymmetric BdG spectra for $q\\neq 0$. Remarkably, the diode efficiency $\\gamma=(I^{c}_{+} - |I^{c}_{-}|)/(I^{c}_{+} + |I^{c}_{-}|)$ shows multiple Fabry-Perot resonances $\\gamma \\simeq 26\\%$ at the double-barrier Andreev bound states as the well depth $V_g$ is varied. Our $\\gamma$ also features sign reversals for increasing $q$ and high sensitiveness to fermion-parity transitions. The latter enables $I^{c}_{+} (\\phi_+)\\rightleftarrows I^{c}_{-}(\\phi_-)$ switchings over narrow phase windows, i.e., $\\phi_+, \\phi_- \\in \\Delta \\phi\\ll\\pi$, possibly relevant for future superconducting electronics.","sentences":["Superconducting diode effects (SDEs) occur in systems with asymmetric critical supercurrents $|I^c_+|\\neq |I^c_-|$ yielding dissipationless flow in one direction $(e.g., +)$, while dissipative transport in the opposite direction $(-)$. Here we investigate the SDE in a phase-biased $\\phi$ Josephson junction with a double-barrier resonant-tunneling InAs nanowire nested between proximitized InAs/Al leads with finite momentum $\\hbar q$ Cooper pairing.","Within the Bogoliubov-de Gennes (BdG) approach, we obtain the exact BCS ground state energy $\\mathcal{E}_G(q,\\phi)$ and $I^{c}_{+} \\neq |I^{c}_{-}|$ from the current-phase relation $I_G(q,\\phi) \\sim \\partial_{\\phi}\\mathcal{E}_G(q,\\phi)$. The SDE arises from the accrued Andreev phase shifts $\\delta \\phi_{L,R}(q,\\phi)$ leading to asymmetric BdG spectra for $q\\neq 0$.","Remarkably, the diode efficiency $\\gamma=(I^{c}_{+} - |I^{c}_{-}|)/(I^{c}_{+} + |I^{c}_{-}|)$ shows multiple Fabry-Perot resonances $\\gamma \\simeq 26\\%$ at the double-barrier Andreev bound states as the well depth","$V_g$ is varied.","Our $\\gamma$ also features sign reversals for increasing $q$ and high sensitiveness to fermion-parity transitions.","The latter enables $I^{c}_{+} (\\phi_+)\\rightleftarrows I^{c}_{-}(\\phi_-)$ switchings over narrow phase windows, i.e., $\\phi_+, \\phi_- \\in \\Delta \\phi\\ll\\pi$, possibly relevant for future superconducting electronics."],"url":"http://arxiv.org/abs/2404.08962v1","category":"cond-mat.supr-con"}
{"created":"2024-04-13 10:55:02","title":"Queues with resetting: a perspective","abstract":"Performance modeling is a key issue in queuing theory and operation research. It is well-known that the length of a queue that awaits service or the time spent by a job in a queue depends not only on the service rate, but also crucially on the fluctuations in service time. The larger the fluctuations, the longer the delay becomes and hence, this is a major hindrance for the queue to operate efficiently. Various strategies have been adapted to prevent this drawback. In this perspective, we investigate the effects of one such novel strategy namely resetting or restart, an emerging concept in statistical physics and stochastic complex process, that was recently introduced to mitigate fluctuations-induced delays in queues. In particular, we show that a service resetting mechanism accompanied with an overhead time can remarkably shorten the average queue lengths and waiting times. We examine various resetting strategies and further shed light on the intricate role of the overhead times to the queuing performance. Our analysis opens up future avenues in operation research where resetting-based strategies can be universally promising.","sentences":["Performance modeling is a key issue in queuing theory and operation research.","It is well-known that the length of a queue that awaits service or the time spent by a job in a queue depends not only on the service rate, but also crucially on the fluctuations in service time.","The larger the fluctuations, the longer the delay becomes and hence, this is a major hindrance for the queue to operate efficiently.","Various strategies have been adapted to prevent this drawback.","In this perspective, we investigate the effects of one such novel strategy namely resetting or restart, an emerging concept in statistical physics and stochastic complex process, that was recently introduced to mitigate fluctuations-induced delays in queues.","In particular, we show that a service resetting mechanism accompanied with an overhead time can remarkably shorten the average queue lengths and waiting times.","We examine various resetting strategies and further shed light on the intricate role of the overhead times to the queuing performance.","Our analysis opens up future avenues in operation research where resetting-based strategies can be universally promising."],"url":"http://arxiv.org/abs/2404.08961v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-13 10:15:51","title":"Constructing and Exploring Intermediate Domains in Mixed Domain Semi-supervised Medical Image Segmentation","abstract":"Both limited annotation and domain shift are prevalent challenges in medical image segmentation. Traditional semi-supervised segmentation and unsupervised domain adaptation methods address one of these issues separately. However, the coexistence of limited annotation and domain shift is quite common, which motivates us to introduce a novel and challenging scenario: Mixed Domain Semi-supervised medical image Segmentation (MiDSS). In this scenario, we handle data from multiple medical centers, with limited annotations available for a single domain and a large amount of unlabeled data from multiple domains. We found that the key to solving the problem lies in how to generate reliable pseudo labels for the unlabeled data in the presence of domain shift with labeled data. To tackle this issue, we employ Unified Copy-Paste (UCP) between images to construct intermediate domains, facilitating the knowledge transfer from the domain of labeled data to the domains of unlabeled data. To fully utilize the information within the intermediate domain, we propose a symmetric Guidance training strategy (SymGD), which additionally offers direct guidance to unlabeled data by merging pseudo labels from intermediate samples. Subsequently, we introduce a Training Process aware Random Amplitude MixUp (TP-RAM) to progressively incorporate style-transition components into intermediate samples. Compared with existing state-of-the-art approaches, our method achieves a notable 13.57% improvement in Dice score on Prostate dataset, as demonstrated on three public datasets. Our code is available at https://github.com/MQinghe/MiDSS .","sentences":["Both limited annotation and domain shift are prevalent challenges in medical image segmentation.","Traditional semi-supervised segmentation and unsupervised domain adaptation methods address one of these issues separately.","However, the coexistence of limited annotation and domain shift is quite common, which motivates us to introduce a novel and challenging scenario: Mixed Domain Semi-supervised medical image Segmentation (MiDSS).","In this scenario, we handle data from multiple medical centers, with limited annotations available for a single domain and a large amount of unlabeled data from multiple domains.","We found that the key to solving the problem lies in how to generate reliable pseudo labels for the unlabeled data in the presence of domain shift with labeled data.","To tackle this issue, we employ Unified Copy-Paste (UCP) between images to construct intermediate domains, facilitating the knowledge transfer from the domain of labeled data to the domains of unlabeled data.","To fully utilize the information within the intermediate domain, we propose a symmetric Guidance training strategy (SymGD), which additionally offers direct guidance to unlabeled data by merging pseudo labels from intermediate samples.","Subsequently, we introduce a Training Process aware Random Amplitude MixUp (TP-RAM) to progressively incorporate style-transition components into intermediate samples.","Compared with existing state-of-the-art approaches, our method achieves a notable 13.57% improvement in Dice score on Prostate dataset, as demonstrated on three public datasets.","Our code is available at https://github.com/MQinghe/MiDSS ."],"url":"http://arxiv.org/abs/2404.08951v1","category":"cs.CV"}
{"created":"2024-04-13 09:24:32","title":"Enforcing Paraphrase Generation via Controllable Latent Diffusion","abstract":"Paraphrase generation aims to produce high-quality and diverse utterances of a given text. Though state-of-the-art generation via the diffusion model reconciles generation quality and diversity, textual diffusion suffers from a truncation issue that hinders efficiency and quality control. In this work, we propose \\textit{L}atent \\textit{D}iffusion \\textit{P}araphraser~(LDP), a novel paraphrase generation by modeling a controllable diffusion process given a learned latent space. LDP achieves superior generation efficiency compared to its diffusion counterparts. It facilitates only input segments to enforce paraphrase semantics, which further improves the results without external features. Experiments show that LDP achieves improved and diverse paraphrase generation compared to baselines. Further analysis shows that our method is also helpful to other similar text generations and domain adaptations. Our code and data are available at https://github.com/NIL-zhuang/ld4pg.","sentences":["Paraphrase generation aims to produce high-quality and diverse utterances of a given text.","Though state-of-the-art generation via the diffusion model reconciles generation quality and diversity, textual diffusion suffers from a truncation issue that hinders efficiency and quality control.","In this work, we propose \\textit{L}atent \\textit{D}iffusion \\textit{P}araphraser~(LDP), a novel paraphrase generation by modeling a controllable diffusion process given a learned latent space.","LDP achieves superior generation efficiency compared to its diffusion counterparts.","It facilitates only input segments to enforce paraphrase semantics, which further improves the results without external features.","Experiments show that LDP achieves improved and diverse paraphrase generation compared to baselines.","Further analysis shows that our method is also helpful to other similar text generations and domain adaptations.","Our code and data are available at https://github.com/NIL-zhuang/ld4pg."],"url":"http://arxiv.org/abs/2404.08938v1","category":"cs.CL"}
{"created":"2024-04-13 08:55:49","title":"Assessment of a Multiphase Formulation of One-Dimensional Turbulence using Direct Numerical Simulation of a Decaying Turbulent Interfacial Flow","abstract":"The interaction between turbulence and surface tension is studied numerically using the one-dimensional-turbulence (ODT) model. ODT is a stochastic model simulating turbulent flow evolution along a notional one-dimensional line of sight by applying instantaneous maps that represent the effects of individual turbulent eddies on property fields. It provides affordable high resolution of interface creation and property gradients within each phase, which are key for capturing the local behavior as well as overall trends, and has been shown to reproduce the main features of an experimentally determined regime diagram for primary jet breakup. Here, ODT is used to investigate the interaction of turbulence with an initially planar interface. The notional flat interface is inserted into a periodic box of decaying homogeneous isotropic turbulence, simulated for a variety of turbulent Reynolds and Weber numbers. Unity density and viscosity ratios serve to focus solely on the interaction between fluid inertia and the surface-tension force. Statistical measures of interface surface density and spatial structure along the direction normal to the initial surface are compared to corresponding direct-numerical-simulation (DNS) data. Allowing the origin of the lateral coordinate system to follow the location of the median interface element improves the agreement between ODT and DNS, reflecting the absence of lateral non-vortical displacements in ODT. Beyond the DNS-accessible regime, ODT is shown to obey the predicted parameter dependencies of the Kolmogorov critical scale in both the inertial and dissipative turbulent-cascade sub-ranges. Notably, the probability density function of local fluctuations of the critical scale is found to collapse to a universal curve across both sub-ranges.","sentences":["The interaction between turbulence and surface tension is studied numerically using the one-dimensional-turbulence (ODT) model.","ODT is a stochastic model simulating turbulent flow evolution along a notional one-dimensional line of sight by applying instantaneous maps that represent the effects of individual turbulent eddies on property fields.","It provides affordable high resolution of interface creation and property gradients within each phase, which are key for capturing the local behavior as well as overall trends, and has been shown to reproduce the main features of an experimentally determined regime diagram for primary jet breakup.","Here, ODT is used to investigate the interaction of turbulence with an initially planar interface.","The notional flat interface is inserted into a periodic box of decaying homogeneous isotropic turbulence, simulated for a variety of turbulent Reynolds and Weber numbers.","Unity density and viscosity ratios serve to focus solely on the interaction between fluid inertia and the surface-tension force.","Statistical measures of interface surface density and spatial structure along the direction normal to the initial surface are compared to corresponding direct-numerical-simulation (DNS) data.","Allowing the origin of the lateral coordinate system to follow the location of the median interface element improves the agreement between ODT and DNS, reflecting the absence of lateral non-vortical displacements in ODT.","Beyond the DNS-accessible regime, ODT is shown to obey the predicted parameter dependencies of the Kolmogorov critical scale in both the inertial and dissipative turbulent-cascade sub-ranges.","Notably, the probability density function of local fluctuations of the critical scale is found to collapse to a universal curve across both sub-ranges."],"url":"http://arxiv.org/abs/2404.08934v1","category":"physics.flu-dyn"}
{"created":"2024-04-15 12:51:51","title":"Layered Uploading for Quantum Convolutional Neural Networks","abstract":"Continuing our analysis of quantum machine learning applied to our use-case of malware detection, we investigate the potential of quantum convolutional neural networks. More precisely, we propose a new architecture where data is uploaded all along the quantum circuit. This allows us to use more features from the data, hence giving to the algorithm more information, without having to increase the number of qubits that we use for the quantum circuit. This approach is motivated by the fact that we do not always have great amounts of data, and that quantum computers are currently restricted in their number of logical qubits.","sentences":["Continuing our analysis of quantum machine learning applied to our use-case of malware detection, we investigate the potential of quantum convolutional neural networks.","More precisely, we propose a new architecture where data is uploaded all along the quantum circuit.","This allows us to use more features from the data, hence giving to the algorithm more information, without having to increase the number of qubits that we use for the quantum circuit.","This approach is motivated by the fact that we do not always have great amounts of data, and that quantum computers are currently restricted in their number of logical qubits."],"url":"http://arxiv.org/abs/2404.09750v1","category":"quant-ph"}
{"created":"2024-04-15 11:59:57","title":"Revisiting the barometric equation and the extent of a planetary atmosphere","abstract":"The standard barometric equation predicts the molecular concentration $n(z)=n_0\\exp(-z/L)$ where $L=k_BT/mg$. Because the mean free path $l=1/n\\sigma$ increases exponentially, we show that at high altitudes $z$, the equation is no longer within the domain of applicability of the standard kinetic theory $l\\ll L$. Here, we predict the dependence $n(z)\\propto z^{-2}$ for the case $l\\gg L$ in uniform gravity. It corresponds to a non-stationary planetary atmosphere with hydrogen accretion. The predicted accretion is accompanied by a release of gravitational potential energy that leads to heating of the atmosphere. In that context, we suggest gravitational energy could be the elusive source that drives the formation of stellar coronas. Other consequences of accretion are: slowly decaying tails of planetary atmospheres, the existence of gas giants, and periodical hydrogen explosions of white dwarfs.","sentences":["The standard barometric equation predicts the molecular concentration $n(z)=n_0\\exp(-z/L)$ where $L=k_BT/mg$. Because the mean free path $l=1/n\\sigma$ increases exponentially, we show that at high altitudes $z$, the equation is no longer within the domain of applicability of the standard kinetic theory $l\\ll L$.","Here, we predict the dependence $n(z)\\propto z^{-2}$ for the case $l\\gg L$ in uniform gravity.","It corresponds to a non-stationary planetary atmosphere with hydrogen accretion.","The predicted accretion is accompanied by a release of gravitational potential energy that leads to heating of the atmosphere.","In that context, we suggest gravitational energy could be the elusive source that drives the formation of stellar coronas.","Other consequences of accretion are: slowly decaying tails of planetary atmospheres, the existence of gas giants, and periodical hydrogen explosions of white dwarfs."],"url":"http://arxiv.org/abs/2404.09700v1","category":"astro-ph.EP"}
{"created":"2024-04-15 10:47:00","title":"Physical properties of strong 1 < z < 3 Balmer and Paschen lines emitters observed with JWST","abstract":"The ultraviolet continuum traces young stars while the near-infrared unveils older stellar populations and dust-obscured regions. Balmer emission lines provide insights on gas properties and young stellar objects but are highly affected by dust attenuation. The near-infrared Paschen lines suffer less dust attenuation and can be used to measure star formation rates (SFRs) in star-forming regions obscured by dust clouds. We select 13 sources between redshifts 1 and 3 observed with HST, JWST/NIRCam and NIRSpec based on the availability of at least one Balmer and one Paschen line with S/N > 5. With a newly-developed version of CIGALE, we fit their hydrogen line equivalent widths (EWs) and photometric data. We assess the impacts of the removal of spectroscopic data by comparing the quality of the fits of the spectro-photometric data to those with photometric data only. We compare the single (BC03) vs binary (BPASS) stellar populations models in the fitting process of spectro-photometric data. We derive the differential attenuation and explore different attenuation recipes by fitting spectro-photometric data with BC03. For each stellar model and for each input dataset (with and without EWs), we quantify the deviation on the SFRs and stellar masses from the \"standard\" choice. On average, the SFRs are overestimated and the stellar masses are underestimated when EWs are not included as input data. We find a major contribution of the H${\\alpha}$ emission line to the broadband photometric measurements of our sources, and a trend of increasing contribution with specific SFR. Using the BPASS models has a significant impact on the derived SFRs and stellar masses. We show that a flexible attenuation recipe provides more accurate estimates of the dust attenuation parameters, especially the differential attenuation which agrees with the original value of Charlot & Fall (2000).","sentences":["The ultraviolet continuum traces young stars while the near-infrared unveils older stellar populations and dust-obscured regions.","Balmer emission lines provide insights on gas properties and young stellar objects but are highly affected by dust attenuation.","The near-infrared Paschen lines suffer less dust attenuation and can be used to measure star formation rates (SFRs) in star-forming regions obscured by dust clouds.","We select 13 sources between redshifts 1 and 3 observed with HST, JWST/NIRCam and NIRSpec based on the availability of at least one Balmer and one Paschen line with S/N > 5.","With a newly-developed version of CIGALE, we fit their hydrogen line equivalent widths (EWs) and photometric data.","We assess the impacts of the removal of spectroscopic data by comparing the quality of the fits of the spectro-photometric data to those with photometric data only.","We compare the single (BC03) vs binary (BPASS) stellar populations models in the fitting process of spectro-photometric data.","We derive the differential attenuation and explore different attenuation recipes by fitting spectro-photometric data with BC03.","For each stellar model and for each input dataset (with and without EWs), we quantify the deviation on the SFRs and stellar masses from the \"standard\" choice.","On average, the SFRs are overestimated and the stellar masses are underestimated when EWs are not included as input data.","We find a major contribution of the H${\\alpha}$ emission line to the broadband photometric measurements of our sources, and a trend of increasing contribution with specific SFR.","Using the BPASS models has a significant impact on the derived SFRs and stellar masses.","We show that a flexible attenuation recipe provides more accurate estimates of the dust attenuation parameters, especially the differential attenuation which agrees with the original value of Charlot & Fall (2000)."],"url":"http://arxiv.org/abs/2404.09659v1","category":"astro-ph.GA"}
{"created":"2024-04-15 09:57:29","title":"Hot Jupiter Diversity and the Onset of TiO/VO Revealed by a Large Grid of Non-Grey Global Circulation Models","abstract":"The population of hot Jupiters is extremely diverse, with large variations in their irradiation, period, gravity and chemical composition. To understand the intrinsic planet diversity through the observed population level trends, we explore the a-priori scatter in the population created by the different responses of atmospheric circulation to planetary parameters. We use the SPARC/MITgcm 3D global circulation model to simulate 345 planets spanning a wide range of instellation, metallicity, gravity and rotation periods typical for hot Jupiters, while differentiating between models with and without TiO/VO in their atmosphere. We show that the combined effect of the planetary parameters leads to a large diversity in the ability of atmospheres to transport heat from day-side to night-side at a given equilibrium temperature. We further show that the hot-spot offset is a non-monotonic function of planetary rotation period and explain our findings by a competition between the rotational and divergent parts of the circulation. As a consequence, hot-spot offset and phase curve amplitude are not necessarily correlated. Finally, we compare the observables from our grid to the population of Spitzer and Hubble observations of hot Jupiters. We find that the sudden jump in brightness temperature observed in the Spitzer secondary eclipse measurements can be naturally explained by the cold-trapping of TiO/VO at approximately 1800K. The grid of modelled spectra, phase curves and thermal structures are made available to the community, together with a python code for visualization of the grid properties, at https://doi.org/10.5281/zenodo.10785321 and http://sim3d.oca.eu/.","sentences":["The population of hot Jupiters is extremely diverse, with large variations in their irradiation, period, gravity and chemical composition.","To understand the intrinsic planet diversity through the observed population level trends, we explore the a-priori scatter in the population created by the different responses of atmospheric circulation to planetary parameters.","We use the SPARC/MITgcm 3D global circulation model to simulate 345 planets spanning a wide range of instellation, metallicity, gravity and rotation periods typical for hot Jupiters, while differentiating between models with and without TiO/VO in their atmosphere.","We show that the combined effect of the planetary parameters leads to a large diversity in the ability of atmospheres to transport heat from day-side to night-side at a given equilibrium temperature.","We further show that the hot-spot offset is a non-monotonic function of planetary rotation period and explain our findings by a competition between the rotational and divergent parts of the circulation.","As a consequence, hot-spot offset and phase curve amplitude are not necessarily correlated.","Finally, we compare the observables from our grid to the population of Spitzer and Hubble observations of hot Jupiters.","We find that the sudden jump in brightness temperature observed in the Spitzer secondary eclipse measurements can be naturally explained by the cold-trapping of TiO/VO at approximately 1800K.","The grid of modelled spectra, phase curves and thermal structures are made available to the community, together with a python code for visualization of the grid properties, at https://doi.org/10.5281/zenodo.10785321 and http://sim3d.oca.eu/."],"url":"http://arxiv.org/abs/2404.09626v1","category":"astro-ph.EP"}
{"created":"2024-04-15 09:33:06","title":"Well-Posedness for Quintic Energy Critical Wave in 3D Cylindrical Convex Domains","abstract":"In this paper, we establish the well-posedness in energy space for the quintic energy critical wave inside a cylindrical convex domain $\\Omega\\subset\\mathbb{R}^3$ with smooth boundary $\\partial\\Omega\\neq\\emptyset$. The key tools to prove local well-posedness are the dispersive estimates obtained in \\cite{L,L1,L3} and the Strichartz estimates in \\cite{L2}. We point out that our result on the local and global existence of the solution to the wave equation in the cylindrical domain setting interpolates between that of in Euclidean space $\\mathbb{R}^3$ (see \\cite{MG}) and in any bounded domains in $\\mathbb{R}^3$ (see \\cite{BLP}). Moreover, the result of the Strichartz estimates in our setting is strong enough when combined with the arguments in \\cite{BLP,SS2} so that we can extend local to global well-posedness.","sentences":["In this paper, we establish the well-posedness in energy space for the quintic energy critical wave inside a cylindrical convex domain $\\Omega\\subset\\mathbb{R}^3$ with smooth boundary $\\partial\\Omega\\neq\\emptyset$. The key tools to prove local well-posedness are the dispersive estimates obtained in \\cite{L,L1,L3} and the Strichartz estimates in \\cite{L2}.","We point out that our result on the local and global existence of the solution to the wave equation in the cylindrical domain setting interpolates between that of in Euclidean space $\\mathbb{R}^3$ (see \\cite{MG}) and in any bounded domains in $\\mathbb{R}^3$ (see \\cite{BLP}).","Moreover, the result of the Strichartz estimates in our setting is strong enough when combined with the arguments in \\cite{BLP,SS2} so that we can extend local to global well-posedness."],"url":"http://arxiv.org/abs/2404.09611v1","category":"math.AP"}
{"created":"2024-04-15 09:18:34","title":"Construction of smooth chiral finite-time blow-up solutions to Calogero--Moser derivative nonlinear Schr\u00f6dinger equation","abstract":"We consider the Calogero--Moser derivative nonlinear Schr\\\"odinger equation (CM-DNLS), which is an $L^{2}$-critical nonlinear Schr\\\"odinger equation with explicit solitons, self-duality, and pseudo-conformal symmetry. More importantly, this equation is known to be completely integrable in the Hardy space $L_{+}^{2}$ and the solutions in this class are referred to as \\emph{chiral} solutions. A rigorous PDE analysis of this equation with complete integrability was recently initiated by G\\'erard and Lenzmann.   Our main result constructs smooth, chiral, and finite energy finite-time blow-up solutions with mass arbitrarily close to that of soliton, answering the global regularity question for chiral solutions raised by G\\'erard and Lenzmann. The blow-up rate obtained for these solutions is different from the pseudo-conformal rate. Our proof also gives a construction of a codimension one set of smooth finite energy initial data (but without addressing chirality) leading to the same blow-up dynamics. Our blow-up construction in the Hardy space might also be contrasted with the global well-posedness of the derivative nonlinear Schr\\\"odinger equation (DNLS), which is another integrable $L^{2}$-critical Schr\\\"odinger equation.   The overall scheme of our proof is the forward construction of blow-up dynamics with modulation analysis and is not reliant on complete integrability. We begin with developing a linear theory for the near soliton dynamics. We discover a nontrivial conjugation identity, which unveils a surprising connection from the linearized (CM-DNLS) to the 1D free Schr\\\"odinger equation, which is a crucial ingredient for overcoming the difficulties from the non-local nonlinearity. Another principal challenge in this work, the slow decay of soliton, is overcome by introducing a trick of decomposing solutions depending on topologies, which we believe is of independent interest.","sentences":["We consider the Calogero--Moser derivative nonlinear Schr\\\"odinger equation (CM-DNLS), which is an $L^{2}$-critical nonlinear Schr\\\"odinger equation with explicit solitons, self-duality, and pseudo-conformal symmetry.","More importantly, this equation is known to be completely integrable in the Hardy space $L_{+}^{2}$ and the solutions in this class are referred to as \\emph{chiral} solutions.","A rigorous PDE analysis of this equation with complete integrability was recently initiated by G\\'erard and Lenzmann.   ","Our main result constructs smooth, chiral, and finite energy finite-time blow-up solutions with mass arbitrarily close to that of soliton, answering the global regularity question for chiral solutions raised by G\\'erard and Lenzmann.","The blow-up rate obtained for these solutions is different from the pseudo-conformal rate.","Our proof also gives a construction of a codimension one set of smooth finite energy initial data (but without addressing chirality) leading to the same blow-up dynamics.","Our blow-up construction in the Hardy space might also be contrasted with the global well-posedness of the derivative nonlinear Schr\\\"odinger equation (DNLS), which is another integrable $L^{2}$-critical Schr\\\"odinger equation.   ","The overall scheme of our proof is the forward construction of blow-up dynamics with modulation analysis and is not reliant on complete integrability.","We begin with developing a linear theory for the near soliton dynamics.","We discover a nontrivial conjugation identity, which unveils a surprising connection from the linearized (CM-DNLS) to the 1D free Schr\\\"odinger equation, which is a crucial ingredient for overcoming the difficulties from the non-local nonlinearity.","Another principal challenge in this work, the slow decay of soliton, is overcome by introducing a trick of decomposing solutions depending on topologies, which we believe is of independent interest."],"url":"http://arxiv.org/abs/2404.09603v1","category":"math.AP"}
{"created":"2024-04-15 09:16:55","title":"Guidelines for accurate and efficient calculations of mobilities in two-dimensional materials","abstract":"Emerging two-dimensional (2D) materials bring unprecedented opportunities for electronic applications. The design of high-performance devices requires an accurate prediction of carrier mobility in 2D materials, which can be obtained using state-of-the-art $ab~initio$ calculations. However, various factors impact the computational accuracy, leading to contradictory estimations for the mobility. In this work, targeting accurate and efficient $ab~initio$ calculations, transport properties in III-V monolayers are reported using the Boltzmann transport equation, and the influences of pseudopotential, quadrupole correction, Berry connection, and spin-orbit coupling (SOC) on mobilities are systematically investigated. Our findings are as follows: (1) The inclusion of semi-core states in pseudopotentials is important to obtain accurate calculations. (2) The variations induced by dynamical quadrupole and Berry connection when treating long range fields can be respectively 40% and 10%. (3) The impact of SOC can reach up to 100% for materials with multi-peak bands. Importantly, although SOC notably modifies the electronic wavefunctions, it negligibly impacts the dynamical matrices and scattering potential variations. As a result, the combination of fully-relativistic electron calculation and scalar-relativistic phonon calculation can strike a good balance between accuracy and cost. This work compares computational methodologies, providing guidelines for accurate and efficient calculations of mobilities in 2D semiconductors.","sentences":["Emerging two-dimensional (2D) materials bring unprecedented opportunities for electronic applications.","The design of high-performance devices requires an accurate prediction of carrier mobility in 2D materials, which can be obtained using state-of-the-art $ab~initio$ calculations.","However, various factors impact the computational accuracy, leading to contradictory estimations for the mobility.","In this work, targeting accurate and efficient $ab~initio$ calculations, transport properties in III-V monolayers are reported using the Boltzmann transport equation, and the influences of pseudopotential, quadrupole correction, Berry connection, and spin-orbit coupling (SOC) on mobilities are systematically investigated.","Our findings are as follows: (1) The inclusion of semi-core states in pseudopotentials is important to obtain accurate calculations.","(2) The variations induced by dynamical quadrupole and Berry connection when treating long range fields can be respectively 40% and 10%.","(3) The impact of SOC can reach up to 100% for materials with multi-peak bands.","Importantly, although SOC notably modifies the electronic wavefunctions, it negligibly impacts the dynamical matrices and scattering potential variations.","As a result, the combination of fully-relativistic electron calculation and scalar-relativistic phonon calculation can strike a good balance between accuracy and cost.","This work compares computational methodologies, providing guidelines for accurate and efficient calculations of mobilities in 2D semiconductors."],"url":"http://arxiv.org/abs/2404.09602v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-15 09:03:05","title":"Trinomials with high differential uniformity","abstract":"The context of this work is the study of the differential uniformity of polynomials defined over finite fields of even characteristic.We provide infinite families of trinomialswith high differential uniformitywhen the base field is large enough.It means in particular that these trinomials are not exceptional almost perfect nonlinear.","sentences":["The context of this work is the study of the differential uniformity of polynomials defined over finite fields of even characteristic.","We provide infinite families of trinomialswith high differential uniformitywhen the base field is large enough.","It means in particular that these trinomials are not exceptional almost perfect nonlinear."],"url":"http://arxiv.org/abs/2404.09594v1","category":"math.NT"}
{"created":"2024-04-15 08:34:34","title":"Strengthened injectivity radius bounds for manifolds with positive scalar curvature","abstract":"Green's inequality shows that a compact Riemannian manifold with scalar curvature at least $n(n-1)$ has injectivity radius at most $\\pi$, and that equality is achieved only for the radius 1 sphere. In this work we show how extra topological assumptions can lead to stronger upper bounds. The topologies we consider are $\\mathbb{S}^2\\times\\mathbb{T}^{n-k-2}\\times\\mathbb{R}^k$ for $n\\leq 7$ and $0\\leq k\\leq 2$ and 3-manifolds with positive scalar curvature except lens spaces $L(p,q)$ with $p$ odd. We also prove a strengthened inequality for $3$-manifolds with positive scalar curvature and large diameter. Our proof uses previous results of Gromov and Zhu.","sentences":["Green's inequality shows that a compact Riemannian manifold with scalar curvature at least $n(n-1)$ has injectivity radius at most $\\pi$, and that equality is achieved only for the radius 1 sphere.","In this work we show how extra topological assumptions can lead to stronger upper bounds.","The topologies we consider are $\\mathbb{S}^2\\times\\mathbb{T}^{n-k-2}\\times\\mathbb{R}^k$ for $n\\leq 7$ and $0\\leq k\\leq 2$ and 3-manifolds with positive scalar curvature except lens spaces $L(p,q)$ with $p$ odd.","We also prove a strengthened inequality for $3$-manifolds with positive scalar curvature and large diameter.","Our proof uses previous results of Gromov and Zhu."],"url":"http://arxiv.org/abs/2404.09573v1","category":"math.DG"}
{"created":"2024-04-15 08:21:29","title":"Optical control of topological end states via soliton formation in a 1D lattice","abstract":"Solitons are self-consistent solutions of the nonlinear Schr\\\"odinger equation that maintain their shape during propagation. Here we show, using a pump-probe technique, that soliton formation can be used to optically induce and control a linear topological end state in the bulk of a Su-Schrieffer-Heeger lattice, using evanescently-coupled waveguide arrays. Specifically, we observe an abrupt nonlinearly-induced transition above a certain power threshold due to an inversion symmetry-breaking nonlinear bifurcation. Our results demonstrate all-optical active control of topological states.","sentences":["Solitons are self-consistent solutions of the nonlinear Schr\\\"odinger equation that maintain their shape during propagation.","Here we show, using a pump-probe technique, that soliton formation can be used to optically induce and control a linear topological end state in the bulk of a Su-Schrieffer-Heeger lattice, using evanescently-coupled waveguide arrays.","Specifically, we observe an abrupt nonlinearly-induced transition above a certain power threshold due to an inversion symmetry-breaking nonlinear bifurcation.","Our results demonstrate all-optical active control of topological states."],"url":"http://arxiv.org/abs/2404.09560v1","category":"physics.optics"}
{"created":"2024-04-15 08:16:17","title":"Refined TMD gluon density in a proton from the HERA and LHC data","abstract":"We update the phenomenological parameters of the Transverse Momentum Dependent (TMD, or unintegrated) gluon density in a proton proposed in our previous studies. This analysis is based on the analytical expression for starting gluon distribution which provides a self-consistent simultaneous description of HERA data on proton structure function $F_2(x,Q^2)$, reduced cross section for the electron-proton deep inelastic scattering at low $Q^2$ and soft hadron production in $pp$ collisions at the LHC conditions. We extend it to the whole kinematical region using the Catani-Ciafaloni-Fiorani-Marchesini (CCFM) evolution equation. Exploiting our previous results on a number of semihard QCD processes, we performed a combined fit to an extended set of LHC and HERA data, comprising a total of $509$ points from $16$ data sets. We illustrate our fit by applying the derived TMD gluon density in a proton to inclusive prompt photon photoproduction at HERA.","sentences":["We update the phenomenological parameters of the Transverse Momentum Dependent (TMD, or unintegrated) gluon density in a proton proposed in our previous studies.","This analysis is based on the analytical expression for starting gluon distribution which provides a self-consistent simultaneous description of HERA data on proton structure function $F_2(x,Q^2)$, reduced cross section for the electron-proton deep inelastic scattering at low $Q^2$ and soft hadron production in $pp$ collisions at the LHC conditions.","We extend it to the whole kinematical region using the Catani-Ciafaloni-Fiorani-Marchesini (CCFM) evolution equation.","Exploiting our previous results on a number of semihard QCD processes, we performed a combined fit to an extended set of LHC and HERA data, comprising a total of $509$ points from $16$ data sets.","We illustrate our fit by applying the derived TMD gluon density in a proton to inclusive prompt photon photoproduction at HERA."],"url":"http://arxiv.org/abs/2404.09550v1","category":"hep-ph"}
{"created":"2024-04-15 08:11:12","title":"Van der Waals epitaxy of Weyl-semimetal Td-WTe$_2$","abstract":"Epitaxial growth of WTe$_2$ offers significant advantages, including the production of high-qualityfilms, possible long range in-plane ordering and precise control over layer thicknesses. However,the mean island size of WTe$_2$ grown by molecular beam epitaxy (MBE) in litterature is only a fewtens of nanometers, which is not suitable for an implementation of devices at large lateral scales.Here we report the growth of Td-WTe$_2$ ultrathin films by MBE on monolayer (ML) graphenereaching a mean flake size of $\\cong$110nm, which is, on overage, more than three time larger thanprevious results. WTe$_2$ films thicker than 5nm have been successfully synthesized and exhibit theexpected Td-phase atomic structure. We rationalize epitaxial growth of Td-WTe$_2$ and propose asimple model to estimate the mean flake size as a function of growth parameters that can be appliedto other transition metal dichalcogenides (TMDCs). Based on nucleation theory and Kolmogorov-Johnson-Meh-Avrami (KJMA) equation, our analytical model supports experimental data showinga critical coverage of 0.13ML above which WTe$_2$ nucleation becomes negligible. The quality ofmonolayer WTe$_2$ films is demonstrated from electronic band structure analysis using angle-resolved photoemission spectroscopy (ARPES) in agreement with first-principle calculationsperformed on free-standing WTe$_2$ and previous reports.","sentences":["Epitaxial growth of WTe$_2$ offers significant advantages, including the production of high-qualityfilms, possible long range in-plane ordering and precise control over layer thicknesses.","However,the mean island size of WTe$_2$ grown by molecular beam epitaxy (MBE) in litterature is only a fewtens of nanometers, which is not suitable for an implementation of devices at large lateral scales.","Here we report the growth of Td-WTe$_2$ ultrathin films by MBE on monolayer (ML) graphenereaching a mean flake size of $\\cong$110nm, which is, on overage, more than three time larger thanprevious results.","WTe$_2$ films thicker than 5nm have been successfully synthesized and exhibit theexpected Td-phase atomic structure.","We rationalize epitaxial growth of Td-WTe$_2$ and propose asimple model to estimate the mean flake size as a function of growth parameters that can be appliedto other transition metal dichalcogenides (TMDCs).","Based on nucleation theory and Kolmogorov-Johnson-Meh-Avrami (KJMA) equation, our analytical model supports experimental data showinga critical coverage of 0.13ML above which WTe$_2$ nucleation becomes negligible.","The quality ofmonolayer WTe$_2$ films is demonstrated from electronic band structure analysis using angle-resolved photoemission spectroscopy (ARPES) in agreement with first-principle calculationsperformed on free-standing WTe$_2$ and previous reports."],"url":"http://arxiv.org/abs/2404.09543v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-15 06:45:21","title":"Data-driven identification of reaction-diffusion dynamics from finitely many non-local noisy measurements by exponential fitting","abstract":"Given a reaction-diffusion equation with unknown right-hand side, we consider a nonlinear inverse problem of estimating the associated leading eigenvalues and initial condition modes from a finite number of non-local noisy measurements. We define a reconstruction criterion and, for a small enough noise, we prove the existence and uniqueness of the desired approximation and derive closed-form expressions for the first-order condition numbers, as well as bounds for their asymptotic behavior in a regime when the number of measured samples is fixed and the inter-sampling interval length tends to infinity. When computing the sought estimates numerically, our simulations show that the exponential fitting algorithm ESPRIT is first-order optimal, as its first-order condition numbers have the same asymptotic behavior as the analytic condition numbers in the considered regime.","sentences":["Given a reaction-diffusion equation with unknown right-hand side, we consider a nonlinear inverse problem of estimating the associated leading eigenvalues and initial condition modes from a finite number of non-local noisy measurements.","We define a reconstruction criterion and, for a small enough noise, we prove the existence and uniqueness of the desired approximation and derive closed-form expressions for the first-order condition numbers, as well as bounds for their asymptotic behavior in a regime when the number of measured samples is fixed and the inter-sampling interval length tends to infinity.","When computing the sought estimates numerically, our simulations show that the exponential fitting algorithm ESPRIT is first-order optimal, as its first-order condition numbers have the same asymptotic behavior as the analytic condition numbers in the considered regime."],"url":"http://arxiv.org/abs/2404.09503v1","category":"math.OC"}
{"created":"2024-04-15 06:38:33","title":"On-chip Real-time Hyperspectral Imager with Full CMOS Resolution Enabled by Massively Parallel Neural Network","abstract":"Traditional spectral imaging methods are constrained by the time-consuming scanning process, limiting the application in dynamic scenarios. One-shot spectral imaging based on reconstruction has been a hot research topic recently and the primary challenges still lie in both efficient fabrication techniques suitable for mass production and the high-speed, high-accuracy reconstruction algorithm for real-time spectral imaging. In this study, we introduce an innovative on-chip real-time hyperspectral imager that leverages nanophotonic film spectral encoders and a Massively Parallel Network (MP-Net), featuring a 4 * 4 array of compact, all-dielectric film units for the micro-spectrometers. Each curved nanophotonic film unit uniquely modulates incident light across the underlying 3 * 3 CMOS image sensor (CIS) pixels, enabling a high spatial resolution equivalent to the full CMOS resolution. The implementation of MP-Net, specially designed to address variability in transmittance and manufacturing errors such as misalignment and non-uniformities in thin film deposition, can greatly increase the structural tolerance of the device and reduce the preparation requirement, further simplifying the manufacturing process. Tested in varied environments on both static and moving objects, the real-time hyperspectral imager demonstrates the robustness and high-fidelity spatial-spectral data capabilities across diverse scenarios. This on-chip hyperspectral imager represents a significant advancement in real-time, high-resolution spectral imaging, offering a versatile solution for applications ranging from environmental monitoring, remote sensing to consumer electronics.","sentences":["Traditional spectral imaging methods are constrained by the time-consuming scanning process, limiting the application in dynamic scenarios.","One-shot spectral imaging based on reconstruction has been a hot research topic recently and the primary challenges still lie in both efficient fabrication techniques suitable for mass production and the high-speed, high-accuracy reconstruction algorithm for real-time spectral imaging.","In this study, we introduce an innovative on-chip real-time hyperspectral imager that leverages nanophotonic film spectral encoders and a Massively Parallel Network (MP-Net), featuring a 4 * 4 array of compact, all-dielectric film units for the micro-spectrometers.","Each curved nanophotonic film unit uniquely modulates incident light across the underlying 3 * 3 CMOS image sensor (CIS) pixels, enabling a high spatial resolution equivalent to the full CMOS resolution.","The implementation of MP-Net, specially designed to address variability in transmittance and manufacturing errors such as misalignment and non-uniformities in thin film deposition, can greatly increase the structural tolerance of the device and reduce the preparation requirement, further simplifying the manufacturing process.","Tested in varied environments on both static and moving objects, the real-time hyperspectral imager demonstrates the robustness and high-fidelity spatial-spectral data capabilities across diverse scenarios.","This on-chip hyperspectral imager represents a significant advancement in real-time, high-resolution spectral imaging, offering a versatile solution for applications ranging from environmental monitoring, remote sensing to consumer electronics."],"url":"http://arxiv.org/abs/2404.09500v1","category":"physics.optics"}
{"created":"2024-04-15 06:34:17","title":"Towards Efficient SRAM-PIM Architecture Design by Exploiting Unstructured Bit-Level Sparsity","abstract":"Bit-level sparsity in neural network models harbors immense untapped potential. Eliminating redundant calculations of randomly distributed zero-bits significantly boosts computational efficiency. Yet, traditional digital SRAM-PIM architecture, limited by rigid crossbar architecture, struggles to effectively exploit this unstructured sparsity. To address this challenge, we propose Dyadic Block PIM (DB-PIM), a groundbreaking algorithm-architecture co-design framework. First, we propose an algorithm coupled with a distinctive sparsity pattern, termed a dyadic block (DB), that preserves the random distribution of non-zero bits to maintain accuracy while restricting the number of these bits in each weight to improve regularity. Architecturally, we develop a custom PIM macro that includes dyadic block multiplication units (DBMUs) and Canonical Signed Digit (CSD)-based adder trees, specifically tailored for Multiply-Accumulate (MAC) operations. An input pre-processing unit (IPU) further refines performance and efficiency by capitalizing on block-wise input sparsity. Results show that our proposed co-design framework achieves a remarkable speedup of up to 7.69x and energy savings of 83.43%.","sentences":["Bit-level sparsity in neural network models harbors immense untapped potential.","Eliminating redundant calculations of randomly distributed zero-bits significantly boosts computational efficiency.","Yet, traditional digital SRAM-PIM architecture, limited by rigid crossbar architecture, struggles to effectively exploit this unstructured sparsity.","To address this challenge, we propose Dyadic Block PIM (DB-PIM), a groundbreaking algorithm-architecture co-design framework.","First, we propose an algorithm coupled with a distinctive sparsity pattern, termed a dyadic block (DB), that preserves the random distribution of non-zero bits to maintain accuracy while restricting the number of these bits in each weight to improve regularity.","Architecturally, we develop a custom PIM macro that includes dyadic block multiplication units (DBMUs) and Canonical Signed Digit (CSD)-based adder trees, specifically tailored for Multiply-Accumulate (MAC) operations.","An input pre-processing unit (IPU) further refines performance and efficiency by capitalizing on block-wise input sparsity.","Results show that our proposed co-design framework achieves a remarkable speedup of up to 7.69x and energy savings of 83.43%."],"url":"http://arxiv.org/abs/2404.09497v1","category":"cs.AR"}
{"created":"2024-04-15 06:31:12","title":"Novel entropy difference-based EEG channel selection technique for automated detection of ADHD","abstract":"Attention deficit hyperactivity disorder (ADHD) is one of the common neurodevelopmental disorders in children. This paper presents an automated approach for ADHD detection using the proposed entropy difference (EnD)- based encephalogram (EEG) channel selection approach. In the proposed approach, we selected the most significant EEG channels for the accurate identification of ADHD using an EnD-based channel selection approach. Secondly, a set of features is extracted from the selected channels and fed to a classifier. To verify the effectiveness of the channels selected, we explored three sets of features and classifiers. More specifically, we explored discrete wavelet transform (DWT), empirical mode decomposition (EMD) and symmetrically-weighted local binary pattern (SLBP)-based features. To perform automated classification, we have used k-nearest neighbor (k-NN), Ensemble classifier, and support vectors machine (SVM) classifiers. Our proposed approach yielded the highest accuracy of 99.29% using the public database. In addition, the proposed EnD-based channel selection has consistently provided better classification accuracies than the entropy-based channel selection approach. Also, the developed method","sentences":["Attention deficit hyperactivity disorder (ADHD) is one of the common neurodevelopmental disorders in children.","This paper presents an automated approach for ADHD detection using the proposed entropy difference (EnD)- based encephalogram (EEG) channel selection approach.","In the proposed approach, we selected the most significant EEG channels for the accurate identification of ADHD using an EnD-based channel selection approach.","Secondly, a set of features is extracted from the selected channels and fed to a classifier.","To verify the effectiveness of the channels selected, we explored three sets of features and classifiers.","More specifically, we explored discrete wavelet transform (DWT), empirical mode decomposition (EMD) and symmetrically-weighted local binary pattern (SLBP)-based features.","To perform automated classification, we have used k-nearest neighbor (k-NN), Ensemble classifier, and support vectors machine (SVM) classifiers.","Our proposed approach yielded the highest accuracy of 99.29% using the public database.","In addition, the proposed EnD-based channel selection has consistently provided better classification accuracies than the entropy-based channel selection approach.","Also, the developed method"],"url":"http://arxiv.org/abs/2404.09493v1","category":"eess.SP"}
{"created":"2024-04-15 04:27:58","title":"Scattering rigidity for standard stationary manifolds via timelike geodesics","abstract":"We study the scattering rigidity problem for standard stationary manifolds using timelike geodesics with a fixed angle. Taking advantage of the symmetry of this manifolds, we use Hamiltonian reduction to show that this problem is related to scattering rigidity for $\\mathcal{MP}$-systems, a problem studied before. This gives several new rigidity results (up to some gauge) for this kind of Lorentzian manifolds.","sentences":["We study the scattering rigidity problem for standard stationary manifolds using timelike geodesics with a fixed angle.","Taking advantage of the symmetry of this manifolds, we use Hamiltonian reduction to show that this problem is related to scattering rigidity for $\\mathcal{MP}$-systems, a problem studied before.","This gives several new rigidity results (up to some gauge) for this kind of Lorentzian manifolds."],"url":"http://arxiv.org/abs/2404.09449v1","category":"math.DG"}
{"created":"2024-04-15 02:09:36","title":"Satellite observations reveal shorter periodic inner core oscillation","abstract":"Detecting the Earth's inner core motions relative to the mantle presents a considerable challenge due to their indirect accessibility. Seismological observations initially provided evidence for differential/super-rotation of the inner core, but recently demonstrated a possibly about 70-year periodic oscillation. The contrasting results underscore the ongoing enigma surrounding inner core motion, leaving debates unresolved, including the precise oscillate period. In parallel to seismic observations, satellite geodesy has accumulated decades of global high-precision records, providing a novel avenue to probe inner core motions. Here, we detect an about 6-year oscillation from the gravitational field degree-2 order-2 Stokes coefficients derived from satellite observations, and find it has a unique phase correlation with the about 6-year signal in the Earth's length-of-day variations. This correlation is attributed to an inner core oscillation which is controlled by the gravitational coupling between the inner core and lower mantle (mainly due to the density heterogeneity of the two large low-velocity provinces; LLVPs). That is, we independently corroborate the inner core periodic oscillation, albeit with a significantly shorter period than previously suggested. Our findings demonstrate the dense layer of the LLVPs (mean density anomalies of about +0.9 percent at the bottom), consistent with inversions from tidal tomography and Stoneley modes. Furthermore, our research reveals equatorial topographic undulations of about 187 m at the inner core boundary.","sentences":["Detecting the Earth's inner core motions relative to the mantle presents a considerable challenge due to their indirect accessibility.","Seismological observations initially provided evidence for differential/super-rotation of the inner core, but recently demonstrated a possibly about 70-year periodic oscillation.","The contrasting results underscore the ongoing enigma surrounding inner core motion, leaving debates unresolved, including the precise oscillate period.","In parallel to seismic observations, satellite geodesy has accumulated decades of global high-precision records, providing a novel avenue to probe inner core motions.","Here, we detect an about 6-year oscillation from the gravitational field degree-2 order-2 Stokes coefficients derived from satellite observations, and find it has a unique phase correlation with the about 6-year signal in the Earth's length-of-day variations.","This correlation is attributed to an inner core oscillation which is controlled by the gravitational coupling between the inner core and lower mantle (mainly due to the density heterogeneity of the two large low-velocity provinces; LLVPs).","That is, we independently corroborate the inner core periodic oscillation, albeit with a significantly shorter period than previously suggested.","Our findings demonstrate the dense layer of the LLVPs (mean density anomalies of about +0.9 percent at the bottom), consistent with inversions from tidal tomography and Stoneley modes.","Furthermore, our research reveals equatorial topographic undulations of about 187 m at the inner core boundary."],"url":"http://arxiv.org/abs/2404.09417v1","category":"physics.geo-ph"}
{"created":"2024-04-15 01:41:18","title":"EQO: Exploring Ultra-Efficient Private Inference with Winograd-Based Protocol and Quantization Co-Optimization","abstract":"Private convolutional neural network (CNN) inference based on secure two-party computation (2PC) suffers from high communication and latency overhead, especially from convolution layers. In this paper, we propose EQO, a quantized 2PC inference framework that jointly optimizes the CNNs and 2PC protocols. EQO features a novel 2PC protocol that combines Winograd transformation with quantization for efficient convolution computation. However, we observe naively combining quantization and Winograd convolution is sub-optimal: Winograd transformations introduce extensive local additions and weight outliers that increase the quantization bit widths and require frequent bit width conversions with non-negligible communication overhead. Therefore, at the protocol level, we propose a series of optimizations for the 2PC inference graph to minimize the communication. At the network level, We develop a sensitivity-based mixed-precision quantization algorithm to optimize network accuracy given communication constraints. We further propose a 2PC-friendly bit re-weighting algorithm to accommodate weight outliers without increasing bit widths. With extensive experiments, EQO demonstrates 11.7x, 3.6x, and 6.3x communication reduction with 1.29%, 1.16%, and 1.29% higher accuracy compared to state-of-the-art frameworks SiRNN, COINN, and CoPriv, respectively.","sentences":["Private convolutional neural network (CNN) inference based on secure two-party computation (2PC) suffers from high communication and latency overhead, especially from convolution layers.","In this paper, we propose EQO, a quantized 2PC inference framework that jointly optimizes the CNNs and 2PC protocols.","EQO features a novel 2PC protocol that combines Winograd transformation with quantization for efficient convolution computation.","However, we observe naively combining quantization and Winograd convolution is sub-optimal: Winograd transformations introduce extensive local additions and weight outliers that increase the quantization bit widths and require frequent bit width conversions with non-negligible communication overhead.","Therefore, at the protocol level, we propose a series of optimizations for the 2PC inference graph to minimize the communication.","At the network level, We develop a sensitivity-based mixed-precision quantization algorithm to optimize network accuracy given communication constraints.","We further propose a 2PC-friendly bit re-weighting algorithm to accommodate weight outliers without increasing bit widths.","With extensive experiments, EQO demonstrates 11.7x, 3.6x, and 6.3x communication reduction with 1.29%, 1.16%, and 1.29% higher accuracy compared to state-of-the-art frameworks SiRNN, COINN, and CoPriv, respectively."],"url":"http://arxiv.org/abs/2404.09404v1","category":"cs.CR"}
{"created":"2024-04-15 01:19:44","title":"Fractional Integral Estimates of Hermite-Hadamard type in Global Nonpositive Curvature Spaces","abstract":"We extend the notion of convexity of functions defined on global nonpositive curvature spaces by introducing (geodesically) $h$-convex functions. We prove estimates of Hermite-Hadamard type via Katugampola's fractional integrals. We obtain an important corollary which gives an essentially sharp estimate involving squared distance mappings between points in a global NPC space. This is a contribution to analysis on spaces with curved geometry.","sentences":["We extend the notion of convexity of functions defined on global nonpositive curvature spaces by introducing (geodesically) $h$-convex functions.","We prove estimates of Hermite-Hadamard type via Katugampola's fractional integrals.","We obtain an important corollary which gives an essentially sharp estimate involving squared distance mappings between points in a global NPC space.","This is a contribution to analysis on spaces with curved geometry."],"url":"http://arxiv.org/abs/2404.09400v1","category":"math.FA"}
{"created":"2024-04-14 21:37:39","title":"Hierarchical Attention Models for Multi-Relational Graphs","abstract":"We present Bi-Level Attention-Based Relational Graph Convolutional Networks (BR-GCN), unique neural network architectures that utilize masked self-attentional layers with relational graph convolutions, to effectively operate on highly multi-relational data. BR-GCN models use bi-level attention to learn node embeddings through (1) node-level attention, and (2) relation-level attention. The node-level self-attentional layers use intra-relational graph interactions to learn relation-specific node embeddings using a weighted aggregation of neighborhood features in a sparse subgraph region. The relation-level self-attentional layers use inter-relational graph interactions to learn the final node embeddings using a weighted aggregation of relation-specific node embeddings. The BR-GCN bi-level attention mechanism extends Transformer-based multiplicative attention from the natural language processing (NLP) domain, and Graph Attention Networks (GAT)-based attention, to large-scale heterogeneous graphs (HGs). On node classification, BR-GCN outperforms baselines from 0.29% to 14.95% as a stand-alone model, and on link prediction, BR-GCN outperforms baselines from 0.02% to 7.40% as an auto-encoder model. We also conduct ablation studies to evaluate the quality of BR-GCN's relation-level attention and discuss how its learning of graph structure may be transferred to enrich other graph neural networks (GNNs). Through various experiments, we show that BR-GCN's attention mechanism is both scalable and more effective in learning compared to state-of-the-art GNNs.","sentences":["We present Bi-Level Attention-Based Relational Graph Convolutional Networks (BR-GCN), unique neural network architectures that utilize masked self-attentional layers with relational graph convolutions, to effectively operate on highly multi-relational data.","BR-GCN models use bi-level attention to learn node embeddings through (1) node-level attention, and (2) relation-level attention.","The node-level self-attentional layers use intra-relational graph interactions to learn relation-specific node embeddings using a weighted aggregation of neighborhood features in a sparse subgraph region.","The relation-level self-attentional layers use inter-relational graph interactions to learn the final node embeddings using a weighted aggregation of relation-specific node embeddings.","The BR-GCN bi-level attention mechanism extends Transformer-based multiplicative attention from the natural language processing (NLP) domain, and Graph Attention Networks (GAT)-based attention, to large-scale heterogeneous graphs (HGs).","On node classification, BR-GCN outperforms baselines from 0.29% to 14.95% as a stand-alone model, and on link prediction, BR-GCN outperforms baselines from 0.02% to 7.40% as an auto-encoder model.","We also conduct ablation studies to evaluate the quality of BR-GCN's relation-level attention and discuss how its learning of graph structure may be transferred to enrich other graph neural networks (GNNs).","Through various experiments, we show that BR-GCN's attention mechanism is both scalable and more effective in learning compared to state-of-the-art GNNs."],"url":"http://arxiv.org/abs/2404.09365v1","category":"cs.LG"}
{"created":"2024-04-14 21:10:57","title":"Two-stage Spatial Regression Models for Spatial Confounding","abstract":"Public health data are often spatially dependent, but standard spatial regression methods can suffer from bias and invalid inference when the independent variable is associated with spatially-correlated residuals. This could occur if, for example, there is an unmeasured environmental contaminant. Geoadditive structural equation modeling (gSEM), in which an estimated spatial trend is removed from both the explanatory and response variables before estimating the parameters of interest, has previously been proposed as a solution, but there has been little investigation of gSEM's properties with point-referenced data. We link gSEM to results on double machine learning and semiparametric regression based on two-stage procedures. We propose using these semiparametric estimators for spatial regression using Gaussian processes with Mat\\`ern covariance to estimate the spatial trends, and term this class of estimators Double Spatial Regression (DSR). We derive regularity conditions for root-$n$ asymptotic normality and consistency and closed-form variance estimation, and show that in simulations where standard spatial regression estimators are highly biased and have poor coverage, DSR can mitigate bias more effectively than competitors and obtain nominal coverage.","sentences":["Public health data are often spatially dependent, but standard spatial regression methods can suffer from bias and invalid inference when the independent variable is associated with spatially-correlated residuals.","This could occur if, for example, there is an unmeasured environmental contaminant.","Geoadditive structural equation modeling (gSEM), in which an estimated spatial trend is removed from both the explanatory and response variables before estimating the parameters of interest, has previously been proposed as a solution, but there has been little investigation of gSEM's properties with point-referenced data.","We link gSEM to results on double machine learning and semiparametric regression based on two-stage procedures.","We propose using these semiparametric estimators for spatial regression using Gaussian processes with Mat\\`ern covariance to estimate the spatial trends, and term this class of estimators Double Spatial Regression (DSR).","We derive regularity conditions for root-$n$ asymptotic normality and consistency and closed-form variance estimation, and show that in simulations where standard spatial regression estimators are highly biased and have poor coverage, DSR can mitigate bias more effectively than competitors and obtain nominal coverage."],"url":"http://arxiv.org/abs/2404.09358v1","category":"stat.ME"}
{"created":"2024-04-14 19:00:04","title":"Existence and regularity results for space-time fractional integro-differential equation of Kirchhoff type with memory","abstract":"This paper analyses a Kirchhoff type quasilinear space-time fractional integro-differential equation with memory $(\\mathcal{K}^{s}_{\\alpha})$. Various a priori bounds are derived in different norms on the solution of the considered equation. Utilizing these a priori bounds, existence and uniqueness of the weak solution to the proposed model are proved. Furthermore, regularity results on the solution of $(\\mathcal{K}^{s}_{\\alpha})$ are established. The contribution made in this work provides a framework for further investigation of such types of partial integro-differential equation $(\\mathcal{K}^{s}_{\\alpha})$.","sentences":["This paper analyses a Kirchhoff type quasilinear space-time fractional integro-differential equation with memory $(\\mathcal{K}^{s}_{\\alpha})$. Various a priori bounds are derived in different norms on the solution of the considered equation.","Utilizing these a priori bounds, existence and uniqueness of the weak solution to the proposed model are proved.","Furthermore, regularity results on the solution of $(\\mathcal{K}^{s}_{\\alpha})$ are established.","The contribution made in this work provides a framework for further investigation of such types of partial integro-differential equation $(\\mathcal{K}^{s}_{\\alpha})$."],"url":"http://arxiv.org/abs/2404.09328v1","category":"math.AP"}
{"created":"2024-04-14 18:02:43","title":"Binary bi-braces and applications to cryptography","abstract":"In a XOR-based alternating block cipher the plaintext is masked by a sequence of layers each performing distinct actions: a highly nonlinear permutation, a linear transformation, and the bitwise key addition. When assessing resistance against classical differential attacks (where differences are computed with respect to XOR), the cryptanalysts must only take into account differential probabilities introduced by the nonlinear layer, this being the only one whose differential transitions are not deterministic. The temptation of computing differentials with respect to another difference operation runs into the difficulty of understanding how differentials propagate through the XOR-affine levels of the cipher. In this paper we introduce a special family of braces that enable the derivation of a set of differences whose interaction with every layer of an XOR-based alternating block cipher can be understood. We show that such braces can be described also in terms of alternating binary algebras of nilpotency class two. Additionally, we present a method to compute the automorphism group of these structures through an equivalence between bilinear maps. By doing so, we characterise the XOR-linear permutations for which the differential transitions with respect to the new difference are deterministic, facilitating an alternative differential attack.","sentences":["In a XOR-based alternating block cipher the plaintext is masked by a sequence of layers each performing distinct actions: a highly nonlinear permutation, a linear transformation, and the bitwise key addition.","When assessing resistance against classical differential attacks (where differences are computed with respect to XOR), the cryptanalysts must only take into account differential probabilities introduced by the nonlinear layer, this being the only one whose differential transitions are not deterministic.","The temptation of computing differentials with respect to another difference operation runs into the difficulty of understanding how differentials propagate through the XOR-affine levels of the cipher.","In this paper we introduce a special family of braces that enable the derivation of a set of differences whose interaction with every layer of an XOR-based alternating block cipher can be understood.","We show that such braces can be described also in terms of alternating binary algebras of nilpotency class two.","Additionally, we present a method to compute the automorphism group of these structures through an equivalence between bilinear maps.","By doing so, we characterise the XOR-linear permutations for which the differential transitions with respect to the new difference are deterministic, facilitating an alternative differential attack."],"url":"http://arxiv.org/abs/2404.09315v1","category":"math.GR"}
{"created":"2024-04-14 16:09:57","title":"Miscibility of Binary Bose-Einstein Condensates with $p$-wave Interaction","abstract":"We investigate the ground-state phase diagram of a binary mixture of Bose-Einstein condensates (BECs) with competing interspecies $s$- and $p$-wave interactions. Exploiting a pseudopotential model for the $l=1$ partial wave, we derive an extended Gross-Pitaevskii (GP) equation for the BEC mixture that incorporates both $s$- and $p$-wave interactions. Based on it, we study the miscible-immiscible transition of a binary BEC mixture in the presence of interspecies $p$-wave interaction, by combining numerical solution of the GP equation and Gaussian variational analysis. Our study uncovers a dual effect -- either enhance or reduce miscibility -- of positive interspecies $p$-wave interaction, which can be precisely controlled by adjusting relevant experimental parameters. By complete characterizing the miscibility phase diagram, we establish a promising avenue towards experimental control of the miscibility of binary BEC mixtures via high partial-wave interactions.","sentences":["We investigate the ground-state phase diagram of a binary mixture of Bose-Einstein condensates (BECs) with competing interspecies $s$- and $p$-wave interactions.","Exploiting a pseudopotential model for the $l=1$ partial wave, we derive an extended Gross-Pitaevskii (GP) equation for the BEC mixture that incorporates both $s$- and $p$-wave interactions.","Based on it, we study the miscible-immiscible transition of a binary BEC mixture in the presence of interspecies $p$-wave interaction, by combining numerical solution of the GP equation and Gaussian variational analysis.","Our study uncovers a dual effect -- either enhance or reduce miscibility -- of positive interspecies $p$-wave interaction, which can be precisely controlled by adjusting relevant experimental parameters.","By complete characterizing the miscibility phase diagram, we establish a promising avenue towards experimental control of the miscibility of binary BEC mixtures via high partial-wave interactions."],"url":"http://arxiv.org/abs/2404.09294v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-14 15:41:40","title":"On the Condensation and fluctuations in reversible coagulation-fragmentation models","abstract":"We study the condensation phenomenon for the invariant measures of the mean-field model of reversible coagulation-fragmentation processes conditioned to a supercritical density of particles. It is shown that when the parameters of the associated balance equation satisfy a subexponential tail condition, there is one single giant particle that corresponds to the missing mass in the macroscopic limit. We also show that in this case, the rest of the particles are asymptotically \\emph{i.i.d.} according to the normalized equilibrium state of the limit hydrodynamic differential equation. Conditions for the normal fluctuations and the $\\alpha$-stable fluctuations around the condensed mass are given. We obtain the large deviation principle for the empirical measure of the masses of the particles at equilibrium as well.","sentences":["We study the condensation phenomenon for the invariant measures of the mean-field model of reversible coagulation-fragmentation processes conditioned to a supercritical density of particles.","It is shown that when the parameters of the associated balance equation satisfy a subexponential tail condition, there is one single giant particle that corresponds to the missing mass in the macroscopic limit.","We also show that in this case, the rest of the particles are asymptotically \\emph{i.i.d.}","according to the normalized equilibrium state of the limit hydrodynamic differential equation.","Conditions for the normal fluctuations and the $\\alpha$-stable fluctuations around the condensed mass are given.","We obtain the large deviation principle for the empirical measure of the masses of the particles at equilibrium as well."],"url":"http://arxiv.org/abs/2404.09287v1","category":"math.PR"}
{"created":"2024-04-14 14:58:52","title":"SyntStereo2Real: Edge-Aware GAN for Remote Sensing Image-to-Image Translation while Maintaining Stereo Constraint","abstract":"In the field of remote sensing, the scarcity of stereo-matched and particularly lack of accurate ground truth data often hinders the training of deep neural networks. The use of synthetically generated images as an alternative, alleviates this problem but suffers from the problem of domain generalization. Unifying the capabilities of image-to-image translation and stereo-matching presents an effective solution to address the issue of domain generalization. Current methods involve combining two networks, an unpaired image-to-image translation network and a stereo-matching network, while jointly optimizing them. We propose an edge-aware GAN-based network that effectively tackles both tasks simultaneously. We obtain edge maps of input images from the Sobel operator and use it as an additional input to the encoder in the generator to enforce geometric consistency during translation. We additionally include a warping loss calculated from the translated images to maintain the stereo consistency. We demonstrate that our model produces qualitatively and quantitatively superior results than existing models, and its applicability extends to diverse domains, including autonomous driving.","sentences":["In the field of remote sensing, the scarcity of stereo-matched and particularly lack of accurate ground truth data often hinders the training of deep neural networks.","The use of synthetically generated images as an alternative, alleviates this problem but suffers from the problem of domain generalization.","Unifying the capabilities of image-to-image translation and stereo-matching presents an effective solution to address the issue of domain generalization.","Current methods involve combining two networks, an unpaired image-to-image translation network and a stereo-matching network, while jointly optimizing them.","We propose an edge-aware GAN-based network that effectively tackles both tasks simultaneously.","We obtain edge maps of input images from the Sobel operator and use it as an additional input to the encoder in the generator to enforce geometric consistency during translation.","We additionally include a warping loss calculated from the translated images to maintain the stereo consistency.","We demonstrate that our model produces qualitatively and quantitatively superior results than existing models, and its applicability extends to diverse domains, including autonomous driving."],"url":"http://arxiv.org/abs/2404.09277v1","category":"cs.CV"}
{"created":"2024-04-14 14:26:33","title":"VRS-NeRF: Visual Relocalization with Sparse Neural Radiance Field","abstract":"Visual relocalization is a key technique to autonomous driving, robotics, and virtual/augmented reality. After decades of explorations, absolute pose regression (APR), scene coordinate regression (SCR), and hierarchical methods (HMs) have become the most popular frameworks. However, in spite of high efficiency, APRs and SCRs have limited accuracy especially in large-scale outdoor scenes; HMs are accurate but need to store a large number of 2D descriptors for matching, resulting in poor efficiency. In this paper, we propose an efficient and accurate framework, called VRS-NeRF, for visual relocalization with sparse neural radiance field. Precisely, we introduce an explicit geometric map (EGM) for 3D map representation and an implicit learning map (ILM) for sparse patches rendering. In this localization process, EGP provides priors of spare 2D points and ILM utilizes these sparse points to render patches with sparse NeRFs for matching. This allows us to discard a large number of 2D descriptors so as to reduce the map size. Moreover, rendering patches only for useful points rather than all pixels in the whole image reduces the rendering time significantly. This framework inherits the accuracy of HMs and discards their low efficiency. Experiments on 7Scenes, CambridgeLandmarks, and Aachen datasets show that our method gives much better accuracy than APRs and SCRs, and close performance to HMs but is much more efficient.","sentences":["Visual relocalization is a key technique to autonomous driving, robotics, and virtual/augmented reality.","After decades of explorations, absolute pose regression (APR), scene coordinate regression (SCR), and hierarchical methods (HMs) have become the most popular frameworks.","However, in spite of high efficiency, APRs and SCRs have limited accuracy especially in large-scale outdoor scenes; HMs are accurate but need to store a large number of 2D descriptors for matching, resulting in poor efficiency.","In this paper, we propose an efficient and accurate framework, called VRS-NeRF, for visual relocalization with sparse neural radiance field.","Precisely, we introduce an explicit geometric map (EGM) for 3D map representation and an implicit learning map (ILM) for sparse patches rendering.","In this localization process, EGP provides priors of spare 2D points and ILM utilizes these sparse points to render patches with sparse NeRFs for matching.","This allows us to discard a large number of 2D descriptors so as to reduce the map size.","Moreover, rendering patches only for useful points rather than all pixels in the whole image reduces the rendering time significantly.","This framework inherits the accuracy of HMs and discards their low efficiency.","Experiments on 7Scenes, CambridgeLandmarks, and Aachen datasets show that our method gives much better accuracy than APRs and SCRs, and close performance to HMs but is much more efficient."],"url":"http://arxiv.org/abs/2404.09271v1","category":"cs.CV"}
{"created":"2024-04-14 13:33:25","title":"Competitive Retrieval: Going Beyond the Single Query","abstract":"Previous work on the competitive retrieval setting focused on a single-query setting: document authors manipulate their documents so as to improve their future ranking for a given query. We study a competitive setting where authors opt to improve their document's ranking for multiple queries. We use game theoretic analysis to prove that equilibrium does not necessarily exist. We then empirically show that it is more difficult for authors to improve their documents' rankings for multiple queries with a neural ranker than with a state-of-the-art feature-based ranker. We also present an effective approach for predicting the document most highly ranked in the next induced ranking.","sentences":["Previous work on the competitive retrieval setting focused on a single-query setting: document authors manipulate their documents so as to improve their future ranking for a given query.","We study a competitive setting where authors opt to improve their document's ranking for multiple queries.","We use game theoretic analysis to prove that equilibrium does not necessarily exist.","We then empirically show that it is more difficult for authors to improve their documents' rankings for multiple queries with a neural ranker than with a state-of-the-art feature-based ranker.","We also present an effective approach for predicting the document most highly ranked in the next induced ranking."],"url":"http://arxiv.org/abs/2404.09253v1","category":"cs.IR"}
{"created":"2024-04-14 12:48:12","title":"Some new bistable transition fronts with changing shape","abstract":"We construct entire solutions of bistable reaction-diffusion equations by mixing finite planar fronts, which form a finite-dimensional manifold. These entire solutions are generalized traveling fronts, that is, transition fronts. We also show their uniqueness and stability. Furthermore, we prove that transition fronts with level sets having finite facets are determined by finite planar fronts and they are in the class of entire solutions constructed by us.","sentences":["We construct entire solutions of bistable reaction-diffusion equations by mixing finite planar fronts, which form a finite-dimensional manifold.","These entire solutions are generalized traveling fronts, that is, transition fronts.","We also show their uniqueness and stability.","Furthermore, we prove that transition fronts with level sets having finite facets are determined by finite planar fronts and they are in the class of entire solutions constructed by us."],"url":"http://arxiv.org/abs/2404.09237v1","category":"math.AP"}
{"created":"2024-04-14 12:06:36","title":"Nonlocal Gravity, Dark Energy and Conformal Symmetry: Testing the Hierarchies of Anomaly-Induced Actions","abstract":"Conformal back-reaction generates cosmological models where the trace anomaly reflects the breaking of Weyl invariance. Analyzing these actions yields a dynamic approach to dark energy through anomaly-induced actions (AIAs), that are variational solutions of the trace anomaly functional constraint. Expanded around Minkowski space, they produce semiclassical correlators subject to hierarchical anomalous Ward identities, tied to conformal symmetry and diffeomorphism invariance. We focus on comparing the hierarchy of a specific 4-point function, particularly the 2-gravitons-2-photons correlator $(TTJJ)$, generated by AIAs, to free field theory realizations of the same correlator. We observe that the free field theory original hierarchy splits into one ordinary and one anomalous hierarchy, both satisfying the conservation Ward identities from diffeomorphism invariance. However, we find that the anomalous hierarchy derived from ordinary AIAs in both the Riegert or Fradkin-Vilkovisky gauges, are either affected by double poles or violate the hierarchy of the trace Ward identity, respectively. We show that correct forms of the anomalous hierarchies of 4-point functions (for the $TTTT$ and $TTJJ$), identified in a perturbative free field theory expansion around flat space, are characterised by anomaly poles, corresponding to a curvature expansion in $R\\Box^{-1}$, together with Weyl invariant terms. We derive the effective action that generates the correct form of the hierarchy for the $TTJJ$.","sentences":["Conformal back-reaction generates cosmological models where the trace anomaly reflects the breaking of Weyl invariance.","Analyzing these actions yields a dynamic approach to dark energy through anomaly-induced actions (AIAs), that are variational solutions of the trace anomaly functional constraint.","Expanded around Minkowski space, they produce semiclassical correlators subject to hierarchical anomalous Ward identities, tied to conformal symmetry and diffeomorphism invariance.","We focus on comparing the hierarchy of a specific 4-point function, particularly the 2-gravitons-2-photons correlator $(TTJJ)$, generated by AIAs, to free field theory realizations of the same correlator.","We observe that the free field theory original hierarchy splits into one ordinary and one anomalous hierarchy, both satisfying the conservation Ward identities from diffeomorphism invariance.","However, we find that the anomalous hierarchy derived from ordinary AIAs in both the Riegert or Fradkin-Vilkovisky gauges, are either affected by double poles or violate the hierarchy of the trace Ward identity, respectively.","We show that correct forms of the anomalous hierarchies of 4-point functions (for the $TTTT$ and $TTJJ$), identified in a perturbative free field theory expansion around flat space, are characterised by anomaly poles, corresponding to a curvature expansion in $R\\Box^{-1}$, together with Weyl invariant terms.","We derive the effective action that generates the correct form of the hierarchy for the $TTJJ$."],"url":"http://arxiv.org/abs/2404.09225v1","category":"hep-th"}
{"created":"2024-04-14 10:04:44","title":"DEGNN: Dual Experts Graph Neural Network Handling Both Edge and Node Feature Noise","abstract":"Graph Neural Networks (GNNs) have achieved notable success in various applications over graph data. However, recent research has revealed that real-world graphs often contain noise, and GNNs are susceptible to noise in the graph. To address this issue, several Graph Structure Learning (GSL) models have been introduced. While GSL models are tailored to enhance robustness against edge noise through edge reconstruction, a significant limitation surfaces: their high reliance on node features. This inherent dependence amplifies their susceptibility to noise within node features. Recognizing this vulnerability, we present DEGNN, a novel GNN model designed to adeptly mitigate noise in both edges and node features. The core idea of DEGNN is to design two separate experts: an edge expert and a node feature expert. These experts utilize self-supervised learning techniques to produce modified edges and node features. Leveraging these modified representations, DEGNN subsequently addresses downstream tasks, ensuring robustness against noise present in both edges and node features of real-world graphs. Notably, the modification process can be trained end-to-end, empowering DEGNN to adjust dynamically and achieves optimal edge and node representations for specific tasks. Comprehensive experiments demonstrate DEGNN's efficacy in managing noise, both in original real-world graphs and in graphs with synthetic noise.","sentences":["Graph Neural Networks (GNNs) have achieved notable success in various applications over graph data.","However, recent research has revealed that real-world graphs often contain noise, and GNNs are susceptible to noise in the graph.","To address this issue, several Graph Structure Learning (GSL) models have been introduced.","While GSL models are tailored to enhance robustness against edge noise through edge reconstruction, a significant limitation surfaces: their high reliance on node features.","This inherent dependence amplifies their susceptibility to noise within node features.","Recognizing this vulnerability, we present DEGNN, a novel GNN model designed to adeptly mitigate noise in both edges and node features.","The core idea of DEGNN is to design two separate experts: an edge expert and a node feature expert.","These experts utilize self-supervised learning techniques to produce modified edges and node features.","Leveraging these modified representations, DEGNN subsequently addresses downstream tasks, ensuring robustness against noise present in both edges and node features of real-world graphs.","Notably, the modification process can be trained end-to-end, empowering DEGNN to adjust dynamically and achieves optimal edge and node representations for specific tasks.","Comprehensive experiments demonstrate DEGNN's efficacy in managing noise, both in original real-world graphs and in graphs with synthetic noise."],"url":"http://arxiv.org/abs/2404.09207v1","category":"cs.LG"}
{"created":"2024-04-14 09:13:29","title":"Wave maps in dimension $1+1$ with an external forcing","abstract":"This paper aims to establish the local and global well-posedness theory in $L^1$, inspired by the approach of Keel and Tao [Internat. Math. Res. Notices, 1998], for the forced wave map equation in the ``external'' formalism. In this context, the target manifold is treated as a submanifold of a Euclidean space. As a corollary, we reprove Zhou's [Math. Z., 1999] uniqueness result, leading to the uniqueness of weak solutions with locally finite energy. Additionally, we achieve the scattering of such solutions through a conformal compactification argument.","sentences":["This paper aims to establish the local and global well-posedness theory in $L^1$, inspired by the approach of Keel and Tao [Internat.","Math.","Res. Notices, 1998], for the forced wave map equation in the ``external'' formalism.","In this context, the target manifold is treated as a submanifold of a Euclidean space.","As a corollary, we reprove Zhou's [Math. Z., 1999] uniqueness result, leading to the uniqueness of weak solutions with locally finite energy.","Additionally, we achieve the scattering of such solutions through a conformal compactification argument."],"url":"http://arxiv.org/abs/2404.09195v1","category":"math.AP"}
{"created":"2024-04-14 08:20:58","title":"Invariant classes for families of complexes","abstract":"We consider families of chain-cochain infinite complexes $\\mathcal C$ of spaces with elements depending on a number of parameters, and endowed with a converging associative multiple product. The existence of left/right local/non-local square-vanishing ideals is assumed for subspaces of $\\mathcal C$-spaces. We show that a set of differential and orthogonality relations together with coherence conditions on indices of a chain-cochain complex $\\mathcal C$ elements generates families of graded differential algebras. With the appropriate orthogonality conditions on completions of $\\mathcal C$ elements in the multiple product, we define the equivalence classes of cohomology invariants.","sentences":["We consider families of chain-cochain infinite complexes $\\mathcal C$ of spaces with elements depending on a number of parameters, and endowed with a converging associative multiple product.","The existence of left/right local/non-local square-vanishing ideals is assumed for subspaces of $\\mathcal C$-spaces.","We show that a set of differential and orthogonality relations together with coherence conditions on indices of a chain-cochain complex $\\mathcal C$ elements generates families of graded differential algebras.","With the appropriate orthogonality conditions on completions of $\\mathcal C$ elements in the multiple product, we define the equivalence classes of cohomology invariants."],"url":"http://arxiv.org/abs/2404.09183v1","category":"math.FA"}
{"created":"2024-04-14 07:17:28","title":"Asymptotic-preserving approximations for stochastic incompressible viscous fluids and SPDEs on group","abstract":"The long-term dynamics of particles involved in an incompressible flow with a small viscosity ($\\epsilon>0$) and slow chemical reactions, is depicted by a class of stochastic reaction-diffusion-advection (RDA) equations with a fast advection term of magnitude $1/\\epsilon$. It has been shown in [7] the fast advection asymptotics of stochastic RDA equation in $\\mathbb{R}^2$ can be characterized through a stochastic partial differential equation (SPDE) on the graph associated with certain Hamiltonian. To simulate such fast advection asymptotics, we introduce and study an asymptotic-preserving (AP) exponential Euler approximation for the multiscale stochastic RDA equation. There are three key ingredients in proving asymptotic-preserving property of the proposed approximation. First, a strong error estimate, which depends on $1/\\epsilon$ linearly, is obtained via a variational argument. Second, we prove the consistency of exponential Euler approximations on the fast advection asymptotics between the original problem and the SPDE on graph. Last, a graph weighted space is introduced to quantify the approximation error for SPDE on graph, which avoids the possible singularity near the vertices. Numerical experiments are carried out to support the theoretical results.","sentences":["The long-term dynamics of particles involved in an incompressible flow with a small viscosity ($\\epsilon>0$) and slow chemical reactions, is depicted by a class of stochastic reaction-diffusion-advection (RDA) equations with a fast advection term of magnitude $1/\\epsilon$. It has been shown in [7] the fast advection asymptotics of stochastic RDA equation in $\\mathbb{R}^2$ can be characterized through a stochastic partial differential equation (SPDE) on the graph associated with certain Hamiltonian.","To simulate such fast advection asymptotics, we introduce and study an asymptotic-preserving (AP) exponential Euler approximation for the multiscale stochastic RDA equation.","There are three key ingredients in proving asymptotic-preserving property of the proposed approximation.","First, a strong error estimate, which depends on $1/\\epsilon$ linearly, is obtained via a variational argument.","Second, we prove the consistency of exponential Euler approximations on the fast advection asymptotics between the original problem and the SPDE on graph.","Last, a graph weighted space is introduced to quantify the approximation error for SPDE on graph, which avoids the possible singularity near the vertices.","Numerical experiments are carried out to support the theoretical results."],"url":"http://arxiv.org/abs/2404.09168v1","category":"math.NA"}
{"created":"2024-04-14 02:17:46","title":"Monotonicity of renormalization group flow, Perelman's entropy functional, and emergent dual holography in the worldsheet nonlinear $\u03c3$ model","abstract":"Based on the renormalization group (RG) flow of worldsheet bosonic string theory, we construct an effective holographic dual description, where an extra dimension is identified with an RG scale. As a result, we obtain a dilaton-gravity effective theory for the dynamics of an emergent target spacetime, analogous to the low-energy description of bosonic M theory. We argue that this holographic dual effective field theory is non-perturbative in nature for the $\\alpha'$ expansion, where the RG flow of the target spacetime manifests in the level of an effective bulk action. Based on the holographic dual effective field theory, we investigate the monotonicity of the RG flow. Inspired by the monotonicity of the Ricci flow given by Perelman, we propose a holographic construction of the Perelman's entropy functional. Based on the equivalence between the Hamilton-Jacobi equation and the local RG equation, we show that the RG flow of holographic Perelman's entropy functional is nothing but the Weyl anomaly. This leads us to the monotonicity of the RG flow of the emergent target spacetime. Furthermore, considering the entropy production along the RG flow, we construct a microscopic entropy functional based on the probability distribution function of the holographic dual effective field theory, regarded as Gibbs or Shannon entropy. We find that the monotonicity of this microscopically constructed entropy functional shows a strong connection with the monotonicity of the holographic Perelman's entropy functional.","sentences":["Based on the renormalization group (RG) flow of worldsheet bosonic string theory, we construct an effective holographic dual description, where an extra dimension is identified with an RG scale.","As a result, we obtain a dilaton-gravity effective theory for the dynamics of an emergent target spacetime, analogous to the low-energy description of bosonic M theory.","We argue that this holographic dual effective field theory is non-perturbative in nature for the $\\alpha'$ expansion, where the RG flow of the target spacetime manifests in the level of an effective bulk action.","Based on the holographic dual effective field theory, we investigate the monotonicity of the RG flow.","Inspired by the monotonicity of the Ricci flow given by Perelman, we propose a holographic construction of the Perelman's entropy functional.","Based on the equivalence between the Hamilton-Jacobi equation and the local RG equation, we show that the RG flow of holographic Perelman's entropy functional is nothing but the Weyl anomaly.","This leads us to the monotonicity of the RG flow of the emergent target spacetime.","Furthermore, considering the entropy production along the RG flow, we construct a microscopic entropy functional based on the probability distribution function of the holographic dual effective field theory, regarded as Gibbs or Shannon entropy.","We find that the monotonicity of this microscopically constructed entropy functional shows a strong connection with the monotonicity of the holographic Perelman's entropy functional."],"url":"http://arxiv.org/abs/2404.09122v1","category":"hep-th"}
{"created":"2024-04-14 02:07:14","title":"Causal Inference for Genomic Data with Multiple Heterogeneous Outcomes","abstract":"With the evolution of single-cell RNA sequencing techniques into a standard approach in genomics, it has become possible to conduct cohort-level causal inferences based on single-cell-level measurements. However, the individual gene expression levels of interest are not directly observable; instead, only repeated proxy measurements from each individual's cells are available, providing a derived outcome to estimate the underlying outcome for each of many genes. In this paper, we propose a generic semiparametric inference framework for doubly robust estimation with multiple derived outcomes, which also encompasses the usual setting of multiple outcomes when the response of each unit is available. To reliably quantify the causal effects of heterogeneous outcomes, we specialize the analysis to the standardized average treatment effects and the quantile treatment effects. Through this, we demonstrate the use of the semiparametric inferential results for doubly robust estimators derived from both Von Mises expansions and estimating equations. A multiple testing procedure based on the Gaussian multiplier bootstrap is tailored for doubly robust estimators to control the false discovery exceedance rate. Applications in single-cell CRISPR perturbation analysis and individual-level differential expression analysis demonstrate the utility of the proposed methods and offer insights into the usage of different estimands for causal inference in genomics.","sentences":["With the evolution of single-cell RNA sequencing techniques into a standard approach in genomics, it has become possible to conduct cohort-level causal inferences based on single-cell-level measurements.","However, the individual gene expression levels of interest are not directly observable; instead, only repeated proxy measurements from each individual's cells are available, providing a derived outcome to estimate the underlying outcome for each of many genes.","In this paper, we propose a generic semiparametric inference framework for doubly robust estimation with multiple derived outcomes, which also encompasses the usual setting of multiple outcomes when the response of each unit is available.","To reliably quantify the causal effects of heterogeneous outcomes, we specialize the analysis to the standardized average treatment effects and the quantile treatment effects.","Through this, we demonstrate the use of the semiparametric inferential results for doubly robust estimators derived from both Von Mises expansions and estimating equations.","A multiple testing procedure based on the Gaussian multiplier bootstrap is tailored for doubly robust estimators to control the false discovery exceedance rate.","Applications in single-cell CRISPR perturbation analysis and individual-level differential expression analysis demonstrate the utility of the proposed methods and offer insights into the usage of different estimands for causal inference in genomics."],"url":"http://arxiv.org/abs/2404.09119v1","category":"stat.ME"}
{"created":"2024-04-13 23:41:04","title":"Coherently controlling robust spin-orbit qubits of electrons in nanowire quantum dots","abstract":"We consider an electron confined in a gated nanowire quantum dot (NQD) with arbitrarily strong spin-orbit coupling (SOC) and weak static magnetic field, and treat the latter as a perturbation to seek the maximal spin-motion entangled states with the exact general solutions of the perturbed equations. From the boundedness and self-consistent conditions of the general solutions we find two corrected energies to any n level of the unperturbed system with ground state n = 0, which are much less than the unperturbed level-difference and corresponds to a spin-orbit qubit. We demonstrate the metastability of the two-level states and the decoherence-averse effect of SOC, and suggest an alternative scheme to perform the qubit control, simply by adjusting the orientation of magnetic field for any fixed SOC. Such a adjustment can lead to the spin flipping of the state vector and the position exchanging of the probability-density wavepackets which can be proposed as the non-Abelian quasiparticles. The results could be directly extended to a weakly coupled array of NQDs for coherently encoding the robust spin-orbit qubits.","sentences":["We consider an electron confined in a gated nanowire quantum dot (NQD) with arbitrarily strong spin-orbit coupling (SOC) and weak static magnetic field, and treat the latter as a perturbation to seek the maximal spin-motion entangled states with the exact general solutions of the perturbed equations.","From the boundedness and self-consistent conditions of the general solutions we find two corrected energies to any n level of the unperturbed system with ground state n = 0, which are much less than the unperturbed level-difference and corresponds to a spin-orbit qubit.","We demonstrate the metastability of the two-level states and the decoherence-averse effect of SOC, and suggest an alternative scheme to perform the qubit control, simply by adjusting the orientation of magnetic field for any fixed SOC.","Such a adjustment can lead to the spin flipping of the state vector and the position exchanging of the probability-density wavepackets which can be proposed as the non-Abelian quasiparticles.","The results could be directly extended to a weakly coupled array of NQDs for coherently encoding the robust spin-orbit qubits."],"url":"http://arxiv.org/abs/2404.09104v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-13 23:18:16","title":"Uniqueness of quasimonochromatic breathers for the generalized Korteweg-de Vries and Zakharov-Kuznetsov models","abstract":"Consider the generalized Korteweg-de Vries (gKdV) equations with power nonlinearities $q=2,3,4\\ldots$ in dimension $N=1$, and the Zakharov-Kuznetsov (ZK) model with integer power nonlinearities $q$ in higher dimensions $N\\geq 2$. Among these power-type models, the only conjectured equation with space localized time periodic breathers is the modified KdV (mKdV), corresponding to the case $q=3$ and $N=1$. Quasimonochromatic solutions were introduced by Mandel to show that sine-Gordon is the only scalar field model with breather solutions among this class. In this paper we consider smooth generalized quasimonochromatic solutions of arbitrary size for gKdV and ZK models and provide a rigorous proof that mKdV is the unique power-like model among them with spatially localized breathers of this type. In particular, we show the nonexistence of breathers of this class in the ZK models. The method of proof involves the use of the naturally coherent algebra of Bell's polynomials to obtain particularly distinctive structural elliptic PDEs satisfied by breather-like quasimonochromatic solutions. A reduction of the problem to the classification of solutions of these elliptic PDEs in the entire space is performed, and de Giorgi type uniqueness results are proved in this particular case, concluding the uniqueness of the mKdV breather, and the nonexistence of localized smooth breathers in the ZK case. No assumption on well-posedness is made, and the power of the nonlinearity is arbitrary.","sentences":["Consider the generalized Korteweg-de Vries (gKdV) equations with power nonlinearities $q=2,3,4\\ldots$ in dimension $N=1$, and the Zakharov-Kuznetsov (ZK) model with integer power nonlinearities $q$ in higher dimensions $N\\geq 2$.","Among these power-type models, the only conjectured equation with space localized time periodic breathers is the modified KdV (mKdV), corresponding to the case $q=3$ and $N=1$. Quasimonochromatic solutions were introduced by Mandel to show that sine-Gordon is the only scalar field model with breather solutions among this class.","In this paper we consider smooth generalized quasimonochromatic solutions of arbitrary size for gKdV and ZK models and provide a rigorous proof that mKdV is the unique power-like model among them with spatially localized breathers of this type.","In particular, we show the nonexistence of breathers of this class in the ZK models.","The method of proof involves the use of the naturally coherent algebra of Bell's polynomials to obtain particularly distinctive structural elliptic PDEs satisfied by breather-like quasimonochromatic solutions.","A reduction of the problem to the classification of solutions of these elliptic PDEs in the entire space is performed, and de Giorgi type uniqueness results are proved in this particular case, concluding the uniqueness of the mKdV breather, and the nonexistence of localized smooth breathers in the ZK case.","No assumption on well-posedness is made, and the power of the nonlinearity is arbitrary."],"url":"http://arxiv.org/abs/2404.09100v1","category":"math.AP"}
{"created":"2024-04-13 23:17:47","title":"The Physisorbate-Layer Problem Arising in Kinetic Theory of Gas-Surface Interaction","abstract":"A half-space problem of a linear kinetic equation for gas molecules physisorbed close to a solid surface, relevant to a kinetic model of gas-surface interactions and derived by Aoki et al. [K.~Aoki et al., in: Phys. Rev. E 106:035306, 2022], is considered. The equation contains a confinement potential in the vicinity of the solid surface and an interaction term between gas molecules and phonons. It is proved that a unique solution exists when the incoming molecular flux is specified at infinity. This validates the natural observation that the half-space problem serves as the boundary condition for the Boltzmann equation. It is also proved that the sequence of approximate solutions used for the existence proof converges exponentially fast. In addition, numerical results showing the details of the solution to the half-space problem are presented.","sentences":["A half-space problem of a linear kinetic equation for gas molecules physisorbed close to a solid surface, relevant to a kinetic model of gas-surface interactions and derived by Aoki et al.","[K.~Aoki et al., in: Phys.","Rev. E 106:035306, 2022], is considered.","The equation contains a confinement potential in the vicinity of the solid surface and an interaction term between gas molecules and phonons.","It is proved that a unique solution exists when the incoming molecular flux is specified at infinity.","This validates the natural observation that the half-space problem serves as the boundary condition for the Boltzmann equation.","It is also proved that the sequence of approximate solutions used for the existence proof converges exponentially fast.","In addition, numerical results showing the details of the solution to the half-space problem are presented."],"url":"http://arxiv.org/abs/2404.09099v1","category":"math.AP"}
{"created":"2024-04-13 21:39:01","title":"Photo-induced Multiply Quantized Vortex States in Dirac-like Materials","abstract":"Subjecting a massive two-dimensional Dirac material to a vortex light beam provides a mechanism for the photo-induction of multiply quantized vortices. Using Floquet theory, we show that electronic vortices, characterized by their total angular momentum, are exclusive to circularly polarized vortex beams. The equations for the driven system at the one photon-resonance are mapped to the Bogoliubov-de Gennes equations of $s$-wave superconductors with multiply quantized vortices. This mapping provides valuable analytical tools for the analysis of the system's spectral properties.","sentences":["Subjecting a massive two-dimensional Dirac material to a vortex light beam provides a mechanism for the photo-induction of multiply quantized vortices.","Using Floquet theory, we show that electronic vortices, characterized by their total angular momentum, are exclusive to circularly polarized vortex beams.","The equations for the driven system at the one photon-resonance are mapped to the Bogoliubov-de Gennes equations of $s$-wave superconductors with multiply quantized vortices.","This mapping provides valuable analytical tools for the analysis of the system's spectral properties."],"url":"http://arxiv.org/abs/2404.09086v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-13 21:21:13","title":"Noncommutative weighted shifts, joint similarity, and function theory in several variables","abstract":"The goal of this paper is to study the structure of noncommutative weighted shifts, their properties, and to understand their role as models (up to similarity) for $n$-tuples of operators on Hilbert spaces as well as their implications to function theory on noncommutative (resp.commutative) Reinhardt domains. We obtain a Rota type similarity result concerning the joint similarity of $n$-tuples of operators to parts of noncommutative weighted multi-shifts and provide a noncommutative multivariable analogue of Foias-Pearcy model for quasinilpotent operators. The model noncommutative weighted multi-shift which is studied in this paper is the $n$-tuple $W=(W_1,\\ldots, W_n)$, where $W_i$ are weighted left creation operators of the full Fock space with $n$ generators associated with a weight sequence $\\boldsymbol \\mu=\\{\\mu_\\beta\\}_{|\\beta|\\geq 1}$ of nonnegative numbers.   We also represent the injective weighted multi-shifts $W_1,\\ldots, W_n$ as ordinary multiplications by $Z_1,\\ldots, Z_n$ on a Hilbert space of noncommutative formal power series. This leads naturally to analytic function theory in several complex variables. One of the goal for the remainder of the paper is to analyze the extent to which our noncommutative formal power series represent analytic functions in several noncommutative (resp. commutative) variables and to develop a functional calculus for arbitrary $n$-tuples of operators on a Hilbert space.","sentences":["The goal of this paper is to study the structure of noncommutative weighted shifts, their properties, and to understand their role as models (up to similarity) for $n$-tuples of operators on Hilbert spaces as well as their implications to function theory on noncommutative (resp.commutative) Reinhardt domains.","We obtain a Rota type similarity result concerning the joint similarity of $n$-tuples of operators to parts of noncommutative weighted multi-shifts and provide a noncommutative multivariable analogue of Foias-Pearcy model for quasinilpotent operators.","The model noncommutative weighted multi-shift which is studied in this paper is the $n$-tuple $W=(W_1,\\ldots, W_n)$, where $W_i$ are weighted left creation operators of the full Fock space with $n$ generators associated with a weight sequence $\\boldsymbol \\mu=\\{\\mu_\\beta\\}_{|\\beta|\\geq 1}$ of nonnegative numbers.   ","We also represent the injective weighted multi-shifts $W_1,\\ldots, W_n$ as ordinary multiplications by $Z_1,\\ldots, Z_n$ on a Hilbert space of noncommutative formal power series.","This leads naturally to analytic function theory in several complex variables.","One of the goal for the remainder of the paper is to analyze the extent to which our noncommutative formal power series represent analytic functions in several noncommutative (resp. commutative) variables and to develop a functional calculus for arbitrary $n$-tuples of operators on a Hilbert space."],"url":"http://arxiv.org/abs/2404.09084v1","category":"math.FA"}
{"created":"2024-04-13 21:02:49","title":"Probabilistic Directed Distance Fields for Ray-Based Shape Representations","abstract":"In modern computer vision, the optimal representation of 3D shape continues to be task-dependent. One fundamental operation applied to such representations is differentiable rendering, as it enables inverse graphics approaches in learning frameworks. Standard explicit shape representations (voxels, point clouds, or meshes) are often easily rendered, but can suffer from limited geometric fidelity, among other issues. On the other hand, implicit representations (occupancy, distance, or radiance fields) preserve greater fidelity, but suffer from complex or inefficient rendering processes, limiting scalability. In this work, we devise Directed Distance Fields (DDFs), a novel neural shape representation that builds upon classical distance fields. The fundamental operation in a DDF maps an oriented point (position and direction) to surface visibility and depth. This enables efficient differentiable rendering, obtaining depth with a single forward pass per pixel, as well as differential geometric quantity extraction (e.g., surface normals), with only additional backward passes. Using probabilistic DDFs (PDDFs), we show how to model inherent discontinuities in the underlying field. We then apply DDFs to several applications, including single-shape fitting, generative modelling, and single-image 3D reconstruction, showcasing strong performance with simple architectural components via the versatility of our representation. Finally, since the dimensionality of DDFs permits view-dependent geometric artifacts, we conduct a theoretical investigation of the constraints necessary for view consistency. We find a small set of field properties that are sufficient to guarantee a DDF is consistent, without knowing, for instance, which shape the field is expressing.","sentences":["In modern computer vision, the optimal representation of 3D shape continues to be task-dependent.","One fundamental operation applied to such representations is differentiable rendering, as it enables inverse graphics approaches in learning frameworks.","Standard explicit shape representations (voxels, point clouds, or meshes) are often easily rendered, but can suffer from limited geometric fidelity, among other issues.","On the other hand, implicit representations (occupancy, distance, or radiance fields) preserve greater fidelity, but suffer from complex or inefficient rendering processes, limiting scalability.","In this work, we devise Directed Distance Fields (DDFs), a novel neural shape representation that builds upon classical distance fields.","The fundamental operation in a DDF maps an oriented point (position and direction) to surface visibility and depth.","This enables efficient differentiable rendering, obtaining depth with a single forward pass per pixel, as well as differential geometric quantity extraction (e.g., surface normals), with only additional backward passes.","Using probabilistic DDFs (PDDFs), we show how to model inherent discontinuities in the underlying field.","We then apply DDFs to several applications, including single-shape fitting, generative modelling, and single-image 3D reconstruction, showcasing strong performance with simple architectural components via the versatility of our representation.","Finally, since the dimensionality of DDFs permits view-dependent geometric artifacts, we conduct a theoretical investigation of the constraints necessary for view consistency.","We find a small set of field properties that are sufficient to guarantee a DDF is consistent, without knowing, for instance, which shape the field is expressing."],"url":"http://arxiv.org/abs/2404.09081v1","category":"cs.CV"}
{"created":"2024-04-13 20:46:41","title":"A static quantum embedding scheme based on coupled cluster theory","abstract":"We develop a static quantum embedding scheme, utilizing projection equations to solve coupled cluster (CC) amplitudes. To reduce the computational cost (for example, of a large basis set calculation), we solve the local fragment problem using a high-level coupled cluster method and address the environment problem with a lower-level M{\\o}ller-Plesset (MP) perturbative method. This embedding approach is consistently formulated within the coupled cluster framework and will be called MP-CC. We demonstrate the effectiveness of our method through several prototypical molecular examples by analyzing a global quantity, that is, the total correlation energy of the system in the study of potential energy curves (PEC) and thermochemical reaction energies. We have shown that our method can achieve comparable accuracy both with a small and large basis set when the fragment Hilbert space size remains the same. Additionally, our results indicate that increasing the fragment size can systematically enhance the accuracy of observables, approaching the precision of the full coupled cluster solver.","sentences":["We develop a static quantum embedding scheme, utilizing projection equations to solve coupled cluster (CC) amplitudes.","To reduce the computational cost (for example, of a large basis set calculation), we solve the local fragment problem using a high-level coupled cluster method and address the environment problem with a lower-level M{\\o}ller-Plesset (MP) perturbative method.","This embedding approach is consistently formulated within the coupled cluster framework and will be called MP-CC.","We demonstrate the effectiveness of our method through several prototypical molecular examples by analyzing a global quantity, that is, the total correlation energy of the system in the study of potential energy curves (PEC) and thermochemical reaction energies.","We have shown that our method can achieve comparable accuracy both with a small and large basis set when the fragment Hilbert space size remains the same.","Additionally, our results indicate that increasing the fragment size can systematically enhance the accuracy of observables, approaching the precision of the full coupled cluster solver."],"url":"http://arxiv.org/abs/2404.09078v1","category":"physics.chem-ph"}
{"created":"2024-04-13 19:13:39","title":"Dymnikova black hole from an infinite tower of higher-curvature corrections","abstract":"Recently, in [arXiv:2403.04827], it was demonstrated that various regular black hole metrics can be derived within a theory featuring an infinite number of higher curvature corrections to General Relativity. Moreover, truncating this infinite series at the first few orders already yields a reliable approximation of the observable characteristics of such black holes [arXiv:2403.07848]. Here, we further establish the existence of another regular black hole solution, particularly the $D$-dimensional extension of the Dymnikova black hole, within the equations of motion incorporating an infinite tower of higher-curvature corrections. This solution is essentially nonperturbative in the coupling parameter, rendering the action, if it exists, incapable of being approximated by a finite number of powers of the curvature.","sentences":["Recently, in [arXiv:2403.04827], it was demonstrated that various regular black hole metrics can be derived within a theory featuring an infinite number of higher curvature corrections to General Relativity.","Moreover, truncating this infinite series at the first few orders already yields a reliable approximation of the observable characteristics of such black holes","[arXiv:2403.07848].","Here, we further establish the existence of another regular black hole solution, particularly the $D$-dimensional extension of the Dymnikova black hole, within the equations of motion incorporating an infinite tower of higher-curvature corrections.","This solution is essentially nonperturbative in the coupling parameter, rendering the action, if it exists, incapable of being approximated by a finite number of powers of the curvature."],"url":"http://arxiv.org/abs/2404.09063v1","category":"gr-qc"}
{"created":"2024-04-13 18:18:02","title":"Off-diagonally symmetric domino tilings of the Aztec diamond of odd order","abstract":"We study the enumeration of off-diagonally symmetric domino tilings of odd-order Aztec diamonds in two directions: (1) with one boundary defect, and (2) with maximally-many zeroes on the diagonal. In the first direction, we prove a symmetry property which states that the numbers of off-diagonally symmetric domino tilings of the Aztec diamond of order $2n-1$ are equal when the boundary defect is at the $k$th position and the $(2n-k)$th position on the boundary, respectively. This symmetry property proves a special case of a recent conjecture by Behrend, Fischer, and Koutschan.   In the second direction, a Pfaffian formula is obtained for the number of ``nearly'' off-diagonally symmetric domino tilings of odd-order Aztec diamonds, where the entries of the Pfaffian satisfy a simple recurrence relation. The numbers of domino tilings mentioned in the above two directions do not seem to have a simple product formula, but we show that these numbers satisfy simple matrix equations in which the entries of the matrix are given by Delannoy numbers. The proof of these results involves the method of non-intersecting lattice paths and a modification of Stembridge's Pfaffian formula for families of non-intersecting lattice paths. Finally, we propose conjectures concerning the log-concavity and asymptotic behavior of the number of off-diagonally symmetric domino tilings of odd-order Aztec diamonds.","sentences":["We study the enumeration of off-diagonally symmetric domino tilings of odd-order Aztec diamonds in two directions: (1) with one boundary defect, and (2) with maximally-many zeroes on the diagonal.","In the first direction, we prove a symmetry property which states that the numbers of off-diagonally symmetric domino tilings of the Aztec diamond of order $2n-1$ are equal when the boundary defect is at the $k$th position and the $(2n-k)$th position on the boundary, respectively.","This symmetry property proves a special case of a recent conjecture by Behrend, Fischer, and Koutschan.   ","In the second direction, a Pfaffian formula is obtained for the number of ``nearly'' off-diagonally symmetric domino tilings of odd-order Aztec diamonds, where the entries of the Pfaffian satisfy a simple recurrence relation.","The numbers of domino tilings mentioned in the above two directions do not seem to have a simple product formula, but we show that these numbers satisfy simple matrix equations in which the entries of the matrix are given by Delannoy numbers.","The proof of these results involves the method of non-intersecting lattice paths and a modification of Stembridge's Pfaffian formula for families of non-intersecting lattice paths.","Finally, we propose conjectures concerning the log-concavity and asymptotic behavior of the number of off-diagonally symmetric domino tilings of odd-order Aztec diamonds."],"url":"http://arxiv.org/abs/2404.09057v1","category":"math.CO"}
{"created":"2024-04-13 17:37:29","title":"Semilinear Klein-Gordon equation in space-time of black hole, which is gaining mass in the universe with accelerating expansion","abstract":"In this paper, we prove the existence of global in-time small-data solutions of semilinear Klein-Gordon equations in space-time with a static Schwarzschild radius and variable mass in the expanding universe.","sentences":["In this paper, we prove the existence of global in-time small-data solutions of semilinear Klein-Gordon equations in space-time with a static Schwarzschild radius and variable mass in the expanding universe."],"url":"http://arxiv.org/abs/2404.09054v1","category":"math.AP"}
{"created":"2024-04-13 17:28:55","title":"Efficient discretization of the Laplacian on complex geometries","abstract":"Highly accurate simulations of problems including second derivatives on complex geometries are of primary interest in academia and industry. Consider for example the Navier-Stokes equations or wave propagation problems of acoustic or elastic waves. Current finite difference discretization methods are accurate and efficient on modern hardware, but they lack flexibility when it comes to complex geometries. In this work I extend the continuous summation-by-parts (SBP) framework to second derivatives and combine it with spectral-type SBP operators on Gauss-Lobatto quadrature points to obtain a highly efficient discretization (accurate with respect to runtime) of the Laplacian on complex domains. The resulting Laplace operator is defined on a grid without duplicated points on the interfaces, thus removing unnecessary degrees of freedom in the scheme, and is proven to satisfy a discrete equivalent to Green's first identity. Semi-discrete stability using the new Laplace operator is proven for the acoustic wave equation in 2D. Furthermore, the method can easily be coupled together with traditional finite difference operators using glue-grid interpolation operators, resulting in a method with great practical potential. Two numerical experiments are done on the acoustic wave equation in 2D. First on a problem with an analytical solution, demonstrating the accuracy and efficiency properties of the method. Finally, a more realistic problem is solved, where a complex region of the domain is discretized using the new method and coupled to the rest of the domain discretized using a traditional finite difference method.","sentences":["Highly accurate simulations of problems including second derivatives on complex geometries are of primary interest in academia and industry.","Consider for example the Navier-Stokes equations or wave propagation problems of acoustic or elastic waves.","Current finite difference discretization methods are accurate and efficient on modern hardware, but they lack flexibility when it comes to complex geometries.","In this work I extend the continuous summation-by-parts (SBP) framework to second derivatives and combine it with spectral-type SBP operators on Gauss-Lobatto quadrature points to obtain a highly efficient discretization (accurate with respect to runtime) of the Laplacian on complex domains.","The resulting Laplace operator is defined on a grid without duplicated points on the interfaces, thus removing unnecessary degrees of freedom in the scheme, and is proven to satisfy a discrete equivalent to Green's first identity.","Semi-discrete stability using the new Laplace operator is proven for the acoustic wave equation in 2D. Furthermore, the method can easily be coupled together with traditional finite difference operators using glue-grid interpolation operators, resulting in a method with great practical potential.","Two numerical experiments are done on the acoustic wave equation in 2D. First on a problem with an analytical solution, demonstrating the accuracy and efficiency properties of the method.","Finally, a more realistic problem is solved, where a complex region of the domain is discretized using the new method and coupled to the rest of the domain discretized using a traditional finite difference method."],"url":"http://arxiv.org/abs/2404.09050v1","category":"math.NA"}
{"created":"2024-04-13 17:15:08","title":"Phase-Amplitude Description of Stochastic Oscillators: A Parameterization Method Approach","abstract":"The parameterization method (PM) provides a broad theoretical and numerical foundation for computing invariant manifolds of dynamical systems. PM implements a change of variables in order to represent trajectories of a system of ordinary differential equations ``as simply as possible.\" In this paper we pursue a similar goal for stochastic oscillator systems. For planar nonlinear stochastic systems that are ``robustly oscillatory\", we find a change of variables through which the dynamics are as simple as possible $\\textit{in the mean}$. We prove existence and uniqueness of a deterministic vector field, the trajectories of which capture the local mean behavior of the stochastic oscillator. We illustrate the construction of such an ``effective vector field\" for several examples, including a limit cycle oscillator perturbed by noise, an excitable system derived from a spiking neuron model, and a spiral sink with noise forcing (2D Ornstein-Uhlenbeck process). The latter examples comprise contingent oscillators that would not sustain rhythmic activity without noise forcing. Finally, we exploit the simplicity of the dynamics after the change of variables to obtain the effective diffusion constant of the resulting phase variable, and the stationary variance of the resulting amplitude (isostable) variable.","sentences":["The parameterization method (PM) provides a broad theoretical and numerical foundation for computing invariant manifolds of dynamical systems.","PM implements a change of variables in order to represent trajectories of a system of ordinary differential equations ``as simply as possible.\"","In this paper we pursue a similar goal for stochastic oscillator systems.","For planar nonlinear stochastic systems that are ``robustly oscillatory\", we find a change of variables through which the dynamics are as simple as possible $\\textit{in the mean}$. We prove existence and uniqueness of a deterministic vector field, the trajectories of which capture the local mean behavior of the stochastic oscillator.","We illustrate the construction of such an ``effective vector field\" for several examples, including a limit cycle oscillator perturbed by noise, an excitable system derived from a spiking neuron model, and a spiral sink with noise forcing (2D Ornstein-Uhlenbeck process).","The latter examples comprise contingent oscillators that would not sustain rhythmic activity without noise forcing.","Finally, we exploit the simplicity of the dynamics after the change of variables to obtain the effective diffusion constant of the resulting phase variable, and the stationary variance of the resulting amplitude (isostable) variable."],"url":"http://arxiv.org/abs/2404.09046v1","category":"math.DS"}
{"created":"2024-04-13 16:50:37","title":"Combinatorics of Complex Maximal Determinant Matrices","abstract":"This doctoral thesis covers several topics related to the construction and study of maximal determinant matrices with complex entries. The first three chapters are devoted to number-theoretic tools to prove the non-solvability of Gram matrix equations over certain fields, with a focus on combinatorial applications. Chapter 4 gives a survey on Butson-type Hadamard matrices, and shows an improved lower bound on primes $p$ for the existence of $BH(12p, p)$ matrices. Chapter 5 contains the main contributions of the thesis, where the maximal determinant problem for matrices over the m-th roots of unity is discussed, and where new upper and lower bounds, as well as constructions at small orders, are given. Chapter 6 studies maximal determinant matrices over association schemes. Chapter 7 gives an application of design theory to privacy in communications, and it is connected to the rest of the thesis by the use of the theory of quadratic forms.","sentences":["This doctoral thesis covers several topics related to the construction and study of maximal determinant matrices with complex entries.","The first three chapters are devoted to number-theoretic tools to prove the non-solvability of Gram matrix equations over certain fields, with a focus on combinatorial applications.","Chapter 4 gives a survey on Butson-type Hadamard matrices, and shows an improved lower bound on primes $p$ for the existence of $BH(12p, p)$ matrices.","Chapter 5 contains the main contributions of the thesis, where the maximal determinant problem for matrices over the m-th roots of unity is discussed, and where new upper and lower bounds, as well as constructions at small orders, are given.","Chapter 6 studies maximal determinant matrices over association schemes.","Chapter 7 gives an application of design theory to privacy in communications, and it is connected to the rest of the thesis by the use of the theory of quadratic forms."],"url":"http://arxiv.org/abs/2404.09040v1","category":"math.CO"}
{"created":"2024-04-13 16:27:26","title":"The Hessian geometry of the ideal gas in rotation","abstract":"We study the Hessian geometry associated with an ideal gas in a spherical centrifuge. According to Souriau, a spherically confined ideal gas admit states of thermal and rotational equilibrium. These states, called Gibbs states, form an exponential family with an action of the Euclidean rotation group. We investigate its Hessian (Fisher-Rao) geometry and show that in the high angular velocity limit, the geometry can be compared to the Hessian geometry of a spherical rigid body which we show to be isometric to a hyperbolic space.","sentences":["We study the Hessian geometry associated with an ideal gas in a spherical centrifuge.","According to Souriau, a spherically confined ideal gas admit states of thermal and rotational equilibrium.","These states, called Gibbs states, form an exponential family with an action of the Euclidean rotation group.","We investigate its Hessian (Fisher-Rao) geometry and show that in the high angular velocity limit, the geometry can be compared to the Hessian geometry of a spherical rigid body which we show to be isometric to a hyperbolic space."],"url":"http://arxiv.org/abs/2404.09035v1","category":"math-ph"}
{"created":"2024-04-13 14:35:49","title":"Existence of solutions for a class of integro-differential equations with the logarithmic Laplacian and transport","abstract":"In this paper, we consider an integro-differential equation in L^2(R), which involves the logarithmic Laplacian in the presence of a drift term. The linear operator associated with the problem has the Fredholm property. By using a fixed point technique, we establish the existence of solutions.","sentences":["In this paper, we consider an integro-differential equation in L^2(R), which involves the logarithmic Laplacian in the presence of a drift term.","The linear operator associated with the problem has the Fredholm property.","By using a fixed point technique, we establish the existence of solutions."],"url":"http://arxiv.org/abs/2404.09019v1","category":"math.AP"}
{"created":"2024-04-13 13:37:14","title":"Statistics for Iwasawa invariants of elliptic curves, $\\rm{III}$","abstract":"Given a prime $p\\geq 5$, a conjecture of Greenberg predicts that the $\\mu$-invariant of the $p$-primary Selmer group should vanish for most elliptic curves with good ordinary reduction at $p$. In support of this conjecture, I show that the $5$-primary Iwasawa $\\mu$- and $\\lambda$-invariants simultaneously vanish for an explicit positive density of elliptic curves $E_{/\\mathbb{Q}}$. The elliptic curves in question have good ordinary reduction at $5$, and are ordered by their height. The results are proven by leveraging work of Bhargava and Shankar on the distribution of $5$-Selmer groups of elliptic curves defined over $\\mathbb{Q}$.","sentences":["Given a prime $p\\geq 5$, a conjecture of Greenberg predicts that the $\\mu$-invariant of the $p$-primary Selmer group should vanish for most elliptic curves with good ordinary reduction at $p$. In support of this conjecture, I show that the $5$-primary Iwasawa $\\mu$- and $\\lambda$-invariants simultaneously vanish for an explicit positive density of elliptic curves $E_{/\\mathbb{Q}}$. The elliptic curves in question have good ordinary reduction at $5$, and are ordered by their height.","The results are proven by leveraging work of Bhargava and Shankar on the distribution of $5$-Selmer groups of elliptic curves defined over $\\mathbb{Q}$."],"url":"http://arxiv.org/abs/2404.09009v1","category":"math.NT"}
{"created":"2024-04-13 13:01:39","title":"Status of Electromagnetic Accelerating Universe","abstract":"To describe the dark side of the Universe, we adopt a novel approach where dark energy is explained as an electrically charged majority of dark matter. Dark energy, as such, does not exist. The Friedmann equation at the present time coincides with that in a conventional approach, although the cosmological \"constant\" in the Electromagnetic Accelerating Universe (EAU) Model shares a time dependence with the matter component. Its equation of state is $\\omega \\equiv P/\\rho \\equiv -1$ within observational accuracy.","sentences":["To describe the dark side of the Universe, we adopt a novel approach where dark energy is explained as an electrically charged majority of dark matter.","Dark energy, as such, does not exist.","The Friedmann equation at the present time coincides with that in a conventional approach, although the cosmological \"constant\" in the Electromagnetic Accelerating Universe (EAU) Model shares a time dependence with the matter component.","Its equation of state is $\\omega \\equiv P/\\rho","\\equiv -1$ within observational accuracy."],"url":"http://arxiv.org/abs/2404.08999v1","category":"gr-qc"}
{"created":"2024-04-13 12:37:56","title":"Interferometer measurements in interstellar communications: methods and observations","abstract":"Extraterrestrial communication signals are hypothesized to be present in an extensive search space. Using principles of communication theory and system design, methods are studied and implemented to reduce the signal search space, while considering intentional transmitter detectability. The design and observational work reported in this paper adds material to previous related reports. (ref. arXiv:2105.03727, arXiv:2106.10168, arXiv:2202.12791, arXiv:2203.10065). In the current work, a two-element radio interferometer telescope and receiver algorithms are utilized to perform differential angle-of-arrival and multi-bandwidth measurements of delta-t delta-f polarized pulse pairs. The system enhances extraterrestrial signal detectability, while reducing signal false positives caused by noise and radio frequency interference. Statistical analysis utilizes a Right Ascension filter spanning celestial coordinate ranges that include the previously determined anomalous celestial direction: 5.25 +- 0.15 hr Right Ascension, -7.6 degrees +- 1 degree Declination. Observations were conducted during a duration of 61 days, comprising 244 hours of interferometer measurements.","sentences":["Extraterrestrial communication signals are hypothesized to be present in an extensive search space.","Using principles of communication theory and system design, methods are studied and implemented to reduce the signal search space, while considering intentional transmitter detectability.","The design and observational work reported in this paper adds material to previous related reports.","(ref.","arXiv:2105.03727, arXiv:2106.10168, arXiv:2202.12791, arXiv:2203.10065).","In the current work, a two-element radio interferometer telescope and receiver algorithms are utilized to perform differential angle-of-arrival and multi-bandwidth measurements of delta-t delta-f polarized pulse pairs.","The system enhances extraterrestrial signal detectability, while reducing signal false positives caused by noise and radio frequency interference.","Statistical analysis utilizes a Right Ascension filter spanning celestial coordinate ranges that include the previously determined anomalous celestial direction: 5.25 +- 0.15 hr","Right Ascension, -7.6 degrees +- 1 degree Declination.","Observations were conducted during a duration of 61 days, comprising 244 hours of interferometer measurements."],"url":"http://arxiv.org/abs/2404.08994v1","category":"eess.SP"}
{"created":"2024-04-13 12:09:40","title":"Correlation between the charge radii difference in mirror partner nuclei and the symmetry energy slope","abstract":"Correlation between the charge radii difference of mirror partner nuclei $\\Delta{R_{\\mathrm{ch}}}$ and the slope parameter $L$ of symmetry energy has been built to ascertain the equation of state of isospin asymmetric nuclear matter. In this work, the influence of pairing correlations and isoscalar compression modulus on the $\\Delta{R_{\\mathrm{ch}}}$ are systematically investigated by Skyrme density functionals theory. The calculated results suggest that the linear correlation between $\\Delta{R_{\\mathrm{ch}}}$ and $L$ is decreased by the surface pairing correlations. The slope parameter deduced from the difference of charge radii of mirror-pair nuclei $^{32}$Ar-$^{32}$Si, $^{36}$Ca-$^{36}$S, $^{38}$Ca-$^{38}$Ar, and $^{54}$Ni-$^{54}$Fe falls into the range of $L=42.57$-$50.64$ MeV, namely the rather soft equation of state of asymmetric nuclear matter. Besides, the range of slope parameter can also be influenced by the effective forces classified by various isoscalar incompressibility coefficients.","sentences":["Correlation between the charge radii difference of mirror partner nuclei $\\Delta{R_{\\mathrm{ch}}}$ and the slope parameter $L$ of symmetry energy has been built to ascertain the equation of state of isospin asymmetric nuclear matter.","In this work, the influence of pairing correlations and isoscalar compression modulus on the $\\Delta{R_{\\mathrm{ch}}}$ are systematically investigated by Skyrme density functionals theory.","The calculated results suggest that the linear correlation between $\\Delta{R_{\\mathrm{ch}}}$ and $L$ is decreased by the surface pairing correlations.","The slope parameter deduced from the difference of charge radii of mirror-pair nuclei $^{32}$Ar-$^{32}$Si, $^{36}$Ca-$^{36}$S, $^{38}$Ca-$^{38}$Ar, and $^{54}$Ni-$^{54}$Fe falls into the range of $L=42.57$-$50.64$ MeV, namely the rather soft equation of state of asymmetric nuclear matter.","Besides, the range of slope parameter can also be influenced by the effective forces classified by various isoscalar incompressibility coefficients."],"url":"http://arxiv.org/abs/2404.08982v1","category":"nucl-th"}
{"created":"2024-04-13 12:07:20","title":"Stability and Generalization in Free Adversarial Training","abstract":"While adversarial training methods have resulted in significant improvements in the deep neural nets' robustness against norm-bounded adversarial perturbations, their generalization performance from training samples to test data has been shown to be considerably worse than standard empirical risk minimization methods. Several recent studies seek to connect the generalization behavior of adversarially trained classifiers to various gradient-based min-max optimization algorithms used for their training. In this work, we study the generalization performance of adversarial training methods using the algorithmic stability framework. Specifically, our goal is to compare the generalization performance of the vanilla adversarial training scheme fully optimizing the perturbations at every iteration vs. the free adversarial training simultaneously optimizing the norm-bounded perturbations and classifier parameters. Our proven generalization bounds indicate that the free adversarial training method could enjoy a lower generalization gap between training and test samples due to the simultaneous nature of its min-max optimization algorithm. We perform several numerical experiments to evaluate the generalization performance of vanilla, fast, and free adversarial training methods. Our empirical findings also show the improved generalization performance of the free adversarial training method and further demonstrate that the better generalization result could translate to greater robustness against black-box attack schemes. The code is available at https://github.com/Xiwei-Cheng/Stability_FreeAT.","sentences":["While adversarial training methods have resulted in significant improvements in the deep neural nets' robustness against norm-bounded adversarial perturbations, their generalization performance from training samples to test data has been shown to be considerably worse than standard empirical risk minimization methods.","Several recent studies seek to connect the generalization behavior of adversarially trained classifiers to various gradient-based min-max optimization algorithms used for their training.","In this work, we study the generalization performance of adversarial training methods using the algorithmic stability framework.","Specifically, our goal is to compare the generalization performance of the vanilla adversarial training scheme fully optimizing the perturbations at every iteration vs. the free adversarial training simultaneously optimizing the norm-bounded perturbations and classifier parameters.","Our proven generalization bounds indicate that the free adversarial training method could enjoy a lower generalization gap between training and test samples due to the simultaneous nature of its min-max optimization algorithm.","We perform several numerical experiments to evaluate the generalization performance of vanilla, fast, and free adversarial training methods.","Our empirical findings also show the improved generalization performance of the free adversarial training method and further demonstrate that the better generalization result could translate to greater robustness against black-box attack schemes.","The code is available at https://github.com/Xiwei-Cheng/Stability_FreeAT."],"url":"http://arxiv.org/abs/2404.08980v1","category":"cs.LG"}
{"created":"2024-04-13 10:23:36","title":"Local control on quaternionic Heisenberg group of dimension $7$","abstract":"We describe the quaternionic Heisenberg group in the dimension $7$ as a matrix group. We study the local control of a compatible left-invariant control system. We describe the impact of symmetries of the corresponding sub-Riemannian structure on the optimality of geodesics.","sentences":["We describe the quaternionic Heisenberg group in the dimension $7$ as a matrix group.","We study the local control of a compatible left-invariant control system.","We describe the impact of symmetries of the corresponding sub-Riemannian structure on the optimality of geodesics."],"url":"http://arxiv.org/abs/2404.08953v1","category":"math.DG"}
{"created":"2024-04-13 10:17:58","title":"Order-lifted data inversion/retrieval method of neighbor cells to implement general high-order schemes in unstructured-mesh-based finite-volume solution framework","abstract":"This study introduces an order-lifted inversion/retrieval method for implementing high-order schemes within the framework of an unstructured-mesh-based finite-volume method. This method defines a special representation called the data order-lifted inversion of neighbor cells (DOLINC) differential, which transforms the degrees of freedom of wide templates into differentials of various orders stored in local grid cells. Furthermore, to retrieve the original far-field information without bias during the reconstruction/interpolation of face values, the corresponding accurate inversion formulas are derived based on the defined DOLINC differentials. The order-lifted inversion method can be applied to multi-dimensional polyhedral-mesh solvers by considering the influence of grid non-uniformity on high-order schemes. It seamlessly accommodates multi-process parallel computing for high-order methods without requiring special consideration for the boundary interface. This method not only enhances the numerical accuracy of second-order finite-volume methods, but also demonstrates a significant computational-speed advantage over similar methods. A series of benchmark cases, including the linear advection, Burgers, and Euler equations, are comprehensively validated to assess the practical performance of the method. The results indicate that the unstructured-mesh high-order schemes implemented based on this method achieve theoretical accuracy in practical computations and substantially reduce computational costs compared with methods that increase grid resolution.","sentences":["This study introduces an order-lifted inversion/retrieval method for implementing high-order schemes within the framework of an unstructured-mesh-based finite-volume method.","This method defines a special representation called the data order-lifted inversion of neighbor cells (DOLINC) differential, which transforms the degrees of freedom of wide templates into differentials of various orders stored in local grid cells.","Furthermore, to retrieve the original far-field information without bias during the reconstruction/interpolation of face values, the corresponding accurate inversion formulas are derived based on the defined DOLINC differentials.","The order-lifted inversion method can be applied to multi-dimensional polyhedral-mesh solvers by considering the influence of grid non-uniformity on high-order schemes.","It seamlessly accommodates multi-process parallel computing for high-order methods without requiring special consideration for the boundary interface.","This method not only enhances the numerical accuracy of second-order finite-volume methods, but also demonstrates a significant computational-speed advantage over similar methods.","A series of benchmark cases, including the linear advection, Burgers, and Euler equations, are comprehensively validated to assess the practical performance of the method.","The results indicate that the unstructured-mesh high-order schemes implemented based on this method achieve theoretical accuracy in practical computations and substantially reduce computational costs compared with methods that increase grid resolution."],"url":"http://arxiv.org/abs/2404.08952v1","category":"physics.flu-dyn"}
{"created":"2024-04-13 10:13:07","title":"Deep Reinforcement Learning based Online Scheduling Policy for Deep Neural Network Multi-Tenant Multi-Accelerator Systems","abstract":"Currently, there is a growing trend of outsourcing the execution of DNNs to cloud services. For service providers, managing multi-tenancy and ensuring high-quality service delivery, particularly in meeting stringent execution time constraints, assumes paramount importance, all while endeavoring to maintain cost-effectiveness. In this context, the utilization of heterogeneous multi-accelerator systems becomes increasingly relevant. This paper presents RELMAS, a low-overhead deep reinforcement learning algorithm designed for the online scheduling of DNNs in multi-tenant environments, taking into account the dataflow heterogeneity of accelerators and memory bandwidths contentions. By doing so, service providers can employ the most efficient scheduling policy for user requests, optimizing Service-Level-Agreement (SLA) satisfaction rates and enhancing hardware utilization. The application of RELMAS to a heterogeneous multi-accelerator system composed of various instances of Simba and Eyeriss sub-accelerators resulted in up to a 173% improvement in SLA satisfaction rate compared to state-of-the-art scheduling techniques across different workload scenarios, with less than a 1.5% energy overhead.","sentences":["Currently, there is a growing trend of outsourcing the execution of DNNs to cloud services.","For service providers, managing multi-tenancy and ensuring high-quality service delivery, particularly in meeting stringent execution time constraints, assumes paramount importance, all while endeavoring to maintain cost-effectiveness.","In this context, the utilization of heterogeneous multi-accelerator systems becomes increasingly relevant.","This paper presents RELMAS, a low-overhead deep reinforcement learning algorithm designed for the online scheduling of DNNs in multi-tenant environments, taking into account the dataflow heterogeneity of accelerators and memory bandwidths contentions.","By doing so, service providers can employ the most efficient scheduling policy for user requests, optimizing Service-Level-Agreement (SLA) satisfaction rates and enhancing hardware utilization.","The application of RELMAS to a heterogeneous multi-accelerator system composed of various instances of Simba and Eyeriss sub-accelerators resulted in up to a 173% improvement in SLA satisfaction rate compared to state-of-the-art scheduling techniques across different workload scenarios, with less than a 1.5% energy overhead."],"url":"http://arxiv.org/abs/2404.08950v1","category":"cs.AR"}
{"created":"2024-04-13 08:40:11","title":"Inhomogeneous brane models","abstract":"Intrinsic Conformal Symmetries have emerged as a tool for developing viable and physically coherent models, particularly in scenarios incorporating a negative cosmological constant. This study demonstrates that when the bulk geometry accommodates a set of $10$ Intrinsic Conformal Symmetries, which operate on three-dimensional hypersurfaces (be they spacelike or timelike), it leads to the existence of two distinct families of five-dimensional spacetimes. These spacetimes represent the general solutions to the bulk field equations. An aspect of these models is the composition of their energy-momentum tensor, which includes two key components: a negative cosmological constant and a parallel pressure $p_{\\parallel}$ aligned with the extra spatial dimension. This framework can be perceived as a $5D$ extension of the recently identified Spatially Inhomogeneous and Irrotational spacetimes. Significantly, these models offer a novel perspective for investigating the impacts of spatial inhomogeneity on the cosmological evolution of the Universe, particularly within the context of the Randall-Sundrum theory. This exploration is essential for deepening our understanding of the role of higher dimensions in cosmological models.","sentences":["Intrinsic Conformal Symmetries have emerged as a tool for developing viable and physically coherent models, particularly in scenarios incorporating a negative cosmological constant.","This study demonstrates that when the bulk geometry accommodates a set of $10$ Intrinsic Conformal Symmetries, which operate on three-dimensional hypersurfaces (be they spacelike or timelike), it leads to the existence of two distinct families of five-dimensional spacetimes.","These spacetimes represent the general solutions to the bulk field equations.","An aspect of these models is the composition of their energy-momentum tensor, which includes two key components: a negative cosmological constant and a parallel pressure $p_{\\parallel}$ aligned with the extra spatial dimension.","This framework can be perceived as a $5D$ extension of the recently identified Spatially Inhomogeneous and Irrotational spacetimes.","Significantly, these models offer a novel perspective for investigating the impacts of spatial inhomogeneity on the cosmological evolution of the Universe, particularly within the context of the Randall-Sundrum theory.","This exploration is essential for deepening our understanding of the role of higher dimensions in cosmological models."],"url":"http://arxiv.org/abs/2404.08929v1","category":"gr-qc"}
{"created":"2024-04-15 12:38:46","title":"Quantization of Large Language Models with an Overdetermined Basis","abstract":"In this paper, we introduce an algorithm for data quantization based on the principles of Kashin representation. This approach hinges on decomposing any given vector, matrix, or tensor into two factors. The first factor maintains a small infinity norm, while the second exhibits a similarly constrained norm when multiplied by an orthogonal matrix. Surprisingly, the entries of factors after decomposition are well-concentrated around several peaks, which allows us to efficiently replace them with corresponding centroids for quantization purposes. We study the theoretical properties of the proposed approach and rigorously evaluate our compression algorithm in the context of next-word prediction tasks and on a set of downstream tasks for text classification. Our findings demonstrate that Kashin Quantization achieves competitive or superior quality in model performance while ensuring data compression, marking a significant advancement in the field of data quantization.","sentences":["In this paper, we introduce an algorithm for data quantization based on the principles of Kashin representation.","This approach hinges on decomposing any given vector, matrix, or tensor into two factors.","The first factor maintains a small infinity norm, while the second exhibits a similarly constrained norm when multiplied by an orthogonal matrix.","Surprisingly, the entries of factors after decomposition are well-concentrated around several peaks, which allows us to efficiently replace them with corresponding centroids for quantization purposes.","We study the theoretical properties of the proposed approach and rigorously evaluate our compression algorithm in the context of next-word prediction tasks and on a set of downstream tasks for text classification.","Our findings demonstrate that Kashin Quantization achieves competitive or superior quality in model performance while ensuring data compression, marking a significant advancement in the field of data quantization."],"url":"http://arxiv.org/abs/2404.09737v1","category":"cs.LG"}
{"created":"2024-04-15 11:51:20","title":"MAM-STM: A software for autonomous control of single moieties towards specific surface positions","abstract":"In this publication we introduce MAM-STM, a software to autonomously manipulate arbitrary moieties towards specific positions on a metal surface utilizing the tip of a scanning tunneling microscope (STM). Finding the optimal manipulation parameters for a specific moiety is challenging and time consuming, even for human experts. MAM-STM combines autonomous data acquisition with a sophisticated Q-learning implementation to determine the optimal bias voltage, the z-approach distance, and the tip position relative to the moiety. This then allows to arrange single molecules and atoms at will. In this work, we provide a tutorial based on a simulated response to offer a comprehensive explanation on how to use and customize MAM-STM. Additionally, we assess the performance of the machine learning algorithm by benchmarking it within a simulated stochastic environment.","sentences":["In this publication we introduce MAM-STM, a software to autonomously manipulate arbitrary moieties towards specific positions on a metal surface utilizing the tip of a scanning tunneling microscope (STM).","Finding the optimal manipulation parameters for a specific moiety is challenging and time consuming, even for human experts.","MAM-STM combines autonomous data acquisition with a sophisticated Q-learning implementation to determine the optimal bias voltage, the z-approach distance, and the tip position relative to the moiety.","This then allows to arrange single molecules and atoms at will.","In this work, we provide a tutorial based on a simulated response to offer a comprehensive explanation on how to use and customize MAM-STM.","Additionally, we assess the performance of the machine learning algorithm by benchmarking it within a simulated stochastic environment."],"url":"http://arxiv.org/abs/2404.09694v1","category":"physics.app-ph"}
{"created":"2024-04-15 11:37:40","title":"AntBatchInfer: Elastic Batch Inference in the Kubernetes Cluster","abstract":"Offline batch inference is a common task in the industry for deep learning applications, but it can be challenging to ensure stability and performance when dealing with large amounts of data and complicated inference pipelines. This paper demonstrated AntBatchInfer, an elastic batch inference framework, which is specially optimized for the non-dedicated cluster. AntBatchInfer addresses these challenges by providing multi-level fault-tolerant capabilities, enabling the stable execution of versatile and long-running inference tasks. It also improves inference efficiency by pipelining, intra-node, and inter-node scaling. It further optimizes the performance in complicated multiple-model batch inference scenarios. Through extensive experiments and real-world statistics, we demonstrate the superiority of our framework in terms of stability and efficiency. In the experiment, it outperforms the baseline by at least $2\\times$ and $6\\times$ in the single-model or multiple-model batch inference. Also, it is widely used at Ant Group, with thousands of daily jobs from various scenarios, including DLRM, CV, and NLP, which proves its practicability in the industry.","sentences":["Offline batch inference is a common task in the industry for deep learning applications, but it can be challenging to ensure stability and performance when dealing with large amounts of data and complicated inference pipelines.","This paper demonstrated AntBatchInfer, an elastic batch inference framework, which is specially optimized for the non-dedicated cluster.","AntBatchInfer addresses these challenges by providing multi-level fault-tolerant capabilities, enabling the stable execution of versatile and long-running inference tasks.","It also improves inference efficiency by pipelining, intra-node, and inter-node scaling.","It further optimizes the performance in complicated multiple-model batch inference scenarios.","Through extensive experiments and real-world statistics, we demonstrate the superiority of our framework in terms of stability and efficiency.","In the experiment, it outperforms the baseline by at least $2\\times$ and $6\\times$ in the single-model or multiple-model batch inference.","Also, it is widely used at Ant Group, with thousands of daily jobs from various scenarios, including DLRM, CV, and NLP, which proves its practicability in the industry."],"url":"http://arxiv.org/abs/2404.09686v1","category":"cs.LG"}
{"created":"2024-04-15 11:36:31","title":"Post-Training Network Compression for 3D Medical Image Segmentation: Reducing Computational Efforts via Tucker Decomposition","abstract":"We address the computational barrier of deploying advanced deep learning segmentation models in clinical settings by studying the efficacy of network compression through tensor decomposition. We propose a post-training Tucker factorization that enables the decomposition of pre-existing models to reduce computational requirements without impeding segmentation accuracy. We applied Tucker decomposition to the convolutional kernels of the TotalSegmentator (TS) model, an nnU-Net model trained on a comprehensive dataset for automatic segmentation of 117 anatomical structures. Our approach reduced the floating-point operations (FLOPs) and memory required during inference, offering an adjustable trade-off between computational efficiency and segmentation quality. This study utilized the publicly available TS dataset, employing various downsampling factors to explore the relationship between model size, inference speed, and segmentation performance. The application of Tucker decomposition to the TS model substantially reduced the model parameters and FLOPs across various compression rates, with limited loss in segmentation accuracy. We removed up to 88% of the model's parameters with no significant performance changes in the majority of classes after fine-tuning. Practical benefits varied across different graphics processing unit (GPU) architectures, with more distinct speed-ups on less powerful hardware. Post-hoc network compression via Tucker decomposition presents a viable strategy for reducing the computational demand of medical image segmentation models without substantially sacrificing accuracy. This approach enables the broader adoption of advanced deep learning technologies in clinical practice, offering a way to navigate the constraints of hardware capabilities.","sentences":["We address the computational barrier of deploying advanced deep learning segmentation models in clinical settings by studying the efficacy of network compression through tensor decomposition.","We propose a post-training Tucker factorization that enables the decomposition of pre-existing models to reduce computational requirements without impeding segmentation accuracy.","We applied Tucker decomposition to the convolutional kernels of the TotalSegmentator (TS) model, an nnU-Net model trained on a comprehensive dataset for automatic segmentation of 117 anatomical structures.","Our approach reduced the floating-point operations (FLOPs) and memory required during inference, offering an adjustable trade-off between computational efficiency and segmentation quality.","This study utilized the publicly available TS dataset, employing various downsampling factors to explore the relationship between model size, inference speed, and segmentation performance.","The application of Tucker decomposition to the TS model substantially reduced the model parameters and FLOPs across various compression rates, with limited loss in segmentation accuracy.","We removed up to 88% of the model's parameters with no significant performance changes in the majority of classes after fine-tuning.","Practical benefits varied across different graphics processing unit (GPU) architectures, with more distinct speed-ups on less powerful hardware.","Post-hoc network compression via Tucker decomposition presents a viable strategy for reducing the computational demand of medical image segmentation models without substantially sacrificing accuracy.","This approach enables the broader adoption of advanced deep learning technologies in clinical practice, offering a way to navigate the constraints of hardware capabilities."],"url":"http://arxiv.org/abs/2404.09683v1","category":"eess.IV"}
{"created":"2024-04-15 11:13:59","title":"Thermodynamic and Transport Properties of Binary Mixtures of Polyethylene and Higher n-Alkanes from Physics-Informed and Machine-Learned Models","abstract":"The thermodynamics and transport properties of polymeric materials are essential for the design of reactors and for the development of polymer deconstruction processes. Existing property prediction tools such as correlations based on entropy scaling, kinetic gas theory, and free-volume model are inadequate for polymers. In this paper, we introduce a data-driven model for polyolefins based on data from molecular dynamics simulations that can accurately predict the transport properties of polyethylenes and their binary mixtures with higher n-alkanes across a range of temperatures, pressures, concentrations, and oligomer molecular weights.","sentences":["The thermodynamics and transport properties of polymeric materials are essential for the design of reactors and for the development of polymer deconstruction processes.","Existing property prediction tools such as correlations based on entropy scaling, kinetic gas theory, and free-volume model are inadequate for polymers.","In this paper, we introduce a data-driven model for polyolefins based on data from molecular dynamics simulations that can accurately predict the transport properties of polyethylenes and their binary mixtures with higher n-alkanes across a range of temperatures, pressures, concentrations, and oligomer molecular weights."],"url":"http://arxiv.org/abs/2404.09676v1","category":"cond-mat.soft"}
{"created":"2024-04-15 10:54:47","title":"Closing the Gap in the Trade-off between Fair Representations and Accuracy","abstract":"The rapid developments of various machine learning models and their deployments in several applications has led to discussions around the importance of looking beyond the accuracies of these models. Fairness of such models is one such aspect that is deservedly gaining more attention. In this work, we analyse the natural language representations of documents and sentences (i.e., encodings) for any embedding-level bias that could potentially also affect the fairness of the downstream tasks that rely on them. We identify bias in these encodings either towards or against different sub-groups based on the difference in their reconstruction errors along various subsets of principal components. We explore and recommend ways to mitigate such bias in the encodings while also maintaining a decent accuracy in classification models that use them.","sentences":["The rapid developments of various machine learning models and their deployments in several applications has led to discussions around the importance of looking beyond the accuracies of these models.","Fairness of such models is one such aspect that is deservedly gaining more attention.","In this work, we analyse the natural language representations of documents and sentences (i.e., encodings) for any embedding-level bias that could potentially also affect the fairness of the downstream tasks that rely on them.","We identify bias in these encodings either towards or against different sub-groups based on the difference in their reconstruction errors along various subsets of principal components.","We explore and recommend ways to mitigate such bias in the encodings while also maintaining a decent accuracy in classification models that use them."],"url":"http://arxiv.org/abs/2404.09664v1","category":"cs.LG"}
{"created":"2024-04-15 09:22:46","title":"Machine learning-based optimization workflow of the homogeneity of spunbond nonwovens with human validation","abstract":"In the last ten years, the average annual growth rate of nonwoven production was 4%. In 2020 and 2021, nonwoven production has increased even further due to the huge demand for nonwoven products needed for protective clothing such as FFP2 masks to combat the COVID19 pandemic. Optimizing the production process is still a challenge due to its high nonlinearity. In this paper, we present a machine learning-based optimization workflow aimed at improving the homogeneity of spunbond nonwovens. The optimization workflow is based on a mathematical model that simulates the microstructures of nonwovens. Based on trainingy data coming from this simulator, different machine learning algorithms are trained in order to find a surrogate model for the time-consuming simulator. Human validation is employed to verify the outputs of machine learning algorithms by assessing the aesthetics of the nonwovens. We include scientific and expert knowledge into the training data to reduce the computational costs involved in the optimization process. We demonstrate the necessity and effectiveness of our workflow in optimizing the homogeneity of nonwovens.","sentences":["In the last ten years, the average annual growth rate of nonwoven production was 4%.","In 2020 and 2021, nonwoven production has increased even further due to the huge demand for nonwoven products needed for protective clothing such as FFP2 masks to combat the COVID19 pandemic.","Optimizing the production process is still a challenge due to its high nonlinearity.","In this paper, we present a machine learning-based optimization workflow aimed at improving the homogeneity of spunbond nonwovens.","The optimization workflow is based on a mathematical model that simulates the microstructures of nonwovens.","Based on trainingy data coming from this simulator, different machine learning algorithms are trained in order to find a surrogate model for the time-consuming simulator.","Human validation is employed to verify the outputs of machine learning algorithms by assessing the aesthetics of the nonwovens.","We include scientific and expert knowledge into the training data to reduce the computational costs involved in the optimization process.","We demonstrate the necessity and effectiveness of our workflow in optimizing the homogeneity of nonwovens."],"url":"http://arxiv.org/abs/2404.09604v1","category":"cs.LG"}
{"created":"2024-04-15 07:48:59","title":"Overfitting Reduction in Convex Regression","abstract":"Convex regression is a method for estimating an unknown function $f_0$ from a data set of $n$ noisy observations when $f_0$ is known to be convex. This method has played an important role in operations research, economics, machine learning, and many other areas. It has been empirically observed that the convex regression estimator produces inconsistent estimates of $f_0$ and extremely large subgradients near the boundary of the domain of $f_0$ as $n$ increases. In this paper, we provide theoretical evidence of this overfitting behaviour. We also prove that the penalised convex regression estimator, one of the variants of the convex regression estimator, exhibits overfitting behaviour. To eliminate this behaviour, we propose two new estimators by placing a bound on the subgradients of the estimated function. We further show that our proposed estimators do not exhibit the overfitting behaviour by proving that (a) they converge to $f_0$ and (b) their subgradients converge to the gradient of $f_0$, both uniformly over the domain of $f_0$ with probability one as $n \\rightarrow \\infty$. We apply the proposed methods to compute the cost frontier function for Finnish electricity distribution firms and confirm their superior performance in predictive power over some existing methods.","sentences":["Convex regression is a method for estimating an unknown function $f_0$ from a data set of $n$ noisy observations when $f_0$ is known to be convex.","This method has played an important role in operations research, economics, machine learning, and many other areas.","It has been empirically observed that the convex regression estimator produces inconsistent estimates of $f_0$ and extremely large subgradients near the boundary of the domain of $f_0$ as $n$ increases.","In this paper, we provide theoretical evidence of this overfitting behaviour.","We also prove that the penalised convex regression estimator, one of the variants of the convex regression estimator, exhibits overfitting behaviour.","To eliminate this behaviour, we propose two new estimators by placing a bound on the subgradients of the estimated function.","We further show that our proposed estimators do not exhibit the overfitting behaviour by proving that (a) they converge to $f_0$ and (b) their subgradients converge to the gradient of $f_0$, both uniformly over the domain of $f_0$ with probability one as $n \\rightarrow \\infty$. We apply the proposed methods to compute the cost frontier function for Finnish electricity distribution firms and confirm their superior performance in predictive power over some existing methods."],"url":"http://arxiv.org/abs/2404.09528v1","category":"stat.ME"}
{"created":"2024-04-15 07:31:19","title":"UniSAR: Modeling User Transition Behaviors between Search and Recommendation","abstract":"Nowadays, many platforms provide users with both search and recommendation services as important tools for accessing information. The phenomenon has led to a correlation between user search and recommendation behaviors, providing an opportunity to model user interests in a fine-grained way. Existing approaches either model user search and recommendation behaviors separately or overlook the different transitions between user search and recommendation behaviors. In this paper, we propose a framework named UniSAR that effectively models the different types of fine-grained behavior transitions for providing users a Unified Search And Recommendation service. Specifically, UniSAR models the user transition behaviors between search and recommendation through three steps: extraction, alignment, and fusion, which are respectively implemented by transformers equipped with pre-defined masks, contrastive learning that aligns the extracted fine-grained user transitions, and cross-attentions that fuse different transitions. To provide users with a unified service, the learned representations are fed into the downstream search and recommendation models. Joint learning on both search and recommendation data is employed to utilize the knowledge and enhance each other. Experimental results on two public datasets demonstrated the effectiveness of UniSAR in terms of enhancing both search and recommendation simultaneously. The experimental analysis further validates that UniSAR enhances the results by successfully modeling the user transition behaviors between search and recommendation.","sentences":["Nowadays, many platforms provide users with both search and recommendation services as important tools for accessing information.","The phenomenon has led to a correlation between user search and recommendation behaviors, providing an opportunity to model user interests in a fine-grained way.","Existing approaches either model user search and recommendation behaviors separately or overlook the different transitions between user search and recommendation behaviors.","In this paper, we propose a framework named UniSAR that effectively models the different types of fine-grained behavior transitions for providing users a Unified Search And Recommendation service.","Specifically, UniSAR models the user transition behaviors between search and recommendation through three steps: extraction, alignment, and fusion, which are respectively implemented by transformers equipped with pre-defined masks, contrastive learning that aligns the extracted fine-grained user transitions, and cross-attentions that fuse different transitions.","To provide users with a unified service, the learned representations are fed into the downstream search and recommendation models.","Joint learning on both search and recommendation data is employed to utilize the knowledge and enhance each other.","Experimental results on two public datasets demonstrated the effectiveness of UniSAR in terms of enhancing both search and recommendation simultaneously.","The experimental analysis further validates that UniSAR enhances the results by successfully modeling the user transition behaviors between search and recommendation."],"url":"http://arxiv.org/abs/2404.09520v1","category":"cs.IR"}
{"created":"2024-04-15 07:05:14","title":"Fuse after Align: Improving Face-Voice Association Learning via Multimodal Encoder","abstract":"Today, there have been many achievements in learning the association between voice and face. However, most previous work models rely on cosine similarity or L2 distance to evaluate the likeness of voices and faces following contrastive learning, subsequently applied to retrieval and matching tasks. This method only considers the embeddings as high-dimensional vectors, utilizing a minimal scope of available information. This paper introduces a novel framework within an unsupervised setting for learning voice-face associations. By employing a multimodal encoder after contrastive learning and addressing the problem through binary classification, we can learn the implicit information within the embeddings in a more effective and varied manner. Furthermore, by introducing an effective pair selection method, we enhance the learning outcomes of both contrastive learning and the matching task. Empirical evidence demonstrates that our framework achieves state-of-the-art results in voice-face matching, verification, and retrieval tasks, improving verification by approximately 3%, matching by about 2.5%, and retrieval by around 1.3%.","sentences":["Today, there have been many achievements in learning the association between voice and face.","However, most previous work models rely on cosine similarity or L2 distance to evaluate the likeness of voices and faces following contrastive learning, subsequently applied to retrieval and matching tasks.","This method only considers the embeddings as high-dimensional vectors, utilizing a minimal scope of available information.","This paper introduces a novel framework within an unsupervised setting for learning voice-face associations.","By employing a multimodal encoder after contrastive learning and addressing the problem through binary classification, we can learn the implicit information within the embeddings in a more effective and varied manner.","Furthermore, by introducing an effective pair selection method, we enhance the learning outcomes of both contrastive learning and the matching task.","Empirical evidence demonstrates that our framework achieves state-of-the-art results in voice-face matching, verification, and retrieval tasks, improving verification by approximately 3%, matching by about 2.5%, and retrieval by around 1.3%."],"url":"http://arxiv.org/abs/2404.09509v1","category":"cs.CV"}
{"created":"2024-04-15 06:32:28","title":"On the Necessity of Collaboration in Online Model Selection with Decentralized Data","abstract":"We consider online model selection with decentralized data over $M$ clients, and study a fundamental problem: the necessity of collaboration. Previous work gave a negative answer from the perspective of worst-case regret minimization, while we give a different answer from the perspective of regret-computational cost trade-off. We separately propose a federated algorithm with and without communication constraint and prove regret bounds that show (i) collaboration is unnecessary if we do not limit the computational cost on each client; (ii) collaboration is necessary if we limit the computational cost on each client to $o(K)$, where $K$ is the number of candidate hypothesis spaces. As a by-product, we improve the regret bounds of algorithms for distributed online multi-kernel learning at a smaller computational and communication cost. Our algorithms rely on three new techniques, i.e., an improved Bernstein's inequality for martingale, a federated algorithmic framework, named FOMD-No-LU, and decoupling model selection and predictions, which might be of independent interest.","sentences":["We consider online model selection with decentralized data over $M$ clients, and study a fundamental problem: the necessity of collaboration.","Previous work gave a negative answer from the perspective of worst-case regret minimization, while we give a different answer from the perspective of regret-computational cost trade-off.","We separately propose a federated algorithm with and without communication constraint and prove regret bounds that show (i) collaboration is unnecessary if we do not limit the computational cost on each client; (ii) collaboration is necessary if we limit the computational cost on each client to $o(K)$, where $K$ is the number of candidate hypothesis spaces.","As a by-product, we improve the regret bounds of algorithms for distributed online multi-kernel learning at a smaller computational and communication cost.","Our algorithms rely on three new techniques, i.e., an improved Bernstein's inequality for martingale, a federated algorithmic framework, named FOMD-No-LU, and decoupling model selection and predictions, which might be of independent interest."],"url":"http://arxiv.org/abs/2404.09494v1","category":"cs.LG"}
{"created":"2024-04-15 04:41:53","title":"Towards Greener Nights: Exploring AI-Driven Solutions for Light Pollution Management","abstract":"This research endeavors to address the pervasive issue of light pollution through an interdisciplinary approach, leveraging data science and machine learning techniques. By analyzing extensive datasets and research findings, we aim to develop predictive models capable of estimating the degree of sky glow observed in various locations and times. Our research seeks to inform evidence-based interventions and promote responsible outdoor lighting practices to mitigate the adverse impacts of light pollution on ecosystems, energy consumption, and human well-being.","sentences":["This research endeavors to address the pervasive issue of light pollution through an interdisciplinary approach, leveraging data science and machine learning techniques.","By analyzing extensive datasets and research findings, we aim to develop predictive models capable of estimating the degree of sky glow observed in various locations and times.","Our research seeks to inform evidence-based interventions and promote responsible outdoor lighting practices to mitigate the adverse impacts of light pollution on ecosystems, energy consumption, and human well-being."],"url":"http://arxiv.org/abs/2404.09453v1","category":"cs.LG"}
{"created":"2024-04-15 00:19:47","title":"Masked and Shuffled Blind Spot Denoising for Real-World Images","abstract":"We introduce a novel approach to single image denoising based on the Blind Spot Denoising principle, which we call MAsked and SHuffled Blind Spot Denoising (MASH). We focus on the case of correlated noise, which often plagues real images. MASH is the result of a careful analysis to determine the relationships between the level of blindness (masking) of the input and the (unknown) noise correlation. Moreover, we introduce a shuffling technique to weaken the local correlation of noise, which in turn yields an additional denoising performance improvement. We evaluate MASH via extensive experiments on real-world noisy image datasets. We demonstrate on par or better results compared to existing self-supervised denoising methods.","sentences":["We introduce a novel approach to single image denoising based on the Blind Spot Denoising principle, which we call MAsked and SHuffled Blind Spot Denoising (MASH).","We focus on the case of correlated noise, which often plagues real images.","MASH is the result of a careful analysis to determine the relationships between the level of blindness (masking) of the input and the (unknown) noise correlation.","Moreover, we introduce a shuffling technique to weaken the local correlation of noise, which in turn yields an additional denoising performance improvement.","We evaluate MASH via extensive experiments on real-world noisy image datasets.","We demonstrate on par or better results compared to existing self-supervised denoising methods."],"url":"http://arxiv.org/abs/2404.09389v1","category":"cs.CV"}
{"created":"2024-04-14 21:22:55","title":"Are University Budget Cuts Becoming A Threat to Mathematics? with Additional Discussion","abstract":"Mathematics as an area of study occupies an important place in higher education. Due in part to its utility in other disciplines as well as its role in student learning, institutions of higher education (IHEs) often have large numbers of mathematics faculty with different balances of teaching and research in different ranks and appointment structures. Most flagship IHEs, especially state land-grant institutions, have large undergraduate populations taking mathematics courses in many cases built around the widespread use of calculus and the connections between mathematics and science, technology, and engineering. These connections have made mathematics departments essential to universities\\cite{olson2012engage} and emphasized the critical role math plays in supporting student success \\cites{reinholz2020time,calcscience} in all areas of post-secondary education. We tend to take that essential nature of mathematics at the undergraduate level, and for research universities at the graduate level, as a given, but that characterization no longer holds for some IHEs.","sentences":["Mathematics as an area of study occupies an important place in higher education.","Due in part to its utility in other disciplines as well as its role in student learning, institutions of higher education (IHEs) often have large numbers of mathematics faculty with different balances of teaching and research in different ranks and appointment structures.","Most flagship IHEs, especially state land-grant institutions, have large undergraduate populations taking mathematics courses in many cases built around the widespread use of calculus and the connections between mathematics and science, technology, and engineering.","These connections have made mathematics departments essential to universities\\cite{olson2012engage} and emphasized the critical role math plays in supporting student success \\cites{reinholz2020time,calcscience} in all areas of post-secondary education.","We tend to take that essential nature of mathematics at the undergraduate level, and for research universities at the graduate level, as a given, but that characterization no longer holds for some IHEs."],"url":"http://arxiv.org/abs/2404.09360v1","category":"math.HO"}
{"created":"2024-04-14 19:32:10","title":"Human Vs. Machines: Who Wins In Semiconductor Market Forecasting?","abstract":"\"If you ask ten experts, you will get ten different opinions.\" This common proverb illustrates the common association of expert forecasts with personal bias and lack of consistency. On the other hand, digitization promises consistency and explainability through data-driven forecasts employing machine learning (ML) and statistical models. In the following, we compare such forecasts to expert forecasts from the World Semiconductor Trade Statistics (WSTS), a leading semiconductor market data provider.","sentences":["\"If you ask ten experts, you will get ten different opinions.\"","This common proverb illustrates the common association of expert forecasts with personal bias and lack of consistency.","On the other hand, digitization promises consistency and explainability through data-driven forecasts employing machine learning (ML) and statistical models.","In the following, we compare such forecasts to expert forecasts from the World Semiconductor Trade Statistics (WSTS), a leading semiconductor market data provider."],"url":"http://arxiv.org/abs/2404.09334v1","category":"stat.AP"}
{"created":"2024-04-14 16:09:33","title":"A Novel State Space Model with Local Enhancement and State Sharing for Image Fusion","abstract":"In image fusion tasks, images from different sources possess distinct characteristics. This has driven the development of numerous methods to explore better ways of fusing them while preserving their respective characteristics. Mamba, as a state space model, has emerged in the field of natural language processing. Recently, many studies have attempted to extend Mamba to vision tasks. However, due to the nature of images different from casual language sequences, the limited state capacity of Mamba weakens its ability to model image information. Additionally, the sequence modeling ability of Mamba is only capable of spatial information and cannot effectively capture the rich spectral information in images. Motivated by these challenges, we customize and improve the vision Mamba network designed for the image fusion task. Specifically, we propose the local-enhanced vision Mamba block, dubbed as LEVM. The LEVM block can improve local information perception of the network and simultaneously learn local and global spatial information. Furthermore, we propose the state sharing technique to enhance spatial details and integrate spatial and spectral information. Finally, the overall network is a multi-scale structure based on vision Mamba, called LE-Mamba. Extensive experiments show the proposed methods achieve state-of-the-art results on multispectral pansharpening and multispectral and hyperspectral image fusion datasets, and demonstrate the effectiveness of the proposed approach. Code will be made available.","sentences":["In image fusion tasks, images from different sources possess distinct characteristics.","This has driven the development of numerous methods to explore better ways of fusing them while preserving their respective characteristics.","Mamba, as a state space model, has emerged in the field of natural language processing.","Recently, many studies have attempted to extend Mamba to vision tasks.","However, due to the nature of images different from casual language sequences, the limited state capacity of Mamba weakens its ability to model image information.","Additionally, the sequence modeling ability of Mamba is only capable of spatial information and cannot effectively capture the rich spectral information in images.","Motivated by these challenges, we customize and improve the vision Mamba network designed for the image fusion task.","Specifically, we propose the local-enhanced vision Mamba block, dubbed as LEVM.","The LEVM block can improve local information perception of the network and simultaneously learn local and global spatial information.","Furthermore, we propose the state sharing technique to enhance spatial details and integrate spatial and spectral information.","Finally, the overall network is a multi-scale structure based on vision Mamba, called LE-Mamba.","Extensive experiments show the proposed methods achieve state-of-the-art results on multispectral pansharpening and multispectral and hyperspectral image fusion datasets, and demonstrate the effectiveness of the proposed approach.","Code will be made available."],"url":"http://arxiv.org/abs/2404.09293v1","category":"cs.CV"}
{"created":"2024-04-14 15:33:38","title":"Egret: Reinforcement Mechanism for Sequential Computation Offloading in Edge Computing","abstract":"As an emerging computing paradigm, edge computing offers computing resources closer to the data sources, helping to improve the service quality of many real-time applications. A crucial problem is designing a rational pricing mechanism to maximize the revenue of the edge computing service provider (ECSP). However, prior works have considerable limitations: clients are static and are required to disclose their preferences, which is impractical in reality. However, previous works assume user privacy information to be known or consider the number of users in edge scenarios to be static. To address this issue, we propose a novel sequential computation offloading mechanism, where the ECSP posts prices of computing resources with different configurations to clients in turn. Clients independently choose which computing resources to purchase and how to offload based on their prices. Then Egret, a deep reinforcement learning-based approach that achieves maximum revenue, is proposed. Egret determines the optimal price and visiting orders online without considering clients' preferences. Experimental results show that the revenue of ECSP in Egret is only 1.29\\% lower than Oracle and 23.43\\% better than the state-of-the-art when the client arrives dynamically.","sentences":["As an emerging computing paradigm, edge computing offers computing resources closer to the data sources, helping to improve the service quality of many real-time applications.","A crucial problem is designing a rational pricing mechanism to maximize the revenue of the edge computing service provider (ECSP).","However, prior works have considerable limitations: clients are static and are required to disclose their preferences, which is impractical in reality.","However, previous works assume user privacy information to be known or consider the number of users in edge scenarios to be static.","To address this issue, we propose a novel sequential computation offloading mechanism, where the ECSP posts prices of computing resources with different configurations to clients in turn.","Clients independently choose which computing resources to purchase and how to offload based on their prices.","Then Egret, a deep reinforcement learning-based approach that achieves maximum revenue, is proposed.","Egret determines the optimal price and visiting orders online without considering clients' preferences.","Experimental results show that the revenue of ECSP in Egret is only 1.29\\% lower than Oracle and 23.43\\% better than the state-of-the-art when the client arrives dynamically."],"url":"http://arxiv.org/abs/2404.09285v1","category":"cs.DC"}
{"created":"2024-04-14 13:14:17","title":"Engaging Young Learners with Testing Using the Code Critters Mutation Game","abstract":"Everyone learns to code nowadays. Writing code, however, does not go without testing, which unfortunately rarely seems to be taught explicitly. Testing is often not deemed important enough or is just not perceived as sufficiently exciting. Testing can be exciting: In this paper, we introduce Code Critters, a serious game designed to teach testing concepts engagingly. In the style of popular tower defense games, players strategically position magical portals that need to distinguish between creatures exhibiting the behavior described by correct code from those that are mutated, and thus faulty. When placing portals, players are implicitly testing: They choose test inputs (i.e., where to place portals), as well as test oracles (i.e., what behavior to expect), and they observe test executions as the creatures wander across the landscape passing the players' portals. An empirical study involving 40 children demonstrates that they actively engage with Code Critters. Their positive feedback provides evidence that they enjoyed playing the game, and some of the children even continued to play Code Critters at home, outside the educational setting of our study.","sentences":["Everyone learns to code nowadays.","Writing code, however, does not go without testing, which unfortunately rarely seems to be taught explicitly.","Testing is often not deemed important enough or is just not perceived as sufficiently exciting.","Testing can be exciting: In this paper, we introduce Code Critters, a serious game designed to teach testing concepts engagingly.","In the style of popular tower defense games, players strategically position magical portals that need to distinguish between creatures exhibiting the behavior described by correct code from those that are mutated, and thus faulty.","When placing portals, players are implicitly testing: They choose test inputs (i.e., where to place portals), as well as test oracles (i.e., what behavior to expect), and they observe test executions as the creatures wander across the landscape passing the players' portals.","An empirical study involving 40 children demonstrates that they actively engage with Code Critters.","Their positive feedback provides evidence that they enjoyed playing the game, and some of the children even continued to play Code Critters at home, outside the educational setting of our study."],"url":"http://arxiv.org/abs/2404.09246v1","category":"cs.SE"}
{"created":"2024-04-14 12:22:42","title":"MAP: Model Aggregation and Personalization in Federated Learning with Incomplete Classes","abstract":"In some real-world applications, data samples are usually distributed on local devices, where federated learning (FL) techniques are proposed to coordinate decentralized clients without directly sharing users' private data. FL commonly follows the parameter server architecture and contains multiple personalization and aggregation procedures. The natural data heterogeneity across clients, i.e., Non-I.I.D. data, challenges both the aggregation and personalization goals in FL. In this paper, we focus on a special kind of Non-I.I.D. scene where clients own incomplete classes, i.e., each client can only access a partial set of the whole class set. The server aims to aggregate a complete classification model that could generalize to all classes, while the clients are inclined to improve the performance of distinguishing their observed classes. For better model aggregation, we point out that the standard softmax will encounter several problems caused by missing classes and propose \"restricted softmax\" as an alternative. For better model personalization, we point out that the hard-won personalized models are not well exploited and propose \"inherited private model\" to store the personalization experience. Our proposed algorithm named MAP could simultaneously achieve the aggregation and personalization goals in FL. Abundant experimental studies verify the superiorities of our algorithm.","sentences":["In some real-world applications, data samples are usually distributed on local devices, where federated learning (FL) techniques are proposed to coordinate decentralized clients without directly sharing users' private data.","FL commonly follows the parameter server architecture and contains multiple personalization and aggregation procedures.","The natural data heterogeneity across clients, i.e., Non-I.I.D. data, challenges both the aggregation and personalization goals in FL.","In this paper, we focus on a special kind of Non-I.I.D. scene where clients own incomplete classes, i.e., each client can only access a partial set of the whole class set.","The server aims to aggregate a complete classification model that could generalize to all classes, while the clients are inclined to improve the performance of distinguishing their observed classes.","For better model aggregation, we point out that the standard softmax will encounter several problems caused by missing classes and propose \"restricted softmax\" as an alternative.","For better model personalization, we point out that the hard-won personalized models are not well exploited and propose \"inherited private model\" to store the personalization experience.","Our proposed algorithm named MAP could simultaneously achieve the aggregation and personalization goals in FL.","Abundant experimental studies verify the superiorities of our algorithm."],"url":"http://arxiv.org/abs/2404.09232v1","category":"cs.LG"}
{"created":"2024-04-14 12:02:52","title":"OSS Myths and Facts","abstract":"We have selected six myths about the OSS community and have tested whether they are true or not. The purpose of this report is to identify the lessons that can be learned from the development style of the OSS community and the issues that need to be addressed in order to achieve better Employee Experience (EX) in software development within companies and organizations. The OSS community has been led by a group of skilled developers known as hackers. We have great respect for the engineers and activities of the OSS community and aim to learn from them. On the other hand, it is important to recognize that having high expectations can sometimes result in misunderstandings. When there are excessive expectations and concerns, misunderstandings (referred to as myths) can arise, particularly when individuals who are not practitioners rely on hearsay to understand the practices of practitioners. We selected the myths to be tested based on a literature review and interviews. These myths are held by software development managers and customers who are not direct participants in the OSS community. We answered questions about each myth through: 1) Our own analysis of repository data, 2) A literature survey of data analysis conducted by previous studies, or 3) A combination of the two approaches.","sentences":["We have selected six myths about the OSS community and have tested whether they are true or not.","The purpose of this report is to identify the lessons that can be learned from the development style of the OSS community and the issues that need to be addressed in order to achieve better Employee Experience (EX) in software development within companies and organizations.","The OSS community has been led by a group of skilled developers known as hackers.","We have great respect for the engineers and activities of the OSS community and aim to learn from them.","On the other hand, it is important to recognize that having high expectations can sometimes result in misunderstandings.","When there are excessive expectations and concerns, misunderstandings (referred to as myths) can arise, particularly when individuals who are not practitioners rely on hearsay to understand the practices of practitioners.","We selected the myths to be tested based on a literature review and interviews.","These myths are held by software development managers and customers who are not direct participants in the OSS community.","We answered questions about each myth through: 1) Our own analysis of repository data, 2) A literature survey of data analysis conducted by previous studies, or 3) A combination of the two approaches."],"url":"http://arxiv.org/abs/2404.09223v1","category":"cs.SE"}
{"created":"2024-04-14 11:48:33","title":"Compass: Large Multilingual Language Model for South-east Asia","abstract":"Large language models have exhibited significant proficiency in languages endowed with extensive linguistic resources, such as English and Chinese. Nevertheless, their effectiveness notably diminishes when applied to languages characterized by limited linguistic resources, particularly within the Southeast Asian linguistic landscape, such as Indonesian. The scarcity of linguistic resources for these languages presents challenges associated with inadequate training, restricted vocabulary coverage, and challenging evaluation processes. In response to these exigencies, we have introduced CompassLLM, a large multilingual model specifically tailored for Southeast Asian languages, with the primary aim of supporting the developmental requirements of Shopee. Our methodology encompasses several key strategies. To progressively enhance multilingual proficiencies, we implemented a multi-stage pre-training strategy integrated with curriculum learning, gradually intensifying the focus on low-resource languages. Concurrently, to better accommodate low-resource human instructions, we curated and generated a repository of high-quality multilingual human instructions, culminating the CompassLLM-SFT model through supervised instruction fine-tuning. Finally, to reinforce the model's alignment with human preference behaviors, we have embraced the principle of Direct Preference Optimization (DPO) to obtain CompassLLM-DPO model. Preliminary evaluation of the CompassLLM model yields promising results, with our model surpassing benchmark models like Vicuna-7b-v1.5, Sealion, Falcon and SeaLLM, across diverse evaluation tasks, as verified through both automated and human-driven assessments. Notably, our model exhibits its superior performance in South-east Asia languages, such as Indonesian language.","sentences":["Large language models have exhibited significant proficiency in languages endowed with extensive linguistic resources, such as English and Chinese.","Nevertheless, their effectiveness notably diminishes when applied to languages characterized by limited linguistic resources, particularly within the Southeast Asian linguistic landscape, such as Indonesian.","The scarcity of linguistic resources for these languages presents challenges associated with inadequate training, restricted vocabulary coverage, and challenging evaluation processes.","In response to these exigencies, we have introduced CompassLLM, a large multilingual model specifically tailored for Southeast Asian languages, with the primary aim of supporting the developmental requirements of Shopee.","Our methodology encompasses several key strategies.","To progressively enhance multilingual proficiencies, we implemented a multi-stage pre-training strategy integrated with curriculum learning, gradually intensifying the focus on low-resource languages.","Concurrently, to better accommodate low-resource human instructions, we curated and generated a repository of high-quality multilingual human instructions, culminating the CompassLLM-SFT model through supervised instruction fine-tuning.","Finally, to reinforce the model's alignment with human preference behaviors, we have embraced the principle of Direct Preference Optimization (DPO) to obtain CompassLLM-DPO model.","Preliminary evaluation of the CompassLLM model yields promising results, with our model surpassing benchmark models like Vicuna-7b-v1.5, Sealion, Falcon and SeaLLM, across diverse evaluation tasks, as verified through both automated and human-driven assessments.","Notably, our model exhibits its superior performance in South-east Asia languages, such as Indonesian language."],"url":"http://arxiv.org/abs/2404.09220v1","category":"cs.CL"}
{"created":"2024-04-14 11:01:44","title":"DetCLIPv3: Towards Versatile Generative Open-vocabulary Object Detection","abstract":"Existing open-vocabulary object detectors typically require a predefined set of categories from users, significantly confining their application scenarios. In this paper, we introduce DetCLIPv3, a high-performing detector that excels not only at both open-vocabulary object detection, but also generating hierarchical labels for detected objects. DetCLIPv3 is characterized by three core designs: 1. Versatile model architecture: we derive a robust open-set detection framework which is further empowered with generation ability via the integration of a caption head. 2. High information density data: we develop an auto-annotation pipeline leveraging visual large language model to refine captions for large-scale image-text pairs, providing rich, multi-granular object labels to enhance the training. 3. Efficient training strategy: we employ a pre-training stage with low-resolution inputs that enables the object captioner to efficiently learn a broad spectrum of visual concepts from extensive image-text paired data. This is followed by a fine-tuning stage that leverages a small number of high-resolution samples to further enhance detection performance. With these effective designs, DetCLIPv3 demonstrates superior open-vocabulary detection performance, \\eg, our Swin-T backbone model achieves a notable 47.0 zero-shot fixed AP on the LVIS minival benchmark, outperforming GLIPv2, GroundingDINO, and DetCLIPv2 by 18.0/19.6/6.6 AP, respectively. DetCLIPv3 also achieves a state-of-the-art 19.7 AP in dense captioning task on VG dataset, showcasing its strong generative capability.","sentences":["Existing open-vocabulary object detectors typically require a predefined set of categories from users, significantly confining their application scenarios.","In this paper, we introduce DetCLIPv3, a high-performing detector that excels not only at both open-vocabulary object detection, but also generating hierarchical labels for detected objects.","DetCLIPv3 is characterized by three core designs: 1. Versatile model architecture: we derive a robust open-set detection framework which is further empowered with generation ability via the integration of a caption head.","2. High information density data: we develop an auto-annotation pipeline leveraging visual large language model to refine captions for large-scale image-text pairs, providing rich, multi-granular object labels to enhance the training.","3. Efficient training strategy: we employ a pre-training stage with low-resolution inputs that enables the object captioner to efficiently learn a broad spectrum of visual concepts from extensive image-text paired data.","This is followed by a fine-tuning stage that leverages a small number of high-resolution samples to further enhance detection performance.","With these effective designs, DetCLIPv3 demonstrates superior open-vocabulary detection performance, \\eg, our Swin-T backbone model achieves a notable 47.0 zero-shot fixed AP on the LVIS minival benchmark, outperforming GLIPv2, GroundingDINO, and DetCLIPv2 by 18.0/19.6/6.6 AP, respectively.","DetCLIPv3 also achieves a state-of-the-art 19.7 AP in dense captioning task on VG dataset, showcasing its strong generative capability."],"url":"http://arxiv.org/abs/2404.09216v1","category":"cs.CV"}
{"created":"2024-04-14 10:02:47","title":"DKE-Research at SemEval-2024 Task 2: Incorporating Data Augmentation with Generative Models and Biomedical Knowledge to Enhance Inference Robustness","abstract":"Safe and reliable natural language inference is critical for extracting insights from clinical trial reports but poses challenges due to biases in large pre-trained language models. This paper presents a novel data augmentation technique to improve model robustness for biomedical natural language inference in clinical trials. By generating synthetic examples through semantic perturbations and domain-specific vocabulary replacement and adding a new task for numerical and quantitative reasoning, we introduce greater diversity and reduce shortcut learning. Our approach, combined with multi-task learning and the DeBERTa architecture, achieved significant performance gains on the NLI4CT 2024 benchmark compared to the original language models. Ablation studies validate the contribution of each augmentation method in improving robustness. Our best-performing model ranked 12th in terms of faithfulness and 8th in terms of consistency, respectively, out of the 32 participants.","sentences":["Safe and reliable natural language inference is critical for extracting insights from clinical trial reports but poses challenges due to biases in large pre-trained language models.","This paper presents a novel data augmentation technique to improve model robustness for biomedical natural language inference in clinical trials.","By generating synthetic examples through semantic perturbations and domain-specific vocabulary replacement and adding a new task for numerical and quantitative reasoning, we introduce greater diversity and reduce shortcut learning.","Our approach, combined with multi-task learning and the DeBERTa architecture, achieved significant performance gains on the NLI4CT 2024 benchmark compared to the original language models.","Ablation studies validate the contribution of each augmentation method in improving robustness.","Our best-performing model ranked 12th in terms of faithfulness and 8th in terms of consistency, respectively, out of the 32 participants."],"url":"http://arxiv.org/abs/2404.09206v1","category":"cs.CL"}
{"created":"2024-04-14 09:30:35","title":"Joint Near Field Uplink Communication and Localization Using Message Passing-Based Sparse Bayesian Learning","abstract":"This work deals with the problem of uplink communication and localization in an integrated sensing and communication system, where users are in the near field (NF) of antenna aperture due to the use of high carrier frequency and large antenna arrays at base stations. We formulate joint NF signal detection and localization as a problem of recovering signals with a sparse pattern. To solve the problem, we develop a message passing based sparse Bayesian learning (SBL) algorithm, where multiple unitary approximate message passing (UAMP)-based sparse signal estimators work jointly to recover the sparse signals with low complexity. Simulation results demonstrate the effectiveness of the proposed method.","sentences":["This work deals with the problem of uplink communication and localization in an integrated sensing and communication system, where users are in the near field (NF) of antenna aperture due to the use of high carrier frequency and large antenna arrays at base stations.","We formulate joint NF signal detection and localization as a problem of recovering signals with a sparse pattern.","To solve the problem, we develop a message passing based sparse Bayesian learning (SBL) algorithm, where multiple unitary approximate message passing (UAMP)-based sparse signal estimators work jointly to recover the sparse signals with low complexity.","Simulation results demonstrate the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2404.09201v1","category":"cs.IT"}
{"created":"2024-04-14 08:01:27","title":"HANet: A Hierarchical Attention Network for Change Detection With Bitemporal Very-High-Resolution Remote Sensing Images","abstract":"Benefiting from the developments in deep learning technology, deep-learning-based algorithms employing automatic feature extraction have achieved remarkable performance on the change detection (CD) task. However, the performance of existing deep-learning-based CD methods is hindered by the imbalance between changed and unchanged pixels. To tackle this problem, a progressive foreground-balanced sampling strategy on the basis of not adding change information is proposed in this article to help the model accurately learn the features of the changed pixels during the early training process and thereby improve detection performance.Furthermore, we design a discriminative Siamese network, hierarchical attention network (HANet), which can integrate multiscale features and refine detailed features. The main part of HANet is the HAN module, which is a lightweight and effective self-attention mechanism. Extensive experiments and ablation studies on two CDdatasets with extremely unbalanced labels validate the effectiveness and efficiency of the proposed method.","sentences":["Benefiting from the developments in deep learning technology, deep-learning-based algorithms employing automatic feature extraction have achieved remarkable performance on the change detection (CD) task.","However, the performance of existing deep-learning-based CD methods is hindered by the imbalance between changed and unchanged pixels.","To tackle this problem, a progressive foreground-balanced sampling strategy on the basis of not adding change information is proposed in this article to help the model accurately learn the features of the changed pixels during the early training process and thereby improve detection performance.","Furthermore, we design a discriminative Siamese network, hierarchical attention network (HANet), which can integrate multiscale features and refine detailed features.","The main part of HANet is the HAN module, which is a lightweight and effective self-attention mechanism.","Extensive experiments and ablation studies on two CDdatasets with extremely unbalanced labels validate the effectiveness and efficiency of the proposed method."],"url":"http://arxiv.org/abs/2404.09178v1","category":"cs.CV"}
{"created":"2024-04-14 07:56:08","title":"An Experimental Comparison Of Multi-view Self-supervised Methods For Music Tagging","abstract":"Self-supervised learning has emerged as a powerful way to pre-train generalizable machine learning models on large amounts of unlabeled data. It is particularly compelling in the music domain, where obtaining labeled data is time-consuming, error-prone, and ambiguous. During the self-supervised process, models are trained on pretext tasks, with the primary objective of acquiring robust and informative features that can later be fine-tuned for specific downstream tasks. The choice of the pretext task is critical as it guides the model to shape the feature space with meaningful constraints for information encoding. In the context of music, most works have relied on contrastive learning or masking techniques. In this study, we expand the scope of pretext tasks applied to music by investigating and comparing the performance of new self-supervised methods for music tagging. We open-source a simple ResNet model trained on a diverse catalog of millions of tracks. Our results demonstrate that, although most of these pre-training methods result in similar downstream results, contrastive learning consistently results in better downstream performance compared to other self-supervised pre-training methods. This holds true in a limited-data downstream context.","sentences":["Self-supervised learning has emerged as a powerful way to pre-train generalizable machine learning models on large amounts of unlabeled data.","It is particularly compelling in the music domain, where obtaining labeled data is time-consuming, error-prone, and ambiguous.","During the self-supervised process, models are trained on pretext tasks, with the primary objective of acquiring robust and informative features that can later be fine-tuned for specific downstream tasks.","The choice of the pretext task is critical as it guides the model to shape the feature space with meaningful constraints for information encoding.","In the context of music, most works have relied on contrastive learning or masking techniques.","In this study, we expand the scope of pretext tasks applied to music by investigating and comparing the performance of new self-supervised methods for music tagging.","We open-source a simple ResNet model trained on a diverse catalog of millions of tracks.","Our results demonstrate that, although most of these pre-training methods result in similar downstream results, contrastive learning consistently results in better downstream performance compared to other self-supervised pre-training methods.","This holds true in a limited-data downstream context."],"url":"http://arxiv.org/abs/2404.09177v1","category":"cs.SD"}
{"created":"2024-04-14 07:17:53","title":"Increasing SLAM Pose Accuracy by Ground-to-Satellite Image Registration","abstract":"Vision-based localization for autonomous driving has been of great interest among researchers. When a pre-built 3D map is not available, the techniques of visual simultaneous localization and mapping (SLAM) are typically adopted. Due to error accumulation, visual SLAM (vSLAM) usually suffers from long-term drift. This paper proposes a framework to increase the localization accuracy by fusing the vSLAM with a deep-learning-based ground-to-satellite (G2S) image registration method. In this framework, a coarse (spatial correlation bound check) to fine (visual odometry consistency check) method is designed to select the valid G2S prediction. The selected prediction is then fused with the SLAM measurement by solving a scaled pose graph problem. To further increase the localization accuracy, we provide an iterative trajectory fusion pipeline. The proposed framework is evaluated on two well-known autonomous driving datasets, and the results demonstrate the accuracy and robustness in terms of vehicle localization.","sentences":["Vision-based localization for autonomous driving has been of great interest among researchers.","When a pre-built 3D map is not available, the techniques of visual simultaneous localization and mapping (SLAM) are typically adopted.","Due to error accumulation, visual SLAM (vSLAM) usually suffers from long-term drift.","This paper proposes a framework to increase the localization accuracy by fusing the vSLAM with a deep-learning-based ground-to-satellite (G2S) image registration method.","In this framework, a coarse (spatial correlation bound check) to fine (visual odometry consistency check) method is designed to select the valid G2S prediction.","The selected prediction is then fused with the SLAM measurement by solving a scaled pose graph problem.","To further increase the localization accuracy, we provide an iterative trajectory fusion pipeline.","The proposed framework is evaluated on two well-known autonomous driving datasets, and the results demonstrate the accuracy and robustness in terms of vehicle localization."],"url":"http://arxiv.org/abs/2404.09169v1","category":"cs.RO"}
{"created":"2024-04-14 06:46:16","title":"Coreset Selection for Object Detection","abstract":"Coreset selection is a method for selecting a small, representative subset of an entire dataset. It has been primarily researched in image classification, assuming there is only one object per image. However, coreset selection for object detection is more challenging as an image can contain multiple objects. As a result, much research has yet to be done on this topic. Therefore, we introduce a new approach, Coreset Selection for Object Detection (CSOD). CSOD generates imagewise and classwise representative feature vectors for multiple objects of the same class within each image. Subsequently, we adopt submodular optimization for considering both representativeness and diversity and utilize the representative vectors in the submodular optimization process to select a subset. When we evaluated CSOD on the Pascal VOC dataset, CSOD outperformed random selection by +6.4%p in AP$_{50}$ when selecting 200 images.","sentences":["Coreset selection is a method for selecting a small, representative subset of an entire dataset.","It has been primarily researched in image classification, assuming there is only one object per image.","However, coreset selection for object detection is more challenging as an image can contain multiple objects.","As a result, much research has yet to be done on this topic.","Therefore, we introduce a new approach, Coreset Selection for Object Detection (CSOD).","CSOD generates imagewise and classwise representative feature vectors for multiple objects of the same class within each image.","Subsequently, we adopt submodular optimization for considering both representativeness and diversity and utilize the representative vectors in the submodular optimization process to select a subset.","When we evaluated CSOD on the Pascal VOC dataset, CSOD outperformed random selection by +6.4%p in AP$_{50}$ when selecting 200 images."],"url":"http://arxiv.org/abs/2404.09161v1","category":"cs.CV"}
{"created":"2024-04-14 06:09:35","title":"Emerging Platforms Meet Emerging LLMs: A Year-Long Journey of Top-Down Development","abstract":"Deploying machine learning (ML) on diverse computing platforms is crucial to accelerate and broaden their applications. However, it presents significant software engineering challenges due to the fast evolution of models, especially the recent \\llmfull{s} (\\llm{s}), and the emergence of new computing platforms. Current ML frameworks are primarily engineered for CPU and CUDA platforms, leaving a big gap in enabling emerging ones like Metal, Vulkan, and WebGPU.   While a traditional bottom-up development pipeline fails to close the gap timely, we introduce TapML, a top-down approach and tooling designed to streamline the deployment of ML systems on diverse platforms, optimized for developer productivity. Unlike traditional bottom-up methods, which involve extensive manual testing and debugging, TapML automates unit testing through test carving and adopts a migration-based strategy for gradually offloading model computations from mature source platforms to emerging target platforms. By leveraging realistic inputs and remote connections for gradual target offloading, TapML accelerates the validation and minimizes debugging scopes, significantly optimizing development efforts.   TapML was developed and applied through a year-long, real-world effort that successfully deployed significant emerging models and platforms. Through serious deployments of 82 emerging models in 17 distinct architectures across 5 emerging platforms, we showcase the effectiveness of TapML in enhancing developer productivity while ensuring model reliability and efficiency. Furthermore, we summarize comprehensive case studies from our real-world development, offering best practices for developing emerging ML systems.","sentences":["Deploying machine learning (ML) on diverse computing platforms is crucial to accelerate and broaden their applications.","However, it presents significant software engineering challenges due to the fast evolution of models, especially the recent \\llmfull{s} (\\llm{s}), and the emergence of new computing platforms.","Current ML frameworks are primarily engineered for CPU and CUDA platforms, leaving a big gap in enabling emerging ones like Metal, Vulkan, and WebGPU.   ","While a traditional bottom-up development pipeline fails to close the gap timely, we introduce TapML, a top-down approach and tooling designed to streamline the deployment of ML systems on diverse platforms, optimized for developer productivity.","Unlike traditional bottom-up methods, which involve extensive manual testing and debugging, TapML automates unit testing through test carving and adopts a migration-based strategy for gradually offloading model computations from mature source platforms to emerging target platforms.","By leveraging realistic inputs and remote connections for gradual target offloading, TapML accelerates the validation and minimizes debugging scopes, significantly optimizing development efforts.   ","TapML was developed and applied through a year-long, real-world effort that successfully deployed significant emerging models and platforms.","Through serious deployments of 82 emerging models in 17 distinct architectures across 5 emerging platforms, we showcase the effectiveness of TapML in enhancing developer productivity while ensuring model reliability and efficiency.","Furthermore, we summarize comprehensive case studies from our real-world development, offering best practices for developing emerging ML systems."],"url":"http://arxiv.org/abs/2404.09151v1","category":"cs.SE"}
{"created":"2024-04-14 05:12:40","title":"Exoplanet Detection : A Detailed Analysis","abstract":"The exoplanet detection is the most exciting and challenging field of astronomy. The discovery of many exoplanets has revolutionized our understanding of the formation and evolution of planetary systems and has showed new ways to search for extra terrestrial life. In recent years, some primary methods of exoplanet detection like transit, radial velocity, gravitational microlensing, direct imaging and astrometry have played a important role for the discovery of exoplanets. In this paper we explored detection methodologies with all the implications and analytics of comparison between them. Here we also discussed on different machine learning algorithms for exoplanet detection and visualization. Finally, concluded with the significant discoveries made by some missions and their implications on our understanding for the properties, environmental conditions and importance of exoplanets in the universe.","sentences":["The exoplanet detection is the most exciting and challenging field of astronomy.","The discovery of many exoplanets has revolutionized our understanding of the formation and evolution of planetary systems and has showed new ways to search for extra terrestrial life.","In recent years, some primary methods of exoplanet detection like transit, radial velocity, gravitational microlensing, direct imaging and astrometry have played a important role for the discovery of exoplanets.","In this paper we explored detection methodologies with all the implications and analytics of comparison between them.","Here we also discussed on different machine learning algorithms for exoplanet detection and visualization.","Finally, concluded with the significant discoveries made by some missions and their implications on our understanding for the properties, environmental conditions and importance of exoplanets in the universe."],"url":"http://arxiv.org/abs/2404.09143v1","category":"astro-ph.EP"}
{"created":"2024-04-14 02:41:25","title":"Data-driven AC Optimal Power Flow with Physics-informed Learning and Calibrations","abstract":"The modern power grid is witnessing a shift in operations from traditional control methods to more advanced operational mechanisms. Due to the nonconvex nature of the Alternating Current Optimal Power Flow (ACOPF) problem and the need for operations with better granularity in the modern smart grid, system operators require a more efficient and reliable ACOPF solver. While data-driven ACOPF methods excel in directly inferring the optimal solution based on power grid demand, achieving both feasibility and optimality remains a challenge due to the NP-hardness of the problem. In this paper, we propose a physics-informed machine learning model and a feasibility calibration algorithm to produce solutions for the ACOPF problem. Notably, the machine learning model produces solutions with a 0.5\\% and 1.4\\% optimality gap for IEEE bus 14 and 118 grids, respectively. The feasibility correction algorithm converges for all test scenarios on bus 14 and achieves a 92.2% convergence rate on bus 118.","sentences":["The modern power grid is witnessing a shift in operations from traditional control methods to more advanced operational mechanisms.","Due to the nonconvex nature of the Alternating Current Optimal Power Flow (ACOPF) problem and the need for operations with better granularity in the modern smart grid, system operators require a more efficient and reliable ACOPF solver.","While data-driven ACOPF methods excel in directly inferring the optimal solution based on power grid demand, achieving both feasibility and optimality remains a challenge due to the NP-hardness of the problem.","In this paper, we propose a physics-informed machine learning model and a feasibility calibration algorithm to produce solutions for the ACOPF problem.","Notably, the machine learning model produces solutions with a 0.5\\% and 1.4\\% optimality gap for IEEE bus 14 and 118 grids, respectively.","The feasibility correction algorithm converges for all test scenarios on bus 14 and achieves a 92.2% convergence rate on bus 118."],"url":"http://arxiv.org/abs/2404.09128v1","category":"eess.SY"}
{"created":"2024-04-14 01:40:11","title":"Extending Mean-Field Variational Inference via Entropic Regularization: Theory and Computation","abstract":"Variational inference (VI) has emerged as a popular method for approximate inference for high-dimensional Bayesian models. In this paper, we propose a novel VI method that extends the naive mean field via entropic regularization, referred to as $\\Xi$-variational inference ($\\Xi$-VI). $\\Xi$-VI has a close connection to the entropic optimal transport problem and benefits from the computationally efficient Sinkhorn algorithm. We show that $\\Xi$-variational posteriors effectively recover the true posterior dependency, where the dependence is downweighted by the regularization parameter. We analyze the role of dimensionality of the parameter space on the accuracy of $\\Xi$-variational approximation and how it affects computational considerations, providing a rough characterization of the statistical-computational trade-off in $\\Xi$-VI. We also investigate the frequentist properties of $\\Xi$-VI and establish results on consistency, asymptotic normality, high-dimensional asymptotics, and algorithmic stability. We provide sufficient criteria for achieving polynomial-time approximate inference using the method. Finally, we demonstrate the practical advantage of $\\Xi$-VI over mean-field variational inference on simulated and real data.","sentences":["Variational inference (VI) has emerged as a popular method for approximate inference for high-dimensional Bayesian models.","In this paper, we propose a novel VI method that extends the naive mean field via entropic regularization, referred to as $\\Xi$-variational inference ($\\Xi$-VI).","$\\Xi$-VI has a close connection to the entropic optimal transport problem and benefits from the computationally efficient Sinkhorn algorithm.","We show that $\\Xi$-variational posteriors effectively recover the true posterior dependency, where the dependence is downweighted by the regularization parameter.","We analyze the role of dimensionality of the parameter space on the accuracy of $\\Xi$-variational approximation and how it affects computational considerations, providing a rough characterization of the statistical-computational trade-off in $\\Xi$-VI.","We also investigate the frequentist properties of $\\Xi$-VI and establish results on consistency, asymptotic normality, high-dimensional asymptotics, and algorithmic stability.","We provide sufficient criteria for achieving polynomial-time approximate inference using the method.","Finally, we demonstrate the practical advantage of $\\Xi$-VI over mean-field variational inference on simulated and real data."],"url":"http://arxiv.org/abs/2404.09113v1","category":"stat.ML"}
{"created":"2024-04-15 12:26:37","title":"Minimal Autocorrelation in Hybrid Monte Carlo simulations using Exact Fourier Acceleration","abstract":"The hybrid Monte Carlo (HMC) algorithm is a ubiquitous method in computational physics with applications ranging from condensed matter to lattice QCD and beyond. However, HMC simulations often suffer from long autocorrelation times, severely reducing their efficiency. In this work two of the main sources of autocorrelations are identified and eliminated. The first source is the sampling of the canonical momenta from a sub-optimal normal distribution, the second is a badly chosen trajectory length. Analytic solutions to both problems are presented and implemented in the exact Fourier acceleration (EFA) method. It completely removes autocorrelations for near-harmonic potentials and consistently yields (close-to-) optimal results for numerical simulations of the Su-Schrieffer-Heeger and the Ising models as well as in lattice gauge theory, in some cases reducing the autocorrelation by multiple orders of magnitude. EFA is advantageous for and easily applicable to any HMC simulation of an action that includes a quadratic part.","sentences":["The hybrid Monte Carlo (HMC) algorithm is a ubiquitous method in computational physics with applications ranging from condensed matter to lattice QCD and beyond.","However, HMC simulations often suffer from long autocorrelation times, severely reducing their efficiency.","In this work two of the main sources of autocorrelations are identified and eliminated.","The first source is the sampling of the canonical momenta from a sub-optimal normal distribution, the second is a badly chosen trajectory length.","Analytic solutions to both problems are presented and implemented in the exact Fourier acceleration (EFA) method.","It completely removes autocorrelations for near-harmonic potentials and consistently yields (close-to-) optimal results for numerical simulations of the Su-Schrieffer-Heeger and the Ising models as well as in lattice gauge theory, in some cases reducing the autocorrelation by multiple orders of magnitude.","EFA is advantageous for and easily applicable to any HMC simulation of an action that includes a quadratic part."],"url":"http://arxiv.org/abs/2404.09723v1","category":"hep-lat"}
{"created":"2024-04-15 09:25:29","title":"Finite-sample expansions for the optimal error probability in asymmetric binary hypothesis testing","abstract":"The problem of binary hypothesis testing between two probability measures is considered. New sharp bounds are derived for the best achievable error probability of such tests based on independent and identically distributed observations. Specifically, the asymmetric version of the problem is examined, where different requirements are placed on the two error probabilities. Accurate nonasymptotic expansions with explicit constants are obtained for the error probability, using tools from large deviations and Gaussian approximation. Examples are shown indicating that, in the asymmetric regime, the approximations suggested by the new bounds are significantly more accurate than the approximations provided by either of the two main earlier approaches -- normal approximation and error exponents.","sentences":["The problem of binary hypothesis testing between two probability measures is considered.","New sharp bounds are derived for the best achievable error probability of such tests based on independent and identically distributed observations.","Specifically, the asymmetric version of the problem is examined, where different requirements are placed on the two error probabilities.","Accurate nonasymptotic expansions with explicit constants are obtained for the error probability, using tools from large deviations and Gaussian approximation.","Examples are shown indicating that, in the asymmetric regime, the approximations suggested by the new bounds are significantly more accurate than the approximations provided by either of the two main earlier approaches -- normal approximation and error exponents."],"url":"http://arxiv.org/abs/2404.09605v1","category":"cs.IT"}
{"created":"2024-04-15 06:04:32","title":"Servers Placement Scheme Based on All-pay Auction Framework in Mobile Edge Computing","abstract":"Task offloading plays a pivotal role in mobile edge computing, enabling terminal devices to enhance task execution efficiency and conserve energy. However, servers are reluctant to offer services without compensation. Currently, pricing mechanisms are commonly employed to incentivize servers to serve terminal devices, with servers earning revenue through payments from these devices. Given the rapid surge in terminal devices, determining the optimal number of servers placement for service providers (SPs) to maximize revenue is crucial. In this paper, we propose a server placement scheme based on an all-pay auction framework. Experimental simulations reveal that an optimal server-user ratio of approximately 25% maximizes SP profits.","sentences":["Task offloading plays a pivotal role in mobile edge computing, enabling terminal devices to enhance task execution efficiency and conserve energy.","However, servers are reluctant to offer services without compensation.","Currently, pricing mechanisms are commonly employed to incentivize servers to serve terminal devices, with servers earning revenue through payments from these devices.","Given the rapid surge in terminal devices, determining the optimal number of servers placement for service providers (SPs) to maximize revenue is crucial.","In this paper, we propose a server placement scheme based on an all-pay auction framework.","Experimental simulations reveal that an optimal server-user ratio of approximately 25% maximizes SP profits."],"url":"http://arxiv.org/abs/2404.09477v1","category":"cs.GT"}
{"created":"2024-04-15 04:50:39","title":"CompGS: Efficient 3D Scene Representation via Compressed Gaussian Splatting","abstract":"Gaussian splatting, renowned for its exceptional rendering quality and efficiency, has emerged as a prominent technique in 3D scene representation. However, the substantial data volume of Gaussian splatting impedes its practical utility in real-world applications. Herein, we propose an efficient 3D scene representation, named Compressed Gaussian Splatting (CompGS), which harnesses compact Gaussian primitives for faithful 3D scene modeling with a remarkably reduced data size. To ensure the compactness of Gaussian primitives, we devise a hybrid primitive structure that captures predictive relationships between each other. Then, we exploit a small set of anchor primitives for prediction, allowing the majority of primitives to be encapsulated into highly compact residual forms. Moreover, we develop a rate-constrained optimization scheme to eliminate redundancies within such hybrid primitives, steering our CompGS towards an optimal trade-off between bitrate consumption and representation efficacy. Experimental results show that the proposed CompGS significantly outperforms existing methods, achieving superior compactness in 3D scene representation without compromising model accuracy and rendering quality. Our code will be released on GitHub for further research.","sentences":["Gaussian splatting, renowned for its exceptional rendering quality and efficiency, has emerged as a prominent technique in 3D scene representation.","However, the substantial data volume of Gaussian splatting impedes its practical utility in real-world applications.","Herein, we propose an efficient 3D scene representation, named Compressed Gaussian Splatting (CompGS), which harnesses compact Gaussian primitives for faithful 3D scene modeling with a remarkably reduced data size.","To ensure the compactness of Gaussian primitives, we devise a hybrid primitive structure that captures predictive relationships between each other.","Then, we exploit a small set of anchor primitives for prediction, allowing the majority of primitives to be encapsulated into highly compact residual forms.","Moreover, we develop a rate-constrained optimization scheme to eliminate redundancies within such hybrid primitives, steering our CompGS towards an optimal trade-off between bitrate consumption and representation efficacy.","Experimental results show that the proposed CompGS significantly outperforms existing methods, achieving superior compactness in 3D scene representation without compromising model accuracy and rendering quality.","Our code will be released on GitHub for further research."],"url":"http://arxiv.org/abs/2404.09458v1","category":"cs.CV"}
{"created":"2024-04-15 04:44:18","title":"Feedback Communication Over the BSC with Sparse Feedback times and Causal Encoding","abstract":"Posterior matching uses variable-length encoding of the message controlled by noiseless feedback of the received symbols to achieve high rates for short average blocklengths. Traditionally, the feedback of a received symbol occurs before the next symbol is transmitted. The transmitter optimizes the next symbol transmission with full knowledge of every past received symbol.   To move posterior matching closer to practical communication, this paper seeks to constrain how often feedback can be sent back to the transmitter. We focus on reducing the frequency of the feedback while still maintaining the high rates that posterior matching achieves with feedback after every symbol. As it turns out, the frequency of the feedback can be reduced significantly with no noticeable reduction in rate.","sentences":["Posterior matching uses variable-length encoding of the message controlled by noiseless feedback of the received symbols to achieve high rates for short average blocklengths.","Traditionally, the feedback of a received symbol occurs before the next symbol is transmitted.","The transmitter optimizes the next symbol transmission with full knowledge of every past received symbol.   ","To move posterior matching closer to practical communication, this paper seeks to constrain how often feedback can be sent back to the transmitter.","We focus on reducing the frequency of the feedback while still maintaining the high rates that posterior matching achieves with feedback after every symbol.","As it turns out, the frequency of the feedback can be reduced significantly with no noticeable reduction in rate."],"url":"http://arxiv.org/abs/2404.09455v1","category":"cs.IT"}
{"created":"2024-04-14 20:43:43","title":"A Paradigm For Collaborative Pervasive Fog Computing Ecosystems at the Network Edge","abstract":"While the success of edge and fog computing increased with the proliferation of the Internet of Things (IoT) solutions, such novel computing paradigm, that moves compute resources closer to the source of data and services, must address many challenges such as reducing communication overhead to/from datacenters, the latency to compute and receive results, as well as energy consumption at the mobile and IoT devices. fog-to-fog (f2f) cooperation has recently been proposed to increase the computation capacity at the network edge through cooperation across multiple stakeholders. In this paper we adopt an analytical approach to studying f2f cooperation paradigm. We highlight the benefits of using such new paradigm in comparison with traditional three-tier fog computing paradigms. We use a Continuous Time Markov Chain (CTMC) model for the N f2f cooperating nodes and cast cooperation as an optimization problem, which we solve using the proposed model.","sentences":["While the success of edge and fog computing increased with the proliferation of the Internet of Things (IoT) solutions, such novel computing paradigm, that moves compute resources closer to the source of data and services, must address many challenges such as reducing communication overhead to/from datacenters, the latency to compute and receive results, as well as energy consumption at the mobile and IoT devices.","fog-to-fog (f2f) cooperation has recently been proposed to increase the computation capacity at the network edge through cooperation across multiple stakeholders.","In this paper we adopt an analytical approach to studying f2f cooperation paradigm.","We highlight the benefits of using such new paradigm in comparison with traditional three-tier fog computing paradigms.","We use a Continuous Time Markov Chain (CTMC) model for the N f2f cooperating nodes and cast cooperation as an optimization problem, which we solve using the proposed model."],"url":"http://arxiv.org/abs/2404.09354v1","category":"cs.NI"}
{"created":"2024-04-14 18:58:37","title":"Measurement-Induced Heating of Trapped Ions","abstract":"We experimentally study the heating of trapped atomic ions during measurement of their internal qubit states. During measurement, ions are projected into one of two basis states and discriminated by their state-dependent fluorescence. We observe that ions in the fluorescing state rapidly scatter photons and heat at a rate of $\\dot{\\bar{n}}\\sim 2\\times 10^4$ quanta/s, which is $\\sim 30$ times faster than the anomalous ion heating rate. We introduce a quantum trajectory-based framework that accurately reproduces the experimental results and provides a unified description of ion heating for both continuous and discrete sources.","sentences":["We experimentally study the heating of trapped atomic ions during measurement of their internal qubit states.","During measurement, ions are projected into one of two basis states and discriminated by their state-dependent fluorescence.","We observe that ions in the fluorescing state rapidly scatter photons and heat at a rate of $\\dot{\\bar{n}}\\sim 2\\times 10^4$ quanta/s, which is $\\sim 30$ times faster than the anomalous ion heating rate.","We introduce a quantum trajectory-based framework that accurately reproduces the experimental results and provides a unified description of ion heating for both continuous and discrete sources."],"url":"http://arxiv.org/abs/2404.09327v1","category":"quant-ph"}
{"created":"2024-04-14 09:29:37","title":"Tube-RRT*: Efficient Homotopic Path Planning for Swarm Robotics Passing-Through Large-Scale Obstacle Environments","abstract":"Recently, the concept of optimal virtual tube has emerged as a novel solution to the challenging task of navigating obstacle-dense environments for swarm robotics, offering a wide ranging of applications. However, it lacks an efficient homotopic path planning method in obstacle-dense environments. This paper introduces Tube-RRT*, an innovative homotopic path planning method that builds upon and improves the Rapidly-exploring Random Tree (RRT) algorithm. Tube-RRT* is specifically designed to generate homotopic paths for the trajectories in the virtual tube, strategically considering opening volume and tube length to mitigate swarm congestion and ensure agile navigation. Through comprehensive comparative simulations conducted within complex, large-scale obstacle environments, we demonstrate the effectiveness of Tube-RRT*.","sentences":["Recently, the concept of optimal virtual tube has emerged as a novel solution to the challenging task of navigating obstacle-dense environments for swarm robotics, offering a wide ranging of applications.","However, it lacks an efficient homotopic path planning method in obstacle-dense environments.","This paper introduces Tube-RRT*, an innovative homotopic path planning method that builds upon and improves the Rapidly-exploring Random Tree (RRT) algorithm.","Tube-RRT* is specifically designed to generate homotopic paths for the trajectories in the virtual tube, strategically considering opening volume and tube length to mitigate swarm congestion and ensure agile navigation.","Through comprehensive comparative simulations conducted within complex, large-scale obstacle environments, we demonstrate the effectiveness of Tube-RRT*."],"url":"http://arxiv.org/abs/2404.09200v1","category":"cs.RO"}
{"created":"2024-04-14 02:48:42","title":"Quantum subspace expansion in the presence of hardware noise","abstract":"Finding ground state energies on current quantum processing units (QPUs) using algorithms like the variational quantum eigensolver (VQE) continues to pose challenges. Hardware noise severely affects both the expressivity and trainability of parametrized quantum circuits, limiting them to shallow depths in practice. Here, we demonstrate that both issues can be addressed by synergistically integrating VQE with a quantum subspace expansion, allowing for an optimal balance between quantum and classical computing capabilities and costs. We perform a systematic benchmark analysis of the iterative quantum-assisted eigensolver of [K. Bharti and T. Haug, Phys. Rev. A {\\bf 104}, L050401 (2021)] in the presence of hardware noise. We determine ground state energies of 1D and 2D mixed-field Ising spin models on noisy simulators and on the IBM QPUs ibmq_quito (5 qubits) and ibmq_guadalupe (16 qubits). To maximize accuracy, we propose a suitable criterion to select the subspace basis vectors according to the trace of the noisy overlap matrix. Finally, we show how to systematically approach the exact solution by performing controlled quantum error mitigation based on probabilistic error reduction on the noisy backend fake_guadalupe.","sentences":["Finding ground state energies on current quantum processing units (QPUs) using algorithms like the variational quantum eigensolver (VQE) continues to pose challenges.","Hardware noise severely affects both the expressivity and trainability of parametrized quantum circuits, limiting them to shallow depths in practice.","Here, we demonstrate that both issues can be addressed by synergistically integrating VQE with a quantum subspace expansion, allowing for an optimal balance between quantum and classical computing capabilities and costs.","We perform a systematic benchmark analysis of the iterative quantum-assisted eigensolver of [K. Bharti and T. Haug, Phys.","Rev. A {\\bf 104}, L050401 (2021)] in the presence of hardware noise.","We determine ground state energies of 1D and 2D mixed-field Ising spin models on noisy simulators and on the IBM QPUs ibmq_quito (5 qubits) and ibmq_guadalupe (16 qubits).","To maximize accuracy, we propose a suitable criterion to select the subspace basis vectors according to the trace of the noisy overlap matrix.","Finally, we show how to systematically approach the exact solution by performing controlled quantum error mitigation based on probabilistic error reduction on the noisy backend fake_guadalupe."],"url":"http://arxiv.org/abs/2404.09132v1","category":"quant-ph"}
{"created":"2024-04-14 00:28:15","title":"Optimizing Disjunctive Queries with Tagged Execution","abstract":"Despite decades of research into query optimization, optimizing queries with disjunctive predicate expressions remains a challenge. Solutions employed by existing systems (if any) are often simplistic and lead to much redundant work being performed by the execution engine. To address these problems, we propose a novel form of query execution called tagged execution. Tagged execution groups tuples into subrelations based on which predicates in the query they satisfy (or don't satisfy) and tags them with that information. These tags then provide additional context for query operators to take advantage of during runtime, allowing them to eliminate much of the redundant work performed by traditional engines and realize predicate pushdown optimizations for disjunctive predicates. However, tagged execution brings its own challenges, and the question of what tags to create is a nontrivial one. Careless creation of tags can lead to an exponential blowup in the tag space, with the overhead outweighing the benefits. To address this issue, we present a technique called tag generalization to minimize the space of tags. We implemented the tagged execution model with tag generalization in our system Basilisk, and our evaluation shows an average 2.7x speedup in runtime over the traditional execution model with up to a 19x speedup in certain situations.","sentences":["Despite decades of research into query optimization, optimizing queries with disjunctive predicate expressions remains a challenge.","Solutions employed by existing systems (if any) are often simplistic and lead to much redundant work being performed by the execution engine.","To address these problems, we propose a novel form of query execution called tagged execution.","Tagged execution groups tuples into subrelations based on which predicates in the query they satisfy (or don't satisfy) and tags them with that information.","These tags then provide additional context for query operators to take advantage of during runtime, allowing them to eliminate much of the redundant work performed by traditional engines and realize predicate pushdown optimizations for disjunctive predicates.","However, tagged execution brings its own challenges, and the question of what tags to create is a nontrivial one.","Careless creation of tags can lead to an exponential blowup in the tag space, with the overhead outweighing the benefits.","To address this issue, we present a technique called tag generalization to minimize the space of tags.","We implemented the tagged execution model with tag generalization in our system Basilisk, and our evaluation shows an average 2.7x speedup in runtime over the traditional execution model with up to a 19x speedup in certain situations."],"url":"http://arxiv.org/abs/2404.09109v1","category":"cs.DB"}
{"created":"2024-04-13 22:26:56","title":"InverseVis: Revealing the Hidden with Curved Sphere Tracing","abstract":"Exploratory analysis of scalar fields on surface meshes presents significant challenges in identifying and visualizing important regions, particularly on the surface's backside. Previous visualization methods achieved only a limited visibility of significant features, i.e., regions with high or low scalar values, during interactive exploration. In response to this, we propose a novel technique, InverseVis, which leverages curved sphere tracing and uses the otherwise unused space to enhance visibility. Our approach combines direct and indirect rendering, allowing camera rays to wrap around the surface and reveal information from the backside. To achieve this, we formulate an energy term that guides the image synthesis in previously unused space, highlighting the most important regions of the backside. By quantifying the amount of visible important features, we optimize the camera position to maximize the visibility of the scalar field on both the front and backsides. InverseVis is benchmarked against state-of-the-art methods and a derived technique, showcasing its effectiveness in revealing essential features and outperforming existing approaches.","sentences":["Exploratory analysis of scalar fields on surface meshes presents significant challenges in identifying and visualizing important regions, particularly on the surface's backside.","Previous visualization methods achieved only a limited visibility of significant features, i.e., regions with high or low scalar values, during interactive exploration.","In response to this, we propose a novel technique, InverseVis, which leverages curved sphere tracing and uses the otherwise unused space to enhance visibility.","Our approach combines direct and indirect rendering, allowing camera rays to wrap around the surface and reveal information from the backside.","To achieve this, we formulate an energy term that guides the image synthesis in previously unused space, highlighting the most important regions of the backside.","By quantifying the amount of visible important features, we optimize the camera position to maximize the visibility of the scalar field on both the front and backsides.","InverseVis is benchmarked against state-of-the-art methods and a derived technique, showcasing its effectiveness in revealing essential features and outperforming existing approaches."],"url":"http://arxiv.org/abs/2404.09092v1","category":"cs.GR"}
{"created":"2024-04-13 21:50:50","title":"On the Benefits of Traffic \"Reprofiling\" -- The Multiple Hops Case -- Part I","abstract":"This paper considers networks where user traffic is regulated through deterministic traffic profiles, e.g., token buckets, and requires hard delay bounds. The network's goal is to minimize the resources it needs to meet those bounds. The paper explores how reprofiling, i.e., proactively modifying how user traffic enters the network, can be of benefit. Reprofiling produces ``smoother'' flows but introduces an up-front access delay that forces tighter network delays. The paper explores this trade-off and demonstrates that, unlike what holds in the single-hop case, reprofiling can be of benefit} even when ``optimal'' schedulers are available at each hop.","sentences":["This paper considers networks where user traffic is regulated through deterministic traffic profiles, e.g., token buckets, and requires hard delay bounds.","The network's goal is to minimize the resources it needs to meet those bounds.","The paper explores how reprofiling, i.e., proactively modifying how user traffic enters the network, can be of benefit.","Reprofiling produces ``smoother'' flows but introduces an up-front access delay that forces tighter network delays.","The paper explores this trade-off and demonstrates that, unlike what holds in the single-hop case, reprofiling can be of benefit} even when ``optimal'' schedulers are available at each hop."],"url":"http://arxiv.org/abs/2404.09087v1","category":"cs.NI"}
{"created":"2024-04-13 20:51:16","title":"Compactness results for a Dirichlet energy of nonlocal gradient with applications","abstract":"We prove two compactness results for function spaces with finite Dirichlet energy of half-space nonlocal gradients. In each of these results, we provide sufficient conditions on a sequence of kernel functions that guarantee the asymptotic compact embedding of the associated nonlocal function spaces into the class of square-integrable functions. Moreover, we will demonstrate that the sequence of nonlocal function spaces converges in an appropriate sense to a limiting function space. As an application, we prove uniform Poincar\\'e-type inequalities for sequence of half-space gradient operators. We also apply the compactness result to demonstrate the convergence of appropriately parameterized nonlocal heterogeneous anisotropic diffusion problems. We will construct asymptotically compatible schemes for these type of problems. Another application concerns the convergence and robust discretization of a nonlocal optimal control problem.","sentences":["We prove two compactness results for function spaces with finite Dirichlet energy of half-space nonlocal gradients.","In each of these results, we provide sufficient conditions on a sequence of kernel functions that guarantee the asymptotic compact embedding of the associated nonlocal function spaces into the class of square-integrable functions.","Moreover, we will demonstrate that the sequence of nonlocal function spaces converges in an appropriate sense to a limiting function space.","As an application, we prove uniform Poincar\\'e-type inequalities for sequence of half-space gradient operators.","We also apply the compactness result to demonstrate the convergence of appropriately parameterized nonlocal heterogeneous anisotropic diffusion problems.","We will construct asymptotically compatible schemes for these type of problems.","Another application concerns the convergence and robust discretization of a nonlocal optimal control problem."],"url":"http://arxiv.org/abs/2404.09079v1","category":"math.AP"}
{"created":"2024-04-13 19:53:14","title":"Statistical Analysis of Block Coordinate Descent Algorithms for Linear Continuous-time System Identification","abstract":"Block coordinate descent is an optimization technique that is used for estimating multi-input single-output (MISO) continuous-time models, as well as single-input single output (SISO) models in additive form. Despite its widespread use in various optimization contexts, the statistical properties of block coordinate descent in continuous-time system identification have not been covered in the literature. The aim of this paper is to formally analyze the bias properties of the block coordinate descent approach for the identification of MISO and additive SISO systems. We characterize the asymptotic bias at each iteration, and provide sufficient conditions for the consistency of the estimator for each identification setting. The theoretical results are supported by simulation examples.","sentences":["Block coordinate descent is an optimization technique that is used for estimating multi-input single-output (MISO) continuous-time models, as well as single-input single output (SISO) models in additive form.","Despite its widespread use in various optimization contexts, the statistical properties of block coordinate descent in continuous-time system identification have not been covered in the literature.","The aim of this paper is to formally analyze the bias properties of the block coordinate descent approach for the identification of MISO and additive SISO systems.","We characterize the asymptotic bias at each iteration, and provide sufficient conditions for the consistency of the estimator for each identification setting.","The theoretical results are supported by simulation examples."],"url":"http://arxiv.org/abs/2404.09071v1","category":"eess.SY"}
{"created":"2024-04-13 19:07:25","title":"Asynchronous Heterogeneous Linear Quadratic Regulator Design","abstract":"We address the problem of designing an LQR controller in a distributed setting, where M similar but not identical systems share their locally computed policy gradient (PG) estimates with a server that aggregates the estimates and computes a controller that, on average, performs well on all systems. Learning in a distributed setting has the potential to offer statistical benefits - multiple datasets can be leveraged simultaneously to produce more accurate policy gradient estimates. However, the interplay of heterogeneous trajectory data and varying levels of local computational power introduce bias to the aggregated PG descent direction, and prevents us from fully exploiting the parallelism in the distributed computation. The latter stems from synchronous aggregation, where straggler systems negatively impact the runtime. To address this, we propose an asynchronous policy gradient algorithm for LQR control design. By carefully controlling the \"staleness\" in the asynchronous aggregation, we show that the designed controller converges to each system's $\\epsilon$-near optimal controller up to a heterogeneity bias. Furthermore, we prove that our asynchronous approach obtains exact local convergence at a sub-linear rate.","sentences":["We address the problem of designing an LQR controller in a distributed setting, where M similar but not identical systems share their locally computed policy gradient (PG) estimates with a server that aggregates the estimates and computes a controller that, on average, performs well on all systems.","Learning in a distributed setting has the potential to offer statistical benefits - multiple datasets can be leveraged simultaneously to produce more accurate policy gradient estimates.","However, the interplay of heterogeneous trajectory data and varying levels of local computational power introduce bias to the aggregated PG descent direction, and prevents us from fully exploiting the parallelism in the distributed computation.","The latter stems from synchronous aggregation, where straggler systems negatively impact the runtime.","To address this, we propose an asynchronous policy gradient algorithm for LQR control design.","By carefully controlling the \"staleness\" in the asynchronous aggregation, we show that the designed controller converges to each system's $\\epsilon$-near optimal controller up to a heterogeneity bias.","Furthermore, we prove that our asynchronous approach obtains exact local convergence at a sub-linear rate."],"url":"http://arxiv.org/abs/2404.09061v1","category":"math.OC"}
{"created":"2024-04-13 12:58:20","title":"Dual-comb mode-locked Yb:CALGO laser based on cavity-shared configuration with separated end mirrors","abstract":"Dual-comb spectroscopy typically requires the utilization of two independent and phase-locked femtosecond lasers, resulting in a complex and expensive system that hinders its industrial applications. Single-cavity dual-comb lasers are considered as one of the primary solution to simplify the system. However, controlling the crucial parameter of difference in repetition rates remains challenging. In this study, we present a dual-comb mode-locked Yb:CALGO laser based on a cavity-shared configuration with separated end mirrors. We employ two pairs of end mirrors and two thin-film polarizers angled at 45 degrees to the cavity axis, leading to separating the cross-polarized laser modes. We achieve simultaneous operation of two combs at approximately 1040 nm with pulse durations of around 400 fs and an average power exceeding 1 W. The repetition rates are approximately 59 MHz and their difference can be easily tuned from zero up to the MHz range. By effectively canceling out common mode noises, we observe minimal fluctuation in the repetition rate difference with a standard deviation of about 1.9 Hz over ten minutes, while experiencing fluctuations in repetition rates as large as 90 Hz. We demonstrate the capabilities of this system by utilizing the free-running dual-comb setup for asynchronous optical sampling on a saturable absorber and measuring etalon transmission spectrum. This system allows for simple and independent control of the repetition rates and their difference during operation, facilitating the selection of optimal repetition rate difference and implementation of phase-locking loops. This advancement paves the way for the development of simple yet high-performance dual-comb laser sources.","sentences":["Dual-comb spectroscopy typically requires the utilization of two independent and phase-locked femtosecond lasers, resulting in a complex and expensive system that hinders its industrial applications.","Single-cavity dual-comb lasers are considered as one of the primary solution to simplify the system.","However, controlling the crucial parameter of difference in repetition rates remains challenging.","In this study, we present a dual-comb mode-locked Yb:CALGO laser based on a cavity-shared configuration with separated end mirrors.","We employ two pairs of end mirrors and two thin-film polarizers angled at 45 degrees to the cavity axis, leading to separating the cross-polarized laser modes.","We achieve simultaneous operation of two combs at approximately 1040 nm with pulse durations of around 400 fs and an average power exceeding 1 W.","The repetition rates are approximately 59 MHz and their difference can be easily tuned from zero up to the MHz range.","By effectively canceling out common mode noises, we observe minimal fluctuation in the repetition rate difference with a standard deviation of about 1.9 Hz over ten minutes, while experiencing fluctuations in repetition rates as large as 90 Hz.","We demonstrate the capabilities of this system by utilizing the free-running dual-comb setup for asynchronous optical sampling on a saturable absorber and measuring etalon transmission spectrum.","This system allows for simple and independent control of the repetition rates and their difference during operation, facilitating the selection of optimal repetition rate difference and implementation of phase-locking loops.","This advancement paves the way for the development of simple yet high-performance dual-comb laser sources."],"url":"http://arxiv.org/abs/2404.08998v1","category":"physics.optics"}
{"created":"2024-04-13 12:06:29","title":"BG-YOLO: A Bidirectional-Guided Method for Underwater Object Detection","abstract":"Degraded underwater images decrease the accuracy of underwater object detection. However, existing methods for underwater image enhancement mainly focus on improving the indicators in visual aspects, which may not benefit the tasks of underwater image detection, and may lead to serious degradation in performance. To alleviate this problem, we proposed a bidirectional-guided method for underwater object detection, referred to as BG-YOLO. In the proposed method, network is organized by constructing an enhancement branch and a detection branch in a parallel way. The enhancement branch consists of a cascade of an image enhancement subnet and an object detection subnet. And the detection branch only consists of a detection subnet. A feature guided module connects the shallow convolution layer of the two branches. When training the enhancement branch, the object detection subnet in the enhancement branch guides the image enhancement subnet to be optimized towards the direction that is most conducive to the detection task. The shallow feature map of the trained enhancement branch will be output to the feature guided module, constraining the optimization of detection branch through consistency loss and prompting detection branch to learn more detailed information of the objects. And hence the detection performance will be refined. During the detection tasks, only detection branch will be reserved so that no additional cost of computation will be introduced. Extensive experiments demonstrate that the proposed method shows significant improvement in performance of the detector in severely degraded underwater scenes while maintaining a remarkable detection speed.","sentences":["Degraded underwater images decrease the accuracy of underwater object detection.","However, existing methods for underwater image enhancement mainly focus on improving the indicators in visual aspects, which may not benefit the tasks of underwater image detection, and may lead to serious degradation in performance.","To alleviate this problem, we proposed a bidirectional-guided method for underwater object detection, referred to as BG-YOLO.","In the proposed method, network is organized by constructing an enhancement branch and a detection branch in a parallel way.","The enhancement branch consists of a cascade of an image enhancement subnet and an object detection subnet.","And the detection branch only consists of a detection subnet.","A feature guided module connects the shallow convolution layer of the two branches.","When training the enhancement branch, the object detection subnet in the enhancement branch guides the image enhancement subnet to be optimized towards the direction that is most conducive to the detection task.","The shallow feature map of the trained enhancement branch will be output to the feature guided module, constraining the optimization of detection branch through consistency loss and prompting detection branch to learn more detailed information of the objects.","And hence the detection performance will be refined.","During the detection tasks, only detection branch will be reserved so that no additional cost of computation will be introduced.","Extensive experiments demonstrate that the proposed method shows significant improvement in performance of the detector in severely degraded underwater scenes while maintaining a remarkable detection speed."],"url":"http://arxiv.org/abs/2404.08979v1","category":"cs.CV"}
{"created":"2024-04-13 11:58:28","title":"RoNID: New Intent Discovery with Generated-Reliable Labels and Cluster-friendly Representations","abstract":"New Intent Discovery (NID) strives to identify known and reasonably deduce novel intent groups in the open-world scenario. But current methods face issues with inaccurate pseudo-labels and poor representation learning, creating a negative feedback loop that degrades overall model performance, including accuracy and the adjusted rand index. To address the aforementioned challenges, we propose a Robust New Intent Discovery (RoNID) framework optimized by an EM-style method, which focuses on constructing reliable pseudo-labels and obtaining cluster-friendly discriminative representations. RoNID comprises two main modules: reliable pseudo-label generation module and cluster-friendly representation learning module. Specifically, the pseudo-label generation module assigns reliable synthetic labels by solving an optimal transport problem in the E-step, which effectively provides high-quality supervised signals for the input of the cluster-friendly representation learning module. To learn cluster-friendly representation with strong intra-cluster compactness and large inter-cluster separation, the representation learning module combines intra-cluster and inter-cluster contrastive learning in the M-step to feed more discriminative features into the generation module. RoNID can be performed iteratively to ultimately yield a robust model with reliable pseudo-labels and cluster-friendly representations. Experimental results on multiple benchmarks demonstrate our method brings substantial improvements over previous state-of-the-art methods by a large margin of +1~+4 points.","sentences":["New Intent Discovery (NID) strives to identify known and reasonably deduce novel intent groups in the open-world scenario.","But current methods face issues with inaccurate pseudo-labels and poor representation learning, creating a negative feedback loop that degrades overall model performance, including accuracy and the adjusted rand index.","To address the aforementioned challenges, we propose a Robust New Intent Discovery (RoNID) framework optimized by an EM-style method, which focuses on constructing reliable pseudo-labels and obtaining cluster-friendly discriminative representations.","RoNID comprises two main modules: reliable pseudo-label generation module and cluster-friendly representation learning module.","Specifically, the pseudo-label generation module assigns reliable synthetic labels by solving an optimal transport problem in the E-step, which effectively provides high-quality supervised signals for the input of the cluster-friendly representation learning module.","To learn cluster-friendly representation with strong intra-cluster compactness and large inter-cluster separation, the representation learning module combines intra-cluster and inter-cluster contrastive learning in the M-step to feed more discriminative features into the generation module.","RoNID can be performed iteratively to ultimately yield a robust model with reliable pseudo-labels and cluster-friendly representations.","Experimental results on multiple benchmarks demonstrate our method brings substantial improvements over previous state-of-the-art methods by a large margin of +1~+4 points."],"url":"http://arxiv.org/abs/2404.08977v1","category":"cs.CL"}
{"created":"2024-04-13 11:57:43","title":"Degrees of Freedom for Radiating Systems","abstract":"Electromagnetic degrees of freedom are instrumental in antenna design, wireless communications, imaging, and scattering. Larger number of degrees of freedom enhances control in antenna design, influencing radiation patterns and directivity, while in communication systems, it links to spatial channels for increased data rates and reliability, and resolution in imaging. The correlation between computed degrees of freedom and physical quantities is not fully understood, prompting a comparison between classical estimates, Weyl's law, modal expansions, and optimization techniques. In this paper, it is shown that NDoF for arbitrary shaped radiating structures approaches the shadow area measured in squared wavelengths.","sentences":["Electromagnetic degrees of freedom are instrumental in antenna design, wireless communications, imaging, and scattering.","Larger number of degrees of freedom enhances control in antenna design, influencing radiation patterns and directivity, while in communication systems, it links to spatial channels for increased data rates and reliability, and resolution in imaging.","The correlation between computed degrees of freedom and physical quantities is not fully understood, prompting a comparison between classical estimates, Weyl's law, modal expansions, and optimization techniques.","In this paper, it is shown that NDoF for arbitrary shaped radiating structures approaches the shadow area measured in squared wavelengths."],"url":"http://arxiv.org/abs/2404.08976v1","category":"eess.SP"}
{"created":"2024-04-13 11:06:27","title":"Facility Assignment with Fair Cost Sharing: Equilibrium and Mechanism Design","abstract":"In the one-dimensional facility assignment problem, m facilities and n agents are positioned along the real line. Each agent will be assigned to a single facility to receive service. Each facility incurs a building cost, which is shared equally among the agents utilizing it. Additionally, each agent independently bears a connection cost to access a facility. Thus, an agent's cost is the sum of the connection cost and her portion of the building cost. The social cost is the total cost of all agents. Notably, the optimal assignment that minimizes the social cost can be found in polynomial time. In this paper, we study the problem from two game-theoretical settings regarding the strategy space of agents and the rule the assignment. In both settings, agents act strategically to minimize their individual costs.   In our first setting, the strategy space of agents is the set of facilities, granting agents the freedom to select any facility. Consequently, the self-formed assignment can exhibit instability, as agents may deviate to other facilities. We focus on the computation of an equilibrium assignment, where no agent has an incentive to unilaterally change her choice. We show that we can compute a pure Nash equilibrium in polynomial time.   In our second setting, agents report their positions to a mechanism for assignment to facilities. The strategy space of agents becomes the set of all positions. Our interest lies in strategyproof mechanisms. It is essential to note that the preference induced by the agents' cost function is more complex as it depends on how other agents are assigned. We establish a strong lower bound against all strategyproof and anonymous mechanisms: none can achieve a bounded social cost approximation ratio. Nonetheless, we identify a class of non-trivial strategyproof mechanisms for any n and m that is unanimous and anonymous.","sentences":["In the one-dimensional facility assignment problem, m facilities and n agents are positioned along the real line.","Each agent will be assigned to a single facility to receive service.","Each facility incurs a building cost, which is shared equally among the agents utilizing it.","Additionally, each agent independently bears a connection cost to access a facility.","Thus, an agent's cost is the sum of the connection cost and her portion of the building cost.","The social cost is the total cost of all agents.","Notably, the optimal assignment that minimizes the social cost can be found in polynomial time.","In this paper, we study the problem from two game-theoretical settings regarding the strategy space of agents and the rule the assignment.","In both settings, agents act strategically to minimize their individual costs.   ","In our first setting, the strategy space of agents is the set of facilities, granting agents the freedom to select any facility.","Consequently, the self-formed assignment can exhibit instability, as agents may deviate to other facilities.","We focus on the computation of an equilibrium assignment, where no agent has an incentive to unilaterally change her choice.","We show that we can compute a pure Nash equilibrium in polynomial time.   ","In our second setting, agents report their positions to a mechanism for assignment to facilities.","The strategy space of agents becomes the set of all positions.","Our interest lies in strategyproof mechanisms.","It is essential to note that the preference induced by the agents' cost function is more complex as it depends on how other agents are assigned.","We establish a strong lower bound against all strategyproof and anonymous mechanisms: none can achieve a bounded social cost approximation ratio.","Nonetheless, we identify a class of non-trivial strategyproof mechanisms for any n and m that is unanimous and anonymous."],"url":"http://arxiv.org/abs/2404.08963v1","category":"cs.GT"}
{"created":"2024-04-13 08:50:44","title":"Performance analysis of a filtering variational quantum algorithm","abstract":"Even a minor boost in solving combinatorial optimization problems can greatly benefit multiple industries. Quantum computers, with their unique information processing capabilities, hold promise for delivering such enhancements. The Filtering Variational Quantum Eigensolver (F-VQE) is a variational hybrid quantum algorithm designed to solve combinatorial optimization problems on existing quantum computers with limited qubit number, connectivity, and fidelity. In this work we employ Instantaneous Quantum Polynomial circuits as our parameterized quantum circuits. We propose a hardware-efficient implementation that respects limited qubit connectivity and show that they halve the number of circuits necessary to evaluate the gradient with the parameter-shift rule. To assess the potential of this protocol in the context of combinatorial optimization, we conduct extensive numerical analysis. We compare the performance against three classical baseline algorithms on weighted MaxCut and the Asymmetric Traveling Salesperson Problem (ATSP). We employ noiseless simulators for problems encoded on 13 to 29 qubits, and up to 37 qubits on the IBMQ real quantum devices. The ATSP encoding employed reduces the number of qubits and avoids the need of constraints compared to the standard QUBO / Ising model. Despite some observed positive signs, we conclude that significant development is necessary for a practical advantage with F-VQE.","sentences":["Even a minor boost in solving combinatorial optimization problems can greatly benefit multiple industries.","Quantum computers, with their unique information processing capabilities, hold promise for delivering such enhancements.","The Filtering Variational Quantum Eigensolver (F-VQE) is a variational hybrid quantum algorithm designed to solve combinatorial optimization problems on existing quantum computers with limited qubit number, connectivity, and fidelity.","In this work we employ Instantaneous Quantum Polynomial circuits as our parameterized quantum circuits.","We propose a hardware-efficient implementation that respects limited qubit connectivity and show that they halve the number of circuits necessary to evaluate the gradient with the parameter-shift rule.","To assess the potential of this protocol in the context of combinatorial optimization, we conduct extensive numerical analysis.","We compare the performance against three classical baseline algorithms on weighted MaxCut and the Asymmetric Traveling Salesperson Problem (ATSP).","We employ noiseless simulators for problems encoded on 13 to 29 qubits, and up to 37 qubits on the IBMQ real quantum devices.","The ATSP encoding employed reduces the number of qubits and avoids the need of constraints compared to the standard QUBO / Ising model.","Despite some observed positive signs, we conclude that significant development is necessary for a practical advantage with F-VQE."],"url":"http://arxiv.org/abs/2404.08933v1","category":"quant-ph"}
