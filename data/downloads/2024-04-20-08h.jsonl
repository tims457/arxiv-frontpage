{"created":"2024-04-17 17:55:17","title":"Learning to Solve the Constrained Most Probable Explanation Task in Probabilistic Graphical Models","abstract":"We propose a self-supervised learning approach for solving the following constrained optimization task in log-linear models or Markov networks. Let $f$ and $g$ be two log-linear models defined over the sets $\\mathbf{X}$ and $\\mathbf{Y}$ of random variables respectively. Given an assignment $\\mathbf{x}$ to all variables in $\\mathbf{X}$ (evidence) and a real number $q$, the constrained most-probable explanation (CMPE) task seeks to find an assignment $\\mathbf{y}$ to all variables in $\\mathbf{Y}$ such that $f(\\mathbf{x}, \\mathbf{y})$ is maximized and $g(\\mathbf{x}, \\mathbf{y})\\leq q$. In our proposed self-supervised approach, given assignments $\\mathbf{x}$ to $\\mathbf{X}$ (data), we train a deep neural network that learns to output near-optimal solutions to the CMPE problem without requiring access to any pre-computed solutions. The key idea in our approach is to use first principles and approximate inference methods for CMPE to derive novel loss functions that seek to push infeasible solutions towards feasible ones and feasible solutions towards optimal ones. We analyze the properties of our proposed method and experimentally demonstrate its efficacy on several benchmark problems.","sentences":["We propose a self-supervised learning approach for solving the following constrained optimization task in log-linear models or Markov networks.","Let $f$ and $g$ be two log-linear models defined over the sets $\\mathbf{X}$ and $\\mathbf{Y}$ of random variables respectively.","Given an assignment $\\mathbf{x}$ to all variables in $\\mathbf{X}$ (evidence) and a real number $q$, the constrained most-probable explanation (CMPE) task seeks to find an assignment $\\mathbf{y}$ to all variables in $\\mathbf{Y}$ such that $f(\\mathbf{x}, \\mathbf{y})$ is maximized and $g(\\mathbf{x}, \\mathbf{y})\\leq q$. In our proposed self-supervised approach, given assignments $\\mathbf{x}$ to $\\mathbf{X}$ (data), we train a deep neural network that learns to output near-optimal solutions to the CMPE problem without requiring access to any pre-computed solutions.","The key idea in our approach is to use first principles and approximate inference methods for CMPE to derive novel loss functions that seek to push infeasible solutions towards feasible ones and feasible solutions towards optimal ones.","We analyze the properties of our proposed method and experimentally demonstrate its efficacy on several benchmark problems."],"url":"http://arxiv.org/abs/2404.11606v1","category":"cs.LG"}
{"created":"2024-04-17 17:54:49","title":"VG4D: Vision-Language Model Goes 4D Video Recognition","abstract":"Understanding the real world through point cloud video is a crucial aspect of robotics and autonomous driving systems. However, prevailing methods for 4D point cloud recognition have limitations due to sensor resolution, which leads to a lack of detailed information. Recent advances have shown that Vision-Language Models (VLM) pre-trained on web-scale text-image datasets can learn fine-grained visual concepts that can be transferred to various downstream tasks. However, effectively integrating VLM into the domain of 4D point clouds remains an unresolved problem. In this work, we propose the Vision-Language Models Goes 4D (VG4D) framework to transfer VLM knowledge from visual-text pre-trained models to a 4D point cloud network. Our approach involves aligning the 4D encoder's representation with a VLM to learn a shared visual and text space from training on large-scale image-text pairs. By transferring the knowledge of the VLM to the 4D encoder and combining the VLM, our VG4D achieves improved recognition performance. To enhance the 4D encoder, we modernize the classic dynamic point cloud backbone and propose an improved version of PSTNet, im-PSTNet, which can efficiently model point cloud videos. Experiments demonstrate that our method achieves state-of-the-art performance for action recognition on both the NTU RGB+D 60 dataset and the NTU RGB+D 120 dataset. Code is available at \\url{https://github.com/Shark0-0/VG4D}.","sentences":["Understanding the real world through point cloud video is a crucial aspect of robotics and autonomous driving systems.","However, prevailing methods for 4D point cloud recognition have limitations due to sensor resolution, which leads to a lack of detailed information.","Recent advances have shown that Vision-Language Models (VLM) pre-trained on web-scale text-image datasets can learn fine-grained visual concepts that can be transferred to various downstream tasks.","However, effectively integrating VLM into the domain of 4D point clouds remains an unresolved problem.","In this work, we propose the Vision-Language Models Goes 4D (VG4D) framework to transfer VLM knowledge from visual-text pre-trained models to a 4D point cloud network.","Our approach involves aligning the 4D encoder's representation with a VLM to learn a shared visual and text space from training on large-scale image-text pairs.","By transferring the knowledge of the VLM to the 4D encoder and combining the VLM, our VG4D achieves improved recognition performance.","To enhance the 4D encoder, we modernize the classic dynamic point cloud backbone and propose an improved version of PSTNet, im-PSTNet, which can efficiently model point cloud videos.","Experiments demonstrate that our method achieves state-of-the-art performance for action recognition on both the NTU RGB+D 60 dataset and the NTU RGB+D 120 dataset.","Code is available at \\url{https://github.com/Shark0-0/VG4D}."],"url":"http://arxiv.org/abs/2404.11605v1","category":"cs.CV"}
{"created":"2024-04-17 17:49:38","title":"Explainable Artificial Intelligence Techniques for Accurate Fault Detection and Diagnosis: A Review","abstract":"As the manufacturing industry advances with sensor integration and automation, the opaque nature of deep learning models in machine learning poses a significant challenge for fault detection and diagnosis. And despite the related predictive insights Artificial Intelligence (AI) can deliver, advanced machine learning engines often remain a black box. This paper reviews the eXplainable AI (XAI) tools and techniques in this context. We explore various XAI methodologies, focusing on their role in making AI decision-making transparent, particularly in critical scenarios where humans are involved. We also discuss current limitations and potential future research that aims to balance explainability with model performance while improving trustworthiness in the context of AI applications for critical industrial use cases.","sentences":["As the manufacturing industry advances with sensor integration and automation, the opaque nature of deep learning models in machine learning poses a significant challenge for fault detection and diagnosis.","And despite the related predictive insights Artificial Intelligence (AI) can deliver, advanced machine learning engines often remain a black box.","This paper reviews the eXplainable AI (XAI) tools and techniques in this context.","We explore various XAI methodologies, focusing on their role in making AI decision-making transparent, particularly in critical scenarios where humans are involved.","We also discuss current limitations and potential future research that aims to balance explainability with model performance while improving trustworthiness in the context of AI applications for critical industrial use cases."],"url":"http://arxiv.org/abs/2404.11597v1","category":"cs.AI"}
{"created":"2024-04-17 17:38:56","title":"Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept Understanding","abstract":"The rapid evolution of text-to-image diffusion models has opened the door of generative AI, enabling the translation of textual descriptions into visually compelling images with remarkable quality. However, a persistent challenge within this domain is the optimization of prompts to effectively convey abstract concepts into concrete objects. For example, text encoders can hardly express \"peace\", while can easily illustrate olive branches and white doves. This paper introduces a novel approach named Prompt Optimizer for Abstract Concepts (POAC) specifically designed to enhance the performance of text-to-image diffusion models in interpreting and generating images from abstract concepts. We propose a Prompt Language Model (PLM), which is initialized from a pre-trained language model, and then fine-tuned with a curated dataset of abstract concept prompts. The dataset is created with GPT-4 to extend the abstract concept to a scene and concrete objects. Our framework employs a Reinforcement Learning (RL)-based optimization strategy, focusing on the alignment between the generated images by a stable diffusion model and optimized prompts. Through extensive experiments, we demonstrate that our proposed POAC significantly improves the accuracy and aesthetic quality of generated images, particularly in the description of abstract concepts and alignment with optimized prompts. We also present a comprehensive analysis of our model's performance across diffusion models under different settings, showcasing its versatility and effectiveness in enhancing abstract concept representation.","sentences":["The rapid evolution of text-to-image diffusion models has opened the door of generative AI, enabling the translation of textual descriptions into visually compelling images with remarkable quality.","However, a persistent challenge within this domain is the optimization of prompts to effectively convey abstract concepts into concrete objects.","For example, text encoders can hardly express \"peace\", while can easily illustrate olive branches and white doves.","This paper introduces a novel approach named Prompt Optimizer for Abstract Concepts (POAC) specifically designed to enhance the performance of text-to-image diffusion models in interpreting and generating images from abstract concepts.","We propose a Prompt Language Model (PLM), which is initialized from a pre-trained language model, and then fine-tuned with a curated dataset of abstract concept prompts.","The dataset is created with GPT-4 to extend the abstract concept to a scene and concrete objects.","Our framework employs a Reinforcement Learning (RL)-based optimization strategy, focusing on the alignment between the generated images by a stable diffusion model and optimized prompts.","Through extensive experiments, we demonstrate that our proposed POAC significantly improves the accuracy and aesthetic quality of generated images, particularly in the description of abstract concepts and alignment with optimized prompts.","We also present a comprehensive analysis of our model's performance across diffusion models under different settings, showcasing its versatility and effectiveness in enhancing abstract concept representation."],"url":"http://arxiv.org/abs/2404.11589v1","category":"cs.CV"}
{"created":"2024-04-17 17:34:52","title":"Study of Entropy-Driven Polymorphic Stability for Aspirin Using Accurate Neural Network Interatomic Potential","abstract":"In this study, we present a systematic computational investigation to analyze the long debated crystal stability of two well known aspirin polymorphs, labeled as Form I and Form II. Specifically, we developed a strategy to collect training configurations covering diverse interatomic interactions between representative functional groups in the aspirin crystals. Utilizing a state-of-the-art neural network interatomic potential (NNIP) model, we developed an accurate machine learning potential to simulate aspirin crystal dynamics under finite temperature conditions with $\\sim$0.46 kJ/mol/molecule accuracy. Employing the trained NNIP model, we performed thermodynamic integration to assess the free energy difference between aspirin Forms I and II, accounting for the anharmonic effects in a large supercell consisting of 512 molecules. For the first time, our results convincingly demonstrated that Form I is more stable than Form II at 300 K, ranging from 0.74 to 1.83 kJ/mol/molecule, aligning with the experimental observations. Unlike the majority of previous simulations based on (quasi)harmonic approximations in a small super cell, which often found the degenerate energies between aspirin I and II, our findings underscore the importance of anharmonic effects in determining polymorphic stability ranking. Furthermore, we proposed the use of rotational degrees of freedom of methyl and ester/phenyl groups in the aspirin crystal, as characteristic motions to highlight rotational entropic contribution that favors the stability of Form I. Beyond the aspirin polymorphism, we anticipate that such entropy-driven stabilization can be broadly applicable to many other organic systems and thus our approach, suggesting our approach holds a great promise for stability studies in small molecule drug design.","sentences":["In this study, we present a systematic computational investigation to analyze the long debated crystal stability of two well known aspirin polymorphs, labeled as Form I and Form II.","Specifically, we developed a strategy to collect training configurations covering diverse interatomic interactions between representative functional groups in the aspirin crystals.","Utilizing a state-of-the-art neural network interatomic potential (NNIP) model, we developed an accurate machine learning potential to simulate aspirin crystal dynamics under finite temperature conditions with $\\sim$0.46 kJ/mol/molecule accuracy.","Employing the trained NNIP model, we performed thermodynamic integration to assess the free energy difference between aspirin Forms I and II, accounting for the anharmonic effects in a large supercell consisting of 512 molecules.","For the first time, our results convincingly demonstrated that Form I is more stable than Form II at 300 K, ranging from 0.74 to 1.83 kJ/mol/molecule, aligning with the experimental observations.","Unlike the majority of previous simulations based on (quasi)harmonic approximations in a small super cell, which often found the degenerate energies between aspirin I and II, our findings underscore the importance of anharmonic effects in determining polymorphic stability ranking.","Furthermore, we proposed the use of rotational degrees of freedom of methyl and ester/phenyl groups in the aspirin crystal, as characteristic motions to highlight rotational entropic contribution that favors the stability of Form I. Beyond the aspirin polymorphism, we anticipate that such entropy-driven stabilization can be broadly applicable to many other organic systems and thus our approach, suggesting our approach holds a great promise for stability studies in small molecule drug design."],"url":"http://arxiv.org/abs/2404.11587v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-17 17:33:32","title":"Spatial Context-based Self-Supervised Learning for Handwritten Text Recognition","abstract":"Handwritten Text Recognition (HTR) is a relevant problem in computer vision, and implies unique challenges owing to its inherent variability and the rich contextualization required for its interpretation. Despite the success of Self-Supervised Learning (SSL) in computer vision, its application to HTR has been rather scattered, leaving key SSL methodologies unexplored. This work focuses on one of them, namely Spatial Context-based SSL. We investigate how this family of approaches can be adapted and optimized for HTR and propose new workflows that leverage the unique features of handwritten text. Our experiments demonstrate that the methods considered lead to advancements in the state-of-the-art of SSL for HTR in a number of benchmark cases.","sentences":["Handwritten Text Recognition (HTR) is a relevant problem in computer vision, and implies unique challenges owing to its inherent variability and the rich contextualization required for its interpretation.","Despite the success of Self-Supervised Learning (SSL) in computer vision, its application to HTR has been rather scattered, leaving key SSL methodologies unexplored.","This work focuses on one of them, namely Spatial Context-based SSL.","We investigate how this family of approaches can be adapted and optimized for HTR and propose new workflows that leverage the unique features of handwritten text.","Our experiments demonstrate that the methods considered lead to advancements in the state-of-the-art of SSL for HTR in a number of benchmark cases."],"url":"http://arxiv.org/abs/2404.11585v1","category":"cs.AI"}
{"created":"2024-04-17 17:32:41","title":"The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey","abstract":"This survey paper examines the recent advancements in AI agent implementations, with a focus on their ability to achieve complex goals that require enhanced reasoning, planning, and tool execution capabilities. The primary objectives of this work are to a) communicate the current capabilities and limitations of existing AI agent implementations, b) share insights gained from our observations of these systems in action, and c) suggest important considerations for future developments in AI agent design. We achieve this by providing overviews of single-agent and multi-agent architectures, identifying key patterns and divergences in design choices, and evaluating their overall impact on accomplishing a provided goal. Our contribution outlines key themes when selecting an agentic architecture, the impact of leadership on agent systems, agent communication styles, and key phases for planning, execution, and reflection that enable robust AI agent systems.","sentences":["This survey paper examines the recent advancements in AI agent implementations, with a focus on their ability to achieve complex goals that require enhanced reasoning, planning, and tool execution capabilities.","The primary objectives of this work are to a) communicate the current capabilities and limitations of existing AI agent implementations, b) share insights gained from our observations of these systems in action, and c) suggest important considerations for future developments in AI agent design.","We achieve this by providing overviews of single-agent and multi-agent architectures, identifying key patterns and divergences in design choices, and evaluating their overall impact on accomplishing a provided goal.","Our contribution outlines key themes when selecting an agentic architecture, the impact of leadership on agent systems, agent communication styles, and key phases for planning, execution, and reflection that enable robust AI agent systems."],"url":"http://arxiv.org/abs/2404.11584v1","category":"cs.AI"}
{"created":"2024-04-17 17:28:05","title":"LLMTune: Accelerate Database Knob Tuning with Large Language Models","abstract":"Database knob tuning is a critical challenge in the database community, aiming to optimize knob values to enhance database performance for specific workloads. DBMS often feature hundreds of tunable knobs, posing a significant challenge for DBAs to recommend optimal configurations. Consequently, many machine learning-based tuning methods have been developed to automate this process. Despite the introduction of various optimizers, practical applications have unveiled a new problem: they typically require numerous workload runs to achieve satisfactory performance, a process that is both time-consuming and resource-intensive. This inefficiency largely stems from the optimal configuration often being substantially different from the default setting, necessitating multiple iterations during tuning. Recognizing this, we argue that an effective starting point could significantly reduce redundant exploration in less efficient areas, thereby potentially speeding up the tuning process for the optimizers. Based on this assumption, we introduce LLMTune, a large language model-based configuration generator designed to produce an initial, high-quality configuration for new workloads. These generated configurations can then serve as starting points for various base optimizers, accelerating their tuning processes. To obtain training data for LLMTune's supervised fine-tuning, we have devised a new automatic data generation framework capable of efficiently creating a large number of <workload, configuration> pairs. We have conducted thorough experiments to evaluate LLMTune's effectiveness with different workloads, such as TPC-H and JOB. In comparison to leading methods, LLMTune demonstrates a quicker ability to identify superior configurations. For instance, with the challenging TPC-H workload, our LLMTune achieves a significant 15.6x speed-up ratio in finding the best-performing configurations.","sentences":["Database knob tuning is a critical challenge in the database community, aiming to optimize knob values to enhance database performance for specific workloads.","DBMS often feature hundreds of tunable knobs, posing a significant challenge for DBAs to recommend optimal configurations.","Consequently, many machine learning-based tuning methods have been developed to automate this process.","Despite the introduction of various optimizers, practical applications have unveiled a new problem: they typically require numerous workload runs to achieve satisfactory performance, a process that is both time-consuming and resource-intensive.","This inefficiency largely stems from the optimal configuration often being substantially different from the default setting, necessitating multiple iterations during tuning.","Recognizing this, we argue that an effective starting point could significantly reduce redundant exploration in less efficient areas, thereby potentially speeding up the tuning process for the optimizers.","Based on this assumption, we introduce LLMTune, a large language model-based configuration generator designed to produce an initial, high-quality configuration for new workloads.","These generated configurations can then serve as starting points for various base optimizers, accelerating their tuning processes.","To obtain training data for LLMTune's supervised fine-tuning, we have devised a new automatic data generation framework capable of efficiently creating a large number of <workload, configuration> pairs.","We have conducted thorough experiments to evaluate LLMTune's effectiveness with different workloads, such as TPC-H and JOB.","In comparison to leading methods, LLMTune demonstrates a quicker ability to identify superior configurations.","For instance, with the challenging TPC-H workload, our LLMTune achieves a significant 15.6x speed-up ratio in finding the best-performing configurations."],"url":"http://arxiv.org/abs/2404.11581v1","category":"cs.AI"}
{"created":"2024-04-17 17:24:44","title":"Deep Policy Optimization with Temporal Logic Constraints","abstract":"Temporal logics, such as linear temporal logic (LTL), offer a precise means of specifying tasks for (deep) reinforcement learning (RL) agents. In our work, we consider the setting where the task is specified by an LTL objective and there is an additional scalar reward that we need to optimize. Previous works focus either on learning a LTL task-satisfying policy alone or are restricted to finite state spaces. We make two contributions: First, we introduce an RL-friendly approach to this setting by formulating this problem as a single optimization objective. Our formulation guarantees that an optimal policy will be reward-maximal from the set of policies that maximize the likelihood of satisfying the LTL specification. Second, we address a sparsity issue that often arises for LTL-guided Deep RL policies by introducing Cycle Experience Replay (CyclER), a technique that automatically guides RL agents towards the satisfaction of an LTL specification. Our experiments demonstrate the efficacy of CyclER in finding performant deep RL policies in both continuous and discrete experimental domains.","sentences":["Temporal logics, such as linear temporal logic (LTL), offer a precise means of specifying tasks for (deep) reinforcement learning (RL) agents.","In our work, we consider the setting where the task is specified by an LTL objective and there is an additional scalar reward that we need to optimize.","Previous works focus either on learning a LTL task-satisfying policy alone or are restricted to finite state spaces.","We make two contributions: First, we introduce an RL-friendly approach to this setting by formulating this problem as a single optimization objective.","Our formulation guarantees that an optimal policy will be reward-maximal from the set of policies that maximize the likelihood of satisfying the LTL specification.","Second, we address a sparsity issue that often arises for LTL-guided Deep RL policies by introducing Cycle Experience Replay (CyclER), a technique that automatically guides RL agents towards the satisfaction of an LTL specification.","Our experiments demonstrate the efficacy of CyclER in finding performant deep RL policies in both continuous and discrete experimental domains."],"url":"http://arxiv.org/abs/2404.11578v1","category":"cs.LG"}
{"created":"2024-04-17 17:20:27","title":"Towards Reliable Empirical Machine Unlearning Evaluation: A Game-Theoretic View","abstract":"Machine unlearning is the process of updating machine learning models to remove the information of specific training data samples, in order to comply with data protection regulations that allow individuals to request the removal of their personal data. Despite the recent development of numerous unlearning algorithms, reliable evaluation of these algorithms remains an open research question. In this work, we focus on membership inference attack (MIA) based evaluation, one of the most common approaches for evaluating unlearning algorithms, and address various pitfalls of existing evaluation metrics that lack reliability. Specifically, we propose a game-theoretic framework that formalizes the evaluation process as a game between unlearning algorithms and MIA adversaries, measuring the data removal efficacy of unlearning algorithms by the capability of the MIA adversaries. Through careful design of the game, we demonstrate that the natural evaluation metric induced from the game enjoys provable guarantees that the existing evaluation metrics fail to satisfy. Furthermore, we propose a practical and efficient algorithm to estimate the evaluation metric induced from the game, and demonstrate its effectiveness through both theoretical analysis and empirical experiments. This work presents a novel and reliable approach to empirically evaluating unlearning algorithms, paving the way for the development of more effective unlearning techniques.","sentences":["Machine unlearning is the process of updating machine learning models to remove the information of specific training data samples, in order to comply with data protection regulations that allow individuals to request the removal of their personal data.","Despite the recent development of numerous unlearning algorithms, reliable evaluation of these algorithms remains an open research question.","In this work, we focus on membership inference attack (MIA) based evaluation, one of the most common approaches for evaluating unlearning algorithms, and address various pitfalls of existing evaluation metrics that lack reliability.","Specifically, we propose a game-theoretic framework that formalizes the evaluation process as a game between unlearning algorithms and MIA adversaries, measuring the data removal efficacy of unlearning algorithms by the capability of the MIA adversaries.","Through careful design of the game, we demonstrate that the natural evaluation metric induced from the game enjoys provable guarantees that the existing evaluation metrics fail to satisfy.","Furthermore, we propose a practical and efficient algorithm to estimate the evaluation metric induced from the game, and demonstrate its effectiveness through both theoretical analysis and empirical experiments.","This work presents a novel and reliable approach to empirically evaluating unlearning algorithms, paving the way for the development of more effective unlearning techniques."],"url":"http://arxiv.org/abs/2404.11577v1","category":"cs.LG"}
{"created":"2024-04-17 17:11:31","title":"On the Scalability of GNNs for Molecular Graphs","abstract":"Scaling deep learning models has been at the heart of recent revolutions in language modelling and image generation. Practitioners have observed a strong relationship between model size, dataset size, and performance. However, structure-based architectures such as Graph Neural Networks (GNNs) are yet to show the benefits of scale mainly due to the lower efficiency of sparse operations, large data requirements, and lack of clarity about the effectiveness of various architectures. We address this drawback of GNNs by studying their scaling behavior. Specifically, we analyze message-passing networks, graph Transformers, and hybrid architectures on the largest public collection of 2D molecular graphs. For the first time, we observe that GNNs benefit tremendously from the increasing scale of depth, width, number of molecules, number of labels, and the diversity in the pretraining datasets, resulting in a 30.25% improvement when scaling to 1 billion parameters and 28.98% improvement when increasing size of dataset to eightfold. We further demonstrate strong finetuning scaling behavior on 38 tasks, outclassing previous large models. We hope that our work paves the way for an era where foundational GNNs drive pharmaceutical drug discovery.","sentences":["Scaling deep learning models has been at the heart of recent revolutions in language modelling and image generation.","Practitioners have observed a strong relationship between model size, dataset size, and performance.","However, structure-based architectures such as Graph Neural Networks (GNNs) are yet to show the benefits of scale mainly due to the lower efficiency of sparse operations, large data requirements, and lack of clarity about the effectiveness of various architectures.","We address this drawback of GNNs by studying their scaling behavior.","Specifically, we analyze message-passing networks, graph Transformers, and hybrid architectures on the largest public collection of 2D molecular graphs.","For the first time, we observe that GNNs benefit tremendously from the increasing scale of depth, width, number of molecules, number of labels, and the diversity in the pretraining datasets, resulting in a 30.25% improvement when scaling to 1 billion parameters and 28.98% improvement when increasing size of dataset to eightfold.","We further demonstrate strong finetuning scaling behavior on 38 tasks, outclassing previous large models.","We hope that our work paves the way for an era where foundational GNNs drive pharmaceutical drug discovery."],"url":"http://arxiv.org/abs/2404.11568v1","category":"cs.LG"}
{"created":"2024-04-17 17:08:05","title":"MoA: Mixture-of-Attention for Subject-Context Disentanglement in Personalized Image Generation","abstract":"We introduce a new architecture for personalization of text-to-image diffusion models, coined Mixture-of-Attention (MoA). Inspired by the Mixture-of-Experts mechanism utilized in large language models (LLMs), MoA distributes the generation workload between two attention pathways: a personalized branch and a non-personalized prior branch. MoA is designed to retain the original model's prior by fixing its attention layers in the prior branch, while minimally intervening in the generation process with the personalized branch that learns to embed subjects in the layout and context generated by the prior branch. A novel routing mechanism manages the distribution of pixels in each layer across these branches to optimize the blend of personalized and generic content creation. Once trained, MoA facilitates the creation of high-quality, personalized images featuring multiple subjects with compositions and interactions as diverse as those generated by the original model. Crucially, MoA enhances the distinction between the model's pre-existing capability and the newly augmented personalized intervention, thereby offering a more disentangled subject-context control that was previously unattainable. Project page: https://snap-research.github.io/mixture-of-attention","sentences":["We introduce a new architecture for personalization of text-to-image diffusion models, coined Mixture-of-Attention (MoA).","Inspired by the Mixture-of-Experts mechanism utilized in large language models (LLMs), MoA distributes the generation workload between two attention pathways: a personalized branch and a non-personalized prior branch.","MoA is designed to retain the original model's prior by fixing its attention layers in the prior branch, while minimally intervening in the generation process with the personalized branch that learns to embed subjects in the layout and context generated by the prior branch.","A novel routing mechanism manages the distribution of pixels in each layer across these branches to optimize the blend of personalized and generic content creation.","Once trained, MoA facilitates the creation of high-quality, personalized images featuring multiple subjects with compositions and interactions as diverse as those generated by the original model.","Crucially, MoA enhances the distinction between the model's pre-existing capability and the newly augmented personalized intervention, thereby offering a more disentangled subject-context control that was previously unattainable.","Project page: https://snap-research.github.io/mixture-of-attention"],"url":"http://arxiv.org/abs/2404.11565v1","category":"cs.CV"}
{"created":"2024-04-17 16:53:16","title":"Quantifying Multilingual Performance of Large Language Models Across Languages","abstract":"The training process of Large Language Models (LLMs) requires extensive text corpus. However, these data are often unevenly distributed in different languages. As a result, LLMs perform well on common languages, such as English, German, and French, but perform poorly on low-resource languages. However, currently there is no work to quantitatively measure the performance of LLMs in low-resource languages. To fill this gap, we proposed the Language Ranker that aims to benchmark and rank different languages according to the performance of LLMs on those languages. We employ the LLM's performance on the English corpus as a baseline to compare the performances of different languages and English. We have the following three findings: 1. The performance rankings of different LLMs in all languages are roughly the same. 2. LLMs with different sizes have the same partial order of performance. 3. There is a strong correlation between LlaMa2's performance in different languages and the proportion of the pre-training corpus. These findings illustrate that the Language Ranker can be used as an indicator to measure the language performance of LLMs.","sentences":["The training process of Large Language Models (LLMs) requires extensive text corpus.","However, these data are often unevenly distributed in different languages.","As a result, LLMs perform well on common languages, such as English, German, and French, but perform poorly on low-resource languages.","However, currently there is no work to quantitatively measure the performance of LLMs in low-resource languages.","To fill this gap, we proposed the Language Ranker that aims to benchmark and rank different languages according to the performance of LLMs on those languages.","We employ the LLM's performance on the English corpus as a baseline to compare the performances of different languages and English.","We have the following three findings: 1.","The performance rankings of different LLMs in all languages are roughly the same.","2. LLMs with different sizes have the same partial order of performance.","3.","There is a strong correlation between LlaMa2's performance in different languages and the proportion of the pre-training corpus.","These findings illustrate that the Language Ranker can be used as an indicator to measure the language performance of LLMs."],"url":"http://arxiv.org/abs/2404.11553v1","category":"cs.CL"}
{"created":"2024-04-17 16:48:39","title":"Modular resurgent structures","abstract":"The theory of resurgence uniquely associates a factorially divergent formal power series with a collection of non-perturbative, exponential-type corrections paired with a set of complex numbers, known as Stokes constants. When the Borel plane displays a single infinite tower of singularities, the secondary resurgent series are trivial, and the Stokes constants are coefficients of an $L$-function, a rich analytic number-theoretic fabric underlies the resurgent structure of the asymptotic series. We propose a new paradigm of modular resurgence that focuses on the role of the Stokes constants and the interplay of the $q$-series acting as their generating functions with the corresponding $L$-functions. Guided by two pivotal examples arising from topological string theory and the theory of Maass cusp forms, we introduce the notion of modular resurgent series, which we conjecture to have specific summability properties as well as to be closely related to quantum modular forms.","sentences":["The theory of resurgence uniquely associates a factorially divergent formal power series with a collection of non-perturbative, exponential-type corrections paired with a set of complex numbers, known as Stokes constants.","When the Borel plane displays a single infinite tower of singularities, the secondary resurgent series are trivial, and the Stokes constants are coefficients of an $L$-function, a rich analytic number-theoretic fabric underlies the resurgent structure of the asymptotic series.","We propose a new paradigm of modular resurgence that focuses on the role of the Stokes constants and the interplay of the $q$-series acting as their generating functions with the corresponding $L$-functions.","Guided by two pivotal examples arising from topological string theory and the theory of Maass cusp forms, we introduce the notion of modular resurgent series, which we conjecture to have specific summability properties as well as to be closely related to quantum modular forms."],"url":"http://arxiv.org/abs/2404.11550v1","category":"math.NT"}
{"created":"2024-04-17 16:37:33","title":"Ordinal Maximin Guarantees for Group Fair Division","abstract":"We investigate fairness in the allocation of indivisible items among groups of agents using the notion of maximin share (MMS). While previous work has shown that no nontrivial multiplicative MMS approximation can be guaranteed in this setting for general group sizes, we demonstrate that ordinal relaxations are much more useful. For example, we show that if $n$ agents are distributed equally across $g$ groups, there exists a $1$-out-of-$k$ MMS allocation for $k = O(g\\log(n/g))$, while if all but a constant number of agents are in the same group, we obtain $k = O(\\log n/\\log \\log n)$. We also establish the tightness of these bounds and provide non-asymptotic results for the case of two groups.","sentences":["We investigate fairness in the allocation of indivisible items among groups of agents using the notion of maximin share (MMS).","While previous work has shown that no nontrivial multiplicative MMS approximation can be guaranteed in this setting for general group sizes, we demonstrate that ordinal relaxations are much more useful.","For example, we show that if $n$ agents are distributed equally across $g$ groups, there exists a $1$-out-of-$k$ MMS allocation for $k = O(g\\log(n/g))$, while if all but a constant number of agents are in the same group, we obtain $k = O(\\log n/\\log \\log n)$. We also establish the tightness of these bounds and provide non-asymptotic results for the case of two groups."],"url":"http://arxiv.org/abs/2404.11543v1","category":"cs.GT"}
{"created":"2024-04-17 16:30:06","title":"FedPFT: Federated Proxy Fine-Tuning of Foundation Models","abstract":"Adapting Foundation Models (FMs) for downstream tasks through Federated Learning (FL) emerges a promising strategy for protecting data privacy and valuable FMs. Existing methods fine-tune FM by allocating sub-FM to clients in FL, however, leading to suboptimal performance due to insufficient tuning and inevitable error accumulations of gradients. In this paper, we propose Federated Proxy Fine-Tuning (FedPFT), a novel method enhancing FMs adaptation in downstream tasks through FL by two key modules. First, the sub-FM construction module employs a layer-wise compression approach, facilitating comprehensive FM fine-tuning across all layers by emphasizing those crucial neurons. Second, the sub-FM alignment module conducts a two-step distillations-layer-level and neuron-level-before and during FL fine-tuning respectively, to reduce error of gradient by accurately aligning sub-FM with FM under theoretical guarantees. Experimental results on seven commonly used datasets (i.e., four text and three vision) demonstrate the superiority of FedPFT.","sentences":["Adapting Foundation Models (FMs) for downstream tasks through Federated Learning (FL) emerges a promising strategy for protecting data privacy and valuable FMs.","Existing methods fine-tune FM by allocating sub-FM to clients in FL, however, leading to suboptimal performance due to insufficient tuning and inevitable error accumulations of gradients.","In this paper, we propose Federated Proxy Fine-Tuning (FedPFT), a novel method enhancing FMs adaptation in downstream tasks through FL by two key modules.","First, the sub-FM construction module employs a layer-wise compression approach, facilitating comprehensive FM fine-tuning across all layers by emphasizing those crucial neurons.","Second, the sub-FM alignment module conducts a two-step distillations-layer-level and neuron-level-before and during FL fine-tuning respectively, to reduce error of gradient by accurately aligning sub-FM with FM under theoretical guarantees.","Experimental results on seven commonly used datasets (i.e., four text and three vision) demonstrate the superiority of FedPFT."],"url":"http://arxiv.org/abs/2404.11536v1","category":"cs.LG"}
{"created":"2024-04-17 16:28:08","title":"Decomposing and Editing Predictions by Modeling Model Computation","abstract":"How does the internal computation of a machine learning model transform inputs into predictions? In this paper, we introduce a task called component modeling that aims to address this question. The goal of component modeling is to decompose an ML model's prediction in terms of its components -- simple functions (e.g., convolution filters, attention heads) that are the \"building blocks\" of model computation. We focus on a special case of this task, component attribution, where the goal is to estimate the counterfactual impact of individual components on a given prediction. We then present COAR, a scalable algorithm for estimating component attributions; we demonstrate its effectiveness across models, datasets, and modalities. Finally, we show that component attributions estimated with COAR directly enable model editing across five tasks, namely: fixing model errors, ``forgetting'' specific classes, boosting subpopulation robustness, localizing backdoor attacks, and improving robustness to typographic attacks. We provide code for COAR at https://github.com/MadryLab/modelcomponents .","sentences":["How does the internal computation of a machine learning model transform inputs into predictions?","In this paper, we introduce a task called component modeling that aims to address this question.","The goal of component modeling is to decompose an ML model's prediction in terms of its components -- simple functions (e.g., convolution filters, attention heads) that are the \"building blocks\" of model computation.","We focus on a special case of this task, component attribution, where the goal is to estimate the counterfactual impact of individual components on a given prediction.","We then present COAR, a scalable algorithm for estimating component attributions; we demonstrate its effectiveness across models, datasets, and modalities.","Finally, we show that component attributions estimated with COAR directly enable model editing across five tasks, namely: fixing model errors, ``forgetting'' specific classes, boosting subpopulation robustness, localizing backdoor attacks, and improving robustness to typographic attacks.","We provide code for COAR at https://github.com/MadryLab/modelcomponents ."],"url":"http://arxiv.org/abs/2404.11534v1","category":"cs.LG"}
{"created":"2024-04-17 16:14:56","title":"Uncertainty estimation and anomaly detection in chiral effective field theory studies of key nuclear electroweak processes","abstract":"Chiral effective field theory ($\\chi$EFT) is a powerful tool for studying electroweak processes in nuclei. I discuss $\\chi$EFT calculations of three key nuclear electroweak processes: primordial deuterium production, proton-proton fusion, and magnetic dipole excitations of $^{48}\\mathrm{Ca}$. This article showcases $\\chi$EFT's ability to quantify theory uncertainties at the appropriate level of rigor for addressing the different precision demands of these three processes.","sentences":["Chiral effective field theory ($\\chi$EFT) is a powerful tool for studying electroweak processes in nuclei.","I discuss $\\chi$EFT calculations of three key nuclear electroweak processes: primordial deuterium production, proton-proton fusion, and magnetic dipole excitations of $^{48}\\mathrm{Ca}$. This article showcases $\\chi$EFT's ability to quantify theory uncertainties at the appropriate level of rigor for addressing the different precision demands of these three processes."],"url":"http://arxiv.org/abs/2404.11522v1","category":"nucl-th"}
{"created":"2024-04-17 16:07:53","title":"Embedding Privacy in Computational Social Science and Artificial Intelligence Research","abstract":"Privacy is a human right. It ensures that individuals are free to engage in discussions, participate in groups, and form relationships online or offline without fear of their data being inappropriately harvested, analyzed, or otherwise used to harm them. Preserving privacy has emerged as a critical factor in research, particularly in the computational social science (CSS), artificial intelligence (AI) and data science domains, given their reliance on individuals' data for novel insights. The increasing use of advanced computational models stands to exacerbate privacy concerns because, if inappropriately used, they can quickly infringe privacy rights and lead to adverse effects for individuals - especially vulnerable groups - and society. We have already witnessed a host of privacy issues emerge with the advent of large language models (LLMs), such as ChatGPT, which further demonstrate the importance of embedding privacy from the start. This article contributes to the field by discussing the role of privacy and the primary issues that researchers working in CSS, AI, data science and related domains are likely to face. It then presents several key considerations for researchers to ensure participant privacy is best preserved in their research design, data collection and use, analysis, and dissemination of research results.","sentences":["Privacy is a human right.","It ensures that individuals are free to engage in discussions, participate in groups, and form relationships online or offline without fear of their data being inappropriately harvested, analyzed, or otherwise used to harm them.","Preserving privacy has emerged as a critical factor in research, particularly in the computational social science (CSS), artificial intelligence (AI) and data science domains, given their reliance on individuals' data for novel insights.","The increasing use of advanced computational models stands to exacerbate privacy concerns because, if inappropriately used, they can quickly infringe privacy rights and lead to adverse effects for individuals - especially vulnerable groups - and society.","We have already witnessed a host of privacy issues emerge with the advent of large language models (LLMs), such as ChatGPT, which further demonstrate the importance of embedding privacy from the start.","This article contributes to the field by discussing the role of privacy and the primary issues that researchers working in CSS, AI, data science and related domains are likely to face.","It then presents several key considerations for researchers to ensure participant privacy is best preserved in their research design, data collection and use, analysis, and dissemination of research results."],"url":"http://arxiv.org/abs/2404.11515v1","category":"cs.AI"}
{"created":"2024-04-17 15:57:50","title":"Towards Coarse-to-Fine Evaluation of Inference Efficiency for Large Language Models","abstract":"In real world, large language models (LLMs) can serve as the assistant to help users accomplish their jobs, and also support the development of advanced applications. For the wide application of LLMs, the inference efficiency is an essential concern, which has been widely studied in existing work, and numerous optimization algorithms and code libraries have been proposed to improve it. Nonetheless, users still find it challenging to compare the effectiveness of all the above methods and understand the underlying mechanisms. In this work, we perform a detailed coarse-to-fine analysis of the inference performance of various code libraries. To evaluate the overall effectiveness, we examine four usage scenarios within two practical applications. We further provide both theoretical and empirical fine-grained analyses of each module in the Transformer architecture. Our experiments yield comprehensive results that are invaluable for researchers to evaluate code libraries and improve inference strategies.","sentences":["In real world, large language models (LLMs) can serve as the assistant to help users accomplish their jobs, and also support the development of advanced applications.","For the wide application of LLMs, the inference efficiency is an essential concern, which has been widely studied in existing work, and numerous optimization algorithms and code libraries have been proposed to improve it.","Nonetheless, users still find it challenging to compare the effectiveness of all the above methods and understand the underlying mechanisms.","In this work, we perform a detailed coarse-to-fine analysis of the inference performance of various code libraries.","To evaluate the overall effectiveness, we examine four usage scenarios within two practical applications.","We further provide both theoretical and empirical fine-grained analyses of each module in the Transformer architecture.","Our experiments yield comprehensive results that are invaluable for researchers to evaluate code libraries and improve inference strategies."],"url":"http://arxiv.org/abs/2404.11502v1","category":"cs.CL"}
{"created":"2024-04-17 15:53:49","title":"Paraphrase and Solve: Exploring and Exploiting the Impact of Surface Form on Mathematical Reasoning in Large Language Models","abstract":"This paper studies the relationship between the surface form of a mathematical problem and its solvability by large language models. We find that subtle alterations in the surface form can significantly impact the answer distribution and the solve rate, exposing the language model's lack of robustness and sensitivity to the surface form in reasoning through complex problems. To improve mathematical reasoning performance, we propose Self-Consistency-over-Paraphrases (SCoP), which diversifies reasoning paths from specific surface forms of the problem. We evaluate our approach on four mathematics reasoning benchmarks over three large language models and show that SCoP improves mathematical reasoning performance over vanilla self-consistency, particularly for problems initially deemed unsolvable. Finally, we provide additional experiments and discussion regarding problem difficulty and surface forms, including cross-model difficulty agreement and paraphrasing transferability, and Variance of Variations (VOV) for language model evaluation.","sentences":["This paper studies the relationship between the surface form of a mathematical problem and its solvability by large language models.","We find that subtle alterations in the surface form can significantly impact the answer distribution and the solve rate, exposing the language model's lack of robustness and sensitivity to the surface form in reasoning through complex problems.","To improve mathematical reasoning performance, we propose Self-Consistency-over-Paraphrases (SCoP), which diversifies reasoning paths from specific surface forms of the problem.","We evaluate our approach on four mathematics reasoning benchmarks over three large language models and show that SCoP improves mathematical reasoning performance over vanilla self-consistency, particularly for problems initially deemed unsolvable.","Finally, we provide additional experiments and discussion regarding problem difficulty and surface forms, including cross-model difficulty agreement and paraphrasing transferability, and Variance of Variations (VOV) for language model evaluation."],"url":"http://arxiv.org/abs/2404.11500v1","category":"cs.CL"}
{"created":"2024-04-17 15:52:38","title":"A Data-Driven Representation for Sign Language Production","abstract":"Phonetic representations are used when recording spoken languages, but no equivalent exists for recording signed languages. As a result, linguists have proposed several annotation systems that operate on the gloss or sub-unit level; however, these resources are notably irregular and scarce.   Sign Language Production (SLP) aims to automatically translate spoken language sentences into continuous sequences of sign language. However, current state-of-the-art approaches rely on scarce linguistic resources to work. This has limited progress in the field. This paper introduces an innovative solution by transforming the continuous pose generation problem into a discrete sequence generation problem. Thus, overcoming the need for costly annotation. Although, if available, we leverage the additional information to enhance our approach.   By applying Vector Quantisation (VQ) to sign language data, we first learn a codebook of short motions that can be combined to create a natural sequence of sign. Where each token in the codebook can be thought of as the lexicon of our representation. Then using a transformer we perform a translation from spoken language text to a sequence of codebook tokens. Each token can be directly mapped to a sequence of poses allowing the translation to be performed by a single network. Furthermore, we present a sign stitching method to effectively join tokens together. We evaluate on the RWTH-PHOENIX-Weather-2014T (PHOENIX14T) and the more challenging Meine DGS Annotated (mDGS) datasets. An extensive evaluation shows our approach outperforms previous methods, increasing the BLEU-1 back translation score by up to 72%.","sentences":["Phonetic representations are used when recording spoken languages, but no equivalent exists for recording signed languages.","As a result, linguists have proposed several annotation systems that operate on the gloss or sub-unit level; however, these resources are notably irregular and scarce.   ","Sign Language Production (SLP) aims to automatically translate spoken language sentences into continuous sequences of sign language.","However, current state-of-the-art approaches rely on scarce linguistic resources to work.","This has limited progress in the field.","This paper introduces an innovative solution by transforming the continuous pose generation problem into a discrete sequence generation problem.","Thus, overcoming the need for costly annotation.","Although, if available, we leverage the additional information to enhance our approach.   ","By applying Vector Quantisation (VQ) to sign language data, we first learn a codebook of short motions that can be combined to create a natural sequence of sign.","Where each token in the codebook can be thought of as the lexicon of our representation.","Then using a transformer we perform a translation from spoken language text to a sequence of codebook tokens.","Each token can be directly mapped to a sequence of poses allowing the translation to be performed by a single network.","Furthermore, we present a sign stitching method to effectively join tokens together.","We evaluate on the RWTH-PHOENIX-Weather-2014T (PHOENIX14T) and the more challenging Meine DGS Annotated (mDGS) datasets.","An extensive evaluation shows our approach outperforms previous methods, increasing the BLEU-1 back translation score by up to 72%."],"url":"http://arxiv.org/abs/2404.11499v1","category":"cs.CL"}
{"created":"2024-04-17 15:51:15","title":"Runtime Analysis of Evolutionary Diversity Optimization on the Multi-objective (LeadingOnes, TrailingZeros) Problem","abstract":"The diversity optimization is the class of optimization problems, in which we aim at finding a diverse set of good solutions. One of the frequently used approaches to solve such problems is to use evolutionary algorithms which evolve a desired diverse population. This approach is called evolutionary diversity optimization (EDO).   In this paper, we analyse EDO on a 3-objective function LOTZ$_k$, which is a modification of the 2-objective benchmark function (LeadingOnes, TrailingZeros). We prove that the GSEMO computes a set of all Pareto-optimal solutions in $O(kn^3)$ expected iterations. We also analyze the runtime of the GSEMO$_D$ (a modification of the GSEMO for diversity optimization) until it finds a population with the best possible diversity for two different diversity measures, the total imbalance and the sorted imbalances vector. For the first measure we show that the GSEMO$_D$ optimizes it asymptotically faster than it finds a Pareto-optimal population, in $O(kn^2\\log(n))$ expected iterations, and for the second measure we show an upper bound of $O(k^2n^3\\log(n))$ expected iterations. We complement our theoretical analysis with an empirical study, which shows a very similar behavior for both diversity measures that is close to the theory predictions.","sentences":["The diversity optimization is the class of optimization problems, in which we aim at finding a diverse set of good solutions.","One of the frequently used approaches to solve such problems is to use evolutionary algorithms which evolve a desired diverse population.","This approach is called evolutionary diversity optimization (EDO).   ","In this paper, we analyse EDO on a 3-objective function LOTZ$_k$, which is a modification of the 2-objective benchmark function (LeadingOnes, TrailingZeros).","We prove that the GSEMO computes a set of all Pareto-optimal solutions in $O(kn^3)$ expected iterations.","We also analyze the runtime of the GSEMO$_D$ (a modification of the GSEMO for diversity optimization) until it finds a population with the best possible diversity for two different diversity measures, the total imbalance and the sorted imbalances vector.","For the first measure we show that the GSEMO$_D$ optimizes it asymptotically faster than it finds a Pareto-optimal population, in $O(kn^2\\log(n))$ expected iterations, and for the second measure we show an upper bound of $O(k^2n^3\\log(n))$ expected iterations.","We complement our theoretical analysis with an empirical study, which shows a very similar behavior for both diversity measures that is close to the theory predictions."],"url":"http://arxiv.org/abs/2404.11496v1","category":"cs.NE"}
{"created":"2024-04-17 15:47:26","title":"arcjetCV: an open-source software to analyze material ablation","abstract":"arcjetCV is an open-source Python software designed to automate time-resolved measurements of heatshield material recession and recession rates from arcjet test video footage. This new automated and accessible capability greatly exceeds previous manual extraction methods, enabling rapid and detailed characterization of material recession for any sample with a profile video. arcjetCV automates the video segmentation process using machine learning models, including a one-dimensional (1D) Convolutional Neural Network (CNN) to infer the time-window of interest, a two-dimensional (2D) CNN for image and edge segmentation, and a Local Outlier Factor (LOF) for outlier filtering. A graphical user interface (GUI) simplifies the user experience and an application programming interface (API) allows users to call the core functions from scripts, enabling video batch processing. arcjetCV's capability to measure time-resolved recession in turn enables characterization of non-linear processes (shrinkage, swelling, melt flows, etc.), contributing to higher fidelity validation and improved modeling of heatshield material performance. The source code associated with this article can be found at https://github.com/magnus-haw/arcjetCV.","sentences":["arcjetCV is an open-source Python software designed to automate time-resolved measurements of heatshield material recession and recession rates from arcjet test video footage.","This new automated and accessible capability greatly exceeds previous manual extraction methods, enabling rapid and detailed characterization of material recession for any sample with a profile video.","arcjetCV automates the video segmentation process using machine learning models, including a one-dimensional (1D) Convolutional Neural Network (CNN) to infer the time-window of interest, a two-dimensional (2D) CNN for image and edge segmentation, and a Local Outlier Factor (LOF) for outlier filtering.","A graphical user interface (GUI) simplifies the user experience and an application programming interface (API) allows users to call the core functions from scripts, enabling video batch processing.","arcjetCV's capability to measure time-resolved recession in turn enables characterization of non-linear processes (shrinkage, swelling, melt flows, etc.), contributing to higher fidelity validation and improved modeling of heatshield material performance.","The source code associated with this article can be found at https://github.com/magnus-haw/arcjetCV."],"url":"http://arxiv.org/abs/2404.11492v1","category":"cs.CV"}
{"created":"2024-04-17 15:45:49","title":"Multi-resolution Rescored ByteTrack for Video Object Detection on Ultra-low-power Embedded Systems","abstract":"This paper introduces Multi-Resolution Rescored Byte-Track (MR2-ByteTrack), a novel video object detection framework for ultra-low-power embedded processors. This method reduces the average compute load of an off-the-shelf Deep Neural Network (DNN) based object detector by up to 2.25$\\times$ by alternating the processing of high-resolution images (320$\\times$320 pixels) with multiple down-sized frames (192$\\times$192 pixels). To tackle the accuracy degradation due to the reduced image input size, MR2-ByteTrack correlates the output detections over time using the ByteTrack tracker and corrects potential misclassification using a novel probabilistic Rescore algorithm. By interleaving two down-sized images for every high-resolution one as the input of different state-of-the-art DNN object detectors with our MR2-ByteTrack, we demonstrate an average accuracy increase of 2.16% and a latency reduction of 43% on the GAP9 microcontroller compared to a baseline frame-by-frame inference scheme using exclusively full-resolution images. Code available at: https://github.com/Bomps4/Multi_Resolution_Rescored_ByteTrack","sentences":["This paper introduces Multi-Resolution Rescored Byte-Track (MR2-ByteTrack), a novel video object detection framework for ultra-low-power embedded processors.","This method reduces the average compute load of an off-the-shelf Deep Neural Network (DNN) based object detector by up to 2.25$\\times$ by alternating the processing of high-resolution images (320$\\times$320 pixels) with multiple down-sized frames (192$\\times$192 pixels).","To tackle the accuracy degradation due to the reduced image input size, MR2-ByteTrack correlates the output detections over time using the ByteTrack tracker and corrects potential misclassification using a novel probabilistic Rescore algorithm.","By interleaving two down-sized images for every high-resolution one as the input of different state-of-the-art DNN object detectors with our MR2-ByteTrack, we demonstrate an average accuracy increase of 2.16% and a latency reduction of 43% on the GAP9 microcontroller compared to a baseline frame-by-frame inference scheme using exclusively full-resolution images.","Code available at: https://github.com/Bomps4/Multi_Resolution_Rescored_ByteTrack"],"url":"http://arxiv.org/abs/2404.11488v1","category":"cs.CV"}
{"created":"2024-04-17 15:40:45","title":"AgentKit: Flow Engineering with Graphs, not Coding","abstract":"We propose an intuitive LLM prompting framework (AgentKit) for multifunctional agents. AgentKit offers a unified framework for explicitly constructing a complex \"thought process\" from simple natural language prompts. The basic building block in AgentKit is a node, containing a natural language prompt for a specific subtask. The user then puts together chains of nodes, like stacking LEGO pieces. The chains of nodes can be designed to explicitly enforce a naturally structured \"thought process\". For example, for the task of writing a paper, one may start with the thought process of 1) identify a core message, 2) identify prior research gaps, etc. The nodes in AgentKit can be designed and combined in different ways to implement multiple advanced capabilities including on-the-fly hierarchical planning, reflection, and learning from interactions. In addition, due to the modular nature and the intuitive design to simulate explicit human thought process, a basic agent could be implemented as simple as a list of prompts for the subtasks and therefore could be designed and tuned by someone without any programming experience. Quantitatively, we show that agents designed through AgentKit achieve SOTA performance on WebShop and Crafter. These advances underscore AgentKit's potential in making LLM agents effective and accessible for a wider range of applications. https://github.com/holmeswww/AgentKit","sentences":["We propose an intuitive LLM prompting framework (AgentKit) for multifunctional agents.","AgentKit offers a unified framework for explicitly constructing a complex \"thought process\" from simple natural language prompts.","The basic building block in AgentKit is a node, containing a natural language prompt for a specific subtask.","The user then puts together chains of nodes, like stacking LEGO pieces.","The chains of nodes can be designed to explicitly enforce a naturally structured \"thought process\".","For example, for the task of writing a paper, one may start with the thought process of 1) identify a core message, 2) identify prior research gaps, etc.","The nodes in AgentKit can be designed and combined in different ways to implement multiple advanced capabilities including on-the-fly hierarchical planning, reflection, and learning from interactions.","In addition, due to the modular nature and the intuitive design to simulate explicit human thought process, a basic agent could be implemented as simple as a list of prompts for the subtasks and therefore could be designed and tuned by someone without any programming experience.","Quantitatively, we show that agents designed through AgentKit achieve SOTA performance on WebShop and Crafter.","These advances underscore AgentKit's potential in making LLM agents effective and accessible for a wider range of applications.","https://github.com/holmeswww/AgentKit"],"url":"http://arxiv.org/abs/2404.11483v1","category":"cs.AI"}
{"created":"2024-04-17 15:32:58","title":"Discovering Nuclear Models from Symbolic Machine Learning","abstract":"Numerous phenomenological nuclear models have been proposed to describe specific observables within different regions of the nuclear chart. However, developing a unified model that describes the complex behavior of all nuclei remains an open challenge. Here, we explore whether novel symbolic Machine Learning (ML) can rediscover traditional nuclear physics models or identify alternatives with improved simplicity, fidelity, and predictive power. To address this challenge, we developed a Multi-objective Iterated Symbolic Regression approach that handles symbolic regressions over multiple target observables, accounts for experimental uncertainties and is robust against high-dimensional problems. As a proof of principle, we applied this method to describe the nuclear binding energies and charge radii of light and medium mass nuclei. Our approach identified simple analytical relationships based on the number of protons and neutrons, providing interpretable models with precision comparable to state-of-the-art nuclear models. Additionally, we integrated this ML-discovered model with an existing complementary model to estimate the limits of nuclear stability. These results highlight the potential of symbolic ML to develop accurate nuclear models and guide our description of complex many-body problems.","sentences":["Numerous phenomenological nuclear models have been proposed to describe specific observables within different regions of the nuclear chart.","However, developing a unified model that describes the complex behavior of all nuclei remains an open challenge.","Here, we explore whether novel symbolic Machine Learning (ML) can rediscover traditional nuclear physics models or identify alternatives with improved simplicity, fidelity, and predictive power.","To address this challenge, we developed a Multi-objective Iterated Symbolic Regression approach that handles symbolic regressions over multiple target observables, accounts for experimental uncertainties and is robust against high-dimensional problems.","As a proof of principle, we applied this method to describe the nuclear binding energies and charge radii of light and medium mass nuclei.","Our approach identified simple analytical relationships based on the number of protons and neutrons, providing interpretable models with precision comparable to state-of-the-art nuclear models.","Additionally, we integrated this ML-discovered model with an existing complementary model to estimate the limits of nuclear stability.","These results highlight the potential of symbolic ML to develop accurate nuclear models and guide our description of complex many-body problems."],"url":"http://arxiv.org/abs/2404.11477v1","category":"nucl-th"}
{"created":"2024-04-17 15:32:56","title":"Taxonomy to Regulation: A (Geo)Political Taxonomy for AI Risks and Regulatory Measures in the EU AI Act","abstract":"Technological innovations have shown remarkable capabilities to benefit and harm society alike. AI constitutes a democratized sophisticated technology accessible to large parts of society, including malicious actors. This work proposes a taxonomy focusing on on (geo)political risks associated with AI. It identifies 12 risks in total divided into four categories: (1) Geopolitical Pressures, (2) Malicious Usage, (3) Environmental, Social, and Ethical Risks, and (4) Privacy and Trust Violations. Incorporating a regulatory side, this paper conducts a policy assessment of the EU AI Act. Adopted in March 2023, the landmark regulation has the potential to have a positive top-down impact concerning AI risk reduction but needs regulatory adjustments to mitigate risks more comprehensively. Regulatory exceptions for open-source models, excessively high parameters for the classification of GPAI models as a systemic risk, and the exclusion of systems designed exclusively for military purposes from the regulation's obligations leave room for future action.","sentences":["Technological innovations have shown remarkable capabilities to benefit and harm society alike.","AI constitutes a democratized sophisticated technology accessible to large parts of society, including malicious actors.","This work proposes a taxonomy focusing on on (geo)political risks associated with AI.","It identifies 12 risks in total divided into four categories: (1) Geopolitical Pressures, (2) Malicious Usage, (3) Environmental, Social, and Ethical Risks, and (4) Privacy and Trust Violations.","Incorporating a regulatory side, this paper conducts a policy assessment of the EU AI Act.","Adopted in March 2023, the landmark regulation has the potential to have a positive top-down impact concerning AI risk reduction but needs regulatory adjustments to mitigate risks more comprehensively.","Regulatory exceptions for open-source models, excessively high parameters for the classification of GPAI models as a systemic risk, and the exclusion of systems designed exclusively for military purposes from the regulation's obligations leave room for future action."],"url":"http://arxiv.org/abs/2404.11476v1","category":"cs.AI"}
{"created":"2024-04-17 15:31:06","title":"AdaIR: Exploiting Underlying Similarities of Image Restoration Tasks with Adapters","abstract":"Existing image restoration approaches typically employ extensive networks specifically trained for designated degradations. Despite being effective, such methods inevitably entail considerable storage costs and computational overheads due to the reliance on task-specific networks. In this work, we go beyond this well-established framework and exploit the inherent commonalities among image restoration tasks. The primary objective is to identify components that are shareable across restoration tasks and augment the shared components with modules specifically trained for individual tasks. Towards this goal, we propose AdaIR, a novel framework that enables low storage cost and efficient training without sacrificing performance. Specifically, a generic restoration network is first constructed through self-supervised pre-training using synthetic degradations. Subsequent to the pre-training phase, adapters are trained to adapt the pre-trained network to specific degradations. AdaIR requires solely the training of lightweight, task-specific modules, ensuring a more efficient storage and training regimen. We have conducted extensive experiments to validate the effectiveness of AdaIR and analyze the influence of the pre-training strategy on discovering shareable components. Extensive experimental results show that AdaIR achieves outstanding results on multi-task restoration while utilizing significantly fewer parameters (1.9 MB) and less training time (7 hours) for each restoration task. The source codes and trained models will be released.","sentences":["Existing image restoration approaches typically employ extensive networks specifically trained for designated degradations.","Despite being effective, such methods inevitably entail considerable storage costs and computational overheads due to the reliance on task-specific networks.","In this work, we go beyond this well-established framework and exploit the inherent commonalities among image restoration tasks.","The primary objective is to identify components that are shareable across restoration tasks and augment the shared components with modules specifically trained for individual tasks.","Towards this goal, we propose AdaIR, a novel framework that enables low storage cost and efficient training without sacrificing performance.","Specifically, a generic restoration network is first constructed through self-supervised pre-training using synthetic degradations.","Subsequent to the pre-training phase, adapters are trained to adapt the pre-trained network to specific degradations.","AdaIR requires solely the training of lightweight, task-specific modules, ensuring a more efficient storage and training regimen.","We have conducted extensive experiments to validate the effectiveness of AdaIR and analyze the influence of the pre-training strategy on discovering shareable components.","Extensive experimental results show that AdaIR achieves outstanding results on multi-task restoration while utilizing significantly fewer parameters (1.9 MB) and less training time (7 hours) for each restoration task.","The source codes and trained models will be released."],"url":"http://arxiv.org/abs/2404.11475v1","category":"cs.CV"}
{"created":"2024-04-17 15:28:53","title":"Towards Highly Realistic Artistic Style Transfer via Stable Diffusion with Step-aware and Layer-aware Prompt","abstract":"Artistic style transfer aims to transfer the learned artistic style onto an arbitrary content image, generating artistic stylized images. Existing generative adversarial network-based methods fail to generate highly realistic stylized images and always introduce obvious artifacts and disharmonious patterns. Recently, large-scale pre-trained diffusion models opened up a new way for generating highly realistic artistic stylized images. However, diffusion model-based methods generally fail to preserve the content structure of input content images well, introducing some undesired content structure and style patterns. To address the above problems, we propose a novel pre-trained diffusion-based artistic style transfer method, called LSAST, which can generate highly realistic artistic stylized images while preserving the content structure of input content images well, without bringing obvious artifacts and disharmonious style patterns. Specifically, we introduce a Step-aware and Layer-aware Prompt Space, a set of learnable prompts, which can learn the style information from the collection of artworks and dynamically adjusts the input images' content structure and style pattern. To train our prompt space, we propose a novel inversion method, called Step-ware and Layer-aware Prompt Inversion, which allows the prompt space to learn the style information of the artworks collection. In addition, we inject a pre-trained conditional branch of ControlNet into our LSAST, which further improved our framework's ability to maintain content structure. Extensive experiments demonstrate that our proposed method can generate more highly realistic artistic stylized images than the state-of-the-art artistic style transfer methods.","sentences":["Artistic style transfer aims to transfer the learned artistic style onto an arbitrary content image, generating artistic stylized images.","Existing generative adversarial network-based methods fail to generate highly realistic stylized images and always introduce obvious artifacts and disharmonious patterns.","Recently, large-scale pre-trained diffusion models opened up a new way for generating highly realistic artistic stylized images.","However, diffusion model-based methods generally fail to preserve the content structure of input content images well, introducing some undesired content structure and style patterns.","To address the above problems, we propose a novel pre-trained diffusion-based artistic style transfer method, called LSAST, which can generate highly realistic artistic stylized images while preserving the content structure of input content images well, without bringing obvious artifacts and disharmonious style patterns.","Specifically, we introduce a Step-aware and Layer-aware Prompt Space, a set of learnable prompts, which can learn the style information from the collection of artworks and dynamically adjusts the input images' content structure and style pattern.","To train our prompt space, we propose a novel inversion method, called Step-ware and Layer-aware Prompt Inversion, which allows the prompt space to learn the style information of the artworks collection.","In addition, we inject a pre-trained conditional branch of ControlNet into our LSAST, which further improved our framework's ability to maintain content structure.","Extensive experiments demonstrate that our proposed method can generate more highly realistic artistic stylized images than the state-of-the-art artistic style transfer methods."],"url":"http://arxiv.org/abs/2404.11474v1","category":"cs.CV"}
{"created":"2024-04-17 15:17:57","title":"Designing Touchscreen Menu Interfaces for In-Vehicle Infotainment Systems: the Effect of Depth and Breadth Trade-off and Task Types on Visual-Manual Distraction","abstract":"Multitasking with a touch screen user-interface while driving is known to impact negatively driving performance and safety. Literature shows that list scrolling interfaces generate more visual-manual distraction than structured menus and sequential navigation. Depth and breadth trade-offs for structured navigation have been studied. However, little is known on how secondary task characteristics interact with those trade-offs. In this study, we make the hypothesis that both menu's depth and task complexity interact in generating visual-manual distraction. Using a driving simulation setup, we collected telemetry and eye-tracking data to evaluate driving performance. Participants were multitasking with a mobile app, presenting a range of eight depth and breadth trade-offs under three types of secondary tasks, involving different cognitive operations (Systematic reading, Search for an item, Memorize items' state). The results confirm our hypothesis. Systematic interaction with menu items generated a visual demand that increased with menu's depth, while visual demand reach an optimum for Search and Memory tasks. We discuss implications for design: In a multitasking context, display design effectiveness must be assessed while considering menu's layout but also cognitive processes involved.","sentences":["Multitasking with a touch screen user-interface while driving is known to impact negatively driving performance and safety.","Literature shows that list scrolling interfaces generate more visual-manual distraction than structured menus and sequential navigation.","Depth and breadth trade-offs for structured navigation have been studied.","However, little is known on how secondary task characteristics interact with those trade-offs.","In this study, we make the hypothesis that both menu's depth and task complexity interact in generating visual-manual distraction.","Using a driving simulation setup, we collected telemetry and eye-tracking data to evaluate driving performance.","Participants were multitasking with a mobile app, presenting a range of eight depth and breadth trade-offs under three types of secondary tasks, involving different cognitive operations (Systematic reading, Search for an item, Memorize items' state).","The results confirm our hypothesis.","Systematic interaction with menu items generated a visual demand that increased with menu's depth, while visual demand reach an optimum for Search and Memory tasks.","We discuss implications for design: In a multitasking context, display design effectiveness must be assessed while considering menu's layout but also cognitive processes involved."],"url":"http://arxiv.org/abs/2404.11469v1","category":"cs.HC"}
{"created":"2024-04-17 15:16:01","title":"A Large-scale Fine-grained Analysis of Packages in Open-Source Software Ecosystems","abstract":"Package managers such as NPM, Maven, and PyPI play a pivotal role in open-source software (OSS) ecosystems, streamlining the distribution and management of various freely available packages. The fine-grained details within software packages can unveil potential risks within existing OSS ecosystems, offering valuable insights for detecting malicious packages. In this study, we undertake a large-scale empirical analysis focusing on fine-grained information (FGI): the metadata, static, and dynamic functions. Specifically, we investigate the FGI usage across a diverse set of 50,000+ legitimate and 1,000+ malicious packages. Based on this diverse data collection, we conducted a comparative analysis between legitimate and malicious packages. Our findings reveal that (1) malicious packages have less metadata content and utilize fewer static and dynamic functions than legitimate ones; (2) malicious packages demonstrate a higher tendency to invoke HTTP/URL functions as opposed to other application services, such as FTP or SMTP; (3) FGI serves as a distinguishable indicator between legitimate and malicious packages; and (4) one dimension in FGI has sufficient distinguishable capability to detect malicious packages, and combining all dimensions in FGI cannot significantly improve overall performance.","sentences":["Package managers such as NPM, Maven, and PyPI play a pivotal role in open-source software (OSS) ecosystems, streamlining the distribution and management of various freely available packages.","The fine-grained details within software packages can unveil potential risks within existing OSS ecosystems, offering valuable insights for detecting malicious packages.","In this study, we undertake a large-scale empirical analysis focusing on fine-grained information (FGI): the metadata, static, and dynamic functions.","Specifically, we investigate the FGI usage across a diverse set of 50,000+ legitimate and 1,000+ malicious packages.","Based on this diverse data collection, we conducted a comparative analysis between legitimate and malicious packages.","Our findings reveal that (1) malicious packages have less metadata content and utilize fewer static and dynamic functions than legitimate ones; (2) malicious packages demonstrate a higher tendency to invoke HTTP/URL functions as opposed to other application services, such as FTP or SMTP; (3) FGI serves as a distinguishable indicator between legitimate and malicious packages; and (4) one dimension in FGI has sufficient distinguishable capability to detect malicious packages, and combining all dimensions in FGI cannot significantly improve overall performance."],"url":"http://arxiv.org/abs/2404.11467v1","category":"cs.SE"}
{"created":"2024-04-17 15:09:31","title":"Using Game Engines and Machine Learning to Create Synthetic Satellite Imagery for a Tabletop Verification Exercise","abstract":"Satellite imagery is regarded as a great opportunity for citizen-based monitoring of activities of interest. Relevant imagery may however not be available at sufficiently high resolution, quality, or cadence -- let alone be uniformly accessible to open-source analysts. This limits an assessment of the true long-term potential of citizen-based monitoring of nuclear activities using publicly available satellite imagery. In this article, we demonstrate how modern game engines combined with advanced machine-learning techniques can be used to generate synthetic imagery of sites of interest with the ability to choose relevant parameters upon request; these include time of day, cloud cover, season, or level of activity onsite. At the same time, resolution and off-nadir angle can be adjusted to simulate different characteristics of the satellite. While there are several possible use-cases for synthetic imagery, here we focus on its usefulness to support tabletop exercises in which simple monitoring scenarios can be examined to better understand verification capabilities enabled by new satellite constellations and very short revisit times.","sentences":["Satellite imagery is regarded as a great opportunity for citizen-based monitoring of activities of interest.","Relevant imagery may however not be available at sufficiently high resolution, quality, or cadence -- let alone be uniformly accessible to open-source analysts.","This limits an assessment of the true long-term potential of citizen-based monitoring of nuclear activities using publicly available satellite imagery.","In this article, we demonstrate how modern game engines combined with advanced machine-learning techniques can be used to generate synthetic imagery of sites of interest with the ability to choose relevant parameters upon request; these include time of day, cloud cover, season, or level of activity onsite.","At the same time, resolution and off-nadir angle can be adjusted to simulate different characteristics of the satellite.","While there are several possible use-cases for synthetic imagery, here we focus on its usefulness to support tabletop exercises in which simple monitoring scenarios can be examined to better understand verification capabilities enabled by new satellite constellations and very short revisit times."],"url":"http://arxiv.org/abs/2404.11461v1","category":"cs.CV"}
{"created":"2024-04-17 15:05:51","title":"Learn to Tour: Operator Design For Solution Feasibility Mapping in Pickup-and-delivery Traveling Salesman Problem","abstract":"This paper aims to develop a learning method for a special class of traveling salesman problems (TSP), namely, the pickup-and-delivery TSP (PDTSP), which finds the shortest tour along a sequence of one-to-one pickup-and-delivery nodes. One-to-one here means that the transported people or goods are associated with designated pairs of pickup and delivery nodes, in contrast to that indistinguishable goods can be delivered to any nodes. In PDTSP, precedence constraints need to be satisfied that each pickup node must be visited before its corresponding delivery node. Classic operations research (OR) algorithms for PDTSP are difficult to scale to large-sized problems. Recently, reinforcement learning (RL) has been applied to TSPs. The basic idea is to explore and evaluate visiting sequences in a solution space. However, this approach could be less computationally efficient, as it has to potentially evaluate many infeasible solutions of which precedence constraints are violated. To restrict solution search within a feasible space, we utilize operators that always map one feasible solution to another, without spending time exploring the infeasible solution space. Such operators are evaluated and selected as policies to solve PDTSPs in an RL framework. We make a comparison of our method and baselines, including classic OR algorithms and existing learning methods. Results show that our approach can find tours shorter than baselines.","sentences":["This paper aims to develop a learning method for a special class of traveling salesman problems (TSP), namely, the pickup-and-delivery TSP (PDTSP), which finds the shortest tour along a sequence of one-to-one pickup-and-delivery nodes.","One-to-one here means that the transported people or goods are associated with designated pairs of pickup and delivery nodes, in contrast to that indistinguishable goods can be delivered to any nodes.","In PDTSP, precedence constraints need to be satisfied that each pickup node must be visited before its corresponding delivery node.","Classic operations research (OR) algorithms for PDTSP are difficult to scale to large-sized problems.","Recently, reinforcement learning (RL) has been applied to TSPs.","The basic idea is to explore and evaluate visiting sequences in a solution space.","However, this approach could be less computationally efficient, as it has to potentially evaluate many infeasible solutions of which precedence constraints are violated.","To restrict solution search within a feasible space, we utilize operators that always map one feasible solution to another, without spending time exploring the infeasible solution space.","Such operators are evaluated and selected as policies to solve PDTSPs in an RL framework.","We make a comparison of our method and baselines, including classic OR algorithms and existing learning methods.","Results show that our approach can find tours shorter than baselines."],"url":"http://arxiv.org/abs/2404.11458v1","category":"cs.AI"}
{"created":"2024-04-17 15:05:03","title":"Unifying Bias and Unfairness in Information Retrieval: A Survey of Challenges and Opportunities with Large Language Models","abstract":"With the rapid advancement of large language models (LLMs), information retrieval (IR) systems, such as search engines and recommender systems, have undergone a significant paradigm shift. This evolution, while heralding new opportunities, introduces emerging challenges, particularly in terms of biases and unfairness, which may threaten the information ecosystem. In this paper, we present a comprehensive survey of existing works on emerging and pressing bias and unfairness issues in IR systems when the integration of LLMs. We first unify bias and unfairness issues as distribution mismatch problems, providing a groundwork for categorizing various mitigation strategies through distribution alignment. Subsequently, we systematically delve into the specific bias and unfairness issues arising from three critical stages of LLMs integration into IR systems: data collection, model development, and result evaluation. In doing so, we meticulously review and analyze recent literature, focusing on the definitions, characteristics, and corresponding mitigation strategies associated with these issues. Finally, we identify and highlight some open problems and challenges for future work, aiming to inspire researchers and stakeholders in the IR field and beyond to better understand and mitigate bias and unfairness issues of IR in this LLM era. We also consistently maintain a GitHub repository for the relevant papers and resources in this rising direction at https://github.com/KID-22/LLM-IR-Bias-Fairness-Survey.","sentences":["With the rapid advancement of large language models (LLMs), information retrieval (IR) systems, such as search engines and recommender systems, have undergone a significant paradigm shift.","This evolution, while heralding new opportunities, introduces emerging challenges, particularly in terms of biases and unfairness, which may threaten the information ecosystem.","In this paper, we present a comprehensive survey of existing works on emerging and pressing bias and unfairness issues in IR systems when the integration of LLMs.","We first unify bias and unfairness issues as distribution mismatch problems, providing a groundwork for categorizing various mitigation strategies through distribution alignment.","Subsequently, we systematically delve into the specific bias and unfairness issues arising from three critical stages of LLMs integration into IR systems: data collection, model development, and result evaluation.","In doing so, we meticulously review and analyze recent literature, focusing on the definitions, characteristics, and corresponding mitigation strategies associated with these issues.","Finally, we identify and highlight some open problems and challenges for future work, aiming to inspire researchers and stakeholders in the IR field and beyond to better understand and mitigate bias and unfairness issues of IR in this LLM era.","We also consistently maintain a GitHub repository for the relevant papers and resources in this rising direction at https://github.com/KID-22/LLM-IR-Bias-Fairness-Survey."],"url":"http://arxiv.org/abs/2404.11457v1","category":"cs.IR"}
{"created":"2024-04-17 14:55:49","title":"Real-Time Trajectory Synthesis with Local Differential Privacy","abstract":"Trajectory streams are being generated from location-aware devices, such as smartphones and in-vehicle navigation systems. Due to the sensitive nature of the location data, directly sharing user trajectories suffers from privacy leakage issues. Local differential privacy (LDP), which perturbs sensitive data on the user side before it is shared or analyzed, emerges as a promising solution for private trajectory stream collection and analysis. Unfortunately, existing stream release approaches often neglect the rich spatial-temporal context information within trajectory streams, resulting in suboptimal utility and limited types of downstream applications. To this end, we propose RetraSyn, a novel real-time trajectory synthesis framework, which is able to perform on-the-fly trajectory synthesis based on the mobility patterns privately extracted from users' trajectory streams. Thus, the downstream trajectory analysis can be performed on the high-utility synthesized data with privacy protection. We also take the genuine behaviors of real-world mobile travelers into consideration, ensuring authenticity and practicality. The key components of RetraSyn include the global mobility model, dynamic mobility update mechanism, real-time synthesis, and adaptive allocation strategy. We conduct extensive experiments on multiple real-world and synthetic trajectory datasets under various location-based utility metrics, encompassing both streaming and historical scenarios. The empirical results demonstrate the superiority and versatility of our proposed framework.","sentences":["Trajectory streams are being generated from location-aware devices, such as smartphones and in-vehicle navigation systems.","Due to the sensitive nature of the location data, directly sharing user trajectories suffers from privacy leakage issues.","Local differential privacy (LDP), which perturbs sensitive data on the user side before it is shared or analyzed, emerges as a promising solution for private trajectory stream collection and analysis.","Unfortunately, existing stream release approaches often neglect the rich spatial-temporal context information within trajectory streams, resulting in suboptimal utility and limited types of downstream applications.","To this end, we propose RetraSyn, a novel real-time trajectory synthesis framework, which is able to perform on-the-fly trajectory synthesis based on the mobility patterns privately extracted from users' trajectory streams.","Thus, the downstream trajectory analysis can be performed on the high-utility synthesized data with privacy protection.","We also take the genuine behaviors of real-world mobile travelers into consideration, ensuring authenticity and practicality.","The key components of RetraSyn include the global mobility model, dynamic mobility update mechanism, real-time synthesis, and adaptive allocation strategy.","We conduct extensive experiments on multiple real-world and synthetic trajectory datasets under various location-based utility metrics, encompassing both streaming and historical scenarios.","The empirical results demonstrate the superiority and versatility of our proposed framework."],"url":"http://arxiv.org/abs/2404.11450v1","category":"cs.DB"}
{"created":"2024-04-17 14:55:03","title":"Research on emotionally intelligent dialogue generation based on automatic dialogue system","abstract":"Automated dialogue systems are important applications of artificial intelligence, and traditional systems struggle to understand user emotions and provide empathetic feedback. This study integrates emotional intelligence technology into automated dialogue systems and creates a dialogue generation model with emotional intelligence through deep learning and natural language processing techniques. The model can detect and understand a wide range of emotions and specific pain signals in real time, enabling the system to provide empathetic interaction. By integrating the results of the study \"Can artificial intelligence detect pain and express pain empathy?\", the model's ability to understand the subtle elements of pain empathy has been enhanced, setting higher standards for emotional intelligence dialogue systems. The project aims to provide theoretical understanding and practical suggestions to integrate advanced emotional intelligence capabilities into dialogue systems, thereby improving user experience and interaction quality.","sentences":["Automated dialogue systems are important applications of artificial intelligence, and traditional systems struggle to understand user emotions and provide empathetic feedback.","This study integrates emotional intelligence technology into automated dialogue systems and creates a dialogue generation model with emotional intelligence through deep learning and natural language processing techniques.","The model can detect and understand a wide range of emotions and specific pain signals in real time, enabling the system to provide empathetic interaction.","By integrating the results of the study \"Can artificial intelligence detect pain and express pain empathy?","\", the model's ability to understand the subtle elements of pain empathy has been enhanced, setting higher standards for emotional intelligence dialogue systems.","The project aims to provide theoretical understanding and practical suggestions to integrate advanced emotional intelligence capabilities into dialogue systems, thereby improving user experience and interaction quality."],"url":"http://arxiv.org/abs/2404.11447v1","category":"cs.AI"}
{"created":"2024-04-17 14:54:58","title":"Open-Ended Wargames with Large Language Models","abstract":"Wargames are a powerful tool for understanding and rehearsing real-world decision making. Automated play of wargames using artificial intelligence (AI) enables possibilities beyond those of human-conducted games, such as playing the game many times over to see a range of possible outcomes. There are two categories of wargames: quantitative games, with discrete types of moves, and qualitative games, which revolve around open-ended responses. Historically, automation efforts have focused on quantitative games, but large language models (LLMs) make it possible to automate qualitative wargames. We introduce \"Snow Globe,\" an LLM-powered multi-agent system for playing qualitative wargames. With Snow Globe, every stage of a text-based qualitative wargame from scenario preparation to post-game analysis can be optionally carried out by AI, humans, or a combination thereof. We describe its software architecture conceptually and release an open-source implementation alongside this publication. As case studies, we simulate a tabletop exercise about an AI incident response and a political wargame about a geopolitical crisis. We discuss potential applications of the approach and how it fits into the broader wargaming ecosystem.","sentences":["Wargames are a powerful tool for understanding and rehearsing real-world decision making.","Automated play of wargames using artificial intelligence (AI) enables possibilities beyond those of human-conducted games, such as playing the game many times over to see a range of possible outcomes.","There are two categories of wargames: quantitative games, with discrete types of moves, and qualitative games, which revolve around open-ended responses.","Historically, automation efforts have focused on quantitative games, but large language models (LLMs) make it possible to automate qualitative wargames.","We introduce \"Snow Globe,\" an LLM-powered multi-agent system for playing qualitative wargames.","With Snow Globe, every stage of a text-based qualitative wargame from scenario preparation to post-game analysis can be optionally carried out by AI, humans, or a combination thereof.","We describe its software architecture conceptually and release an open-source implementation alongside this publication.","As case studies, we simulate a tabletop exercise about an AI incident response and a political wargame about a geopolitical crisis.","We discuss potential applications of the approach and how it fits into the broader wargaming ecosystem."],"url":"http://arxiv.org/abs/2404.11446v1","category":"cs.CL"}
{"created":"2024-04-17 14:53:03","title":"Prediction of Unmanned Surface Vessel Motion Attitude Based on CEEMDAN-PSO-SVM","abstract":"Unmanned boats, while navigating at sea, utilize active compensation systems to mitigate wave disturbances experienced by onboard instruments and equipment. However, there exists a lag in the measurement of unmanned boat attitudes, thus introducing unmanned boat motion attitude prediction to compensate for the lag in the signal acquisition process. This paper, based on the basic principles of waves, derives the disturbance patterns of waves on unmanned boats from the wave energy spectrum. Through simulation analysis of unmanned boat motion attitudes, motion attitude data is obtained, providing experimental data for subsequent work. A combined prediction model based on Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN), Particle Swarm Optimization (PSO), and Support Vector Machine (SVM) is designed to predict the motion attitude of unmanned boats. Simulation results validate its superior prediction accuracy compared to traditional prediction models. For example, in terms of mean absolute error, it improves by 17% compared to the EMD-PSO-SVM model.","sentences":["Unmanned boats, while navigating at sea, utilize active compensation systems to mitigate wave disturbances experienced by onboard instruments and equipment.","However, there exists a lag in the measurement of unmanned boat attitudes, thus introducing unmanned boat motion attitude prediction to compensate for the lag in the signal acquisition process.","This paper, based on the basic principles of waves, derives the disturbance patterns of waves on unmanned boats from the wave energy spectrum.","Through simulation analysis of unmanned boat motion attitudes, motion attitude data is obtained, providing experimental data for subsequent work.","A combined prediction model based on Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN), Particle Swarm Optimization (PSO), and Support Vector Machine (SVM) is designed to predict the motion attitude of unmanned boats.","Simulation results validate its superior prediction accuracy compared to traditional prediction models.","For example, in terms of mean absolute error, it improves by 17% compared to the EMD-PSO-SVM model."],"url":"http://arxiv.org/abs/2404.11443v1","category":"cs.AI"}
{"created":"2024-04-17 14:36:47","title":"Instantiations and Computational Aspects of Non-Flat Assumption-based Argumentation","abstract":"Most existing computational tools for assumption-based argumentation (ABA) focus on so-called flat frameworks, disregarding the more general case. In this paper, we study an instantiation-based approach for reasoning in possibly non-flat ABA. We make use of a semantics-preserving translation between ABA and bipolar argumentation frameworks (BAFs). By utilizing compilability theory, we establish that the constructed BAFs will in general be of exponential size. In order to keep the number of arguments and computational cost low, we present three ways of identifying redundant arguments. Moreover, we identify fragments of ABA which admit a poly-sized instantiation. We propose two algorithmic approaches for reasoning in possibly non-flat ABA. The first approach utilizes the BAF instantiation while the second works directly without constructing arguments. An empirical evaluation shows that the former outperforms the latter on many instances, reflecting the lower complexity of BAF reasoning. This result is in contrast to flat ABA, where direct approaches dominate instantiation-based approaches.","sentences":["Most existing computational tools for assumption-based argumentation (ABA) focus on so-called flat frameworks, disregarding the more general case.","In this paper, we study an instantiation-based approach for reasoning in possibly non-flat ABA.","We make use of a semantics-preserving translation between ABA and bipolar argumentation frameworks (BAFs).","By utilizing compilability theory, we establish that the constructed BAFs will in general be of exponential size.","In order to keep the number of arguments and computational cost low, we present three ways of identifying redundant arguments.","Moreover, we identify fragments of ABA which admit a poly-sized instantiation.","We propose two algorithmic approaches for reasoning in possibly non-flat ABA.","The first approach utilizes the BAF instantiation while the second works directly without constructing arguments.","An empirical evaluation shows that the former outperforms the latter on many instances, reflecting the lower complexity of BAF reasoning.","This result is in contrast to flat ABA, where direct approaches dominate instantiation-based approaches."],"url":"http://arxiv.org/abs/2404.11431v1","category":"cs.AI"}
{"created":"2024-04-17 14:34:35","title":"Explainable Lung Disease Classification from Chest X-Ray Images Utilizing Deep Learning and XAI","abstract":"Lung diseases remain a critical global health concern, and it's crucial to have accurate and quick ways to diagnose them. This work focuses on classifying different lung diseases into five groups: viral pneumonia, bacterial pneumonia, COVID, tuberculosis, and normal lungs. Employing advanced deep learning techniques, we explore a diverse range of models including CNN, hybrid models, ensembles, transformers, and Big Transfer. The research encompasses comprehensive methodologies such as hyperparameter tuning, stratified k-fold cross-validation, and transfer learning with fine-tuning.Remarkably, our findings reveal that the Xception model, fine-tuned through 5-fold cross-validation, achieves the highest accuracy of 96.21\\%. This success shows that our methods work well in accurately identifying different lung diseases. The exploration of explainable artificial intelligence (XAI) methodologies further enhances our understanding of the decision-making processes employed by these models, contributing to increased trust in their clinical applications.","sentences":["Lung diseases remain a critical global health concern, and it's crucial to have accurate and quick ways to diagnose them.","This work focuses on classifying different lung diseases into five groups: viral pneumonia, bacterial pneumonia, COVID, tuberculosis, and normal lungs.","Employing advanced deep learning techniques, we explore a diverse range of models including CNN, hybrid models, ensembles, transformers, and Big Transfer.","The research encompasses comprehensive methodologies such as hyperparameter tuning, stratified k-fold cross-validation, and transfer learning with fine-tuning.","Remarkably, our findings reveal that the Xception model, fine-tuned through 5-fold cross-validation, achieves the highest accuracy of 96.21\\%.","This success shows that our methods work well in accurately identifying different lung diseases.","The exploration of explainable artificial intelligence (XAI) methodologies further enhances our understanding of the decision-making processes employed by these models, contributing to increased trust in their clinical applications."],"url":"http://arxiv.org/abs/2404.11428v1","category":"eess.IV"}
{"created":"2024-04-17 14:27:45","title":"Short-term wind speed forecasting model based on an attention-gated recurrent neural network and error correction strategy","abstract":"The accurate wind speed series forecast is very pivotal to security of grid dispatching and the application of wind power. Nevertheless, on account of their nonlinear and non-stationary nature, their short-term forecast is extremely challenging. Therefore, this dissertation raises one short-term wind speed forecast pattern on the foundation of attention with an improved gated recurrent neural network (AtGRU) and a tactic of error correction. That model uses the AtGRU model as the preliminary predictor and the GRU model as the error corrector. At the beginning, SSA (singular spectrum analysis) is employed in previous wind speed series for lessening the noise. Subsequently, historical wind speed series is going to be used for the predictor training. During this process, the prediction can have certain errors. The sequence of these errors processed by variational modal decomposition (VMD) is used to train the corrector of error. The eventual forecast consequence is just the sum of predictor forecast and error corrector. The proposed SSA-AtGRU-VMD-GRU model outperforms the compared models in three case studies on Woodburn, St. Thomas, and Santa Cruz. It is indicated that the model evidently enhances the correction of the wind speed forecast.","sentences":["The accurate wind speed series forecast is very pivotal to security of grid dispatching and the application of wind power.","Nevertheless, on account of their nonlinear and non-stationary nature, their short-term forecast is extremely challenging.","Therefore, this dissertation raises one short-term wind speed forecast pattern on the foundation of attention with an improved gated recurrent neural network (AtGRU) and a tactic of error correction.","That model uses the AtGRU model as the preliminary predictor and the GRU model as the error corrector.","At the beginning, SSA (singular spectrum analysis) is employed in previous wind speed series for lessening the noise.","Subsequently, historical wind speed series is going to be used for the predictor training.","During this process, the prediction can have certain errors.","The sequence of these errors processed by variational modal decomposition (VMD) is used to train the corrector of error.","The eventual forecast consequence is just the sum of predictor forecast and error corrector.","The proposed SSA-AtGRU-VMD-GRU model outperforms the compared models in three case studies on Woodburn, St. Thomas, and Santa Cruz.","It is indicated that the model evidently enhances the correction of the wind speed forecast."],"url":"http://arxiv.org/abs/2404.11422v1","category":"cs.LG"}
{"created":"2024-04-17 14:16:43","title":"Achromatic Full Stokes Polarimetry Metasurface for Full-color Polarization Imaging in the Visible","abstract":"Metasurfaces composed of anisotropic subwavelength structures provide an ultrathin platform for a compact, real-time polarimeter. However, applications in polychromatic scenes are restricted by the limited operating bandwidths and degraded imaging quality due to the loss of spectral information. Here, we demonstrated full-color polarization imaging based on an achromatic polarimeter consisting of four polarization-dependent metalenses. Boosted by an intelligent design scheme, arbitrary phase compensation and multi-objective matching are effectively compatible with a limited database. Broadband achromaticity for wavelengths ranging from 450 nm to 650 nm, with a relative bandwidth of nearly 0.435, is achieved for the full Stokes imaging. The experimental polarization reconstructed errors for operating wavelengths of 450 nm, 550 nm, and 650 nm are 7.5%, 5.9%, and 3.8%, respectively. The full-color and full-polarization imaging capability of the device is also verified with a customized object. The proposed scheme paves the way for further developing polarization imaging toward practical applications.","sentences":["Metasurfaces composed of anisotropic subwavelength structures provide an ultrathin platform for a compact, real-time polarimeter.","However, applications in polychromatic scenes are restricted by the limited operating bandwidths and degraded imaging quality due to the loss of spectral information.","Here, we demonstrated full-color polarization imaging based on an achromatic polarimeter consisting of four polarization-dependent metalenses.","Boosted by an intelligent design scheme, arbitrary phase compensation and multi-objective matching are effectively compatible with a limited database.","Broadband achromaticity for wavelengths ranging from 450 nm to 650 nm, with a relative bandwidth of nearly 0.435, is achieved for the full Stokes imaging.","The experimental polarization reconstructed errors for operating wavelengths of 450 nm, 550 nm, and 650 nm are 7.5%, 5.9%, and 3.8%, respectively.","The full-color and full-polarization imaging capability of the device is also verified with a customized object.","The proposed scheme paves the way for further developing polarization imaging toward practical applications."],"url":"http://arxiv.org/abs/2404.11415v1","category":"physics.optics"}
{"created":"2024-04-17 14:12:47","title":"EcoMLS: A Self-Adaptation Approach for Architecting Green ML-Enabled Systems","abstract":"The sustainability of Machine Learning-Enabled Systems (MLS), particularly with regard to energy efficiency, is an important challenge in their development and deployment. Self-adaptation techniques, recognized for their potential in energy savings within software systems, have yet to be extensively explored in Machine Learning-Enabled Systems (MLS), where runtime uncertainties can significantly impact model performance and energy consumption. This variability, alongside the fluctuating energy demands of ML models during operation, necessitates a dynamic approach. Addressing these challenges, we introduce EcoMLS approach, which leverages the Machine Learning Model Balancer concept to enhance the sustainability of MLS through runtime ML model switching. By adapting to monitored runtime conditions, EcoMLS optimally balances energy consumption with model confidence, demonstrating a significant advancement towards sustainable, energy-efficient machine learning solutions. Through an object detection exemplar, we illustrate the application of EcoMLS, showcasing its ability to reduce energy consumption while maintaining high model accuracy throughout its use. This research underscores the feasibility of enhancing MLS sustainability through intelligent runtime adaptations, contributing a valuable perspective to the ongoing discourse on energy-efficient machine learning.","sentences":["The sustainability of Machine Learning-Enabled Systems (MLS), particularly with regard to energy efficiency, is an important challenge in their development and deployment.","Self-adaptation techniques, recognized for their potential in energy savings within software systems, have yet to be extensively explored in Machine Learning-Enabled Systems (MLS), where runtime uncertainties can significantly impact model performance and energy consumption.","This variability, alongside the fluctuating energy demands of ML models during operation, necessitates a dynamic approach.","Addressing these challenges, we introduce EcoMLS approach, which leverages the Machine Learning Model Balancer concept to enhance the sustainability of MLS through runtime ML model switching.","By adapting to monitored runtime conditions, EcoMLS optimally balances energy consumption with model confidence, demonstrating a significant advancement towards sustainable, energy-efficient machine learning solutions.","Through an object detection exemplar, we illustrate the application of EcoMLS, showcasing its ability to reduce energy consumption while maintaining high model accuracy throughout its use.","This research underscores the feasibility of enhancing MLS sustainability through intelligent runtime adaptations, contributing a valuable perspective to the ongoing discourse on energy-efficient machine learning."],"url":"http://arxiv.org/abs/2404.11411v1","category":"cs.SE"}
{"created":"2024-04-17 14:10:27","title":"DUPE: Detection Undermining via Prompt Engineering for Deepfake Text","abstract":"As large language models (LLMs) become increasingly commonplace, concern about distinguishing between human and AI text increases as well. The growing power of these models is of particular concern to teachers, who may worry that students will use LLMs to write school assignments. Facing a technology with which they are unfamiliar, teachers may turn to publicly-available AI text detectors. Yet the accuracy of many of these detectors has not been thoroughly verified, posing potential harm to students who are falsely accused of academic dishonesty. In this paper, we evaluate three different AI text detectors-Kirchenbauer et al. watermarks, ZeroGPT, and GPTZero-against human and AI-generated essays. We find that watermarking results in a high false positive rate, and that ZeroGPT has both high false positive and false negative rates. Further, we are able to significantly increase the false negative rate of all detectors by using ChatGPT 3.5 to paraphrase the original AI-generated texts, thereby effectively bypassing the detectors.","sentences":["As large language models (LLMs) become increasingly commonplace, concern about distinguishing between human and AI text increases as well.","The growing power of these models is of particular concern to teachers, who may worry that students will use LLMs to write school assignments.","Facing a technology with which they are unfamiliar, teachers may turn to publicly-available AI text detectors.","Yet the accuracy of many of these detectors has not been thoroughly verified, posing potential harm to students who are falsely accused of academic dishonesty.","In this paper, we evaluate three different AI text detectors-Kirchenbauer et al. watermarks, ZeroGPT, and GPTZero-against human and AI-generated essays.","We find that watermarking results in a high false positive rate, and that ZeroGPT has both high false positive and false negative rates.","Further, we are able to significantly increase the false negative rate of all detectors by using ChatGPT 3.5 to paraphrase the original AI-generated texts, thereby effectively bypassing the detectors."],"url":"http://arxiv.org/abs/2404.11408v1","category":"cs.AI"}
{"created":"2024-04-17 14:07:22","title":"RainyScape: Unsupervised Rainy Scene Reconstruction using Decoupled Neural Rendering","abstract":"We propose RainyScape, an unsupervised framework for reconstructing clean scenes from a collection of multi-view rainy images. RainyScape consists of two main modules: a neural rendering module and a rain-prediction module that incorporates a predictor network and a learnable latent embedding that captures the rain characteristics of the scene. Specifically, based on the spectral bias property of neural networks, we first optimize the neural rendering pipeline to obtain a low-frequency scene representation. Subsequently, we jointly optimize the two modules, driven by the proposed adaptive direction-sensitive gradient-based reconstruction loss, which encourages the network to distinguish between scene details and rain streaks, facilitating the propagation of gradients to the relevant components. Extensive experiments on both the classic neural radiance field and the recently proposed 3D Gaussian splatting demonstrate the superiority of our method in effectively eliminating rain streaks and rendering clean images, achieving state-of-the-art performance. The constructed high-quality dataset and source code will be publicly available.","sentences":["We propose RainyScape, an unsupervised framework for reconstructing clean scenes from a collection of multi-view rainy images.","RainyScape consists of two main modules: a neural rendering module and a rain-prediction module that incorporates a predictor network and a learnable latent embedding that captures the rain characteristics of the scene.","Specifically, based on the spectral bias property of neural networks, we first optimize the neural rendering pipeline to obtain a low-frequency scene representation.","Subsequently, we jointly optimize the two modules, driven by the proposed adaptive direction-sensitive gradient-based reconstruction loss, which encourages the network to distinguish between scene details and rain streaks, facilitating the propagation of gradients to the relevant components.","Extensive experiments on both the classic neural radiance field and the recently proposed 3D Gaussian splatting demonstrate the superiority of our method in effectively eliminating rain streaks and rendering clean images, achieving state-of-the-art performance.","The constructed high-quality dataset and source code will be publicly available."],"url":"http://arxiv.org/abs/2404.11401v1","category":"cs.CV"}
{"created":"2024-04-17 13:44:29","title":"Exploring Key Point Analysis with Pairwise Generation and Graph Partitioning","abstract":"Key Point Analysis (KPA), the summarization of multiple arguments into a concise collection of key points, continues to be a significant and unresolved issue within the field of argument mining. Existing models adapt a two-stage pipeline of clustering arguments or generating key points for argument clusters. This approach rely on semantic similarity instead of measuring the existence of shared key points among arguments. Additionally, it only models the intra-cluster relationship among arguments, disregarding the inter-cluster relationship between arguments that do not share key points. To address these limitations, we propose a novel approach for KPA with pairwise generation and graph partitioning. Our objective is to train a generative model that can simultaneously provide a score indicating the presence of shared key point between a pair of arguments and generate the shared key point. Subsequently, to map generated redundant key points to a concise set of key points, we proceed to construct an arguments graph by considering the arguments as vertices, the generated key points as edges, and the scores as edge weights. We then propose a graph partitioning algorithm to partition all arguments sharing the same key points to the same subgraph. Notably, our experimental findings demonstrate that our proposed model surpasses previous models when evaluated on both the ArgKP and QAM datasets.","sentences":["Key Point Analysis (KPA), the summarization of multiple arguments into a concise collection of key points, continues to be a significant and unresolved issue within the field of argument mining.","Existing models adapt a two-stage pipeline of clustering arguments or generating key points for argument clusters.","This approach rely on semantic similarity instead of measuring the existence of shared key points among arguments.","Additionally, it only models the intra-cluster relationship among arguments, disregarding the inter-cluster relationship between arguments that do not share key points.","To address these limitations, we propose a novel approach for KPA with pairwise generation and graph partitioning.","Our objective is to train a generative model that can simultaneously provide a score indicating the presence of shared key point between a pair of arguments and generate the shared key point.","Subsequently, to map generated redundant key points to a concise set of key points, we proceed to construct an arguments graph by considering the arguments as vertices, the generated key points as edges, and the scores as edge weights.","We then propose a graph partitioning algorithm to partition all arguments sharing the same key points to the same subgraph.","Notably, our experimental findings demonstrate that our proposed model surpasses previous models when evaluated on both the ArgKP and QAM datasets."],"url":"http://arxiv.org/abs/2404.11384v1","category":"cs.CL"}
{"created":"2024-04-17 13:30:45","title":"Characterizing and modeling harms from interactions with design patterns in AI interfaces","abstract":"The proliferation of applications using artificial intelligence (AI) systems has led to a growing number of users interacting with these systems through sophisticated interfaces. Human-computer interaction research has long shown that interfaces shape both user behavior and user perception of technical capabilities and risks. Yet, practitioners and researchers evaluating the social and ethical risks of AI systems tend to overlook the impact of anthropomorphic, deceptive, and immersive interfaces on human-AI interactions. Here, we argue that design features of interfaces with adaptive AI systems can have cascading impacts, driven by feedback loops, which extend beyond those previously considered. We first conduct a scoping review of AI interface designs and their negative impact to extract salient themes of potentially harmful design patterns in AI interfaces. Then, we propose Design-Enhanced Control of AI systems (DECAI), a conceptual model to structure and facilitate impact assessments of AI interface designs. DECAI draws on principles from control systems theory -- a theory for the analysis and design of dynamic physical systems -- to dissect the role of the interface in human-AI systems. Through two case studies on recommendation systems and conversational language model systems, we show how DECAI can be used to evaluate AI interface designs.","sentences":["The proliferation of applications using artificial intelligence (AI) systems has led to a growing number of users interacting with these systems through sophisticated interfaces.","Human-computer interaction research has long shown that interfaces shape both user behavior and user perception of technical capabilities and risks.","Yet, practitioners and researchers evaluating the social and ethical risks of AI systems tend to overlook the impact of anthropomorphic, deceptive, and immersive interfaces on human-AI interactions.","Here, we argue that design features of interfaces with adaptive AI systems can have cascading impacts, driven by feedback loops, which extend beyond those previously considered.","We first conduct a scoping review of AI interface designs and their negative impact to extract salient themes of potentially harmful design patterns in AI interfaces.","Then, we propose Design-Enhanced Control of AI systems (DECAI), a conceptual model to structure and facilitate impact assessments of AI interface designs.","DECAI draws on principles from control systems theory -- a theory for the analysis and design of dynamic physical systems -- to dissect the role of the interface in human-AI systems.","Through two case studies on recommendation systems and conversational language model systems, we show how DECAI can be used to evaluate AI interface designs."],"url":"http://arxiv.org/abs/2404.11370v1","category":"cs.HC"}
{"created":"2024-04-17 13:21:04","title":"Sinking an Algorithmic Isthmus: (1 + \u03b5)-Approximate Min-Sum Subset Convolution","abstract":"Given functions $f$ and $g$ defined on the subset lattice of order $n$, their min-sum subset convolution, defined for all $S \\subseteq [n]$ as \\[   (f \\star g)(S) = \\min_{T \\subseteq S}\\:\\big(f(T) + g(S \\setminus T)\\big), \\] is a fundamental tool in parameterized algorithms. However, since its na\\\"ive $O(3^n)$-time evaluation is also the fastest known, it has been used only in settings where the input functions have a bounded integer range $\\{-M, \\ldots, M\\}$. In this case, the running time becomes $\\tilde O(2^n M)$ by resorting to fast subset convolution in the sum-product ring. This is disadvantageous due to the dependence on $M$, limiting its practicality.   In this light, we study whether the problem admits an $(1 + \\varepsilon)$-approximation scheme in time independent of $M$. Our main result is the first $\\tilde O(2^\\frac{3n}{2} / \\sqrt{\\varepsilon})$-time algorithm for the $(1 + \\varepsilon)$-approximate min-sum subset convolution. To show its applicability, we present $(1 + \\varepsilon)$-approximation schemes in the same exponential time bound for several NP-hard problems using this convolution, such as the minimum-cost $k$-coloring problem -- in time $\\tilde O(2^\\frac{3n}{2} / \\sqrt{\\varepsilon})$, and the prize-collecting Steiner tree problem -- in time $\\tilde O(2^\\frac{3s^+}{2} / \\sqrt{\\varepsilon})$, where $n$ is the number of vertices and $s^+$ is the number of proper potential terminals. We also discuss two other applications in computational biology.   Our algorithms lie at the intersection of two lines of research that have been considered separately: $\\textit{sequence}$ and $\\textit{subset}$ convolutions in semi-rings. In particular, we extend the recent framework of Bringmann, K\\\"unnemann, and W\\k{e}grzycki [STOC 2019] to the context of subset convolutions.","sentences":["Given functions $f$ and $g$ defined on the subset lattice of order $n$, their min-sum subset convolution, defined for all $S \\subseteq","[n]$ as \\[   (f \\star g)(S) = \\min_{T \\subseteq S}\\:\\big(f(T) +","g(S \\setminus T)\\big), \\] is a fundamental tool in parameterized algorithms.","However, since its na\\\"ive $O(3^n)$-time evaluation is also the fastest known, it has been used only in settings where the input functions have a bounded integer range $\\{-M, \\ldots, M\\}$.","In this case, the running time becomes $\\tilde O(2^n M)$ by resorting to fast subset convolution in the sum-product ring.","This is disadvantageous due to the dependence on $M$, limiting its practicality.   ","In this light, we study whether the problem admits an $(1 + \\varepsilon)$-approximation scheme in time independent of $M$. Our main result is the first $\\tilde O(2^\\frac{3n}{2} / \\sqrt{\\varepsilon})$-time algorithm for the $(1 + \\varepsilon)$-approximate min-sum subset convolution.","To show its applicability, we present $(1 + \\varepsilon)$-approximation schemes in the same exponential time bound for several NP-hard problems using this convolution, such as the minimum-cost $k$-coloring problem -- in time $\\tilde O(2^\\frac{3n}{2} / \\sqrt{\\varepsilon})$, and the prize-collecting Steiner tree problem -- in time $\\tilde O(2^\\frac{3s^+}{2} / \\sqrt{\\varepsilon})$, where $n$ is the number of vertices and $s^+$ is the number of proper potential terminals.","We also discuss two other applications in computational biology.   ","Our algorithms lie at the intersection of two lines of research that have been considered separately: $\\textit{sequence}$ and $\\textit{subset}$ convolutions in semi-rings.","In particular, we extend the recent framework of Bringmann, K\\\"unnemann, and W\\k{e}grzycki","[STOC 2019] to the context of subset convolutions."],"url":"http://arxiv.org/abs/2404.11364v1","category":"cs.DS"}
{"created":"2024-04-17 13:08:26","title":"Calibrating Bayesian Learning via Regularization, Confidence Minimization, and Selective Inference","abstract":"The application of artificial intelligence (AI) models in fields such as engineering is limited by the known difficulty of quantifying the reliability of an AI's decision. A well-calibrated AI model must correctly report its accuracy on in-distribution (ID) inputs, while also enabling the detection of out-of-distribution (OOD) inputs. A conventional approach to improve calibration is the application of Bayesian ensembling. However, owing to computational limitations and model misspecification, practical ensembling strategies do not necessarily enhance calibration. This paper proposes an extension of variational inference (VI)-based Bayesian learning that integrates calibration regularization for improved ID performance, confidence minimization for OOD detection, and selective calibration to ensure a synergistic use of calibration regularization and confidence minimization. The scheme is constructed successively by first introducing calibration-regularized Bayesian learning (CBNN), then incorporating out-of-distribution confidence minimization (OCM) to yield CBNN-OCM, and finally integrating also selective calibration to produce selective CBNN-OCM (SCBNN-OCM). Selective calibration rejects inputs for which the calibration performance is expected to be insufficient. Numerical results illustrate the trade-offs between ID accuracy, ID calibration, and OOD calibration attained by both frequentist and Bayesian learning methods. Among the main conclusions, SCBNN-OCM is seen to achieve best ID and OOD performance as compared to existing state-of-the-art approaches at the cost of rejecting a sufficiently large number of inputs.","sentences":["The application of artificial intelligence (AI) models in fields such as engineering is limited by the known difficulty of quantifying the reliability of an AI's decision.","A well-calibrated AI model must correctly report its accuracy on in-distribution (ID) inputs, while also enabling the detection of out-of-distribution (OOD) inputs.","A conventional approach to improve calibration is the application of Bayesian ensembling.","However, owing to computational limitations and model misspecification, practical ensembling strategies do not necessarily enhance calibration.","This paper proposes an extension of variational inference (VI)-based Bayesian learning that integrates calibration regularization for improved ID performance, confidence minimization for OOD detection, and selective calibration to ensure a synergistic use of calibration regularization and confidence minimization.","The scheme is constructed successively by first introducing calibration-regularized Bayesian learning (CBNN), then incorporating out-of-distribution confidence minimization (OCM) to yield CBNN-OCM, and finally integrating also selective calibration to produce selective CBNN-OCM (SCBNN-OCM).","Selective calibration rejects inputs for which the calibration performance is expected to be insufficient.","Numerical results illustrate the trade-offs between ID accuracy, ID calibration, and OOD calibration attained by both frequentist and Bayesian learning methods.","Among the main conclusions, SCBNN-OCM is seen to achieve best ID and OOD performance as compared to existing state-of-the-art approaches at the cost of rejecting a sufficiently large number of inputs."],"url":"http://arxiv.org/abs/2404.11350v1","category":"cs.LG"}
{"created":"2024-04-17 13:07:01","title":"Modern tools for computing neutron star properties","abstract":"Astronomical observations place increasingly tighter and more diverse constraints on the properties of neutron stars (NS). Examples include observations of radio or gamma-ray pulsars, accreting neutron stars and x-ray bursts, magnetar giant flares, and recently, the gravitational waves (GW) from coalescing binary neutron stars. Computing NS properties for a given EOS, such as mass, radius, moment of inertia, tidal deformability, and innermost stable circular orbits (ISCO), is therefore an important task. This task is unnecessarily difficult because relevant formulas are scattered throughout the literature and publicly available software tools are far from being complete and easy to use. Further, naive implementations are unreliable in numerical corner cases, most notably when using equations of state (EOS) with phase transitions. To improve the situation, we provide a public library for computing NS properties and handling of EOS data. Further, we include a collection of EOS based on existing nuclear physics models together with precomputed sequences of NS models. All methods are accessible via a Python interface. This article collects all relevant equations and numerical methods in full detail, including a novel formulation for the tidal deformability equations suitable for use with phase transitions. As a sidenote to the topic of ISCOs, we discuss the stability of non-interacting dark matter particle circular orbits inside NSs. Finally, we present some simple applications relevant for parameter estimation studies of GW data. For example, we explore the validity of universal relations, and discuss the appearance of multiple stable branches for parametrized EOS.","sentences":["Astronomical observations place increasingly tighter and more diverse constraints on the properties of neutron stars (NS).","Examples include observations of radio or gamma-ray pulsars, accreting neutron stars and x-ray bursts, magnetar giant flares, and recently, the gravitational waves (GW) from coalescing binary neutron stars.","Computing NS properties for a given EOS, such as mass, radius, moment of inertia, tidal deformability, and innermost stable circular orbits (ISCO), is therefore an important task.","This task is unnecessarily difficult because relevant formulas are scattered throughout the literature and publicly available software tools are far from being complete and easy to use.","Further, naive implementations are unreliable in numerical corner cases, most notably when using equations of state (EOS) with phase transitions.","To improve the situation, we provide a public library for computing NS properties and handling of EOS data.","Further, we include a collection of EOS based on existing nuclear physics models together with precomputed sequences of NS models.","All methods are accessible via a Python interface.","This article collects all relevant equations and numerical methods in full detail, including a novel formulation for the tidal deformability equations suitable for use with phase transitions.","As a sidenote to the topic of ISCOs, we discuss the stability of non-interacting dark matter particle circular orbits inside NSs.","Finally, we present some simple applications relevant for parameter estimation studies of GW data.","For example, we explore the validity of universal relations, and discuss the appearance of multiple stable branches for parametrized EOS."],"url":"http://arxiv.org/abs/2404.11346v1","category":"gr-qc"}
{"created":"2024-04-17 17:59:59","title":"Factorized Diffusion: Perceptual Illusions by Noise Decomposition","abstract":"Given a factorization of an image into a sum of linear components, we present a zero-shot method to control each individual component through diffusion model sampling. For example, we can decompose an image into low and high spatial frequencies and condition these components on different text prompts. This produces hybrid images, which change appearance depending on viewing distance. By decomposing an image into three frequency subbands, we can generate hybrid images with three prompts. We also use a decomposition into grayscale and color components to produce images whose appearance changes when they are viewed in grayscale, a phenomena that naturally occurs under dim lighting. And we explore a decomposition by a motion blur kernel, which produces images that change appearance under motion blurring. Our method works by denoising with a composite noise estimate, built from the components of noise estimates conditioned on different prompts. We also show that for certain decompositions, our method recovers prior approaches to compositional generation and spatial control. Finally, we show that we can extend our approach to generate hybrid images from real images. We do this by holding one component fixed and generating the remaining components, effectively solving an inverse problem.","sentences":["Given a factorization of an image into a sum of linear components, we present a zero-shot method to control each individual component through diffusion model sampling.","For example, we can decompose an image into low and high spatial frequencies and condition these components on different text prompts.","This produces hybrid images, which change appearance depending on viewing distance.","By decomposing an image into three frequency subbands, we can generate hybrid images with three prompts.","We also use a decomposition into grayscale and color components to produce images whose appearance changes when they are viewed in grayscale, a phenomena that naturally occurs under dim lighting.","And we explore a decomposition by a motion blur kernel, which produces images that change appearance under motion blurring.","Our method works by denoising with a composite noise estimate, built from the components of noise estimates conditioned on different prompts.","We also show that for certain decompositions, our method recovers prior approaches to compositional generation and spatial control.","Finally, we show that we can extend our approach to generate hybrid images from real images.","We do this by holding one component fixed and generating the remaining components, effectively solving an inverse problem."],"url":"http://arxiv.org/abs/2404.11615v1","category":"cs.CV"}
{"created":"2024-04-17 17:49:08","title":"Urban highways are barriers to social ties","abstract":"Urban highways are common, especially in the US, making cities more car-centric. They promise the annihilation of distance but obstruct pedestrian mobility, thus playing a key role in limiting social interactions locally. Although this limiting role is widely acknowledged in urban studies, the quantitative relationship between urban highways and social ties is barely tested. Here we define a Barrier Score that relates massive, geolocated online social network data to highways in the 50 largest US cities. At the unprecedented granularity of individual social ties, we show that urban highways are associated with decreased social connectivity. This barrier effect is especially strong for short distances and consistent with historical cases of highways that were built to purposefully disrupt or isolate Black neighborhoods. By combining spatial infrastructure with social tie data, our method adds a new dimension to demographic studies of social segregation. Our study can inform reparative planning for an evidence-based reduction of spatial inequality, and more generally, support a better integration of the social fabric in urban planning.","sentences":["Urban highways are common, especially in the US, making cities more car-centric.","They promise the annihilation of distance but obstruct pedestrian mobility, thus playing a key role in limiting social interactions locally.","Although this limiting role is widely acknowledged in urban studies, the quantitative relationship between urban highways and social ties is barely tested.","Here we define a Barrier Score that relates massive, geolocated online social network data to highways in the 50 largest US cities.","At the unprecedented granularity of individual social ties, we show that urban highways are associated with decreased social connectivity.","This barrier effect is especially strong for short distances and consistent with historical cases of highways that were built to purposefully disrupt or isolate Black neighborhoods.","By combining spatial infrastructure with social tie data, our method adds a new dimension to demographic studies of social segregation.","Our study can inform reparative planning for an evidence-based reduction of spatial inequality, and more generally, support a better integration of the social fabric in urban planning."],"url":"http://arxiv.org/abs/2404.11596v2","category":"physics.soc-ph"}
{"created":"2024-04-17 17:45:08","title":"IntrinsicAnything: Learning Diffusion Priors for Inverse Rendering Under Unknown Illumination","abstract":"This paper aims to recover object materials from posed images captured under an unknown static lighting condition. Recent methods solve this task by optimizing material parameters through differentiable physically based rendering. However, due to the coupling between object geometry, materials, and environment lighting, there is inherent ambiguity during the inverse rendering process, preventing previous methods from obtaining accurate results. To overcome this ill-posed problem, our key idea is to learn the material prior with a generative model for regularizing the optimization process. We observe that the general rendering equation can be split into diffuse and specular shading terms, and thus formulate the material prior as diffusion models of albedo and specular. Thanks to this design, our model can be trained using the existing abundant 3D object data, and naturally acts as a versatile tool to resolve the ambiguity when recovering material representations from RGB images. In addition, we develop a coarse-to-fine training strategy that leverages estimated materials to guide diffusion models to satisfy multi-view consistent constraints, leading to more stable and accurate results. Extensive experiments on real-world and synthetic datasets demonstrate that our approach achieves state-of-the-art performance on material recovery. The code will be available at https://zju3dv.github.io/IntrinsicAnything.","sentences":["This paper aims to recover object materials from posed images captured under an unknown static lighting condition.","Recent methods solve this task by optimizing material parameters through differentiable physically based rendering.","However, due to the coupling between object geometry, materials, and environment lighting, there is inherent ambiguity during the inverse rendering process, preventing previous methods from obtaining accurate results.","To overcome this ill-posed problem, our key idea is to learn the material prior with a generative model for regularizing the optimization process.","We observe that the general rendering equation can be split into diffuse and specular shading terms, and thus formulate the material prior as diffusion models of albedo and specular.","Thanks to this design, our model can be trained using the existing abundant 3D object data, and naturally acts as a versatile tool to resolve the ambiguity when recovering material representations from RGB images.","In addition, we develop a coarse-to-fine training strategy that leverages estimated materials to guide diffusion models to satisfy multi-view consistent constraints, leading to more stable and accurate results.","Extensive experiments on real-world and synthetic datasets demonstrate that our approach achieves state-of-the-art performance on material recovery.","The code will be available at https://zju3dv.github.io/IntrinsicAnything."],"url":"http://arxiv.org/abs/2404.11593v1","category":"cs.CV"}
{"created":"2024-04-17 17:37:30","title":"Related Work and Citation Text Generation: A Survey","abstract":"To convince readers of the novelty of their research paper, authors must perform a literature review and compose a coherent story that connects and relates prior works to the current work. This challenging nature of literature review writing makes automatic related work generation (RWG) academically and computationally interesting, and also makes it an excellent test bed for examining the capability of SOTA natural language processing (NLP) models. Since the initial proposal of the RWG task, its popularity has waxed and waned, following the capabilities of mainstream NLP approaches. In this work, we survey the zoo of RWG historical works, summarizing the key approaches and task definitions and discussing the ongoing challenges of RWG.","sentences":["To convince readers of the novelty of their research paper, authors must perform a literature review and compose a coherent story that connects and relates prior works to the current work.","This challenging nature of literature review writing makes automatic related work generation (RWG) academically and computationally interesting, and also makes it an excellent test bed for examining the capability of SOTA natural language processing (NLP) models.","Since the initial proposal of the RWG task, its popularity has waxed and waned, following the capabilities of mainstream NLP approaches.","In this work, we survey the zoo of RWG historical works, summarizing the key approaches and task definitions and discussing the ongoing challenges of RWG."],"url":"http://arxiv.org/abs/2404.11588v1","category":"cs.CL"}
{"created":"2024-04-17 17:33:48","title":"Ring momentum distributions as a general feature of Vlasov dynamics in the synchrotron dominated regime","abstract":"We study how radiation reaction leads plasmas initially in kinetic equilibrium to develop features in momentum space, such as anisotropies and population inversion, resulting in a ring-shaped momentum distribution that can drive kinetic instabilities. We employ the Landau-Lifshiftz radiation reaction model for a plasma in a strong magnetic field, and we obtain the necessary condition for the development of population inversion, we show that isotropic Maxwellian and Maxwell-J\\\"uttner plasmas, with thermal temperature $T>m_e c^2/\\sqrt{3}$, will develop a ring-like momentum distribution. The timescales and features for forming ring-shaped momentum distributions, the effect of collisions and non-uniform magnetic fields are disscussed, and compared with typical astrophysical and laboratory plasmas parameters. Our results show the pervasiveness of ring-like momentum distribution functions in synchrotron dominated plasma conditions.","sentences":["We study how radiation reaction leads plasmas initially in kinetic equilibrium to develop features in momentum space, such as anisotropies and population inversion, resulting in a ring-shaped momentum distribution that can drive kinetic instabilities.","We employ the Landau-Lifshiftz radiation reaction model for a plasma in a strong magnetic field, and we obtain the necessary condition for the development of population inversion, we show that isotropic Maxwellian and Maxwell-J\\\"uttner plasmas, with thermal temperature $T>m_e","c^2/\\sqrt{3}$, will develop a ring-like momentum distribution.","The timescales and features for forming ring-shaped momentum distributions, the effect of collisions and non-uniform magnetic fields are disscussed, and compared with typical astrophysical and laboratory plasmas parameters.","Our results show the pervasiveness of ring-like momentum distribution functions in synchrotron dominated plasma conditions."],"url":"http://arxiv.org/abs/2404.11586v1","category":"physics.plasm-ph"}
{"created":"2024-04-17 17:19:48","title":"State-space Decomposition Model for Video Prediction Considering Long-term Motion Trend","abstract":"Stochastic video prediction enables the consideration of uncertainty in future motion, thereby providing a better reflection of the dynamic nature of the environment. Stochastic video prediction methods based on image auto-regressive recurrent models need to feed their predictions back into the latent space. Conversely, the state-space models, which decouple frame synthesis and temporal prediction, proves to be more efficient. However, inferring long-term temporal information about motion and generalizing to dynamic scenarios under non-stationary assumptions remains an unresolved challenge. In this paper, we propose a state-space decomposition stochastic video prediction model that decomposes the overall video frame generation into deterministic appearance prediction and stochastic motion prediction. Through adaptive decomposition, the model's generalization capability to dynamic scenarios is enhanced. In the context of motion prediction, obtaining a prior on the long-term trend of future motion is crucial. Thus, in the stochastic motion prediction branch, we infer the long-term motion trend from conditional frames to guide the generation of future frames that exhibit high consistency with the conditional frames. Experimental results demonstrate that our model outperforms baselines on multiple datasets.","sentences":["Stochastic video prediction enables the consideration of uncertainty in future motion, thereby providing a better reflection of the dynamic nature of the environment.","Stochastic video prediction methods based on image auto-regressive recurrent models need to feed their predictions back into the latent space.","Conversely, the state-space models, which decouple frame synthesis and temporal prediction, proves to be more efficient.","However, inferring long-term temporal information about motion and generalizing to dynamic scenarios under non-stationary assumptions remains an unresolved challenge.","In this paper, we propose a state-space decomposition stochastic video prediction model that decomposes the overall video frame generation into deterministic appearance prediction and stochastic motion prediction.","Through adaptive decomposition, the model's generalization capability to dynamic scenarios is enhanced.","In the context of motion prediction, obtaining a prior on the long-term trend of future motion is crucial.","Thus, in the stochastic motion prediction branch, we infer the long-term motion trend from conditional frames to guide the generation of future frames that exhibit high consistency with the conditional frames.","Experimental results demonstrate that our model outperforms baselines on multiple datasets."],"url":"http://arxiv.org/abs/2404.11576v1","category":"cs.CV"}
{"created":"2024-04-17 16:30:56","title":"SSDiff: Spatial-spectral Integrated Diffusion Model for Remote Sensing Pansharpening","abstract":"Pansharpening is a significant image fusion technique that merges the spatial content and spectral characteristics of remote sensing images to generate high-resolution multispectral images. Recently, denoising diffusion probabilistic models have been gradually applied to visual tasks, enhancing controllable image generation through low-rank adaptation (LoRA). In this paper, we introduce a spatial-spectral integrated diffusion model for the remote sensing pansharpening task, called SSDiff, which considers the pansharpening process as the fusion process of spatial and spectral components from the perspective of subspace decomposition. Specifically, SSDiff utilizes spatial and spectral branches to learn spatial details and spectral features separately, then employs a designed alternating projection fusion module (APFM) to accomplish the fusion. Furthermore, we propose a frequency modulation inter-branch module (FMIM) to modulate the frequency distribution between branches. The two components of SSDiff can perform favorably against the APFM when utilizing a LoRA-like branch-wise alternative fine-tuning method. It refines SSDiff to capture component-discriminating features more sufficiently. Finally, extensive experiments on four commonly used datasets, i.e., WorldView-3, WorldView-2, GaoFen-2, and QuickBird, demonstrate the superiority of SSDiff both visually and quantitatively. The code will be made open source after possible acceptance.","sentences":["Pansharpening is a significant image fusion technique that merges the spatial content and spectral characteristics of remote sensing images to generate high-resolution multispectral images.","Recently, denoising diffusion probabilistic models have been gradually applied to visual tasks, enhancing controllable image generation through low-rank adaptation (LoRA).","In this paper, we introduce a spatial-spectral integrated diffusion model for the remote sensing pansharpening task, called SSDiff, which considers the pansharpening process as the fusion process of spatial and spectral components from the perspective of subspace decomposition.","Specifically, SSDiff utilizes spatial and spectral branches to learn spatial details and spectral features separately, then employs a designed alternating projection fusion module (APFM) to accomplish the fusion.","Furthermore, we propose a frequency modulation inter-branch module (FMIM) to modulate the frequency distribution between branches.","The two components of SSDiff can perform favorably against the APFM when utilizing a LoRA-like branch-wise alternative fine-tuning method.","It refines SSDiff to capture component-discriminating features more sufficiently.","Finally, extensive experiments on four commonly used datasets, i.e., WorldView-3, WorldView-2, GaoFen-2, and QuickBird, demonstrate the superiority of SSDiff both visually and quantitatively.","The code will be made open source after possible acceptance."],"url":"http://arxiv.org/abs/2404.11537v1","category":"cs.CV"}
{"created":"2024-04-17 15:58:17","title":"Mixing Time of Open Quantum Systems via Hypocoercivity","abstract":"Understanding the mixing of open quantum systems is a fundamental problem in physics and quantum information science. Existing approaches for estimating the mixing time often rely on the spectral gap estimation of the Lindbladian generator, which can be challenging to obtain in practice. We propose a novel theoretical framework to estimate the mixing time of open quantum systems that treats the Hamiltonian and dissipative part separately, thus circumventing the need for a priori estimation of the spectral gap of the full Lindbladian generator. The technique is based on the construction of an energy functional inspired by the hypocoercivity of (classical) kinetic theory.","sentences":["Understanding the mixing of open quantum systems is a fundamental problem in physics and quantum information science.","Existing approaches for estimating the mixing time often rely on the spectral gap estimation of the Lindbladian generator, which can be challenging to obtain in practice.","We propose a novel theoretical framework to estimate the mixing time of open quantum systems that treats the Hamiltonian and dissipative part separately, thus circumventing the need for a priori estimation of the spectral gap of the full Lindbladian generator.","The technique is based on the construction of an energy functional inspired by the hypocoercivity of (classical) kinetic theory."],"url":"http://arxiv.org/abs/2404.11503v1","category":"quant-ph"}
{"created":"2024-04-17 13:35:52","title":"Non-hermitian magnonic knobbing between electromagnetically induced reflection and transparancy","abstract":"Manipulation of wave propagation through open resonant systems has attracted tremendous interest. When accessible to the open system, the system under study is prone to tempering to out of equilibrium, and a lack of reciprocity is the rule rather than the exception. Open systems correspond to non-hermitian Hamiltonians with very unique properties such as resulting exceptional points and ideal isolation. Here, we have found a highly sensitive modulation for the intersection of resonant patch antennas with respect to cavity magnonic coupling by means of an open coupling system of three resonant modes. Two types of crossings are implemented in this study: the first type of crossing remotely controls the sharp switching of the transmission line 's transmittance, while regulating the repulsive behavior of its zero-reflection states. The second type of crossing corresponds to the modulation of non-reciprocal phase transitions, which enables a more desirable isolation effect. Three different coupling models are realized by a non-Hermitian scattering Hamiltonian, revealing distinct spatial overlaps between modes. This elucidates that dissipative coupling of at least two modes to the environment is crucial for non-reciprocal transport. Our work not only reveals the versatility of cavity magnonic systems but also provides a way to design functional devices for general wave optics using patch antenna crossings.","sentences":["Manipulation of wave propagation through open resonant systems has attracted tremendous interest.","When accessible to the open system, the system under study is prone to tempering to out of equilibrium, and a lack of reciprocity is the rule rather than the exception.","Open systems correspond to non-hermitian Hamiltonians with very unique properties such as resulting exceptional points and ideal isolation.","Here, we have found a highly sensitive modulation for the intersection of resonant patch antennas with respect to cavity magnonic coupling by means of an open coupling system of three resonant modes.","Two types of crossings are implemented in this study: the first type of crossing remotely controls the sharp switching of the transmission line 's transmittance, while regulating the repulsive behavior of its zero-reflection states.","The second type of crossing corresponds to the modulation of non-reciprocal phase transitions, which enables a more desirable isolation effect.","Three different coupling models are realized by a non-Hermitian scattering Hamiltonian, revealing distinct spatial overlaps between modes.","This elucidates that dissipative coupling of at least two modes to the environment is crucial for non-reciprocal transport.","Our work not only reveals the versatility of cavity magnonic systems but also provides a way to design functional devices for general wave optics using patch antenna crossings."],"url":"http://arxiv.org/abs/2404.11380v1","category":"physics.app-ph"}
{"created":"2024-04-17 13:09:33","title":"Distributed Fractional Bayesian Learning for Adaptive Optimization","abstract":"This paper considers a distributed adaptive optimization problem, where all agents only have access to their local cost functions with a common unknown parameter, whereas they mean to collaboratively estimate the true parameter and find the optimal solution over a connected network. A general mathematical framework for such a problem has not been studied yet. We aim to provide valuable insights for addressing parameter uncertainty in distributed optimization problems and simultaneously find the optimal solution. Thus, we propose a novel Prediction while Optimization scheme, which utilizes distributed fractional Bayesian learning through weighted averaging on the log-beliefs to update the beliefs of unknown parameters, and distributed gradient descent for renewing the estimation of the optimal solution. Then under suitable assumptions, we prove that all agents' beliefs and decision variables converge almost surely to the true parameter and the optimal solution under the true parameter, respectively. We further establish a sublinear convergence rate for the belief sequence. Finally, numerical experiments are implemented to corroborate the theoretical analysis.","sentences":["This paper considers a distributed adaptive optimization problem, where all agents only have access to their local cost functions with a common unknown parameter, whereas they mean to collaboratively estimate the true parameter and find the optimal solution over a connected network.","A general mathematical framework for such a problem has not been studied yet.","We aim to provide valuable insights for addressing parameter uncertainty in distributed optimization problems and simultaneously find the optimal solution.","Thus, we propose a novel Prediction while Optimization scheme, which utilizes distributed fractional Bayesian learning through weighted averaging on the log-beliefs to update the beliefs of unknown parameters, and distributed gradient descent for renewing the estimation of the optimal solution.","Then under suitable assumptions, we prove that all agents' beliefs and decision variables converge almost surely to the true parameter and the optimal solution under the true parameter, respectively.","We further establish a sublinear convergence rate for the belief sequence.","Finally, numerical experiments are implemented to corroborate the theoretical analysis."],"url":"http://arxiv.org/abs/2404.11354v1","category":"math.OC"}
{"created":"2024-04-17 13:07:10","title":"Farthest Point Sampling in Property Designated Chemical Feature Space as a General Strategy for Enhancing the Machine Learning Model Performance for Small Scale Chemical Dataset","abstract":"Machine learning model development in chemistry and materials science often grapples with the challenge of small scale, unbalanced labelled datasets, a common limitation in scientific experiments. These dataset imbalances can precipitate overfit ting and diminish model generalization. Our study explores the efficacy of the farthest point sampling (FPS) strategy within target ed chemical feature spaces, demonstrating its capacity to generate well-distributed training datasets and consequently enhance model performance. We rigorously evaluated this strategy across various machine learning models, including artificial neural net works (ANN), support vector machines (SVM), and random forests (RF), using datasets encapsulating physicochemical properties like standard boiling points and enthalpy of vaporization. Our findings reveal that FPS-based models consistently surpass those trained via random sampling, exhibiting superior predictive accuracy and robustness, alongside a marked reduction in overfitting. This improvement is particularly pronounced in smaller training datasets, attributable to increased diversity within the training data's chemical feature space. Consequently, FPS emerges as a universally effective and adaptable approach in approaching high performance machine learning models by small and biased experimental datasets prevalent in chemistry and materials science.","sentences":["Machine learning model development in chemistry and materials science often grapples with the challenge of small scale, unbalanced labelled datasets, a common limitation in scientific experiments.","These dataset imbalances can precipitate overfit ting and diminish model generalization.","Our study explores the efficacy of the farthest point sampling (FPS) strategy within target ed chemical feature spaces, demonstrating its capacity to generate well-distributed training datasets and consequently enhance model performance.","We rigorously evaluated this strategy across various machine learning models, including artificial neural net works (ANN), support vector machines (SVM), and random forests (RF), using datasets encapsulating physicochemical properties like standard boiling points and enthalpy of vaporization.","Our findings reveal that FPS-based models consistently surpass those trained via random sampling, exhibiting superior predictive accuracy and robustness, alongside a marked reduction in overfitting.","This improvement is particularly pronounced in smaller training datasets, attributable to increased diversity within the training data's chemical feature space.","Consequently, FPS emerges as a universally effective and adaptable approach in approaching high performance machine learning models by small and biased experimental datasets prevalent in chemistry and materials science."],"url":"http://arxiv.org/abs/2404.11348v1","category":"physics.chem-ph"}
{"created":"2024-04-17 12:56:38","title":"Global topological synchronization of weighted simplicial complexes","abstract":"Higher-order networks are able to capture the many-body interactions present in complex systems and to unveil new fundamental phenomena revealing the rich interplay between topology, geometry, and dynamics. Simplicial complexes are higher-order networks that encode higher-order topology and dynamics of complex systems. Specifically, simplicial complexes can sustain topological signals, i.e., dynamical variables not only defined on nodes of the network but also on their edges, triangles, and so on. Topological signals can undergo collective phenomena such as synchronization, however, only some higher-order network topologies can sustain global synchronization of topological signals. Here we consider global topological synchronization of topological signals on weighted simplicial complexes. We demonstrate that topological signals can globally synchronize on weighted simplicial complexes, even if they are odd-dimensional, e.g., edge signals, overcoming thus a limitation of the unweighted case. These results thus demonstrate that weighted simplicial complexes are more advantageous for observing these collective phenomena than their unweighted counterpart. In particular, we present two weighted simplicial complexes the Weighted Triangulated Torus and the Weighted Waffle. We completely characterize their higher-order spectral properties and we demonstrate that, under suitable conditions on their weights, they can sustain global synchronization of edge signals. Our results are interpreted geometrically by showing, among the other results, that in some cases edge weights can be associated with the lengths of the sides of curved simplices.","sentences":["Higher-order networks are able to capture the many-body interactions present in complex systems and to unveil new fundamental phenomena revealing the rich interplay between topology, geometry, and dynamics.","Simplicial complexes are higher-order networks that encode higher-order topology and dynamics of complex systems.","Specifically, simplicial complexes can sustain topological signals, i.e., dynamical variables not only defined on nodes of the network but also on their edges, triangles, and so on.","Topological signals can undergo collective phenomena such as synchronization, however, only some higher-order network topologies can sustain global synchronization of topological signals.","Here we consider global topological synchronization of topological signals on weighted simplicial complexes.","We demonstrate that topological signals can globally synchronize on weighted simplicial complexes, even if they are odd-dimensional, e.g., edge signals, overcoming thus a limitation of the unweighted case.","These results thus demonstrate that weighted simplicial complexes are more advantageous for observing these collective phenomena than their unweighted counterpart.","In particular, we present two weighted simplicial complexes the Weighted Triangulated Torus and the Weighted Waffle.","We completely characterize their higher-order spectral properties and we demonstrate that, under suitable conditions on their weights, they can sustain global synchronization of edge signals.","Our results are interpreted geometrically by showing, among the other results, that in some cases edge weights can be associated with the lengths of the sides of curved simplices."],"url":"http://arxiv.org/abs/2404.11337v1","category":"cond-mat.dis-nn"}
{"created":"2024-04-17 11:55:43","title":"Inductive Cognitive Diagnosis for Fast Student Learning in Web-Based Online Intelligent Education Systems","abstract":"Cognitive diagnosis aims to gauge students' mastery levels based on their response logs. Serving as a pivotal module in web-based online intelligent education systems (WOIESs), it plays an upstream and fundamental role in downstream tasks like learning item recommendation and computerized adaptive testing. WOIESs are open learning environment where numerous new students constantly register and complete exercises. In WOIESs, efficient cognitive diagnosis is crucial to fast feedback and accelerating student learning. However, the existing cognitive diagnosis methods always employ intrinsically transductive student-specific embeddings, which become slow and costly due to retraining when dealing with new students who are unseen during training. To this end, this paper proposes an inductive cognitive diagnosis model (ICDM) for fast new students' mastery levels inference in WOIESs. Specifically, in ICDM, we propose a novel student-centered graph (SCG). Rather than inferring mastery levels through updating student-specific embedding, we derive the inductive mastery levels as the aggregated outcomes of students' neighbors in SCG. Namely, SCG enables to shift the task from finding the most suitable student-specific embedding that fits the response logs to finding the most suitable representations for different node types in SCG, and the latter is more efficient since it no longer requires retraining. To obtain this representation, ICDM consists of a construction-aggregation-generation-transformation process to learn the final representation of students, exercises and concepts. Extensive experiments across real-world datasets show that, compared with the existing cognitive diagnosis methods that are always transductive, ICDM is much more faster while maintains the competitive inference performance for new students.","sentences":["Cognitive diagnosis aims to gauge students' mastery levels based on their response logs.","Serving as a pivotal module in web-based online intelligent education systems (WOIESs), it plays an upstream and fundamental role in downstream tasks like learning item recommendation and computerized adaptive testing.","WOIESs are open learning environment where numerous new students constantly register and complete exercises.","In WOIESs, efficient cognitive diagnosis is crucial to fast feedback and accelerating student learning.","However, the existing cognitive diagnosis methods always employ intrinsically transductive student-specific embeddings, which become slow and costly due to retraining when dealing with new students who are unseen during training.","To this end, this paper proposes an inductive cognitive diagnosis model (ICDM) for fast new students' mastery levels inference in WOIESs.","Specifically, in ICDM, we propose a novel student-centered graph (SCG).","Rather than inferring mastery levels through updating student-specific embedding, we derive the inductive mastery levels as the aggregated outcomes of students' neighbors in SCG.","Namely, SCG enables to shift the task from finding the most suitable student-specific embedding that fits the response logs to finding the most suitable representations for different node types in SCG, and the latter is more efficient since it no longer requires retraining.","To obtain this representation, ICDM consists of a construction-aggregation-generation-transformation process to learn the final representation of students, exercises and concepts.","Extensive experiments across real-world datasets show that, compared with the existing cognitive diagnosis methods that are always transductive, ICDM is much more faster while maintains the competitive inference performance for new students."],"url":"http://arxiv.org/abs/2404.11290v1","category":"cs.AI"}
{"created":"2024-04-17 11:20:14","title":"DACAD: Domain Adaptation Contrastive Learning for Anomaly Detection in Multivariate Time Series","abstract":"Time series anomaly detection (TAD) faces a significant challenge due to the scarcity of labelled data, which hinders the development of accurate detection models. Unsupervised domain adaptation (UDA) addresses this challenge by leveraging a labelled dataset from a related domain to detect anomalies in a target dataset. Existing domain adaptation techniques assume that the number of anomalous classes does not change between the source and target domains. In this paper, we propose a novel Domain Adaptation Contrastive learning for Anomaly Detection in multivariate time series (DACAD) model to address this issue by combining UDA and contrastive representation learning. DACAD's approach includes an anomaly injection mechanism that introduces various types of synthetic anomalies, enhancing the model's ability to generalise across unseen anomalous classes in different domains. This method significantly broadens the model's adaptability and robustness. Additionally, we propose a supervised contrastive loss for the source domain and a self-supervised contrastive triplet loss for the target domain, improving comprehensive feature representation learning and extraction of domain-invariant features. Finally, an effective Centre-based Entropy Classifier (CEC) is proposed specifically for anomaly detection, facilitating accurate learning of normal boundaries in the source domain. Our extensive evaluation across multiple real-world datasets against leading models in time series anomaly detection and UDA underscores DACAD's effectiveness. The results validate DACAD's superiority in transferring knowledge across domains and its potential to mitigate the challenge of limited labelled data in time series anomaly detection.","sentences":["Time series anomaly detection (TAD) faces a significant challenge due to the scarcity of labelled data, which hinders the development of accurate detection models.","Unsupervised domain adaptation (UDA) addresses this challenge by leveraging a labelled dataset from a related domain to detect anomalies in a target dataset.","Existing domain adaptation techniques assume that the number of anomalous classes does not change between the source and target domains.","In this paper, we propose a novel Domain Adaptation Contrastive learning for Anomaly Detection in multivariate time series (DACAD) model to address this issue by combining UDA and contrastive representation learning.","DACAD's approach includes an anomaly injection mechanism that introduces various types of synthetic anomalies, enhancing the model's ability to generalise across unseen anomalous classes in different domains.","This method significantly broadens the model's adaptability and robustness.","Additionally, we propose a supervised contrastive loss for the source domain and a self-supervised contrastive triplet loss for the target domain, improving comprehensive feature representation learning and extraction of domain-invariant features.","Finally, an effective Centre-based Entropy Classifier (CEC) is proposed specifically for anomaly detection, facilitating accurate learning of normal boundaries in the source domain.","Our extensive evaluation across multiple real-world datasets against leading models in time series anomaly detection and UDA underscores DACAD's effectiveness.","The results validate DACAD's superiority in transferring knowledge across domains and its potential to mitigate the challenge of limited labelled data in time series anomaly detection."],"url":"http://arxiv.org/abs/2404.11269v1","category":"cs.LG"}
{"created":"2024-04-17 10:51:36","title":"Learning Social Navigation from Demonstrations with Deep Neural Networks","abstract":"Traditional path-planning techniques treat humans as obstacles. This has changed since robots started to enter human environments. On modern robots, social navigation has become an important aspect of navigation systems. To use learning-based techniques to achieve social navigation, a powerful framework that is capable of representing complex functions with as few data as possible is required. In this study, we benefited from recent advances in deep learning at both global and local planning levels to achieve human-aware navigation on a simulated robot. Two distinct deep models are trained with respective objectives: one for global planning and one for local planning. These models are then employed in the simulated robot. In the end, it has been shown that our model can successfully carry out both global and local planning tasks. We have shown that our system could generate paths that successfully reach targets while avoiding obstacles with better performance compared to feed-forward neural networks.","sentences":["Traditional path-planning techniques treat humans as obstacles.","This has changed since robots started to enter human environments.","On modern robots, social navigation has become an important aspect of navigation systems.","To use learning-based techniques to achieve social navigation, a powerful framework that is capable of representing complex functions with as few data as possible is required.","In this study, we benefited from recent advances in deep learning at both global and local planning levels to achieve human-aware navigation on a simulated robot.","Two distinct deep models are trained with respective objectives: one for global planning and one for local planning.","These models are then employed in the simulated robot.","In the end, it has been shown that our model can successfully carry out both global and local planning tasks.","We have shown that our system could generate paths that successfully reach targets while avoiding obstacles with better performance compared to feed-forward neural networks."],"url":"http://arxiv.org/abs/2404.11246v1","category":"cs.RO"}
{"created":"2024-04-17 10:19:15","title":"In-Context Learning State Vector with Inner and Momentum Optimization","abstract":"Large Language Models (LLMs) have exhibited an impressive ability to perform In-Context Learning (ICL) from only a few examples. Recent works have indicated that the functions learned by ICL can be represented through compressed vectors derived from the transformer. However, the working mechanisms and optimization of these vectors are yet to be thoroughly explored. In this paper, we address this gap by presenting a comprehensive analysis of these compressed vectors, drawing parallels to the parameters trained with gradient descent, and introduce the concept of state vector. Inspired by the works on model soup and momentum-based gradient descent, we propose inner and momentum optimization methods that are applied to refine the state vector progressively as test-time adaptation. Moreover, we simulate state vector aggregation in the multiple example setting, where demonstrations comprising numerous examples are usually too lengthy for regular ICL, and further propose a divide-and-conquer aggregation method to address this challenge. We conduct extensive experiments using Llama-2 and GPT-J in both zero-shot setting and few-shot setting. The experimental results show that our optimization method effectively enhances the state vector and achieves the state-of-the-art performance on diverse tasks. Code is available at https://github.com/HITsz-TMG/ICL-State-Vector","sentences":["Large Language Models (LLMs) have exhibited an impressive ability to perform In-Context Learning (ICL) from only a few examples.","Recent works have indicated that the functions learned by ICL can be represented through compressed vectors derived from the transformer.","However, the working mechanisms and optimization of these vectors are yet to be thoroughly explored.","In this paper, we address this gap by presenting a comprehensive analysis of these compressed vectors, drawing parallels to the parameters trained with gradient descent, and introduce the concept of state vector.","Inspired by the works on model soup and momentum-based gradient descent, we propose inner and momentum optimization methods that are applied to refine the state vector progressively as test-time adaptation.","Moreover, we simulate state vector aggregation in the multiple example setting, where demonstrations comprising numerous examples are usually too lengthy for regular ICL, and further propose a divide-and-conquer aggregation method to address this challenge.","We conduct extensive experiments using Llama-2 and GPT-J in both zero-shot setting and few-shot setting.","The experimental results show that our optimization method effectively enhances the state vector and achieves the state-of-the-art performance on diverse tasks.","Code is available at https://github.com/HITsz-TMG/ICL-State-Vector"],"url":"http://arxiv.org/abs/2404.11225v1","category":"cs.CL"}
{"created":"2024-04-17 09:57:40","title":"Revisiting Noise Resilience Strategies in Gesture Recognition: Short-Term Enhancement in Surface Electromyographic Signal Analysis","abstract":"Gesture recognition based on surface electromyography (sEMG) has been gaining importance in many 3D Interactive Scenes. However, sEMG is easily influenced by various forms of noise in real-world environments, leading to challenges in providing long-term stable interactions through sEMG. Existing methods often struggle to enhance model noise resilience through various predefined data augmentation techniques. In this work, we revisit the problem from a short term enhancement perspective to improve precision and robustness against various common noisy scenarios with learnable denoise using sEMG intrinsic pattern information and sliding-window attention. We propose a Short Term Enhancement Module(STEM) which can be easily integrated with various models. STEM offers several benefits: 1) Learnable denoise, enabling noise reduction without manual data augmentation; 2) Scalability, adaptable to various models; and 3) Cost-effectiveness, achieving short-term enhancement through minimal weight-sharing in an efficient attention mechanism. In particular, we incorporate STEM into a transformer, creating the Short Term Enhanced Transformer (STET). Compared with best-competing approaches, the impact of noise on STET is reduced by more than 20%. We also report promising results on both classification and regression datasets and demonstrate that STEM generalizes across different gesture recognition tasks.","sentences":["Gesture recognition based on surface electromyography (sEMG) has been gaining importance in many 3D Interactive Scenes.","However, sEMG is easily influenced by various forms of noise in real-world environments, leading to challenges in providing long-term stable interactions through sEMG.","Existing methods often struggle to enhance model noise resilience through various predefined data augmentation techniques.","In this work, we revisit the problem from a short term enhancement perspective to improve precision and robustness against various common noisy scenarios with learnable denoise using sEMG intrinsic pattern information and sliding-window attention.","We propose a Short Term Enhancement Module(STEM) which can be easily integrated with various models.","STEM offers several benefits: 1) Learnable denoise, enabling noise reduction without manual data augmentation; 2) Scalability, adaptable to various models; and 3) Cost-effectiveness, achieving short-term enhancement through minimal weight-sharing in an efficient attention mechanism.","In particular, we incorporate STEM into a transformer, creating the Short Term Enhanced Transformer (STET).","Compared with best-competing approaches, the impact of noise on STET is reduced by more than 20%.","We also report promising results on both classification and regression datasets and demonstrate that STEM generalizes across different gesture recognition tasks."],"url":"http://arxiv.org/abs/2404.11213v1","category":"eess.SP"}
{"created":"2024-04-17 09:39:07","title":"Exploring the Transferability of Visual Prompting for Multimodal Large Language Models","abstract":"Although Multimodal Large Language Models (MLLMs) have demonstrated promising versatile capabilities, their performance is still inferior to specialized models on downstream tasks, which makes adaptation necessary to enhance their utility. However, fine-tuning methods require independent training for every model, leading to huge computation and memory overheads. In this paper, we propose a novel setting where we aim to improve the performance of diverse MLLMs with a group of shared parameters optimized for a downstream task. To achieve this, we propose Transferable Visual Prompting (TVP), a simple and effective approach to generate visual prompts that can transfer to different models and improve their performance on downstream tasks after trained on only one model. We introduce two strategies to address the issue of cross-model feature corruption of existing visual prompting methods and enhance the transferability of the learned prompts, including 1) Feature Consistency Alignment: which imposes constraints to the prompted feature changes to maintain task-agnostic knowledge; 2) Task Semantics Enrichment: which encourages the prompted images to contain richer task-specific semantics with language guidance. We validate the effectiveness of TVP through extensive experiments with 6 modern MLLMs on a wide variety of tasks ranging from object recognition and counting to multimodal reasoning and hallucination correction.","sentences":["Although Multimodal Large Language Models (MLLMs) have demonstrated promising versatile capabilities, their performance is still inferior to specialized models on downstream tasks, which makes adaptation necessary to enhance their utility.","However, fine-tuning methods require independent training for every model, leading to huge computation and memory overheads.","In this paper, we propose a novel setting where we aim to improve the performance of diverse MLLMs with a group of shared parameters optimized for a downstream task.","To achieve this, we propose Transferable Visual Prompting (TVP), a simple and effective approach to generate visual prompts that can transfer to different models and improve their performance on downstream tasks after trained on only one model.","We introduce two strategies to address the issue of cross-model feature corruption of existing visual prompting methods and enhance the transferability of the learned prompts, including 1) Feature Consistency Alignment: which imposes constraints to the prompted feature changes to maintain task-agnostic knowledge; 2) Task Semantics Enrichment: which encourages the prompted images to contain richer task-specific semantics with language guidance.","We validate the effectiveness of TVP through extensive experiments with 6 modern MLLMs on a wide variety of tasks ranging from object recognition and counting to multimodal reasoning and hallucination correction."],"url":"http://arxiv.org/abs/2404.11207v1","category":"cs.CV"}
{"created":"2024-04-17 09:29:24","title":"Uniform Regularity for Incompressible MHD Equations in a Bounded Domain with Curved Boundary in 3D","abstract":"For the initial boundary problem of the incompressible MHD equations in a bounded domain with general curved boundary in 3D with the general Navier-slip boundary conditions for the velocity field and the perfect conducting condition for the magnetic field, we establish the uniform regularity of conormal Sobolev norms and Lipschitz norms to addressing the anisotropic regularity of tangential and normal directions, which enable us to prove the vanishing dissipation limit as the viscosity and the magnetic diffusion coefficients tend to zero. We overcome the difficulties caused by the intricate interaction of boundary curvature, velocity field, and magnetic fields and resolve the issue caused by the problem that the viscosity and the magnetic diffusion coefficients are not required to be equal.","sentences":["For the initial boundary problem of the incompressible MHD equations in a bounded domain with general curved boundary in 3D with the general Navier-slip boundary conditions for the velocity field and the perfect conducting condition for the magnetic field, we establish the uniform regularity of conormal Sobolev norms and Lipschitz norms to addressing the anisotropic regularity of tangential and normal directions, which enable us to prove the vanishing dissipation limit as the viscosity and the magnetic diffusion coefficients tend to zero.","We overcome the difficulties caused by the intricate interaction of boundary curvature, velocity field, and magnetic fields and resolve the issue caused by the problem that the viscosity and the magnetic diffusion coefficients are not required to be equal."],"url":"http://arxiv.org/abs/2404.11197v1","category":"math.AP"}
{"created":"2024-04-17 08:42:42","title":"Deep Neural Networks via Complex Network Theory: a Perspective","abstract":"Deep Neural Networks (DNNs) can be represented as graphs whose links and vertices iteratively process data and solve tasks sub-optimally. Complex Network Theory (CNT), merging statistical physics with graph theory, provides a method for interpreting neural networks by analysing their weights and neuron structures. However, classic works adapt CNT metrics that only permit a topological analysis as they do not account for the effect of the input data. In addition, CNT metrics have been applied to a limited range of architectures, mainly including Fully Connected neural networks. In this work, we extend the existing CNT metrics with measures that sample from the DNNs' training distribution, shifting from a purely topological analysis to one that connects with the interpretability of deep learning. For the novel metrics, in addition to the existing ones, we provide a mathematical formalisation for Fully Connected, AutoEncoder, Convolutional and Recurrent neural networks, of which we vary the activation functions and the number of hidden layers. We show that these metrics differentiate DNNs based on the architecture, the number of hidden layers, and the activation function. Our contribution provides a method rooted in physics for interpreting DNNs that offers insights beyond the traditional input-output relationship and the CNT topological analysis.","sentences":["Deep Neural Networks (DNNs) can be represented as graphs whose links and vertices iteratively process data and solve tasks sub-optimally.","Complex Network Theory (CNT), merging statistical physics with graph theory, provides a method for interpreting neural networks by analysing their weights and neuron structures.","However, classic works adapt CNT metrics that only permit a topological analysis as they do not account for the effect of the input data.","In addition, CNT metrics have been applied to a limited range of architectures, mainly including Fully Connected neural networks.","In this work, we extend the existing CNT metrics with measures that sample from the DNNs' training distribution, shifting from a purely topological analysis to one that connects with the interpretability of deep learning.","For the novel metrics, in addition to the existing ones, we provide a mathematical formalisation for Fully Connected, AutoEncoder, Convolutional and Recurrent neural networks, of which we vary the activation functions and the number of hidden layers.","We show that these metrics differentiate DNNs based on the architecture, the number of hidden layers, and the activation function.","Our contribution provides a method rooted in physics for interpreting DNNs that offers insights beyond the traditional input-output relationship and the CNT topological analysis."],"url":"http://arxiv.org/abs/2404.11172v2","category":"cs.LG"}
{"created":"2024-04-17 08:25:42","title":"AI-equipped scanning probe microscopy for autonomous site-specific atomic-level characterization at room temperature","abstract":"We present an advanced scanning probe microscopy system enhanced with artificial intelligence (AI-SPM) designed for self-driving atomic-scale measurements. This system expertly identifies and manipulates atomic positions with high precision, autonomously performing tasks such as spectroscopic data acquisition and atomic adjustment. An outstanding feature of AI-SPM is its ability to detect and adapt to surface defects, targeting or avoiding them as necessary. It's also engineered to address typical challenges such as positional drift and tip apex atomic variations due to the thermal effect, ensuring accurate, site-specific surface analyses. Our tests under the demanding conditions of room temperature have demonstrated the robustness of the system, successfully navigating thermal drift and tip fluctuations. During these tests on the Si(111)-(7x7) surface, AI-SPM autonomously identified defect-free regions and performed a large number of current-voltage spectroscopy measurements at different adatom sites, while autonomously compensating for thermal drift and monitoring probe health. These experiments produce extensive data sets that are critical for reliable materials characterization and demonstrate the potential of AI-SPM to significantly improve data acquisition. The integration of AI into SPM technologies represents a step toward more effective, precise and reliable atomic-level surface analysis, revolutionizing materials characterization methods.","sentences":["We present an advanced scanning probe microscopy system enhanced with artificial intelligence (AI-SPM) designed for self-driving atomic-scale measurements.","This system expertly identifies and manipulates atomic positions with high precision, autonomously performing tasks such as spectroscopic data acquisition and atomic adjustment.","An outstanding feature of AI-SPM is its ability to detect and adapt to surface defects, targeting or avoiding them as necessary.","It's also engineered to address typical challenges such as positional drift and tip apex atomic variations due to the thermal effect, ensuring accurate, site-specific surface analyses.","Our tests under the demanding conditions of room temperature have demonstrated the robustness of the system, successfully navigating thermal drift and tip fluctuations.","During these tests on the Si(111)-(7x7) surface, AI-SPM autonomously identified defect-free regions and performed a large number of current-voltage spectroscopy measurements at different adatom sites, while autonomously compensating for thermal drift and monitoring probe health.","These experiments produce extensive data sets that are critical for reliable materials characterization and demonstrate the potential of AI-SPM to significantly improve data acquisition.","The integration of AI into SPM technologies represents a step toward more effective, precise and reliable atomic-level surface analysis, revolutionizing materials characterization methods."],"url":"http://arxiv.org/abs/2404.11162v1","category":"physics.comp-ph"}
{"created":"2024-04-17 17:59:55","title":"Dynamic Typography: Bringing Text to Life via Video Diffusion Prior","abstract":"Text animation serves as an expressive medium, transforming static communication into dynamic experiences by infusing words with motion to evoke emotions, emphasize meanings, and construct compelling narratives. Crafting animations that are semantically aware poses significant challenges, demanding expertise in graphic design and animation. We present an automated text animation scheme, termed \"Dynamic Typography\", which combines two challenging tasks. It deforms letters to convey semantic meaning and infuses them with vibrant movements based on user prompts. Our technique harnesses vector graphics representations and an end-to-end optimization-based framework. This framework employs neural displacement fields to convert letters into base shapes and applies per-frame motion, encouraging coherence with the intended textual concept. Shape preservation techniques and perceptual loss regularization are employed to maintain legibility and structural integrity throughout the animation process. We demonstrate the generalizability of our approach across various text-to-video models and highlight the superiority of our end-to-end methodology over baseline methods, which might comprise separate tasks. Through quantitative and qualitative evaluations, we demonstrate the effectiveness of our framework in generating coherent text animations that faithfully interpret user prompts while maintaining readability. Our code is available at: https://animate-your-word.github.io/demo/.","sentences":["Text animation serves as an expressive medium, transforming static communication into dynamic experiences by infusing words with motion to evoke emotions, emphasize meanings, and construct compelling narratives.","Crafting animations that are semantically aware poses significant challenges, demanding expertise in graphic design and animation.","We present an automated text animation scheme, termed \"Dynamic Typography\", which combines two challenging tasks.","It deforms letters to convey semantic meaning and infuses them with vibrant movements based on user prompts.","Our technique harnesses vector graphics representations and an end-to-end optimization-based framework.","This framework employs neural displacement fields to convert letters into base shapes and applies per-frame motion, encouraging coherence with the intended textual concept.","Shape preservation techniques and perceptual loss regularization are employed to maintain legibility and structural integrity throughout the animation process.","We demonstrate the generalizability of our approach across various text-to-video models and highlight the superiority of our end-to-end methodology over baseline methods, which might comprise separate tasks.","Through quantitative and qualitative evaluations, we demonstrate the effectiveness of our framework in generating coherent text animations that faithfully interpret user prompts while maintaining readability.","Our code is available at: https://animate-your-word.github.io/demo/."],"url":"http://arxiv.org/abs/2404.11614v2","category":"cs.CV"}
{"created":"2024-04-17 17:52:02","title":"Isoparametric Virtual Element Methods","abstract":"We present two approaches to constructing isoparametric Virtual Element Methods of arbitrary order for linear elliptic partial differential equations on general two-dimensional domains. The first method approximates the variational problem transformed onto a computational reference domain. The second method computes a virtual domain and uses bespoke polynomial approximation operators to construct a computable method. Both methods are shown to converge optimally, a behaviour confirmed in practice for the solution of problems posed on curved domains.","sentences":["We present two approaches to constructing isoparametric Virtual Element Methods of arbitrary order for linear elliptic partial differential equations on general two-dimensional domains.","The first method approximates the variational problem transformed onto a computational reference domain.","The second method computes a virtual domain and uses bespoke polynomial approximation operators to construct a computable method.","Both methods are shown to converge optimally, a behaviour confirmed in practice for the solution of problems posed on curved domains."],"url":"http://arxiv.org/abs/2404.11603v2","category":"math.NA"}
{"created":"2024-04-17 17:29:03","title":"Quasinormal Modes in Modified Gravity using Physics-Informed Neural Networks","abstract":"In this paper, we apply a novel approach based on physics-informed neural networks to the computation of quasinormal modes of black hole solutions in modified gravity. In particular, we focus on the case of Einstein-scalar-Gauss-Bonnet theory, with several choices of the coupling function between the scalar field and the Gauss-Bonnet invariant. This type of calculation introduces a number of challenges with respect to the case of General Relativity, mainly due to the extra complexity of the perturbation equations and to the fact that the background solution is known only numerically. The solution of these perturbation equations typically requires sophisticated numerical techniques that are not easy to develop in computational codes. We show that physics-informed neural networks have an accuracy which is comparable to traditional numerical methods in the case of numerical backgrounds, while being very simple to implement. Additionally, the use of GPU parallelization is straightforward thanks to the use of standard machine learning environments.","sentences":["In this paper, we apply a novel approach based on physics-informed neural networks to the computation of quasinormal modes of black hole solutions in modified gravity.","In particular, we focus on the case of Einstein-scalar-Gauss-Bonnet theory, with several choices of the coupling function between the scalar field and the Gauss-Bonnet invariant.","This type of calculation introduces a number of challenges with respect to the case of General Relativity, mainly due to the extra complexity of the perturbation equations and to the fact that the background solution is known only numerically.","The solution of these perturbation equations typically requires sophisticated numerical techniques that are not easy to develop in computational codes.","We show that physics-informed neural networks have an accuracy which is comparable to traditional numerical methods in the case of numerical backgrounds, while being very simple to implement.","Additionally, the use of GPU parallelization is straightforward thanks to the use of standard machine learning environments."],"url":"http://arxiv.org/abs/2404.11583v1","category":"gr-qc"}
{"created":"2024-04-17 16:32:13","title":"GenFighter: A Generative and Evolutive Textual Attack Removal","abstract":"Adversarial attacks pose significant challenges to deep neural networks (DNNs) such as Transformer models in natural language processing (NLP). This paper introduces a novel defense strategy, called GenFighter, which enhances adversarial robustness by learning and reasoning on the training classification distribution. GenFighter identifies potentially malicious instances deviating from the distribution, transforms them into semantically equivalent instances aligned with the training data, and employs ensemble techniques for a unified and robust response. By conducting extensive experiments, we show that GenFighter outperforms state-of-the-art defenses in accuracy under attack and attack success rate metrics. Additionally, it requires a high number of queries per attack, making the attack more challenging in real scenarios. The ablation study shows that our approach integrates transfer learning, a generative/evolutive procedure, and an ensemble method, providing an effective defense against NLP adversarial attacks.","sentences":["Adversarial attacks pose significant challenges to deep neural networks (DNNs) such as Transformer models in natural language processing (NLP).","This paper introduces a novel defense strategy, called GenFighter, which enhances adversarial robustness by learning and reasoning on the training classification distribution.","GenFighter identifies potentially malicious instances deviating from the distribution, transforms them into semantically equivalent instances aligned with the training data, and employs ensemble techniques for a unified and robust response.","By conducting extensive experiments, we show that GenFighter outperforms state-of-the-art defenses in accuracy under attack and attack success rate metrics.","Additionally, it requires a high number of queries per attack, making the attack more challenging in real scenarios.","The ablation study shows that our approach integrates transfer learning, a generative/evolutive procedure, and an ensemble method, providing an effective defense against NLP adversarial attacks."],"url":"http://arxiv.org/abs/2404.11538v1","category":"cs.LG"}
{"created":"2024-04-17 16:15:43","title":"On effective constructions of existentially closed groups","abstract":"Existentially closed groups are, informally, groups that contain solutions to every consistent finite system of equations and inequations. They were introduced in 1951 in an algebraic context and subsequent research elucidated deep connections with group theory and computability theory. We continue this investigation, with particular emphasis on illuminating the relationship with computability theory.   In particular, we show that existentially closed groups can be built at the level of the halting problem and that this is optimal. Moreover, using the the theory of the enumeration degrees and some work of Martin Ziegler in computable group theory, we show that the previous result relativises in a somewhat subtle way. We then tease apart the complexity contributed by ``global'' and ``local'' structure, showing that the finitely generated subgroups have complexity at the level of the PA degrees. Finally, we investigate the computability-theoretic complexity of omitting the non-principal quantifier-free types from a list of types, from which we obtain an upper bound on the complexity of building two existentially closed groups that are ``as different as possible''.","sentences":["Existentially closed groups are, informally, groups that contain solutions to every consistent finite system of equations and inequations.","They were introduced in 1951 in an algebraic context and subsequent research elucidated deep connections with group theory and computability theory.","We continue this investigation, with particular emphasis on illuminating the relationship with computability theory.   ","In particular, we show that existentially closed groups can be built at the level of the halting problem and that this is optimal.","Moreover, using the the theory of the enumeration degrees and some work of Martin Ziegler in computable group theory, we show that the previous result relativises in a somewhat subtle way.","We then tease apart the complexity contributed by ``global'' and ``local'' structure, showing that the finitely generated subgroups have complexity at the level of the PA degrees.","Finally, we investigate the computability-theoretic complexity of omitting the non-principal quantifier-free types from a list of types, from which we obtain an upper bound on the complexity of building two existentially closed groups that are ``as different as possible''."],"url":"http://arxiv.org/abs/2404.11524v1","category":"math.LO"}
{"created":"2024-04-17 15:56:16","title":"Solving the Yang-Baxter, tetrahedron and higher simplex equations using Clifford algebras","abstract":"Bethe Ansatz was discoverd in 1932. Half a century later its algebraic structure was unearthed: Yang-Baxter equation was discovered, as well as its multidimensional generalizations [tetrahedron equation and $d$-simplex equations]. Here we describe a universal method to solve these equations using Clifford algebras. The Yang-Baxter equation ($d=2$), Zamalodchikov's tetrahedron equation ($d=3$) and the Bazhanov-Stroganov equation ($d=4$) are special cases. Our solutions form a linear space. This helps us to include spectral parameters. Potential applications are discussed.","sentences":["Bethe Ansatz was discoverd in 1932.","Half a century later its algebraic structure was unearthed: Yang-Baxter equation was discovered, as well as its multidimensional generalizations","[tetrahedron equation and $d$-simplex equations].","Here we describe a universal method to solve these equations using Clifford algebras.","The Yang-Baxter equation ($d=2$), Zamalodchikov's tetrahedron equation ($d=3$) and the Bazhanov-Stroganov equation ($d=4$) are special cases.","Our solutions form a linear space.","This helps us to include spectral parameters.","Potential applications are discussed."],"url":"http://arxiv.org/abs/2404.11501v1","category":"hep-th"}
{"created":"2024-04-17 15:40:17","title":"BSDE-based stochastic control for optimal reinsurance in a dynamic contagion model","abstract":"We investigate the optimal reinsurance problem in the risk model with jump clustering features introduced in [7]. This modeling framework is inspired by the concept initially proposed in [15], combining Hawkes and Cox processes with shot noise intensity models. Specifically, these processes describe self-exciting and externally excited jumps in the claim arrival intensity, respectively. The insurer aims to maximize the expected exponential utility of terminal wealth for general reinsurance contracts and reinsurance premiums. We discuss two different methodologies: the classical stochastic control approach based on the Hamilton-Jacobi-Bellman (HJB) equation and a backward stochastic differential equation (BSDE) approach. In a Markovian setting, differently from the classical HJB-approach, the BSDE method enables us to solve the problem without imposing any requirements for regularity on the associated value function. We provide a Verification Theorem in terms of a suitable BSDE driven by a two-dimensional marked point process and we prove an existence result relaying on the theory developed in [27] for stochastic Lipschitz generators. After discussing the optimal strategy for general reinsurance contracts and reinsurance premiums, we provide more explicit results in some relevant cases. Finally, we provide comparison results that highlight the heightened risk stemming from the self-exciting component in contrast to the externally-excited counterpart and discuss the monotonicity property of the value function.","sentences":["We investigate the optimal reinsurance problem in the risk model with jump clustering features introduced in [7].","This modeling framework is inspired by the concept initially proposed in [15], combining Hawkes and Cox processes with shot noise intensity models.","Specifically, these processes describe self-exciting and externally excited jumps in the claim arrival intensity, respectively.","The insurer aims to maximize the expected exponential utility of terminal wealth for general reinsurance contracts and reinsurance premiums.","We discuss two different methodologies: the classical stochastic control approach based on the Hamilton-Jacobi-Bellman (HJB) equation and a backward stochastic differential equation (BSDE) approach.","In a Markovian setting, differently from the classical HJB-approach, the BSDE method enables us to solve the problem without imposing any requirements for regularity on the associated value function.","We provide a Verification Theorem in terms of a suitable BSDE driven by a two-dimensional marked point process and we prove an existence result relaying on the theory developed in [27] for stochastic Lipschitz generators.","After discussing the optimal strategy for general reinsurance contracts and reinsurance premiums, we provide more explicit results in some relevant cases.","Finally, we provide comparison results that highlight the heightened risk stemming from the self-exciting component in contrast to the externally-excited counterpart and discuss the monotonicity property of the value function."],"url":"http://arxiv.org/abs/2404.11482v1","category":"math.OC"}
{"created":"2024-04-17 15:33:34","title":"Inverse problem in energy-dependent potentials using semi-classical methods","abstract":"Wave equations with energy-dependent potentials appear in many areas of physics, ranging from nuclear physics to black hole perturbation theory. In this work, we use the semi-classical WKB method to first revisit the computation of bound states of potential wells and reflection/transmission coefficients in terms of the Bohr-Sommerfeld rule and the Gamow formula. We then discuss the inverse problem, in which the latter observables are used as a starting point to reconstruct the properties of the potentials. By extending known inversion techniques to energy-dependent potentials, we demonstrate that so-called width-equivalent or WKB-equivalent potentials are not isospectral anymore. Instead, we explicitly demonstrate that constructing quasi-isospectral potentials with the inverse techniques is still possible. Those reconstructed, energy-independent potentials share key properties with the width-equivalent potentials. We report that including energy-dependent terms allows for a rich phenomenology, particularly for the energy-independent equivalent potentials.","sentences":["Wave equations with energy-dependent potentials appear in many areas of physics, ranging from nuclear physics to black hole perturbation theory.","In this work, we use the semi-classical WKB method to first revisit the computation of bound states of potential wells and reflection/transmission coefficients in terms of the Bohr-Sommerfeld rule and the Gamow formula.","We then discuss the inverse problem, in which the latter observables are used as a starting point to reconstruct the properties of the potentials.","By extending known inversion techniques to energy-dependent potentials, we demonstrate that so-called width-equivalent or WKB-equivalent potentials are not isospectral anymore.","Instead, we explicitly demonstrate that constructing quasi-isospectral potentials with the inverse techniques is still possible.","Those reconstructed, energy-independent potentials share key properties with the width-equivalent potentials.","We report that including energy-dependent terms allows for a rich phenomenology, particularly for the energy-independent equivalent potentials."],"url":"http://arxiv.org/abs/2404.11478v1","category":"hep-ph"}
{"created":"2024-04-17 15:16:50","title":"Conformal Killing cosmology with Sinyukov tensors: geometry and growth of structures","abstract":"We introduce perfect fluid Sinyukov-like tensors, a special kind of conformal Killing tensors, and prove a characterization of generalized RW spacetimes. In cosmological (RW) framework, we study the Friedmann equations of Conformal Killing Gravity. In addition to ordinary matter they contain a dark fluid and a Lambda term emergent from the conformal Killing extension. We then solve the equation for the evolution of the density contrast in matter-dominated universe. The main result is that the simultaneous presence of the dark sector and the Lambda term gives a deviation from Lambda-CDM and ordinary GR towards an enhancement of the overdensity growth.","sentences":["We introduce perfect fluid Sinyukov-like tensors, a special kind of conformal Killing tensors, and prove a characterization of generalized RW spacetimes.","In cosmological (RW) framework, we study the Friedmann equations of Conformal Killing Gravity.","In addition to ordinary matter they contain a dark fluid and a Lambda term emergent from the conformal Killing extension.","We then solve the equation for the evolution of the density contrast in matter-dominated universe.","The main result is that the simultaneous presence of the dark sector and the Lambda term gives a deviation from Lambda-CDM and ordinary GR towards an enhancement of the overdensity growth."],"url":"http://arxiv.org/abs/2404.11468v1","category":"gr-qc"}
{"created":"2024-04-17 14:47:34","title":"$SO(4)$ Symmetry in Hydrogen Atom with Spin","abstract":"As the simplest atom in nature, the hydrogen atom has been explored thoroughly from the perspective of non-relativistic quantum mechanics to relativistic quantum mechanics. Among the research on hydrogen atom, its energy level is the most basic, which can be obtained more conveniently predicated on the $SO(4)$ symmetry than the wave-equation resolution. Moreover, ``spin'' is another indispensable topic in quantum mechanics, appearing as an intrinsic degree of freedom. In this work, we generalize the quantum Runge-Lenz vector to a spin-dependent one, and then extract a novel Hamiltonian of hydrogen atom with spin based on the requirement of $SO(4)$ symmetry. Furthermore, the energy spectrum of hydrogen atom with spin potentials is also determined by the remarkable approach of $SO(4)$ symmetry. Our findings extend the ground of hydrogen atom, and may contribute to other complicated models based on hydrogen atom.","sentences":["As the simplest atom in nature, the hydrogen atom has been explored thoroughly from the perspective of non-relativistic quantum mechanics to relativistic quantum mechanics.","Among the research on hydrogen atom, its energy level is the most basic, which can be obtained more conveniently predicated on the $SO(4)$ symmetry than the wave-equation resolution.","Moreover, ``spin'' is another indispensable topic in quantum mechanics, appearing as an intrinsic degree of freedom.","In this work, we generalize the quantum Runge-Lenz vector to a spin-dependent one, and then extract a novel Hamiltonian of hydrogen atom with spin based on the requirement of $SO(4)$ symmetry.","Furthermore, the energy spectrum of hydrogen atom with spin potentials is also determined by the remarkable approach of $SO(4)$ symmetry.","Our findings extend the ground of hydrogen atom, and may contribute to other complicated models based on hydrogen atom."],"url":"http://arxiv.org/abs/2404.11437v1","category":"quant-ph"}
{"created":"2024-04-17 14:08:17","title":"Six decades of the FitzHugh-Nagumo model: A guide through its spatio-temporal dynamics and influence across disciplines","abstract":"The FitzHugh-Nagumo equation, originally conceived in neuroscience during the 1960s, became a key model providing a simplified view of excitable neuron cell behavior. Its applicability, however, extends beyond neuroscience into fields like cardiac physiology, cell division, population dynamics, electronics, and other natural phenomena. In this review spanning six decades of research, we discuss the diverse spatio-temporal dynamical behaviors described by the FitzHugh-Nagumo equation. These include dynamics like bistability, oscillations, and excitability, but it also addresses more complex phenomena such as traveling waves and extended patterns in coupled systems. The review serves as a guide for modelers aiming to utilize the strengths of the FitzHugh-Nagumo model to capture generic dynamical behavior. It not only catalogs known dynamical states and bifurcations, but also extends previous studies by providing stability and bifurcation analyses for coupled spatial systems.","sentences":["The FitzHugh-Nagumo equation, originally conceived in neuroscience during the 1960s, became a key model providing a simplified view of excitable neuron cell behavior.","Its applicability, however, extends beyond neuroscience into fields like cardiac physiology, cell division, population dynamics, electronics, and other natural phenomena.","In this review spanning six decades of research, we discuss the diverse spatio-temporal dynamical behaviors described by the FitzHugh-Nagumo equation.","These include dynamics like bistability, oscillations, and excitability, but it also addresses more complex phenomena such as traveling waves and extended patterns in coupled systems.","The review serves as a guide for modelers aiming to utilize the strengths of the FitzHugh-Nagumo model to capture generic dynamical behavior.","It not only catalogs known dynamical states and bifurcations, but also extends previous studies by providing stability and bifurcation analyses for coupled spatial systems."],"url":"http://arxiv.org/abs/2404.11403v1","category":"nlin.PS"}
{"created":"2024-04-17 17:00:26","title":"Spatio-Temporal Motion Retargeting for Quadruped Robots","abstract":"This work introduces a motion retargeting approach for legged robots, which aims to create motion controllers that imitate the fine behavior of animals. Our approach, namely spatio-temporal motion retargeting (STMR), guides imitation learning procedures by transferring motion from source to target, effectively bridging the morphological disparities by ensuring the feasibility of imitation on the target system. Our STMR method comprises two components: spatial motion retargeting (SMR) and temporal motion retargeting (TMR). On the one hand, SMR tackles motion retargeting at the kinematic level by generating kinematically feasible whole-body motions from keypoint trajectories. On the other hand, TMR aims to retarget motion at the dynamic level by optimizing motion in the temporal domain. We showcase the effectiveness of our method in facilitating Imitation Learning (IL) for complex animal movements through a series of simulation and hardware experiments. In these experiments, our STMR method successfully tailored complex animal motions from various media, including video captured by a hand-held camera, to fit the morphology and physical properties of the target robots. This enabled RL policy training for precise motion tracking, while baseline methods struggled with highly dynamic motion involving flying phases. Moreover, we validated that the control policy can successfully imitate six different motions in two quadruped robots with different dimensions and physical properties in real-world settings.","sentences":["This work introduces a motion retargeting approach for legged robots, which aims to create motion controllers that imitate the fine behavior of animals.","Our approach, namely spatio-temporal motion retargeting (STMR), guides imitation learning procedures by transferring motion from source to target, effectively bridging the morphological disparities by ensuring the feasibility of imitation on the target system.","Our STMR method comprises two components: spatial motion retargeting (SMR) and temporal motion retargeting (TMR).","On the one hand, SMR tackles motion retargeting at the kinematic level by generating kinematically feasible whole-body motions from keypoint trajectories.","On the other hand, TMR aims to retarget motion at the dynamic level by optimizing motion in the temporal domain.","We showcase the effectiveness of our method in facilitating Imitation Learning (IL) for complex animal movements through a series of simulation and hardware experiments.","In these experiments, our STMR method successfully tailored complex animal motions from various media, including video captured by a hand-held camera, to fit the morphology and physical properties of the target robots.","This enabled RL policy training for precise motion tracking, while baseline methods struggled with highly dynamic motion involving flying phases.","Moreover, we validated that the control policy can successfully imitate six different motions in two quadruped robots with different dimensions and physical properties in real-world settings."],"url":"http://arxiv.org/abs/2404.11557v1","category":"cs.RO"}
{"created":"2024-04-17 16:24:07","title":"Pack of LLMs: Model Fusion at Test-Time via Perplexity Optimization","abstract":"Fusing knowledge from multiple Large Language Models (LLMs) can combine their diverse strengths to achieve improved performance on a given task. However, current fusion approaches either rely on learning-based fusers that do not generalize to new LLMs, or do not take into account how well each LLM understands the input. In this work, we study LLM fusion at test-time, which enables leveraging knowledge from arbitrary user-specified LLMs during inference. We introduce Pack of LLMs (PackLLM), an effective method for test-time fusion that leverages each LLM's expertise, given an input prompt. PackLLM performs model fusion by solving an optimization problem for determining each LLM's importance, so that perplexity over the input prompt is minimized. First, our simple PackLLM-sim variant validates that perplexity is a good indicator for measuring each LLM's expertise. Second, our PackLLM-opt variant approximately solves the perplexity minimization problem via a greedy algorithm. The derived importance weights are used to combine the LLMs during inference. We conduct experiments with over 100 total LLMs on a diverse set of tasks. Experimental results show that (i) perplexity is a reliable measure for LLM fusion, (ii) PackLLM outperforms test-time fusion baselines by 1.89% accuracy points, and (iii) PackLLM can leverage new LLMs to improve performance over learning-based fusion approaches by 3.92-11.94% accuracy points.","sentences":["Fusing knowledge from multiple Large Language Models (LLMs) can combine their diverse strengths to achieve improved performance on a given task.","However, current fusion approaches either rely on learning-based fusers that do not generalize to new LLMs, or do not take into account how well each LLM understands the input.","In this work, we study LLM fusion at test-time, which enables leveraging knowledge from arbitrary user-specified LLMs during inference.","We introduce Pack of LLMs (PackLLM), an effective method for test-time fusion that leverages each LLM's expertise, given an input prompt.","PackLLM performs model fusion by solving an optimization problem for determining each LLM's importance, so that perplexity over the input prompt is minimized.","First, our simple PackLLM-sim variant validates that perplexity is a good indicator for measuring each LLM's expertise.","Second, our PackLLM-opt variant approximately solves the perplexity minimization problem via a greedy algorithm.","The derived importance weights are used to combine the LLMs during inference.","We conduct experiments with over 100 total LLMs on a diverse set of tasks.","Experimental results show that (i) perplexity is a reliable measure for LLM fusion, (ii) PackLLM outperforms test-time fusion baselines by 1.89% accuracy points, and (iii) PackLLM can leverage new LLMs to improve performance over learning-based fusion approaches by 3.92-11.94% accuracy points."],"url":"http://arxiv.org/abs/2404.11531v1","category":"cs.CL"}
{"created":"2024-04-17 15:42:31","title":"Mesh Optimization for the Virtual Element Method: How Small Can an Agglomerated Mesh Become?","abstract":"We present an optimization procedure for generic polygonal or polyhedral meshes, tailored for the Virtual Element Method (VEM).   Once the local quality of the mesh elements is analyzed through a quality indicator specific to the VEM, groups of elements are agglomerated to optimize the global mesh quality.   The resulting discretization is significantly lighter: we can remove up to 80$\\%$ of the mesh elements, based on a user-set parameter, thus reducing the number of faces, edges, and vertices.   This results in a drastic reduction of the total number of degrees of freedom associated with a discrete problem defined over the mesh with the VEM, in particular, for high-order formulations.   We show how the VEM convergence rate is preserved in the optimized meshes, and the approximation errors are comparable with those obtained with the original ones.   We observe that the optimization has a regularization effect over low-quality meshes, removing the most pathological elements.   This regularization effect is evident in cases where the original meshes cause the VEM to diverge, while the optimized meshes lead to convergence.   We conclude by showing how the optimization of a real CAD model can be used effectively in the simulation of a time-dependent problem.","sentences":["We present an optimization procedure for generic polygonal or polyhedral meshes, tailored for the Virtual Element Method (VEM).   ","Once the local quality of the mesh elements is analyzed through a quality indicator specific to the VEM, groups of elements are agglomerated to optimize the global mesh quality.   ","The resulting discretization is significantly lighter: we can remove up to 80$\\%$ of the mesh elements, based on a user-set parameter, thus reducing the number of faces, edges, and vertices.   ","This results in a drastic reduction of the total number of degrees of freedom associated with a discrete problem defined over the mesh with the VEM, in particular, for high-order formulations.   ","We show how the VEM convergence rate is preserved in the optimized meshes, and the approximation errors are comparable with those obtained with the original ones.   ","We observe that the optimization has a regularization effect over low-quality meshes, removing the most pathological elements.   ","This regularization effect is evident in cases where the original meshes cause the VEM to diverge, while the optimized meshes lead to convergence.   ","We conclude by showing how the optimization of a real CAD model can be used effectively in the simulation of a time-dependent problem."],"url":"http://arxiv.org/abs/2404.11484v1","category":"math.NA"}
{"created":"2024-04-17 14:49:21","title":"Solving Power Grid Optimization Problems with Rydberg Atoms","abstract":"The rapid development of neutral atom quantum hardware provides a unique opportunity to design hardware-centered algorithms for solving real-world problems aimed at establishing quantum utility. In this work, we study the performance of two such algorithms on solving MaxCut problem for various weighted graphs. The first method uses a state-of-the-art machine learning tool to optimize the pulse shape and embedding of the graph using an adiabatic Ansatz to find the ground state. We tested the performance of this method on finding maximum power section task of the IEEE 9-bus power system and obtaining MaxCut of randomly generated problems of size up to 12 on the Aquila quantum processor. To the best of our knowledge, this work presents the first MaxCut results on Quera's Aquila quantum hardware. Our experiments run on Aquila demonstrate that even though the probability of obtaining the solution is reduced, one can still solve the MaxCut problem on cloud-accessed neutral atom quantum hardware. The second method uses local detuning, which is an emergent update on the Aquila hardware, to obtain a near exact realization of the standard QAOA Ansatz with similar performance. Finally, we study the fidelity throughout the time evolution realized in the adiabatic method as a benchmark for the IEEE 9-bus power grid graph state.","sentences":["The rapid development of neutral atom quantum hardware provides a unique opportunity to design hardware-centered algorithms for solving real-world problems aimed at establishing quantum utility.","In this work, we study the performance of two such algorithms on solving MaxCut problem for various weighted graphs.","The first method uses a state-of-the-art machine learning tool to optimize the pulse shape and embedding of the graph using an adiabatic Ansatz to find the ground state.","We tested the performance of this method on finding maximum power section task of the IEEE 9-bus power system and obtaining MaxCut of randomly generated problems of size up to 12 on the Aquila quantum processor.","To the best of our knowledge, this work presents the first MaxCut results on Quera's Aquila quantum hardware.","Our experiments run on Aquila demonstrate that even though the probability of obtaining the solution is reduced, one can still solve the MaxCut problem on cloud-accessed neutral atom quantum hardware.","The second method uses local detuning, which is an emergent update on the Aquila hardware, to obtain a near exact realization of the standard QAOA Ansatz with similar performance.","Finally, we study the fidelity throughout the time evolution realized in the adiabatic method as a benchmark for the IEEE 9-bus power grid graph state."],"url":"http://arxiv.org/abs/2404.11440v1","category":"quant-ph"}
{"created":"2024-04-17 14:33:41","title":"SPAMming Labels: Efficient Annotations for the Trackers of Tomorrow","abstract":"Increasing the annotation efficiency of trajectory annotations from videos has the potential to enable the next generation of data-hungry tracking algorithms to thrive on large-scale datasets. Despite the importance of this task, there are currently very few works exploring how to efficiently label tracking datasets comprehensively. In this work, we introduce SPAM, a tracking data engine that provides high-quality labels with minimal human intervention. SPAM is built around two key insights: i) most tracking scenarios can be easily resolved. To take advantage of this, we utilize a pre-trained model to generate high-quality pseudo-labels, reserving human involvement for a smaller subset of more difficult instances; ii) handling the spatiotemporal dependencies of track annotations across time can be elegantly and efficiently formulated through graphs. Therefore, we use a unified graph formulation to address the annotation of both detections and identity association for tracks across time. Based on these insights, SPAM produces high-quality annotations with a fraction of ground truth labeling cost. We demonstrate that trackers trained on SPAM labels achieve comparable performance to those trained on human annotations while requiring only 3-20% of the human labeling effort. Hence, SPAM paves the way towards highly efficient labeling of large-scale tracking datasets. Our code and models will be available upon acceptance.","sentences":["Increasing the annotation efficiency of trajectory annotations from videos has the potential to enable the next generation of data-hungry tracking algorithms to thrive on large-scale datasets.","Despite the importance of this task, there are currently very few works exploring how to efficiently label tracking datasets comprehensively.","In this work, we introduce SPAM, a tracking data engine that provides high-quality labels with minimal human intervention.","SPAM is built around two key insights: i) most tracking scenarios can be easily resolved.","To take advantage of this, we utilize a pre-trained model to generate high-quality pseudo-labels, reserving human involvement for a smaller subset of more difficult instances; ii) handling the spatiotemporal dependencies of track annotations across time can be elegantly and efficiently formulated through graphs.","Therefore, we use a unified graph formulation to address the annotation of both detections and identity association for tracks across time.","Based on these insights, SPAM produces high-quality annotations with a fraction of ground truth labeling cost.","We demonstrate that trackers trained on SPAM labels achieve comparable performance to those trained on human annotations while requiring only 3-20% of the human labeling effort.","Hence, SPAM paves the way towards highly efficient labeling of large-scale tracking datasets.","Our code and models will be available upon acceptance."],"url":"http://arxiv.org/abs/2404.11426v1","category":"cs.CV"}
{"created":"2024-04-17 14:08:22","title":"Multi-layer continuous carbon fiber pattern optimization and a spline based path planning interpretation","abstract":"A novel approach for creating tool paths for continuous carbon fiber-reinforced thermoplastic 3D printing is introduced. The aim is to enable load-bearing connections while avoiding non-manufacturable crossings of paths by generating layer specific patterns. We require a graph representation of the structural design with given desired continuous fiber connections. From this, optimal fiber patterns are obtained for each printing layer by solving linear integer optimization problems. Each layer may have a unique solution based on the history of the previous layers. Additionally, a path planning approach is presented which interprets the obtained layers via curves based on quadratic and cubic B\\'ezier splines and their offset curves in constant distance. The single parameter for the construction of the paths is the minimal turning radius of the fibers. The path planning provides a new interpretation for the final geometry of the design to be printed.","sentences":["A novel approach for creating tool paths for continuous carbon fiber-reinforced thermoplastic 3D printing is introduced.","The aim is to enable load-bearing connections while avoiding non-manufacturable crossings of paths by generating layer specific patterns.","We require a graph representation of the structural design with given desired continuous fiber connections.","From this, optimal fiber patterns are obtained for each printing layer by solving linear integer optimization problems.","Each layer may have a unique solution based on the history of the previous layers.","Additionally, a path planning approach is presented which interprets the obtained layers via curves based on quadratic and cubic B\\'ezier splines and their offset curves in constant distance.","The single parameter for the construction of the paths is the minimal turning radius of the fibers.","The path planning provides a new interpretation for the final geometry of the design to be printed."],"url":"http://arxiv.org/abs/2404.11404v1","category":"math.OC"}
{"created":"2024-04-17 13:55:05","title":"What-if Analysis Framework for Digital Twins in 6G Wireless Network Management","abstract":"This study explores implementing a digital twin network (DTN) for efficient 6G wireless network management, aligning with the fault, configuration, accounting, performance, and security (FCAPS) model. The DTN architecture comprises the Physical Twin Layer, implemented using NS-3, and the Service Layer, featuring machine learning and reinforcement learning for optimizing carrier sensitivity threshold and transmit power control in wireless networks. We introduce a robust \"What-if Analysis\" module, utilizing conditional tabular generative adversarial network (CTGAN) for synthetic data generation to mimic various network scenarios. These scenarios assess four network performance metrics: throughput, latency, packet loss, and coverage. Our findings demonstrate the efficiency of the proposed what-if analysis framework in managing complex network conditions, highlighting the importance of the scenario-maker step and the impact of twinning intervals on network performance.","sentences":["This study explores implementing a digital twin network (DTN) for efficient 6G wireless network management, aligning with the fault, configuration, accounting, performance, and security (FCAPS) model.","The DTN architecture comprises the Physical Twin Layer, implemented using NS-3, and the Service Layer, featuring machine learning and reinforcement learning for optimizing carrier sensitivity threshold and transmit power control in wireless networks.","We introduce a robust \"What-if Analysis\" module, utilizing conditional tabular generative adversarial network (CTGAN) for synthetic data generation to mimic various network scenarios.","These scenarios assess four network performance metrics: throughput, latency, packet loss, and coverage.","Our findings demonstrate the efficiency of the proposed what-if analysis framework in managing complex network conditions, highlighting the importance of the scenario-maker step and the impact of twinning intervals on network performance."],"url":"http://arxiv.org/abs/2404.11394v1","category":"cs.NI"}
{"created":"2024-04-17 13:14:52","title":"DeblurGS: Gaussian Splatting for Camera Motion Blur","abstract":"Although significant progress has been made in reconstructing sharp 3D scenes from motion-blurred images, a transition to real-world applications remains challenging. The primary obstacle stems from the severe blur which leads to inaccuracies in the acquisition of initial camera poses through Structure-from-Motion, a critical aspect often overlooked by previous approaches. To address this challenge, we propose DeblurGS, a method to optimize sharp 3D Gaussian Splatting from motion-blurred images, even with the noisy camera pose initialization. We restore a fine-grained sharp scene by leveraging the remarkable reconstruction capability of 3D Gaussian Splatting. Our approach estimates the 6-Degree-of-Freedom camera motion for each blurry observation and synthesizes corresponding blurry renderings for the optimization process. Furthermore, we propose Gaussian Densification Annealing strategy to prevent the generation of inaccurate Gaussians at erroneous locations during the early training stages when camera motion is still imprecise. Comprehensive experiments demonstrate that our DeblurGS achieves state-of-the-art performance in deblurring and novel view synthesis for real-world and synthetic benchmark datasets, as well as field-captured blurry smartphone videos.","sentences":["Although significant progress has been made in reconstructing sharp 3D scenes from motion-blurred images, a transition to real-world applications remains challenging.","The primary obstacle stems from the severe blur which leads to inaccuracies in the acquisition of initial camera poses through Structure-from-Motion, a critical aspect often overlooked by previous approaches.","To address this challenge, we propose DeblurGS, a method to optimize sharp 3D Gaussian Splatting from motion-blurred images, even with the noisy camera pose initialization.","We restore a fine-grained sharp scene by leveraging the remarkable reconstruction capability of 3D Gaussian Splatting.","Our approach estimates the 6-Degree-of-Freedom camera motion for each blurry observation and synthesizes corresponding blurry renderings for the optimization process.","Furthermore, we propose Gaussian Densification Annealing strategy to prevent the generation of inaccurate Gaussians at erroneous locations during the early training stages when camera motion is still imprecise.","Comprehensive experiments demonstrate that our DeblurGS achieves state-of-the-art performance in deblurring and novel view synthesis for real-world and synthetic benchmark datasets, as well as field-captured blurry smartphone videos."],"url":"http://arxiv.org/abs/2404.11358v2","category":"cs.CV"}
{"created":"2024-04-17 13:07:56","title":"TeClass: A Human-Annotated Relevance-based Headline Classification and Generation Dataset for Telugu","abstract":"News headline generation is a crucial task in increasing productivity for both the readers and producers of news. This task can easily be aided by automated News headline-generation models. However, the presence of irrelevant headlines in scraped news articles results in sub-optimal performance of generation models. We propose that relevance-based headline classification can greatly aid the task of generating relevant headlines. Relevance-based headline classification involves categorizing news headlines based on their relevance to the corresponding news articles. While this task is well-established in English, it remains under-explored in low-resource languages like Telugu due to a lack of annotated data. To address this gap, we present TeClass, the first-ever human-annotated Telugu news headline classification dataset, containing 78,534 annotations across 26,178 article-headline pairs. We experiment with various baseline models and provide a comprehensive analysis of their results. We further demonstrate the impact of this work by fine-tuning various headline generation models using TeClass dataset. The headlines generated by the models fine-tuned on highly relevant article-headline pairs, showed about a 5 point increment in the ROUGE-L scores. To encourage future research, the annotated dataset as well as the annotation guidelines will be made publicly available.","sentences":["News headline generation is a crucial task in increasing productivity for both the readers and producers of news.","This task can easily be aided by automated News headline-generation models.","However, the presence of irrelevant headlines in scraped news articles results in sub-optimal performance of generation models.","We propose that relevance-based headline classification can greatly aid the task of generating relevant headlines.","Relevance-based headline classification involves categorizing news headlines based on their relevance to the corresponding news articles.","While this task is well-established in English, it remains under-explored in low-resource languages like Telugu due to a lack of annotated data.","To address this gap, we present TeClass, the first-ever human-annotated Telugu news headline classification dataset, containing 78,534 annotations across 26,178 article-headline pairs.","We experiment with various baseline models and provide a comprehensive analysis of their results.","We further demonstrate the impact of this work by fine-tuning various headline generation models using TeClass dataset.","The headlines generated by the models fine-tuned on highly relevant article-headline pairs, showed about a 5 point increment in the ROUGE-L scores.","To encourage future research, the annotated dataset as well as the annotation guidelines will be made publicly available."],"url":"http://arxiv.org/abs/2404.11349v1","category":"cs.CL"}
{"created":"2024-04-17 12:52:08","title":"The dynamics of diversity on corporate boards","abstract":"Diversity in leadership positions and corporate boards is an important aspect of equality. It is important because it is the key to better decision-making and innovation, and above all, it paves the way for future generations to participate and shape our society. Many studies emphasize the importance of the visibility of role models and the effect that connectivity has on the success of minorities in leadership. However, the connectivity of firms, the dynamics of the adoption of minorities, and the long-term effects have not been well understood. Here, we present a model that shows how these effects work together in a dynamic model that is calibrated with empirical data of firm and board networks. We show that homophily -- the appointment of minorities is influenced by the presence of minorities in a board and its neighboring entities -- is an important effect shaping the trajectory towards equality. We further show how perception biases and feedback related to the centrality of minority members influence the dynamic. We find that reaching equality can be sped up or slowed down depending on the distribution of minorities in central firms. These insights bear significant implications for policy-making geared towards fostering equality and diversity within corporate boards.","sentences":["Diversity in leadership positions and corporate boards is an important aspect of equality.","It is important because it is the key to better decision-making and innovation, and above all, it paves the way for future generations to participate and shape our society.","Many studies emphasize the importance of the visibility of role models and the effect that connectivity has on the success of minorities in leadership.","However, the connectivity of firms, the dynamics of the adoption of minorities, and the long-term effects have not been well understood.","Here, we present a model that shows how these effects work together in a dynamic model that is calibrated with empirical data of firm and board networks.","We show that homophily -- the appointment of minorities is influenced by the presence of minorities in a board and its neighboring entities -- is an important effect shaping the trajectory towards equality.","We further show how perception biases and feedback related to the centrality of minority members influence the dynamic.","We find that reaching equality can be sped up or slowed down depending on the distribution of minorities in central firms.","These insights bear significant implications for policy-making geared towards fostering equality and diversity within corporate boards."],"url":"http://arxiv.org/abs/2404.11334v1","category":"econ.TH"}
{"created":"2024-04-17 12:38:58","title":"Single-temporal Supervised Remote Change Detection for Domain Generalization","abstract":"Change detection is widely applied in remote sensing image analysis. Existing methods require training models separately for each dataset, which leads to poor domain generalization. Moreover, these methods rely heavily on large amounts of high-quality pair-labelled data for training, which is expensive and impractical. In this paper, we propose a multimodal contrastive learning (ChangeCLIP) based on visual-language pre-training for change detection domain generalization. Additionally, we propose a dynamic context optimization for prompt learning. Meanwhile, to address the data dependency issue of existing methods, we introduce a single-temporal and controllable AI-generated training strategy (SAIN). This allows us to train the model using a large number of single-temporal images without image pairs in the real world, achieving excellent generalization. Extensive experiments on series of real change detection datasets validate the superiority and strong generalization of ChangeCLIP, outperforming state-of-the-art change detection methods. Code will be available.","sentences":["Change detection is widely applied in remote sensing image analysis.","Existing methods require training models separately for each dataset, which leads to poor domain generalization.","Moreover, these methods rely heavily on large amounts of high-quality pair-labelled data for training, which is expensive and impractical.","In this paper, we propose a multimodal contrastive learning (ChangeCLIP) based on visual-language pre-training for change detection domain generalization.","Additionally, we propose a dynamic context optimization for prompt learning.","Meanwhile, to address the data dependency issue of existing methods, we introduce a single-temporal and controllable AI-generated training strategy (SAIN).","This allows us to train the model using a large number of single-temporal images without image pairs in the real world, achieving excellent generalization.","Extensive experiments on series of real change detection datasets validate the superiority and strong generalization of ChangeCLIP, outperforming state-of-the-art change detection methods.","Code will be available."],"url":"http://arxiv.org/abs/2404.11326v2","category":"cs.CV"}
{"created":"2024-04-17 12:35:36","title":"Bayesian Optimization for Identification of Optimal Biological Dose Combinations in Personalized Dose-Finding Trials","abstract":"Early phase, personalized dose-finding trials for combination therapies seek to identify patient-specific optimal biological dose (OBD) combinations, which are defined as safe dose combinations which maximize therapeutic benefit for a specific covariate pattern. Given the small sample sizes which are typical of these trials, it is challenging for traditional parametric approaches to identify OBD combinations across multiple dosing agents and covariate patterns. To address these challenges, we propose a Bayesian optimization approach to dose-finding which formally incorporates toxicity information into both the initial data collection process and the sequential search strategy. Independent Gaussian processes are used to model the efficacy and toxicity surfaces, and an acquisition function is utilized to define the dose-finding strategy and an early stopping rule. This work is motivated by a personalized dose-finding trial which considers a dual-agent therapy for obstructive sleep apnea, where OBD combinations are tailored to obstructive sleep apnea severity. To compare the performance of the personalized approach to a standard approach where covariate information is ignored, a simulation study is performed. We conclude that personalized dose-finding is essential in the presence of heterogeneity.","sentences":["Early phase, personalized dose-finding trials for combination therapies seek to identify patient-specific optimal biological dose (OBD) combinations, which are defined as safe dose combinations which maximize therapeutic benefit for a specific covariate pattern.","Given the small sample sizes which are typical of these trials, it is challenging for traditional parametric approaches to identify OBD combinations across multiple dosing agents and covariate patterns.","To address these challenges, we propose a Bayesian optimization approach to dose-finding which formally incorporates toxicity information into both the initial data collection process and the sequential search strategy.","Independent Gaussian processes are used to model the efficacy and toxicity surfaces, and an acquisition function is utilized to define the dose-finding strategy and an early stopping rule.","This work is motivated by a personalized dose-finding trial which considers a dual-agent therapy for obstructive sleep apnea, where OBD combinations are tailored to obstructive sleep apnea severity.","To compare the performance of the personalized approach to a standard approach where covariate information is ignored, a simulation study is performed.","We conclude that personalized dose-finding is essential in the presence of heterogeneity."],"url":"http://arxiv.org/abs/2404.11323v1","category":"stat.ME"}
{"created":"2024-04-17 12:30:54","title":"Improving Composed Image Retrieval via Contrastive Learning with Scaling Positives and Negatives","abstract":"The Composed Image Retrieval (CIR) task aims to retrieve target images using a composed query consisting of a reference image and a modified text. Advanced methods often utilize contrastive learning as the optimization objective, which benefits from adequate positive and negative examples. However, the triplet for CIR incurs high manual annotation costs, resulting in limited positive examples. Furthermore, existing methods commonly use in-batch negative sampling, which reduces the negative number available for the model. To address the problem of lack of positives, we propose a data generation method by leveraging a multi-modal large language model to construct triplets for CIR. To introduce more negatives during fine-tuning, we design a two-stage fine-tuning framework for CIR, whose second stage introduces plenty of static representations of negatives to optimize the representation space rapidly. The above two improvements can be effectively stacked and designed to be plug-and-play, easily applied to existing CIR models without changing their original architectures. Extensive experiments and ablation analysis demonstrate that our method effectively scales positives and negatives and achieves state-of-the-art results on both FashionIQ and CIRR datasets. In addition, our methods also perform well in zero-shot composed image retrieval, providing a new CIR solution for the low-resources scenario.","sentences":["The Composed Image Retrieval (CIR) task aims to retrieve target images using a composed query consisting of a reference image and a modified text.","Advanced methods often utilize contrastive learning as the optimization objective, which benefits from adequate positive and negative examples.","However, the triplet for CIR incurs high manual annotation costs, resulting in limited positive examples.","Furthermore, existing methods commonly use in-batch negative sampling, which reduces the negative number available for the model.","To address the problem of lack of positives, we propose a data generation method by leveraging a multi-modal large language model to construct triplets for CIR.","To introduce more negatives during fine-tuning, we design a two-stage fine-tuning framework for CIR, whose second stage introduces plenty of static representations of negatives to optimize the representation space rapidly.","The above two improvements can be effectively stacked and designed to be plug-and-play, easily applied to existing CIR models without changing their original architectures.","Extensive experiments and ablation analysis demonstrate that our method effectively scales positives and negatives and achieves state-of-the-art results on both FashionIQ and CIRR datasets.","In addition, our methods also perform well in zero-shot composed image retrieval, providing a new CIR solution for the low-resources scenario."],"url":"http://arxiv.org/abs/2404.11317v1","category":"cs.CV"}
{"created":"2024-04-17 12:26:32","title":"Destructive and constructive RIS beamforming in an ISAC-multi-user MIMO network","abstract":"Integrated sensing and communication (ISAC) has already established itself as a promising solution to the spectrum scarcity problem, even more so when paired with a reconfigurable intelligent surface (RIS) as RISs can shape the propagation environment by adjusting their phase-shift coefficients. Albeit the potential performance gain, a RIS also poses a security threat to the system: in this paper, we explore both sides of the RIS presence in a multi-user MIMO (multiple-input multiple-output) ISAC network. We first develop an alternating optimization algorithm, obtaining the active and passive beamforming vectors maximizing the sensing signal-to-noise ratio (SNR) under minimum signal-to-interference-plus-noise ratio (SINR) constraints for the communication users and finite power budget. We also investigate the destructive potential of RIS by devising a RIS phase-shift optimization algorithm that minimizes sensing SNR while preserving the same minimum communication SINR previously guaranteed by the system. We further investigate the impact of the RIS's individual element failures on the system performances. The simulation results show that the RIS performance-boosting potential is as good as its destructive one and that both of our optimization strategies show some resilience towards the investigated impairments.","sentences":["Integrated sensing and communication (ISAC) has already established itself as a promising solution to the spectrum scarcity problem, even more so when paired with a reconfigurable intelligent surface (RIS) as RISs can shape the propagation environment by adjusting their phase-shift coefficients.","Albeit the potential performance gain, a RIS also poses a security threat to the system: in this paper, we explore both sides of the RIS presence in a multi-user MIMO (multiple-input multiple-output) ISAC network.","We first develop an alternating optimization algorithm, obtaining the active and passive beamforming vectors maximizing the sensing signal-to-noise ratio (SNR) under minimum signal-to-interference-plus-noise ratio (SINR) constraints for the communication users and finite power budget.","We also investigate the destructive potential of RIS by devising a RIS phase-shift optimization algorithm that minimizes sensing SNR while preserving the same minimum communication SINR previously guaranteed by the system.","We further investigate the impact of the RIS's individual element failures on the system performances.","The simulation results show that the RIS performance-boosting potential is as good as its destructive one and that both of our optimization strategies show some resilience towards the investigated impairments."],"url":"http://arxiv.org/abs/2404.11314v1","category":"cs.IT"}
{"created":"2024-04-17 12:13:49","title":"Nanojet Visualization and Dark-field Imaging of Optically Trapped Vaterite Capsules with Endoscopic Illumination","abstract":"Optical responsivity grants biomedical capsules additional capabilities, promoting them towards multifunctional theragnostic nanodevices. In this endeavor, screening candidates under conditions that closely resemble in situ environments is crucial for both the initial optimization and the subsequent inspection stages of development and operation. Optical tweezers equipped with dark-field spectroscopy are among the preferable tools for nanoparticle imaging and refractometry. However, the effectiveness of conventional illumination and light collection arrangements for inspecting anisotropic complex inner composition particles is quite limited due to reduced collection angles, which can result in the omission of features in scattering diagrams. Here we introduce an endoscopic dark-field illumination scheme, where light is launched on an optically trapped particle from a single-mode fiber, immersed into a fluid cell. This arrangement disentangles illumination and collection paths, thus allowing the collection of scattered light with a very high numerical aperture. This methodology is applied to vaterite nanocapsules, which are known to possess strong anisotropic responses. Tweezer configuration allows revealing optical properties for different crystallographic orientations of vaterite, which is complex to do otherwise. Furthermore, endoscopic dark-field images reveal the emergence of polarization-dependent long-range photonic nanojets, which are capable of interacting with nearby particles, demonstrating a new pathway for nanojet image formation.","sentences":["Optical responsivity grants biomedical capsules additional capabilities, promoting them towards multifunctional theragnostic nanodevices.","In this endeavor, screening candidates under conditions that closely resemble in situ environments is crucial for both the initial optimization and the subsequent inspection stages of development and operation.","Optical tweezers equipped with dark-field spectroscopy are among the preferable tools for nanoparticle imaging and refractometry.","However, the effectiveness of conventional illumination and light collection arrangements for inspecting anisotropic complex inner composition particles is quite limited due to reduced collection angles, which can result in the omission of features in scattering diagrams.","Here we introduce an endoscopic dark-field illumination scheme, where light is launched on an optically trapped particle from a single-mode fiber, immersed into a fluid cell.","This arrangement disentangles illumination and collection paths, thus allowing the collection of scattered light with a very high numerical aperture.","This methodology is applied to vaterite nanocapsules, which are known to possess strong anisotropic responses.","Tweezer configuration allows revealing optical properties for different crystallographic orientations of vaterite, which is complex to do otherwise.","Furthermore, endoscopic dark-field images reveal the emergence of polarization-dependent long-range photonic nanojets, which are capable of interacting with nearby particles, demonstrating a new pathway for nanojet image formation."],"url":"http://arxiv.org/abs/2404.11303v1","category":"physics.optics"}
{"created":"2024-04-17 12:06:17","title":"How to Exhibit More Predictable Behaviors","abstract":"This paper looks at predictability problems, i.e., wherein an agent must choose its strategy in order to optimize the predictions that an external observer could make. We address these problems while taking into account uncertainties on the environment dynamics and on the observed agent's policy. To that end, we assume that the observer 1. seeks to predict the agent's future action or state at each time step, and 2. models the agent using a stochastic policy computed from a known underlying problem, and we leverage on the framework of observer-aware Markov decision processes (OAMDPs). We propose action and state predictability performance criteria through reward functions built on the observer's belief about the agent policy; show that these induced predictable OAMDPs can be represented by goal-oriented or discounted MDPs; and analyze the properties of the proposed reward functions both theoretically and empirically on two types of grid-world problems.","sentences":["This paper looks at predictability problems, i.e., wherein an agent must choose its strategy in order to optimize the predictions that an external observer could make.","We address these problems while taking into account uncertainties on the environment dynamics and on the observed agent's policy.","To that end, we assume that the observer 1. seeks to predict the agent's future action or state at each time step, and 2. models the agent using a stochastic policy computed from a known underlying problem, and we leverage on the framework of observer-aware Markov decision processes (OAMDPs).","We propose action and state predictability performance criteria through reward functions built on the observer's belief about the agent policy; show that these induced predictable OAMDPs can be represented by goal-oriented or discounted MDPs; and analyze the properties of the proposed reward functions both theoretically and empirically on two types of grid-world problems."],"url":"http://arxiv.org/abs/2404.11296v1","category":"cs.AI"}
{"created":"2024-04-17 10:40:12","title":"Runtime Analysis of a Multi-Valued Compact Genetic Algorithm on Generalized OneMax","abstract":"A class of metaheuristic techniques called estimation-of-distribution algorithms (EDAs) are employed in optimization as more sophisticated substitutes for traditional strategies like evolutionary algorithms. EDAs generally drive the search for the optimum by creating explicit probabilistic models of potential candidate solutions through repeated sampling and selection from the underlying search space.   Most theoretical research on EDAs has focused on pseudo-Boolean optimization. Jedidia et al. (GECCO 2023) proposed the first EDAs for optimizing problems involving multi-valued decision variables. By building a framework, they have analyzed the runtime of a multi-valued UMDA on the r-valued LeadingOnes function. Using their framework, here we focus on the multi-valued compact genetic algorithm (r-cGA) and provide a first runtime analysis of a generalized OneMax function.   To prove our results, we investigate the effect of genetic drift and progress of the probabilistic model towards the optimum. After finding the right algorithm parameters, we prove that the r-cGA solves this r-valued OneMax problem efficiently. We show that with high probability, the runtime bound is O(r2 n log2 r log3 n). At the end of experiments, we state one conjecture related to the expected runtime of another variant of multi-valued OneMax function.","sentences":["A class of metaheuristic techniques called estimation-of-distribution algorithms (EDAs) are employed in optimization as more sophisticated substitutes for traditional strategies like evolutionary algorithms.","EDAs generally drive the search for the optimum by creating explicit probabilistic models of potential candidate solutions through repeated sampling and selection from the underlying search space.   ","Most theoretical research on EDAs has focused on pseudo-Boolean optimization.","Jedidia et al. (GECCO 2023) proposed the first EDAs for optimizing problems involving multi-valued decision variables.","By building a framework, they have analyzed the runtime of a multi-valued UMDA on the r-valued LeadingOnes function.","Using their framework, here we focus on the multi-valued compact genetic algorithm (r-cGA) and provide a first runtime analysis of a generalized OneMax function.   ","To prove our results, we investigate the effect of genetic drift and progress of the probabilistic model towards the optimum.","After finding the right algorithm parameters, we prove that the r-cGA solves this r-valued OneMax problem efficiently.","We show that with high probability, the runtime bound is O(r2 n log2 r log3 n).","At the end of experiments, we state one conjecture related to the expected runtime of another variant of multi-valued OneMax function."],"url":"http://arxiv.org/abs/2404.11239v1","category":"cs.NE"}
{"created":"2024-04-17 09:18:24","title":"Photonic indistinguishability characterization and optimization for cavity-based single-photon source","abstract":"Indistinguishability of single photons from independent sources is critically important for scalable quantum technologies. We provide a comprehensive comparison of single-photon indistinguishability of different kinds of cavity quantum electrodynamics (CQED) systems by numerically simulating Hong-Ou-Mandel (HOM) two-photon interference. We find that the CQED system using nature atoms exhibit superiority in indistinguishability, benefiting from the inherently identical features. Moreover, a $\\Lambda-$type three-level atoms show essential robust against variation of various system parameters because it exploits the two ground states with considerable smaller decay rates for single-photon generation. Furthermore, a machine learning-based framework is proposed to significantly and robustly improve single-photon indistinguishability for non-identical two CQED systems. This work may pave the way for designing and engineering reliable and scalable photon-based quantum technologies.","sentences":["Indistinguishability of single photons from independent sources is critically important for scalable quantum technologies.","We provide a comprehensive comparison of single-photon indistinguishability of different kinds of cavity quantum electrodynamics (CQED) systems by numerically simulating Hong-Ou-Mandel (HOM) two-photon interference.","We find that the CQED system using nature atoms exhibit superiority in indistinguishability, benefiting from the inherently identical features.","Moreover, a $\\Lambda-$type three-level atoms show essential robust against variation of various system parameters because it exploits the two ground states with considerable smaller decay rates for single-photon generation.","Furthermore, a machine learning-based framework is proposed to significantly and robustly improve single-photon indistinguishability for non-identical two CQED systems.","This work may pave the way for designing and engineering reliable and scalable photon-based quantum technologies."],"url":"http://arxiv.org/abs/2404.11193v1","category":"quant-ph"}
{"created":"2024-04-17 08:53:59","title":"KI-GAN: Knowledge-Informed Generative Adversarial Networks for Enhanced Multi-Vehicle Trajectory Forecasting at Signalized Intersections","abstract":"Reliable prediction of vehicle trajectories at signalized intersections is crucial to urban traffic management and autonomous driving systems. However, it presents unique challenges, due to the complex roadway layout at intersections, involvement of traffic signal controls, and interactions among different types of road users. To address these issues, we present in this paper a novel model called Knowledge-Informed Generative Adversarial Network (KI-GAN), which integrates both traffic signal information and multi-vehicle interactions to predict vehicle trajectories accurately. Additionally, we propose a specialized attention pooling method that accounts for vehicle orientation and proximity at intersections. Based on the SinD dataset, our KI-GAN model is able to achieve an Average Displacement Error (ADE) of 0.05 and a Final Displacement Error (FDE) of 0.12 for a 6-second observation and 6-second prediction cycle. When the prediction window is extended to 9 seconds, the ADE and FDE values are further reduced to 0.11 and 0.26, respectively. These results demonstrate the effectiveness of the proposed KI-GAN model in vehicle trajectory prediction under complex scenarios at signalized intersections, which represents a significant advancement in the target field.","sentences":["Reliable prediction of vehicle trajectories at signalized intersections is crucial to urban traffic management and autonomous driving systems.","However, it presents unique challenges, due to the complex roadway layout at intersections, involvement of traffic signal controls, and interactions among different types of road users.","To address these issues, we present in this paper a novel model called Knowledge-Informed Generative Adversarial Network (KI-GAN), which integrates both traffic signal information and multi-vehicle interactions to predict vehicle trajectories accurately.","Additionally, we propose a specialized attention pooling method that accounts for vehicle orientation and proximity at intersections.","Based on the SinD dataset, our KI-GAN model is able to achieve an Average Displacement Error (ADE) of 0.05 and a Final Displacement Error (FDE) of 0.12 for a 6-second observation and 6-second prediction cycle.","When the prediction window is extended to 9 seconds, the ADE and FDE values are further reduced to 0.11 and 0.26, respectively.","These results demonstrate the effectiveness of the proposed KI-GAN model in vehicle trajectory prediction under complex scenarios at signalized intersections, which represents a significant advancement in the target field."],"url":"http://arxiv.org/abs/2404.11181v1","category":"cs.LG"}
{"created":"2024-04-17 08:48:07","title":"Compression of quantum shallow-circuit states","abstract":"Shallow quantum circuits feature not only computational advantage over their classical counterparts but also cutting-edge applications. Storing quantum information generated by shallow circuits is a fundamental question of both theoretical and practical importance that remained largely unexplored. In this work, we show that $N$ copies of an unknown $n$-qubit state generated by a fixed-depth circuit can be compressed into a hybrid memory of $O(n \\log_2 N)$ (qu)bits, which achieves the optimal scaling of memory cost. Our work shows that the computational complexity of resources can significantly impact the rate of quantum information processing, offering a unique and unified view of quantum Shannon theory and quantum computing in the NISQ era.","sentences":["Shallow quantum circuits feature not only computational advantage over their classical counterparts but also cutting-edge applications.","Storing quantum information generated by shallow circuits is a fundamental question of both theoretical and practical importance that remained largely unexplored.","In this work, we show that $N$ copies of an unknown $n$-qubit state generated by a fixed-depth circuit can be compressed into a hybrid memory of $O(n \\log_2 N)$ (qu)bits, which achieves the optimal scaling of memory cost.","Our work shows that the computational complexity of resources can significantly impact the rate of quantum information processing, offering a unique and unified view of quantum Shannon theory and quantum computing in the NISQ era."],"url":"http://arxiv.org/abs/2404.11177v1","category":"quant-ph"}
{"created":"2024-04-17 08:32:11","title":"Microwave photonic short-time Fourier transform based on stabilized period-one nonlinear laser dynamics and stimulated Brillouin scattering","abstract":"A microwave photonic short-time Fourier transform (STFT) system based on stabilized period-one (P1) nonlinear laser dynamics and stimulated Brillouin scattering (SBS) is proposed. By using an optoelectronic feedback loop, the frequency-sweep optical signal generated by the P1 nonlinear laser dynamics is stabilized, which is further used in conjunction with an optical bandpass filter implemented by stimulated Brillouin scattering (SBS) to achieve the frequency-to-time mapping of microwave signals and the final STFT. By comparing the experimental results with and without optoelectronic feedback, it is found that the time-frequency diagram of the signal under test (SUT) obtained by STFT is clearer and more regular, and the frequency of the SUT measured in each frequency-sweep period is more accurate. The mean absolute error is reduced by 50% under the optimal filter bandwidth.","sentences":["A microwave photonic short-time Fourier transform (STFT) system based on stabilized period-one (P1) nonlinear laser dynamics and stimulated Brillouin scattering (SBS) is proposed.","By using an optoelectronic feedback loop, the frequency-sweep optical signal generated by the P1 nonlinear laser dynamics is stabilized, which is further used in conjunction with an optical bandpass filter implemented by stimulated Brillouin scattering (SBS) to achieve the frequency-to-time mapping of microwave signals and the final STFT.","By comparing the experimental results with and without optoelectronic feedback, it is found that the time-frequency diagram of the signal under test (SUT) obtained by STFT is clearer and more regular, and the frequency of the SUT measured in each frequency-sweep period is more accurate.","The mean absolute error is reduced by 50% under the optimal filter bandwidth."],"url":"http://arxiv.org/abs/2404.11168v1","category":"physics.optics"}
{"created":"2024-04-18 17:22:10","title":"Quantum thermophoresis","abstract":"Thermophoresis is the migration of a particle due to a thermal gradient. Here, we theoretically uncover the quantum version of thermophoresis. As a proof of principle, we analytically find a thermophoretic force on a trapped quantum particle having three energy levels in $\\Lambda$ configuration. We then consider a model of N sites, each coupled to its first neighbors and subjected to a local bath at a certain temperature, so as to show numerically how quantum thermophoresis behaves with increasing delocalization of the quantum particle. We discuss how negative thermophoresis and the Dufour effect appear in the quantum regime.","sentences":["Thermophoresis is the migration of a particle due to a thermal gradient.","Here, we theoretically uncover the quantum version of thermophoresis.","As a proof of principle, we analytically find a thermophoretic force on a trapped quantum particle having three energy levels in $\\Lambda$ configuration.","We then consider a model of N sites, each coupled to its first neighbors and subjected to a local bath at a certain temperature, so as to show numerically how quantum thermophoresis behaves with increasing delocalization of the quantum particle.","We discuss how negative thermophoresis and the Dufour effect appear in the quantum regime."],"url":"http://arxiv.org/abs/2404.12346v1","category":"quant-ph"}
{"created":"2024-04-18 16:58:05","title":"A Perspective on Deep Vision Performance with Standard Image and Video Codecs","abstract":"Resource-constrained hardware, such as edge devices or cell phones, often rely on cloud servers to provide the required computational resources for inference in deep vision models. However, transferring image and video data from an edge or mobile device to a cloud server requires coding to deal with network constraints. The use of standardized codecs, such as JPEG or H.264, is prevalent and required to ensure interoperability. This paper aims to examine the implications of employing standardized codecs within deep vision pipelines. We find that using JPEG and H.264 coding significantly deteriorates the accuracy across a broad range of vision tasks and models. For instance, strong compression rates reduce semantic segmentation accuracy by more than 80% in mIoU. In contrast to previous findings, our analysis extends beyond image and action classification to localization and dense prediction tasks, thus providing a more comprehensive perspective.","sentences":["Resource-constrained hardware, such as edge devices or cell phones, often rely on cloud servers to provide the required computational resources for inference in deep vision models.","However, transferring image and video data from an edge or mobile device to a cloud server requires coding to deal with network constraints.","The use of standardized codecs, such as JPEG or H.264, is prevalent and required to ensure interoperability.","This paper aims to examine the implications of employing standardized codecs within deep vision pipelines.","We find that using JPEG and H.264 coding significantly deteriorates the accuracy across a broad range of vision tasks and models.","For instance, strong compression rates reduce semantic segmentation accuracy by more than 80% in mIoU. In contrast to previous findings, our analysis extends beyond image and action classification to localization and dense prediction tasks, thus providing a more comprehensive perspective."],"url":"http://arxiv.org/abs/2404.12330v1","category":"cs.CV"}
{"created":"2024-04-18 16:45:39","title":"Laser excitation of the $^{229}$Th nuclear isomeric transition in a solid-state host","abstract":"LiSrAlF$_6$ crystals doped with $^{229}$Th are used in a laser-based search for the nuclear isomeric transition. Two spectroscopic features near the nuclear transition energy are observed. The first is a broad excitation feature that produces red-shifted fluorescence that decays with a timescale of a few seconds. The second is a narrow, laser-linewidth-limited spectral feature at $148.38219(4)_{\\textrm{stat}}(20)_{\\textrm{sys}}$ nm ($2020407.3(5)_{\\textrm{stat}}(30)_{\\textrm{sys}}$ GHz) that decays with a lifetime of $568(13)_{\\textrm{stat}}(20)_{\\textrm{sys}}$ s. This feature is assigned to the excitation of the $^{229}$Th nuclear isomeric state, whose energy is found to be $8.355733(2)_{\\textrm{stat}}(10)_{\\textrm{sys}}$ eV in $^{229}$Th:\\thor:LiSrAlF$_6$.","sentences":["LiSrAlF$_6$ crystals doped with $^{229}$Th are used in a laser-based search for the nuclear isomeric transition.","Two spectroscopic features near the nuclear transition energy are observed.","The first is a broad excitation feature that produces red-shifted fluorescence that decays with a timescale of a few seconds.","The second is a narrow, laser-linewidth-limited spectral feature at $148.38219(4)_{\\textrm{stat}}(20)_{\\textrm{sys}}$ nm ($2020407.3(5)_{\\textrm{stat}}(30)_{\\textrm{sys}}$ GHz) that decays with a lifetime of $568(13)_{\\textrm{stat}}(20)_{\\textrm{sys}}$ s.","This feature is assigned to the excitation of the $^{229}$Th nuclear isomeric state, whose energy is found to be $8.355733(2)_{\\textrm{stat}}(10)_{\\textrm{sys}}$ eV in $^{229}$Th:\\thor:LiSrAlF$_6$."],"url":"http://arxiv.org/abs/2404.12311v1","category":"physics.atom-ph"}
{"created":"2024-04-18 16:33:01","title":"Switchable Single/Dual Edge Registers for Pipeline Architecture","abstract":"The demand for low power processing is increasing due to mobile and portable devices. In a processor unit, an adder is an important building block since it is used in Floating Point Units (FPU) and Arithmetic Logic Units (ALU). Also, pipeline techniques are used extensively to improve the throughput of the processing unit. To implement a pipeline requires adding a register at each sub-stage that result in increasing the latency. Moreover, designing a low power pipeline adder with low latency has drawn a lot of attention. In a pipelined architecture that uses Dual Edge Triggered (DET) based registers can help in reducing the latency since they can capture input data at both clock edges. However, for high input activity, a DET flip-flop consumes more power than a Single-Edge Triggered (SET) flip-flop. Moreover, it is required to replace each Flip-Flop (FF) in the processor with Dual Edge Triggered (DET) Flip-Flop which will be a considerable area and power overhead. Therefore, it is desirable to have a switchable DET to SET depending on input activity or load condition to reduce the dynamic power consumption. In this paper, we are proposing a new shift register which imitates DET FF based shift register without the need of special DET FF. The proposed shift register improved the latency in a 4-bit pipelined adder by two-fold. Additionally, the power delay product was reduced by 44.16 %.","sentences":["The demand for low power processing is increasing due to mobile and portable devices.","In a processor unit, an adder is an important building block since it is used in Floating Point Units (FPU) and Arithmetic Logic Units (ALU).","Also, pipeline techniques are used extensively to improve the throughput of the processing unit.","To implement a pipeline requires adding a register at each sub-stage that result in increasing the latency.","Moreover, designing a low power pipeline adder with low latency has drawn a lot of attention.","In a pipelined architecture that uses Dual Edge Triggered (DET) based registers can help in reducing the latency since they can capture input data at both clock edges.","However, for high input activity, a DET flip-flop consumes more power than a Single-Edge Triggered (SET) flip-flop.","Moreover, it is required to replace each Flip-Flop (FF) in the processor with Dual Edge Triggered (DET) Flip-Flop which will be a considerable area and power overhead.","Therefore, it is desirable to have a switchable DET to SET depending on input activity or load condition to reduce the dynamic power consumption.","In this paper, we are proposing a new shift register which imitates DET FF based shift register without the need of special DET FF.","The proposed shift register improved the latency in a 4-bit pipelined adder by two-fold.","Additionally, the power delay product was reduced by 44.16 %."],"url":"http://arxiv.org/abs/2404.12306v1","category":"cs.AR"}
{"created":"2024-04-18 15:15:47","title":"Effects of Reduced Interlayer Interactions on the K-point Excitons of MoS$_2$ Nanoscrolls","abstract":"Transition metal dichalcogenide (TMD) nanoscrolls (NS) exhibit significant photoluminescence (PL) signals despite their multilayer structure, which cannot be explained by the strained multilayer description of NS. Here, we investigate the interlayer interactions in NS to address this discrepancy. The reduction of interlayer interactions in NS is attributed to two factors: (1) the symmetry-broken mixed stacking order between neighbouring layers due to misalignment, and (2) the high inhomogeneity in the strain landscape resulting from the unique Archimedean spiral-like geometry with positive eccentricity. These were confirmed through transmission electron microscopy, field emission scanning electron microscopy and atomic force microscopy. To probe the effect of reduction of interlayer interactions in multilayered MoS$_2$ nanoscrolls, low-temperature PL spectroscopy was employed investigating the behaviour of K-point excitons. The effects of reduced interlayer interactions on exciton-phonon coupling (EXPC), exciton energy, and exciton oscillator strength are discussed, providing insights into the unique properties of TMD nanoscrolls.","sentences":["Transition metal dichalcogenide (TMD) nanoscrolls (NS) exhibit significant photoluminescence (PL) signals despite their multilayer structure, which cannot be explained by the strained multilayer description of NS.","Here, we investigate the interlayer interactions in NS to address this discrepancy.","The reduction of interlayer interactions in NS is attributed to two factors: (1) the symmetry-broken mixed stacking order between neighbouring layers due to misalignment, and (2) the high inhomogeneity in the strain landscape resulting from the unique Archimedean spiral-like geometry with positive eccentricity.","These were confirmed through transmission electron microscopy, field emission scanning electron microscopy and atomic force microscopy.","To probe the effect of reduction of interlayer interactions in multilayered MoS$_2$ nanoscrolls, low-temperature PL spectroscopy was employed investigating the behaviour of K-point excitons.","The effects of reduced interlayer interactions on exciton-phonon coupling (EXPC), exciton energy, and exciton oscillator strength are discussed, providing insights into the unique properties of TMD nanoscrolls."],"url":"http://arxiv.org/abs/2404.12250v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-18 14:36:04","title":"Entangled states from arborescent knots","abstract":"In this paper we discuss how to use arborescent knots to construct entangled multi-qubit states. We show that Bell-states, GHZ-states and cluster states can be constructed from such knots. The latter are particularly interesting since they form a base for the measurement-based quantum computers.","sentences":["In this paper we discuss how to use arborescent knots to construct entangled multi-qubit states.","We show that Bell-states, GHZ-states and cluster states can be constructed from such knots.","The latter are particularly interesting since they form a base for the measurement-based quantum computers."],"url":"http://arxiv.org/abs/2404.12222v1","category":"hep-th"}
{"created":"2024-04-18 13:10:57","title":"The 2018 outburst of MAXI J1820+070 as seen by Insight-HXMT","abstract":"We present an analysis of the whole 2018 outburst of the black hole X-ray binary MAXI J1820+070 with Insight-HXMT data. We focus our study on the temporal evolution of the parameters of the source. We employ two different models to fit the thermal spectrum of the disk: the Newtonian model DISKBB and the relativistic model NKBB. These two models provide different pictures of the source in the soft state. With DISKBB, we find that the inner edge of the disk is close to the innermost stable circular orbit of a fast-rotating black hole and the corona changes geometry from the hard to the soft state, probably becoming compact and close to the black hole. With NKBB, we find that the disk is truncated in the soft state and that the coronal geometry does not change significantly during the whole outburst. However, we find that the model with NKBB can predict an untruncated disk around a fast-rotating black hole if we assume that the inclination angle of the disk is around 30 degrees (instead of 60 degrees, which is the inclination angle of the jet and is usually adopted as the inclination angle of the disk in the literature) and we employ a high-density reflection model. In such a case, we measure a high value of the black hole spin parameter with observations in the soft state, in agreement with the high spin value found from the analysis of the reflection features and in disagreement with the low spin value found by previous continuum-fitting method measurements with the inclination angle of the disk set to the value of the inclination angle of the jet.","sentences":["We present an analysis of the whole 2018 outburst of the black hole X-ray binary MAXI J1820+070 with Insight-HXMT data.","We focus our study on the temporal evolution of the parameters of the source.","We employ two different models to fit the thermal spectrum of the disk: the Newtonian model DISKBB and the relativistic model NKBB.","These two models provide different pictures of the source in the soft state.","With DISKBB, we find that the inner edge of the disk is close to the innermost stable circular orbit of a fast-rotating black hole and the corona changes geometry from the hard to the soft state, probably becoming compact and close to the black hole.","With NKBB, we find that the disk is truncated in the soft state and that the coronal geometry does not change significantly during the whole outburst.","However, we find that the model with NKBB can predict an untruncated disk around a fast-rotating black hole if we assume that the inclination angle of the disk is around 30 degrees (instead of 60 degrees, which is the inclination angle of the jet and is usually adopted as the inclination angle of the disk in the literature) and we employ a high-density reflection model.","In such a case, we measure a high value of the black hole spin parameter with observations in the soft state, in agreement with the high spin value found from the analysis of the reflection features and in disagreement with the low spin value found by previous continuum-fitting method measurements with the inclination angle of the disk set to the value of the inclination angle of the jet."],"url":"http://arxiv.org/abs/2404.12161v1","category":"astro-ph.HE"}
{"created":"2024-04-18 13:05:17","title":"Euclid preparation. Improving cosmological constraints using a new multi-tracer method with the spectroscopic and photometric samples","abstract":"Future data provided by the \\Euclid mission will allow us to better understand the cosmic history of the Universe. A metric of its performance is the figure-of-merit (FoM) of dark energy, usually estimated with Fisher forecasts. The expected FoM has previously been estimated taking into account the two main probes of \\Euclid, namely the three-dimensional clustering of the spectroscopic galaxy sample, and the so-called 3$\\times$2\\,pt signal from the photometric sample (i.e., the weak lensing signal, the galaxy clustering, and their cross-correlation). So far, these two probes have been treated as independent. In this paper, we introduce a new observable given by the ratio of the (angular) two-point correlation function of galaxies from the two surveys. For identical (normalised) selection functions, this observable is unaffected by sampling noise, and its variance is solely controlled by Poisson noise. We present forecasts for \\Euclid where this multi-tracer method is applied and is particularly relevant because the two surveys will cover the same area of the sky. This method allows for the exploitation of the combination of the spectroscopic and photometric samples. When the correlation between this new observable and the other probes is not taken into account, a significant gain is obtained in the FoM, as well as in the constraints on other cosmological parameters. The benefit is more pronounced for a commonly investigated modified gravity model, namely the $\\gamma$ parametrisation of the growth factor. However, the correlation between the different probes is found to be significant and hence the actual gain is uncertain. We present various strategies for circumventing this issue and still extract useful information from the new observable.","sentences":["Future data provided by the \\Euclid mission will allow us to better understand the cosmic history of the Universe.","A metric of its performance is the figure-of-merit (FoM) of dark energy, usually estimated with Fisher forecasts.","The expected FoM has previously been estimated taking into account the two main probes of \\Euclid, namely the three-dimensional clustering of the spectroscopic galaxy sample, and the so-called 3$\\times$2\\,pt signal from the photometric sample (i.e., the weak lensing signal, the galaxy clustering, and their cross-correlation).","So far, these two probes have been treated as independent.","In this paper, we introduce a new observable given by the ratio of the (angular) two-point correlation function of galaxies from the two surveys.","For identical (normalised) selection functions, this observable is unaffected by sampling noise, and its variance is solely controlled by Poisson noise.","We present forecasts for \\Euclid where this multi-tracer method is applied and is particularly relevant because the two surveys will cover the same area of the sky.","This method allows for the exploitation of the combination of the spectroscopic and photometric samples.","When the correlation between this new observable and the other probes is not taken into account, a significant gain is obtained in the FoM, as well as in the constraints on other cosmological parameters.","The benefit is more pronounced for a commonly investigated modified gravity model, namely the $\\gamma$ parametrisation of the growth factor.","However, the correlation between the different probes is found to be significant and hence the actual gain is uncertain.","We present various strategies for circumventing this issue and still extract useful information from the new observable."],"url":"http://arxiv.org/abs/2404.12157v1","category":"astro-ph.CO"}
{"created":"2024-04-18 13:00:36","title":"The birth of StatPhys: The 1949 Florence conference at the juncture of post-WWII reconstruction of national and international physics","abstract":"In spring 1949 about 70 physicists from eight countries met in Florence to discuss recent trends in statistical mechanics. This scientific gathering, co-organized by the Commission on Thermodynamics and Statistical Mechanics of the International Union of Pure and Applied Physics (IUPAP) and the Italian Physical Society (SIF), initiated a tradition of IUPAP-sponsored international conferences on statistical mechanics that lasts to this day. In 1977, when this conference series took the name of StatPhys, the foundational role of the Florence conference was recognized by retrospectively naming it StatPhys1. This paper examines the dual scientific and social significance of the conference, situating it in the broader contexts of the post-World War II reconstruction in Italian physics and of the revitalization of the international science organization. Through an analysis of IUPAP archives and Italian records, we illustrate how the event's success hinged on the aligned objectives of its organizers. Internationally, it was instrumental in defining the scientific and organizational foundations for the activities of IUPAP commissions during a critical phase of IUPAP's history, when the Union was resurging on the international stage post-interwar period inactivity. Nationally, the conference served as a cornerstone in SIF's strategy to re-establish Italian physics' international stature and to aid the domestic revitalization of physics through the internationalization of its activities, notably of its flagship journal, \\textit{Il Nuovo Cimento}. This analysis not only sheds light on the conference's impact but also informs recent discussions in the history of science about the multiple roles of international scientific conferences.","sentences":["In spring 1949 about 70 physicists from eight countries met in Florence to discuss recent trends in statistical mechanics.","This scientific gathering, co-organized by the Commission on Thermodynamics and Statistical Mechanics of the International Union of Pure and Applied Physics (IUPAP) and the Italian Physical Society (SIF), initiated a tradition of IUPAP-sponsored international conferences on statistical mechanics that lasts to this day.","In 1977, when this conference series took the name of StatPhys, the foundational role of the Florence conference was recognized by retrospectively naming it StatPhys1.","This paper examines the dual scientific and social significance of the conference, situating it in the broader contexts of the post-World War II reconstruction in Italian physics and of the revitalization of the international science organization.","Through an analysis of IUPAP archives and Italian records, we illustrate how the event's success hinged on the aligned objectives of its organizers.","Internationally, it was instrumental in defining the scientific and organizational foundations for the activities of IUPAP commissions during a critical phase of IUPAP's history, when the Union was resurging on the international stage post-interwar period inactivity.","Nationally, the conference served as a cornerstone in SIF's strategy to re-establish Italian physics' international stature and to aid the domestic revitalization of physics through the internationalization of its activities, notably of its flagship journal, \\textit{Il Nuovo Cimento}.","This analysis not only sheds light on the conference's impact but also informs recent discussions in the history of science about the multiple roles of international scientific conferences."],"url":"http://arxiv.org/abs/2404.12156v1","category":"physics.hist-ph"}
{"created":"2024-04-18 12:49:14","title":"Strong decays of the $P_{cs}(4338)$ and its high isospin cousin via the QCD sum rules","abstract":"In the present work, the strong decays of the newly observed $P_{cs}(4338)$ as well as its high isospin cousin $P_{cs}(4460)$ are studied via the QCD sum rules. According to conservation of isospin, spin and parity, the hadronic coupling constants in four decay channels are obtained, then the partial decay widths are obtained. The total width of the $P_{cs}(4338)$ coincides with the experimental data nicely, while the predictions for the $P_{cs}(4460)$ can be testified in the future experiment, and shed light on the nature of the $P_{cs}(4338)$.","sentences":["In the present work, the strong decays of the newly observed $P_{cs}(4338)$ as well as its high isospin cousin $P_{cs}(4460)$ are studied via the QCD sum rules.","According to conservation of isospin, spin and parity, the hadronic coupling constants in four decay channels are obtained, then the partial decay widths are obtained.","The total width of the $P_{cs}(4338)$ coincides with the experimental data nicely, while the predictions for the $P_{cs}(4460)$ can be testified in the future experiment, and shed light on the nature of the $P_{cs}(4338)$."],"url":"http://arxiv.org/abs/2404.12146v1","category":"hep-ph"}
{"created":"2024-04-18 12:11:58","title":"Controlling 4f antiferromagnetic dynamics via itinerant electronic susceptibility","abstract":"Optical manipulation of magnetism holds promise for future ultrafast spintronics, especially with lanthanides and their huge, localized 4f magnetic moments. These moments interact indirectly via the conduction electrons (RKKY exchange), influenced by interatomic orbital overlap, and the conduction electron susceptibility. Here, we study this influence in a series of 4f antiferromagnets, GdT2Si2 (T=Co, Rh, Ir), using ultrafast resonant X-ray diffraction. We observe a twofold increase in ultrafast angular momentum transfer between the materials, originating from modifications in the conduction electron susceptibility, as confirmed by first-principles calculations.","sentences":["Optical manipulation of magnetism holds promise for future ultrafast spintronics, especially with lanthanides and their huge, localized 4f magnetic moments.","These moments interact indirectly via the conduction electrons (RKKY exchange), influenced by interatomic orbital overlap, and the conduction electron susceptibility.","Here, we study this influence in a series of 4f antiferromagnets, GdT2Si2 (T=Co, Rh, Ir), using ultrafast resonant X-ray diffraction.","We observe a twofold increase in ultrafast angular momentum transfer between the materials, originating from modifications in the conduction electron susceptibility, as confirmed by first-principles calculations."],"url":"http://arxiv.org/abs/2404.12119v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-18 11:31:28","title":"Higgs Inflation in Unimodular Gravity","abstract":"The discovery of Higgs mechanism within the context of spontaneous symmetry breaking has offered a new perspective on the early time cosmic inflation and also on the relationship between elementary particles and dark energy, believed to drive the universe's accelerating expansion. We suggest an inflation scenario driven by the Higgs boson within the framework of unimodular gravity, where the Higgs field acts as the inflaton and has a significant non-minimal coupling to the gravity. We present a detailed analysis of the problem in the Jordan and then Einstein frame for a unimodular Higgs inflation, followed by a comparison of our findings with the Cosmic Microwave Background observations made by the Planck Collaboration and other joint data sets. Therefore, new constraints are imposed on the non-minimal coupling parameter, $\\xi$, by determining the magnitudes required for effective cosmic inflation. We demonstrate that a substantial non-minimal coupling of order $\\xi\\sim 10^{2}-10^{4}$ is required for this model to match with the observed primordial spectrum.","sentences":["The discovery of Higgs mechanism within the context of spontaneous symmetry breaking has offered a new perspective on the early time cosmic inflation and also on the relationship between elementary particles and dark energy, believed to drive the universe's accelerating expansion.","We suggest an inflation scenario driven by the Higgs boson within the framework of unimodular gravity, where the Higgs field acts as the inflaton and has a significant non-minimal coupling to the gravity.","We present a detailed analysis of the problem in the Jordan and then Einstein frame for a unimodular Higgs inflation, followed by a comparison of our findings with the Cosmic Microwave Background observations made by the Planck Collaboration and other joint data sets.","Therefore, new constraints are imposed on the non-minimal coupling parameter, $\\xi$, by determining the magnitudes required for effective cosmic inflation.","We demonstrate that a substantial non-minimal coupling of order $\\xi\\sim 10^{2}-10^{4}$ is required for this model to match with the observed primordial spectrum."],"url":"http://arxiv.org/abs/2404.12099v1","category":"hep-th"}
{"created":"2024-04-18 10:27:03","title":"Solitonic ground state in supersymmetric theory in background","abstract":"A solitonic ground state called a chiral soliton lattice (CSL) is realized in a supersymmetric theory with background magnetic field and finite chemical potential. To this end, we construct, in the superfield formalism, a supersymmetric chiral sine-Gordon model as a neutral pion sector of a supersymmetric two-flavor chiral Lagrangian with a Wess-Zumino-Witten term. The CSL ground state appears in the presence of either a strong magnetic field and/or large chemical potential, or a background fermionic condensate in the form of a fermion bilinear consisting of the gaugino and a superpartner of a baryon gauge field.","sentences":["A solitonic ground state called a chiral soliton lattice (CSL) is realized in a supersymmetric theory with background magnetic field and finite chemical potential.","To this end, we construct, in the superfield formalism, a supersymmetric chiral sine-Gordon model as a neutral pion sector of a supersymmetric two-flavor chiral Lagrangian with a Wess-Zumino-Witten term.","The CSL ground state appears in the presence of either a strong magnetic field and/or large chemical potential, or a background fermionic condensate in the form of a fermion bilinear consisting of the gaugino and a superpartner of a baryon gauge field."],"url":"http://arxiv.org/abs/2404.12066v1","category":"hep-th"}
{"created":"2024-04-18 09:59:50","title":"tt+X measurements by CMS and ATLAS","abstract":"These proceedings present the observation of the 4-top production process and the latest results of the production of top-quark pairs associated with W and $\\gamma$ bosons ($t\\bar{t}W$ and $t\\bar{t}\\gamma$) and b jets ($t\\bar{t}b\\bar{b}$) at a collision energy of 13 TeV carried out by the CMS and ATLAS Collaborations.","sentences":["These proceedings present the observation of the 4-top production process and the latest results of the production of top-quark pairs associated with W and $\\gamma$ bosons ($t\\bar{t}W$ and $t\\bar{t}\\gamma$) and b jets ($t\\bar{t}b\\bar{b}$) at a collision energy of 13 TeV carried out by the CMS and ATLAS Collaborations."],"url":"http://arxiv.org/abs/2404.12046v1","category":"hep-ex"}
{"created":"2024-04-18 09:46:25","title":"Uncovering Safety Risks in Open-source LLMs through Concept Activation Vector","abstract":"Current open-source large language models (LLMs) are often undergone careful safety alignment before public release. Some attack methods have also been proposed that help check for safety vulnerabilities in LLMs to ensure alignment robustness. However, many of these methods have moderate attack success rates. Even when successful, the harmfulness of their outputs cannot be guaranteed, leading to suspicions that these methods have not accurately identified the safety vulnerabilities of LLMs. In this paper, we introduce a LLM attack method utilizing concept-based model explanation, where we extract safety concept activation vectors (SCAVs) from LLMs' activation space, enabling efficient attacks on well-aligned LLMs like LLaMA-2, achieving near 100% attack success rate as if LLMs are completely unaligned. This suggests that LLMs, even after thorough safety alignment, could still pose potential risks to society upon public release. To evaluate the harmfulness of outputs resulting with various attack methods, we propose a comprehensive evaluation method that reduces the potential inaccuracies of existing evaluations, and further validate that our method causes more harmful content. Additionally, we discover that the SCAVs show some transferability across different open-source LLMs.","sentences":["Current open-source large language models (LLMs) are often undergone careful safety alignment before public release.","Some attack methods have also been proposed that help check for safety vulnerabilities in LLMs to ensure alignment robustness.","However, many of these methods have moderate attack success rates.","Even when successful, the harmfulness of their outputs cannot be guaranteed, leading to suspicions that these methods have not accurately identified the safety vulnerabilities of LLMs.","In this paper, we introduce a LLM attack method utilizing concept-based model explanation, where we extract safety concept activation vectors (SCAVs) from LLMs' activation space, enabling efficient attacks on well-aligned LLMs like LLaMA-2, achieving near 100% attack success rate as if LLMs are completely unaligned.","This suggests that LLMs, even after thorough safety alignment, could still pose potential risks to society upon public release.","To evaluate the harmfulness of outputs resulting with various attack methods, we propose a comprehensive evaluation method that reduces the potential inaccuracies of existing evaluations, and further validate that our method causes more harmful content.","Additionally, we discover that the SCAVs show some transferability across different open-source LLMs."],"url":"http://arxiv.org/abs/2404.12038v1","category":"cs.CL"}
{"created":"2024-04-18 08:55:45","title":"First-principles study of phase transition in cadmium titanate by molecular dynamics incorporating nuclear quantum effects","abstract":"First-principles molecular dynamics (FPMD) simulations were applied for the paraelectric-ferroelectric phase transition in the perovskite-type cadmium titanate, CdTiO3. Since the phase transition is reported to occur at the low temperature around 80 K, the quantum thermal bath (QTB) method was utilized in this study, which incorporates the nuclear quantum effects (NQEs). The structural evolutions in the QTB-FPMD simulations are in reasonable agreement with the experimental results, by contrast in the conventional FPMD simulations using the classical thermal bath (CTB-FPMD). According to our phonon calculations, volume expansion is the key in the stabilization of the ferroelectric phase at low temperatures, which was well reproduced in the QTB-FPMD with the NQEs. Thus, the NQEs are of importance in phase transitions at low temperatures, particularly below the room temperature, and the QTB is useful in that it incorporates the NQEs in MD simulations with low computational costs comparable to the conventional CTB.","sentences":["First-principles molecular dynamics (FPMD) simulations were applied for the paraelectric-ferroelectric phase transition in the perovskite-type cadmium titanate, CdTiO3.","Since the phase transition is reported to occur at the low temperature around 80 K, the quantum thermal bath (QTB) method was utilized in this study, which incorporates the nuclear quantum effects (NQEs).","The structural evolutions in the QTB-FPMD simulations are in reasonable agreement with the experimental results, by contrast in the conventional FPMD simulations using the classical thermal bath (CTB-FPMD).","According to our phonon calculations, volume expansion is the key in the stabilization of the ferroelectric phase at low temperatures, which was well reproduced in the QTB-FPMD with the NQEs.","Thus, the NQEs are of importance in phase transitions at low temperatures, particularly below the room temperature, and the QTB is useful in that it incorporates the NQEs in MD simulations with low computational costs comparable to the conventional CTB."],"url":"http://arxiv.org/abs/2404.12004v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-18 08:52:22","title":"Comparing the three-dimensional morphological asymmetries in the ejecta of Kepler and Tycho in X-rays","abstract":"Recent simulations have shown that asymmetries in the ejecta distribution of supernova remnants (SNRs) may be a reflection of asymmetries left over from the initial supernova explosion. Thus, SNR studies provide a vital means for testing and constraining model predictions in relation to the distribution of heavy elements, which are key to improving our understanding of the explosion mechanisms in Type Ia supernovae.   The use of a novel blind source separation method applied to the megasecond X-ray observations of the historic Kepler and Tycho supernova remnants has revealed maps of the ejecta distribution. These maps are endowed with an unprecedented level of detail and clear separations from the continuum emission. Our method also provides a three-dimensional (3D) view of the ejecta by individually disentangling red- and blueshifted spectral components associated with images of the Si, S, Ar, Ca, and Fe emission. This approach provides insights into the morphology of the ejecta distribution in those two remnants.   Those mappings have allowed us to thoroughly investigate the asymmetries in the intermediate-mass elements and Fe distribution in two Type Ia supernova remnants. We also compared the results with the core-collapse Cassiopeia A remnant, which we had studied previously. The images obtained confirm, as expected for Type Ia SNRs, that the Fe distribution is mostly closer to the core than that of intermediate-mass elements. They also highlight peculiar features in the ejecta distribution, such as the Fe-rich southeastern knot in Tycho.","sentences":["Recent simulations have shown that asymmetries in the ejecta distribution of supernova remnants (SNRs) may be a reflection of asymmetries left over from the initial supernova explosion.","Thus, SNR studies provide a vital means for testing and constraining model predictions in relation to the distribution of heavy elements, which are key to improving our understanding of the explosion mechanisms in Type Ia supernovae.   ","The use of a novel blind source separation method applied to the megasecond X-ray observations of the historic Kepler and Tycho supernova remnants has revealed maps of the ejecta distribution.","These maps are endowed with an unprecedented level of detail and clear separations from the continuum emission.","Our method also provides a three-dimensional (3D) view of the ejecta by individually disentangling red- and blueshifted spectral components associated with images of the Si, S, Ar, Ca, and Fe emission.","This approach provides insights into the morphology of the ejecta distribution in those two remnants.   ","Those mappings have allowed us to thoroughly investigate the asymmetries in the intermediate-mass elements and Fe distribution in two Type Ia supernova remnants.","We also compared the results with the core-collapse Cassiopeia A remnant, which we had studied previously.","The images obtained confirm, as expected for Type Ia SNRs, that the Fe distribution is mostly closer to the core than that of intermediate-mass elements.","They also highlight peculiar features in the ejecta distribution, such as the Fe-rich southeastern knot in Tycho."],"url":"http://arxiv.org/abs/2404.12002v1","category":"astro-ph.HE"}
{"created":"2024-04-18 08:36:21","title":"Single-channel, single-energy partial-wave analysis with continuity improved through minimal phase constraints","abstract":"Single-energy partial-wave analysis has often been applied as a way to fit data with minimal model dependence. However, remaining unconstrained, partial waves at neighboring energies will vary discontinuously because the overall amplitude phase cannot be determined through single-channel measurements. This problem can be mitigated through the use of a constraining penalty function based on an associated energy-dependent fit. However, the weight given to this constraint results in a biased fit to the data. In this paper, for the first time, we explore a constraining function which does not influence the fit to data. The constraint comes from the overall phase found in multi-channel fits which, in the present study, are the Bonn-Gatchina and J\\\"ulich-Bonn multi-channel analyses. The data are well reproduced and weighting of the penalty function does not influence the result. The method is applied to $K \\Lambda$ photoproduction data and all observables can be maximally well reproduced. While the employed multi-channel analyses display very different multipole amplitudes, we show that the major difference between two sets of multipoles can be related to the different overall phases.","sentences":["Single-energy partial-wave analysis has often been applied as a way to fit data with minimal model dependence.","However, remaining unconstrained, partial waves at neighboring energies will vary discontinuously because the overall amplitude phase cannot be determined through single-channel measurements.","This problem can be mitigated through the use of a constraining penalty function based on an associated energy-dependent fit.","However, the weight given to this constraint results in a biased fit to the data.","In this paper, for the first time, we explore a constraining function which does not influence the fit to data.","The constraint comes from the overall phase found in multi-channel fits which, in the present study, are the Bonn-Gatchina and J\\\"ulich-Bonn multi-channel analyses.","The data are well reproduced and weighting of the penalty function does not influence the result.","The method is applied to $K \\Lambda$ photoproduction data and all observables can be maximally well reproduced.","While the employed multi-channel analyses display very different multipole amplitudes, we show that the major difference between two sets of multipoles can be related to the different overall phases."],"url":"http://arxiv.org/abs/2404.11990v1","category":"nucl-th"}
{"created":"2024-04-18 07:13:30","title":"Generation of Ultrarelativistic Vortex Leptons with Large Orbital Angular Momenta","abstract":"Ultrarelativistic vortex leptons with intrinsic orbital angular momenta (OAM) have important applications in high energy particle physics, nuclear physics, astrophysics, etc. However, unfortunately, their generation still poses a great challenge. Here, we put forward a novel method for generating ultrarelativistic vortex positrons and electrons through nonlinear Breit-Wheeler (NBW) scattering of vortex $\\gamma$ photons. For the first time, a complete angular momentum-resolved scattering theory has been formulated, introducing the angular momentum of laser photons and vortex particles into the conventional NBW scattering framework. We find that vortex positron (electron) can be produced when the outgoing electron (positron) is generated along the collision axis. By unveiling the angular momentum transfer mechanism, we clarify that OAM of the $\\gamma$ photon and angular momenta of multiple laser photons are entirely transferred to the generated pairs, leading to the production of ultrarelativistic vortex positrons or electrons with large OAM. Furthermore, we find that the cone opening angle and superposition state of the vortex $\\gamma$ photon, distinct characteristics aside from its intrinsic OAM, can be determined via the angular distribution of created pairs in NBW processes. Our method paves the way for investigating strong-field quantum electrodynamics processes concerning the generation and detection of vortex particle beams in intense lasers.","sentences":["Ultrarelativistic vortex leptons with intrinsic orbital angular momenta (OAM) have important applications in high energy particle physics, nuclear physics, astrophysics, etc.","However, unfortunately, their generation still poses a great challenge.","Here, we put forward a novel method for generating ultrarelativistic vortex positrons and electrons through nonlinear Breit-Wheeler (NBW) scattering of vortex $\\gamma$ photons.","For the first time, a complete angular momentum-resolved scattering theory has been formulated, introducing the angular momentum of laser photons and vortex particles into the conventional NBW scattering framework.","We find that vortex positron (electron) can be produced when the outgoing electron (positron) is generated along the collision axis.","By unveiling the angular momentum transfer mechanism, we clarify that OAM of the $\\gamma$ photon and angular momenta of multiple laser photons are entirely transferred to the generated pairs, leading to the production of ultrarelativistic vortex positrons or electrons with large OAM.","Furthermore, we find that the cone opening angle and superposition state of the vortex $\\gamma$ photon, distinct characteristics aside from its intrinsic OAM, can be determined via the angular distribution of created pairs in NBW processes.","Our method paves the way for investigating strong-field quantum electrodynamics processes concerning the generation and detection of vortex particle beams in intense lasers."],"url":"http://arxiv.org/abs/2404.11952v1","category":"hep-ph"}
{"created":"2024-04-18 07:10:20","title":"Pair density waves in the strong-coupling two-dimensional Holstein-Hubbard model: a variational Monte Carlo study","abstract":"A robust theory of the mechanism of pair density wave (PDW) superconductivity (i.e. where Cooper pairs have nonzero center of mass momentum) remains elusive. Here we explore the triangular lattice $t$-$J$-$V$ model, a low-energy effective theory derived from the strong-coupling limit of the Holstein-Hubbard model, by large-scale variational Monte Carlo simulations. When the electron density is sufficiently low, the favored ground state is an s-wave PDW, consistent with results obtained from previous studies in this limit. Additionally, a PDW ground state with nematic d-wave pairing emerges in intermediate range of electron densities and phonon frequencies. For these s-wave and d-wave PDWs arising in states with spontaneous breaking of time-reversal and inversion symmetries, PDW formation derives from valley-polarization and intra-pocket pairing.","sentences":["A robust theory of the mechanism of pair density wave (PDW) superconductivity (i.e. where Cooper pairs have nonzero center of mass momentum) remains elusive.","Here we explore the triangular lattice $t$-$J$-$V$ model, a low-energy effective theory derived from the strong-coupling limit of the Holstein-Hubbard model, by large-scale variational Monte Carlo simulations.","When the electron density is sufficiently low, the favored ground state is an s-wave PDW, consistent with results obtained from previous studies in this limit.","Additionally, a PDW ground state with nematic d-wave pairing emerges in intermediate range of electron densities and phonon frequencies.","For these s-wave and d-wave PDWs arising in states with spontaneous breaking of time-reversal and inversion symmetries, PDW formation derives from valley-polarization and intra-pocket pairing."],"url":"http://arxiv.org/abs/2404.11950v1","category":"cond-mat.str-el"}
{"created":"2024-04-18 07:02:57","title":"Coherent Inverse Compton Scattering in Fast Radio Bursts Revisited","abstract":"Growing observations of temporal, spectral, and polarization properties of fast radio bursts (FRBs) indicate that the radio emission of the majority of bursts is likely produced inside the magnetosphere of its central engine, likely a magnetar. We revisit the idea that FRBs are generated via coherent inverse Compton scattering (ICS) off low-frequency X-mode electromagnetic waves (fast magnetosonic waves) by bunches at a distance of a few hundred times of the magnetar radius. Following findings are revealed: 1. Crustal oscillations during a flaring event would excite kHz Alfv\\'en waves. Fast magnetosonic waves with the same frequency can be generated directly or be converted from Alfv\\'en waves at a large radius, with an amplitude large enough to power FRBs via the ICS process. 2. The cross section increases rapidly with radius and significant ICS can occur at $r \\gtrsim 100 R_\\star$ with emission power much greater than the curvature radiation power but still in the linear scattering regime. 3. The low-frequency fast magnetosonic waves naturally redistribute a fluctuating relativistic plasma in the charge-depleted region to form bunches with the right size to power FRBs. 4. The required bunch net charge density can be sub-Goldreich-Julian, which allows a strong parallel electric field to accelerate the charges, maintain the bunches, and continuously power FRB emission. 5. This model can account for a wide range of observed properties of repeating FRB bursts, including high degrees of linear and circular polarization and narrow spectra as observed in many bursts from repeating FRB sources.","sentences":["Growing observations of temporal, spectral, and polarization properties of fast radio bursts (FRBs) indicate that the radio emission of the majority of bursts is likely produced inside the magnetosphere of its central engine, likely a magnetar.","We revisit the idea that FRBs are generated via coherent inverse Compton scattering (ICS) off low-frequency X-mode electromagnetic waves (fast magnetosonic waves) by bunches at a distance of a few hundred times of the magnetar radius.","Following findings are revealed: 1.","Crustal oscillations during a flaring event would excite kHz Alfv\\'en waves.","Fast magnetosonic waves with the same frequency can be generated directly or be converted from Alfv\\'en waves at a large radius, with an amplitude large enough to power FRBs via the ICS process.","2.","The cross section increases rapidly with radius and significant ICS can occur at $r \\gtrsim 100 R_\\star$ with emission power much greater than the curvature radiation power but still in the linear scattering regime.","3.","The low-frequency fast magnetosonic waves naturally redistribute a fluctuating relativistic plasma in the charge-depleted region to form bunches with the right size to power FRBs.","4.","The required bunch net charge density can be sub-Goldreich-Julian, which allows a strong parallel electric field to accelerate the charges, maintain the bunches, and continuously power FRB emission.","5.","This model can account for a wide range of observed properties of repeating FRB bursts, including high degrees of linear and circular polarization and narrow spectra as observed in many bursts from repeating FRB sources."],"url":"http://arxiv.org/abs/2404.11948v1","category":"astro-ph.HE"}
{"created":"2024-04-18 06:21:10","title":"Cosmological Inflation and Dark Sector from 11D Supergravity","abstract":"We explore compactifications of the form of three tori and one circle in the framework of 11D supergravity. By imposing suitable gauge conditions and boundary conditions, we find that the four-dimensional FRW universe emerges as a solution representing cosmological D3-branes in the eleven-dimensional bulk. These specific compactification methods can produce cosmological inflation that aligns with the observational constraints set by the 2021 BICEP/Keck and Planck 2018 results. In the cosmological inflation models we construct, the inflaton can be interpreted as the conformal vibrations of extra dimensions with a size around 10^5 times the reduced Planck length. Additionally, we offer the theoretical predictions for the mass of the inflaton, and the tree-level Newton's gravity law between two massive point particles surrounded by a spherically symmetric distribution of the inflaton, which can reproduce the Tully-Fisher relation and explain the flat rotation curves of galaxies.","sentences":["We explore compactifications of the form of three tori and one circle in the framework of 11D supergravity.","By imposing suitable gauge conditions and boundary conditions, we find that the four-dimensional FRW universe emerges as a solution representing cosmological D3-branes in the eleven-dimensional bulk.","These specific compactification methods can produce cosmological inflation that aligns with the observational constraints set by the 2021 BICEP/Keck and Planck 2018 results.","In the cosmological inflation models we construct, the inflaton can be interpreted as the conformal vibrations of extra dimensions with a size around 10^5 times the reduced Planck length.","Additionally, we offer the theoretical predictions for the mass of the inflaton, and the tree-level Newton's gravity law between two massive point particles surrounded by a spherically symmetric distribution of the inflaton, which can reproduce the Tully-Fisher relation and explain the flat rotation curves of galaxies."],"url":"http://arxiv.org/abs/2404.11933v1","category":"hep-th"}
{"created":"2024-04-18 06:19:33","title":"Study of structure of deuteron from analysis of bremsstrahlung emission in proton-deuteron scattering in cluster models","abstract":"Purpose: In this paper we investigated emission of bremsstrahlung photons in the scattering of protons off deuterons within the microscopic cluster models in a wide region of the beam energy from low energies up to 1.5 GeV. Methods: Three-cluster model of bremsstrahlung is constructed for such a reaction. Formalism of the model includes form factor of deuteron which characterizes dependence of bremsstrahlung cross sections on structure of deuteron. This gives possibility to investigate the structure of nuclei from analysis of bremsstrahlung cross sections. Results: We studied dependence of the bremsstrahlung cross section on the structure of deuteron. We use three different shapes of the deuteron wave functions. Besides, we also calculate the cross section by neglecting internal structure of deuteron. Analysis of dependence of the cross section on such a parameter shows the following. (1) At beam energies 145 and 195 MeV used in experiments bremsstrahlung cross section is not sensitive visibly on variations of the shape of the deuteron wave functions. (2) Stable difference between cross sections calculated with and without internal structure of deuteron is observed at higher energy of beam (larger 500 MeV). (3) The spectrum is increased as we pass from structureless deuteron (the oscillator length $b=0$) to the deuteron discribed by the shell-model wave function (the realistic oscillator length) inside the full energy region of the emitted photons. Conclusion: Our cluster model is a suitable tool to study the structure of deuteron with high enough precision from bremsstrahlung analysis. We propose new experiments for such an investigation.","sentences":["Purpose: In this paper we investigated emission of bremsstrahlung photons in the scattering of protons off deuterons within the microscopic cluster models in a wide region of the beam energy from low energies up to 1.5 GeV. Methods: Three-cluster model of bremsstrahlung is constructed for such a reaction.","Formalism of the model includes form factor of deuteron which characterizes dependence of bremsstrahlung cross sections on structure of deuteron.","This gives possibility to investigate the structure of nuclei from analysis of bremsstrahlung cross sections.","Results:","We studied dependence of the bremsstrahlung cross section on the structure of deuteron.","We use three different shapes of the deuteron wave functions.","Besides, we also calculate the cross section by neglecting internal structure of deuteron.","Analysis of dependence of the cross section on such a parameter shows the following.","(1) At beam energies 145 and 195 MeV used in experiments bremsstrahlung cross section is not sensitive visibly on variations of the shape of the deuteron wave functions.","(2) Stable difference between cross sections calculated with and without internal structure of deuteron is observed at higher energy of beam (larger 500 MeV).","(3) The spectrum is increased as we pass from structureless deuteron (the oscillator length $b=0$) to the deuteron discribed by the shell-model wave function (the realistic oscillator length) inside the full energy region of the emitted photons.","Conclusion: Our cluster model is a suitable tool to study the structure of deuteron with high enough precision from bremsstrahlung analysis.","We propose new experiments for such an investigation."],"url":"http://arxiv.org/abs/2404.11930v1","category":"nucl-th"}
{"created":"2024-04-18 05:31:23","title":"Time Flow and Flavor Mixing in the Flavor Spin Theory","abstract":"We show that in the flavor spin theories (theories with non-Euclidean signature of the kinematic quadratic term of the Lagrangian) for physical mass eigenstates the transition from the interaction to the mass eigenstate basis is constrained. In the corresponding modification of the SM the constraints result in the experimentally observed textures of both lepton and quark flavor mixing matrices. Furthermore, the number of real mixing parameters is reduced by one for each of the quark and lepton sectors. In addition some pairs of elements of V_CKM and separately U_PMNS must have the same absolute values. The equalities are observed experimentally within the error bounds. Apart from its imprint on the flavor mixing the modified SM is identical to the SM.","sentences":["We show that in the flavor spin theories (theories with non-Euclidean signature of the kinematic quadratic term of the Lagrangian) for physical mass eigenstates the transition from the interaction to the mass eigenstate basis is constrained.","In the corresponding modification of the SM the constraints result in the experimentally observed textures of both lepton and quark flavor mixing matrices.","Furthermore, the number of real mixing parameters is reduced by one for each of the quark and lepton sectors.","In addition some pairs of elements of V_CKM and separately U_PMNS must have the same absolute values.","The equalities are observed experimentally within the error bounds.","Apart from its imprint on the flavor mixing the modified SM is identical to the SM."],"url":"http://arxiv.org/abs/2404.11914v1","category":"hep-ph"}
{"created":"2024-04-18 05:26:14","title":"General Relativity from Intersection Theory","abstract":"This paper combines the post-Minkowskian expansion of general relativity with the language of intersection theory. Due to the nature of the soft limit inherent to the post-Minkowskian expansion, the intersection-based approach is of enhanced utility in that theory compared to a generic QFT. In the language of intersection theory, Feynman integrals are rephrased in terms of twisted cocycles. The intersection number is a pairing between two such cocycles and its existence allows for the direct projection onto a basis of master integrals. In this paper we use this approach to compute the 2PM contribution to the scattering of two compact astronomical objects, getting results in agreement with previous findings.","sentences":["This paper combines the post-Minkowskian expansion of general relativity with the language of intersection theory.","Due to the nature of the soft limit inherent to the post-Minkowskian expansion, the intersection-based approach is of enhanced utility in that theory compared to a generic QFT.","In the language of intersection theory, Feynman integrals are rephrased in terms of twisted cocycles.","The intersection number is a pairing between two such cocycles and its existence allows for the direct projection onto a basis of master integrals.","In this paper we use this approach to compute the 2PM contribution to the scattering of two compact astronomical objects, getting results in agreement with previous findings."],"url":"http://arxiv.org/abs/2404.11913v1","category":"gr-qc"}
{"created":"2024-04-18 04:51:35","title":"Systematic shell-model analysis of $2\u03bd\u03b2\u03b2$ decay of $^{76}$Ge and $^{96}$Zr to the ground and excited states of $^{76}$Se and $^{96}$Mo","abstract":"In this work, we have studied the $2\\nu\\beta\\beta$ decay of $^{76}$Ge and $^{96}$Zr isotopes utilizing large-scale shell-model calculations. The GWBXG effective interaction has been employed in the calculation of $2\\nu\\beta\\beta$-decay nuclear matrix elements (NMEs). We have tested the effective interaction by comparing the predicted spectroscopic properties, such as energy spectra and transition probabilities, with the available experimental data. The variation of cumulative NMEs with respect to the $1^+$ state energies of the intermediate nucleus is also studied, corresponding to $0^+_{\\rm g.s}\\rightarrow0^+_{\\rm g.s}$, $0^+_{\\rm g.s}\\rightarrow0^+_{2}$, and $0^+_{\\rm g.s}\\rightarrow2^+_{1}$ transitions between the parent and granddaughter nuclei. The extracted half-lives using the shell-model predicted NMEs show good agreement with the recent experimental data. The comparison of the shell-model predicted NMEs with previous NMEs available in the literature is discussed. Also, the computed branching ratios for the $2\\nu\\beta\\beta$ decay of $^{76}$Ge and both the $2\\nu\\beta\\beta$ and single-$\\beta$ decay of $^{96}$Zr are reported.","sentences":["In this work, we have studied the $2\\nu\\beta\\beta$ decay of $^{76}$Ge and $^{96}$Zr isotopes utilizing large-scale shell-model calculations.","The GWBXG effective interaction has been employed in the calculation of $2\\nu\\beta\\beta$-decay nuclear matrix elements (NMEs).","We have tested the effective interaction by comparing the predicted spectroscopic properties, such as energy spectra and transition probabilities, with the available experimental data.","The variation of cumulative NMEs with respect to the $1^+$ state energies of the intermediate nucleus is also studied, corresponding to $0^+_{\\rm g.s}\\rightarrow0^+_{\\rm g.s}$, $0^+_{\\rm g.s}\\rightarrow0^+_{2}$, and $0^+_{\\rm g.s}\\rightarrow2^+_{1}$ transitions between the parent and granddaughter nuclei.","The extracted half-lives using the shell-model predicted NMEs show good agreement with the recent experimental data.","The comparison of the shell-model predicted NMEs with previous NMEs available in the literature is discussed.","Also, the computed branching ratios for the $2\\nu\\beta\\beta$ decay of $^{76}$Ge and both the $2\\nu\\beta\\beta$ and single-$\\beta$ decay of $^{96}$Zr are reported."],"url":"http://arxiv.org/abs/2404.11896v1","category":"nucl-th"}
{"created":"2024-04-18 04:36:56","title":"Dark supernova remnant buried in the Galactic-Centre \"Brick\" G0.253+0.016 revealed by an expanding CO-line bubble","abstract":"We performed a $^{12}$CO- and $^{13}$CO($J=1-0$)-line study of the \"Brick\" (G0.253+0.016) in the Galactic Centre (GC) by analyzing the archival data obtained with the Nobeyama 45-m telescope. We present kinematics and molecular gas distributions in the longitude-velocity diagrams, and suggest that the Brick is located along the GC Arm I in the central molecular zone (CMZ) in front of the GC, which yields a distance of 8 kpc and GC radius 0.2 kpc. The major and minor-axis diameters of the Brick are $D_x\\times D_y=8.4 \\ {\\rm pc} \\times 4.1 \\ {\\rm pc}$, and the scale radius is $r_{\\rm bri}=\\sqrt{D_x D_y}=2.96$ pc. The molecular mass inferred from the CO-line integrated intensity is $M_{\\rm brixco} \\sim 5.1\\times 10^4 M_\\odot$ for a conversion factor $X_{\\rm gc}=1.0\\times 10^{20}$ H$_2$ cm$^{-2}$ [K km s$^{-1}$]$^{-1}$, while the Virial mass for the velocity dispersion of $\\sigma_v=10.0 $ km s $^{-1}$ is calculated to be $M_{\\rm brivir}\\sim 6.8 \\times 10^4 \\ M_\\odot$, which yields a new conversion factor of $X_{\\rm bri} =1.3\\times 10^{20}$ H$_2$ cm$^{-2}$ [K km s$^{-1}$]$^{-1}$. No thermal radio emission indicative of HII region and star formation is found in radio-continuum archive. The Brick's center has a cavity surrounded by a spherical molecular bubble of radius $r_{\\rm bub}=1.85$ pc and mass $\\sim 1.7\\times 10^4 M_\\odot$ expanding at $v_{\\rm exp}=10$ km s$^{-1}$ with kinetic energy of $E_0\\sim 1.7\\times 10^{49}$ erg. If the bubble is approximated by an adiabatic spherical shock wave, its age is $t\\sim 2/5 r_{\\rm bub}/v_{\\rm exp}\\sim 7.2\\times 10^4$ y. We suggest that the bubble will be a dark supernova remnant buried in the dense molecular cloud. The Brick, therefore, experienced massive-star formation followed by a supernova explosion more than $\\sim 10^5$ y ago.","sentences":["We performed a $^{12}$CO- and $^{13}$CO($J=1-0$)-line study of the \"Brick\" (G0.253+0.016) in the Galactic Centre (GC) by analyzing the archival data obtained with the Nobeyama 45-m telescope.","We present kinematics and molecular gas distributions in the longitude-velocity diagrams, and suggest that the Brick is located along the GC Arm I in the central molecular zone (CMZ) in front of the GC, which yields a distance of 8 kpc and GC radius 0.2 kpc.","The major and minor-axis diameters of the Brick are $D_x\\times D_y=8.4 \\ {\\rm pc} \\times 4.1 \\ {\\rm pc}$, and the scale radius is $r_{\\rm bri}=\\sqrt{D_x D_y}=2.96$ pc.","The molecular mass inferred from the CO-line integrated intensity is $M_{\\rm brixco} \\sim 5.1\\times 10^4 M_\\odot$ for a conversion factor $X_{\\rm gc}=1.0\\times 10^{20}$ H$_2$ cm$^{-2}$","[K km s$^{-1}$]$^{-1}$, while the Virial mass for the velocity dispersion of $\\sigma_v=10.0 $ km s $^{-1}$ is calculated to be $M_{\\rm brivir}\\sim 6.8 \\times 10^4 \\ M_\\odot$, which yields a new conversion factor of $X_{\\rm bri} =1.3\\times 10^{20}$ H$_2$ cm$^{-2}$","[K km s$^{-1}$]$^{-1}$. No thermal radio emission indicative of HII region and star formation is found in radio-continuum archive.","The Brick's center has a cavity surrounded by a spherical molecular bubble of radius $r_{\\rm bub}=1.85$ pc and mass $\\sim 1.7\\times 10","^4 M_\\odot$ expanding at $v_{\\rm exp}=10$ km s$^{-1}$ with kinetic energy of $E_0\\sim 1.7\\times 10^{49}$ erg.","If the bubble is approximated by an adiabatic spherical shock wave, its age is $t\\sim 2/5 r_{\\rm bub}/v_{\\rm exp}\\sim 7.2\\times 10^4$ y. We suggest that the bubble will be a dark supernova remnant buried in the dense molecular cloud.","The Brick, therefore, experienced massive-star formation followed by a supernova explosion more than $\\sim 10^5$ y ago."],"url":"http://arxiv.org/abs/2404.11892v1","category":"astro-ph.GA"}
{"created":"2024-04-18 02:52:05","title":"Gravitational waves from nonradial oscillations of stochastically accreting neutron stars","abstract":"Oscillating neutron stars are sources of continuous gravitational waves. We study analytically the excitation of stellar oscillations by the mechanical impact on the stellar surface of ''clumps'' of stochastically accreted matter. We calculate the waveform and spectrum of the gravitational wave signal emitted by the accretion-driven pulsations. Results are generated for an idealised model of a nonrotating, unmagnetised, one-component star with uniform polytropic index $n_{\\rm poly}$ assuming Newtonian gravity and the Cowling approximation. We find that the excited mode amplitudes grow with increasing $n_{\\rm poly}$ and mode order $n$. The gravitational wave signal forms a sequence of amplitude-modulated packets for $n_{\\rm poly}=1$, lasting $\\sim 10^{-3}$s after each impact. The gravitational wave strain increases with increasing $n_{\\rm poly}$, but decreases with increasing $n$ and increasing multipole order $l$ for $n_{\\rm poly}=1$. In the observing band of current long-baseline interferometers, $g$-modes emit higher, narrower peaks in the amplitude spectral density than $f$- and $p$-modes, with the highest peaks reaching $\\sim 10^{-26}$Hz$^{-1/2}$ for modes with damping time $\\tau_{nl} \\sim 10^{8}$yr. The root-mean-square strain $h_{\\text{rms}}$, calculated by summing over modes with $2\\leq l\\leq4$ and $\\tau_{nl} \\leq 10^{8}$yr, spans the range $10^{-33} \\leq h_{\\text{rms}} \\leq 10^{-32}$ for $1\\leq n_{\\text{poly}}\\leq 2$.","sentences":["Oscillating neutron stars are sources of continuous gravitational waves.","We study analytically the excitation of stellar oscillations by the mechanical impact on the stellar surface of ''clumps'' of stochastically accreted matter.","We calculate the waveform and spectrum of the gravitational wave signal emitted by the accretion-driven pulsations.","Results are generated for an idealised model of a nonrotating, unmagnetised, one-component star with uniform polytropic index $n_{\\rm poly}$ assuming Newtonian gravity and the Cowling approximation.","We find that the excited mode amplitudes grow with increasing $n_{\\rm poly}$ and mode order $n$. The gravitational wave signal forms a sequence of amplitude-modulated packets for $n_{\\rm poly}=1$, lasting $\\sim 10^{-3}$s after each impact.","The gravitational wave strain increases with increasing $n_{\\rm poly}$, but decreases with increasing $n$ and increasing multipole order $l$ for $n_{\\rm poly}=1$.","In the observing band of current long-baseline interferometers, $g$-modes emit higher, narrower peaks in the amplitude spectral density than $f$- and $p$-modes, with the highest peaks reaching $\\sim 10^{-26}$Hz$^{-1/2}$ for modes with damping time $\\tau_{nl} \\sim 10^{8}$yr.","The root-mean-square strain $h_{\\text{rms}}$, calculated by summing over modes with $2\\leq l\\leq4$ and $\\tau_{nl} \\leq 10^{8}$yr, spans the range $10^{-33} \\leq h_{\\text{rms}} \\leq 10^{-32}$ for $1\\leq n_{\\text{poly}}\\leq 2$."],"url":"http://arxiv.org/abs/2404.11866v1","category":"astro-ph.HE"}
{"created":"2024-04-18 01:24:48","title":"Evidence for a nearly orthogonal rotator in GX 301--2 with phase-resolved cyclotron resonant scattering features","abstract":"Cyclotron resonant scattering features (CRSFs) are the absorption features in the X-ray spectra of strongly magnetized accretion neutron stars (NSs), which are probably the most reliable probe to the surface magnetic fields of NSs. The high mass X-ray binary GX 301--2 exhibits a very wide, variable and complicated CRSF in the average spectra, which should be two absorption lines based on NuStar and Insight-HXMT observations. With the Insight-HXMT frequent observations, we performed the phase-resolved spectroscopy and confirmed two cyclotron absorption lines in the phase-resolved spectra, with their centroid energy ratio $\\sim 1.6-1.7$ in the super-critical luminosity case. A major hindrance in understanding those CRSFs is the very poorly constrained magnetic inclination angle, which is also a fundamental property of a NS and key to understanding the emission characteristics of a pulsar. Comparing the phase-resolved CRSF with simulated X-ray spectra, the magnetic inclination angle is found to be $\\gtrsim 70^{\\circ}$, i.e., nearly orthogonal between the NS's spin and magnetic axies. The implications of an orthogonal rotator and magnetic structure evolution in the accreting X-ray binary are also discussed.","sentences":["Cyclotron resonant scattering features (CRSFs) are the absorption features in the X-ray spectra of strongly magnetized accretion neutron stars (NSs), which are probably the most reliable probe to the surface magnetic fields of NSs.","The high mass X-ray binary GX 301--2 exhibits a very wide, variable and complicated CRSF in the average spectra, which should be two absorption lines based on NuStar and Insight-HXMT observations.","With the Insight-HXMT frequent observations, we performed the phase-resolved spectroscopy and confirmed two cyclotron absorption lines in the phase-resolved spectra, with their centroid energy ratio $\\sim 1.6-1.7$ in the super-critical luminosity case.","A major hindrance in understanding those CRSFs is the very poorly constrained magnetic inclination angle, which is also a fundamental property of a NS and key to understanding the emission characteristics of a pulsar.","Comparing the phase-resolved CRSF with simulated X-ray spectra, the magnetic inclination angle is found to be $\\gtrsim 70^{\\circ}$, i.e., nearly orthogonal between the NS's spin and magnetic axies.","The implications of an orthogonal rotator and magnetic structure evolution in the accreting X-ray binary are also discussed."],"url":"http://arxiv.org/abs/2404.11829v1","category":"astro-ph.HE"}
{"created":"2024-04-18 00:58:44","title":"An analogue of non-interacting quantum field theory in Riemannian signature","abstract":"In this paper, we define a model of non-interacting quantum fields satisfying $(\\Delta_g-\\lambda^2)\\phi=0$ on a Riemannian scattering space $(M,g)$ with two boundary components, i.e. a manifold with two asymptotically conic ends (meaning asymptotic to the \"large end\" of a cone). Our main result describes a canonical construction of two-point functions satisfying a version of the Hadamard condition.","sentences":["In this paper, we define a model of non-interacting quantum fields satisfying $(\\Delta_g-\\lambda^2)\\phi=0$ on a Riemannian scattering space $(M,g)$ with two boundary components, i.e. a manifold with two asymptotically conic ends (meaning asymptotic to the \"large end\" of a cone).","Our main result describes a canonical construction of two-point functions satisfying a version of the Hadamard condition."],"url":"http://arxiv.org/abs/2404.11821v1","category":"hep-th"}
{"created":"2024-04-18 00:03:07","title":"Future Perspectives for Gamma-ray Burst Detection from Space","abstract":"Since their first discovery in the late 1960s, Gamma-ray bursts have attracted an exponentially growing interest from the international community due to their central role in the most highly debated open questions of the modern research of astronomy, astrophysics, cosmology, and fundamental physics. These range from the intimate nuclear composition of high density material within the core of ultra-dense neuron stars, to stellar evolution via the collapse of massive stars, the production and propagation of gravitational waves, as well as the exploration of the early Universe by unveiling first stars and galaxies (assessing also their evolution and cosmic re-ionization). GRBs have stimulated in the past $\\sim$50 years the development of cutting-edge technological instruments for observations of high energy celestial sources from space, leading to the launch and successful operations of many different scientific missions (several of them still in data taking mode nowadays). In this review, we provide a brief description of the GRB-dedicated missions from space being designed and developed for the future. The list of these projects, not meant to be exhaustive, shall serve as a reference to interested readers to understand what is likely to come next to lead the further development of GRB research and associated phenomenology.","sentences":["Since their first discovery in the late 1960s, Gamma-ray bursts have attracted an exponentially growing interest from the international community due to their central role in the most highly debated open questions of the modern research of astronomy, astrophysics, cosmology, and fundamental physics.","These range from the intimate nuclear composition of high density material within the core of ultra-dense neuron stars, to stellar evolution via the collapse of massive stars, the production and propagation of gravitational waves, as well as the exploration of the early Universe by unveiling first stars and galaxies (assessing also their evolution and cosmic re-ionization).","GRBs have stimulated in the past $\\sim$50 years the development of cutting-edge technological instruments for observations of high energy celestial sources from space, leading to the launch and successful operations of many different scientific missions (several of them still in data taking mode nowadays).","In this review, we provide a brief description of the GRB-dedicated missions from space being designed and developed for the future.","The list of these projects, not meant to be exhaustive, shall serve as a reference to interested readers to understand what is likely to come next to lead the further development of GRB research and associated phenomenology."],"url":"http://arxiv.org/abs/2404.11808v1","category":"astro-ph.IM"}
{"created":"2024-04-17 23:53:19","title":"Nakanishi covariant operator formalism for higher derivative systems: Vector spin-$0$ dual model as a prelude to generalized QED$_4$","abstract":"In this work we extend the Kugo-Ojima-Nakanishi covariant operator formalism to quantize two higher derivative systems, taking into account their extended phase space structures. More specifically, the one describing spin-$0$ particles by a vector field and the generalized electrodynamics. We investigate the commutator structure of these theories and present the definition of their physical Hilbert subspaces. Remarkably, the establishment of a second-class nature for the primary constraints of such models demands a higher derivative structure for the auxiliary field Lagrangian following previous claims. Regarding the first model, it presents a reducible gauge symmetry implying the necessity of two sets of auxiliary fields. We also discuss its massless limit. For the case of the generalized QED$_4$, we derive a set of suitable definitions for the positive-definite Hilbert subspace in order to eliminate contributions from spurious modes and also the problematic negative norm transverse excitation. We show that the Hamiltonian operator taken within the domain of this subspace presents no instabilities. Finally, a set of discussions on the interacting regime are developed to ensure that the scattering processes restricted to the physical Hilbert subspace remain unitary even at this context.","sentences":["In this work we extend the Kugo-Ojima-Nakanishi covariant operator formalism to quantize two higher derivative systems, taking into account their extended phase space structures.","More specifically, the one describing spin-$0$ particles by a vector field and the generalized electrodynamics.","We investigate the commutator structure of these theories and present the definition of their physical Hilbert subspaces.","Remarkably, the establishment of a second-class nature for the primary constraints of such models demands a higher derivative structure for the auxiliary field Lagrangian following previous claims.","Regarding the first model, it presents a reducible gauge symmetry implying the necessity of two sets of auxiliary fields.","We also discuss its massless limit.","For the case of the generalized QED$_4$, we derive a set of suitable definitions for the positive-definite Hilbert subspace in order to eliminate contributions from spurious modes and also the problematic negative norm transverse excitation.","We show that the Hamiltonian operator taken within the domain of this subspace presents no instabilities.","Finally, a set of discussions on the interacting regime are developed to ensure that the scattering processes restricted to the physical Hilbert subspace remain unitary even at this context."],"url":"http://arxiv.org/abs/2404.11805v1","category":"hep-th"}
{"created":"2024-04-17 23:42:11","title":"Semiclassical bremsstrahlung from a charge radially falling into a Schwarzschild black hole","abstract":"A semiclassical investigation of the electromagnetic radiation emitted by a charged particle in a radially freely falling motion in Schwarzschild spacetime is carried out. We use quantum field theory at tree level to obtain the one-particle-emission amplitudes. We analyze and compare the energy spectrum and total energy released, which are calculated from these amplitudes, for particles with varying initial positions and for particles originating from infinity with varying kinetic energy. We also compare the results with those due to a falling charged \"string\" extended in the radial direction.","sentences":["A semiclassical investigation of the electromagnetic radiation emitted by a charged particle in a radially freely falling motion in Schwarzschild spacetime is carried out.","We use quantum field theory at tree level to obtain the one-particle-emission amplitudes.","We analyze and compare the energy spectrum and total energy released, which are calculated from these amplitudes, for particles with varying initial positions and for particles originating from infinity with varying kinetic energy.","We also compare the results with those due to a falling charged \"string\" extended in the radial direction."],"url":"http://arxiv.org/abs/2404.11801v1","category":"gr-qc"}
{"created":"2024-04-17 22:44:03","title":"Inducing mechanical self-healing in glassy polymer melts","abstract":"Glassy polymer melts such as the plastics used in pipes, structural materials, and medical devices are ubiquitous in daily life. They accumulate damage over time due to their use, which limits their functionalities and demands periodic replacement. The resulting economic and social burden could be mitigated by the design of self-healing mechanisms that expand the lifespan of materials. However, the characteristic low molecular mobility in glassy polymer melts intrinsically limits the design of self-healing behavior. We demonstrate through numerical simulations that controlled oscillatory deformations enhance the local molecular mobility of glassy polymers without compromising their structural or mechanical stability. We apply this principle to increase the molecular mobility around the surface of a crack, inducing fracture repair and recovering the mechanical properties of the pristine material. Our findings establish a general physical mechanism of self-healing in glasses that may inspire the design and processing of new glassy materials.","sentences":["Glassy polymer melts such as the plastics used in pipes, structural materials, and medical devices are ubiquitous in daily life.","They accumulate damage over time due to their use, which limits their functionalities and demands periodic replacement.","The resulting economic and social burden could be mitigated by the design of self-healing mechanisms that expand the lifespan of materials.","However, the characteristic low molecular mobility in glassy polymer melts intrinsically limits the design of self-healing behavior.","We demonstrate through numerical simulations that controlled oscillatory deformations enhance the local molecular mobility of glassy polymers without compromising their structural or mechanical stability.","We apply this principle to increase the molecular mobility around the surface of a crack, inducing fracture repair and recovering the mechanical properties of the pristine material.","Our findings establish a general physical mechanism of self-healing in glasses that may inspire the design and processing of new glassy materials."],"url":"http://arxiv.org/abs/2404.11787v1","category":"cond-mat.soft"}
{"created":"2024-04-17 22:13:49","title":"Casimir Effect and Holographic Dual of Wedges","abstract":"This paper investigates the Casimir effect of a wedge and its holographic dual. We prove that the displacement operator universally determines the wedge Casimir effect in the smooth limit. Besides, we argue that the wedge Casimir energy increases with the opening angle and test it with several examples. Furthermore, we construct the holographic dual of wedges in AdS/BCFT in general dimensions. We verify that our proposal can produce universal relations within smooth and singular limits. We find that the negative brane tension tends to yield smaller wedge Casimir energy. Next, we discuss the wedge contribution to holographic entanglement entropy and find it increases with the opening angle, similar to the wedge Casimir energy. Finally, we briefly discuss the holographic polygon in AdS$_3$/BCFT$_2$ and its generalization to higher dimensions.","sentences":["This paper investigates the Casimir effect of a wedge and its holographic dual.","We prove that the displacement operator universally determines the wedge Casimir effect in the smooth limit.","Besides, we argue that the wedge Casimir energy increases with the opening angle and test it with several examples.","Furthermore, we construct the holographic dual of wedges in AdS/BCFT in general dimensions.","We verify that our proposal can produce universal relations within smooth and singular limits.","We find that the negative brane tension tends to yield smaller wedge Casimir energy.","Next, we discuss the wedge contribution to holographic entanglement entropy and find it increases with the opening angle, similar to the wedge Casimir energy.","Finally, we briefly discuss the holographic polygon in AdS$_3$/BCFT$_2$ and its generalization to higher dimensions."],"url":"http://arxiv.org/abs/2404.11783v1","category":"hep-th"}
{"created":"2024-04-17 22:06:20","title":"Hydrogen plasma inhibits ion beam restructuring of materials","abstract":"Focused ion beam (FIB) techniques are employed widely for nanofabrication, and processing of materials and devices. However, ion irradiation often gives rise to severe damage due to atomic displacements that cause defect formation, migration and clustering within the ion-solid interaction volume. The resulting restructuring degrades the functionality of materials, and limits the utility FIB ablation and nanofabrication techniques. Here we show that such restructuring can be inhibited by performing FIB irradiation in a hydrogen plasma environment via chemical pathways that modify defect binding energies and transport kinetics, as well as material ablation rates. The method is minimally-invasive and has the potential to greatly expand the utility of FIB nanofabrication techniques in processing of functional materials and devices.","sentences":["Focused ion beam (FIB) techniques are employed widely for nanofabrication, and processing of materials and devices.","However, ion irradiation often gives rise to severe damage due to atomic displacements that cause defect formation, migration and clustering within the ion-solid interaction volume.","The resulting restructuring degrades the functionality of materials, and limits the utility FIB ablation and nanofabrication techniques.","Here we show that such restructuring can be inhibited by performing FIB irradiation in a hydrogen plasma environment via chemical pathways that modify defect binding energies and transport kinetics, as well as material ablation rates.","The method is minimally-invasive and has the potential to greatly expand the utility of FIB nanofabrication techniques in processing of functional materials and devices."],"url":"http://arxiv.org/abs/2404.11779v1","category":"physics.app-ph"}
{"created":"2024-04-17 22:01:54","title":"Direct measurement of energy transfer in strongly driven rotating turbulence","abstract":"A short, abrupt increase in energy injection rate into steady strongly-driven rotating turbulent flow is used as a probe for energy transfer in the system. The injected excessive energy is localized in time and space and its spectra differ from those of the steady turbulent flow. This allows measuring energy transfer rates, in three different domains: In real space, the injected energy propagates within the turbulent field, as a wave packet of inertial waves. In the frequency domain, energy is transferred non-locally to the low, quasi-geostrophic modes. In wavenumber space, energy locally cascades toward small wavenumbers, in a rate that is consistent with two-dimensionsal (2D) turbulence models. Surprisingly however, the inverse cascade of energy is mediated by inertial waves that propagate within the flow with small, but non-vanishing frequency. Our observations differ from measurements and theoretical predictions of weakly driven turbulence. Yet, they show that in strongly-driven rotating turbulence, inertial waves play an important role in energy transfer, even at the vicinity of the 2D manifold.","sentences":["A short, abrupt increase in energy injection rate into steady strongly-driven rotating turbulent flow is used as a probe for energy transfer in the system.","The injected excessive energy is localized in time and space and its spectra differ from those of the steady turbulent flow.","This allows measuring energy transfer rates, in three different domains: In real space, the injected energy propagates within the turbulent field, as a wave packet of inertial waves.","In the frequency domain, energy is transferred non-locally to the low, quasi-geostrophic modes.","In wavenumber space, energy locally cascades toward small wavenumbers, in a rate that is consistent with two-dimensionsal (2D) turbulence models.","Surprisingly however, the inverse cascade of energy is mediated by inertial waves that propagate within the flow with small, but non-vanishing frequency.","Our observations differ from measurements and theoretical predictions of weakly driven turbulence.","Yet, they show that in strongly-driven rotating turbulence, inertial waves play an important role in energy transfer, even at the vicinity of the 2D manifold."],"url":"http://arxiv.org/abs/2404.11777v1","category":"physics.flu-dyn"}
{"created":"2024-04-17 21:35:05","title":"Modelling infectious disease transmission dynamics in conference environments: An individual-based approach","abstract":"The global public health landscape is perpetually challenged by the looming threat of infectious diseases. Central to addressing this concern is the imperative to prevent and manage disease transmission during pandemics, particularly in unique settings. This study addresses the transmission dynamics of infectious diseases within conference venues, presenting a computational model designed to simulate transmission processes within a condensed timeframe (one day), beginning with sporadic cases. Our model intricately captures the activities of individual attendees within the conference venue, encompassing meetings, rest intervals, and meal breaks. While meetings entail proximity seating, rest and lunch periods allow attendees to interact with diverse individuals. Moreover, the restroom environment poses an additional avenue for potential infection transmission. Employing an individual-based model, we meticulously replicated the transmission dynamics of infectious diseases, with a specific emphasis on close-contact interactions between infected and susceptible individuals. Through comprehensive analysis of model simulations, we elucidated the intricacies of disease transmission dynamics within conference settings and assessed the efficacy of control strategies to curb disease dissemination. Ultimately, our study proffers a numerical framework for assessing the risk of infectious disease transmission during short-duration conferences, furnishing conference organizers with valuable insights to inform the implementation of targeted prevention and control measures.","sentences":["The global public health landscape is perpetually challenged by the looming threat of infectious diseases.","Central to addressing this concern is the imperative to prevent and manage disease transmission during pandemics, particularly in unique settings.","This study addresses the transmission dynamics of infectious diseases within conference venues, presenting a computational model designed to simulate transmission processes within a condensed timeframe (one day), beginning with sporadic cases.","Our model intricately captures the activities of individual attendees within the conference venue, encompassing meetings, rest intervals, and meal breaks.","While meetings entail proximity seating, rest and lunch periods allow attendees to interact with diverse individuals.","Moreover, the restroom environment poses an additional avenue for potential infection transmission.","Employing an individual-based model, we meticulously replicated the transmission dynamics of infectious diseases, with a specific emphasis on close-contact interactions between infected and susceptible individuals.","Through comprehensive analysis of model simulations, we elucidated the intricacies of disease transmission dynamics within conference settings and assessed the efficacy of control strategies to curb disease dissemination.","Ultimately, our study proffers a numerical framework for assessing the risk of infectious disease transmission during short-duration conferences, furnishing conference organizers with valuable insights to inform the implementation of targeted prevention and control measures."],"url":"http://arxiv.org/abs/2404.11759v1","category":"q-bio.PE"}
